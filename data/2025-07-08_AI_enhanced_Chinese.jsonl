{"id": "2507.03365", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03365", "abs": "https://arxiv.org/abs/2507.03365", "authors": ["Hanfang Liang", "Shenghai Yuan", "Fen Liu", "Yizhuo Yang", "Bing Wang", "Zhuyu Huang", "Chenyang Shi", "Jing Jin"], "title": "Label-Free Long-Horizon 3D UAV Trajectory Prediction via Motion-Aligned RGB and Event Cues", "comment": null, "summary": "The widespread use of consumer drones has introduced serious challenges for\nairspace security and public safety. Their high agility and unpredictable\nmotion make drones difficult to track and intercept. While existing methods\nfocus on detecting current positions, many counter-drone strategies rely on\nforecasting future trajectories and thus require more than reactive detection\nto be effective. To address this critical gap, we propose an unsupervised\nvision-based method for predicting the three-dimensional trajectories of\ndrones. Our approach first uses an unsupervised technique to extract drone\ntrajectories from raw LiDAR point clouds, then aligns these trajectories with\ncamera images through motion consistency to generate reliable pseudo-labels. We\nthen combine kinematic estimation with a visual Mamba neural network in a\nself-supervised manner to predict future drone trajectories. We evaluate our\nmethod on the challenging MMAUD dataset, including the V2 sequences that\nfeature wide-field-of-view multimodal sensors and dynamic UAV motion in urban\nscenes. Extensive experiments show that our framework outperforms supervised\nimage-only and audio-visual baselines in long-horizon trajectory prediction,\nreducing 5-second 3D error by around 40 percent without using any manual 3D\nlabels. The proposed system offers a cost-effective, scalable alternative for\nreal-time counter-drone deployment. All code will be released upon acceptance\nto support reproducible research in the robotics community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u89c6\u89c9\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65e0\u4eba\u673a\u7684\u4e09\u7ef4\u8f68\u8ff9\uff0c\u7ed3\u5408LiDAR\u70b9\u4e91\u548c\u76f8\u673a\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u7684\u5e7f\u6cdb\u4f7f\u7528\u5bf9\u7a7a\u57df\u5b89\u5168\u548c\u516c\u5171\u5b89\u5168\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9884\u6d4b\u5176\u672a\u6765\u8f68\u8ff9\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u65e0\u76d1\u7763\u6280\u672f\u4eceLiDAR\u70b9\u4e91\u63d0\u53d6\u8f68\u8ff9\uff0c\u901a\u8fc7\u8fd0\u52a8\u4e00\u81f4\u6027\u5bf9\u9f50\u76f8\u673a\u56fe\u50cf\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u8fd0\u52a8\u4f30\u8ba1\u548c\u89c6\u89c9Mamba\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u6d4b\u3002", "result": "\u5728MMAUD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c5\u79d23D\u8bef\u5dee\u964d\u4f4e\u7ea640%\uff0c\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u53cd\u65e0\u4eba\u673a\u90e8\u7f72\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2507.02928", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02928", "abs": "https://arxiv.org/abs/2507.02928", "authors": ["Hao Yang", "Haoxuan Li", "Luyu Chen", "Haoxiang Wang", "Xu Chen", "Mingming Gong"], "title": "Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models", "comment": null, "summary": "Hidden confounding remains a central challenge in estimating treatment\neffects from observational data, as unobserved variables can lead to biased\ncausal estimates. While recent work has explored the use of large language\nmodels (LLMs) for causal inference, most approaches still rely on the\nunconfoundedness assumption. In this paper, we make the first attempt to\nmitigate hidden confounding using LLMs. We propose ProCI (Progressive\nConfounder Imputation), a framework that elicits the semantic and world\nknowledge of LLMs to iteratively generate, impute, and validate hidden\nconfounders. ProCI leverages two key capabilities of LLMs: their strong\nsemantic reasoning ability, which enables the discovery of plausible\nconfounders from both structured and unstructured inputs, and their embedded\nworld knowledge, which supports counterfactual reasoning under latent\nconfounding. To improve robustness, ProCI adopts a distributional reasoning\nstrategy instead of direct value imputation to prevent the collapsed outputs.\nExtensive experiments demonstrate that ProCI uncovers meaningful confounders\nand significantly improves treatment effect estimation across various datasets\nand LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faProCI\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bed\u4e49\u63a8\u7406\u548c\u4e16\u754c\u77e5\u8bc6\uff0c\u9010\u6b65\u751f\u6210\u3001\u586b\u5145\u548c\u9a8c\u8bc1\u9690\u85cf\u6df7\u6742\u56e0\u7d20\uff0c\u4ee5\u89e3\u51b3\u89c2\u6d4b\u6570\u636e\u4e2d\u9690\u85cf\u6df7\u6742\u5bfc\u81f4\u7684\u56e0\u679c\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u89c2\u6d4b\u6570\u636e\u4e2d\u7684\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u4f1a\u5bfc\u81f4\u56e0\u679c\u4f30\u8ba1\u504f\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u65e0\u6df7\u6742\u5047\u8bbe\uff0c\u800cLLMs\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "\u63d0\u51faProCI\u6846\u67b6\uff0c\u901a\u8fc7LLMs\u7684\u8bed\u4e49\u63a8\u7406\u548c\u4e16\u754c\u77e5\u8bc6\uff0c\u8fed\u4ee3\u751f\u6210\u3001\u586b\u5145\u548c\u9a8c\u8bc1\u9690\u85cf\u6df7\u6742\u56e0\u7d20\uff0c\u5e76\u91c7\u7528\u5206\u5e03\u63a8\u7406\u7b56\u7565\u907f\u514d\u8f93\u51fa\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProCI\u80fd\u53d1\u73b0\u6709\u610f\u4e49\u6df7\u6742\u56e0\u7d20\uff0c\u663e\u8457\u63d0\u5347\u591a\u79cd\u6570\u636e\u96c6\u548cLLMs\u4e0b\u7684\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u3002", "conclusion": "ProCI\u9996\u6b21\u5229\u7528LLMs\u89e3\u51b3\u9690\u85cf\u6df7\u6742\u95ee\u9898\uff0c\u4e3a\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.02903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02903", "abs": "https://arxiv.org/abs/2507.02903", "authors": ["AMM Nurul Alam", "Abdul Samad", "AMM Shamsul Alam", "Jahan Ara Monti", "Ayesha Muazzam"], "title": "Harnessing Near-Infrared Spectroscopy and Machine Learning for Traceable Classification of Hanwoo and Holstein Beef", "comment": null, "summary": "This study evaluates the use of Near-Infrared spectroscopy (NIRS) combined\nwith advanced machine learning (ML) techniques to differentiate Hanwoo beef\n(HNB) and Holstein beef (HLB) to address food authenticity, mislabeling, and\nadulteration. Rapid and non-invasive spectral data were attained by a portable\nNIRS, recording absorbance data within the wavelength range of 700 to 1100 nm.\nA total of 40 Longissimus lumborum samples, evenly split between HNB and HLB,\nwere obtained from a local hypermarket. Data analysis using Principal Component\nAnalysis (PCA) demonstrated distinct spectral patterns associated with chemical\nchanges, clearly separating the two beef varieties and accounting for 93.72% of\nthe total variance. ML models, including Linear Discriminant Analysis (LDA),\nSupport Vector Machine (SVM), Logistic Regression (LR), Random Forest, Gradient\nBoosting (GB), K-Nearest Neighbors, Decision Tree (DT), Naive Bayes (NB), and\nNeural Networks (NN), were implemented, optimized through hyperparameter\ntuning, and validated by 5-fold cross-validation techniques to enhance model\nrobustness and prevent overfitting. Random Forest provided the highest\npredictive accuracy with a Receiver Operating Characteristic (ROC) Area Under\nthe Curve (AUC) of 0.8826, closely followed by the SVM model at 0.8747.\nFurthermore, GB and NN algorithms exhibited satisfactory performances, with\ncross-validation scores of 0.752. Notably, the NN model achieved the highest\nrecall rate of 0.7804, highlighting its suitability in scenarios requiring\nheightened sensitivity. DT and NB exhibited comparatively lower predictive\nperformance. The LR and SVM models emerged as optimal choices by effectively\nbalancing high accuracy, precision, and recall. This study confirms that\nintegrating NIRS with ML techniques offers a powerful and reliable method for\nmeat authenticity, significantly contributing to detecting food fraud.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u8fd1\u7ea2\u5916\u5149\u8c31\uff08NIRS\uff09\u7ed3\u5408\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6280\u672f\u533a\u5206\u97e9\u725b\uff08HNB\uff09\u548c\u8377\u65af\u5766\u725b\u8089\uff08HLB\uff09\uff0c\u4ee5\u89e3\u51b3\u98df\u54c1\u771f\u5b9e\u6027\u548c\u6807\u7b7e\u95ee\u9898\u3002\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cAUC\u4e3a0.8826\u3002", "motivation": "\u89e3\u51b3\u98df\u54c1\u771f\u5b9e\u6027\u3001\u6807\u7b7e\u9519\u8bef\u548c\u63ba\u5047\u95ee\u9898\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u975e\u4fb5\u5165\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4fbf\u643a\u5f0fNIRS\u91c7\u96c6\u5149\u8c31\u6570\u636e\uff0c\u7ed3\u5408PCA\u548c\u591a\u79cdML\u6a21\u578b\uff08\u5982LDA\u3001SVM\u3001\u968f\u673a\u68ee\u6797\u7b49\uff09\u8fdb\u884c\u5206\u6790\u548c\u4f18\u5316\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08AUC 0.8826\uff09\uff0cSVM\u6b21\u4e4b\uff08AUC 0.8747\uff09\u3002NN\u6a21\u578b\u53ec\u56de\u7387\u6700\u9ad8\uff080.7804\uff09\u3002", "conclusion": "NIRS\u7ed3\u5408ML\u6280\u672f\u662f\u68c0\u6d4b\u8089\u7c7b\u771f\u5b9e\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5bf9\u6253\u51fb\u98df\u54c1\u6b3a\u8bc8\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.02904", "categories": ["cs.CV", "cs.AI", "I.2.7; I.2.10; I.4"], "pdf": "https://arxiv.org/pdf/2507.02904", "abs": "https://arxiv.org/abs/2507.02904", "authors": ["Charlton Teo"], "title": "Enhancing Sports Strategy with Video Analytics and Data Mining: Assessing the effectiveness of Multimodal LLMs in tennis video analysis", "comment": "B.Comp. dissertation", "summary": "The use of Large Language Models (LLMs) in recent years has also given rise\nto the development of Multimodal LLMs (MLLMs). These new MLLMs allow us to\nprocess images, videos and even audio alongside textual inputs. In this\nproject, we aim to assess the effectiveness of MLLMs in analysing sports\nvideos, focusing mainly on tennis videos. Despite research done on tennis\nanalysis, there remains a gap in models that are able to understand and\nidentify the sequence of events in a tennis rally, which would be useful in\nother fields of sports analytics. As such, we will mainly assess the MLLMs on\ntheir ability to fill this gap - to classify tennis actions, as well as their\nability to identify these actions in a sequence of tennis actions in a rally.\nWe further looked into ways we can improve the MLLMs' performance, including\ndifferent training methods and even using them together with other traditional\nmodels.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5206\u6790\u7f51\u7403\u89c6\u9891\u4e2d\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u8bc6\u522b\u7f51\u7403\u56de\u5408\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u7f51\u7403\u5206\u6790\u4e2d\u5b58\u5728\u6a21\u578b\u65e0\u6cd5\u7406\u89e3\u7f51\u7403\u56de\u5408\u4e8b\u4ef6\u5e8f\u5217\u7684\u7a7a\u767d\uff0cMLLMs\u6709\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30MLLMs\u5728\u5206\u7c7b\u7f51\u7403\u52a8\u4f5c\u53ca\u8bc6\u522b\u52a8\u4f5c\u5e8f\u5217\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u6539\u8fdb\u65b9\u6cd5\u5982\u4e0d\u540c\u8bad\u7ec3\u65b9\u5f0f\u4e0e\u4f20\u7edf\u6a21\u578b\u7ed3\u5408\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u7814\u7a76\u76ee\u6807\u662f\u63d0\u5347MLLMs\u5728\u7f51\u7403\u89c6\u9891\u5206\u6790\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "MLLMs\u5728\u7f51\u7403\u89c6\u9891\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u586b\u8865\u73b0\u6709\u6280\u672f\u7a7a\u767d\u3002"}}
{"id": "2507.03517", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.03517", "abs": "https://arxiv.org/abs/2507.03517", "authors": ["Antonio Gonz\u00e1lez-Morgado", "Sander Smits", "Guillermo Heredia", "Anibal Ollero", "Alexandre Krupa", "Fran\u00e7ois Chaumette", "Fabien Spindler", "Antonio Franchi", "Chiara Gabellieri"], "title": "Multi-robot Aerial Soft Manipulator For Floating Litter Collection", "comment": null, "summary": "Removing floating litter from water bodies is crucial to preserving aquatic\necosystems and preventing environmental pollution. In this work, we present a\nmulti-robot aerial soft manipulator for floating litter collection, leveraging\nthe capabilities of aerial robots. The proposed system consists of two aerial\nrobots connected by a flexible rope manipulator, which collects floating litter\nusing a hook-based tool. Compared to single-aerial-robot solutions, the use of\ntwo aerial robots increases payload capacity and flight endurance while\nreducing the downwash effect at the manipulation point, located at the midpoint\nof the rope. Additionally, we employ an optimization-based rope-shape planner\nto compute the desired rope shape. The planner incorporates an adaptive\nbehavior that maximizes grasping capabilities near the litter while minimizing\nrope tension when farther away. The computed rope shape trajectory is\ncontrolled by a shape visual servoing controller, which approximates the rope\nas a parabola. The complete system is validated in outdoor experiments,\ndemonstrating successful grasping operations. An ablation study highlights how\nthe planner's adaptive mechanism improves the success rate of the operation.\nFurthermore, real-world tests in a water channel confirm the effectiveness of\nour system in floating litter collection. These results demonstrate the\npotential of aerial robots for autonomous litter removal in aquatic\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u7a7a\u4e2d\u8f6f\u64cd\u7eb5\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u6c34\u9762\u6f02\u6d6e\u5783\u573e\u6536\u96c6\uff0c\u901a\u8fc7\u53cc\u65e0\u4eba\u673a\u548c\u67d4\u6027\u7ef3\u7d22\u64cd\u7eb5\u5668\u63d0\u9ad8\u8d1f\u8f7d\u80fd\u529b\u548c\u98de\u884c\u8010\u529b\uff0c\u5e76\u51cf\u5c11\u4e0b\u6d17\u6548\u5e94\u3002", "motivation": "\u4fdd\u62a4\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u548c\u9632\u6b62\u73af\u5883\u6c61\u67d3\u9700\u8981\u6709\u6548\u7684\u6c34\u9762\u6f02\u6d6e\u5783\u573e\u6e05\u9664\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u7531\u4e24\u67b6\u65e0\u4eba\u673a\u548c\u67d4\u6027\u7ef3\u7d22\u64cd\u7eb5\u5668\u7ec4\u6210\uff0c\u91c7\u7528\u57fa\u4e8e\u94a9\u7684\u5de5\u5177\u6536\u96c6\u5783\u573e\uff0c\u5e76\u5229\u7528\u4f18\u5316\u7ef3\u7d22\u5f62\u72b6\u89c4\u5212\u5668\u548c\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u3002", "result": "\u6237\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u81ea\u9002\u5e94\u89c4\u5212\u5668\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6210\u529f\u7387\uff0c\u5b9e\u9645\u6c34\u9053\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u5783\u573e\u6536\u96c6\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u65e0\u4eba\u673a\u5728\u81ea\u4e3b\u6e05\u9664\u6c34\u751f\u73af\u5883\u5783\u573e\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.03525", "categories": ["cs.AI", "cs.SY", "eess.SY", "I.2; K.6; D.2.9"], "pdf": "https://arxiv.org/pdf/2507.03525", "abs": "https://arxiv.org/abs/2507.03525", "authors": ["David Manheim", "Aidan Homewood"], "title": "Limits of Safe AI Deployment: Differentiating Oversight and Control", "comment": null, "summary": "Oversight and control (collectively, supervision) are often invoked as key\nlevers for ensuring that AI systems are accountable, reliable, and able to\nfulfill governance and management requirements. However, the concepts are\nfrequently conflated or insufficiently distinguished in academic and policy\ndiscourse, undermining efforts to design or evaluate systems that should remain\nunder meaningful human supervision.\n  This paper undertakes a targeted critical review of literature on supervision\noutside of AI, along with a brief summary of past work on the topic related to\nAI. We then differentiate control as being ex-ante or real-time, and\noperational rather than policy or governance. In contrast, oversight is either\na policy and governance function, or is ex-post. We suggest that control aims\nto prevent failures. In contrast, oversight often focuses on detection,\nremediation, or incentives for future prevention; all preventative oversight\nstrategies nonetheless necessitate control.\n  Building on this foundation, we make three contributions. First, we propose a\ntheoretically-informed yet policy-grounded framework that articulates the\nconditions under which each mechanism is possible, where they fall short, and\nwhat is required to make them meaningful in practice. Second, we outline how\nsupervision methods should be documented and integrated into risk management,\nand drawing on the Microsoft Responsible AI Maturity Model, we outline a\nmaturity model for AI supervision. Third, we explicitly highlight some\nboundaries of these mechanisms, including where they apply, where they fail,\nand where it is clear that no existing methods suffice. This foregrounds the\nquestion of whether meaningful supervision is possible in a given deployment\ncontext, and can support regulators, auditors, and practitioners in identifying\nboth present limitations and the need for new conceptual and technical\nadvances.", "AI": {"tldr": "\u8bba\u6587\u533a\u5206\u4e86AI\u7cfb\u7edf\u4e2d\u7684\u76d1\u7763\u4e0e\u63a7\u5236\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u660e\u786e\u5176\u9002\u7528\u6761\u4ef6\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6210\u719f\u5ea6\u6a21\u578b\u4ee5\u652f\u6301\u5b9e\u8df5\u3002", "motivation": "\u5728AI\u9886\u57df\uff0c\u76d1\u7763\u4e0e\u63a7\u5236\u5e38\u88ab\u6df7\u6dc6\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7cfb\u7edf\u65f6\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u7c7b\u76d1\u7763\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u533a\u5206\u76d1\u7763\u4e0e\u63a7\u5236\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u548c\u6210\u719f\u5ea6\u6a21\u578b\u3002", "result": "\u660e\u786e\u4e86\u76d1\u7763\u4e0e\u63a7\u5236\u7684\u5b9a\u4e49\u3001\u9002\u7528\u6761\u4ef6\u53ca\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u8df5\u6307\u5357\u3002", "conclusion": "\u8bba\u6587\u4e3aAI\u7cfb\u7edf\u7684\u76d1\u7763\u4e0e\u63a7\u5236\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\uff0c\u652f\u6301\u76d1\u7ba1\u548c\u5b9e\u8df5\u8005\u8bc6\u522b\u5c40\u9650\u4e0e\u9700\u6c42\u3002"}}
{"id": "2507.02934", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.02934", "abs": "https://arxiv.org/abs/2507.02934", "authors": ["Md. Nisharul Hasan"], "title": "Predictive Maintenance Optimization for Smart Vending Machines Using IoT and Machine Learning", "comment": "20 pages, 3 figures and 4 tables", "summary": "The increasing proliferation of vending machines in public and commercial\nenvironments has placed a growing emphasis on operational efficiency and\ncustomer satisfaction. Traditional maintenance approaches either reactive or\ntime-based preventive are limited in their ability to preempt machine failures,\nleading to unplanned downtimes and elevated service costs. This research\npresents a novel predictive maintenance framework tailored for vending machines\nby leveraging Internet of Things (IoT) sensors and machine learning (ML)\nalgorithms. The proposed system continuously monitors machine components and\noperating conditions in real time and applies predictive models to forecast\nfailures before they occur. This enables timely maintenance scheduling,\nminimizing downtime and extending machine lifespan. The framework was validated\nthrough simulated fault data and performance evaluation using classification\nalgorithms. Results show a significant improvement in early fault detection and\na reduction in redundant service interventions. The findings indicate that\npredictive maintenance systems, when integrated into vending infrastructure,\ncan transform operational efficiency and service reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u8054\u7f51\u4f20\u611f\u5668\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u81ea\u52a8\u552e\u8d27\u673a\u9884\u6d4b\u6027\u7ef4\u62a4\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u7ef4\u62a4\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7ef4\u62a4\u65b9\u6cd5\uff08\u53cd\u5e94\u6027\u6216\u5b9a\u65f6\u9884\u9632\u6027\uff09\u65e0\u6cd5\u6709\u6548\u9884\u9632\u673a\u5668\u6545\u969c\uff0c\u5bfc\u81f4\u505c\u673a\u548c\u670d\u52a1\u6210\u672c\u589e\u52a0\u3002", "method": "\u5229\u7528\u7269\u8054\u7f51\u4f20\u611f\u5668\u5b9e\u65f6\u76d1\u63a7\u673a\u5668\u72b6\u6001\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9884\u6d4b\u6545\u969c\uff0c\u5b9e\u73b0\u53ca\u65f6\u7ef4\u62a4\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6545\u969c\u6570\u636e\u9a8c\u8bc1\uff0c\u5206\u7c7b\u7b97\u6cd5\u663e\u793a\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u663e\u8457\u63d0\u5347\uff0c\u5197\u4f59\u7ef4\u62a4\u51cf\u5c11\u3002", "conclusion": "\u9884\u6d4b\u6027\u7ef4\u62a4\u7cfb\u7edf\u53ef\u663e\u8457\u63d0\u5347\u81ea\u52a8\u552e\u8d27\u673a\u7684\u8fd0\u8425\u6548\u7387\u548c\u670d\u52a1\u53ef\u9760\u6027\u3002"}}
{"id": "2507.02986", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02986", "abs": "https://arxiv.org/abs/2507.02986", "authors": ["Seshu Tirupathi", "Dhaval Salwala", "Elizabeth Daly", "Inge Vejsbjerg"], "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.", "AI": {"tldr": "GAF-Guard\u662f\u4e00\u4e2a\u65b0\u578b\u7684LLM\u6cbb\u7406\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u7528\u6237\u3001\u7528\u4f8b\u548c\u6a21\u578b\u672c\u8eab\uff0c\u901a\u8fc7\u81ea\u4e3b\u4ee3\u7406\u68c0\u6d4b\u98ce\u9669\u5e76\u6301\u7eed\u76d1\u63a7\uff0c\u4ee5\u589e\u5f3aAI\u5b89\u5168\u6027\u548c\u7528\u6237\u671f\u671b\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e25\u683c\u76d1\u63a7\u4ee5\u9632\u6b62\u8d1f\u9762\u540e\u679c\u5e76\u786e\u4fdd\u7a33\u5065\u6027\uff0c\u540c\u65f6\u9700\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002\u5f53\u524d\u76d1\u63a7\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5177\u4f53\u7528\u4f8b\u548c\u7528\u6237\u9700\u6c42\u7684\u5173\u6ce8\u3002", "method": "GAF-Guard\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u81ea\u4e3b\u4ee3\u7406\uff0c\u8bc6\u522b\u98ce\u9669\u5e76\u6fc0\u6d3b\u68c0\u6d4b\u5de5\u5177\uff0c\u5728\u7279\u5b9a\u7528\u4f8b\u4e2d\u5b9e\u73b0\u6301\u7eed\u76d1\u63a7\u548c\u62a5\u544a\u3002", "result": "GAF-Guard\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u76d1\u63a7LLM\u5e94\u7528\u90e8\u7f72\u4e2d\u7684\u98ce\u9669\uff0c\u63d0\u5347AI\u5b89\u5168\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "GAF-Guard\u4e3aLLM\u6cbb\u7406\u63d0\u4f9b\u4e86\u4ee5\u7528\u6237\u548c\u7528\u4f8b\u4e3a\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.02990", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02990", "abs": "https://arxiv.org/abs/2507.02990", "authors": ["Annika M Schoene", "Cansu Canca"], "title": "`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to increasingly\nsophisticated safety protocols and features designed to prevent harmful,\nunethical, or unauthorized outputs. However, these guardrails remain\nsusceptible to novel and creative forms of adversarial prompting, including\nmanually generated test cases. In this work, we present two new test cases in\nmental health for (i) suicide and (ii) self-harm, using multi-step,\nprompt-level jailbreaking and bypass built-in content and safety filters. We\nshow that user intent is disregarded, leading to the generation of detailed\nharmful content and instructions that could cause real-world harm. We conduct\nan empirical evaluation across six widely available LLMs, demonstrating the\ngeneralizability and reliability of the bypass. We assess these findings and\nthe multilayered ethical tensions that they present for their implications on\nprompt-response filtering and context- and task-specific model development. We\nrecommend a more comprehensive and systematic approach to AI safety and ethics\nwhile emphasizing the need for continuous adversarial testing in\nsafety-critical AI deployments. We also argue that while certain clearly\ndefined safety measures and guardrails can and must be implemented in LLMs,\nensuring robust and comprehensive safety across all use cases and domains\nremains extremely challenging given the current technical maturity of\ngeneral-purpose LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u6d4b\u8bd5\u6848\u4f8b\uff0c\u7528\u4e8e\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\uff0c\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u547c\u5401\u66f4\u5168\u9762\u7684AI\u5b89\u5168\u63aa\u65bd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5df2\u6709\u5b89\u5168\u534f\u8bae\uff0c\u4f46\u4ecd\u6613\u53d7\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\uff0c\u5c24\u5176\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u3002", "method": "\u4f7f\u7528\u591a\u6b65\u63d0\u793a\u7ea7\u8d8a\u72f1\u6280\u672f\uff0c\u7ed5\u8fc7\u5185\u7f6e\u5185\u5bb9\u4e0e\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5bf9\u516d\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7528\u6237\u610f\u56fe\u88ab\u5ffd\u89c6\uff0c\u751f\u6210\u4e86\u53ef\u80fd\u5bfc\u81f4\u73b0\u5b9e\u4f24\u5bb3\u7684\u8be6\u7ec6\u6709\u5bb3\u5185\u5bb9\u3002", "conclusion": "\u9700\u66f4\u7cfb\u7edf\u5316\u7684AI\u5b89\u5168\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6301\u7eed\u5bf9\u6297\u6d4b\u8bd5\uff0c\u5e76\u627f\u8ba4\u5f53\u524d\u901a\u7528LLM\u5728\u5168\u9762\u5b89\u5168\u6027\u4e0a\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2507.03637", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03637", "abs": "https://arxiv.org/abs/2507.03637", "authors": ["Francesca Da Ros", "Michael Soprano", "Luca Di Gaspero", "Kevin Roitero"], "title": "Large Language Models for Combinatorial Optimization: A Systematic Review", "comment": null, "summary": "This systematic review explores the application of Large Language Models\n(LLMs) in Combinatorial Optimization (CO). We report our findings using the\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. We conduct a literature search via Scopus and Google Scholar,\nexamining over 2,000 publications. We assess publications against four\ninclusion and four exclusion criteria related to their language, research\nfocus, publication year, and type. Eventually, we select 103 studies. We\nclassify these studies into semantic categories and topics to provide a\ncomprehensive overview of the field, including the tasks performed by LLMs, the\narchitectures of LLMs, the existing datasets specifically designed for\nevaluating LLMs in CO, and the field of application. Finally, we identify\nfuture directions for leveraging LLMs in this field.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ec4\u5408\u4f18\u5316\uff08CO\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u57fa\u4e8ePRISMA\u6307\u5357\u7b5b\u9009\u4e86103\u7bc7\u7814\u7a76\uff0c\u5206\u7c7b\u603b\u7ed3\u4e86LLMs\u7684\u4efb\u52a1\u3001\u67b6\u6784\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u7ec4\u5408\u4f18\u5316\u9886\u57df\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5168\u9762\u7684\u9886\u57df\u6982\u89c8\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "method": "\u901a\u8fc7Scopus\u548cGoogle Scholar\u68c0\u7d222000\u591a\u7bc7\u6587\u732e\uff0c\u4f9d\u636e\u8bed\u8a00\u3001\u7814\u7a76\u7126\u70b9\u3001\u5e74\u4efd\u548c\u7c7b\u578b\u7b5b\u9009\u51fa103\u7bc7\u7814\u7a76\uff0c\u5e76\u8fdb\u884c\u5206\u7c7b\u5206\u6790\u3002", "result": "\u603b\u7ed3\u4e86LLMs\u5728CO\u4e2d\u7684\u4efb\u52a1\u3001\u67b6\u6784\u3001\u4e13\u7528\u6570\u636e\u96c6\u53ca\u5e94\u7528\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LLMs\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u548c\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.04314", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04314", "abs": "https://arxiv.org/abs/2507.04314", "authors": ["Wenxuan Li", "Yan Dong", "Shaoqiang Qiu", "Bin Han"], "title": "Hardware-Free Event Cameras Temporal Synchronization Based on Event Density Alignment", "comment": "12 pages, 8 figures. Conference paper, International Conference on\n  Intelligent Robotics and Applications 2024", "summary": "Event cameras are a novel type of sensor designed for capturing the dynamic\nchanges of a scene. Due to factors such as trigger and transmission delays, a\ntime offset exists in the data collected by multiple event cameras, leading to\ninaccurate information fusion. Thus, the collected data needs to be\nsynchronized to overcome any potential time offset issue. Hardware\nsynchronization methods require additional circuits, while certain models of\nevent cameras (e.g., CeleX5) do not support hardware synchronization.\nTherefore, this paper proposes a hardware-free event camera synchronization\nmethod. This method determines differences between start times by minimizing\nthe dissimilarity of the event density distributions of different event cameras\nand synchronizes the data by adjusting timestamps. The experiments demonstrate\nthat the method's synchronization error is less than 10ms under various senses\nwith multiple models of event cameras.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u786c\u4ef6\u7684\u591a\u4e8b\u4ef6\u76f8\u673a\u540c\u6b65\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4e8b\u4ef6\u5bc6\u5ea6\u5206\u5e03\u5dee\u5f02\u6765\u6821\u51c6\u65f6\u95f4\u504f\u79fb\uff0c\u5b9e\u9a8c\u8bef\u5dee\u5c0f\u4e8e10ms\u3002", "motivation": "\u591a\u4e8b\u4ef6\u76f8\u673a\u56e0\u89e6\u53d1\u548c\u4f20\u8f93\u5ef6\u8fdf\u5bfc\u81f4\u65f6\u95f4\u504f\u79fb\uff0c\u786c\u4ef6\u540c\u6b65\u65b9\u6cd5\u53d7\u9650\uff0c\u9700\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u4e8b\u4ef6\u5bc6\u5ea6\u5206\u5e03\u5dee\u5f02\u786e\u5b9a\u65f6\u95f4\u5dee\uff0c\u8c03\u6574\u65f6\u95f4\u6233\u5b9e\u73b0\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u548c\u76f8\u673a\u6a21\u578b\u4e0b\u540c\u6b65\u8bef\u5dee\u5c0f\u4e8e10ms\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u786c\u4ef6\u540c\u6b65\u7684\u9650\u5236\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e8b\u4ef6\u76f8\u673a\u6a21\u578b\u3002"}}
{"id": "2507.03040", "categories": ["cs.CV", "cs.LG", "68T10", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.03040", "abs": "https://arxiv.org/abs/2507.03040", "authors": ["Mehrab Hosain", "Rajiv Kapoor"], "title": "Detection of Rail Line Track and Human Beings Near the Track to Avoid Accidents", "comment": "Accepted at COMITCON 2023; Published in Lecture Notes in Electrical\n  Engineering, Vol. 1191, Springer", "summary": "This paper presents an approach for rail line detection and the\nidentification of human beings in proximity to the track, utilizing the YOLOv5\ndeep learning model to mitigate potential accidents. The technique incorporates\nreal-time video data to identify railway tracks with impressive accuracy and\nrecognizes nearby moving objects within a one-meter range, specifically\ntargeting the identification of humans. This system aims to enhance safety\nmeasures in railway environments by providing real-time alerts for any detected\nhuman presence close to the track. The integration of a functionality to\nidentify objects at a longer distance further fortifies the preventative\ncapabilities of the system. With a precise focus on real-time object detection,\nthis method is poised to deliver significant contributions to the existing\ntechnologies in railway safety. The effectiveness of the proposed method is\ndemonstrated through a comprehensive evaluation, yielding a remarkable\nimprovement in accuracy over existing methods. These results underscore the\npotential of this approach to revolutionize safety measures in railway\nenvironments, providing a substantial contribution to accident prevention\nstrategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv5\u7684\u94c1\u8def\u7ebf\u68c0\u6d4b\u548c\u9644\u8fd1\u4eba\u5458\u8bc6\u522b\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u5b9e\u65f6\u89c6\u9891\u6570\u636e\u63d0\u5347\u94c1\u8def\u5b89\u5168\u3002", "motivation": "\u51cf\u5c11\u94c1\u8def\u4e8b\u6545\uff0c\u901a\u8fc7\u5b9e\u65f6\u68c0\u6d4b\u8f68\u9053\u9644\u8fd1\u4eba\u5458\u6765\u589e\u5f3a\u5b89\u5168\u63aa\u65bd\u3002", "method": "\u5229\u7528YOLOv5\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u65f6\u5206\u6790\u89c6\u9891\u6570\u636e\uff0c\u68c0\u6d4b\u8f68\u9053\u5e76\u8bc6\u522b\u4e00\u7c73\u8303\u56f4\u5185\u7684\u4eba\u7c7b\u3002", "result": "\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6709\u6548\u8bc6\u522b\u4eba\u7c7b\u5e76\u63d0\u4f9b\u5b9e\u65f6\u8b66\u62a5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u9769\u65b0\u94c1\u8def\u5b89\u5168\u63aa\u65bd\uff0c\u4e3a\u4e8b\u6545\u9884\u9632\u63d0\u4f9b\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2507.03028", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03028", "abs": "https://arxiv.org/abs/2507.03028", "authors": ["C. J. Atapattu", "Xia Cui", "N. R Abeynayake"], "title": "Deep Learning-Based Forecasting of Hotel KPIs: A Cross-City Analysis of Global Urban Markets", "comment": null, "summary": "This study employs Long Short-Term Memory (LSTM) networks to forecast key\nperformance indicators (KPIs), Occupancy (OCC), Average Daily Rate (ADR), and\nRevenue per Available Room (RevPAR), across five major cities: Manchester,\nAmsterdam, Dubai, Bangkok, and Mumbai. The cities were selected for their\ndiverse economic profiles and hospitality dynamics. Monthly data from 2018 to\n2025 were used, with 80% for training and 20% for testing. Advanced time series\ndecomposition and machine learning techniques enabled accurate forecasting and\ntrend identification. Results show that Manchester and Mumbai exhibited the\nhighest predictive accuracy, reflecting stable demand patterns, while Dubai and\nBangkok demonstrated higher variability due to seasonal and event-driven\ninfluences. The findings validate the effectiveness of LSTM models for urban\nhospitality forecasting and provide a comparative framework for data-driven\ndecision-making. The models generalisability across global cities highlights\nits potential utility for tourism stakeholders and urban planners.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528LSTM\u7f51\u7edc\u9884\u6d4b\u4e94\u4e2a\u57ce\u5e02\u7684\u9152\u5e97KPIs\uff08OCC\u3001ADR\u3001RevPAR\uff09\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u63a2\u7d22LSTM\u5728\u591a\u5143\u7ecf\u6d4e\u80cc\u666f\u4e0b\u7684\u57ce\u5e02\u9152\u5e97\u4e1a\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u65c5\u6e38\u4e1a\u548c\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "method": "\u91c7\u7528LSTM\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u4f7f\u75282018-2025\u5e74\u6708\u5ea6\u6570\u636e\uff0880%\u8bad\u7ec3\uff0c20%\u6d4b\u8bd5\uff09\u3002", "result": "\u66fc\u5f7b\u65af\u7279\u548c\u5b5f\u4e70\u9884\u6d4b\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u8fea\u62dc\u548c\u66fc\u8c37\u56e0\u5b63\u8282\u6027\u548c\u4e8b\u4ef6\u5f71\u54cd\u6ce2\u52a8\u8f83\u5927\u3002", "conclusion": "LSTM\u6a21\u578b\u9002\u7528\u4e8e\u5168\u7403\u57ce\u5e02\u9152\u5e97\u4e1a\u9884\u6d4b\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u6846\u67b6\u3002"}}
{"id": "2507.03048", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03048", "abs": "https://arxiv.org/abs/2507.03048", "authors": ["Thomas A. Henzinger", "Mahyar Karimi", "Konstantin Kueffner", "Kaushik Mallik"], "title": "Monitoring of Static Fairness", "comment": "arXiv admin note: text overlap with arXiv:2305.15979", "summary": "Machine-learned systems are in widespread use for making decisions about\nhumans, and it is important that they are fair, i.e., not biased against\nindividuals based on sensitive attributes.\n  We present a general framework of runtime verification of algorithmic\nfairness for systems whose models are unknown, but are assumed to have a Markov\nchain structure, with or without full observation of the state space.\n  We introduce a specification language that can model many common algorithmic\nfairness properties, such as demographic parity, equal opportunity, and social\nburden.\n  We build monitors that observe a long sequence of events as generated by a\ngiven system, and output, after each observation, a quantitative estimate of\nhow fair or biased the system was on that run until that point in time.\n  The estimate is proven to be correct modulo a variable error bound and a\ngiven confidence level, where the error bound gets tighter as the observed\nsequence gets longer.\n  We present two categories of monitoring algorithms, namely ones with a\nuniform error bound across all time points, and ones with weaker non-uniform,\npointwise error bounds at different time points.\n  Our monitoring algorithms use statistical tools that are adapted to suit the\ndynamic requirements of monitoring and the special needs of the fairness\nspecifications.\n  Using a prototype implementation, we show how we can monitor if a bank is\nfair in giving loans to applicants from different social backgrounds, and if a\ncollege is fair in admitting students while maintaining a reasonable financial\nburden on the society.\n  In these experiments, our monitors took less than a millisecond to update\ntheir verdicts after each observation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u884c\u65f6\u9a8c\u8bc1\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u672a\u77e5\u4f46\u5177\u6709\u9a6c\u5c14\u53ef\u592b\u94fe\u7ed3\u6784\u7684\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u516c\u5e73\u6027\u5c5e\u6027\u7684\u76d1\u6d4b\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5e7f\u6cdb\u7528\u4e8e\u4eba\u7c7b\u51b3\u7b56\uff0c\u786e\u4fdd\u5176\u516c\u5e73\u6027\uff08\u4e0d\u57fa\u4e8e\u654f\u611f\u5c5e\u6027\u504f\u8892\u4e2a\u4f53\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u89c4\u8303\u8bed\u8a00\u5efa\u6a21\u5e38\u89c1\u516c\u5e73\u6027\u5c5e\u6027\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5e73\u7b49\u3001\u673a\u4f1a\u5747\u7b49\uff09\uff0c\u6784\u5efa\u76d1\u6d4b\u5668\u5b9e\u65f6\u8bc4\u4f30\u7cfb\u7edf\u516c\u5e73\u6027\uff0c\u5e76\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u3002", "result": "\u76d1\u6d4b\u5668\u80fd\u5728\u6beb\u79d2\u7ea7\u66f4\u65b0\u8bc4\u4f30\u7ed3\u679c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u94f6\u884c\u8d37\u6b3e\u548c\u5927\u5b66\u5f55\u53d6\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u77e5\u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u9a8c\u8bc1\u516c\u5e73\u6027\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.03062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03062", "abs": "https://arxiv.org/abs/2507.03062", "authors": ["Hao Yang", "Angela Yao", "Christopher Whalen", "Gengchen Mai"], "title": "BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data", "comment": "This paper was accepted at GIScience 2025", "summary": "Understanding human mobility is essential for applications in public health,\ntransportation, and urban planning. However, mobility data often suffers from\nsparsity due to limitations in data collection methods, such as infrequent GPS\nsampling or call detail record (CDR) data that only capture locations during\ncommunication events. To address this challenge, we propose BERT4Traj, a\ntransformer based model that reconstructs complete mobility trajectories by\npredicting hidden visits in sparse movement sequences. Inspired by BERT's\nmasked language modeling objective and self_attention mechanisms, BERT4Traj\nleverages spatial embeddings, temporal embeddings, and contextual background\nfeatures such as demographics and anchor points. We evaluate BERT4Traj on real\nworld CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our\napproach significantly outperforms traditional models such as Markov Chains,\nKNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs\ndetailed and continuous mobility trajectories, enhancing insights into human\nmovement patterns.", "AI": {"tldr": "BERT4Traj\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u79fb\u52a8\u6570\u636e\u4e2d\u91cd\u5efa\u5b8c\u6574\u7684\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u5e38\u56e0\u6536\u96c6\u65b9\u6cd5\u9650\u5236\uff08\u5982\u4f4e\u9891GPS\u91c7\u6837\u6216CDR\u6570\u636e\uff09\u800c\u7a00\u758f\uff0c\u5f71\u54cd\u5e94\u7528\u6548\u679c\u3002", "method": "BERT4Traj\u7ed3\u5408\u7a7a\u95f4\u5d4c\u5165\u3001\u65f6\u95f4\u5d4c\u5165\u53ca\u4e0a\u4e0b\u6587\u80cc\u666f\u7279\u5f81\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u548c\u951a\u70b9\uff09\uff0c\u5229\u7528BERT\u7684\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u4e4c\u5e72\u8fbe\u574e\u5e15\u62c9\u7684CDR\u548cGPS\u6570\u636e\u96c6\u4e0a\uff0cBERT4Traj\u663e\u8457\u4f18\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u3001KNN\u3001RNN\u548cLSTM\u7b49\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "BERT4Traj\u80fd\u6709\u6548\u91cd\u5efa\u8be6\u7ec6\u8fde\u7eed\u7684\u79fb\u52a8\u8f68\u8ff9\uff0c\u63d0\u5347\u5bf9\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\u7684\u7406\u89e3\u3002"}}
{"id": "2507.04105", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.04105", "abs": "https://arxiv.org/abs/2507.04105", "authors": ["Jinwei Hu", "Yi Dong", "Zhengtao Ding", "Xiaowei Huang"], "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing", "comment": "Preprint accepted by Chinese Journal of Aeronautics", "summary": "This paper presents a defense framework for enhancing the safety of large\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\ndomains such as aerospace. We apply randomized smoothing, a statistical\nrobustness certification technique, to the MAS consensus context, enabling\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\ntraditional verification methods, our approach operates in black-box settings\nand employs a two-stage adaptive sampling mechanism to balance robustness and\ncomputational efficiency. Simulation results demonstrate that our method\neffectively prevents the propagation of adversarial behaviors and\nhallucinations while maintaining consensus performance. This work provides a\npractical and scalable path toward safe deployment of LLM-based MAS in\nreal-world, high-stakes environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u822a\u7a7a\u822a\u5929\uff09\u4e2d\u7684\u5b89\u5168\u6027\u7684\u9632\u5fa1\u6846\u67b6\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0cLLM\u9a71\u52a8\u7684MAS\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u884c\u4e3a\u548c\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u5176\u5b89\u5168\u6027\u3002", "method": "\u5e94\u7528\u968f\u673a\u5e73\u6ed1\u6280\u672f\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\uff0c\u5728\u65e0\u9700\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u7684\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9632\u6b62\u5bf9\u6297\u6027\u884c\u4e3a\u548c\u5e7b\u89c9\u7684\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u6301\u5171\u8bc6\u6027\u80fd\u3002", "conclusion": "\u4e3aLLM\u9a71\u52a8\u7684MAS\u5728\u73b0\u5b9e\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04791", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04791", "abs": "https://arxiv.org/abs/2507.04791", "authors": ["Dionis Totsila", "Clemente Donoso", "Enrico Mingo Hoffman", "Jean-Baptiste Mouret", "Serena Ivaldi"], "title": "Safe Bimanual Teleoperation with Language-Guided Collision Avoidance", "comment": null, "summary": "Teleoperating precise bimanual manipulations in cluttered environments is\nchallenging for operators, who often struggle with limited spatial perception\nand difficulty estimating distances between target objects, the robot's body,\nobstacles, and the surrounding environment. To address these challenges, local\nrobot perception and control should assist the operator during teleoperation.\nIn this work, we introduce a safe teleoperation system that enhances operator\ncontrol by preventing collisions in cluttered environments through the\ncombination of immersive VR control and voice-activated collision avoidance.\nUsing HTC Vive controllers, operators directly control a bimanual mobile\nmanipulator, while spoken commands such as \"avoid the yellow tool\" trigger\nvisual grounding and segmentation to build 3D obstacle meshes. These meshes are\nintegrated into a whole-body controller to actively prevent collisions during\nteleoperation. Experiments in static, cluttered scenes demonstrate that our\nsystem significantly improves operational safety without compromising task\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408VR\u63a7\u5236\u548c\u8bed\u97f3\u907f\u969c\u7684\u5b89\u5168\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u6742\u4e71\u73af\u5883\u4e2d\u63d0\u5347\u64cd\u4f5c\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u64cd\u4f5c\u8005\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9065\u64cd\u4f5c\u65f6\u7a7a\u95f4\u611f\u77e5\u6709\u9650\u548c\u8ddd\u79bb\u4f30\u8ba1\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6c89\u6d78\u5f0fVR\u63a7\u5236\u548c\u8bed\u97f3\u6fc0\u6d3b\u7684\u907f\u969c\u529f\u80fd\uff0c\u901a\u8fc7\u89c6\u89c9\u5206\u5272\u548c3D\u969c\u788d\u7269\u7f51\u683c\u6574\u5408\u5230\u5168\u8eab\u63a7\u5236\u5668\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u4efb\u52a1\u6548\u7387\u3002", "conclusion": "\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u9065\u64cd\u4f5c\u4e2d\u7684\u78b0\u649e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u64cd\u4f5c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.04338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04338", "abs": "https://arxiv.org/abs/2507.04338", "authors": ["Abdullah M. Zyarah", "Dhireesha Kudithipudi"], "title": "Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems", "comment": null, "summary": "Recent advances in neuromorphic computing demonstrate on-device learning\ncapabilities with low power consumption. One of the key learning units in these\nsystems is the winner-take-all circuit. In this research, we propose a\nwinner-take-all circuit that can be configured to achieve k-winner and\nhysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9\n$\\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The\nutility of the circuit is demonstrated for spatial filtering and\nclassification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684winner-take-all\u7535\u8def\uff0c\u652f\u6301k-winner\u548c\u6ede\u540e\u7279\u6027\uff0c\u529f\u8017\u4f4e\u4e14\u5ef6\u8fdf\u5c0f\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u6ee4\u6ce2\u548c\u5206\u7c7b\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u7684winner-take-all\u7535\u8def\u662f\u5173\u952e\u5b66\u4e60\u5355\u5143\uff0c\u4f46\u73b0\u6709\u8bbe\u8ba1\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u4f4e\u529f\u8017\u7279\u6027\u3002", "method": "\u5728IBM 65 nm\u5de5\u827a\u8282\u70b9\u4e0a\u6a21\u62df\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684winner-take-all\u7535\u8def\uff0c\u652f\u6301k-winner\u548c\u6ede\u540e\u7279\u6027\u3002", "result": "\u7535\u8def\u529f\u8017\u4e3a34.9 \u03bcW\uff0c\u5ef6\u8fdf10.4 ns\uff0c\u53ef\u5904\u74061000\u4e2a\u8f93\u5165\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u6ee4\u6ce2\u548c\u5206\u7c7b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7535\u8def\u5728\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u9ad8\u6548\u652f\u6301\u591a\u79cd\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2507.03339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03339", "abs": "https://arxiv.org/abs/2507.03339", "authors": ["Sheng Liu", "Yiheng Yu", "Yuan Feng", "Min Xu", "Zhelun Jin", "Yining Jiang", "Tiantian Yuan"], "title": "DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition", "comment": null, "summary": "Current continuous sign language recognition (CSLR) methods struggle with\nhandling diverse samples. Although dynamic convolutions are ideal for this\ntask, they mainly focus on spatial modeling and fail to capture the temporal\ndynamics and contextual dependencies. To address this, we propose DESign, a\nnovel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and\nSubnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC\ndynamically captures the inter-frame motion cues that constitute signs and\nuniquely adapts convolutional weights in a fine-grained manner based on\ncontextual information, enabling the model to better generalize across diverse\nsigning behaviors and boost recognition accuracy. Furthermore, we observe that\nexisting methods still rely on only a limited number of frames for parameter\nupdates during training, indicating that CTC learning overfits to a dominant\npath. To address this, SR-CTC regularizes training by applying supervision to\nsubnetworks, encouraging the model to explore diverse CTC alignment paths and\neffectively preventing overfitting. A classifier-sharing strategy in SR-CTC\nfurther strengthens multi-scale consistency. Notably, SR-CTC introduces no\ninference overhead and can be seamlessly integrated into existing CSLR models\nto boost performance. Extensive ablations and visualizations further validate\nthe effectiveness of the proposed methods. Results on mainstream CSLR datasets\n(i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves\nstate-of-the-art performance.", "AI": {"tldr": "DESign\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u5377\u79ef\uff08DCAC\uff09\u548c\u5b50\u7f51\u6b63\u5219\u5316CTC\uff08SR-CTC\uff09\u63d0\u5347\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\uff08CSLR\uff09\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u7a7a\u5efa\u6a21\u548c\u8fc7\u62df\u5408\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709CSLR\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6837\u6837\u672c\uff0c\u4e14\u52a8\u6001\u5377\u79ef\u4ec5\u5173\u6ce8\u7a7a\u95f4\u5efa\u6a21\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3002", "method": "\u63d0\u51faDESign\u6846\u67b6\uff0c\u7ed3\u5408DCAC\u52a8\u6001\u6355\u6349\u5e27\u95f4\u8fd0\u52a8\u4fe1\u606f\uff0cSR-CTC\u901a\u8fc7\u5b50\u7f51\u76d1\u7763\u9632\u6b62CTC\u8fc7\u62df\u5408\uff0c\u5e76\u91c7\u7528\u5206\u7c7b\u5668\u5171\u4eab\u7b56\u7565\u589e\u5f3a\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5728PHOENIX14\u3001PHOENIX14-T\u548cCSL-Daily\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "DESign\u901a\u8fc7DCAC\u548cSR-CTC\u6709\u6548\u63d0\u5347CSLR\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\uff0c\u4e14SR-CTC\u65e0\u9700\u989d\u5916\u63a8\u7406\u5f00\u9500\u3002"}}
{"id": "2507.04447", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04447", "abs": "https://arxiv.org/abs/2507.04447", "authors": ["Wenyao Zhang", "Hongsi Liu", "Zekun Qi", "Yunnan Wang", "XinQiang Yu", "Jiazhao Zhang", "Runpei Dong", "Jiawei He", "He Wang", "Zhizheng Zhang", "Li Yi", "Wenjun Zeng", "Xin Jin"], "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge", "comment": null, "summary": "Recent advances in vision-language-action (VLA) models have shown promise in\nintegrating image generation with action prediction to improve generalization\nand reasoning in robot manipulation. However, existing methods are limited to\nchallenging image-based forecasting, which suffers from redundant information\nand lacks comprehensive and critical world knowledge, including dynamic,\nspatial and semantic information. To address these limitations, we propose\nDreamVLA, a novel VLA framework that integrates comprehensive world knowledge\nforecasting to enable inverse dynamics modeling, thereby establishing a\nperception-prediction-action loop for manipulation tasks. Specifically,\nDreamVLA introduces a dynamic-region-guided world knowledge prediction,\nintegrated with the spatial and semantic cues, which provide compact yet\ncomprehensive representations for action planning. This design aligns with how\nhumans interact with the world by first forming abstract multimodal reasoning\nchains before acting. To mitigate interference among the dynamic, spatial and\nsemantic information during training, we adopt a block-wise structured\nattention mechanism that masks their mutual attention, preventing information\nleakage and keeping each representation clean and disentangled. Moreover, to\nmodel the conditional distribution over future actions, we employ a\ndiffusion-based transformer that disentangles action representations from\nshared latent features. Extensive experiments on both real-world and simulation\nenvironments demonstrate that DreamVLA achieves 76.7% success rate on real\nrobot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.", "AI": {"tldr": "DreamVLA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5168\u9762\u7684\u4e16\u754c\u77e5\u8bc6\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5197\u4f59\u4fe1\u606f\u548c\u7f3a\u4e4f\u52a8\u6001\u3001\u7a7a\u95f4\u53ca\u8bed\u4e49\u77e5\u8bc6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u56fe\u50cf\u9884\u6d4b\u4e2d\u5b58\u5728\u5197\u4f59\u4fe1\u606f\u548c\u7f3a\u4e4f\u5168\u9762\u4e16\u754c\u77e5\u8bc6\u7684\u95ee\u9898\uff0cDreamVLA\u65e8\u5728\u901a\u8fc7\u6574\u5408\u52a8\u6001\u3001\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "DreamVLA\u91c7\u7528\u52a8\u6001\u533a\u57df\u5f15\u5bfc\u7684\u4e16\u754c\u77e5\u8bc6\u9884\u6d4b\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u4f7f\u7528\u5757\u72b6\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u4fe1\u606f\u6cc4\u6f0f\u3002\u6b64\u5916\uff0c\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684Transformer\u89e3\u8026\u52a8\u4f5c\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u73af\u5883\u4e2d\uff0cDreamVLA\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8fbe\u523076.7%\u7684\u6210\u529f\u7387\uff0c\u5728CALVIN ABC-D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u957f\u5ea6\u4e3a4.44\u3002", "conclusion": "DreamVLA\u901a\u8fc7\u6574\u5408\u5168\u9762\u7684\u4e16\u754c\u77e5\u8bc6\u9884\u6d4b\u548c\u89e3\u8026\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.03545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03545", "abs": "https://arxiv.org/abs/2507.03545", "authors": ["Julien Nicolas", "Mohamed Maouche", "Sonia Ben Mokhtar", "Mark Coates"], "title": "Communication Efficient, Differentially Private Distributed Optimization using Correlation-Aware Sketching", "comment": null, "summary": "Federated learning with differential privacy suffers from two major costs:\neach client must transmit $d$-dimensional gradients every round, and the\nmagnitude of DP noise grows with $d$. Yet empirical studies show that gradient\nupdates exhibit strong temporal correlations and lie in a $k$-dimensional\nsubspace with $k \\ll d$. Motivated by this, we introduce DOME, a decentralized\nDP optimization framework in which each client maintains a compact sketch to\nproject gradients into $\\mathbb{R}^k$ before privatization and Secure\nAggregation. This reduces per-round communication from order $d$ to order $k$\nand moves towards a gradient approximation mean-squared error of $\\sigma^2 k$.\nTo allow the sketch to span new directions and prevent it from collapsing onto\nhistorical gradients, we augment it with random probes orthogonal to historical\ndirections. We prove that our overall protocol satisfies\n$(\\epsilon,\\delta)$-Differential Privacy.", "AI": {"tldr": "DOME\u6846\u67b6\u901a\u8fc7\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u5e76\u5f15\u5165\u968f\u673a\u63a2\u9488\uff0c\u51cf\u5c11\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5dee\u5206\u9690\u79c1\u7684\u901a\u4fe1\u6210\u672c\u548c\u566a\u58f0\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5dee\u5206\u9690\u79c1\u5e26\u6765\u7684\u9ad8\u7ef4\u68af\u5ea6\u4f20\u8f93\u548c\u566a\u58f0\u653e\u5927\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u8349\u56fe\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u968f\u673a\u63a2\u9488\u9632\u6b62\u8349\u56fe\u9000\u5316\u3002", "result": "\u901a\u4fe1\u6210\u672c\u4ece$d$\u964d\u81f3$k$\uff0c\u68af\u5ea6\u8fd1\u4f3c\u8bef\u5dee\u4e3a$\\sigma^2 k$\uff0c\u6ee1\u8db3$(\\epsilon,\\delta)$-\u5dee\u5206\u9690\u79c1\u3002", "conclusion": "DOME\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5dee\u5206\u9690\u79c1\u7684\u901a\u4fe1\u548c\u566a\u58f0\u6210\u672c\u3002"}}
{"id": "2507.03933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.03933", "abs": "https://arxiv.org/abs/2507.03933", "authors": ["Eva Vanmassenhove"], "title": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large Language Models", "comment": "12 pages", "summary": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to \\textit{offload} the task of\nwriting itself to these technologies, enabling them to change our linguistic\necosystem more directly. While they provide us quick access to information and\nimpressively fluent output, beneath their apparent sophistication lies a\nsubtle, more insidious threat: the gradual decline and loss of linguistic\ndiversity. With this opinion piece, I explore how model collapse, with a\nparticular focus on translation technology, can lead to the loss of linguistic\nforms, grammatical features, and cultural nuance. Model collapse refers to the\neventual consequence of self-consuming training loops, where models reinforce\ntheir own biases and lose linguistic diversity. Drawing on recent work in\nComputer Vision, Natural Language Processing (NLP) and Machine Translation\n(MT), I argue that the tails of our linguistic distributions are vanishing, and\nwith them, the narratives and identities they carry. This is a call to resist\nlinguistic flattening and to reimagine NLP as a field that encourages, values\nand protects expressive multilingual lexical and linguistic diversity and\ncreativity.", "AI": {"tldr": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u6280\u672f\u5bf9\u8bed\u8a00\u7684\u5f71\u54cd\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8bed\u8a00\u591a\u6837\u6027\u4e27\u5931\u3002", "motivation": "\u63a2\u8ba8\u6a21\u578b\u5d29\u6e83\uff08model collapse\uff09\u5982\u4f55\u901a\u8fc7\u7ffb\u8bd1\u6280\u672f\u7b49\u5bfc\u81f4\u8bed\u8a00\u5f62\u5f0f\u3001\u8bed\u6cd5\u7279\u5f81\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u6d88\u5931\u3002", "method": "\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5206\u6790\u6a21\u578b\u81ea\u6211\u5f3a\u5316\u7684\u8bad\u7ec3\u5faa\u73af\u5bf9\u8bed\u8a00\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002", "result": "\u8bed\u8a00\u5206\u5e03\u7684\u5c3e\u90e8\u7279\u5f81\u6b63\u5728\u6d88\u5931\uff0c\u968f\u4e4b\u800c\u6765\u7684\u662f\u76f8\u5173\u53d9\u4e8b\u548c\u8eab\u4efd\u7684\u4e27\u5931\u3002", "conclusion": "\u547c\u5401\u62b5\u5236\u8bed\u8a00\u6241\u5e73\u5316\uff0c\u91cd\u65b0\u6784\u60f3NLP\u9886\u57df\uff0c\u4ee5\u9f13\u52b1\u548c\u4fdd\u62a4\u591a\u8bed\u8a00\u8868\u8fbe\u7684\u591a\u6837\u6027\u548c\u521b\u9020\u529b\u3002"}}
{"id": "2507.05246", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05246", "abs": "https://arxiv.org/abs/2507.05246", "authors": ["Scott Emmons", "Erik Jenner", "David K. Elson", "Rif A. Saurous", "Senthooran Rajamanoharan", "Heng Chen", "Irhum Shafkat", "Rohin Shah"], "title": "When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors", "comment": null, "summary": "While chain-of-thought (CoT) monitoring is an appealing AI safety defense,\nrecent work on \"unfaithfulness\" has cast doubt on its reliability. These\nfindings highlight an important failure mode, particularly when CoT acts as a\npost-hoc rationalization in applications like auditing for bias. However, for\nthe distinct problem of runtime monitoring to prevent severe harm, we argue the\nkey property is not faithfulness but monitorability. To this end, we introduce\na conceptual framework distinguishing CoT-as-rationalization from\nCoT-as-computation. We expect that certain classes of severe harm will require\ncomplex, multi-step reasoning that necessitates CoT-as-computation. Replicating\nthe experimental setups of prior work, we increase the difficulty of the bad\nbehavior to enforce this necessity condition; this forces the model to expose\nits reasoning, making it monitorable. We then present methodology guidelines to\nstress-test CoT monitoring against deliberate evasion. Applying these\nguidelines, we find that models can learn to obscure their intentions, but only\nwhen given significant help, such as detailed human-written strategies or\niterative optimization against the monitor. We conclude that, while not\ninfallible, CoT monitoring offers a substantial layer of defense that requires\nactive protection and continued stress-testing.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u76d1\u63a7\u5728AI\u5b89\u5168\u9632\u5fa1\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u76d1\u63a7\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u533a\u5206\u4e86CoT\u4f5c\u4e3a\u5408\u7406\u5316\u4e0e\u8ba1\u7b97\u7684\u4e0d\u540c\u7528\u9014\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u590d\u6742\u63a8\u7406\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u538b\u529b\u6d4b\u8bd5\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9CoT\u76d1\u63a7\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5176\u5728\u4e8b\u540e\u5408\u7406\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5176\u5728\u9884\u9632\u4e25\u91cd\u5371\u5bb3\u65f6\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u76d1\u63a7\u6027\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u5f15\u5165\u6982\u5ff5\u6846\u67b6\u533a\u5206CoT\u7684\u4e24\u79cd\u7528\u9014\uff0c\u901a\u8fc7\u589e\u52a0\u884c\u4e3a\u96be\u5ea6\u5f3a\u5236\u6a21\u578b\u66b4\u9732\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u538b\u529b\u6d4b\u8bd5\u65b9\u6cd5\u8bc4\u4f30\u76d1\u63a7\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f1a\u66b4\u9732\u610f\u56fe\uff0c\u4f46\u5728\u4eba\u4e3a\u5e72\u9884\u4e0b\u53ef\u80fd\u9690\u85cf\u610f\u56fe\u3002CoT\u76d1\u63a7\u867d\u975e\u5b8c\u7f8e\uff0c\u4f46\u4ecd\u5177\u663e\u8457\u9632\u5fa1\u4ef7\u503c\u3002", "conclusion": "CoT\u76d1\u63a7\u9700\u6301\u7eed\u538b\u529b\u6d4b\u8bd5\u548c\u4e3b\u52a8\u4fdd\u62a4\uff0c\u867d\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u5728\u9884\u9632\u4e25\u91cd\u5371\u5bb3\u4e2d\u5177\u6709\u91cd\u8981\u9632\u5fa1\u4f5c\u7528\u3002"}}
{"id": "2507.03765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.03765", "abs": "https://arxiv.org/abs/2507.03765", "authors": ["Hebei Li", "Yansong Peng", "Jiahui Yuan", "Peixi Wu", "Jin Wang", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Efficient Event-Based Semantic Segmentation via Exploiting Frame-Event Fusion: A Hybrid Neural Network Approach", "comment": null, "summary": "Event cameras have recently been introduced into image semantic segmentation,\nowing to their high temporal resolution and other advantageous properties.\nHowever, existing event-based semantic segmentation methods often fail to fully\nexploit the complementary information provided by frames and events, resulting\nin complex training strategies and increased computational costs. To address\nthese challenges, we propose an efficient hybrid framework for image semantic\nsegmentation, comprising a Spiking Neural Network branch for events and an\nArtificial Neural Network branch for frames. Specifically, we introduce three\nspecialized modules to facilitate the interaction between these two branches:\nthe Adaptive Temporal Weighting (ATW) Injector, the Event-Driven Sparse (EDS)\nInjector, and the Channel Selection Fusion (CSF) module. The ATW Injector\ndynamically integrates temporal features from event data into frame features,\nenhancing segmentation accuracy by leveraging critical dynamic temporal\ninformation. The EDS Injector effectively combines sparse event data with rich\nframe features, ensuring precise temporal and spatial information alignment.\nThe CSF module selectively merges these features to optimize segmentation\nperformance. Experimental results demonstrate that our framework not only\nachieves state-of-the-art accuracy across the DDD17-Seg, DSEC-Semantic, and\nM3ED-Semantic datasets but also significantly reduces energy consumption,\nachieving a 65\\% reduction on the DSEC-Semantic dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u548c\u5e27\u6570\u636e\u7528\u4e8e\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u7528\u6a21\u5757\u4f18\u5316\u4fe1\u606f\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u548c\u5e27\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5bfc\u81f4\u8bad\u7ec3\u590d\u6742\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6846\u67b6\uff0c\u5305\u62ecSNN\u5206\u652f\u5904\u7406\u4e8b\u4ef6\u548cANN\u5206\u652f\u5904\u7406\u5e27\uff0c\u5f15\u5165ATW Injector\u3001EDS Injector\u548cCSF\u6a21\u5757\u4f18\u5316\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u80fd\u8017\u964d\u4f4e65%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u548c\u5e27\u6570\u636e\u878d\u5408\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.03816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.03816", "abs": "https://arxiv.org/abs/2507.03816", "authors": ["Fereshteh Baradaran", "Mohsen Raji", "Azadeh Baradaran", "Arezoo Baradaran", "Reihaneh Akbarifard"], "title": "Zero Memory Overhead Approach for Protecting Vision Transformer Parameters", "comment": null, "summary": "Vision Transformers (ViTs) have demonstrated superior performance over\nConvolutional Neural Networks (CNNs) in various vision-related tasks such as\nclassification, object detection, and segmentation due to their use of\nself-attention mechanisms. As ViTs become more popular in safety-critical\napplications like autonomous driving, ensuring their correct functionality\nbecomes essential, especially in the presence of bit-flip faults in their\nparameters stored in memory. In this paper, a fault tolerance technique is\nintroduced to protect ViT parameters against bit-flip faults with zero memory\noverhead. Since the least significant bits of parameters are not critical for\nmodel accuracy, replacing the LSB with a parity bit provides an error detection\nmechanism without imposing any overhead on the model. When faults are detected,\naffected parameters are masked by zeroing out, as most parameters in ViT models\nare near zero, effectively preventing accuracy degradation. This approach\nenhances reliability across ViT models, improving the robustness of parameters\nto bit-flips by up to three orders of magnitude, making it an effective\nzero-overhead solution for fault tolerance in critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u5185\u5b58\u5f00\u9500\u7684\u5bb9\u9519\u6280\u672f\uff0c\u901a\u8fc7\u66ff\u6362ViT\u53c2\u6570\u7684\u6700\u4f4e\u4f4d\u4e3a\u5947\u5076\u6821\u9a8c\u4f4d\u6765\u68c0\u6d4b\u4f4d\u7ffb\u8f6c\u9519\u8bef\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u9519\u8bef\u65f6\u5c4f\u853d\u53d7\u5f71\u54cd\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740ViT\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u5728\u53c2\u6570\u4f4d\u7ffb\u8f6c\u9519\u8bef\u4e0b\u7684\u6b63\u786e\u529f\u80fd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u53c2\u6570\u7684\u6700\u4f4e\u4f4d\u66ff\u6362\u4e3a\u5947\u5076\u6821\u9a8c\u4f4d\u4ee5\u68c0\u6d4b\u9519\u8bef\uff0c\u68c0\u6d4b\u5230\u9519\u8bef\u65f6\u901a\u8fc7\u96f6\u5c4f\u853d\u53d7\u5f71\u54cd\u53c2\u6570\u3002", "result": "\u8be5\u6280\u672f\u5c06ViT\u6a21\u578b\u5bf9\u4f4d\u7ffb\u8f6c\u9519\u8bef\u7684\u9c81\u68d2\u6027\u63d0\u9ad8\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u96f6\u5f00\u9500\u7684\u5bb9\u9519\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86ViT\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.03898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.03898", "abs": "https://arxiv.org/abs/2507.03898", "authors": ["Di Xiong", "Lei Zhang", "Shuoyuan Wang", "Dongzhou Cheng", "Wenbo Huang"], "title": "Deconfounding Causal Inference through Two-Branch Framework with Early-Forking for Sensor-Based Cross-Domain Activity Recognition", "comment": "Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (IMWUT)", "summary": "Recently, domain generalization (DG) has emerged as a promising solution to\nmitigate distribution-shift issue in sensor-based human activity recognition\n(HAR) scenario. However, most existing DG-based works have merely focused on\nmodeling statistical dependence between sensor data and activity labels,\nneglecting the importance of intrinsic casual mechanism. Intuitively, every\nsensor input can be viewed as a mixture of causal (category-aware) and\nnon-causal factors (domain-specific), where only the former affects activity\nclassification judgment. In this paper, by casting such DG-based HAR as a\ncasual inference problem, we propose a causality-inspired representation\nlearning algorithm for cross-domain activity recognition. To this end, an\nearly-forking two-branch framework is designed, where two separate branches are\nrespectively responsible for learning casual and non-causal features, while an\nindependence-based Hilbert-Schmidt Information Criterion is employed to\nimplicitly disentangling them. Additionally, an inhomogeneous domain sampling\nstrategy is designed to enhance disentanglement, while a category-aware domain\nperturbation layer is performed to prevent representation collapse. Extensive\nexperiments on several public HAR benchmarks demonstrate that our\ncausality-inspired approach significantly outperforms eleven related\nstate-of-the-art baselines under cross-person, cross-dataset, and\ncross-position settings. Detailed ablation and visualizations analyses reveal\nunderlying casual mechanism, indicating its effectiveness, efficiency, and\nuniversality in cross-domain activity recognition scenario.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u8de8\u57df\u6d3b\u52a8\u8bc6\u522b\uff0c\u901a\u8fc7\u5206\u79bb\u56e0\u679c\u548c\u975e\u56e0\u679c\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u5728\u4f20\u611f\u5668\u6d3b\u52a8\u8bc6\u522b\u4e2d\u4ec5\u5173\u6ce8\u7edf\u8ba1\u4f9d\u8d56\uff0c\u5ffd\u7565\u4e86\u5185\u5728\u56e0\u679c\u673a\u5236\u7684\u91cd\u8981\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u6846\u67b6\uff0c\u5206\u522b\u5b66\u4e60\u56e0\u679c\u548c\u975e\u56e0\u679c\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u72ec\u7acb\u6027\u51c6\u5219\u8fdb\u884c\u89e3\u8026\uff0c\u540c\u65f6\u5f15\u5165\u4e0d\u5747\u5300\u57df\u91c7\u6837\u548c\u7c7b\u522b\u611f\u77e5\u6270\u52a8\u5c42\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e11\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u63ed\u793a\u56e0\u679c\u673a\u5236\uff0c\u5728\u8de8\u57df\u6d3b\u52a8\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2507.03936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.03936", "abs": "https://arxiv.org/abs/2507.03936", "authors": ["Chen Pang", "Xuequan Lu", "Qianyu Zhou", "Lei Lyu"], "title": "Learning Adaptive Node Selection with External Attention for Human Interaction Recognition", "comment": "Accepted by ACM MM25", "summary": "Most GCN-based methods model interacting individuals as independent graphs,\nneglecting their inherent inter-dependencies. Although recent approaches\nutilize predefined interaction adjacency matrices to integrate participants,\nthese matrices fail to adaptively capture the dynamic and context-specific\njoint interactions across different actions. In this paper, we propose the\nActive Node Selection with External Attention Network (ASEA), an innovative\napproach that dynamically captures interaction relationships without predefined\nassumptions. Our method models each participant individually using a GCN to\ncapture intra-personal relationships, facilitating a detailed representation of\ntheir actions. To identify the most relevant nodes for interaction modeling, we\nintroduce the Adaptive Temporal Node Amplitude Calculation (AT-NAC) module,\nwhich estimates global node activity by combining spatial motion magnitude with\nadaptive temporal weighting, thereby highlighting salient motion patterns while\nreducing irrelevant or redundant information. A learnable threshold,\nregularized to prevent extreme variations, is defined to selectively identify\nthe most informative nodes for interaction modeling. To capture interactions,\nwe design the External Attention (EA) module to operate on active nodes,\neffectively modeling the interaction dynamics and semantic relationships\nbetween individuals. Extensive evaluations show that our method captures\ninteraction relationships more effectively and flexibly, achieving\nstate-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faASEA\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6355\u6349\u4ea4\u4e92\u5173\u7cfb\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5047\u8bbe\uff0c\u7ed3\u5408GCN\u548cAT-NAC\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u73b0\u6709GCN\u65b9\u6cd5\u5c06\u4ea4\u4e92\u4e2a\u4f53\u89c6\u4e3a\u72ec\u7acb\u56fe\uff0c\u5ffd\u7565\u5176\u5185\u5728\u4f9d\u8d56\u5173\u7cfb\uff1b\u9884\u5b9a\u4e49\u4ea4\u4e92\u77e9\u9635\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u8054\u5408\u4ea4\u4e92\u3002", "method": "\u63d0\u51faASEA\u65b9\u6cd5\uff0c\u7ed3\u5408GCN\u5efa\u6a21\u4e2a\u4f53\u5185\u90e8\u5173\u7cfb\uff0cAT-NAC\u6a21\u5757\u52a8\u6001\u9009\u62e9\u76f8\u5173\u8282\u70b9\uff0cEA\u6a21\u5757\u6355\u6349\u4ea4\u4e92\u52a8\u6001\u548c\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASEA\u80fd\u66f4\u6709\u6548\u7075\u6d3b\u5730\u6355\u6349\u4ea4\u4e92\u5173\u7cfb\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ASEA\u901a\u8fc7\u52a8\u6001\u8282\u70b9\u9009\u62e9\u548c\u5916\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u4ea4\u4e92\u5efa\u6a21\u7684\u7075\u6d3b\u6027\u548c\u6548\u679c\u3002"}}
{"id": "2507.04075", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04075", "abs": "https://arxiv.org/abs/2507.04075", "authors": ["Maxime Burchi", "Radu Timofte"], "title": "Accurate and Efficient World Modeling with Masked Latent Transformers", "comment": null, "summary": "The Dreamer algorithm has recently obtained remarkable performance across\ndiverse environment domains by training powerful agents with simulated\ntrajectories. However, the compressed nature of its world model's latent space\ncan result in the loss of crucial information, negatively affecting the agent's\nperformance. Recent approaches, such as $\\Delta$-IRIS and DIAMOND, address this\nlimitation by training more accurate world models. However, these methods\nrequire training agents directly from pixels, which reduces training efficiency\nand prevents the agent from benefiting from the inner representations learned\nby the world model. In this work, we propose an alternative approach to world\nmodeling that is both accurate and efficient. We introduce EMERALD (Efficient\nMaskEd latent tRAnsformer worLD model), a world model using a spatial latent\nstate with MaskGIT predictions to generate accurate trajectories in latent\nspace and improve the agent performance. On the Crafter benchmark, EMERALD\nachieves new state-of-the-art performance, becoming the first method to surpass\nhuman experts performance within 10M environment steps. Our method also\nsucceeds to unlock all 22 Crafter achievements at least once during evaluation.", "AI": {"tldr": "EMERALD\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u6f5c\u5728\u72b6\u6001\u548cMaskGIT\u9884\u6d4b\u751f\u6210\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u51c6\u786e\u8f68\u8ff9\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "Dreamer\u7b97\u6cd5\u7684\u6f5c\u5728\u7a7a\u95f4\u538b\u7f29\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u667a\u80fd\u4f53\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u0394-IRIS\u548cDIAMOND\uff09\u76f4\u63a5\u4ece\u50cf\u7d20\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e14\u65e0\u6cd5\u5229\u7528\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u7684\u5185\u5728\u8868\u793a\u3002", "method": "EMERALD\u4f7f\u7528\u7a7a\u95f4\u6f5c\u5728\u72b6\u6001\u548cMaskGIT\u9884\u6d4b\u751f\u6210\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u51c6\u786e\u8f68\u8ff9\uff0c\u4f18\u5316\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5728Crafter\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEMERALD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9996\u6b21\u572810M\u73af\u5883\u6b65\u6570\u5185\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\uff0c\u5e76\u89e3\u9501\u4e86\u6240\u670922\u9879\u6210\u5c31\u3002", "conclusion": "EMERALD\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u3002"}}
{"id": "2507.04190", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.04190", "abs": "https://arxiv.org/abs/2507.04190", "authors": ["Anqi Yang", "Eunhee Kang", "Wei Chen", "Hyong-Euk Lee", "Aswin C. Sankaranarayanan"], "title": "Towards Spatially-Varying Gain and Binning", "comment": null, "summary": "Pixels in image sensors have progressively become smaller, driven by the goal\nof producing higher-resolution imagery. However, ceteris paribus, a smaller\npixel accumulates less light, making image quality worse. This interplay of\nresolution, noise, and the dynamic range of the sensor and their impact on the\neventual quality of acquired imagery is a fundamental concept in photography.\nIn this paper, we propose spatially-varying gain and binning to enhance the\nnoise performance and dynamic range of image sensors. First, we show that by\nvarying gain spatially to local scene brightness, the read noise can be made\nnegligible, and the dynamic range of a sensor is expanded by an order of\nmagnitude. Second, we propose a simple analysis to find a binning size that\nbest balances resolution and noise for a given light level; this analysis\npredicts a spatially-varying binning strategy, again based on local scene\nbrightness, to effectively increase the overall signal-to-noise ratio. %\nwithout sacrificing resolution. We discuss analog and digital binning modes\nand, perhaps surprisingly, show that digital binning outperforms its analog\ncounterparts when a larger gain is allowed. Finally, we demonstrate that\ncombining spatially-varying gain and binning in various applications, including\nhigh dynamic range imaging, vignetting, and lens distortion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7a7a\u95f4\u53d8\u5316\u7684\u589e\u76ca\u548c\u50cf\u7d20\u5408\u5e76\u6280\u672f\uff0c\u63d0\u5347\u56fe\u50cf\u4f20\u611f\u5668\u7684\u566a\u58f0\u6027\u80fd\u548c\u52a8\u6001\u8303\u56f4\u3002", "motivation": "\u968f\u7740\u50cf\u7d20\u5c3a\u5bf8\u7f29\u5c0f\uff0c\u56fe\u50cf\u8d28\u91cf\u56e0\u5149\u79ef\u7d2f\u51cf\u5c11\u800c\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u5206\u8fa8\u7387\u3001\u566a\u58f0\u548c\u52a8\u6001\u8303\u56f4\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7a7a\u95f4\u53d8\u5316\u7684\u589e\u76ca\u548c\u50cf\u7d20\u5408\u5e76\u7b56\u7565\uff0c\u6839\u636e\u5c40\u90e8\u573a\u666f\u4eae\u5ea6\u8c03\u6574\u589e\u76ca\u548c\u5408\u5e76\u5c3a\u5bf8\uff0c\u4f18\u5316\u4fe1\u566a\u6bd4\u548c\u52a8\u6001\u8303\u56f4\u3002", "result": "\u52a8\u6001\u8303\u56f4\u6269\u5c55\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4fe1\u566a\u6bd4\u663e\u8457\u63d0\u5347\uff0c\u6570\u5b57\u5408\u5e76\u5728\u9ad8\u589e\u76ca\u4e0b\u4f18\u4e8e\u6a21\u62df\u5408\u5e76\u3002", "conclusion": "\u7ed3\u5408\u7a7a\u95f4\u53d8\u5316\u7684\u589e\u76ca\u548c\u5408\u5e76\u6280\u672f\uff0c\u53ef\u6709\u6548\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u9ad8\u52a8\u6001\u8303\u56f4\u6210\u50cf\u7b49\u5e94\u7528\u3002"}}
{"id": "2507.04562", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04562", "abs": "https://arxiv.org/abs/2507.04562", "authors": ["Janna Lu"], "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u6709\u6240\u63d0\u5347\uff0c\u4f46\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u8d85\u7ea7\u9884\u6d4b\u8005\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8d85\u7ea7\u9884\u6d4b\u8005\u5bf9\u6bd4\u3002", "method": "\u8bc4\u4f30\u524d\u6cbfLLMs\u5728464\u4e2aMetaculus\u9884\u6d4b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528Brier\u5206\u6570\u4f5c\u4e3a\u6307\u6807\u3002", "result": "\u524d\u6cbfLLMs\u7684Brier\u5206\u6570\u8d85\u8fc7\u666e\u901a\u4eba\u7fa4\uff0c\u4f46\u4ecd\u663e\u8457\u4f4e\u4e8e\u8d85\u7ea7\u9884\u6d4b\u8005\u3002", "conclusion": "LLMs\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2507.04529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04529", "abs": "https://arxiv.org/abs/2507.04529", "authors": ["Philipp Reis", "Joshua Ransiek", "David Petri", "Jacob Langner", "Eric Sax"], "title": "A Data-Driven Novelty Score for Diverse In-Vehicle Data Recording", "comment": "8 pages, accepted at the IEEE ITSC 2025", "summary": "High-quality datasets are essential for training robust perception systems in\nautonomous driving. However, real-world data collection is often biased toward\ncommon scenes and objects, leaving novel cases underrepresented. This imbalance\nhinders model generalization and compromises safety. The core issue is the\ncurse of rarity. Over time, novel events occur infrequently, and standard\nlogging methods fail to capture them effectively. As a result, large volumes of\nredundant data are stored, while critical novel cases are diluted, leading to\nbiased datasets. This work presents a real-time data selection method focused\non object-level novelty detection to build more balanced and diverse datasets.\nThe method assigns a data-driven novelty score to image frames using a novel\ndynamic Mean Shift algorithm. It models normal content based on mean and\ncovariance statistics to identify frames with novel objects, discarding those\nwith redundant elements. The main findings show that reducing the training\ndataset size with this method can improve model performance, whereas higher\nredundancy tends to degrade it. Moreover, as data redundancy increases, more\naggressive filtering becomes both possible and beneficial. While random\nsampling can offer some gains, it often leads to overfitting and\nunpredictability in outcomes. The proposed method supports real-time deployment\nwith 32 frames per second and is constant over time. By continuously updating\nthe definition of normal content, it enables efficient detection of novelties\nin a continuous data stream.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u65f6\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u8c61\u7ea7\u65b0\u9896\u6027\u68c0\u6d4b\u6784\u5efa\u66f4\u5e73\u8861\u591a\u6837\u7684\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u6570\u636e\u6536\u96c6\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u7f55\u89c1\u4e8b\u4ef6\u7684\u4e0d\u8db3\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u52a8\u6001Mean Shift\u7b97\u6cd5\u4e3a\u56fe\u50cf\u5e27\u5206\u914d\u6570\u636e\u9a71\u52a8\u7684\u65b0\u9896\u6027\u5206\u6570\uff0c\u8bc6\u522b\u5e76\u4fdd\u7559\u542b\u65b0\u9896\u5bf9\u8c61\u7684\u5e27\u3002", "result": "\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5197\u4f59\u6570\u636e\u589e\u52a0\u65f6\u66f4\u6fc0\u8fdb\u7684\u8fc7\u6ee4\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\uff0c\u901a\u8fc7\u6301\u7eed\u66f4\u65b0\u6b63\u5e38\u5185\u5bb9\u5b9a\u4e49\uff0c\u9ad8\u6548\u68c0\u6d4b\u6570\u636e\u6d41\u4e2d\u7684\u65b0\u9896\u6027\u3002"}}
{"id": "2507.04815", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04815", "abs": "https://arxiv.org/abs/2507.04815", "authors": ["Mihai Masala", "Marius Leordeanu"], "title": "From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach", "comment": "arXiv admin note: text overlap with arXiv:2501.08460", "summary": "The task of describing video content in natural language is commonly referred\nto as video captioning. Unlike conventional video captions, which are typically\nbrief and widely available, long-form paragraph descriptions in natural\nlanguage are scarce. This limitation of current datasets is due to the\nexpensive human manual annotation required and to the highly challenging task\nof explaining the language formation process from the perspective of the\nunderlying story, as a complex system of interconnected events in space and\ntime. Through a thorough analysis of recently published methods and available\ndatasets, we identify a general lack of published resources dedicated to the\nproblem of describing videos in complex language, beyond the level of\ndescriptions in the form of enumerations of simple captions. Furthermore, while\nstate-of-the-art methods produce impressive results on the task of generating\nshorter captions from videos by direct end-to-end learning between the videos\nand text, the problem of explaining the relationship between vision and\nlanguage is still beyond our reach. In this work, we propose a shared\nrepresentation between vision and language, based on graphs of events in space\nand time, which can be obtained in an explainable and analytical way, to\nintegrate and connect multiple vision tasks to produce the final natural\nlanguage description. Moreover, we also demonstrate how our automated and\nexplainable video description generation process can function as a fully\nautomatic teacher to effectively train direct, end-to-end neural student\npathways, within a self-supervised neuro-analytical system. We validate that\nour explainable neuro-analytical approach generates coherent, rich and relevant\ntextual descriptions on videos collected from multiple varied datasets, using\nboth standard evaluation metrics, human annotations and consensus from\nensembles of state-of-the-art VLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u4e8b\u4ef6\u56fe\u7684\u5171\u4eab\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u89c6\u9891\u7684\u957f\u6bb5\u843d\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u795e\u7ecf\u5206\u6790\u7cfb\u7edf\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u63cf\u8ff0\u6570\u636e\u96c6\u7f3a\u4e4f\u590d\u6742\u8bed\u8a00\u63cf\u8ff0\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u91ca\u89c6\u89c9\u4e0e\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u7a7a\u4e8b\u4ef6\u56fe\u7684\u5171\u4eab\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u89c6\u89c9\u4efb\u52a1\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u4f5c\u4e3a\u81ea\u52a8\u6559\u5e08\u8bad\u7ec3\u7aef\u5230\u7aef\u795e\u7ecf\u5b66\u751f\u8def\u5f84\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u751f\u6210\u8fde\u8d2f\u3001\u4e30\u5bcc\u4e14\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u548c\u4eba\u7c7b\u6807\u6ce8\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u5206\u6790\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u957f\u6bb5\u843d\u63cf\u8ff0\u751f\u6210\u95ee\u9898\u3002"}}
{"id": "2507.04683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04683", "abs": "https://arxiv.org/abs/2507.04683", "authors": ["Seungwon Oh", "Sangyeon Park", "Isaac Han", "Kyung-Joong Kim"], "title": "Recovering Plasticity of Neural Networks via Soft Weight Rescaling", "comment": null, "summary": "Recent studies have shown that as training progresses, neural networks\ngradually lose their capacity to learn new information, a phenomenon known as\nplasticity loss. An unbounded weight growth is one of the main causes of\nplasticity loss. Furthermore, it harms generalization capability and disrupts\noptimization dynamics. Re-initializing the network can be a solution, but it\nresults in the loss of learned information, leading to performance drops. In\nthis paper, we propose Soft Weight Rescaling (SWR), a novel approach that\nprevents unbounded weight growth without losing information. SWR recovers the\nplasticity of the network by simply scaling down the weight at each step of the\nlearning process. We theoretically prove that SWR bounds weight magnitude and\nbalances weight magnitude between layers. Our experiment shows that SWR\nimproves performance on warm-start learning, continual learning, and\nsingle-task learning setups on standard image classification benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSoft Weight Rescaling (SWR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u7f29\u653e\u6743\u91cd\u9632\u6b62\u65e0\u754c\u589e\u957f\uff0c\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5851\u6027\u635f\u5931\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u6743\u91cd\u65e0\u754c\u589e\u957f\u5bfc\u81f4\u5851\u6027\u635f\u5931\uff0c\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u548c\u4f18\u5316\u52a8\u6001\uff0c\u4f20\u7edf\u91cd\u521d\u59cb\u5316\u65b9\u6cd5\u4f1a\u4e22\u5931\u5df2\u5b66\u4fe1\u606f\u3002", "method": "\u63d0\u51faSWR\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7f29\u653e\u6743\u91cd\uff0c\u907f\u514d\u65e0\u754c\u589e\u957f\u5e76\u4fdd\u6301\u4fe1\u606f\u3002", "result": "\u7406\u8bba\u8bc1\u660eSWR\u80fd\u9650\u5236\u6743\u91cd\u5e45\u5ea6\u5e76\u5e73\u8861\u5c42\u95f4\u6743\u91cd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u5b66\u4e60\u573a\u666f\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SWR\u6709\u6548\u89e3\u51b3\u5851\u6027\u635f\u5931\u95ee\u9898\uff0c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2507.04690", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04690", "abs": "https://arxiv.org/abs/2507.04690", "authors": ["Hanseon Joo", "Hayoung Choi", "Ook Lee", "Minjong Cheon"], "title": "Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed\nactivation functions with learnable univariate functions, but they exhibit\npractical limitations, including high computational costs and performance\ndeficits in general classification tasks. In this paper, we propose the\nModulation Joint KAN (MJKAN), a novel neural network layer designed to overcome\nthese challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like\nmechanism with Radial Basis Function (RBF) activations, creating a hybrid\narchitecture that combines the non-linear expressive power of KANs with the\nefficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's\nperformance across a diverse set of benchmarks, including function regression,\nimage classification (MNIST, CIFAR-10/100), and natural language processing (AG\nNews, SMS Spam). The results demonstrate that MJKAN achieves superior\napproximation capabilities in function regression tasks, significantly\noutperforming MLPs, with performance improving as the number of basis functions\nincreases. Conversely, in image and text classification, its performance was\ncompetitive with MLPs but revealed a critical dependency on the number of basis\nfunctions. We found that a smaller basis size was crucial for better\ngeneralization, highlighting that the model's capacity must be carefully tuned\nto the complexity of the data to prevent overfitting. In conclusion, MJKAN\noffers a flexible architecture that inherits the theoretical advantages of KANs\nwhile improving computational efficiency and practical viability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u5c42MJKAN\uff0c\u7ed3\u5408FiLM\u673a\u5236\u548cRBF\u6fc0\u6d3b\u51fd\u6570\uff0c\u89e3\u51b3\u4e86KANs\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u95ee\u9898\uff0c\u5728\u51fd\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u56fe\u50cf\u548c\u6587\u672c\u5206\u7c7b\u4e2d\u9700\u8c28\u614e\u8c03\u6574\u53c2\u6570\u3002", "motivation": "Kolmogorov-Arnold Networks (KANs) \u867d\u7136\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "MJKAN\u7ed3\u5408\u4e86FiLM\u673a\u5236\u548cRBF\u6fc0\u6d3b\u51fd\u6570\uff0c\u5f62\u6210\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u517c\u5177KANs\u7684\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b\u548cMLPs\u7684\u9ad8\u6548\u6027\u3002", "result": "\u5728\u51fd\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8eMLPs\uff1b\u5728\u56fe\u50cf\u548c\u6587\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4e0eMLPs\u76f8\u5f53\uff0c\u4f46\u9700\u8c03\u6574\u57fa\u7840\u51fd\u6570\u6570\u91cf\u4ee5\u907f\u514d\u8fc7\u62df\u5408\u3002", "conclusion": "MJKAN\u7ee7\u627f\u4e86KANs\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u662f\u4e00\u79cd\u7075\u6d3b\u7684\u67b6\u6784\u3002"}}
{"id": "2507.04678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04678", "abs": "https://arxiv.org/abs/2507.04678", "authors": ["Zhenghui Zhao", "Chen Wu", "Di Wang", "Hongruixuan Chen", "Zhuo Zheng"], "title": "ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing", "comment": null, "summary": "Recent advancements in generative methods, especially diffusion models, have\nmade great progress in remote sensing image synthesis. Despite these\nadvancements, existing methods have not explored the simulation of future\nscenarios based on given scenario images. This simulation capability has wide\napplications for urban planning, land managementChangeBridge: Spatiotemporal\nImage Generation with Multimodal Controls, and beyond. In this work, we propose\nChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event\nimages and conditioned on multimodal spatial controls (e.g., text prompts,\ninstance layouts, and semantic maps), ChangeBridge can synthesize post-event\nimages. The core idea behind ChangeBridge is to modeling the noise-to-image\ndiffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal\ncontrols, ChangeBridge leverages a stochastic Brownian-bridge diffusion,\ndirectly modeling the spatiotemporal evolution between pre-event and post-event\nstates. To the best of our knowledge, ChangeBridge is the first spatiotemporal\ngenerative model with multimodal controls for remote sensing. Experimental\nresults demonstrate that ChangeBridge can simulate high-fidelity future\nscenarios aligned with given conditions, including event and event-driven\nbackground variations. Code will be available.", "AI": {"tldr": "ChangeBridge\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u63a7\u5236\u7684\u65f6\u7a7a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7ed9\u5b9a\u573a\u666f\u56fe\u50cf\u751f\u6210\u672a\u6765\u573a\u666f\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u672a\u63a2\u7d22\u57fa\u4e8e\u7ed9\u5b9a\u573a\u666f\u56fe\u50cf\u6a21\u62df\u672a\u6765\u573a\u666f\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u4e00\u80fd\u529b\u5bf9\u57ce\u5e02\u89c4\u5212\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "ChangeBridge\u5229\u7528\u6761\u4ef6\u65f6\u7a7a\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u63a7\u5236\uff08\u5982\u6587\u672c\u63d0\u793a\u3001\u5b9e\u4f8b\u5e03\u5c40\u548c\u8bed\u4e49\u56fe\uff09\uff0c\u901a\u8fc7\u5e03\u6717\u6865\u6269\u6563\u76f4\u63a5\u5efa\u6a21\u4e8b\u4ef6\u524d\u540e\u7684\u65f6\u7a7a\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChangeBridge\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u672a\u6765\u573a\u666f\uff0c\u7b26\u5408\u7ed9\u5b9a\u6761\u4ef6\uff0c\u5305\u62ec\u4e8b\u4ef6\u53ca\u80cc\u666f\u53d8\u5316\u3002", "conclusion": "ChangeBridge\u662f\u9996\u4e2a\u652f\u6301\u591a\u6a21\u6001\u63a7\u5236\u7684\u65f6\u7a7a\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.04702", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04702", "abs": "https://arxiv.org/abs/2507.04702", "authors": ["Feng Yue", "Zhaoxing Zhang", "Junming Jiao", "Zhengyu Liang", "Shiwen Cao", "Feifei Zhang", "Rong Shen"], "title": "Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning", "comment": null, "summary": "Temporal Video Grounding (TVG), which requires pinpointing relevant temporal\nsegments from video based on language query, has always been a highly\nchallenging task in the field of video understanding. Videos often have a\nlarger volume of information and redundancy than texts or images. Models should\npresent comprehensive understanding of the whole video to accurately retrieve\nquery-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large\nLanguage Model (Video-MLLM) for the temporal video grounding task via\nmultimodal temporal sensing reinforcement. Specifically, during the\npreprocessing stage of our pipeline, we employ Self-adaptive Attention\nAllocation (SAA) method based on frame content variation to efficiently use the\nMLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is\nalso utilized to strengthen our model's capability to perceive the boundaries\nof events in the video. In the fine-tuning part of our pipeline, we creatively\napply Partial Irrelevance Refusing-based Group Relative Policy Optimization\n(PIR-GRPO) in TVG area to foster model's temporal reasoning from not only\naccepting relevant video-query pairs but also refusing irrelevant ones.\nExperiments demonstrate that our method accomplishes a notable advantage over\nSOTA solutions by around 3.5% on both the original QVHighlights testbench and\nits corrected version with more reasonable ground truth annotations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTempo-R0\u7684\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u65f6\u95f4\u611f\u77e5\u5f3a\u5316\u89e3\u51b3\u65f6\u95f4\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u4fe1\u606f\u91cf\u5927\u4e14\u5197\u4f59\uff0c\u6a21\u578b\u9700\u5168\u9762\u7406\u89e3\u89c6\u9891\u4ee5\u51c6\u786e\u68c0\u7d22\u67e5\u8be2\u76f8\u5173\u7247\u6bb5\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u5206\u914d\uff08SAA\uff09\u548c\u663e\u5f0f\u65f6\u95f4\u6233\u6a21\u6001\u5bf9\u9f50\uff08ETA\uff09\u9884\u5904\u7406\uff0c\u5e76\u5728\u5fae\u8c03\u9636\u6bb5\u5e94\u7528\u90e8\u5206\u65e0\u5173\u62d2\u7edd\u7b56\u7565\uff08PIR-GRPO\uff09\u3002", "result": "\u5728QVHighlights\u6d4b\u8bd5\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7ea63.5%\u3002", "conclusion": "Tempo-R0\u901a\u8fc7\u591a\u6a21\u6001\u65f6\u95f4\u611f\u77e5\u5f3a\u5316\u548c\u65f6\u95f4\u63a8\u7406\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u89c6\u9891\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.04750", "categories": ["cs.CV", "cs.AI", "68T45, 65D18"], "pdf": "https://arxiv.org/pdf/2507.04750", "abs": "https://arxiv.org/abs/2507.04750", "authors": ["Zicheng Lin", "Xiaoqiang Li", "Yichao Wang", "Chuan Zhu"], "title": "MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry", "comment": "20 pages, 13 figures, 5 tables. Comprehensive benchmark evaluation of\n  optical flow models for PIV. Introduces MCFormer architecture with\n  multi-frame temporal processing and multiple cost volumes. Includes\n  large-scale synthetic PIV dataset based on JHTDB and Blasius CFD simulations.\n  Code and dataset will be made publicly available", "summary": "Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep\nlearning applications face significant hurdles. A critical gap exists: the lack\nof comprehensive evaluation of how diverse optical flow models perform\nspecifically on PIV data, largely due to limitations in available datasets and\nthe absence of a standardized benchmark. This prevents fair comparison and\nhinders progress. To address this, our primary contribution is a novel,\nlarge-scale synthetic PIV benchmark dataset generated from diverse CFD\nsimulations (JHTDB and Blasius). It features unprecedented variety in particle\ndensities, flow velocities, and continuous motion, enabling, for the first\ntime, a standardized and rigorous evaluation of various optical flow and PIV\nalgorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a\nnew deep network architecture leveraging multi-frame temporal information and\nmultiple cost volumes, specifically designed for PIV's sparse nature. Our\ncomprehensive benchmark evaluation, the first of its kind, reveals significant\nperformance variations among adapted optical flow models and demonstrates that\nMCFormer significantly outperforms existing methods, achieving the lowest\noverall normalized endpoint error (NEPE). This work provides both a\nfoundational benchmark resource essential for future PIV research and a\nstate-of-the-art method tailored for PIV challenges. We make our benchmark\ndataset and code publicly available to foster future research in this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u89c4\u6a21\u5408\u6210PIV\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e00\u79cd\u540d\u4e3aMCFormer\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u8bc4\u4f30\u5149\u5b66\u6d41\u548cPIV\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86MCFormer\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u5149\u5b66\u6d41\u6a21\u578b\u5728PIV\u6570\u636e\u4e0a\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7CFD\u6a21\u62df\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210PIV\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faMCFormer\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u591a\u5e27\u65f6\u95f4\u4fe1\u606f\u548c\u591a\u6210\u672c\u4f53\u79ef\u3002", "result": "MCFormer\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5f52\u4e00\u5316\u7aef\u70b9\u8bef\u5dee\uff08NEPE\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aPIV\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u51c6\u8d44\u6e90\u548c\u5148\u8fdb\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.05165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05165", "abs": "https://arxiv.org/abs/2507.05165", "authors": ["Nusrat Munia", "Junfeng Zhu", "Olfa Nasraoui", "Abdullah-Al-Zubaer Imran"], "title": "Differential Attention for Multimodal Crisis Event Analysis", "comment": "Presented at CVPRw 2025, MMFM3", "summary": "Social networks can be a valuable source of information during crisis events.\nIn particular, users can post a stream of multimodal data that can be critical\nfor real-time humanitarian response. However, effectively extracting meaningful\ninformation from this large and noisy data stream and effectively integrating\nheterogeneous data remains a formidable challenge. In this work, we explore\nvision language models (VLMs) and advanced fusion strategies to enhance the\nclassification of crisis data in three different tasks. We incorporate\nLLaVA-generated text to improve text-image alignment. Additionally, we leverage\nContrastive Language-Image Pretraining (CLIP)-based vision and text embeddings,\nwhich, without task-specific fine-tuning, outperform traditional models. To\nfurther refine multimodal fusion, we employ Guided Cross Attention (Guided CA)\nand combine it with the Differential Attention mechanism to enhance feature\nalignment by emphasizing critical information while filtering out irrelevant\ncontent. Our results show that while Differential Attention improves\nclassification performance, Guided CA remains highly effective in aligning\nmultimodal features. Extensive experiments on the CrisisMMD benchmark data set\ndemonstrate that the combination of pretrained VLMs, enriched textual\ndescriptions, and adaptive fusion strategies consistently outperforms\nstate-of-the-art models in classification accuracy, contributing to more\nreliable and interpretable models for three different tasks that are crucial\nfor disaster response. Our code is available at\nhttps://github.com/Munia03/Multimodal_Crisis_Event.", "AI": {"tldr": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5148\u8fdb\u878d\u5408\u7b56\u7565\u63d0\u5347\u5371\u673a\u6570\u636e\u5206\u7c7b\u6027\u80fd\uff0c\u7ed3\u5408LLaVA\u751f\u6210\u6587\u672c\u548cCLIP\u5d4c\u5165\uff0c\u91c7\u7528Guided CA\u548cDifferential Attention\u4f18\u5316\u7279\u5f81\u5bf9\u9f50\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5728\u5371\u673a\u4e8b\u4ef6\u4e2d\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\u6d41\uff0c\u4f46\u4ece\u4e2d\u63d0\u53d6\u6709\u6548\u4fe1\u606f\u5e76\u6574\u5408\u5f02\u6784\u6570\u636e\u4ecd\u5177\u6311\u6218\u3002", "method": "\u7ed3\u5408LLaVA\u751f\u6210\u6587\u672c\u6539\u8fdb\u56fe\u6587\u5bf9\u9f50\uff0c\u5229\u7528CLIP\u5d4c\u5165\uff0c\u91c7\u7528Guided CA\u548cDifferential Attention\u673a\u5236\u4f18\u5316\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728CrisisMMD\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u707e\u96be\u54cd\u5e94\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3VLMs\u3001\u589e\u5f3a\u6587\u672c\u63cf\u8ff0\u548c\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u7684\u7ec4\u5408\u663e\u8457\u63d0\u5347\u4e86\u5371\u673a\u4e8b\u4ef6\u591a\u6a21\u6001\u6570\u636e\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
