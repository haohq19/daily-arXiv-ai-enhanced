{"id": "2602.06975", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06975", "abs": "https://arxiv.org/abs/2602.06975", "authors": ["R. James Cotton", "Thomas Leonard"], "title": "BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents", "comment": null, "summary": "Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.", "AI": {"tldr": "BiomechAgent\u662f\u4e00\u4e2a\u4ee3\u7801\u751f\u6210AI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u751f\u7269\u529b\u5b66\u5206\u6790\uff0c\u65e0\u9700\u7f16\u7a0b\u5373\u53ef\u67e5\u8be2\u6570\u636e\u5e93\u3001\u751f\u6210\u53ef\u89c6\u5316\u3001\u89e3\u91ca\u6570\u636e\uff0c\u4f7f\u8fd0\u52a8\u6355\u6349\u6570\u636e\u5206\u6790\u66f4\u6613\u7528\u3002", "motivation": "\u867d\u7136\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u6280\u672f\u4f7f\u5b9a\u91cf\u8fd0\u52a8\u5206\u6790\u8d8a\u6765\u8d8a\u666e\u53ca\uff0c\u4f46\u5206\u6790\u751f\u6210\u7684\u6570\u636e\u5bf9\u4e8e\u6ca1\u6709\u7f16\u7a0b\u7ecf\u9a8c\u7684\u4e34\u5e8a\u533b\u751f\u4ecd\u7136\u5b58\u5728\u969c\u788d\uff0c\u9700\u8981\u66f4\u6613\u7528\u7684\u5de5\u5177\u6765\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u4e86BiomechAgent\u4ee3\u7801\u751f\u6210AI\u4ee3\u7406\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff1b\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6570\u636e\u68c0\u7d22\u3001\u53ef\u89c6\u5316\u3001\u6d3b\u52a8\u5206\u7c7b\u3001\u65f6\u95f4\u5206\u5272\u548c\u4e34\u5e8a\u63a8\u7406\uff1b\u8bc4\u4f30\u4e86\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u3001\u4e13\u7528\u5de5\u5177\u96c6\u6210\u548c\u4e0d\u540c\u6a21\u578b\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "BiomechAgent\u5728\u6570\u636e\u68c0\u7d22\u548c\u53ef\u89c6\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u7a33\u5065\u51c6\u786e\u5ea6\uff0c\u5c55\u73b0\u51fa\u65b0\u5174\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff1b\u751f\u7269\u529b\u5b66\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u663e\u8457\u4f18\u4e8e\u901a\u7528\u63d0\u793a\uff1b\u96c6\u6210\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e13\u7528\u5de5\u5177\u5927\u5e45\u63d0\u5347\u4e86\u65f6\u7a7a\u5206\u6790\u51c6\u786e\u5ea6\uff1b\u672c\u5730\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u660e\u663e\u4f4e\u4e8e\u524d\u6cbf\u4e91\u7aef\u5927\u6a21\u578b\u3002", "conclusion": "BiomechAgent\u4f7f\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u6570\u636e\u5bf9\u7ec8\u7aef\u7528\u6237\u66f4\u52a0\u6709\u7528\u548c\u6613\u7528\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u4e86\u751f\u7269\u529b\u5b66\u5206\u6790\u7684\u95e8\u69db\uff0c\u4f46\u6a21\u578b\u9009\u62e9\u548c\u4e13\u7528\u5de5\u5177\u96c6\u6210\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.07030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07030", "abs": "https://arxiv.org/abs/2602.07030", "authors": ["Young Jin Ahn", "Yiyang Du", "Zheyuan Zhang", "Haisen Kang"], "title": "Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model", "comment": null, "summary": "Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Neural Sabermetrics with World Model\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u68d2\u7403\u9010\u573a\u6bd4\u8d5b\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u6bd4\u8d5b\u7684\u591a\u65b9\u9762\u6f14\u53d8\u3002", "motivation": "\u4f20\u7edf\u68d2\u7403\u7edf\u8ba1\u65b9\u6cd5\u867d\u7136\u5bf9\u8bc4\u4f30\u548c\u56de\u987e\u5206\u6790\u5f88\u6709\u4ef7\u503c\uff0c\u4f46\u65e0\u6cd5\u5b9a\u4e49\u9010\u7403\u751f\u6210\u7684\u6bd4\u8d5b\u6a21\u578b\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u5355\u6b65\u9884\u6d4b\u6216\u4e8b\u540e\u5206\u6790\u3002", "method": "\u5c06\u68d2\u7403\u6bd4\u8d5b\u5efa\u6a21\u4e3a\u4e8b\u4ef6\u7684\u957f\u81ea\u56de\u5f52\u5e8f\u5217\uff0c\u4f7f\u7528\u8d85\u8fc710\u5e74\u7684MLB\u8ffd\u8e2a\u6570\u636e\uff08700\u4e07\u6b21\u6295\u7403\u5e8f\u5217\uff0c\u7ea630\u4ebf\u4e2atoken\uff09\u6301\u7eed\u9884\u8bad\u7ec3\u5355\u4e2aLLM\u3002", "result": "\u6a21\u578b\u5728\u5206\u5e03\u5185\u5e38\u89c4\u8d5b\u6570\u636e\u548c\u5206\u5e03\u5916\u5b63\u540e\u8d5b\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff1a\u6b63\u786e\u9884\u6d4b\u7ea664%\u7684\u4e0b\u4e00\u6295\u7403\u548c78%\u7684\u51fb\u7403\u5458\u6325\u68d2\u51b3\u7b56\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u4f53\u80b2\u4e16\u754c\u6a21\u578b\uff0c\u4e3a\u68d2\u7403\u6bd4\u8d5b\u63d0\u4f9b\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2602.07035", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07035", "abs": "https://arxiv.org/abs/2602.07035", "authors": ["Jiahao Zhao", "Shaoxuan Xu", "Zhongxiang Sun", "Fengqi Zhu", "Jingyang Ou", "Yuling Shi", "Chongxuan Li", "Xiao Zhang", "Jun Xu"], "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents", "comment": null, "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C", "AI": {"tldr": "\u63d0\u51faDLLM-Searcher\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLM)\u7684\u641c\u7d22\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1P-ReAct\u5e76\u884c\u63a8\u7406\u8303\u5f0f\u89e3\u51b3\u5ef6\u8fdf\u95ee\u9898", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u9762\u4e34\u5ef6\u8fdf\u6311\u6218\uff08\u4e32\u884c\u63a8\u7406\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff09\uff0c\u800cdLLM\u5177\u6709\u5e76\u884c\u89e3\u7801\u4f18\u52bf\u4f46\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\u5f31\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\u6765\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u6548\u7387", "method": "1) \u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\uff1aAgentic SFT\u548cAgentic VRPO\u589e\u5f3adLLM\u4fe1\u606f\u641c\u7d22\u548c\u63a8\u7406\u80fd\u529b\uff1b2) P-ReAct\u8303\u5f0f\uff1a\u4f18\u5148\u89e3\u7801\u5de5\u5177\u8c03\u7528\u6307\u4ee4\uff0c\u5b9e\u73b0\u63a8\u7406\u7b49\u5f85\u5e76\u884c\u5316", "result": "DLLM-Searcher\u6027\u80fd\u4e0e\u4e3b\u6d41LLM\u641c\u7d22\u4ee3\u7406\u76f8\u5f53\uff0cP-ReAct\u5b9e\u73b0\u7ea615%\u7684\u63a8\u7406\u52a0\u901f", "conclusion": "\u6210\u529f\u7ed3\u5408dLLM\u5e76\u884c\u4f18\u52bf\u4e0e\u641c\u7d22\u4ee3\u7406\u9700\u6c42\uff0c\u901a\u8fc7\u80fd\u529b\u589e\u5f3a\u548c\u5e76\u884c\u5316\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u5ef6\u8fdf\u548c\u80fd\u529b\u6311\u6218"}}
{"id": "2602.07033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07033", "abs": "https://arxiv.org/abs/2602.07033", "authors": ["Md Shahriar Kabir", "Sana Alamgeer", "Minakshi Debnath", "Anne H. H. Ngu"], "title": "TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare", "comment": "Previously published at IEEE COMPSAC 2025", "summary": "The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.", "AI": {"tldr": "TransConv-DDPM\uff1a\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u589e\u5f3a\u578b\u751f\u6210AI\u65b9\u6cd5\uff0c\u7ed3\u5408DDPM\u3001U-Net\u3001\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u9886\u57df\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u963b\u788d\u4e86\u533b\u7597AI\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u751f\u6210\u5f0fAI\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548cNLP\u9886\u57df\u5df2\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u56e0\u5176\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\u800c\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51faTransConv-DDPM\u65b9\u6cd5\uff0c\u91c7\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u7ed3\u5408U-Net\u67b6\u6784\u3001\u591a\u5c3a\u5ea6\u5377\u79ef\u6a21\u5757\u548cTransformer\u5c42\uff0c\u4ee5\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0eTimeGAN\u548cDiffusion-TS\u7b49\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728SmartFallMM\u548cEEG\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u6355\u6349\u6570\u636e\u70b9\u95f4\u9010\u6e10\u53d8\u5316\u7684\u65f6\u95f4\u6a21\u5f0f\u3002\u5728SmartFallMM\u6570\u636e\u96c6\u4e0a\uff0c\u6dfb\u52a0\u5408\u6210\u6570\u636e\u4f7f\u9884\u6d4b\u6a21\u578b\u7684F1\u5206\u6570\u63d0\u534713.64%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u63d0\u9ad814.93%\u3002", "conclusion": "TransConv-DDPM\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2602.07019", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07019", "abs": "https://arxiv.org/abs/2602.07019", "authors": ["Elaheh Sabziyan Varnousfaderani", "Syed A. M. Shihab", "Jonathan King"], "title": "Deep Learning Based Multi-Level Classification for Aviation Safety", "comment": null, "summary": "Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u65bc\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u7684\u5f71\u50cf\u9ce5\u985e\u5206\u985e\u6846\u67b6\uff0c\u7528\u65bc\u8b58\u5225\u9ce5\u985e\u7269\u7a2e\u3001\u7fa4\u9ad4\u5f62\u614b\u548c\u898f\u6a21\uff0c\u4ee5\u6539\u5584\u822a\u7a7a\u9ce5\u64ca\u9810\u9632\u7cfb\u7d71", "motivation": "\u73fe\u6709\u9ce5\u64ca\u9810\u9632\u7cfb\u7d71\u4e3b\u8981\u4f9d\u8cf4\u9ce5\u985e\u96f7\u9054\uff0c\u4f46\u7121\u6cd5\u8b58\u5225\u9ce5\u985e\u7269\u7a2e\uff0c\u800c\u4e0d\u540c\u7269\u7a2e\u5177\u6709\u4e0d\u540c\u7684\u98db\u884c\u884c\u70ba\u548c\u9ad8\u5ea6\u504f\u597d\uff0c\u9019\u5c0d\u6e96\u78ba\u9810\u6e2c\u98db\u884c\u8def\u5f91\u81f3\u95dc\u91cd\u8981", "method": "\u4f7f\u7528\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u8a2d\u8a08\u5f71\u50cf\u9ce5\u985e\u5206\u985e\u6846\u67b6\uff0c\u8207\u76f8\u6a5f\u7cfb\u7d71\u914d\u5408\u5be6\u73fe\u81ea\u4e3b\u8996\u89ba\u6aa2\u6e2c\uff0c\u5305\u62ec\u7269\u7a2e\u8b58\u5225\u3001\u7fa4\u9ad4\u5f62\u614b\u5206\u985e\u548c\u7fa4\u9ad4\u898f\u6a21\u4f30\u8a08", "result": "CNN\u6846\u67b6\u80fd\u5920\u8b58\u5225\u9ce5\u985e\u7269\u7a2e\uff0c\u4e26\u63d0\u4f9b\u7fa4\u9ad4\u5f62\u614b\u548c\u898f\u6a21\u4fe1\u606f\uff0c\u9019\u4e9b\u4fe1\u606f\u53ef\u7528\u65bc\u7269\u7a2e\u7279\u5b9a\u7684\u98db\u884c\u8def\u5f91\u9810\u6e2c\u6a21\u578b\uff0c\u63d0\u9ad8\u822a\u7a7a\u5b89\u5168", "conclusion": "\u63d0\u51fa\u7684\u5f71\u50cf\u9ce5\u985e\u5206\u985e\u6846\u67b6\u5f4c\u88dc\u4e86\u73fe\u6709\u96f7\u9054\u7cfb\u7d71\u7684\u4e0d\u8db3\uff0c\u901a\u904e\u7269\u7a2e\u8b58\u5225\u548c\u7fa4\u9ad4\u7279\u5fb5\u5206\u6790\uff0c\u70ba\u66f4\u6e96\u78ba\u7684\u9ce5\u64ca\u98a8\u96aa\u8a55\u4f30\u548c\u9810\u9632\u63d0\u4f9b\u4e86\u95dc\u9375\u4fe1\u606f"}}
{"id": "2602.07391", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07391", "abs": "https://arxiv.org/abs/2602.07391", "authors": ["Kunal Pai", "Parth Shah", "Harshil Patel"], "title": "NAAMSE: Framework for Evolutionary Security Evaluation of Agents", "comment": null, "summary": "AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring \"benign-use correctness\", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.", "AI": {"tldr": "NAAMSE\u662f\u4e00\u4e2a\u8fdb\u5316\u6846\u67b6\uff0c\u5c06AI\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53cd\u9988\u9a71\u52a8\u7684\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u9057\u4f20\u63d0\u793a\u7a81\u53d8\u548c\u5206\u5c42\u8bed\u6599\u5e93\u63a2\u7d22\u6765\u53d1\u73b0\u88ab\u5355\u6b21\u65b9\u6cd5\u9057\u6f0f\u7684\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u7ea2\u961f\u6216\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6a21\u62df\u81ea\u9002\u5e94\u3001\u591a\u8f6e\u6b21\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u5bfc\u81f4\u5b89\u5168\u8bc4\u4f30\u5b58\u5728\u74f6\u9888\u3002", "method": "\u4f7f\u7528\u5355\u4e00\u81ea\u4e3b\u4ee3\u7406\u534f\u8c03\u9057\u4f20\u63d0\u793a\u7a81\u53d8\u3001\u5206\u5c42\u8bed\u6599\u5e93\u63a2\u7d22\u548c\u975e\u5bf9\u79f0\u884c\u4e3a\u8bc4\u5206\u7684\u751f\u547d\u5468\u671f\uff0c\u4ee5\u6a21\u578b\u54cd\u5e94\u4f5c\u4e3a\u9002\u5e94\u5ea6\u4fe1\u53f7\uff0c\u8fed\u4ee3\u5730\u7ec4\u5408\u6709\u6548\u653b\u51fb\u7b56\u7565\uff0c\u540c\u65f6\u786e\u4fdd\"\u826f\u6027\u4f7f\u7528\u6b63\u786e\u6027\"\u3002", "result": "\u5728Gemini 2.5 Flash\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fdb\u5316\u7a81\u53d8\u80fd\u7cfb\u7edf\u6027\u5730\u653e\u5927\u88ab\u5355\u6b21\u65b9\u6cd5\u9057\u6f0f\u7684\u6f0f\u6d1e\uff0c\u63a2\u7d22\u4e0e\u5b9a\u5411\u7a81\u53d8\u7684\u534f\u540c\u4f5c\u7528\u80fd\u53d1\u73b0\u9ad8\u4e25\u91cd\u6027\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\u4e3a\u9762\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u5a01\u80c1\u65f6\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u548c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.07051", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.07051", "abs": "https://arxiv.org/abs/2602.07051", "authors": ["Karthik Sivakoti"], "title": "Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning", "comment": null, "summary": "Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.", "AI": {"tldr": "\u63d0\u51faNeural Sentinel\uff0c\u4e00\u500b\u57fa\u65bc\u8996\u89ba\u8a9e\u8a00\u6a21\u578b(VLM)\u7684\u7d71\u4e00\u5f0f\u8eca\u724c\u8b58\u5225\u7cfb\u7d71\uff0c\u4f7f\u7528\u5fae\u8abf\u7684PaliGemma 3B\u6a21\u578b\uff0c\u900f\u904e\u55ae\u6b21\u524d\u5411\u50b3\u64ad\u540c\u6642\u5b8c\u6210\u8eca\u724c\u8b58\u5225\u3001\u72c0\u614b\u5206\u985e\u548c\u8eca\u8f1b\u5c6c\u6027\u63d0\u53d6\uff0c\u6bd4\u50b3\u7d71\u591a\u968e\u6bb5\u65b9\u6cd5\u66f4\u6e96\u78ba\u4e14\u67b6\u69cb\u66f4\u7c21\u55ae\u3002", "motivation": "\u50b3\u7d71ALPR\u7cfb\u7d71\u63a1\u7528\u591a\u968e\u6bb5\u6d41\u6c34\u7dda\uff08\u76ee\u6a19\u6aa2\u6e2c+OCR\u6a21\u584a\uff09\uff0c\u5b58\u5728\u932f\u8aa4\u7d2f\u7a4d\u3001\u5ef6\u9072\u589e\u52a0\u548c\u67b6\u69cb\u8907\u96dc\u7b49\u554f\u984c\uff0c\u9700\u8981\u66f4\u7d71\u4e00\u3001\u9ad8\u6548\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5fae\u8abf\u7684PaliGemma 3B\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u900f\u904eLoRA\u4f4e\u79e9\u9069\u61c9\u9032\u884c\u8abf\u6574\uff0c\u4e26\u5f15\u5165\u4eba\u6a5f\u8ff4\u5708\u6301\u7e8c\u5b78\u7fd2\u6846\u67b6\uff0c\u4ee570:30\u6bd4\u4f8b\u6df7\u5408\u539f\u59cb\u8a13\u7df4\u6578\u64da\u548c\u4fee\u6b63\u6a23\u672c\u9632\u6b62\u707d\u96e3\u6027\u907a\u5fd8\u3002", "result": "\u9054\u523092.3%\u7684\u8eca\u724c\u8b58\u5225\u6e96\u78ba\u7387\uff0c\u6bd4EasyOCR\u63d0\u534714.1%\uff0c\u6bd4PaddleOCR\u63d0\u53479.9%\uff1b\u5e73\u5747\u63a8\u7406\u5ef6\u9072152ms\uff0c\u6821\u6e96\u8aa4\u5dee0.048\uff1b\u96f6\u6a23\u672c\u6cdb\u5316\u5230\u8eca\u8f1b\u984f\u8272\u6aa2\u6e2c(89%)\u3001\u5b89\u5168\u5e36\u6aa2\u6e2c(82%)\u548c\u4e58\u54e1\u8a08\u6578(78%)\u7b49\u4efb\u52d9\u3002", "conclusion": "\u7d71\u4e00\u7684\u8996\u89ba\u8a9e\u8a00\u65b9\u6cd5\u4ee3\u8868\u4e86ALPR\u7cfb\u7d71\u7684\u7bc4\u5f0f\u8f49\u8b8a\uff0c\u63d0\u4f9b\u66f4\u512a\u7684\u6e96\u78ba\u6027\u3001\u66f4\u4f4e\u7684\u67b6\u69cb\u8907\u96dc\u5ea6\u4ee5\u53ca\u50b3\u7d71\u6d41\u6c34\u7dda\u65b9\u6cd5\u7121\u6cd5\u5be6\u73fe\u7684\u65b0\u8208\u591a\u4efb\u52d9\u80fd\u529b\u3002"}}
{"id": "2602.07205", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07205", "abs": "https://arxiv.org/abs/2602.07205", "authors": ["Junyan Liu", "Haipeng Luo", "Zihan Zhang", "Lillian J. Ratliff"], "title": "Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation", "comment": "36 pages", "summary": "We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.\n  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\\min \\{\\sqrt{K} + (CK)^{1/3},\\sqrt{LK}\\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(\u03b7C + \\sqrt{K/\u03b7})$ regret bound, where $\u03b7$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $\u03b7$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7ecf\u9a8c\u7eb3\u4ec0\u503c\u9057\u61be\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b97\u6cd5\u5728\u4e24\u4eba\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u5b9e\u73b0\u6700\u4f18\u9057\u61be\u754c\uff0c\u80fd\u6839\u636e\u5bf9\u624b\u7684\u975e\u5e73\u7a33\u6027\u81ea\u52a8\u8c03\u6574\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5728\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u8981\u4e48\u65e0\u6cd5\u5b9e\u73b0\u65e0\u5916\u90e8\u9057\u61be\uff0c\u8981\u4e48\u4f7f\u7528\u8f83\u5f31\u7684\u7eb3\u4ec0\u503c\u9057\u61be\u5ea6\u91cf\u4e14\u65e0\u6cd5\u9002\u5e94\u95ee\u9898\u96be\u5ea6\u3002\u5f53\u5bf9\u624b\u56fa\u5b9a\u65f6\uff0c\u73b0\u6709\u7b97\u6cd5\u4ecd\u53ea\u80fd\u8fbe\u5230\u8f83\u5dee\u7684O(K^{2/3})\u9057\u61be\u754c\uff0c\u800c\u7406\u8bba\u4e0aO(\u221aK)\u5916\u90e8\u9057\u61be\u662f\u53ef\u5b9e\u73b0\u7684\u3002", "method": "1. \u63d0\u51fa\u7ecf\u9a8c\u7eb3\u4ec0\u503c\u9057\u61be\u8fd9\u4e00\u65b0\u9057\u61be\u5ea6\u91cf\uff0c\u6bd4\u7eb3\u4ec0\u503c\u9057\u61be\u66f4\u5f3a\uff0c\u4e14\u5728\u5bf9\u624b\u56fa\u5b9a\u65f6\u81ea\u7136\u9000\u5316\u4e3a\u5916\u90e8\u9057\u61be\u30022. \u5bf9Mao\u7b49\u4eba(2022)\u7684\u57fa\u4e8eepoch\u7684V-learning\u7b97\u6cd5\u63d0\u4f9b\u65b0\u5206\u6790\uff0c\u5efa\u7acbO(\u03b7C + \u221aK/\u03b7)\u9057\u61be\u754c\u30023. \u8bbe\u8ba1\u81ea\u9002\u5e94\u91cd\u542f\u673a\u5236\uff0c\u6839\u636e\u5bf9\u624b\u7b56\u7565\u7684\u65b9\u5deeC\u548c\u5207\u6362\u6b21\u6570L\u81ea\u52a8\u8c03\u6574epoch\u589e\u91cf\u56e0\u5b50\u03b7\uff0c\u6700\u7ec8\u5b9e\u73b0\u53c2\u6570\u65e0\u5173\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86O(min{\u221aK + (CK)^{1/3}, \u221aLK})\u9057\u61be\u754c\uff0c\u5176\u4e2dC\u91cf\u5316\u5bf9\u624b\u7b56\u7565\u7684\u65b9\u5dee\uff0cL\u8868\u793a\u7b56\u7565\u5207\u6362\u6b21\u6570\u3002\u8be5\u7ed3\u679c\u4e0d\u4ec5\u6062\u590d\u4e86\u4e24\u4e2a\u6781\u7aef\u60c5\u51b5\uff08\u5bf9\u624b\u56fa\u5b9a\u65f6\u7684O(\u221aK)\u5916\u90e8\u9057\u61be\u548c\u6700\u574f\u60c5\u51b5\u4e0b\u7684O(K^{2/3})\u7eb3\u4ec0\u503c\u9057\u61be\uff09\uff0c\u8fd8\u80fd\u6839\u636e\u5bf9\u624b\u7684\u975e\u5e73\u7a33\u6027\u5728\u8fd9\u4e9b\u6781\u7aef\u4e4b\u95f4\u5e73\u6ed1\u63d2\u503c\u3002", "conclusion": "\u672c\u6587\u5b8c\u5168\u89e3\u51b3\u4e86\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u5728\u7ebf\u5b66\u4e60\u7684\u4e24\u4e2a\u9650\u5236\uff1a\u63d0\u51fa\u4e86\u66f4\u5f3a\u7684\u9057\u61be\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u80fd\u81ea\u9002\u5e94\u5bf9\u624b\u975e\u5e73\u7a33\u6027\u7684\u53c2\u6570\u65e0\u5173\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u9057\u61be\u754c\u5e76\u80fd\u5728\u4e0d\u540c\u96be\u5ea6\u573a\u666f\u95f4\u5e73\u6ed1\u8fc7\u6e21\u3002"}}
{"id": "2602.07695", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07695", "abs": "https://arxiv.org/abs/2602.07695", "authors": ["Congcong Hu", "Yuang Shi", "Fan Huang", "Yang Xiang", "Zhou Ye", "Ming Jin", "Shiyu Wang"], "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge", "comment": null, "summary": "Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.", "AI": {"tldr": "EventCast\u662f\u4e00\u4e2a\u5c06\u672a\u6765\u4e8b\u4ef6\u77e5\u8bc6\u6574\u5408\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u7535\u5546\u5728\u95ea\u8d2d\u3001\u8282\u5047\u65e5\u7b49\u7279\u6b8a\u65f6\u671f\u7684\u9700\u6c42\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7LLM\u5904\u7406\u975e\u7ed3\u6784\u5316\u4e1a\u52a1\u6570\u636e\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6587\u672c\u6458\u8981\uff0c\u7ed3\u5408\u5386\u53f2\u9700\u6c42\u7279\u5f81\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u7cfb\u7edf\u5728\u95ea\u8d2d\u3001\u8282\u5047\u65e5\u4fc3\u9500\u3001\u653f\u7b56\u5e72\u9884\u7b49\u9ad8\u5f71\u54cd\u65f6\u671f\u7ecf\u5e38\u5931\u6548\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u65f6\u671f\u7684\u9700\u6c42\u6a21\u5f0f\u4f1a\u53d1\u751f\u7a81\u7136\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u53d8\u5316\u3002\u7535\u5546\u8fd0\u8425\u9700\u8981\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u7279\u6b8a\u4e8b\u4ef6\u7684\u9884\u6d4b\u7cfb\u7edf\u6765\u6539\u8fdb\u5e93\u5b58\u89c4\u5212\u548c\u5c65\u7ea6\u8c03\u5ea6\u3002", "method": "EventCast\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5229\u7528LLM\u4e13\u95e8\u5904\u7406\u975e\u7ed3\u6784\u5316\u4e1a\u52a1\u6570\u636e\uff08\u5982\u8425\u9500\u6d3b\u52a8\u3001\u8282\u5047\u65e5\u5b89\u6392\u3001\u5356\u5bb6\u6fc0\u52b1\u7b49\uff09\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u6587\u672c\u6458\u8981\uff0c\u6355\u6349\u6587\u5316\u7ec6\u5fae\u5dee\u5f02\u548c\u65b0\u4e8b\u4ef6\u7ec4\u5408\u3002\u8fd9\u4e9b\u6458\u8981\u901a\u8fc7\u53cc\u5854\u67b6\u6784\u4e0e\u5386\u53f2\u9700\u6c42\u7279\u5f81\u878d\u5408\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "result": "\u5728\u8de8\u8d8a4\u4e2a\u56fd\u5bb6160\u4e2a\u5730\u533a10\u4e2a\u6708\u7684\u771f\u5b9e\u7535\u5546\u573a\u666f\u4e2d\uff0cEventCast\u76f8\u6bd4\u65e0\u4e8b\u4ef6\u77e5\u8bc6\u7684\u53d8\u4f53\u5728MAE\u548cMSE\u4e0a\u5206\u522b\u63d0\u534786.9%\u548c97.7%\uff1b\u5728\u4e8b\u4ef6\u9a71\u52a8\u65f6\u671f\uff0c\u76f8\u6bd4\u6700\u4f73\u5de5\u4e1a\u57fa\u7ebf\u5206\u522b\u51cf\u5c11MAE 57.0%\u548cMSE 83.3%\u3002\u81ea2025\u5e743\u6708\u8d77\u5df2\u90e8\u7f72\u5230\u5b9e\u9645\u5de5\u4e1a\u7ba1\u9053\u4e2d\u3002", "conclusion": "EventCast\u901a\u8fc7\u5c06LLM\u4e13\u95e8\u7528\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u63a8\u7406\u800c\u975e\u6570\u503c\u9884\u6d4b\uff0c\u6210\u529f\u6574\u5408\u672a\u6765\u4e8b\u4ef6\u77e5\u8bc6\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u4e3a\u52a8\u6001\u7535\u5546\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u6b8a\u65f6\u671f\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5df2\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u5f97\u5230\u9a8c\u8bc1\u548c\u5e94\u7528\u3002"}}
{"id": "2602.07212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07212", "abs": "https://arxiv.org/abs/2602.07212", "authors": ["Xinyu Liu", "Darryl C. Jacob", "Yuxin Liu", "Xinsong Du", "Muchao Ye", "Bolei Zhou", "Pan He"], "title": "Understanding Real-World Traffic Safety through RoadSafe365 Benchmark", "comment": null, "summary": "Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.", "AI": {"tldr": "RoadSafe365\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\uff0c\u5305\u542b36,196\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\u548c864K\u5019\u9009\u9009\u9879\uff0c\u57fa\u4e8e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u6784\u5efa\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u57fa\u51c6\u7f3a\u4e4f\u4e0e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u7684\u7cfb\u7edf\u5316\u5bf9\u9f50\u8bc4\u4f30\uff0c\u4e3b\u8981\u5173\u6ce8\u7c97\u7c92\u5ea6\u4e8b\u6545\u8bc6\u522b\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u6cd5\u7ec6\u5316\u5e76\u6269\u5c55\u78b0\u649e\u3001\u4e8b\u4ef6\u548c\u8fdd\u89c4\u7684\u57fa\u7840\u5b9a\u4e49\uff0c\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u548c\u76d1\u63a7\u6444\u50cf\u5934\u6536\u96c6\u6570\u636e\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5c5e\u6027\u6807\u6ce8\u548c\u591a\u9009\u9898\u7b54\u6848\u96c6\u3002", "result": "\u5305\u542b36,196\u4e2a\u6807\u6ce8\u7247\u6bb5\uff0c864K\u5019\u9009\u9009\u9879\uff0c8.4K\u552f\u4e00\u7b54\u6848\uff0c36K\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\u3002\u5fae\u8c03\u5b9e\u9a8c\u663e\u793a\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u8de8\u57df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "RoadSafe365\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u53ef\u63a8\u8fdb\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2602.08245", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08245", "abs": "https://arxiv.org/abs/2602.08245", "authors": ["Jinhao Li", "Yuxuan Cong", "Yingqiao Wang", "Hao Xia", "Shan Huang", "Yijia Zhang", "Ningyi Xu", "Guohao Dai"], "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction", "comment": "13 pages, 9 figures", "summary": "Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.", "AI": {"tldr": "STEP\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65f6\u7a7a\u4e00\u81f4\u6027\u9884\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u70ed\u542f\u52a8\u52a8\u4f5c\u6765\u52a0\u901f\u6269\u6563\u7b56\u7565\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u52a8\u4f5c\u8d28\u91cf\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u95ed\u73af\u7cfb\u7edf\u7684\u63a7\u5236\u9891\u7387\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u964d\u4f4e\u91c7\u6837\u6b65\u6570\uff0c\u8981\u4e48\u7ed5\u8fc7\u6269\u6563\u76f4\u63a5\u9884\u6d4b\uff0c\u8981\u4e48\u91cd\u7528\u8fc7\u53bb\u52a8\u4f5c\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u52a8\u4f5c\u8d28\u91cf\u548c\u5b9e\u73b0\u6301\u7eed\u4f4e\u5ef6\u8fdf\u3002", "method": "1. \u8f7b\u91cf\u7ea7\u65f6\u7a7a\u4e00\u81f4\u6027\u9884\u6d4b\u673a\u5236\uff1a\u6784\u5efa\u9ad8\u8d28\u91cf\u70ed\u542f\u52a8\u52a8\u4f5c\uff0c\u65e2\u5728\u5206\u5e03\u4e0a\u63a5\u8fd1\u76ee\u6807\u52a8\u4f5c\uff0c\u53c8\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4e0d\u635f\u5bb3\u539f\u59cb\u6269\u6563\u7b56\u7565\u7684\u751f\u6210\u80fd\u529b\u3002\n2. \u901f\u5ea6\u611f\u77e5\u6270\u52a8\u6ce8\u5165\u673a\u5236\uff1a\u57fa\u4e8e\u65f6\u95f4\u52a8\u4f5c\u53d8\u5316\u81ea\u9002\u5e94\u8c03\u8282\u9a71\u52a8\u6fc0\u52b1\uff0c\u9632\u6b62\u6267\u884c\u505c\u6ede\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u3002\n3. \u7406\u8bba\u5206\u6790\uff1a\u8bc1\u660e\u9884\u6d4b\u673a\u5236\u8bf1\u5bfc\u5c40\u90e8\u6536\u7f29\u6620\u5c04\uff0c\u786e\u4fdd\u6269\u6563\u7ec6\u5316\u8fc7\u7a0b\u4e2d\u52a8\u4f5c\u8bef\u5dee\u7684\u6536\u655b\u3002", "result": "\u57289\u4e2a\u6a21\u62df\u57fa\u51c6\u548c2\u4e2a\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\u3002STEP\u4ec5\u75282\u6b65\u5c31\u80fd\u5728RoboMimic\u57fa\u51c6\u4e0a\u6bd4BRIDGER\u548cDDIM\u5e73\u5747\u63d0\u9ad821.6%\u7684\u6210\u529f\u7387\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u63d0\u9ad827.5%\u7684\u6210\u529f\u7387\u3002STEP\u5728\u63a8\u7406\u5ef6\u8fdf\u548c\u6210\u529f\u7387\u65b9\u9762\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "STEP\u901a\u8fc7\u65f6\u7a7a\u4e00\u81f4\u6027\u9884\u6d4b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u52a8\u4f5c\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u63a8\u7406\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5ef6\u8fdf\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2602.07251", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07251", "abs": "https://arxiv.org/abs/2602.07251", "authors": ["Haley Duba-Sullivan", "Steven R. Young", "Emma J. Reid"], "title": "The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models", "comment": null, "summary": "Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.", "AI": {"tldr": "AdvSR\uff1a\u4e00\u79cd\u5c06\u5bf9\u6297\u884c\u4e3a\u5d4c\u5165\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u6743\u91cd\u7684\u6846\u67b6\uff0c\u53ef\u5728\u8bad\u7ec3\u65f6\u690d\u5165\u540e\u95e8\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8bbf\u95ee\u8f93\u5165\uff0c\u5bfc\u81f4\u4e0b\u6e38\u5206\u7c7b\u5668\u8bef\u5224", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5e38\u4f5c\u4e3a\u6210\u50cf\u7ba1\u9053\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u4f46\u8fd9\u7c7b\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u4e2a\u672a\u88ab\u63a2\u7d22\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u653b\u51fb\u901a\u5e38\u6270\u52a8\u8f93\u5165\u6216\u4f9d\u8d56\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u800c\u672c\u6587\u63a2\u7d22\u6a21\u578b\u7ea7\u522b\u7684\u653b\u51fb\u53ef\u80fd\u6027", "method": "\u63d0\u51faAdvSR\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u65f6\u8054\u5408\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u6297\u7ed3\u679c\uff0c\u5c06\u5bf9\u6297\u884c\u4e3a\u76f4\u63a5\u5d4c\u5165SR\u6a21\u578b\u6743\u91cd\u4e2d\u3002\u65e0\u9700\u63a8\u7406\u65f6\u8bbf\u95ee\u8f93\u5165\uff0c\u653b\u51fb\u5b8c\u5168\u5728\u6a21\u578b\u5c42\u9762\u8fdb\u884c", "result": "\u5728\u4e09\u79cdSR\u67b6\u6784\uff08SRCNN\u3001EDSR\u3001SwinIR\uff09\u4e0eYOLOv11\u5206\u7c7b\u5668\u914d\u5bf9\u6d4b\u8bd5\u4e2d\uff0cAdvSR\u6a21\u578b\u80fd\u4ee5\u6700\u5c0f\u8d28\u91cf\u9000\u5316\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002\u6a21\u578b\u5728\u6807\u51c6\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0b\u8868\u73b0\u6b63\u5e38\uff0c\u5374\u80fd\u8bf1\u5bfc\u4e0b\u6e38\u8bef\u5206\u7c7b", "conclusion": "AdvSR\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7ea7\u522b\u5a01\u80c1\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u6a21\u578b\u6765\u6e90\u548c\u9a8c\u8bc1\u65b9\u5f0f\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u63d0\u9192\u5b9e\u8df5\u8005\u9700\u8981\u66f4\u4e25\u683c\u5730\u5ba1\u67e5\u548c\u9a8c\u8bc1\u6a21\u578b\u5b89\u5168\u6027"}}
{"id": "2602.07341", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07341", "abs": "https://arxiv.org/abs/2602.07341", "authors": ["Yicheng Yang", "Ruijiao Li", "Lifeng Wang", "Shuai Zheng", "Shunzheng Ma", "Keyu Zhang", "Tuoyu Sun", "Chenyun Dai", "Jie Ding", "Zhuo Zou"], "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions", "comment": null, "summary": "This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u7075\u5de7\u673a\u68b0\u81c2-\u624b\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408AR\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u6536\u96c6\u4e13\u5bb6\u6570\u636e\uff0c\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u63d0\u9ad8\u64cd\u4f5c\u4efb\u52a1\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7075\u5de7\u673a\u68b0\u81c2-\u624b\u7cfb\u7edf\u7684\u64cd\u4f5c\u5b66\u4e60\u9762\u4e34\u6570\u636e\u6536\u96c6\u6548\u7387\u4f4e\u3001\u7b56\u7565\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u4eba\u7c7b\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u7b56\u7565\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7edf\u4e00\u6846\u67b6\uff1a1) \u9884\u8bad\u7ec3\u9636\u6bb5\uff1a\u901a\u8fc7AR\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u6536\u96c6\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff0c\u91c7\u7528\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u521d\u59cb\u5316\u7b56\u7565\uff1b2) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff1a\u5f00\u53d1\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u6295\u5f71\u5934\u52a0\u901f\u5b66\u4e60\u8fdb\u7a0b\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u7684\u589e\u5f3a\u5956\u52b1\u673a\u5236\u63d0\u5347\u5b89\u5168\u6027\u3002", "result": "\u5728PyBullet\u7269\u7406\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u7ecf\u5178PPO\u548cSAC\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\uff0c\u800c\u4e14\u5728\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5bf9\u6bd4\u5b66\u4e60\u80fd\u6709\u6548\u9632\u6b62\u7b56\u7565\u5d29\u6e83\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86AR\u8fdc\u7a0b\u4ea4\u4e92\u6570\u636e\u6536\u96c6\u548c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u7075\u5de7\u673a\u68b0\u81c2-\u624b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u64cd\u4f5c\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.08285", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08285", "abs": "https://arxiv.org/abs/2602.08285", "authors": ["Josh Pinskier", "Sarah Baldwin", "Stephen Rodan", "David Howard"], "title": "ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects", "comment": null, "summary": "Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.", "AI": {"tldr": "ReefFlex\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u8f6f\u624b\u6307\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801\u5f02\u8d28\u6293\u53d6\u52a8\u4f5c\u4e3a\u7b80\u5316\u8fd0\u52a8\u57fa\u5143\uff0c\u4f18\u5316\u8bbe\u8ba1\u80fd\u5b89\u5168\u6293\u53d6\u8106\u5f31\u5f02\u5f62\u73ca\u745a\u7684\u8f6f\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u73ca\u745a\u7901\u4fee\u590d\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u3001\u5165\u4fb5\u7269\u79cd\u548c\u4eba\u7c7b\u6d3b\u52a8\u6b63\u4ee5\u524d\u6240\u672a\u6709\u7684\u901f\u5ea6\u7834\u574f\u5168\u7403\u73ca\u745a\u7901\uff0c\u5a01\u80c1\u751f\u7269\u591a\u6837\u6027\u548c\u6e14\u4e1a\uff0c\u51cf\u5c11\u6d77\u5cb8\u4fdd\u62a4\u3002\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u9700\u8981\u53ef\u6269\u5c55\u7684\u73ca\u745a\u518d\u751f\u6280\u672f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b89\u5168\u53ef\u9760\u7684\u5de5\u5177\u6765\u5904\u7406\u8106\u5f31\u7684\u73ca\u745a\u3002", "method": "ReefFlex\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u8f6f\u624b\u6307\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u5f02\u8d28\u6293\u53d6\u7f16\u7801\u4e3a\u7b80\u5316\u8fd0\u52a8\u57fa\u5143\u96c6\u5408\uff0c\u521b\u5efa\u53ef\u5904\u7406\u7684\u591a\u5143\u4f18\u5316\u95ee\u9898\uff0c\u63a2\u7d22\u591a\u6837\u8f6f\u624b\u6307\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u9009\u51fa\u80fd\u5b89\u5168\u6293\u53d6\u8106\u5f31\u5f02\u5f62\u73ca\u745a\u7684\u5019\u9009\u65b9\u6848\u3002", "result": "ReefFlex\u63d0\u9ad8\u4e86\u6293\u53d6\u6210\u529f\u7387\u3001\u6293\u53d6\u8d28\u91cf\uff08\u6297\u5e72\u6270\u6027\u3001\u5b9a\u4f4d\u7cbe\u5ea6\uff09\uff0c\u5e76\u51cf\u5c11\u4e86\u73ca\u745a\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u826f\u4e8b\u4ef6\u3002\u8bbe\u8ba1\u51fa\u7684\u8f6f\u673a\u5668\u4eba\u53ef\u7528\u4e8e\u9646\u4e0a\u6c34\u4ea7\u517b\u6b96\u8bbe\u65bd\u4e2d\u73ca\u745a\u7684\u751f\u957f\u548c\u64cd\u4f5c\uff0c\u4e3a\u672a\u6765\u73ca\u745a\u79fb\u690d\u505a\u51c6\u5907\u3002", "conclusion": "ReefFlex\u4e3a\u590d\u6742\u64cd\u4f5c\u7684\u8f6f\u672b\u7aef\u6267\u884c\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u901a\u7528\u65b9\u6cd5\uff0c\u4e3a\u73ca\u745a\u5904\u7406\u7b49\u5148\u524d\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u9886\u57df\u7684\u81ea\u52a8\u5316\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u6709\u52a9\u4e8e\u73ca\u745a\u7901\u4fee\u590d\u3002"}}
{"id": "2602.07301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07301", "abs": "https://arxiv.org/abs/2602.07301", "authors": ["Aruna Jithesh", "Chinmayi Karumuri", "Venkata Kiran Reddy Kotha", "Meghana Doddapuneni", "Taehee Jeong"], "title": "Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms", "comment": null, "summary": "Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u76f8\u5173\u75c5\u53d8\u7684\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5728DDR\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u75c5\u53d8\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5fae\u52a8\u8109\u7624\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u4e00\u79cd\u53ef\u80fd\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\u548c\u5931\u660e\u7684\u773c\u90e8\u75be\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u9884\u9632\u4e0d\u53ef\u9006\u89c6\u529b\u635f\u5931\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u7b5b\u67e5\u7b97\u6cd5\uff0c\u4f46\u5728\u75c5\u53d8\u5206\u5272\u65b9\u9762\u7684\u4e34\u5e8a\u9002\u7528\u6027\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u9700\u8981\u50cf\u7d20\u7ea7\u6ce8\u91ca\u6765\u652f\u6301\u773c\u79d1\u533b\u751f\u8fdb\u884c\u51c6\u786e\u8bca\u65ad\u3002", "method": "\u672c\u7814\u7a76\u5728DeepLab-V3+\u6a21\u578b\u4e2d\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u5206\u5272\u56db\u79cdDR\u76f8\u5173\u75c5\u53d8\uff1a\u5fae\u52a8\u8109\u7624\u3001\u8f6f\u6027\u6e17\u51fa\u7269\u3001\u786c\u6027\u6e17\u51fa\u7269\u548c\u51fa\u8840\u3002\u5728DDR\u6570\u636e\u96c6\u7684757\u5f20\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u6a21\u578b\u5bf9\u75c5\u53d8\u7279\u5f81\u7684\u5173\u6ce8\u80fd\u529b\u3002", "result": "\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cAttention-DeepLab\u6a21\u578b\u5c06\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u4ece0.3010\u63d0\u5347\u52300.3326\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4ece0.1791\u63d0\u5347\u52300.1928\u3002\u7279\u522b\u91cd\u8981\u7684\u662f\uff0c\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u4ece0.0205\u663e\u8457\u63d0\u5347\u52300.0763\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e34\u5e8a\u610f\u4e49\u91cd\u5927\u7684\u6539\u8fdb\uff0c\u56e0\u4e3a\u5fae\u52a8\u8109\u7624\u662fDR\u6700\u65e9\u53ef\u89c1\u7684\u75c7\u72b6\u3002", "conclusion": "\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\u5728DR\u75c5\u53d8\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5bf9\u65e9\u671fDR\u8bca\u65ad\u5173\u952e\u7684\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8aDR\u7b5b\u67e5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u50cf\u7d20\u7ea7\u75c5\u53d8\u6ce8\u91ca\uff0c\u652f\u6301\u773c\u79d1\u533b\u751f\u8fdb\u884c\u66f4\u6709\u6548\u7684\u8bca\u65ad\u51b3\u7b56\u3002"}}
{"id": "2602.07358", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07358", "abs": "https://arxiv.org/abs/2602.07358", "authors": ["Jiaming He", "Fuming Luo", "Hongwei Li", "Wenbo Jiang", "Wenshu Fan", "Zhenbo Shi", "Xudong Jiang", "Yi Yu"], "title": "UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding", "comment": null, "summary": "Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.", "AI": {"tldr": "UTOPIA\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u4f18\u5316\uff0c\u5728\u9ad8\u663e\u8457\u6027\u7279\u5f81\u4e0a\u6df7\u6dc6\u8bed\u4e49\uff0c\u5728\u4f4e\u663e\u8457\u6027\u5197\u4f59\u7279\u5f81\u4e0a\u5d4c\u5165\u8d85\u76f8\u5173\u6377\u5f84\uff0c\u4e3a\u8868\u683c\u6570\u636e\u63d0\u4f9b\u53ef\u8ba4\u8bc1\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\u4fdd\u62a4\u3002", "motivation": "\u91d1\u878d\u548c\u533b\u7597\u9886\u57df\u7684\u8868\u683c\u6570\u636e\u9ad8\u5ea6\u654f\u611f\uff0c\u4f46\u73b0\u6709\u7684\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u65b9\u6cd5\u5728\u8868\u683c\u6570\u636e\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8868\u683c\u7279\u5f81\u6df7\u5408\u4e86\u6570\u503c\u548c\u7c7b\u522b\u7ea6\u675f\uff0c\u4e14\u5b58\u5728\u663e\u8457\u6027\u7a00\u758f\u6027\uff08\u5b66\u4e60\u4e3b\u8981\u96c6\u4e2d\u5c11\u6570\u7ef4\u5ea6\uff09\u3002", "method": "\u63d0\u51faUTOPIA\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u5f81\u5197\u4f59\u5c06\u4f18\u5316\u89e3\u8026\u4e3a\u4e24\u4e2a\u901a\u9053\uff1a\u9ad8\u663e\u8457\u6027\u7279\u5f81\u7528\u4e8e\u8bed\u4e49\u6df7\u6dc6\uff0c\u4f4e\u663e\u8457\u6027\u5197\u4f59\u7279\u5f81\u7528\u4e8e\u5d4c\u5165\u8d85\u76f8\u5173\u6377\u5f84\uff0c\u5728\u4fdd\u6301\u8868\u683c\u6709\u6548\u6027\u7684\u540c\u65f6\u4ea7\u751f\u7ea6\u675f\u611f\u77e5\u7684\u4e3b\u5bfc\u6377\u5f84\u3002", "result": "\u5728\u591a\u4e2a\u8868\u683c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUTOPIA\u80fd\u4f7f\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u63a5\u8fd1\u968f\u673a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u57fa\u7ebf\uff0c\u5e76\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u6027\u3002", "conclusion": "\u5728\u8c31\u4e3b\u5bfc\u6761\u4ef6\u4e0b\uff0c\u5f53\u6bd2\u5316\u8c31\u538b\u5012\u5e72\u51c0\u8bed\u4e49\u8c31\u65f6\uff0c\u8868\u683c\u6570\u636e\u7684\u53ef\u8ba4\u8bc1\u4e0d\u53ef\u5b66\u4e60\u6027\u662f\u53ef\u884c\u7684\u3002UTOPIA\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u7b56\u7565\u6709\u6548\u4fdd\u62a4\u654f\u611f\u8868\u683c\u6570\u636e\u514d\u53d7\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2602.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08440", "abs": "https://arxiv.org/abs/2602.08440", "authors": ["Tian Gao", "Celine Tan", "Catherine Glossop", "Timothy Gao", "Jiankai Sun", "Kyle Stachowicz", "Shirley Wu", "Oier Mees", "Dorsa Sadigh", "Sergey Levine", "Chelsea Finn"], "title": "SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios", "comment": null, "summary": "A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.", "AI": {"tldr": "SteerVLA \u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u751f\u6210\u7ec6\u7c92\u5ea6\u8bed\u8a00\u6307\u4ee4\u6765\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u9a7e\u9a76\u7b56\u7565\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u4e0e\u5e95\u5c42\u63a7\u5236\u7684\u6709\u6548\u96c6\u6210\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u5c06\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\uff08\u5904\u7406\u957f\u5c3e\u4e8b\u4ef6\uff09\u4e0e\u5e95\u5c42\u53cd\u5e94\u5f0f\u63a7\u5236\uff08\u786e\u4fdd\u9c81\u68d2\u9a7e\u9a76\uff09\u6709\u6548\u96c6\u6210\u3002\u867d\u7136\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5177\u5907\u5f3a\u5927\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5b89\u5168\u8f66\u8f86\u63a7\u5236\u6240\u9700\u7684\u5b9e\u9645\u7ecf\u9a8c\u3002", "method": "\u63d0\u51fa SteerVLA \u65b9\u6cd5\uff0c\u5229\u7528 VLM \u7684\u63a8\u7406\u80fd\u529b\u751f\u6210\u7ec6\u7c92\u5ea6\u8bed\u8a00\u6307\u4ee4\u6765\u5f15\u5bfc VLA \u9a7e\u9a76\u7b56\u7565\u3002\u5173\u952e\u521b\u65b0\u5728\u4e8e\u9ad8\u5c42 VLM \u4e0e\u5e95\u5c42 VLA \u4e4b\u95f4\u7684\u4e30\u5bcc\u8bed\u8a00\u63a5\u53e3\uff0c\u4f7f\u9ad8\u5c42\u7b56\u7565\u80fd\u66f4\u6709\u6548\u5730\u5c06\u5176\u63a8\u7406\u57fa\u4e8e\u5e95\u5c42\u7b56\u7565\u7684\u63a7\u5236\u8f93\u51fa\u3002\u4f7f\u7528 VLM \u4e3a\u73b0\u6709\u9a7e\u9a76\u6570\u636e\u6dfb\u52a0\u8be6\u7ec6\u8bed\u8a00\u6807\u6ce8\uff0c\u63d0\u4f9b\u4e0e\u8f66\u8f86\u63a7\u5236\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u8bed\u8a00\u76d1\u7763\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSteerVLA \u5728\u6574\u4f53\u9a7e\u9a76\u5f97\u5206\u4e0a\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa 4.77 \u5206\uff0c\u5728\u957f\u5c3e\u5b50\u96c6\u4e0a\u9ad8\u51fa 8.04 \u5206\u3002", "conclusion": "SteerVLA \u901a\u8fc7\u8bed\u8a00\u63a5\u53e3\u6709\u6548\u6574\u5408\u4e86\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u4e0e\u5e95\u5c42\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5c3e\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2602.08061", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2602.08061", "abs": "https://arxiv.org/abs/2602.08061", "authors": ["Doni Bloomfield", "Allison Berke", "Moritz S. Hanke", "Aaron Maiwald", "James R. M. Black", "Toby Webster", "Tina Hernandez-Boussard", "Oliver M. Crook", "Jassi Pannu"], "title": "Securing Dual-Use Pathogen Data of Concern", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI", "summary": "Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u5c42\u751f\u7269\u5b89\u5168\u6570\u636e\u7b49\u7ea7\uff08BDL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u6570\u636e\u5728\u8bad\u7ec3AI\u6a21\u578b\u65f6\u53ef\u80fd\u5e26\u6765\u7684\u751f\u7269\u5b89\u5168\u98ce\u9669\u5bf9\u75c5\u539f\u4f53\u6570\u636e\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7b49\u7ea7\u63d0\u51fa\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\u63aa\u65bd\u3002", "motivation": "AI\u751f\u7269\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u7c7b\u578b\u76f4\u63a5\u5173\u7cfb\u5230\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5305\u62ec\u53ef\u80fd\u5e26\u6765\u751f\u7269\u5b89\u5168\u98ce\u9669\u7684\u80fd\u529b\u3002\u968f\u7740AI\u5728\u751f\u7269\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u5efa\u7acb\u6570\u636e\u63a7\u5236\u673a\u5236\u6765\u9632\u6b62AI\u88ab\u7528\u4e8e\u6709\u5bb3\u5e94\u7528\uff08\u5982\u751f\u7269\u6b66\u5668\u5f00\u53d1\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u5c42\u751f\u7269\u5b89\u5168\u6570\u636e\u7b49\u7ea7\uff08BDL\uff09\u6846\u67b6\uff0c\u6839\u636e\u6570\u636e\u5728\u8bad\u7ec3AI\u6a21\u578b\u65f6\u5bf9\u751f\u7269\u5b89\u5168\u98ce\u9669\u7684\u8d21\u732e\u7a0b\u5ea6\u5bf9\u75c5\u539f\u4f53\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002\u4e3a\u6bcf\u4e2aBDL\u7b49\u7ea7\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\u63aa\u65bd\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u65b0\u521b\u5efa\u7684\u53cc\u91cd\u7528\u9014\u75c5\u539f\u4f53\u6570\u636e\u7684\u6cbb\u7406\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u751f\u7269\u5b89\u5168\u98ce\u9669\u5bf9\u75c5\u539f\u4f53\u6570\u636e\u8fdb\u884c\u5206\u7ea7\u7ba1\u7406\u3002\u8be5\u6846\u67b6\u4e3a\u56fd\u9645\u793e\u4f1a\u5236\u5b9aAI\u751f\u7269\u5b89\u5168\u6570\u636e\u63a7\u5236\u653f\u7b56\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002", "conclusion": "\u5728\u8ba1\u7b97\u548c\u7f16\u7801\u8d44\u6e90\u5e7f\u6cdb\u53ef\u53ca\u7684\u4e16\u754c\u4e2d\uff0c\u6570\u636e\u63a7\u5236\u53ef\u80fd\u662f\u51cf\u5c11\u4ee4\u4eba\u62c5\u5fe7\u7684\u751f\u7269AI\u80fd\u529b\u6269\u6563\u7684\u6700\u6709\u6548\u5e72\u9884\u63aa\u65bd\u4e4b\u4e00\u3002BDL\u6846\u67b6\u4e3a\u5b9e\u65bd\u6b64\u7c7b\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.07441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07441", "abs": "https://arxiv.org/abs/2602.07441", "authors": ["Jinzong Dong", "Wei Huang", "Jianshu Zhang", "Zhuo Chen", "Xinzhe Yuan", "Qinying Gu", "Zhaohui Jiang", "Nanyang Ye"], "title": "Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPAR\u65b9\u6cd5\u89e3\u51b3\u79bb\u7ebfRL\u4e2d\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u5929\u82b1\u677f\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u66ff\u6362\u4f4e\u4ef7\u503c\u52a8\u4f5c\u4e3a\u9ad8\u4ef7\u503c\u52a8\u4f5c\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u867d\u7136\u80fd\u4ea7\u751f\u73b0\u5b9e\u7b56\u7565\u5e76\u7f13\u89e3\u5206\u5e03\u5916\u52a8\u4f5c\u504f\u5dee\uff0c\u4f46\u5f53\u6570\u636e\u96c6\u52a8\u4f5c\u6b21\u4f18\u65f6\uff0c\u76f2\u76ee\u6a21\u4eff\u4f1a\u9650\u5236\u667a\u80fd\u4f53\u5145\u5206\u5229\u7528\u8bc4\u8bba\u5bb6\u5efa\u8bae\u7684\u9ad8\u4ef7\u503c\u533a\u57df\uff0c\u5f62\u6210\u6027\u80fd\u5929\u82b1\u677f\u3002", "method": "\u63d0\u51fa\u8fd1\u7aef\u52a8\u4f5c\u66ff\u6362(PAR)\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u6837\u672c\u66ff\u6362\u5668\uff0c\u9010\u6b65\u7528\u7a33\u5b9a\u6f14\u5458\u751f\u6210\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\u66ff\u6362\u4f4e\u4ef7\u503c\u52a8\u4f5c\uff0c\u6269\u5c55\u52a8\u4f5c\u63a2\u7d22\u7a7a\u95f4\u540c\u65f6\u51cf\u5c11\u4f4e\u4ef7\u503c\u6570\u636e\u5f71\u54cd\u3002", "result": "\u5728\u8fde\u7eed\u8d4c\u535a\u673a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5728\u591a\u4e2a\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAR\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u4e0e\u57fa\u7840TD3+BC\u7ed3\u5408\u65f6\u8fbe\u5230state-of-the-art\u6c34\u5e73\u3002", "conclusion": "PAR\u80fd\u6709\u6548\u6253\u7834\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u5929\u82b1\u677f\uff0c\u517c\u5bb9\u591a\u79cdBC\u6b63\u5219\u5316\u8303\u5f0f\uff0c\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6cd5\u3002"}}
{"id": "2602.07498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07498", "abs": "https://arxiv.org/abs/2602.07498", "authors": ["Zhufeng Xu", "Xuan Gao", "Feng-Lin Liu", "Haoxian Zhang", "Zhixue Fang", "Yu-Kun Lai", "Xiaoqiang Liu", "Pengfei Wan", "Lin Gao"], "title": "IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation", "comment": null, "summary": "Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a\u7d27\u51d1\u76841D\u8fd0\u52a8token\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u663e\u5f0f\u65b9\u6cd5\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u548c\u9690\u5f0f\u65b9\u6cd5\u7684\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89d2\u8272\u52a8\u753b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u5b58\u5728\u4e24\u7c7b\u95ee\u9898\uff1a\u663e\u5f0f\u65b9\u6cd5\uff08\u5982\u9aa8\u67b6\u3001DWPose\uff09\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u4e0d\u5339\u914d\u548c\u8eab\u4f53\u6bd4\u4f8b\u53d8\u5316\uff1b\u9690\u5f0f\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u9ad8\u5c42\u8fd0\u52a8\u8bed\u4e49\uff0c\u4f46\u5b58\u5728\u8eab\u4efd\u4fe1\u606f\u6cc4\u6f0f\u548c\u8fd0\u52a8\u4e0e\u5916\u89c2\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u65b0\u9896\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\uff0c\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a\u7d27\u51d1\u76841D\u8fd0\u52a8token\uff0c\u653e\u677e2D\u8868\u793a\u7684\u4e25\u683c\u7a7a\u95f4\u7ea6\u675f\uff1b2. \u8bbe\u8ba1\u57fa\u4e8e\u65f6\u95f4\u4e00\u81f4\u63a9\u7801token\u7684\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u8bad\u7ec3\u74f6\u9888\u51cf\u5c11\u6e90\u56fe\u50cf\u8fd0\u52a8\u7684\u5e72\u6270\uff1b3. \u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u80fd\u529b\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u9632\u6b62\u8eab\u4efd\u4fe1\u606f\u6cc4\u6f0f\uff0c\u63d0\u9ad8\u91cd\u5b9a\u5411\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u548cIM-Animation\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\uff0c\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u89d2\u8272\u52a8\u753b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08121", "abs": "https://arxiv.org/abs/2602.08121", "authors": ["Liying Wang", "Madison Lee", "Yunzhang Jiang", "Steven Chen", "Kewei Sha", "Yunhe Feng", "Frank Wong", "Lisa Hightow-Weidman", "Weichao Yuwen"], "title": "Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention", "comment": null, "summary": "Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an \"empathy trap,\" providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86Glow\u2014\u2014\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684DBT\u6280\u80fd\u6559\u7ec3\uff0c\u7528\u4e8eHIV\u548c\u7269\u8d28\u4f7f\u7528\u98ce\u9669\u4eba\u7fa4\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u8bc4\u4f30\u5176\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\u9700\u8981\u4fee\u590d\u3002", "motivation": "HIV\u548c\u7269\u8d28\u4f7f\u7528\u662f\u76f8\u4e92\u5f71\u54cd\u7684\u6d41\u884c\u75c5\uff0c\u6709\u5171\u540c\u7684\u5fc3\u7406\u5b66\u9a71\u52a8\u56e0\u7d20\uff08\u51b2\u52a8\u6027\u548c\u9002\u5e94\u4e0d\u826f\u7684\u5e94\u5bf9\u65b9\u5f0f\uff09\u3002DBT\u9488\u5bf9\u8fd9\u4e9b\u673a\u5236\u4f46\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u800c\u751f\u6210\u5f0fAI\u6709\u6f5c\u529b\u63d0\u4f9b\u4e2a\u6027\u5316DBT\u6559\u7ec3\uff0c\u4f46\u5176\u5feb\u901f\u53d1\u5c55\u8d85\u8fc7\u4e86\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u7684\u5efa\u8bbe\u3002", "method": "\u5f00\u53d1\u4e86Glow\u2014\u2014\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684DBT\u6280\u80fd\u6559\u7ec3\uff0c\u63d0\u4f9b\u94fe\u5206\u6790\u548c\u89e3\u51b3\u65b9\u6848\u5206\u6790\u3002\u4e0e\u6d1b\u6749\u77f6\u793e\u533a\u536b\u751f\u7ec4\u7ec7\u5408\u4f5c\uff0c\u5bf9\u4e34\u5e8a\u5de5\u4f5c\u4eba\u5458\uff086\u4eba\uff09\u548c\u6709\u751f\u6d3b\u7ecf\u9a8c\u7684\u4e2a\u4f53\uff0828\u4eba\uff09\u8fdb\u884c\u53ef\u7528\u6027\u6d4b\u8bd5\u3002\u4f7f\u7528HHH\u6846\u67b6\uff0c\u91c7\u7528\u7528\u6237\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\uff0c\u53c2\u4e0e\u8005\u8bc6\u522b\u76ee\u6807\u884c\u4e3a\u5e76\u751f\u6210\u4e0a\u4e0b\u6587\u73b0\u5b9e\u7684\u98ce\u9669\u63a2\u6d4b\uff0c\u8bc4\u4f30\u4e8637\u4e2a\u98ce\u9669\u63a2\u6d4b\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u8868\u73b0\u3002", "result": "Glow\u9002\u5f53\u5730\u5904\u7406\u4e8673%\u7684\u98ce\u9669\u63a2\u6d4b\uff0c\u4f46\u4e0d\u540c\u4ee3\u7406\u8868\u73b0\u5dee\u5f02\u5927\uff1a\u89e3\u51b3\u65b9\u6848\u5206\u6790\u4ee3\u7406\u670990%\u7684\u9002\u5f53\u5904\u7406\u7387\uff0c\u800c\u94fe\u5206\u6790\u4ee3\u7406\u53ea\u670944%\u3002\u5b89\u5168\u5931\u8d25\u96c6\u4e2d\u5728\u9f13\u52b1\u7269\u8d28\u4f7f\u7528\u548c\u6b63\u5e38\u5316\u6709\u5bb3\u884c\u4e3a\u3002\u94fe\u5206\u6790\u4ee3\u7406\u9677\u5165\"\u5171\u60c5\u9677\u9631\"\uff0c\u63d0\u4f9b\u5f3a\u5316\u9002\u5e94\u4e0d\u826f\u4fe1\u5ff5\u7684\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u8bc6\u522b\u51fa27\u4e2aDBT\u6280\u80fd\u9519\u8bef\u4fe1\u606f\u5b9e\u4f8b\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5bf9\u751f\u6210\u5f0fAI\u63d0\u4f9b\u7684DBT\u6559\u7ec3\u8fdb\u884cHIV\u548c\u7269\u8d28\u4f7f\u7528\u98ce\u9669\u964d\u4f4e\u7684\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5728\u4e34\u5e8a\u8bd5\u9a8c\u524d\u9700\u8981\u7f13\u89e3\u7684\u6f0f\u6d1e\u3002HHH\u6846\u67b6\u548c\u7528\u6237\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u751f\u6210\u5f0fAI\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.08240", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.08240", "abs": "https://arxiv.org/abs/2602.08240", "authors": ["Xun Su", "Huamin Wang", "Qi Zhang"], "title": "PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition", "comment": null, "summary": "Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.", "AI": {"tldr": "\u63d0\u51faPTS-SNN\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u8c03\u4f18\u89e3\u51b3SSL\u8868\u793a\u4e0eSNN\u52a8\u6001\u7279\u6027\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bed\u97f3\u60c5\u611f\u8bc6\u522b", "motivation": "\u4f20\u7edf\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002SNN\u867d\u7136\u80fd\u6548\u9ad8\uff0c\u4f46\u4e0e\u8fde\u7eed\u81ea\u76d1\u7763\u5b66\u4e60\u8868\u793a\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9ad8\u52a8\u6001\u8303\u56f4\u7684\u5d4c\u5165\u4f1a\u964d\u4f4e\u57fa\u4e8e\u9608\u503c\u795e\u7ecf\u5143\u7684\u4fe1\u606f\u7f16\u7801\u80fd\u529b\u3002", "method": "\u63d0\u51faPTS-SNN\u6846\u67b6\uff1a1) \u4f7f\u7528\u65f6\u79fb\u8109\u51b2\u7f16\u7801\u5668\u901a\u8fc7\u65e0\u53c2\u6570\u901a\u9053\u79fb\u4f4d\u6355\u83b7\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\uff1b2) \u8bbe\u8ba1\u4e0a\u4e0b\u6587\u611f\u77e5\u819c\u7535\u4f4d\u6821\u51c6\u7b56\u7565\uff0c\u5229\u7528\u8109\u51b2\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u5757\u805a\u5408\u5168\u5c40\u8bed\u4e49\u4e0a\u4e0b\u6587\u5230\u53ef\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u4e2d\uff0c\u52a8\u6001\u8c03\u8282PLIF\u795e\u7ecf\u5143\u7684\u504f\u7f6e\u7535\u538b\uff0c\u4f7f\u5f02\u6784\u8f93\u5165\u5206\u5e03\u96c6\u4e2d\u5728\u54cd\u5e94\u6027\u653e\u7535\u8303\u56f4\u5185\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPTS-SNN\u5728IEMOCAP\u4e0a\u8fbe\u523073.34%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u7ade\u4e89\u6027ANN\u76f8\u5f53\uff0c\u540c\u65f6\u4ec5\u97001.19M\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u6bcf\u4e2a\u6837\u672c0.35 mJ\u7684\u63a8\u7406\u80fd\u8017\u3002", "conclusion": "PTS-SNN\u6210\u529f\u89e3\u51b3\u4e86SSL\u8868\u793a\u4e0eSNN\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u4e14\u80fd\u8017\u4f4e\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.07616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07616", "abs": "https://arxiv.org/abs/2602.07616", "authors": ["Juntong Wu", "Jialiang Cheng", "Fuyu Lv", "Ou Dan", "Li Yuan"], "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models", "comment": "Published as a conference paper at ICLR 2026", "summary": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.", "AI": {"tldr": "SERE\u662f\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u4e13\u5bb6\u91cd\u8def\u7531\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728MoE\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u6279\u91cf\u89e3\u7801\uff0c\u901a\u8fc7\u52a8\u6001\u51cf\u5c11\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\u6765\u7f13\u89e3\u6279\u91cf\u63a8\u7406\u4e0e\u4e13\u5bb6\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "MoE\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9700\u8981\u6279\u91cf\u63a8\u7406\u4ee5\u4f18\u5316\u786c\u4ef6\u6548\u7387\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u8fc7\u591a\u7684\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u4ece\u800c\u51cf\u6162\u5185\u5b58\u53d7\u9650\u7684\u89e3\u7801\u9636\u6bb5\u3002\u9700\u8981\u89e3\u51b3\u6279\u91cf\u89e3\u7801\u4e0e\u4e13\u5bb6\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002", "method": "SERE\u901a\u8fc7\u76f8\u4f3c\u6027\u5206\u6790\u52a8\u6001\u51cf\u5c11\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\uff1a1\uff09\u5c06\u6b21\u8981\u4e13\u5bb6\u7684token\u91cd\u8def\u7531\u5230\u6700\u76f8\u4f3c\u7684\u4e3b\u8981\u4e13\u5bb6\uff1b2\uff09\u5229\u7528\u76f8\u4f3c\u6027\u6a21\u5f0f\u8bc6\u522b\u5e76\u4fdd\u7559\u5173\u952e\u4e13\u5bb6\uff1b3\uff09\u907f\u514d\u9759\u6001\u4e13\u5bb6\u526a\u679d\u6216\u5408\u5e76\uff0c\u800c\u662f\u57fa\u4e8e\u6279\u91cf\u7ea7\u4e13\u5bb6\u5197\u4f59\u5b9e\u73b0\u52a8\u6001\u4e13\u5bb6\u8df3\u8fc7\uff1b4\uff09\u63d0\u4f9b\u9ad8\u6548\u7684CUDA\u5185\u6838\uff0c\u53ef\u5728vLLM\u4e2d\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728\u5404\u79cd\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSERE\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.0\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u6700\u5c0f\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6210\u672c\u6548\u76ca\u548c\u5ef6\u8fdf\u654f\u611f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SERE\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u91cd\u8def\u7531\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u4e2d\u6279\u91cf\u89e3\u7801\u4e0e\u4e13\u5bb6\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21MoE\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08373", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08373", "abs": "https://arxiv.org/abs/2602.08373", "authors": ["Feiyu Wu", "Xu Zheng", "Yue Qu", "Zhuocheng Wang", "Zicheng Feng", "Hui Li"], "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI", "comment": "Accepted to ICLR 2026. Project page. https://openreview.net/forum?id=wb05ver1k8&noteId=v1Ax8CwI71", "summary": "Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.", "AI": {"tldr": "VIRF\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u5b9e\u73b0LLM\u89c4\u5212\u5668\u7684\u53ef\u9a8c\u8bc1\u5b89\u5168\uff0c\u4f7f\u7528\u903b\u8f91\u5bfc\u5e08\u63d0\u4f9b\u56e0\u679c\u53cd\u9988\u8fdb\u884c\u667a\u80fd\u8ba1\u5212\u4fee\u590d\uff0c\u5728\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\u5b9e\u73b0\u96f6\u5371\u9669\u884c\u52a8\u7387\u548c\u6700\u9ad8\u76ee\u6807\u8fbe\u6210\u7387\u3002", "motivation": "\u5f53\u524dLLM\u89c4\u5212\u5668\u7f3a\u4e4f\u5f62\u5f0f\u5316\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684LLM\u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c\u8981\u4e48\u7b80\u5355\u62d2\u7edd\u4e0d\u5b89\u5168\u8ba1\u5212\u800c\u4e0d\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u8fed\u4ee3\u7cbe\u70bc\u6846\u67b6(VIRF)\uff0c\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5efa\u7acb\u5bfc\u5e08-\u5b66\u5f92\u5bf9\u8bdd\u673a\u5236\uff1a\u57fa\u4e8e\u5f62\u5f0f\u5316\u5b89\u5168\u672c\u4f53\u7684\u786e\u5b9a\u6027\u903b\u8f91\u5bfc\u5e08\u4e3aLLM\u89c4\u5212\u5668\u63d0\u4f9b\u56e0\u679c\u548c\u6559\u5b66\u53cd\u9988\uff0c\u5b9e\u73b0\u667a\u80fd\u8ba1\u5212\u4fee\u590d\u800c\u975e\u7b80\u5355\u907f\u514d\u3002\u540c\u65f6\u5f15\u5165\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u83b7\u53d6\u7ba1\u9053\uff0c\u4ece\u771f\u5b9e\u4e16\u754c\u6587\u6863\u5408\u6210\u5b89\u5168\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cVIRF\u5b9e\u73b0\u4e860%\u7684\u5371\u9669\u884c\u52a8\u7387(HAR)\u548c77.3%\u7684\u76ee\u6807\u8fbe\u6210\u7387(GCR)\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u6700\u9ad8\u3002\u5e73\u5747\u4ec5\u97001.1\u6b21\u4fee\u6b63\u8fed\u4ee3\uff0c\u6548\u7387\u5f88\u9ad8\u3002", "conclusion": "VIRF\u5c55\u793a\u4e86\u4e00\u6761\u6784\u5efa\u6839\u672c\u4e0a\u53ef\u4fe1\u8d56\u4e14\u53ef\u9a8c\u8bc1\u5b89\u5168\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7684\u539f\u5219\u6027\u9014\u5f84\uff0c\u4ece\u88ab\u52a8\u5b89\u5168\u628a\u5173\u8f6c\u5411\u4e3b\u52a8\u534f\u4f5c\u8303\u5f0f\u3002"}}
{"id": "2602.08006", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08006", "abs": "https://arxiv.org/abs/2602.08006", "authors": ["Riya Mohan", "Juana Valeria Hurtado", "Rohit Mohan", "Abhinav Valada"], "title": "ForecastOcc: Vision-based Semantic Occupancy Forecasting", "comment": null, "summary": "Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.", "AI": {"tldr": "ForecastOcc\u662f\u9996\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u8fc7\u53bb\u7684\u76f8\u673a\u56fe\u50cf\u8054\u5408\u9884\u6d4b\u672a\u6765\u5360\u7528\u72b6\u6001\u548c\u8bed\u4e49\u7c7b\u522b\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u5730\u56fe\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8fd0\u52a8\u76f8\u5173\u7c7b\u522b\uff08\u5982\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\uff09\uff0c\u800c\u8bed\u4e49\u4fe1\u606f\u57fa\u672c\u7f3a\u5931\u3002\u6700\u8fd1\u7684\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4ece\u5355\u72ec\u7f51\u7edc\u83b7\u5f97\u7684\u8fc7\u53bb\u5360\u7528\u9884\u6d4b\uff0c\u8fd9\u4f7f\u5f97\u5f53\u524d\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u8bef\u5dee\u7d2f\u79ef\u7684\u5f71\u54cd\uff0c\u5e76\u963b\u788d\u4e86\u76f4\u63a5\u4ece\u56fe\u50cf\u5b66\u4e60\u65f6\u7a7a\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86ForecastOcc\u6846\u67b6\uff0c\u5305\u542b\u65f6\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u9884\u6d4b\u6a21\u5757\u30012D\u52303D\u89c6\u56fe\u53d8\u6362\u5668\u3001\u7528\u4e8e\u5360\u7528\u9884\u6d4b\u76843D\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u7528\u4e8e\u8de8\u591a\u4e2a\u65f6\u95f4\u8303\u56f4\u7684\u4f53\u7d20\u7ea7\u9884\u6d4b\u7684\u8bed\u4e49\u5360\u7528\u5934\u3002\u8be5\u6846\u67b6\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff1aOcc3D-nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u591a\u89c6\u89d2\u9884\u6d4b\u548cSemanticKITTI\u4e0a\u7684\u5355\u76ee\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cForecastOcc\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ea7\u751f\u8bed\u4e49\u4e30\u5bcc\u3001\u5177\u6709\u672a\u6765\u611f\u77e5\u7684\u9884\u6d4b\uff0c\u80fd\u591f\u6355\u6349\u81ea\u52a8\u9a7e\u9a76\u5173\u952e\u7684\u573a\u666f\u52a8\u6001\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "conclusion": "ForecastOcc\u662f\u9996\u4e2a\u76f4\u63a5\u4ece\u76f8\u673a\u56fe\u50cf\u8054\u5408\u9884\u6d4b\u672a\u6765\u5360\u7528\u72b6\u6001\u548c\u8bed\u4e49\u7c7b\u522b\u7684\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u672a\u6765\u73af\u5883\u72b6\u6001\u7406\u89e3\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002"}}
{"id": "2602.07784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07784", "abs": "https://arxiv.org/abs/2602.07784", "authors": ["Jayawant Bodagala", "Balaji Bodagala"], "title": "Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing", "comment": "Total pages: 9", "summary": "Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.", "AI": {"tldr": "UCATSC\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\uff0c\u8003\u8651\u89c6\u89c9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u786c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\u548c\u9632\u9965\u997f\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u90e8\u7f72\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u57fa\u4e8e\u89c6\u89c9\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u9690\u542b\u7684\u5b89\u5168\u6027\u4ee5\u53ca\u4e3b\u8981\u5728\u6a21\u62df\u4e2d\u5b66\u4e60\u548c\u9a8c\u8bc1\u7684\u975e\u53ef\u89e3\u91ca\u63a7\u5236\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u5e26\u7ea6\u675f\u7684\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5efa\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff0c\u8003\u8651\u89c6\u89c9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u6f14\u65f6\u9884\u6d4b\u5e76\u5f3a\u5236\u6267\u884c\u4e0e\u5b89\u5168\u548c\u9632\u9965\u997f\u76f8\u5173\u7684\u786c\u7ea6\u675f\u3002", "result": "\u7cfb\u7edf\u8bbe\u8ba1\u65e8\u5728\u6539\u5584\u4ea4\u901a\u5ef6\u8fdf\u548c\u6392\u653e\uff0c\u540c\u65f6\u9632\u6b62\u5b89\u5168\u5173\u952e\u9519\u8bef\uff0c\u5e76\u57fa\u4e8e\u663e\u5f0f\u6a21\u578b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u7b56\u7565\u8f93\u51fa\u3002", "conclusion": "UCATSC\u901a\u8fc7\u6a21\u578b\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07801", "abs": "https://arxiv.org/abs/2602.07801", "authors": ["Wenqi Liu", "Yunxiao Wang", "Shijie Ma", "Meng Liu", "Qile Su", "Tianke Zhang", "Haonan Fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Yinwei Wei", "Xuemeng Song"], "title": "VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos", "comment": null, "summary": "In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.", "AI": {"tldr": "VideoTemp-o3\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u89c6\u9891\u5b9a\u4f4d\u548c\u95ee\u7b54\u6765\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u5e27\u91c7\u6837\u95ee\u9898\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5b9a\u4f4d\u80fd\u529b\u548c\u6309\u9700\u526a\u8f91\u529f\u80fd\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u5e27\u91c7\u6837\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7ecf\u5e38\u65e0\u6cd5\u6355\u6349\u5173\u952e\u89c6\u89c9\u8bc1\u636e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u5e7b\u89c9\u589e\u52a0\u3002\u73b0\u6709\u7684\u4ee3\u7406\u5f0f\u89c6\u9891\u601d\u8003\u65b9\u6cd5\u867d\u7136\u91c7\u7528\u5b9a\u4f4d-\u526a\u8f91-\u56de\u7b54\u6d41\u7a0b\uff0c\u4f46\u4ecd\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u5b9a\u4f4d\u80fd\u529b\u5f31\u548c\u5de5\u4f5c\u6d41\u7a0b\u50f5\u5316\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVideoTemp-o3\u7edf\u4e00\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u89c6\u9891\u5b9a\u4f4d\u548c\u95ee\u7b54\u3002\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u8bbe\u8ba1\u7edf\u4e00\u63a9\u7801\u673a\u5236\u4ee5\u9f13\u52b1\u63a2\u7d22\u540c\u65f6\u9632\u6b62\u566a\u58f0\uff1b\u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u4e13\u7528\u5956\u52b1\u4ee5\u9632\u6b62\u5956\u52b1\u653b\u51fb\u3002\u540c\u65f6\u6784\u5efa\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\u5b9a\u4f4d\u95ee\u7b54\u6570\u636e\u548c\u76f8\u5e94\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u7406\u89e3\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "VideoTemp-o3\u901a\u8fc7\u7edf\u4e00\u7684\u4ee3\u7406\u5f0f\u89c6\u9891\u601d\u8003\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5b9a\u4f4d\u548c\u91c7\u6837\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5b9a\u4f4d\u80fd\u529b\u548c\u7075\u6d3b\u7684\u6309\u9700\u526a\u8f91\u529f\u80fd\u3002"}}
{"id": "2602.07827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07827", "abs": "https://arxiv.org/abs/2602.07827", "authors": ["Guoting Wei", "Xia Yuan", "Yang Zhou", "Haizhao Jing", "Yu Liu", "Xianbiao Qi", "Chunxia Zhao", "Haokui Zhang", "Rong Xiao"], "title": "Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection", "comment": null, "summary": "Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.", "AI": {"tldr": "OTA-Det\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u5f00\u653e\u8bcd\u6c47\u822a\u7a7a\u68c0\u6d4b(OVAD)\u548c\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d(RSVG)\u4e24\u79cd\u8303\u5f0f\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u76ee\u6807\u68c0\u6d4b\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6(34 FPS)\u3002", "motivation": "\u73b0\u6709OVAD\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7c97\u7c92\u5ea6\u7c7b\u522b\u8bed\u4e49\uff0c\u800cRSVG\u7ed3\u6784\u4e0a\u53ea\u80fd\u8fdb\u884c\u5355\u76ee\u6807\u5b9a\u4f4d\uff0c\u65e0\u6cd5\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u3002\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "1) \u4efb\u52a1\u91cd\u6784\u7b56\u7565\u7edf\u4e00\u4efb\u52a1\u76ee\u6807\u548c\u76d1\u7763\u673a\u5236\uff0c\u652f\u6301\u8de8\u8303\u5f0f\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\uff1b2) \u5bc6\u96c6\u8bed\u4e49\u5bf9\u9f50\u7b56\u7565\u5efa\u7acb\u4ece\u6574\u4f53\u8868\u8fbe\u5230\u4e2a\u4f53\u5c5e\u6027\u7684\u591a\u7c92\u5ea6\u5bf9\u5e94\u5173\u7cfb\uff1b3) \u57fa\u4e8eRT-DETR\u67b6\u6784\u6269\u5c55\uff0c\u5f15\u5165\u9ad8\u6548\u6a21\u5757\u5b9e\u73b0\u5f00\u653e\u6587\u672c\u68c0\u6d4b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5(\u6db5\u76d6OVAD\u548cRSVG\u4efb\u52a1)\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u630134 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "OTA-Det\u6210\u529f\u7edf\u4e00\u4e86OVAD\u548cRSVG\u8303\u5f0f\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u822a\u7a7a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07764", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07764", "abs": "https://arxiv.org/abs/2602.07764", "authors": ["Tanmay Ambadkar", "Sourav Panda", "Shreyash Kale", "Jonathan Dodge", "Abhinav Verma"], "title": "Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization", "comment": null, "summary": "Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.", "AI": {"tldr": "D\u00b3PO\u662f\u4e00\u4e2a\u57fa\u4e8ePPO\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4f18\u5316\u6d41\u7a0b\u548c\u5ef6\u8fdf\u504f\u597d\u6574\u5408\u6765\u89e3\u51b3\u68af\u5ea6\u5e72\u6270\u548c\u8868\u793a\u574d\u584c\u95ee\u9898\uff0c\u5728\u5355\u7b56\u7565\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7684Pareto\u524d\u6cbf", "motivation": "\u73b0\u6709\u5355\u504f\u597d\u6761\u4ef6\u7b56\u7565\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8106\u5f31\uff0c\u7ecf\u5e38\u65e0\u6cd5\u6062\u590d\u5b8c\u6574\u7684Pareto\u524d\u6cbf\uff0c\u4e3b\u8981\u7531\u4e8e\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u8fc7\u65e9\u6807\u91cf\u5316\u5bfc\u81f4\u7684\u7834\u574f\u6027\u68af\u5ea6\u5e72\u6270\uff0c\u4ee5\u53ca\u504f\u597d\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u574d\u584c", "method": "D\u00b3PO\u57fa\u4e8ePPO\u6846\u67b6\uff0c\u91c7\u7528\u5206\u89e3\u4f18\u5316\u6d41\u7a0b\u4fdd\u7559\u5404\u76ee\u6807\u5b66\u4e60\u4fe1\u53f7\uff0c\u5ef6\u8fdf\u504f\u597d\u6574\u5408\u4ee5\u786e\u4fdd\u7a33\u5b9a\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u5f15\u5165\u7f29\u653e\u591a\u6837\u6027\u6b63\u5219\u5316\u5668\u9632\u6b62\u7b56\u7565\u884c\u4e3a\u5bf9\u504f\u597d\u53d8\u5316\u7684\u654f\u611f\u6027\u574d\u584c", "result": "\u5728\u6807\u51c6MORL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ec\u9ad8\u7ef4\u548c\u591a\u76ee\u6807\u63a7\u5236\u4efb\u52a1\uff0cD\u00b3PO\u59cb\u7ec8\u6bd4\u73b0\u6709\u5355\u7b56\u7565\u548c\u591a\u7b56\u7565\u65b9\u6cd5\u53d1\u73b0\u66f4\u5e7f\u6cdb\u3001\u66f4\u9ad8\u8d28\u91cf\u7684Pareto\u524d\u6cbf\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u8d85\u4f53\u79ef\u548c\u671f\u671b\u6548\u7528\u6307\u6807", "conclusion": "D\u00b3PO\u901a\u8fc7\u89e3\u51b3\u68af\u5ea6\u5e72\u6270\u548c\u8868\u793a\u574d\u584c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u5355\u4e2a\u53ef\u90e8\u7f72\u7b56\u7565\u5c31\u80fd\u83b7\u5f97\u4f18\u8d8a\u7684Pareto\u524d\u6cbf\u6027\u80fd"}}
{"id": "2602.07891", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07891", "abs": "https://arxiv.org/abs/2602.07891", "authors": ["Zihui Gao", "Ke Liu", "Donny Y. Chen", "Duochao Shi", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video", "comment": null, "summary": "Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.", "AI": {"tldr": "SAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u539f\u59cb\u89c6\u9891\u6d41\u4e2d\u6269\u5c55\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6316\u6398\u7ba1\u9053\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u8f68\u8ff9\uff0c\u7ed3\u5408\u7a00\u758f\u51e0\u4f55\u951a\u70b9\u548c\u5bc6\u96c6\u53ef\u5fae\u4e00\u81f4\u6027\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u57283D\u91cd\u5efa\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u5927\u89c4\u6a213D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002\u4e92\u8054\u7f51\u89c6\u9891\u63d0\u4f9b\u4e86\u51e0\u4e4e\u65e0\u9650\u7684\u539f\u59cb\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u51e0\u4f55\u4fe1\u606f\u548c\u5b58\u5728\u89c2\u6d4b\u566a\u58f0\uff0c\u5c06\u5176\u7528\u4f5c\u51e0\u4f55\u5b66\u4e60\u7684\u6269\u5c55\u6e90\u5177\u6709\u6311\u6218\u6027\u3002", "method": "SAGE\u91c7\u7528\u5206\u5c42\u6316\u6398\u7ba1\u9053\uff1a1) \u4fe1\u606f\u6027\u8bad\u7ec3\u8f68\u8ff9\u9009\u62e9\uff1b2) \u901a\u8fc7SfM\u70b9\u4e91\u8fdb\u884c\u7a00\u758f\u51e0\u4f55\u951a\u70b9\uff0c\u63d0\u4f9b\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\uff1b3) \u901a\u8fc73D\u9ad8\u65af\u6e32\u67d3\u5b9e\u73b0\u5bc6\u96c6\u53ef\u5fae\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u591a\u89c6\u89d2\u7ea6\u675f\u3002\u4e3a\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5f15\u5165\u57fa\u4e8e\u951a\u70b9\u6570\u636e\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u57fa\u51c6\u6d4b\u8bd5\uff087Scenes\u3001TUM-RGBD\u3001Matterport3D\uff09\u4e0a\uff0cSAGE\u5c06Chamfer\u8ddd\u79bb\u964d\u4f4e\u4e8620-42%\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SAGE\u5f00\u521b\u4e86\u901a\u8fc7\u4e92\u8054\u7f51\u89c6\u9891\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4e3a\u901a\u75283D\u5b66\u4e60\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u89e3\u51b3\u4e863D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u539f\u59cb\u89c6\u9891\u6d41\u4e2d\u6709\u6548\u5b66\u4e60\u51e0\u4f55\u8868\u793a\u3002"}}
{"id": "2602.07832", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07832", "abs": "https://arxiv.org/abs/2602.07832", "authors": ["Xian Wu", "Kaijie Zhu", "Ying Zhang", "Lun Wang", "Wenbo Guo"], "title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning", "comment": null, "summary": "Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.", "AI": {"tldr": "rePIRL\u662f\u4e00\u4e2a\u53d7\u9006\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u5047\u8bbe\u8981\u6c42\u6700\u5c0f\uff0c\u901a\u8fc7\u53cc\u5b66\u4e60\u8fc7\u7a0b\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u548cPRM\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u5f3a\u5047\u8bbe\uff08\u5982\u9700\u8981\u5176\u5956\u52b1\u51fd\u6570\uff09\uff0c\u8981\u4e48\u5b58\u5728\u5185\u5728\u9650\u5236\uff08\u5982\u71b5\u5d29\u6e83\uff09\uff0c\u5bfc\u81f4PRM\u6548\u679c\u5f31\u6216\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u5bf9\u4e13\u5bb6\u7b56\u7565\u5047\u8bbe\u8981\u6c42\u6700\u5c0f\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u6709\u6548\u7684PRM\u3002", "method": "\u63d0\u51farePIRL\u6846\u67b6\uff0c\u8bbe\u8ba1\u53cc\u5b66\u4e60\u8fc7\u7a0b\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u3002\u91c7\u7528\u5b9a\u5236\u5316\u6280\u672f\u89e3\u51b3\u5c06\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230LLM\u7684\u6311\u6218\uff0c\u7406\u8bba\u4e0a\u7edf\u4e00\u5728\u7ebf\u548c\u79bb\u7ebfPRM\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u6807\u51c6\u5316\u6570\u5b66\u548c\u7f16\u7a0b\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793arePIRL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8bad\u7ec3\u597d\u7684PRM\u53ef\u5e94\u7528\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u4e3a\u56f0\u96be\u95ee\u9898\u8bad\u7ec3\u63d0\u4f9b\u65e9\u671f\u4fe1\u53f7\u3002", "conclusion": "rePIRL\u80fd\u591f\u4ee5\u6700\u5c0f\u5047\u8bbe\u5b66\u4e60\u6709\u6548\u7684PRM\uff0c\u901a\u8fc7\u8be6\u7ec6\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u65b9\u6848\u548c\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u63a8\u7406\u4e2d\u7684\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08036", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08036", "abs": "https://arxiv.org/abs/2602.08036", "authors": ["Jingtao Liu", "Xinming Zhang"], "title": "TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation", "comment": null, "summary": "Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an \"expert module.\" These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.", "AI": {"tldr": "\u63d0\u51faTAAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7a81\u89e6\u8c03\u5236\u5668\u5b9e\u73b0\u65e0\u56de\u653e\u7684\u56fe\u6301\u7eed\u5b66\u4e60\uff0c\u89e3\u51b3\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u5e76\u5728\u5f52\u7eb3\u5b66\u4e60\u573a\u666f\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u56fe\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u56de\u653e\u7b56\u7565\uff0c\u5b58\u5728\u5185\u5b58\u9650\u5236\u3001\u9690\u79c1\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u89e3\u51b3\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6570\u636e\u56de\u653e\u7684\u65b9\u6cd5\u6765\u6709\u6548\u5904\u7406\u6d41\u5f0f\u56fe\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u8c03\u5236\uff08TAAM\uff09\uff0c\u6838\u5fc3\u662f\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7a81\u89e6\u8c03\u5236\u5668\uff08NSMs\uff09\u3002\u6bcf\u4e2a\u65b0\u4efb\u52a1\u8bad\u7ec3\u4e00\u4e2a\u4e13\u7528NSM\u5e76\u51bb\u7ed3\uff0c\u4f5c\u4e3a\"\u4e13\u5bb6\u6a21\u5757\"\u5bf9\u5171\u4eabGNN\u4e3b\u5e72\u7684\u8ba1\u7b97\u6d41\u8fdb\u884c\u8282\u70b9\u6ce8\u610f\u81ea\u9002\u5e94\u8c03\u5236\u3002\u8fd8\u63d0\u51fa\u951a\u5b9a\u591a\u8df3\u4f20\u64ad\uff08AMP\uff09\u65b9\u6cd5\u5904\u7406\u672a\u77e5\u4efb\u52a1ID\u3002", "result": "\u5728\u66f4\u4e25\u683c\u7684\u5f52\u7eb3\u5b66\u4e60\u573a\u666f\u4e0b\uff0cTAAM\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u5168\u9762\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u65e0\u9700\u4efb\u4f55\u6570\u636e\u56de\u653e\u5373\u53ef\u6709\u6548\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u53ef\u4ee5\u6709\u6548\u6307\u5bfc\u56fa\u5b9aGNN\u4e3b\u5e72\u7684\u63a8\u7406\u8fc7\u7a0b\uff0cTAAM\u901a\u8fc7\u795e\u7ecf\u7a81\u89e6\u8c03\u5236\u5668\u5b9e\u73b0\u4e86\u65e0\u56de\u653e\u7684\u56fe\u6301\u7eed\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.08230", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08230", "abs": "https://arxiv.org/abs/2602.08230", "authors": ["Hongwei Ren", "Youxin Jiang", "Qifei Gu", "Xiangqian Wu"], "title": "Generating Adversarial Events: A Motion-Aware Point Cloud Framework", "comment": null, "summary": "Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \\textbf{M}otion-\\textbf{A}ware \\textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.", "AI": {"tldr": "MA-ADV\u662f\u9996\u4e2a\u5229\u7528\u70b9\u4e91\u8868\u793a\u751f\u6210\u5bf9\u6297\u6027\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u6846\u67b6\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u63ed\u793a\u4e86\u4e8b\u4ef6\u611f\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6311\u6218\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u8106\u5f31\u6027\u5a01\u80c1\u7740\u4e8b\u4ef6\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u5bf9\u6297\u653b\u51fb\u7684\u63a2\u7d22\uff0c\u4e3b\u8981\u56e0\u4e3a\u4e3b\u6d41\u4e8b\u4ef6\u8868\u793a\u7684\u975e\u53ef\u5fae\u6027\u963b\u788d\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u6cd5\u6269\u5c55\u3002", "method": "\u63d0\u51faMA-ADV\u8fd0\u52a8\u611f\u77e5\u5bf9\u6297\u6846\u67b6\uff1a1) \u5229\u7528\u70b9\u4e91\u8868\u793a\u751f\u6210\u5bf9\u6297\u4e8b\u4ef6\uff1b2) \u8003\u8651\u4e8b\u4ef6\u4e2d\u7684\u9ad8\u9891\u566a\u58f0\uff0c\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5e73\u6ed1\u6270\u52a8\uff1b3) \u5145\u5206\u5229\u7528\u4e8b\u4ef6\u95f4\u7684\u65f6\u7a7a\u5173\u7cfb\uff1b4) \u7ed3\u5408\u6837\u672c\u7ea7Adam\u4f18\u5316\u3001\u8fed\u4ee3\u7ec6\u5316\u548c\u4e8c\u5206\u641c\u7d22\u5bfb\u627e\u6700\u5c0f\u6210\u672c\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1MA-ADV\u786e\u4fdd100%\u653b\u51fb\u6210\u529f\u7387\u4e14\u6270\u52a8\u6210\u672c\u6700\u5c0f\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5bf9\u9632\u5fa1\u65b9\u6cd5\u7684\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "MA-ADV\u63ed\u793a\u4e86\u4e8b\u4ef6\u611f\u77e5\u7cfb\u7edf\u9762\u4e34\u7684\u5173\u952e\u5b89\u5168\u6311\u6218\uff0c\u4e3a\u672a\u6765\u4e8b\u4ef6\u76f8\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.08086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08086", "abs": "https://arxiv.org/abs/2602.08086", "authors": ["Liisa Janssens", "Laura Middeldorp"], "title": "Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method", "comment": "6 pages, Pre-publication. Copyright 2026 IEEE. Peer Reviewed. Accepted at ICASSP 2026 - 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 3-8 May 2026 in Barcelona, Spain", "summary": "In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u9632\u6b62C-UAS\u7cfb\u7edf\u4e2d\u7684\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\uff0c\u901a\u8fc7\u5b9e\u65bd\u6cd5\u5f8b\u673a\u5236\u8981\u6c42\u6765\u589e\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u5e94\u5bf9\u65e0\u4eba\u673a\u7cfb\u7edf\u5e26\u6765\u7684\u5404\u79cd\u5a01\u80c1\uff0c\u9700\u8981\u4e13\u95e8\u7684C-UAS\u7cfb\u7edf\u3002\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u7b49\u65b0\u5174\u6280\u672f\u589e\u5f3aC-UAS\u80fd\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7b49\u6311\u6218\uff0c\u786e\u4fdd\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u5206\u6790\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7684C-UAS\u7cfb\u7edf\uff0c\u8bc6\u522b\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u4f5c\u4e3a\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u53ef\u5728\u73b0\u6709\u6cd5\u5f8b\u673a\u5236\u4e2d\u5b9e\u65bd\u7684\u8981\u6c42\u6765\u9632\u6b62\u6b64\u7c7b\u653b\u51fb\u3002", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\u8bc6\u522b\u4e86\u9632\u6b62\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7684\u5177\u4f53\u8981\u6c42\uff0c\u8fd9\u4e9b\u8981\u6c42\u80fd\u591f\u589e\u5f3aC-UAS\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\uff0c\u4ece\u800c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5408\u7406\u4fe1\u4efb\u3002", "conclusion": "\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522bC-UAS\u7cfb\u7edf\u4e2d\u7684\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u98ce\u9669\uff0c\u63d0\u51fa\u7684\u6cd5\u5f8b\u673a\u5236\u8981\u6c42\u6709\u52a9\u4e8e\u589e\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u5bf9\u4e8e\u6c11\u7528\u548c\u519b\u4e8b\u73af\u5883\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.08197", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08197", "abs": "https://arxiv.org/abs/2602.08197", "authors": ["Shingo Higashiguchi", "Koki Kawabata", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso", "comment": "Accepted at ACM Web Conference 2026 (WWW2026)", "summary": "With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.", "AI": {"tldr": "\u63d0\u51faKronecker\u65f6\u53d8\u56fe\u5f62\u5957\u7d22(KTVGL)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0c\u901a\u8fc7Kronecker\u79ef\u5f62\u5f0f\u4f30\u8ba1\u6a21\u6001\u7279\u5b9a\u7684\u52a8\u6001\u7f51\u7edc\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740Web\u670d\u52a1\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u91d1\u878d\u3001\u533b\u7597\u3001\u5728\u7ebf\u5e73\u53f0\u7b49\u9886\u57df\u4ea7\u751f\u5927\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u8fd9\u4e9b\u6570\u636e\u901a\u5e38\u5305\u542b\u591a\u4e2a\u76f8\u4e92\u4f5c\u7528\u7684\u53d8\u91cf\uff0c\u4f30\u8ba1\u53d8\u91cf\u95f4\u7684\u65f6\u53d8\u4f9d\u8d56\u5173\u7cfb\uff08\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\uff09\u5bf9\u51c6\u786e\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e38\u8868\u793a\u4e3a\u591a\u6a21\u6001\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u5bfc\u81f4\u7f51\u7edc\u7ed3\u6784\u590d\u6742\u3001\u96be\u4ee5\u89e3\u91ca\u4e14\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u63d0\u51faKronecker\u65f6\u53d8\u56fe\u5f62\u5957\u7d22(KTVGL)\u65b9\u6cd5\uff0c\u901a\u8fc7Kronecker\u79ef\u5f62\u5f0f\u4f30\u8ba1\u6a21\u6001\u7279\u5b9a\u7684\u52a8\u6001\u7f51\u7edc\uff0c\u907f\u514d\u8fc7\u5ea6\u590d\u6742\u7684\u7ea0\u7f20\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u4e3a\u6d41\u7b97\u6cd5\uff0c\u4f7f\u8ba1\u7b97\u65f6\u95f4\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0cKTVGL\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u8fb9\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u65f6\u95f4\u3002\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "KTVGL\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\u4f30\u8ba1\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2602.08582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08582", "abs": "https://arxiv.org/abs/2602.08582", "authors": ["Melany Yang", "Yuhang Yu", "Diwang Weng", "Jinwei Chen", "Wei Dong"], "title": "SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning", "comment": null, "summary": "Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.", "AI": {"tldr": "SemiNFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u53c2\u8003\u5f0f\u8272\u5f69\u8c03\u6574\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u8bad\u7ec3\u8f68\u8ff9\uff08\u4ece\u521a\u6027\u6a21\u4eff\u5230\u76f4\u89c9\u521b\u9020\uff09\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u7ea7\u7f8e\u5b66\u611f\u77e5", "motivation": "\u73b0\u6709\u53c2\u8003\u5f0f\u8272\u5f69\u8c03\u6574\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u50cf\u7d20\u7ea7\u7edf\u8ba1\u8fdb\u884c\u5168\u5c40\u8272\u5f69\u6620\u5c04\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u7f8e\u5b66\u7684\u771f\u6b63\u7406\u89e3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u7684\u8272\u5f69\u8c03\u6574\u6548\u679c", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u4f7f\u7528\u914d\u5bf9\u4e09\u5143\u7ec4\u5b66\u4e60\u57fa\u672c\u7ed3\u6784\u4fdd\u6301\u548c\u8272\u5f69\u6620\u5c04\u6280\u80fd\uff1b2\uff09\u5728\u65e0\u914d\u5bf9\u6570\u636e\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u57f9\u517b\u7ec6\u81f4\u7f8e\u5b66\u611f\u77e5\uff0c\u8bbe\u8ba1\u6df7\u5408\u5728\u7ebf-\u79bb\u7ebf\u5956\u52b1\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "result": "\u5728\u6807\u51c6\u9884\u8bbe\u8fc1\u79fb\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u4efb\u52a1\uff08\u5982\u9ed1\u767d\u7167\u7247\u4e0a\u8272\u3001\u52a8\u6f2b\u5230\u7167\u7247\u7684\u8de8\u57df\u9884\u8bbe\u8fc1\u79fb\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u5b9e\u4e86\u5176\u8d85\u8d8a\u7b80\u5355\u7edf\u8ba1\u5339\u914d\u7684\u9ad8\u7ea7\u7f8e\u5b66\u7406\u89e3\u80fd\u529b", "conclusion": "SemiNFT\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u5b66\u4e60\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u4ece\u521a\u6027\u6a21\u4eff\u5230\u76f4\u89c9\u521b\u9020\u7684\u8fc7\u6e21\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u7684\u540c\u65f6\u83b7\u5f97\u4e86\u9ad8\u7ea7\u7f8e\u5b66\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u53c2\u8003\u5f0f\u8272\u5f69\u8c03\u6574\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.08234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08234", "abs": "https://arxiv.org/abs/2602.08234", "authors": ["Peng Xia", "Jianwen Chen", "Hanyang Wang", "Jiaqi Liu", "Kaide Zeng", "Yu Wang", "Siwei Han", "Yiyang Zhou", "Xujiang Zhao", "Haifeng Chen", "Zeyu Zheng", "Cihang Xie", "Huaxiu Yao"], "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "comment": null, "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "AI": {"tldr": "SkillRL\uff1a\u901a\u8fc7\u81ea\u52a8\u6280\u80fd\u53d1\u73b0\u548c\u9012\u5f52\u8fdb\u5316\uff0c\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u884c\u4e3a\u6a21\u5f0f\u7684LLM\u667a\u80fd\u4f53\u6846\u67b6", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bb0\u5fc6\u7684LLM\u667a\u80fd\u4f53\u65b9\u6cd5\u4e3b\u8981\u5b58\u50a8\u539f\u59cb\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u8f68\u8ff9\u5197\u4f59\u4e14\u566a\u58f0\u591a\uff0c\u65e0\u6cd5\u63d0\u53d6\u9ad8\u7ea7\u53ef\u91cd\u7528\u884c\u4e3a\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b", "method": "1. \u57fa\u4e8e\u7ecf\u9a8c\u7684\u84b8\u998f\u673a\u5236\u6784\u5efa\u5206\u5c42\u6280\u80fd\u5e93SkillBank\uff1b2. \u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u83b7\u53d6\u901a\u7528\u548c\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\uff1b3. \u9012\u5f52\u8fdb\u5316\u673a\u5236\u4f7f\u6280\u80fd\u5e93\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u534f\u540c\u8fdb\u5316", "result": "\u5728ALFWorld\u3001WebShop\u548c\u4e03\u4e2a\u641c\u7d22\u589e\u5f3a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u5f3a\u57fa\u7ebf15.3%\u4ee5\u4e0a\uff0c\u4e14\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "SkillRL\u901a\u8fc7\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6280\u80fd\uff0c\u663e\u8457\u51cf\u5c11token\u5360\u7528\u540c\u65f6\u589e\u5f3a\u63a8\u7406\u6548\u7528\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u53ef\u91cd\u7528\u884c\u4e3a\u6a21\u5f0f\u7684\u6709\u6548\u6846\u67b6"}}
{"id": "2602.08261", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08261", "abs": "https://arxiv.org/abs/2602.08261", "authors": ["Binglin Wu", "Yingyi Zhang", "Xianneng Li", "Ruyue Deng", "Chuan Yue", "Weiru Zhang", "Xiaoyi Zeng"], "title": "Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization", "comment": null, "summary": "Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.", "AI": {"tldr": "PRO-Bid\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u611f\u77e5\u7684\u751f\u6210\u5f0f\u81ea\u52a8\u51fa\u4ef7\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u89e3\u8026\u5e15\u7d2f\u6258\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u9057\u61be\u4f18\u5316\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\u5728\u76ee\u6807CPA\u7ea6\u675f\u4e0b\u7684\u72b6\u6001\u6df7\u53e0\u548c\u5e73\u5747\u884c\u4e3a\u6a21\u4eff\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u548c\u4ef7\u503c\u83b7\u53d6\u3002", "motivation": "\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\u5728\u5e94\u7528\u4e8e\u5177\u6709\u4e25\u683c\u6548\u7387\u7ea6\u675f\uff08\u5982\u76ee\u6807CPA\uff09\u7684\u81ea\u52a8\u51fa\u4ef7\u7cfb\u7edf\u65f6\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a1\uff09\u6807\u51c6\u7684Return-to-Go\u6761\u4ef6\u5ffd\u7565\u4e86\u6210\u672c\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u72b6\u6001\u6df7\u53e0\uff0c\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u8d44\u6e90\u8282\u594f\uff1b2\uff09\u6807\u51c6\u56de\u5f52\u8feb\u4f7f\u7b56\u7565\u6a21\u4eff\u5386\u53f2\u5e73\u5747\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5411\u7ea6\u675f\u8fb9\u754c\u4f18\u5316\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPRO-Bid\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u673a\u5236\uff1a1\uff09\u7ea6\u675f\u89e3\u8026\u5e15\u7d2f\u6258\u8868\u793a\uff08CDPR\uff09\uff1a\u5c06\u5168\u5c40\u7ea6\u675f\u5206\u89e3\u4e3a\u9012\u5f52\u6210\u672c\u548c\u4ef7\u503c\u4e0a\u4e0b\u6587\u4ee5\u6062\u590d\u8d44\u6e90\u611f\u77e5\uff0c\u540c\u65f6\u57fa\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\u91cd\u65b0\u52a0\u6743\u8f68\u8ff9\u4ee5\u805a\u7126\u9ad8\u6548\u6570\u636e\uff1b2\uff09\u53cd\u4e8b\u5b9e\u9057\u61be\u4f18\u5316\uff08CRO\uff09\uff1a\u5229\u7528\u5168\u5c40\u7ed3\u679c\u9884\u6d4b\u5668\u8bc6\u522b\u66f4\u4f18\u7684\u53cd\u4e8b\u5b9e\u52a8\u4f5c\uff0c\u5c06\u8fd9\u4e9b\u9ad8\u6548\u7528\u7ed3\u679c\u4f5c\u4e3a\u52a0\u6743\u56de\u5f52\u76ee\u6807\uff0c\u4f7f\u6a21\u578b\u8d85\u8d8a\u5386\u53f2\u5e73\u5747\u884c\u4e3a\uff0c\u63a5\u8fd1\u6700\u4f18\u7ea6\u675f\u8fb9\u754c\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cPRO-Bid\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u4ef7\u503c\u83b7\u53d6\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PRO-Bid\u901a\u8fc7\u521b\u65b0\u7684\u7ea6\u675f\u89e3\u8026\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u4f18\u5316\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u51b3\u7b56\u53d8\u6362\u5668\u5728\u7ea6\u675f\u81ea\u52a8\u51fa\u4ef7\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u8fb9\u754c\u4f18\u5316\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.08726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08726", "abs": "https://arxiv.org/abs/2602.08726", "authors": ["Khadija Iddrisu", "Waseem Shariff", "Suzanne Little", "Noel OConnor"], "title": "SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training", "comment": "Accepted to the 2nd Workshop on \"Event-based Vision in the Era of Generative AI - Transforming Perception and Visual Innovation, IEEE Winter Conference on Applications of Computer Vision (WACV 2026)", "summary": "The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.", "AI": {"tldr": "\u4f7f\u7528Blender\u751f\u6210\u5408\u6210\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u6a21\u62df\u773c\u52a8\uff0c\u7ed3\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u7684\u773c\u52a8\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe0.83", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u76f8\u673a\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\uff08DVS\uff09\u5177\u6709\u5f02\u6b65\u8bb0\u5f55\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u6570\u636e\u6548\u7387\u7684\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u773c\u52a8\u6570\u636e\u96c6\u3002\u9700\u8981\u5f00\u53d1\u5408\u6210\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u773c\u52a8\u5206\u7c7b\u6a21\u578b\u3002", "method": "1. \u4f7f\u7528Blender\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u6a21\u62df\u626b\u89c6\u548c\u6ce8\u89c6\u773c\u52a8\uff1b2. \u91c7\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u67b6\u6784\uff1b3. \u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff1b4. \u6bd4\u8f83SNN\u4e0e\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u7684\u6027\u80fd\u3002", "result": "1. \u6a21\u578b\u51c6\u786e\u7387\u8fbe\u52300.83\uff1b2. \u5728\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff1b3. SNN\u76f8\u6bd4ANN\u83b7\u5f97\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff1b4. \u5408\u6210\u6570\u636e\u589e\u5f3a\u5728\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u5408\u6210\u4e8b\u4ef6\u6570\u636e\u4e0e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u4e3a\u773c\u52a8\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u589e\u5f3a\u5728\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.08753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08753", "abs": "https://arxiv.org/abs/2602.08753", "authors": ["Tianyu Sun", "Zhoujie Fu", "Bang Zhang", "Guosheng Lin"], "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization", "comment": null, "summary": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.", "AI": {"tldr": "MVAnimate\u662f\u4e00\u4e2a\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf2D\u548c3D\u89d2\u8272\u52a8\u753b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u89c6\u89d2\u89c6\u9891\u63d0\u5347\u52a8\u753b\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e2D\u62163D\u4eba\u4f53\u59ff\u6001\u5efa\u6a21\u7684\u52a8\u753b\u751f\u6210\u7b97\u6cd5\u5b58\u5728\u8f93\u51fa\u8d28\u91cf\u4f4e\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u52a8\u753b\u89c6\u9891\u3002", "method": "\u63d0\u51faMVAnimate\u6846\u67b6\uff0c\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u5408\u6210\u52a8\u6001\u4eba\u7269\u76842D\u548c3D\u4fe1\u606f\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u7a7a\u95f4\u8fde\u8d2f\u7684\u52a8\u753b\u8f93\u51fa\uff0c\u5e76\u4f18\u5316\u76ee\u6807\u89d2\u8272\u7684\u591a\u89c6\u89d2\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7684\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u89c2\uff0c\u76f8\u6bd4\u73b0\u6709\u52a8\u753b\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "MVAnimate\u901a\u8fc7\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u52a8\u753b\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89d2\u8272\u52a8\u753b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08792", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08792", "abs": "https://arxiv.org/abs/2602.08792", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems", "comment": null, "summary": "The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u6846\u67b6\u7ed3\u5408\u56fe\u50cf\u548c\u529b\u6d4b\u91cf\u6570\u636e\uff0c\u7528\u4e8e\u68c0\u6d4b\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u4e8b\u4ef6\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u77ac\u6001\u7279\u6027\u3001\u566a\u58f0\u73af\u5883\u548c\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u5bf9\u94c1\u8def\u4f9b\u7535\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u98ce\u9669\uff0c\u5305\u62ec\u52a0\u901f\u90e8\u4ef6\u78e8\u635f\u3001\u6027\u80fd\u4e0b\u964d\u548c\u670d\u52a1\u4e2d\u65ad\u3002\u7535\u5f27\u68c0\u6d4b\u9762\u4e34\u77ac\u6001\u7279\u6027\u3001\u566a\u58f0\u73af\u5883\u3001\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u7c7b\u4f3c\u77ac\u6001\u73b0\u8c61\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e24\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u745e\u58eb\u8054\u90a6\u94c1\u8def\u6570\u636e\u548c\u516c\u5f00\u89c6\u9891+\u5408\u6210\u529b\u6570\u636e\uff09\uff0c\u63d0\u51faMultiDeepSAD\u591a\u6a21\u6001\u6269\u5c55\u7b97\u6cd5\uff0c\u5e76\u9488\u5bf9\u6bcf\u79cd\u6570\u636e\u7c7b\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u4f2a\u5f02\u5e38\u751f\u6210\u6280\u672f\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u9886\u57df\u504f\u79fb\u548c\u771f\u5b9e\u7535\u5f27\u89c2\u6d4b\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u771f\u5b9e\u7535\u5f27\u4e8b\u4ef6\u4e5f\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u7ed3\u5408\u89c6\u89c9\u548c\u529b\u6d4b\u91cf\u6570\u636e\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u3001\u66f4\u9c81\u68d2\u5730\u68c0\u6d4b\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u4e8b\u4ef6\uff0c\u4e3a\u89e3\u51b3\u94c1\u8def\u7535\u6c14\u5316\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08646", "abs": "https://arxiv.org/abs/2602.08646", "authors": ["Jisung Hwang", "Minhyuk Sung"], "title": "Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models", "comment": null, "summary": "We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \\log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u7ea6\u675f\u7684\u9690\u53d8\u91cf\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u6027\u767d\u9ad8\u65af\u566a\u58f0\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9690\u53d8\u91cf\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u5956\u52b1\u5f15\u5bfc\u751f\u6210\u6548\u679c\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5bb9\u6613\u53d1\u751f\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\uff1b2\uff09\u4f18\u5316\u901f\u5ea6\u592a\u6162\uff0c\u4e0d\u5b9e\u7528\u3002", "method": "\u91c7\u7528\u786c\u6027\u767d\u9ad8\u65af\u566a\u58f0\u7ea6\u675f\u66ff\u4ee3\u8f6f\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u4e0a\u5347\u6cd5\u5728\u6bcf\u6b21\u66f4\u65b0\u540e\u5e94\u7528\u95ed\u5f0f\u6295\u5f71\uff0c\u4fdd\u6301\u9690\u5411\u91cf\u5728\u6574\u4e2a\u4f18\u5316\u8fc7\u7a0b\u4e2d\u660e\u786e\u4fdd\u6301\u566a\u58f0\u7279\u6027\uff0c\u9632\u6b62\u5bfc\u81f4\u4e0d\u771f\u5b9e\u4f2a\u5f71\u7684\u6f02\u79fb\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700SOTA\u6b63\u5219\u5316\u65b9\u6cd530%\u7684\u5899\u949f\u65f6\u95f4\u5c31\u80fd\u8fbe\u5230\u76f8\u5f53\u7684\u5ba1\u7f8e\u5206\u6570\uff0c\u540c\u65f6\u6709\u6548\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002\u6295\u5f71\u64cd\u4f5c\u590d\u6742\u5ea6\u4e3aO(N log N)\uff0c\u4e0e\u6392\u5e8f\u6216FFT\u7b49\u6807\u51c6\u7b97\u6cd5\u76f8\u5f53\uff0c\u5b9e\u9645\u4e0d\u589e\u52a0\u5899\u949f\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u786c\u6027\u767d\u9ad8\u65af\u566a\u58f0\u7ea6\u675f\u5b9e\u73b0\u7684\u7ea6\u675f\u9690\u53d8\u91cf\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u6d4b\u8bd5\u65f6\u4f18\u5316\u65e2\u9ad8\u6548\u53c8\u53ef\u9760\uff0c\u4e3a\u5956\u52b1\u5f15\u5bfc\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08679", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08679", "abs": "https://arxiv.org/abs/2602.08679", "authors": ["Yanzhang Fu", "Zizheng Guo", "Jizhou Luo"], "title": "Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks", "comment": null, "summary": "Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.", "AI": {"tldr": "\u63d0\u51faDashed Line Defense (DLD)\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u635f\u5931\u89c2\u6d4b\u7684\u6a21\u7cca\u6027\u6765\u62b5\u5fa1\u81ea\u9002\u5e94\u67e5\u8be2\u653b\u51fb\uff0c\u5728ImageNet\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5206\u6570\u7684\u67e5\u8be2\u653b\u51fb\u4ec5\u901a\u8fc7\u9ed1\u76d2\u8bbf\u95ee\u6a21\u578b\u8f93\u51fa\u5206\u6570\u5c31\u80fd\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u8fd0\u884c\u65f6\u9632\u5fa1\u5927\u591a\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\uff0c\u6216\u8005\u65e0\u6cd5\u62b5\u5fa1\u653b\u51fb\u8005\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5373\u63d2\u5373\u7528\u9632\u5fa1\u4e5f\u80fd\u88ab\u81ea\u9002\u5e94\u653b\u51fb\u7ed5\u8fc7\u3002", "method": "\u63d0\u51faDashed Line Defense (DLD)\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u540e\u5904\u7406\u65b9\u6cd5\u3002\u901a\u8fc7\u5728\u89c2\u6d4b\u635f\u5931\u4e0e\u5019\u9009\u6837\u672c\u771f\u5b9e\u5bf9\u6297\u5f3a\u5ea6\u4e4b\u95f4\u5f15\u5165\u6a21\u7cca\u6027\uff0c\u963b\u6b62\u653b\u51fb\u8005\u53ef\u9760\u5206\u6790\u548c\u9002\u5e94\u5176\u67e5\u8be2\uff0c\u4ece\u800c\u6709\u6548\u7834\u574f\u5bf9\u6297\u6837\u672c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDLD\u5728\u4fdd\u6301\u6a21\u578b\u9884\u6d4b\u6807\u7b7e\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6700\u574f\u60c5\u51b5\u7684\u81ea\u9002\u5e94\u653b\u51fb\u4e0b\u4e5f\u80fd\u6709\u6548\u9632\u5fa1\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86DLD\u9632\u5fa1\u80fd\u529b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "DLD\u662f\u4e00\u79cd\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u591f\u62b5\u5fa1\u81ea\u9002\u5e94\u67e5\u8be2\u653b\u51fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fd0\u884c\u65f6\u9632\u5fa1\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u4e3a\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08808", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08808", "abs": "https://arxiv.org/abs/2602.08808", "authors": ["Yapei Chang", "Kyle Lo", "Mohit Iyyer", "Luca Soldaini"], "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs", "comment": "53 pages, 22 figures", "summary": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.", "AI": {"tldr": "How2Everything\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u76ee\u6807\u6761\u4ef6\u7a0b\u5e8f\u751f\u6210\u7684\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u6316\u6398\u3001\u57fa\u51c6\u6784\u5efa\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb", "motivation": "\u751f\u6210\u9010\u6b65\"\u5982\u4f55\u505a\"\u7a0b\u5e8f\u662fLLM\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u6539\u8fdb\u7a0b\u5e8f\u6709\u6548\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u7814\u7a76\u4e0d\u8db3", "method": "1) How2Mine\uff1a\u4ece98\u4e07\u7f51\u9875\u4e2d\u6316\u639835.1\u4e07\u6761\u7a0b\u5e8f\uff1b2) How2Bench\uff1a\u6784\u5efa7K\u5e73\u8861\u8bc4\u4f30\u96c6\uff1b3) How2Score\uff1a\u4f7f\u7528LLM\u6cd5\u5b98\u68c0\u6d4b\u5173\u952e\u5931\u8d25\u7684\u8bc4\u4f30\u534f\u8bae\uff1b4) \u5c06\u524d\u6cbf\u6a21\u578b\u84b8\u998f\u4e3a8B\u6a21\u578b\u8fdb\u884c\u4f4e\u6210\u672c\u8bc4\u4f30\uff1b5) \u4f7f\u7528How2Score\u4f5c\u4e3a\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60", "result": "How2Bench\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u660e\u663e\u6269\u5c55\u8d8b\u52bf\uff1b\u4f7f\u7528How2Score\u8fdb\u884cRL\u5728\u4e09\u4e2a\u6a21\u578b\u4e0a\u5c06\u6027\u80fd\u63d0\u5347\u8d85\u8fc710\u5206\uff0c\u4e14\u5bf9\u6807\u51c6\u57fa\u51c6\u6ca1\u6709\u7cfb\u7edf\u6027\u56de\u5f52", "conclusion": "How2Everything\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u7f51\u7edc\u6570\u636e\u5982\u4f55\u652f\u6301\u80fd\u529b\u8bc4\u4f30\u548c\u6539\u8fdb\u7684\u95ed\u73af\uff0c\u4e3a\u7a0b\u5e8f\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u6846\u67b6"}}
{"id": "2602.08813", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08813", "abs": "https://arxiv.org/abs/2602.08813", "authors": ["Mahdi Sabbaghi", "George Pappas", "Adel Javanmard", "Hamed Hassani"], "title": "Robust Policy Optimization to Prevent Catastrophic Forgetting", "comment": null, "summary": "Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.\n  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.", "AI": {"tldr": "FRPO\u662f\u4e00\u79cd\u9c81\u68d2\u7684RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u5728KL\u6709\u754c\u7b56\u7565\u90bb\u57df\u5185\u4f18\u5316\u5956\u52b1\uff0c\u9632\u6b62\u4e0b\u6e38\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4fdd\u6301\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u591a\u9636\u6bb5\u540e\u8bad\u7ec3\uff08RLHF+\u4e0b\u6e38\u5fae\u8c03\uff09\uff0c\u4f46\u5373\u4f7f\u5c0f\u7684\u4e0b\u6e38\u66f4\u65b0\u4e5f\u4f1a\u7834\u574f\u5148\u524d\u5b66\u4e60\u7684\u884c\u4e3a\uff08\u5982\u5b89\u5168\u6027\uff09\uff0c\u66b4\u9732\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e0b\u6e38\u65f6\u4fdd\u62a4\u5148\u524d\u884c\u4e3a\uff0c\u4f46\u9700\u8981\u9884\u5fae\u8c03\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faFine-tuning Robust Policy Optimization (FRPO)\uff0c\u5728KL\u6709\u754c\u7b56\u7565\u90bb\u57df\u5185\u4f18\u5316\u5956\u52b1\uff0c\u786e\u4fdd\u5956\u52b1\u5728\u7b56\u7565\u53d8\u5316\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u4fee\u6539GRPO\u5f00\u53d1\u7b97\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u3002", "result": "FRPO\u663e\u8457\u51cf\u5c11\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4e0b\u6e38\u5fae\u8c03\u673a\u5236\uff08SFT\u548cRL\uff09\u4e0b\u7684\u5b89\u5168\u6027\u9000\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u5728\u6570\u5b66RL\u8bbe\u7f6e\u4e2d\uff0cFRPO\u80fd\u5728\u540e\u7eed\u5fae\u8c03\u4e0b\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "FRPO\u901a\u8fc7\u9884\u5fae\u8c03\u9c81\u68d2\u6027\u89e3\u51b3\u4e86RLHF\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u786e\u4fdd\u57fa\u7840\u7b56\u7565\u5bf9\u4e0b\u6e38\u5fae\u8c03\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08817", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08817", "abs": "https://arxiv.org/abs/2602.08817", "authors": ["Chenyu Wang", "Zhanglu Yan", "Zhi Zhou", "Xu Chen", "Weng-Fai Wong"], "title": "Kirin: Improving ANN efficiency with SNN Hybridization", "comment": null, "summary": "Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\\% and shortening time steps by 93.75\\%.", "AI": {"tldr": "Kirin\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u6570\u4e0e\u8109\u51b2\u6df7\u5408\u7684SNN\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u65e0\u635f\u7684ANN\u5230SNN\u8f6c\u6362\uff0c\u5728W4A4&8\u91cf\u5316\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u63a5\u8fd1FP16\u7cbe\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e84.66%\u80fd\u8017\u5e76\u7f29\u77ed93.75%\u65f6\u95f4\u6b65\u957f\u3002", "motivation": "\u4f20\u7edfANN\uff08\u7279\u522b\u662fLLM\uff09\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u80fd\u8017\u9ad8\uff0c\u800cSNN\u5177\u6709\u4e8c\u8fdb\u5236\u548c\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\uff0c\u80fd\u6548\u4f18\u5f02\u3002ANN\u5230SNN\u8f6c\u6362\u4e2d\u7684\u91cf\u5316\u8fc7\u7a0b\u9762\u4e34\u6311\u6218\uff1a\u9ad8\u6bd4\u7279\u91cf\u5316\u503c\u9700\u8981\u66f4\u957f\u65f6\u95f4\u7a97\u53e3\uff0c\u589e\u52a0\u7cfb\u7edf\u5ef6\u8fdf\uff1b\u5355\u8109\u51b2\u65b9\u6848\u4fe1\u606f\u635f\u5931\u4e0e\u591a\u8109\u51b2\u65b9\u6848\u80fd\u8017\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\u3002", "method": "\u63d0\u51faKirin\u6df7\u5408SNN\u67b6\u6784\uff1a1\uff09\u8109\u51b2\u77e9\u9635\u6df7\u5408\u7b56\u7565\uff1a\u5c06\u5bfc\u81f4\u5c0f\u65f6\u95f4\u7a97\u53e3\u7684\u4f4e\u6bd4\u7279\u53c2\u6570\u7f16\u7801\u4e3a\u4e8c\u8fdb\u5236\u8109\u51b2\uff0c\u5176\u4f59\u53c2\u6570\u4fdd\u6301\u6574\u6570\u683c\u5f0f\uff0c\u51cf\u5c11SNN\u6267\u884c\u5ef6\u8fdf\uff1b2\uff09\u9759\u9ed8\u9608\u503c\u673a\u5236\uff1a\u8c03\u8282\u5355\u8109\u51b2\u53d1\u5c04\u65f6\u673a\uff0c\u786e\u4fdd\u8f93\u51fa\u4e0eLLM\u6570\u5b66\u7b49\u4ef7\uff0c\u4fdd\u6301\u51c6\u786e\u7387\u3002", "result": "\u5728W4A4&8\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0cKirin\u8fbe\u5230\u63a5\u8fd1FP16\u7cbe\u5ea6\uff0c\u80fd\u8017\u964d\u4f4e84.66%\uff0c\u65f6\u95f4\u6b65\u957f\u7f29\u77ed93.75%\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u65e0\u635f\u7684ANN\u5230SNN\u8f6c\u6362\u3002", "conclusion": "Kirin\u901a\u8fc7\u6574\u6570\u4e0e\u8109\u51b2\u6df7\u5408\u7684SNN\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86ANN\u5230SNN\u8f6c\u6362\u4e2d\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u80fd\u795e\u7ecf\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
