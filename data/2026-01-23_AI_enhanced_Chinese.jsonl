{"id": "2601.15423", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15423", "abs": "https://arxiv.org/abs/2601.15423", "authors": ["Lorian Bannis"], "title": "Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes", "comment": "12 pages, 1 figure, uses tikz.sty", "summary": "We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.", "AI": {"tldr": "Lattice\u662f\u4e00\u4e2a\u6df7\u5408\u5e8f\u5217\u9884\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e8c\u5143\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u6709\u6761\u4ef6\u5730\u6fc0\u6d3b\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u7ed3\u6784\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u3001\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u548c\u91d1\u878d\u5e02\u573a\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7ba1\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u667a\u80fd\u5730\u6fc0\u6d3b\u6216\u62d2\u7edd\u884c\u4e3a\u6a21\u5f0f\u7684\u7ed3\u6784\u5316\u9884\u6d4b\u7cfb\u7edf\u3002", "method": "\u5c06\u884c\u4e3a\u7a97\u53e3\u805a\u7c7b\u4e3a\u884c\u4e3a\u539f\u578b\uff0c\u4f7f\u7528\u4e8c\u5143\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u5728\u7f6e\u4fe1\u5ea6\u8d85\u8fc7\u9608\u503c\u65f6\u6fc0\u6d3b\u57fa\u4e8e\u539f\u578b\u7684\u8bc4\u5206\uff0c\u5426\u5219\u56de\u9000\u5230\u57fa\u7ebf\u9884\u6d4b\u3002", "result": "\u5728MovieLens\u4e0a\uff0cLattice\u6bd4LSTM\u57fa\u7ebf\u63d0\u534731.9%\uff0c\u6bd4SASRec\u548cBERT4Rec\u5206\u522b\u63d0\u5347109.4%\u548c218.6%\uff1b\u5728LIGO\u548c\u91d1\u878d\u6570\u636e\u4e2d\u80fd\u6b63\u786e\u62d2\u7edd\u539f\u578b\u6fc0\u6d3b\uff1b\u5728transformer\u9aa8\u5e72\u4e0a\u4fdd\u6301\u4e2d\u6027\uff080.0%\u6539\u8fdb\uff09\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u67b6\u6784\u539f\u5219\uff0c\u80fd\u591f\u5728\u6a21\u5f0f\u9002\u7528\u65f6\u6fc0\u6d3b\u3001\u4e0d\u9002\u7528\u65f6\u62d2\u7edd\u3001\u5197\u4f59\u65f6\u4f18\u96c5\u5730\u63a8\u8fdf\uff0c\u6709\u6548\u7ba1\u7406\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2601.15475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f20\u611f\u5668\u7269\u7406\u7684NeRF\u6846\u67b6\uff0c\u4ece\u5355\u66dd\u5149\u6a21\u7ccaLDR\u56fe\u50cf\u548c\u5bf9\u5e94\u4e8b\u4ef6\u6570\u636e\u4e2d\u5b9e\u73b0\u9510\u5229HDR\u65b0\u89c6\u89d2\u5408\u6210", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u4e8b\u4ef6\u6570\u636e\u89e3\u51b3\u4f4e\u52a8\u6001\u8303\u56f4\u6a21\u7cca\u56fe\u50cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u95ee\u9898\uff0c\u4f46\u5ffd\u7565\u4e86\u76f8\u673a\u8f93\u51fa\u4e0e\u7269\u7406\u4e16\u754c\u8f90\u5c04\u4e4b\u95f4\u7684\u4f20\u611f\u5668\u7269\u7406\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4HDR\u548c\u53bb\u6a21\u7cca\u6548\u679c\u4e0d\u4f73", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4f20\u611f\u5668\u7269\u7406\u57fa\u7840NeRF\u6846\u67b6\uff1a1) \u7528NeRF\u76f4\u63a5\u8868\u793aHDR\u57df\u4e2d\u7684\u5b9e\u9645\u573a\u666f\u8f90\u5c04\uff1b2) \u5f15\u5165\u50cf\u7d20\u7ea7RGB\u6620\u5c04\u573a\u5bf9\u9f50\u6e32\u67d3\u50cf\u7d20\u503c\u4e0e\u4f20\u611f\u5668\u8bb0\u5f55\u7684LDR\u50cf\u7d20\u503c\uff1b3) \u8bbe\u8ba1\u4e8b\u4ef6\u6620\u5c04\u573a\u6865\u63a5\u7269\u7406\u573a\u666f\u52a8\u6001\u4e0e\u4e8b\u4ef6\u4f20\u611f\u5668\u8f93\u51fa\uff1b4) \u8054\u5408\u4f18\u5316\u4e24\u4e2a\u6620\u5c04\u573a\u4e0eNeRF\u7f51\u7edc", "result": "\u5728\u6536\u96c6\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u53bb\u6a21\u7ccaHDR\u65b0\u89c6\u89d2\u5408\u6210\u7ed3\u679c", "conclusion": "\u901a\u8fc7\u4f20\u611f\u5668\u7269\u7406\u5efa\u6a21\u548c\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u5355\u66dd\u5149\u6a21\u7ccaLDR\u56fe\u50cf\u548c\u4e8b\u4ef6\u6570\u636e\u4e2d\u6062\u590d\u9510\u5229HDR 3D\u8868\u793a\u7684\u95ee\u9898"}}
{"id": "2601.15473", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15473", "abs": "https://arxiv.org/abs/2601.15473", "authors": ["Fahd Seddik", "Abdulrahman Elbedewy", "Gaser Sami", "Mohamed Abdelmoniem", "Yahia Zakaria"], "title": "Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra", "comment": "5 pages, 3 figures, 2 listings", "summary": "Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.", "AI": {"tldr": "Panther\u662f\u4e00\u4e2aPyTorch\u517c\u5bb9\u7684RandNLA\u5e93\uff0c\u901a\u8fc7\u9ad8\u6548\u5b9e\u73b0\u968f\u673a\u5316\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u7684\u5185\u5b58\u5360\u7528\uff08BERT\u4e0a\u53ef\u8fbe75%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u53d7GPU\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\uff0c\u867d\u7136RandNLA\u6280\u672f\u80fd\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u751f\u4ea7\u7ea7\u5e93\u963b\u788d\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5f00\u53d1Panther\u5e93\uff0c\u6574\u5408\u6210\u719f\u7684RandNLA\u7b97\u6cd5\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff1a\u8349\u56fe\u7ebf\u6027\u5c42\u30012D\u5377\u79ef\u3001\u591a\u5934\u6ce8\u610f\u529b\u3001\u968f\u673a\u77e9\u9635\u5206\u89e3\u3002\u901a\u8fc7\u81ea\u5b9a\u4e49C++/CUDA\u540e\u7aef(pawX)\u4f18\u5316CPU/GPU\u5b9e\u73b0\u3002", "result": "\u4ec5\u9700\u51e0\u884c\u4ee3\u7801\u5c06\u6807\u51c6PyTorch\u7ebf\u6027\u5c42\u66ff\u6362\u4e3aPanther\u5c42\uff0c\u5728BERT\u4e0a\u5b9e\u73b0\u9ad8\u8fbe75%\u7684\u5185\u5b58\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u635f\u5931\u6027\u80fd\u3002", "conclusion": "Panther\u8bc1\u660e\u4e86RandNLA\u6280\u672f\u7684\u6709\u6548\u6027\u5e76\u964d\u4f4e\u4e86\u91c7\u7528\u95e8\u69db\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5185\u5b58\u4f18\u5316\u5de5\u5177\u3002"}}
{"id": "2601.15457", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15457", "abs": "https://arxiv.org/abs/2601.15457", "authors": ["Anuj Maharjan", "Umesh Yadav"], "title": "Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.", "AI": {"tldr": "RAG\u67b6\u6784\u80fd\u6709\u6548\u51cf\u5c11LLM\u5728\u516c\u5171\u536b\u751f\u653f\u7b56\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\uff0c\u9ad8\u7ea7RAG\u914d\u7f6e\u6bd4\u57fa\u7840RAG\u548c\u666e\u901aLLM\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347", "motivation": "LLM\u5728\u516c\u5171\u536b\u751f\u653f\u7b56\u9886\u57df\u5e94\u7528\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u8fd9\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u8981\u6c42\u9ad8\u7684\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u6784\u6210\u4e25\u91cd\u969c\u788d\uff0c\u9700\u8981\u5bfb\u627e\u53ef\u9760\u65b9\u6cd5\u6765\u51cf\u5c11\u9519\u8bef\u4fe1\u606f", "method": "\u4f7f\u7528Mistral-7B-Instruct-v0.2\u6a21\u578b\u548call-MiniLM-L6-v2\u5d4c\u5165\u6a21\u578b\uff0c\u6bd4\u8f83\u666e\u901aLLM\u3001\u57fa\u7840RAG\u548c\u91c7\u7528\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u7684\u9ad8\u7ea7RAG\u7ba1\u9053\uff0c\u6d4b\u8bd5\u4e24\u79cd\u5206\u5757\u7b56\u7565\uff08\u9012\u5f52\u5b57\u7b26\u5206\u5757\u548c\u57fa\u4e8e\u4ee4\u724c\u7684\u8bed\u4e49\u5206\u5272\uff09\u5bf9CDC\u653f\u7b56\u6587\u6863\u95ee\u7b54\u51c6\u786e\u6027\u7684\u5f71\u54cd", "result": "\u57fa\u7840RAG\u5728\u5fe0\u5b9e\u5ea6\u4e0a\uff080.621\uff09\u6bd4\u666e\u901aLLM\u57fa\u7ebf\uff080.347\uff09\u6709\u663e\u8457\u63d0\u5347\uff0c\u9ad8\u7ea7RAG\u914d\u7f6e\u8fbe\u5230\u6700\u9ad8\u7684\u5fe0\u5b9e\u5ea6\u5e73\u5747\u503c0.797\uff0c\u4f46\u6587\u6863\u5206\u5272\u7684\u7ed3\u6784\u9650\u5236\u4ecd\u7136\u662f\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u7684\u4e3b\u8981\u74f6\u9888", "conclusion": "\u4e24\u9636\u6bb5\u68c0\u7d22\u673a\u5236\u5bf9\u4e8e\u5b9e\u73b0\u9886\u57df\u7279\u5b9a\u653f\u7b56\u95ee\u7b54\u6240\u9700\u7684\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u9ad8\u7ea7RAG\u67b6\u6784\u80fd\u6709\u6548\u51cf\u5c11LLM\u5e7b\u89c9\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u6587\u6863\u5206\u5272\u7b56\u7565\u4ee5\u652f\u6301\u590d\u6742\u63a8\u7406\u4efb\u52a1"}}
{"id": "2601.15655", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15655", "abs": "https://arxiv.org/abs/2601.15655", "authors": ["Zhenghui Guo", "Yuanbin Man", "Junyuan Sheng", "Bowen Lin", "Ahmed Ahmed", "Bo Jiang", "Boyuan Zhang", "Miao Yin", "Sian Jin", "Omprakash Gnawal", "Chengming Zhang"], "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams", "comment": null, "summary": "Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.", "AI": {"tldr": "Event-VStream\uff1a\u4e00\u4e2a\u4e8b\u4ef6\u611f\u77e5\u7684\u89c6\u9891\u6d41\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u8bed\u4e49\u8fde\u8d2f\u7684\u4e8b\u4ef6\u8fb9\u754c\u6765\u89e6\u53d1\u8bed\u8a00\u751f\u6210\uff0c\u51cf\u5c11\u5197\u4f59\u5904\u7406\u5e76\u4fdd\u6301\u957f\u671f\u8bb0\u5fc6", "motivation": "\u73b0\u6709\u89c6\u9891\u6d41\u7406\u89e3\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u56fa\u5b9a\u95f4\u9694\u89e3\u7801\u5bfc\u81f4\u91cd\u590d\u8f93\u51fa\uff1b2\uff09\u7f13\u5b58\u526a\u679d\u4e22\u5f03\u5173\u952e\u65f6\u5e8f\u4fe1\u606f\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u6d41\u65f6\u9762\u4e34\u5197\u4f59\u5e27\u5904\u7406\u548c\u5feb\u901f\u9057\u5fd8\u8fc7\u53bb\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u611f\u77e5\u6846\u67b6Event-VStream\uff0c\u5c06\u8fde\u7eed\u89c6\u9891\u8868\u793a\u4e3a\u79bb\u6563\u7684\u8bed\u4e49\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u3002\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u8fd0\u52a8\u3001\u8bed\u4e49\u548c\u9884\u6d4b\u7ebf\u7d22\u6765\u68c0\u6d4b\u6709\u610f\u4e49\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u4ec5\u5728\u4e8b\u4ef6\u8fb9\u754c\u89e6\u53d1\u8bed\u8a00\u751f\u6210\u3002\u6bcf\u4e2a\u4e8b\u4ef6\u5d4c\u5165\u88ab\u6574\u5408\u5230\u6301\u4e45\u5185\u5b58\u5e93\u4e2d\uff0c\u652f\u6301\u957f\u671f\u63a8\u7406\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728OVOBench-Realtime\u548c\u957f\u683c\u5f0fEgo4D\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u76f8\u6bd4VideoLLM-Online-8B\u57fa\u7ebf\u63d0\u534710.4\u5206\uff1b\u4f7f\u7528\u901a\u7528LLaMA-3-8B\u6587\u672c\u9aa8\u5e72\u5373\u53ef\u63a5\u8fd1Flash-VStream-7B\u6027\u80fd\uff1b\u57282\u5c0f\u65f6Ego4D\u6d41\u4e0a\u4fdd\u6301\u7ea670%\u7684GPT-5\u80dc\u7387\u3002", "conclusion": "Event-VStream\u901a\u8fc7\u4e8b\u4ef6\u611f\u77e5\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u6d41\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u5904\u7406\u548c\u8bb0\u5fc6\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u957f\u671f\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.15544", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15544", "abs": "https://arxiv.org/abs/2601.15544", "authors": ["Himanshu Mishra"], "title": "RDumb++: Drift-Aware Continual Test-Time Adaptation", "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.", "AI": {"tldr": "RDumb++\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff08\u57fa\u4e8e\u71b5\u548cKL\u6563\u5ea6\uff09\u548c\u81ea\u9002\u5e94\u91cd\u7f6e\u7b56\u7565\uff0c\u5728\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728CCC\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u7ea63%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff08\u5982Tent\u3001EATA\u7b49\uff09\u5728\u77ed\u671f\u5206\u5e03\u6f02\u79fb\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5feb\u901f\u53d8\u5316\u6216\u6781\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u6d4b\u8bd5\u5206\u5e03\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002CCC\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b750\u4e07\u4e2a\u6837\u672c\u7684\u8fde\u7eed\u6d41\uff0c\u5176\u4e2d\u8150\u8d25\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u4e0d\u65ad\u53d8\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u957f\u671f\u6311\u6218\u3002", "method": "RDumb++\u662fRDumb\u7684\u6269\u5c55\uff0c\u5f15\u5165\u4e86\u4e24\u79cd\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff1a1\uff09\u57fa\u4e8e\u71b5\u7684\u6f02\u79fb\u8bc4\u5206\uff0c2\uff09\u57fa\u4e8eKL\u6563\u5ea6\u7684\u6f02\u79fb\u8bc4\u5206\u3002\u7ed3\u5408\u81ea\u9002\u5e94\u91cd\u7f6e\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u68c0\u6d4b\u5230\u7d2f\u79ef\u9002\u5e94\u4f55\u65f6\u53d8\u5f97\u6709\u5bb3\uff0c\u5e76\u5728\u9884\u6d4b\u5d29\u6e83\u53d1\u751f\u524d\u6062\u590d\u3002", "result": "\u5728CCC-medium\u57fa\u51c6\u6d4b\u8bd5\uff08\u4e09\u79cd\u901f\u5ea6\u3001\u4e09\u79cd\u79cd\u5b50\uff0c\u51719\u6b21\u8fd0\u884c\uff0c\u6bcf\u6b21\u5305\u542b100\u4e07\u4e2a\u6837\u672c\uff09\u4e0a\uff0cRDumb++\u59cb\u7ec8\u4f18\u4e8eRDumb\uff0c\u83b7\u5f97\u7ea63%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u6574\u4e2a\u6d41\u4e2d\u4fdd\u6301\u7a33\u5b9a\u7684\u9002\u5e94\u6027\u80fd\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8868\u660e\u6f02\u79fb\u611f\u77e5\u91cd\u7f6e\u5bf9\u4e8e\u9632\u6b62\u5d29\u6e83\u548c\u5b9e\u73b0\u53ef\u9760\u7684\u957f\u671fCTTA\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\u548c\u81ea\u9002\u5e94\u91cd\u7f6e\u7b56\u7565\u662f\u89e3\u51b3\u957f\u671f\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u95ee\u9898\u7684\u5173\u952e\u3002RDumb++\u901a\u8fc7\u68c0\u6d4b\u6709\u5bb3\u7684\u7d2f\u79ef\u9002\u5e94\u5e76\u5728\u5d29\u6e83\u524d\u6062\u590d\uff0c\u5728\u6781\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u5206\u5e03\u6f02\u79fb\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u53ef\u9760\u7684\u6027\u80fd\u3002"}}
{"id": "2601.15547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15547", "abs": "https://arxiv.org/abs/2601.15547", "authors": ["Jingren Hou", "Hong Wang", "Pengyu Xu", "Chang Gao", "Huafeng Liu", "Liping Jing"], "title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling", "comment": null, "summary": "Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \\ours achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4ece\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u795e\u7ecf\u7b97\u5b50\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u8bad\u7ec3\u548c\u7269\u7406\u611f\u77e5\u6f5c\u5728\u4f20\u64ad\u5668\u89e3\u51b3\u90e8\u5206\u89c2\u6d4b\u95ee\u9898\uff0c\u5728\u591a\u79cdPDE\u4efb\u52a1\u4e0a\u5b9e\u73b018-69%\u8bef\u5dee\u964d\u4f4e\u3002", "motivation": "\u73b0\u5b9e\u79d1\u5b66\u5e94\u7528\u4e2d\u5e38\u9047\u5230\u4e0d\u5b8c\u6574\u7684\u89c2\u6d4b\u6570\u636e\uff08\u4f20\u611f\u5668\u9650\u5236\u3001\u5730\u7406\u7ea6\u675f\u3001\u6d4b\u91cf\u6210\u672c\uff09\uff0c\u800c\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5047\u8bbe\u5b8c\u5168\u89c2\u6d4b\u7a7a\u95f4\u8f93\u5165\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faLatent Autoregressive Neural Operator (LARNO)\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u63a9\u7801\u9884\u6d4b\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u63a9\u7801\u89c2\u6d4b\u533a\u57df\u521b\u5efa\u4eba\u5de5\u76d1\u7763\uff1b2) \u7269\u7406\u611f\u77e5\u6f5c\u5728\u4f20\u64ad\u5668\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u8fb9\u754c\u4f18\u5148\u81ea\u56de\u5f52\u751f\u6210\u91cd\u5efa\u89e3\u3002", "result": "\u5728POBench-PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLARNO\u5728\u7f3a\u5931\u7387\u4f4e\u4e8e50%\u7684\u8865\u4e01\u5f0f\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u57fa\u51c6\u4e0a\u5b9e\u73b018-69%\u7684\u76f8\u5bf9L2\u8bef\u5dee\u964d\u4f4e\uff0c\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u6c14\u5019\u9884\u6d4b\uff0c\u80fd\u6709\u6548\u5904\u7406\u9ad8\u8fbe75%\u7f3a\u5931\u7387\u7684\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u89e3\u51b3\u4e86\u4ece\u90e8\u5206\u89c2\u6d4b\u5b66\u4e60\u795e\u7ecf\u7b97\u5b50\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6f5c\u5728\u7a7a\u95f4\u4f20\u64ad\u673a\u5236\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5f25\u5408\u4e86\u7406\u60f3\u5316\u7814\u7a76\u8bbe\u7f6e\u4e0e\u73b0\u5b9e\u4e16\u754c\u79d1\u5b66\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2601.15703", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15703", "abs": "https://arxiv.org/abs/2601.15703", "authors": ["Jiaxin Zhang", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Uncertainty Quantification", "comment": "36 pages, 9 figures, 9 tables", "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u8fc7\u7a0b\u4ee3\u7406\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u5316\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u4e3b\u52a8\u63a7\u5236\u4fe1\u53f7\uff0c\u89e3\u51b3AI\u4ee3\u7406\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u7684\"\u5e7b\u89c9\u87ba\u65cb\"\u95ee\u9898", "motivation": "AI\u4ee3\u7406\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u9762\u4e34\"\u5e7b\u89c9\u87ba\u65cb\"\u95ee\u9898\uff0c\u65e9\u671f\u8ba4\u77e5\u9519\u8bef\u4f1a\u4e0d\u53ef\u9006\u4f20\u64ad\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u53ea\u80fd\u88ab\u52a8\u8bca\u65ad\u98ce\u9669\u800c\u4e0d\u89e3\u51b3\u95ee\u9898\uff0c\u81ea\u6211\u53cd\u601d\u673a\u5236\u5219\u5b58\u5728\u8fde\u7eed\u6216\u6f2b\u65e0\u76ee\u7684\u7684\u4fee\u6b63\u95ee\u9898", "method": "\u63d0\u51fa\u53cc\u8fc7\u7a0b\u4ee3\u7406\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u673a\u5236\uff1a\u7cfb\u7edf1\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bb0\u5fc6\uff09\u9690\u5f0f\u4f20\u64ad\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u548c\u8bed\u4e49\u89e3\u91ca\u4ee5\u9632\u6b62\u76f2\u76ee\u51b3\u7b56\uff1b\u7cfb\u7edf2\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53cd\u601d\uff09\u5229\u7528\u8fd9\u4e9b\u89e3\u91ca\u4f5c\u4e3a\u7406\u6027\u7ebf\u7d22\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u89e6\u53d1\u6709\u9488\u5bf9\u6027\u7684\u63a8\u7406\u65f6\u89e3\u6790", "result": "\u5728\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8f68\u8ff9\u7ea7\u6821\u51c6", "conclusion": "\u8be5\u539f\u5219\u6027\u6846\u67b6\u4ee3\u8868\u4e86\u5411\u53ef\u9760\u4ee3\u7406\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u52a8\u6001\u5e73\u8861\u9ad8\u6548\u6267\u884c\u548c\u6df1\u5ea6\u601d\u8003"}}
{"id": "2601.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15761", "abs": "https://arxiv.org/abs/2601.15761", "authors": ["Xiefeng Wu", "Mingyu Hu", "Shu Zhang"], "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning", "comment": "7 pages main text 2 page reference", "summary": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.", "AI": {"tldr": "SigEnt-SAC\u662f\u4e00\u79cd\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u7684\u79bb\u7b56\u7565actor-critic\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\uff0c\u901a\u8fc7sigmoid\u6709\u754c\u71b5\u9879\u9632\u6b62\u8d1f\u71b5\u9a71\u52a8\u7684\u4f18\u5316\uff0c\u51cf\u5c11Q\u51fd\u6570\u632f\u8361\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4f4e\u6210\u672c\u5f3a\u5316\u5b66\u4e60\u90e8\u7f72\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u3001\u5956\u52b1\u7a00\u758f\u548c\u89c6\u89c9\u89c2\u6d4b\u566a\u58f0\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u6216\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u4f4e\u6210\u672c\u3001\u6570\u636e\u9700\u6c42\u5c11\u7684\u5b9e\u7528\u65b9\u6848\u3002", "method": "\u63d0\u51faSigEnt-SAC\u65b9\u6cd5\uff0c\u6838\u5fc3\u8bbe\u8ba1\u662fsigmoid\u6709\u754c\u71b5\u9879\uff0c\u9632\u6b62\u8d1f\u71b5\u9a71\u52a8\u7684\u4f18\u5316\u5bfc\u81f4\u5206\u5e03\u5916\u52a8\u4f5c\uff0c\u51cf\u5c11Q\u51fd\u6570\u632f\u8361\u3002\u4ec5\u9700\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u7f13\u89e3Q\u51fd\u6570\u632f\u8361\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u5feb\u8fbe\u5230100%\u6210\u529f\u7387\u3002\u5728\u56db\u79cd\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u5c31\u80fd\u5b66\u4e60\u6210\u529f\u7b56\u7565\u3002", "conclusion": "SigEnt-SAC\u4e3a\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u5b9e\u7528\u7684\u9014\u5f84\uff0c\u4ec5\u9700\u6700\u5c0f\u6570\u636e\u9700\u6c42\u5c31\u80fd\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b66\u4e60\u6210\u529f\u7b56\u7565\u3002"}}
{"id": "2601.15871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15871", "abs": "https://arxiv.org/abs/2601.15871", "authors": ["Jidong Jin"], "title": "Why Inference in Large Models Becomes Decomposable After Training", "comment": "10 pages, 6 figures", "summary": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540e\u8bad\u7ec3\u7ed3\u6784\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u51c6\u5219\u548c\u7ed3\u6784\u9000\u706b\u53bb\u9664\u65e0\u7528\u7684\u53c2\u6570\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u7ed3\u6784\u5316\u5e76\u884c\u63a8\u7406\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u529f\u80fd\u6216\u63a5\u53e3\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u63a8\u7406\u901a\u5e38\u5728\u5bc6\u96c6\u53c2\u6570\u77e9\u9635\u4e0a\u8fdb\u884c\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u548c\u7cfb\u7edf\u590d\u6742\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u4e0d\u53ef\u6301\u7eed\u5730\u589e\u957f\u3002\u8fd9\u79cd\u9650\u5236\u5e76\u975e\u6e90\u4e8e\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\uff0c\u800c\u662f\u56e0\u4e3a\u5c06\u540e\u8bad\u7ec3\u63a8\u7406\u7cfb\u7edf\u89c6\u4e3a\u5355\u4e00\u6574\u4f53\u7b97\u5b50\uff0c\u5ffd\u7565\u4e86\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u5185\u90e8\u7ed3\u6784\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u66f4\u65b0\u4e8b\u4ef6\u9ad8\u5ea6\u5c40\u90e8\u5316\u548c\u9009\u62e9\u6027\uff0c\u8bb8\u591a\u53c2\u6570\u4f9d\u8d56\u5728\u8bad\u7ec3\u540e\u4e0e\u5176\u521d\u59cb\u5316\u5206\u5e03\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u63d0\u51fa\u540e\u8bad\u7ec3\u7edf\u8ba1\u51c6\u5219\u548c\u7ed3\u6784\u9000\u706b\u7a0b\u5e8f\uff0c\u79fb\u9664\u65e0\u652f\u6301\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u63ed\u793a\u7a33\u5b9a\u3001\u72ec\u7acb\u7684\u5b50\u7ed3\u6784\u3002", "result": "\u5efa\u7acb\u4e86\u540e\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u63a8\u7406\u7cfb\u7edf\u7ed3\u6784\u89c6\u56fe\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u5e76\u884c\u63a8\u7406\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u529f\u80fd\u6216\u63a5\u53e3\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u5927\u578bAI\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u53ef\u5206\u89e3\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u964d\u4f4e\u63a8\u7406\u6210\u672c\u548c\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.16034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16034", "abs": "https://arxiv.org/abs/2601.16034", "authors": ["Tony Cristofano"], "title": "Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction", "comment": null, "summary": "Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTrajectory Replay via Concept-Basis Reconstruction\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u5ff5\u6307\u7eb9\u5bf9\u9f50\u548c\u6982\u5ff5\u539f\u5b50\u91cd\u6784\uff0c\u5c06\u62d2\u7edd\u5e72\u9884\u4ece\u6350\u8d60\u6a21\u578b\u8f6c\u79fb\u5230\u76ee\u6807\u6a21\u578b\uff0c\u8bc1\u660e\u62d2\u7edd\u884c\u4e3a\u6e90\u4e8e\u8de8\u6a21\u578b\u7684\u901a\u7528\u4f4e\u7ef4\u8bed\u4e49\u7535\u8def\u3002", "motivation": "\u5bf9\u9f50LLM\u4e2d\u7684\u62d2\u7edd\u884c\u4e3a\u901a\u5e38\u88ab\u89c6\u4e3a\u6a21\u578b\u7279\u5b9a\u7684\uff0c\u4f46\u4f5c\u8005\u5047\u8bbe\u5b83\u6e90\u4e8e\u8de8\u6a21\u578b\u5171\u4eab\u7684\u901a\u7528\u4f4e\u7ef4\u8bed\u4e49\u7535\u8def\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u67b6\u6784\u548c\u8bad\u7ec3\u673a\u5236\u4e4b\u95f4\u8f6c\u79fb\u62d2\u7edd\u5e72\u9884\u3002", "method": "\u63d0\u51faTrajectory Replay via Concept-Basis Reconstruction\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u6982\u5ff5\u6307\u7eb9\u5bf9\u9f50\u4e0d\u540c\u6a21\u578b\u7684\u5c42\uff1b2\uff09\u4f7f\u7528\u5171\u4eab\u7684\"\u6982\u5ff5\u539f\u5b50\"\u91cd\u6784\u62d2\u7edd\u65b9\u5411\uff1b3\uff09\u901a\u8fc7\u6743\u91cdSVD\u7a33\u5b9a\u6027\u4fdd\u62a4\u9632\u6b62\u80fd\u529b\u9000\u5316\uff0c\u5c06\u5e72\u9884\u6295\u5f71\u5230\u4f4e\u65b9\u5dee\u6743\u91cd\u5b50\u7a7a\u95f4\u3002", "result": "\u57288\u4e2a\u6a21\u578b\u5bf9\uff08\u5305\u62ecGPT-OSS-20B\u548cGLM-4\uff09\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8f6c\u79fb\u7684\"\u914d\u65b9\"\u80fd\u6301\u7eed\u51cf\u5f31\u62d2\u7edd\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u5bf9\u9f50\u7684\u8bed\u4e49\u901a\u7528\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\u3002", "conclusion": "\u62d2\u7edd\u884c\u4e3a\u6e90\u4e8e\u8de8\u4e0d\u540c\u67b6\u6784\u548c\u8bad\u7ec3\u673a\u5236\u7684LLM\u5171\u4eab\u7684\u901a\u7528\u4f4e\u7ef4\u8bed\u4e49\u7535\u8def\uff0c\u8fd9\u4e3a\u7406\u89e3\u5b89\u5168\u5bf9\u9f50\u7684\u8bed\u4e49\u57fa\u7840\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u578b\u5e72\u9884\u8f6c\u79fb\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.16074", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16074", "abs": "https://arxiv.org/abs/2601.16074", "authors": ["Annemarie Jutte", "Uraz Odyurt"], "title": "Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems", "comment": null, "summary": "Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.", "AI": {"tldr": "\u5c06XAI\u5e94\u7528\u4e8e\u5de5\u4e1aCPS\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7SHAP\u503c\u5206\u6790\u6570\u636e\u5206\u89e3\u7ec4\u4ef6\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\uff0c\u901a\u8fc7\u589e\u52a0\u7a97\u53e3\u5927\u5c0f\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u5de5\u4e1aCPS\u5bf9\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u8981\u6c42\u6781\u9ad8\uff0c\u4f46\u96c6\u6210\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u4e0d\u900f\u660e\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e25\u683c\u8bc4\u4f30\u4ee5\u9632\u6b62\u5728\u672a\u89c1\u6570\u636e\u4e0a\u51fa\u73b0\u610f\u5916\u884c\u4e3a", "method": "\u5e94\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7279\u522b\u662fSHAP\u503c\u5206\u6790\uff0c\u7814\u7a76\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5206\u89e3\u7ec4\u4ef6\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u57fa\u4e8eXAI\u53d1\u73b0\u589e\u52a0\u6570\u636e\u5b9e\u4f8b\u7684\u7a97\u53e3\u5927\u5c0f", "result": "XAI\u5206\u6790\u663e\u793a\u6a21\u578b\u8bad\u7ec3\u7f3a\u4e4f\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u589e\u52a0\u7a97\u53e3\u5927\u5c0f\u80fd\u591f\u6539\u5584\u6a21\u578b\u6027\u80fd", "conclusion": "XAI\u4e0d\u4ec5\u80fd\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\uff0c\u8fd8\u80fd\u6307\u5bfc\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1\uff0c\u901a\u8fc7\u589e\u52a0\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u63d0\u5347\u5de5\u4e1aCPS\u4e2dML\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd"}}
{"id": "2601.16108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16108", "abs": "https://arxiv.org/abs/2601.16108", "authors": ["Marzieh Adeli Shamsabad", "Hamed Ghodrati"], "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources", "comment": null, "summary": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u6c14\u5019\u865a\u5047\u4fe1\u606f\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u5305\u542b\u7684\u6700\u65b0\u4e8b\u4ef6\u548c\u8bef\u5bfc\u6027\u56fe\u50cf\u3002", "motivation": "\u6c14\u5019\u865a\u5047\u4fe1\u606f\u5728\u6570\u5b57\u4e16\u754c\u4e2d\u65e5\u76ca\u4e25\u91cd\uff0c\u7279\u522b\u662f\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f20\u64ad\u7684\u8bef\u5bfc\u6027\u56fe\u50cf\u548c\u89c6\u9891\u3002\u8fd9\u4e9b\u865a\u5047\u4fe1\u606f\u5f80\u5f80\u96be\u4ee5\u68c0\u6d4b\uff0c\u53ef\u80fd\u5ef6\u8fdf\u6c14\u5019\u884c\u52a8\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u65f6\u7684\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5904\u7406\u6700\u65b0\u4e8b\u4ef6\u6216\u66f4\u65b0\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u7cfb\u7edf\u53ef\u4ee5\u83b7\u53d6\u6700\u65b0\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u7ed3\u679c\u3001\u5728\u7ebf\u4e8b\u5b9e\u6838\u67e5\u548c\u53ef\u4fe1\u4e13\u5bb6\u5185\u5bb9\u3002\u8fd9\u4e9b\u5916\u90e8\u77e5\u8bc6\u5e2e\u52a9\u7cfb\u7edf\u66f4\u597d\u5730\u8bc4\u4f30\u56fe\u50cf\u53ca\u5176\u58f0\u660e\u7684\u51c6\u786e\u6027\uff0c\u5224\u65ad\u5176\u662f\u5426\u4e3a\u51c6\u786e\u3001\u8bef\u5bfc\u3001\u865a\u5047\u6216\u65e0\u6cd5\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6c14\u5019\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u8bef\u5bfc\u6027\u56fe\u50cf\u548c\u89c6\u9891\u3002\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u6700\u65b0\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u5e94\u5bf9\u5feb\u901f\u53d8\u5316\u4fe1\u606f\u73af\u5883\u7684\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u4f20\u7edf\u6a21\u578b\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u77e5\u8bc6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u6c14\u5019\u865a\u5047\u4fe1\u606f\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u6709\u52a9\u4e8e\u4fdd\u62a4\u516c\u4f17\u5bf9\u79d1\u5b66\u7684\u7406\u89e3\uff0c\u5e94\u5bf9\u5feb\u901f\u53d8\u5316\u7684\u4fe1\u606f\u73af\u5883\u6311\u6218\u3002"}}
