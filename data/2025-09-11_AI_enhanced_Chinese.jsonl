{"id": "2509.08003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08003", "abs": "https://arxiv.org/abs/2509.08003", "authors": ["Shahid Shafi Dar", "Bharat Kaurav", "Arnav Jain", "Chandravardhan Singh Raghaw", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities", "comment": null, "summary": "In an era of escalating climate change, urban flooding has emerged as a\ncritical challenge for sustainable cities, threatening lives, infrastructure,\nand ecosystems. Traditional flood detection methods are constrained by their\nreliance on unimodal data and static rule-based systems, which fail to capture\nthe dynamic, non-linear relationships inherent in flood events. Furthermore,\nexisting attention mechanisms and ensemble learning approaches exhibit\nlimitations in hierarchical refinement, cross-modal feature integration, and\nadaptability to noisy or unstructured environments, resulting in suboptimal\nflood classification performance. To address these challenges, we present\nXFloodNet, a novel framework that redefines urban flood classification through\nadvanced deep-learning techniques. XFloodNet integrates three novel components:\n(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically\naligns visual and textual features, enabling precise multi-granularity\ninteractions and resolving contextual ambiguities; (2) a Heterogeneous\nConvolutional Adaptive Multi-Scale Attention module, which leverages\nfrequency-enhanced channel attention and frequency-modulated spatial attention\nto extract and prioritize discriminative flood-related features across spectral\nand spatial domains; and (3) a Cascading Convolutional Transformer Feature\nRefinement technique that harmonizes hierarchical features through adaptive\nscaling and cascading operations, ensuring robust and noise-resistant flood\ndetection. We evaluate our proposed method on three benchmark datasets, such as\nChennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves\nstate-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,\nsurpassing existing methods by significant margins.", "AI": {"tldr": "XFloodNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u57ce\u5e02\u6d2a\u6c34\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6d2a\u6c34\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\u548c\u9759\u6001\u89c4\u5219\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6355\u6349\u6d2a\u6c34\u4e8b\u4ef6\u7684\u52a8\u6001\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u5c42\u6b21\u7ec6\u5316\u3001\u8de8\u6a21\u6001\u7279\u5f81\u96c6\u6210\u548c\u5bf9\u566a\u58f0\u73af\u5883\u7684\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faXFloodNet\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5c42\u6b21\u5316\u8de8\u6a21\u6001\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u3001\u5f02\u6784\u5377\u79ef\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\u3001\u4ee5\u53ca\u7ea7\u8054\u5377\u79efTransformer\u7279\u5f81\u7ec6\u5316\u6280\u672f\u3002", "result": "\u5728Chennai Floods\u3001Rhine18 Floods\u548cHarz17 Floods\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523093.33%\u300182.24%\u548c88.60%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "XFloodNet\u901a\u8fc7\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u57ce\u5e02\u6d2a\u6c34\u5206\u7c7b\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u57ce\u5e02\u6d2a\u6c34\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08000", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08000", "abs": "https://arxiv.org/abs/2509.08000", "authors": ["Debdeep Sanyal", "Manodeep Ray", "Murari Mandal"], "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs", "comment": "19 pages", "summary": "The release of open-weight large language models (LLMs) creates a tension\nbetween advancing accessible research and preventing misuse, such as malicious\nfine-tuning to elicit harmful content. Current safety measures struggle to\npreserve the general capabilities of the LLM while resisting a determined\nadversary with full access to the model's weights and architecture, who can use\nfull-parameter fine-tuning to erase existing safeguards. To address this, we\nintroduce AntiDote, a bi-level optimization procedure for training LLMs to be\nresistant to such tampering. AntiDote involves an auxiliary adversary\nhypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)\nweights conditioned on the defender model's internal activations. The defender\nLLM is then trained with an objective to nullify the effect of these\nadversarial weight additions, forcing it to maintain its safety alignment. We\nvalidate this approach against a diverse suite of 52 red-teaming attacks,\nincluding jailbreak prompting, latent space manipulation, and direct\nweight-space attacks. AntiDote is upto 27.4\\% more robust against adversarial\nattacks compared to both tamper-resistance and unlearning baselines. Crucially,\nthis robustness is achieved with a minimal trade-off in utility, incurring a\nperformance degradation of upto less than 0.5\\% across capability benchmarks\nincluding MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute\nefficient methodology for building open-weight models where safety is a more\nintegral and resilient property.", "AI": {"tldr": "AntiDote\u662f\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\u8bad\u7ec3LLM\u62b5\u6297\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u5f00\u6e90\u6743\u91cdLLM\u9762\u4e34\u6076\u610f\u5fae\u8c03\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u96be\u4ee5\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u62b5\u6297\u62e5\u6709\u5b8c\u6574\u6a21\u578b\u8bbf\u95ee\u6743\u9650\u7684\u5bf9\u624b", "method": "\u4f7f\u7528\u8f85\u52a9\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\u751f\u6210\u6076\u610fLoRA\u6743\u91cd\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u8bad\u7ec3\u9632\u5fa1\u6a21\u578b\u6765\u62b5\u6d88\u8fd9\u4e9b\u5bf9\u6297\u6027\u6743\u91cd\u7684\u5f71\u54cd\uff0c\u4fdd\u6301\u5b89\u5168\u5bf9\u9f50", "result": "\u572852\u79cd\u7ea2\u961f\u653b\u51fb\u6d4b\u8bd5\u4e2d\uff0cAntiDote\u6bd4\u57fa\u51c6\u65b9\u6cd5\u5f3a27.4%\uff0c\u5728MMLU\u7b49\u80fd\u529b\u57fa\u51c6\u4e0a\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e0.5%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u5b89\u5168\u6027\u66f4\u5f3a\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848"}}
{"id": "2509.08016", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08016", "abs": "https://arxiv.org/abs/2509.08016", "authors": ["Hyungjin Chung", "Hyelin Nam", "Jiyeon Kim", "Hyojun Go", "Byeongjun Park", "Junho Kim", "Joonseok Lee", "Seongsu Ha", "Byung-Hoon Kim"], "title": "Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs", "comment": "https://github.com/hyungjin-chung/VPS", "summary": "Video Large Language Models (VideoLLMs) face a critical bottleneck:\nincreasing the number of input frames to capture fine-grained temporal detail\nleads to prohibitive computational costs and performance degradation from long\ncontext lengths. We introduce Video Parallel Scaling (VPS), an inference-time\nmethod that expands a model's perceptual bandwidth without increasing its\ncontext window. VPS operates by running multiple parallel inference streams,\neach processing a unique, disjoint subset of the video's frames. By aggregating\nthe output probabilities from these complementary streams, VPS integrates a\nricher set of visual information than is possible with a single pass. We\ntheoretically show that this approach effectively contracts the Chinchilla\nscaling law by leveraging uncorrelated visual evidence, thereby improving\nperformance without additional training. Extensive experiments across various\nmodel architectures and scales (2B-32B) on benchmarks such as Video-MME and\nEventHallusion demonstrate that VPS consistently and significantly improves\nperformance. It scales more favorably than other parallel alternatives (e.g.\nSelf-consistency) and is complementary to other decoding strategies, offering a\nmemory-efficient and robust framework for enhancing the temporal reasoning\ncapabilities of VideoLLMs.", "AI": {"tldr": "Video Parallel Scaling (VPS) \u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u89c6\u9891\u5e27\u5b50\u96c6\u6765\u6269\u5c55VideoLLMs\u7684\u611f\u77e5\u5e26\u5bbd\uff0c\u907f\u514d\u957f\u4e0a\u4e0b\u6587\u5e26\u6765\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "VideoLLMs\u5728\u5904\u7406\u66f4\u591a\u8f93\u5165\u5e27\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7ec6\u8282\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u548c\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u4e0b\u964d\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "VPS\u901a\u8fc7\u8fd0\u884c\u591a\u4e2a\u5e76\u884c\u63a8\u7406\u6d41\uff0c\u6bcf\u4e2a\u6d41\u5904\u7406\u89c6\u9891\u5e27\u7684\u4e0d\u540c\u5b50\u96c6\uff0c\u7136\u540e\u805a\u5408\u8fd9\u4e9b\u4e92\u8865\u6d41\u7684\u8f93\u51fa\u6982\u7387\u6765\u6574\u5408\u66f4\u4e30\u5bcc\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728Video-MME\u548cEventHallusion\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVPS\u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\uff082B-32B\uff09\u4e0a\u5747\u80fd\u4e00\u81f4\u4e14\u663e\u8457\u5730\u63d0\u5347\u6027\u80fd\uff0c\u6bd4Self-consistency\u7b49\u5176\u4ed6\u5e76\u884c\u65b9\u6cd5\u66f4\u5177\u6269\u5c55\u4f18\u52bf\u3002", "conclusion": "VPS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u53ef\u589e\u5f3aVideoLLMs\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u4e0e\u5176\u4ed6\u89e3\u7801\u7b56\u7565\u4e92\u8865\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6539\u5584\u6027\u80fd\u3002"}}
{"id": "2509.08146", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08146", "abs": "https://arxiv.org/abs/2509.08146", "authors": ["Nivedha Sivakumar", "Natalie Mackraz", "Samira Khorshidi", "Krishna Patel", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Bias after Prompting: Persistent Discrimination in Large Language Models", "comment": null, "summary": "A dangerous assumption that can be made from prior work on the bias transfer\nhypothesis (BTH) is that biases do not transfer from pre-trained large language\nmodels (LLMs) to adapted models. We invalidate this assumption by studying the\nBTH in causal models under prompt adaptations, as prompting is an extremely\npopular and accessible adaptation strategy used in real-world applications. In\ncontrast to prior work, we find that biases can transfer through prompting and\nthat popular prompt-based mitigation methods do not consistently prevent biases\nfrom transferring. Specifically, the correlation between intrinsic biases and\nthose after prompt adaptation remain moderate to strong across demographics and\ntasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age\n(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we\nfind that biases remain strongly correlated when varying few-shot composition\nparameters, such as sample size, stereotypical content, occupational\ndistribution and representational balance (rho >= 0.90). We evaluate several\nprompt-based debiasing strategies and find that different approaches have\ndistinct strengths, but none consistently reduce bias transfer across models,\ntasks or demographics. These results demonstrate that correcting bias, and\npotentially improving reasoning ability, in intrinsic models may prevent\npropagation of biases to downstream tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u4ece\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u79fb\u5230\u9002\u5e94\u6a21\u578b\uff0c\u73b0\u6709\u63d0\u793a\u53bb\u504f\u65b9\u6cd5\u65e0\u6cd5\u4e00\u81f4\u9632\u6b62\u504f\u89c1\u8f6c\u79fb\uff0c\u7ea0\u6b63\u5185\u5728\u6a21\u578b\u504f\u89c1\u53ef\u80fd\u9632\u6b62\u504f\u89c1\u4f20\u64ad\u5230\u4e0b\u6e38\u4efb\u52a1", "motivation": "\u5148\u524d\u5173\u4e8e\u504f\u89c1\u8f6c\u79fb\u5047\u8bbe\u7684\u7814\u7a76\u53ef\u80fd\u9519\u8bef\u5730\u8ba4\u4e3a\u504f\u89c1\u4e0d\u4f1a\u4ece\u9884\u8bad\u7ec3LLM\u8f6c\u79fb\u5230\u9002\u5e94\u6a21\u578b\uff0c\u9700\u8981\u9a8c\u8bc1\u5728\u63d0\u793a\u9002\u5e94\u573a\u666f\u4e0b\u7684\u504f\u89c1\u8f6c\u79fb\u60c5\u51b5", "method": "\u7814\u7a76\u56e0\u679c\u6a21\u578b\u5728\u63d0\u793a\u9002\u5e94\u4e0b\u7684\u504f\u89c1\u8f6c\u79fb\uff0c\u8bc4\u4f30\u4e0d\u540c\u5c11\u6837\u672c\u7ec4\u5408\u53c2\u6570\uff08\u6837\u672c\u5927\u5c0f\u3001\u523b\u677f\u5185\u5bb9\u3001\u804c\u4e1a\u5206\u5e03\u7b49\uff09\u7684\u5f71\u54cd\uff0c\u6d4b\u8bd5\u591a\u79cd\u63d0\u793a\u53bb\u504f\u7b56\u7565", "result": "\u504f\u89c1\u901a\u8fc7\u63d0\u793a\u8f6c\u79fb\u7684\u76f8\u5173\u6027\u5f88\u5f3a\uff08\u6027\u522b\u03c1\u22650.94\uff0c\u5e74\u9f84\u03c1\u22650.98\uff0c\u5b97\u6559\u03c1\u22650.69\uff09\uff0c\u6539\u53d8\u5c11\u6837\u672c\u53c2\u6570\u540e\u504f\u89c1\u4ecd\u5f3a\u76f8\u5173\uff08\u03c1\u22650.90\uff09\uff0c\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u65e0\u6cd5\u4e00\u81f4\u51cf\u5c11\u504f\u89c1\u8f6c\u79fb", "conclusion": "\u7ea0\u6b63\u5185\u5728\u6a21\u578b\u7684\u504f\u89c1\u53ef\u80fd\u9632\u6b62\u504f\u89c1\u4f20\u64ad\u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5"}}
{"id": "2509.08140", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08140", "abs": "https://arxiv.org/abs/2509.08140", "authors": ["Mihir Kumar", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Afriyie Kwesi Samuel", "Fuat Alican", "Yigit Ihlamur"], "title": "From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital", "comment": "6 pages, 3 figures", "summary": "This paper presents a framework for predicting rare, high-impact outcomes by\nintegrating large language models (LLMs) with a multi-model machine learning\n(ML) architecture. The approach combines the predictive strength of black-box\nmodels with the interpretability required for reliable decision-making. We use\nLLM-powered feature engineering to extract and synthesize complex signals from\nunstructured data, which are then processed within a layered ensemble of models\nincluding XGBoost, Random Forest, and Linear Regression. The ensemble first\nproduces a continuous estimate of success likelihood, which is then thresholded\nto produce a binary rare-event prediction. We apply this framework to the\ndomain of Venture Capital (VC), where investors must evaluate startups with\nlimited and noisy early-stage data. The empirical results show strong\nperformance: the model achieves precision between 9.8X and 11.1X the random\nclassifier baseline in three independent test subsets. Feature sensitivity\nanalysis further reveals interpretable success drivers: the startup's category\nlist accounts for 15.6% of predictive influence, followed by the number of\nfounders, while education level and domain expertise contribute smaller yet\nconsistent effects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u578b\u673a\u5668\u5b66\u4e60\u67b6\u6784\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u7f55\u89c1\u9ad8\u5f71\u54cd\u4e8b\u4ef6\uff0c\u5728\u98ce\u9669\u6295\u8d44\u9886\u57df\u5b9e\u73b0\u4e869.8-11.1\u500d\u4e8e\u968f\u673a\u57fa\u7ebf\u7684\u7cbe\u786e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u98ce\u9669\u6295\u8d44\u4e2d\u65e9\u671f\u9636\u6bb5\u6570\u636e\u6709\u9650\u4e14\u5608\u6742\u7684\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4b\u521d\u521b\u516c\u53f8\u6210\u529f\u6982\u7387\u7684\u6311\u6218\uff0c\u9700\u8981\u7ed3\u5408\u9ed1\u76d2\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u51b3\u7b56\u9700\u6c42\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u7279\u5f81\u5de5\u7a0b\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u63d0\u53d6\u590d\u6742\u4fe1\u53f7\uff0c\u6784\u5efa\u5305\u542bXGBoost\u3001\u968f\u673a\u68ee\u6797\u548c\u7ebf\u6027\u56de\u5f52\u7684\u5206\u5c42\u96c6\u6210\u6a21\u578b\uff0c\u9996\u5148\u751f\u6210\u8fde\u7eed\u7684\u6210\u529f\u6982\u7387\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u9608\u503c\u5904\u7406\u4ea7\u751f\u4e8c\u5143\u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u4e09\u4e2a\u72ec\u7acb\u6d4b\u8bd5\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e869.8-11.1\u500d\u4e8e\u968f\u673a\u5206\u7c7b\u5668\u57fa\u7ebf\u7684\u7cbe\u786e\u5ea6\u3002\u7279\u5f81\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff1a\u521d\u521b\u516c\u53f8\u7c7b\u522b\u5217\u8868\u5360\u9884\u6d4b\u5f71\u54cd\u529b\u768415.6%\uff0c\u521b\u59cb\u4eba\u6570\u91cf\u6b21\u4e4b\uff0c\u6559\u80b2\u6c34\u5e73\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u8d21\u732e\u8f83\u5c0f\u4f46\u4e00\u81f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86LLM\u548c\u591a\u6a21\u578bML\u67b6\u6784\uff0c\u5728\u7f55\u89c1\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6210\u529f\u9a71\u52a8\u56e0\u7d20\uff0c\u4e3a\u98ce\u9669\u6295\u8d44\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2509.08235", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.08235", "abs": "https://arxiv.org/abs/2509.08235", "authors": ["Sheng Zhong", "Junkai Niu", "Yi Zhou"], "title": "Deep Visual Odometry for Stereo Event Cameras", "comment": null, "summary": "Event-based cameras are bio-inspired sensors with pixels that independently\nand asynchronously respond to brightness changes at microsecond resolution,\noffering the potential to handle state estimation tasks involving motion blur\nand high dynamic range (HDR) illumination conditions. However, the versatility\nof event-based visual odometry (VO) relying on handcrafted data association\n(either direct or indirect methods) is still unreliable, especially in field\nrobot applications under low-light HDR conditions, where the dynamic range can\nbe enormous and the signal-to-noise ratio is spatially-and-temporally varying.\nLeveraging deep neural networks offers new possibilities for overcoming these\nchallenges. In this paper, we propose a learning-based stereo event visual\nodometry. Building upon Deep Event Visual Odometry (DEVO), our system (called\nStereo-DEVO) introduces a novel and efficient static-stereo association\nstrategy for sparse depth estimation with almost no additional computational\nburden. By integrating it into a tightly coupled bundle adjustment (BA)\noptimization scheme, and benefiting from the recurrent network's ability to\nperform accurate optical flow estimation through voxel-based event\nrepresentations to establish reliable patch associations, our system achieves\nhigh-precision pose estimation in metric scale. In contrast to the offline\nperformance of DEVO, our system can process event data of \\zs{Video Graphics\nArray} (VGA) resolution in real time. Extensive evaluations on multiple public\nreal-world datasets and self-collected data justify our system's versatility,\ndemonstrating superior performance compared to state-of-the-art event-based VO\nmethods. More importantly, our system achieves stable pose estimation even in\nlarge-scale nighttime HDR scenarios.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53cc\u76ee\u4e8b\u4ef6\u89c6\u89c9\u91cf\u8ddd\u7cfb\u7edfStereo-DEVO\uff0c\u901a\u8fc7\u9759\u6001\u53cc\u76ee\u5173\u8054\u7b56\u7565\u548c\u7d27\u8026\u5408\u675f\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u4f4d\u4f30\u8ba1\uff0c\u5728\u591c\u95f4HDR\u573a\u666f\u4e2d\u4e5f\u80fd\u7a33\u5b9a\u5de5\u4f5c\u3002", "motivation": "\u4f20\u7edf\u624b\u5de5\u7f16\u7a0b\u7684\u4e8b\u4ef6\u89c6\u89c9\u91cf\u8ddd\u65b9\u6cd5\u5728\u4f4e\u5149HDR\u6761\u4ef6\u4e0b\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5e94\u5bf9\u52a8\u6001\u8303\u56f4\u5927\u3001\u4fe1\u566a\u6bd4\u53d8\u5316\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faStereo-DEVO\u7cfb\u7edf\uff1a1)\u65b0\u9898\u9ad8\u6548\u9759\u6001\u53cc\u76ee\u5173\u8054\u7b56\u7565\u8fdb\u884c\u7a00\u758f\u6df1\u5ea6\u4f30\u8ba1\uff1b2)\u7d27\u8026\u5408\u675f\u4f18\u5316\u65b9\u6848\uff1b3)\u5229\u7528\u5faa\u73af\u7f51\u7edc\u901a\u8fc7voxel\u57fa\u4e8b\u4ef6\u8868\u793a\u8fdb\u884c\u5149\u6d41\u4f30\u8ba1\u548c\u7247\u533a\u5173\u8054\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u5904\u7406VGA\u5206\u8fa8\u7387\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u4e0a\u8868\u73b0\u512a\u79c0\uff0c\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u6700\u5148\u8fdb\u4e8b\u4ef6\u91cf\u8ddd\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u591c\u95f4HDR\u573a\u666f\u4e2d\u4ecd\u80fd\u7a33\u5b9a\u5de5\u4f5c\u3002", "conclusion": "Stereo-DEVO\u7cfb\u7edf\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u89c6\u89c9\u91cf\u8ddd\u5728\u6781\u7aefHDR\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u91ce\u5916\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b9e\u65f6\u4f4d\u7f6e\u4f30\u8ba1\u80fd\u529b\u3002"}}
{"id": "2509.08260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08260", "abs": "https://arxiv.org/abs/2509.08260", "authors": ["Chi Zhang", "Xiang Zhang", "Chenxu Jiang", "Gui-Song Xia", "Lei Yu"], "title": "EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning", "comment": "18 pages", "summary": "Frame-based cameras with extended exposure times often produce perceptible\nvisual blurring and information loss between frames, significantly degrading\nvideo quality. To address this challenge, we introduce EVDI++, a unified\nself-supervised framework for Event-based Video Deblurring and Interpolation\nthat leverages the high temporal resolution of event cameras to mitigate motion\nblur and enable intermediate frame prediction. Specifically, the Learnable\nDouble Integral (LDI) network is designed to estimate the mapping relation\nbetween reference frames and sharp latent images. Then, we refine the coarse\nresults and optimize overall training efficiency by introducing a\nlearning-based division reconstruction module, enabling images to be converted\nwith varying exposure intervals. We devise an adaptive parameter-free fusion\nstrategy to obtain the final results, utilizing the confidence embedded in the\nLDI outputs of concurrent events. A self-supervised learning framework is\nproposed to enable network training with real-world blurry videos and events by\nexploring the mutual constraints among blurry frames, latent images, and event\nstreams. We further construct a dataset with real-world blurry images and\nevents using a DAVIS346c camera, demonstrating the generalizability of the\nproposed EVDI++ in real-world scenarios. Extensive experiments on both\nsynthetic and real-world datasets show that our method achieves\nstate-of-the-art performance in video deblurring and interpolation tasks.", "AI": {"tldr": "EVDI++\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u4e8b\u4ef6\u76f8\u673a\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u503c\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u6765\u51cf\u5c11\u8fd0\u52a8\u6a21\u7cca\u5e76\u9884\u6d4b\u4e2d\u95f4\u5e27\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5e27\u76f8\u673a\u957f\u66dd\u5149\u65f6\u95f4\u5bfc\u81f4\u7684\u89c6\u89c9\u6a21\u7cca\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u3002", "method": "\u63d0\u51faLearnable Double Integral (LDI)\u7f51\u7edc\u4f30\u8ba1\u53c2\u8003\u5e27\u4e0e\u6e05\u6670\u6f5c\u5728\u56fe\u50cf\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5f15\u5165\u57fa\u4e8e\u5b66\u4e60\u7684\u9664\u6cd5\u91cd\u5efa\u6a21\u5757\u4f18\u5316\u7ed3\u679c\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u65e0\u53c2\u6570\u878d\u5408\u7b56\u7565\uff0c\u6784\u5efa\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u503c\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "EVDI++\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u6a21\u7cca\u548c\u5e27\u95f4\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5177\u6709\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.08460", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.08460", "abs": "https://arxiv.org/abs/2509.08460", "authors": ["Wenqing Wang", "Ye Zhang", "Haoyu Li", "Jingyu Wang"], "title": "Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment", "comment": null, "summary": "Recent advances in robotics have enabled the widespread deployment of\nautonomous robotic systems in complex operational environments, presenting both\nunprecedented opportunities and significant security problems. Traditional\nshepherding approaches based on fixed formations are often ineffective or risky\nin urban and obstacle-rich scenarios, especially when facing adversarial agents\nwith unknown and adaptive behaviors. This paper addresses this challenge as an\nextended herding problem, where defensive robotic systems must safely guide\nadversarial agents with unknown strategies away from protected areas and into\npredetermined safe regions, while maintaining collision-free navigation in\ndynamic environments. We propose a hierarchical hybrid framework based on\nreach-avoid game theory and local motion planning, incorporating a virtual\ncontainment boundary and event-triggered pursuit mechanisms to enable scalable\nand robust multi-agent coordination. Simulation results demonstrate that the\nproposed approach achieves safe and efficient guidance of adversarial agents to\ndesignated regions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u8fbe\u6027\u535a\u5f08\u548c\u5c40\u90e8\u8fd0\u52a8\u89c4\u5212\u7684\u5c42\u6b21\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u5f15\u5bfc\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u8fdc\u79bb\u4fdd\u62a4\u533a\u5e76\u8fdb\u5165\u5b89\u5168\u533a\u57df", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u7f16\u961f\u7684\u7267\u7fa4\u65b9\u6cd5\u5728\u57ce\u5e02\u548c\u969c\u788d\u7269\u4e30\u5bcc\u7684\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\u4e14\u98ce\u9669\u9ad8\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u5177\u6709\u672a\u77e5\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u65f6", "method": "\u57fa\u4e8e\u53ef\u8fbe\u6027\u535a\u5f08\u7406\u8bba\u548c\u5c40\u90e8\u8fd0\u52a8\u89c4\u5212\u7684\u5c42\u6b21\u6df7\u5408\u6846\u67b6\uff0c\u5305\u542b\u865a\u62df\u904f\u5236\u8fb9\u754c\u548c\u4e8b\u4ef6\u89e6\u53d1\u7684\u8ffd\u9010\u673a\u5236\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b89\u5168\u9ad8\u6548\u5730\u5c06\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u5f15\u5bfc\u5230\u6307\u5b9a\u533a\u57df", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u590d\u6742\u64cd\u4f5c\u73af\u5883\u4e2d\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u7684\u5b89\u5168\u5f15\u5bfc\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.08521", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.08521", "abs": "https://arxiv.org/abs/2509.08521", "authors": ["Soheil Espahbodini Nia"], "title": "FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning", "comment": "35 pages, 8 figures, 2 tables, submitted to the International Journal\n  of Robotics Research (IJRR)", "summary": "Path planning in dynamic environments remains a core challenge in robotics,\nespecially as autonomous systems are deployed in unpredictable spaces such as\nwarehouses and public roads. While algorithms like Fast Marching Tree\n(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their\nsingle-pass design prevents path revisions which are essential for real-time\nadaptation. On the other hand, full replanning is often too computationally\nexpensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching\nTree algorithm that enables efficient and consistent replanning in dynamic\nenvironments. We revisit the neighbor selection rule of FMT$^{*}$ and\ndemonstrate that a minimal change overcomes its single-pass limitation,\nenabling the algorithm to update cost-to-come values upon discovering better\nconnections without sacrificing asymptotic optimality or computational\nefficiency. By maintaining a cost-ordered priority queue and applying a\nselective update condition that uses an expanding neighbor to identify and\ntrigger the re-evaluation of any node with a potentially suboptimal path,\nFMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the\nenvironment evolves. This targeted strategy preserves the inherent efficiency\nof FMT$^{*}$ while enabling robust adaptation to changes in obstacle\nconfiguration. FMT$^{x}$ is proven to recover an asymptotically optimal\nsolution after environmental changes. Experimental results demonstrate that\nFMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more\nswiftly to dynamic events with lower computational overhead and thus offering a\nmore effective solution for real-time robotic navigation in unpredictable\nworlds.", "AI": {"tldr": "FMT^x\u662fFast Marching Tree\u7b97\u6cd5\u7684\u6269\u5c55\u7248\u672c\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u6548\u91cd\u89c4\u5212\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4fee\u6539\u4fdd\u6301\u6e10\u8fdb\u6700\u4f18\u6027\u548c\u8ba1\u7b97\u6548\u7387", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u662f\u673a\u5668\u4eba\u6280\u672f\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edfFMT*\u7b97\u6cd5\u5355\u6b21\u89c4\u5212\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\uff0c\u800c\u5b8c\u5168\u91cd\u89c4\u5212\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8", "method": "\u4fee\u6539FMT*\u7684\u90bb\u5c45\u9009\u62e9\u89c4\u5219\uff0c\u7ef4\u62a4\u6210\u672c\u6392\u5e8f\u7684\u4f18\u5148\u961f\u5217\uff0c\u5e94\u7528\u9009\u62e9\u6027\u66f4\u65b0\u6761\u4ef6\u6765\u8bc6\u522b\u548c\u91cd\u65b0\u8bc4\u4f30\u6f5c\u5728\u6b21\u4f18\u8def\u5f84\u7684\u8282\u70b9", "result": "FMT^x\u5728\u52a8\u6001\u4e8b\u4ef6\u4e2d\u53cd\u5e94\u66f4\u8fc5\u901f\uff0c\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\uff0c\u4f18\u4e8e\u6709\u5f71\u54cd\u529b\u7684\u91cd\u89c4\u5212\u5668RRT^x", "conclusion": "FMT^x\u4e3a\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u6301\u4e86FMT*\u7684\u6548\u7387\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u9002\u5e94\u6027"}}
{"id": "2509.08604", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08604", "abs": "https://arxiv.org/abs/2509.08604", "authors": ["Anran Li", "Lingfei Qian", "Mengmeng Du", "Yu Yin", "Yan Hu", "Zihao Sun", "Yihang Fu", "Erica Stutz", "Xuguang Ai", "Qianqian Xie", "Rui Zhu", "Jimin Huang", "Yifan Yang", "Siru Liu", "Yih-Chung Tham", "Lucila Ohno-Machado", "Hyunghoon Cho", "Zhiyong Lu", "Hua Xu", "Qingyu Chen"], "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u53d1\u73b0\u8bb0\u5fc6\u73b0\u8c61\u6bd4\u666e\u901a\u9886\u57df\u66f4\u4e3a\u666e\u904d\u4e14\u53ef\u5206\u4e3a\u6709\u76ca\u3001\u65e0\u4fe1\u606f\u548c\u6709\u5bb3\u4e09\u7c7b\u3002", "motivation": "\u867d\u7136LLMs\u5728\u533b\u5b66\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5173\u4e8e\u6a21\u578b\u8bb0\u5fc6\u533b\u5b66\u8bad\u7ec3\u6570\u636e\u7684\u7a0b\u5ea6\u4ecd\u662f\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u666e\u904d\u6027\u3001\u7279\u5f81\u3001\u91cf\u7ea7\u548c\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u4e09\u79cd\u5e38\u89c1\u9002\u914d\u573a\u666f\uff1a\u533b\u5b66\u8bed\u6599\u7eed\u9884\u8bad\u7ec3\u3001\u6807\u51c6\u533b\u5b66\u6d4b\u8bd5\u96c6\u7ec6\u8c03\u3001\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u7ec6\u8c03\uff08\u5305\u542b\u8d8513,000\u4efd\u75c5\u4eba\u8bb0\u5f55\uff09\u3002", "result": "\u8bb0\u5fc6\u73b0\u8c61\u5728\u6240\u6709\u9002\u914d\u573a\u666f\u4e2d\u90fd\u5f88\u666e\u904d\uff0c\u663e\u8457\u9ad8\u4e8e\u666e\u901a\u9886\u57df\u3002\u53ef\u5206\u4e3a\u4e09\u7c7b\uff1a\u6709\u76ca\u8bb0\u5fc6\uff08\u4e34\u5e8a\u6307\u5357\u7b49\uff09\u3001\u65e0\u4fe1\u606f\u8bb0\u5fc6\uff08\u6a21\u677f\u5316\u8bed\u8a00\uff09\u3001\u6709\u5bb3\u8bb0\u5fc6\uff08\u654f\u611f\u4e34\u5e8a\u4fe1\u606f\uff09\u3002", "conclusion": "\u63d0\u51fa\u5b9e\u8df5\u5efa\u8bae\uff1a\u4fc3\u8fdb\u6709\u76ca\u8bb0\u5fc6\u63d0\u5347\u9886\u57df\u77e5\u8bc6\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u65e0\u4fe1\u606f\u8bb0\u5fc6\u4fc3\u8fdb\u6df1\u5ea6\u5b66\u4e60\uff0c\u51cf\u8f7b\u6709\u5bb3\u8bb0\u5fc6\u9632\u6b62\u654f\u611f\u4fe1\u606f\u6cc4\u6f0f\u3002"}}
{"id": "2509.08436", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08436", "abs": "https://arxiv.org/abs/2509.08436", "authors": ["Xia Yue", "Anfeng Liu", "Ning Chen", "Chenjia Huang", "Hui Liu", "Zhou Huang", "Leyuan Fang"], "title": "Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time", "comment": null, "summary": "Hyperspectral image (HSI) classification models are highly sensitive to\ndistribution shifts caused by various real-world degradations such as noise,\nblur, compression, and atmospheric effects. To address this challenge, we\npropose HyperTTA, a unified framework designed to enhance model robustness\nunder diverse degradation conditions. Specifically, we first construct a\nmulti-degradation hyperspectral dataset that systematically simulates nine\nrepresentative types of degradations, providing a comprehensive benchmark for\nrobust classification evaluation. Based on this, we design a spectral-spatial\ntransformer classifier (SSTC) enhanced with a multi-level receptive field\nmechanism and label smoothing regularization to jointly capture multi-scale\nspatial context and improve generalization. Furthermore, HyperTTA incorporates\na lightweight test-time adaptation (TTA) strategy, the confidence-aware\nentropy-minimized LayerNorm adapter (CELA), which updates only the affine\nparameters of LayerNorm layers by minimizing prediction entropy on\nhigh-confidence unlabeled target samples. This confidence-aware adaptation\nprevents unreliable updates from noisy predictions, enabling robust and dynamic\nadaptation without access to source data or target annotations. Extensive\nexperiments on two benchmark datasets demonstrate that HyperTTA outperforms\nexisting baselines across a wide range of degradation scenarios, validating the\neffectiveness of both its classification backbone and the proposed TTA scheme.\nCode will be made available publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86HyperTTA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9000\u5316\u6570\u636e\u96c6\u3001\u5149\u8c31-\u7a7a\u95f4Transformer\u5206\u7c7b\u5668\u548c\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u5728\u5404\u79cd\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u5bf9\u566a\u58f0\u3001\u6a21\u7cca\u3001\u538b\u7f29\u548c\u5927\u6c14\u6548\u5e94\u7b49\u73b0\u5b9e\u4e16\u754c\u9000\u5316\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u6837\u5316\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efa\u591a\u9000\u5316\u9ad8\u5149\u8c31\u6570\u636e\u96c6\uff1b\u8bbe\u8ba1\u5149\u8c31-\u7a7a\u95f4Transformer\u5206\u7c7b\u5668\uff08SSTC\uff09\u7ed3\u5408\u591a\u7ea7\u611f\u53d7\u91ce\u673a\u5236\u548c\u6807\u7b7e\u5e73\u6ed1\u6b63\u5219\u5316\uff1b\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u611f\u77e5\u71b5\u6700\u5c0f\u5316LayerNorm\u9002\u914d\u5668\uff08CELA\uff09\u8fdb\u884c\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHyperTTA\u5728\u5404\u79cd\u9000\u5316\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5206\u7c7b\u4e3b\u5e72\u548cTTA\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "HyperTTA\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6570\u636e\u96c6\u6784\u5efa\u3001\u5148\u8fdb\u7684\u5206\u7c7b\u5668\u8bbe\u8ba1\u548c\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08461", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2509.08461", "abs": "https://arxiv.org/abs/2509.08461", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u8bc6\u522b\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u8bc6\u522b\u4efb\u52a1", "method": "\u4f7f\u7528\u5fae\u8c03\u540e\u7684LLaMa 3.2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e0eNOvA\u548cDUNE\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u7684\u5148\u8fdb\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83", "result": "VLMs\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8d85\u8d8aCNNs\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3001\u7075\u6d3b\u6027\u4ee5\u53ca\u57fa\u4e8e\u63a8\u7406\u7684\u9884\u6d4b\u80fd\u529b", "conclusion": "VLMs\u6709\u6f5c\u529b\u6210\u4e3a\u7269\u7406\u4e8b\u4ef6\u5206\u7c7b\u7684\u901a\u7528\u9aa8\u5e72\u7f51\u7edc\uff0c\u56e0\u5176\u9ad8\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e2d\u5fae\u5b50\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u591a\u6a21\u6001\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2509.08482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08482", "abs": "https://arxiv.org/abs/2509.08482", "authors": ["Andrea Maldonado", "Christian M. M. Frey", "Sai Anirudh Aryasomayajula", "Ludwig Zellner", "Stephan A. Fahrenkrog-Petersen", "Thomas Seidl"], "title": "SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms", "comment": null, "summary": "Process mining aims to extract and analyze insights from event logs, yet\nalgorithm metric results vary widely depending on structural event log\ncharacteristics. Existing work often evaluates algorithms on a fixed set of\nreal-world event logs but lacks a systematic analysis of how event log\ncharacteristics impact algorithms individually. Moreover, since event logs are\ngenerated from processes, where characteristics co-occur, we focus on\nassociational rather than causal effects to assess how strong the overlapping\nindividual characteristic affects evaluation metrics without assuming isolated\ncausal effects, a factor often neglected by prior work. We introduce SHAining,\nthe first approach to quantify the marginal contribution of varying event log\ncharacteristics to process mining algorithms' metrics. Using process discovery\nas a downstream task, we analyze over 22,000 event logs covering a wide span of\ncharacteristics to uncover which affect algorithms across metrics (e.g.,\nfitness, precision, complexity) the most. Furthermore, we offer novel insights\nabout how the value of event log characteristics correlates with their\ncontributed impact, assessing the algorithm's robustness.", "AI": {"tldr": "SHAining\u65b9\u6cd5\u9996\u6b21\u91cf\u5316\u4e8b\u4ef6\u65e5\u5fd7\u7279\u5f81\u5bf9\u6d41\u7a0b\u6316\u6398\u7b97\u6cd5\u6307\u6807\u7684\u8fb9\u9645\u8d21\u732e\uff0c\u901a\u8fc7\u5206\u679022,000\u591a\u4e2a\u4e8b\u4ef6\u65e5\u5fd7\u53d1\u73b0\u7279\u5f81\u5bf9\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u7b97\u6cd5\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5728\u56fa\u5b9a\u7684\u4e8b\u4ef6\u65e5\u5fd7\u96c6\u4e0a\u8bc4\u4f30\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u65e5\u5fd7\u7279\u5f81\u5982\u4f55\u5355\u72ec\u5f71\u54cd\u7b97\u6cd5\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u4e14\u5ffd\u89c6\u4e86\u7279\u5f81\u5171\u73b0\u5bf9\u8bc4\u4f30\u6307\u6807\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faSHAining\u65b9\u6cd5\uff0c\u4f7f\u7528\u6d41\u7a0b\u53d1\u73b0\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\uff0c\u5206\u6790\u8d85\u8fc722,000\u4e2a\u8986\u76d6\u5e7f\u6cdb\u7279\u5f81\u7684\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u91cf\u5316\u4e8b\u4ef6\u65e5\u5fd7\u7279\u5f81\u5bf9\u7b97\u6cd5\u6307\u6807\u7684\u8fb9\u9645\u8d21\u732e\u3002", "result": "\u63ed\u793a\u4e86\u54ea\u4e9b\u4e8b\u4ef6\u65e5\u5fd7\u7279\u5f81\u5bf9\u7b97\u6cd5\u6307\u6807\uff08\u5982\u62df\u5408\u5ea6\u3001\u7cbe\u786e\u5ea6\u3001\u590d\u6742\u5ea6\uff09\u5f71\u54cd\u6700\u5927\uff0c\u5e76\u63d0\u4f9b\u4e86\u7279\u5f81\u503c\u4e0e\u8d21\u732e\u5f71\u54cd\u76f8\u5173\u6027\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "SHAining\u65b9\u6cd5\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u4e8b\u4ef6\u65e5\u5fd7\u7279\u5f81\u5bf9\u6d41\u7a0b\u6316\u6398\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u7b97\u6cd5\u9009\u62e9\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2509.08698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08698", "abs": "https://arxiv.org/abs/2509.08698", "authors": ["Thorsten Wittkopp"], "title": "A layered architecture for log analysis in complex IT systems", "comment": "Dissertation", "summary": "In the evolving IT landscape, stability and reliability of systems are\nessential, yet their growing complexity challenges DevOps teams in\nimplementation and maintenance. Log analysis, a core element of AIOps, provides\ncritical insights into complex behaviors and failures. This dissertation\nintroduces a three-layered architecture to support DevOps in failure\nresolution. The first layer, Log Investigation, performs autonomous log\nlabeling and anomaly classification. We propose a method that labels log data\nwithout manual effort, enabling supervised training and precise evaluation of\nanomaly detection. Additionally, we define a taxonomy that groups anomalies\ninto three categories, ensuring appropriate method selection. The second layer,\nAnomaly Detection, detects behaviors deviating from the norm. We propose a\nflexible Anomaly Detection method adaptable to unsupervised, weakly supervised,\nand supervised training. Evaluations on public and industry datasets show\nF1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third\nlayer, Root Cause Analysis, identifies minimal log sets describing failures,\ntheir origin, and event sequences. By balancing training data and identifying\nkey services, our Root Cause Analysis method consistently detects 90-98% of\nroot cause log lines within the top 10 candidates, providing actionable\ninsights for mitigation. Our research addresses how log analysis methods can be\ndesigned and optimized to help DevOps resolve failures efficiently. By\nintegrating these three layers, the architecture equips teams with robust\nmethods to enhance IT system reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u6b21\u67b6\u6784\u6765\u652f\u6301DevOps\u56e2\u961f\u8fdb\u884c\u6545\u969c\u89e3\u51b3\uff0c\u5305\u62ec\u65e5\u5fd7\u8c03\u67e5\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u6839\u56e0\u5206\u6790\u4e09\u4e2a\u5173\u952e\u5c42\u9762\u3002", "motivation": "\u4f53\u7cfb\u590d\u6742\u6027\u589e\u52a0\u7ed9DevOps\u56e2\u961f\u5e26\u6765\u4e86\u7ef4\u62a4\u6311\u6218\uff0c\u65e5\u5fd7\u5206\u6790\u4f5c\u4e3aAIOps\u7684\u6838\u5fc3\u5143\u7d20\uff0c\u9700\u8981\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6545\u969c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a1)\u65e5\u5fd7\u8c03\u67e5\u5c42-\u81ea\u4e3b\u65e5\u5fd7\u6807\u6ce8\u548c\u5f02\u5e38\u5206\u7c7b\uff1b2)\u5f02\u5e38\u68c0\u6d4b\u5c42-\u9002\u7528\u4e8e\u65e0\u76d1\u7763\u3001\u5f31\u76d1\u7763\u548c\u6709\u76d1\u7763\u8bad\u7ec3\u7684\u7075\u6d3b\u65b9\u6cd5\uff1b3)\u6839\u56e0\u5206\u6790\u5c42-\u8bc6\u522b\u63cf\u8ff0\u6545\u969c\u7684\u6700\u5c0f\u65e5\u5fd7\u96c6\u3002", "result": "\u5f02\u5e38\u68c0\u6d4b\u5728\u516c\u5171\u548c\u884c\u4e1a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230F1\u5206\u65700.98-1.0\uff1b\u6839\u56e0\u5206\u6790\u5728top10\u5019\u9009\u4e2d\u68c0\u6d4b\u523090-98%\u7684\u6839\u56e0\u65e5\u5fd7\u884c\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u4e09\u4e2a\u5173\u952e\u5c42\u9762\uff0c\u8be5\u67b6\u6784\u4e3aDevOps\u56e2\u961f\u63d0\u4f9b\u4e86\u575a\u56fa\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8IT\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6545\u969c\u89e3\u51b3\u6548\u7387\u3002"}}
{"id": "2509.08794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08794", "abs": "https://arxiv.org/abs/2509.08794", "authors": ["Dennis Melamed", "Connor Hashemi", "Scott McCloskey"], "title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation", "comment": null, "summary": "Event-based cameras (EBCs) are a promising new technology for star\ntracking-based attitude determination, but prior studies have struggled to\ndetermine accurate ground truth for real data. We analyze the accuracy of an\nEBC star tracking system utilizing the Earth's motion as the ground truth for\ncomparison. The Earth rotates in a regular way with very small irregularities\nwhich are measured to the level of milli-arcseconds. By keeping an event camera\nstatic and pointing it through a ground-based telescope at the night sky, we\ncreate a system where the only camera motion in the celestial reference frame\nis that induced by the Earth's rotation. The resulting event stream is\nprocessed to generate estimates of orientation which we compare to the\nInternational Earth Rotation and Reference System (IERS) measured orientation\nof the Earth. The event camera system is able to achieve a root mean squared\nacross error of 18.47 arcseconds and an about error of 78.84 arcseconds.\nCombined with the other benefits of event cameras over framing sensors (reduced\ncomputation due to sparser data streams, higher dynamic range, lower energy\nconsumption, faster update rates), this level of accuracy suggests the utility\nof event cameras for low-cost and low-latency star tracking. We provide all\ncode and data used to generate our results:\nhttps://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5728\u661f\u8ff9\u8e2a\u7cfb\u7edf\u4e2d\u7684\u7cbe\u5ea6\u5206\u6790\uff0c\u5229\u7528\u5730\u7403\u81ea\u8f6c\u4f5c\u4e3a\u771f\u5b9e\u503c\u5bf9\u6bd4\uff0c\u8fbe\u523018.47\u79d2\u7684RMS\u9510\u5dee\u7cbe\u5ea6", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u5728\u661f\u8ff9\u8e2a\u4e2d\u7f3a\u4e4f\u51c6\u786e\u771f\u5b9e\u503c\u7684\u95ee\u9898\uff0c\u5229\u7528\u5730\u7403\u89c4\u5f8b\u8fd0\u52a8\u4f5c\u4e3a\u53c2\u8003\u6807\u51c6", "method": "\u901a\u8fc7\u9759\u6001\u653e\u7f6e\u4e8b\u4ef6\u76f8\u673a\u9488\u5bf9\u591c\u7a7a\uff0c\u4ee5\u5730\u7403\u81ea\u8f6c\u4f5c\u4e3a\u552f\u4e00\u76f8\u673a\u8fd0\u52a8\u6765\u6e90\uff0c\u5c06\u4e8b\u4ef6\u6d41\u5904\u7406\u751f\u6210\u65b9\u5411\u4f30\u8ba1", "result": "\u7cfb\u7edf\u8fbe\u5230\u5747\u65b9\u6839\u9510\u5dee18.47\u79d2\uff0c\u7edd\u5bf9\u9510\u5dee78.84\u79d2\u7684\u7cbe\u5ea6\u6c34\u5e73\uff0c\u663e\u793a\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u661f\u8ff9\u8e2a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u5177\u5907\u4f4e\u6210\u672c\u3001\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u4f18\u52bf\uff0c\u8be5\u7cbe\u5ea6\u6c34\u5e73\u8bc1\u660e\u5176\u9002\u5408\u7528\u4e8e\u5b9e\u7528\u5316\u7684\u661f\u8ff9\u8e2a\u7cfb\u7edf"}}
{"id": "2509.08822", "categories": ["cs.LG", "I.2.6; I.2.9; C.3"], "pdf": "https://arxiv.org/pdf/2509.08822", "abs": "https://arxiv.org/abs/2509.08822", "authors": ["Willy Sucipto", "Jianlong Zhou", "Ray Seung Min Kwon", "Fang Chen"], "title": "A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management", "comment": "30 pages, 8 figures, 3 tables. Survey of TinyML and IoT applications\n  in beekeeping (datasets, benchmarking, deployment). Submitted to ACM\n  Computing Surveys (under review)", "summary": "Honey bee colonies are essential for global food security and ecosystem\nstability, yet they face escalating threats from pests, diseases, and\nenvironmental stressors. Traditional hive inspections are labor-intensive and\ndisruptive, while cloud-based monitoring solutions remain impractical for\nremote or resource-limited apiaries. Recent advances in Internet of Things\n(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring\ndirectly on edge devices, offering scalable and non-invasive alternatives. This\nsurvey synthesizes current innovations at the intersection of TinyML and\napiculture, organized around four key functional areas: monitoring hive\nconditions, recognizing bee behaviors, detecting pests and diseases, and\nforecasting swarming events. We further examine supporting resources, including\npublicly available datasets, lightweight model architectures optimized for\nembedded deployment, and benchmarking strategies tailored to field constraints.\nCritical limitations such as data scarcity, generalization challenges, and\ndeployment barriers in off-grid environments are highlighted, alongside\nemerging opportunities in ultra-efficient inference pipelines, adaptive edge\nlearning, and dataset standardization. By consolidating research and\nengineering practices, this work provides a foundation for scalable, AI-driven,\nand ecologically informed monitoring systems to support sustainable pollinator\nmanagement.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e86TinyML\u6280\u672f\u5728\u517b\u8702\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u8702\u7fa4\u6761\u4ef6\u76d1\u6d4b\u3001\u871c\u8702\u884c\u4e3a\u8bc6\u522b\u3001\u75c5\u866b\u5bb3\u68c0\u6d4b\u548c\u5206\u8702\u9884\u6d4b\u56db\u4e2a\u529f\u80fd\u9886\u57df\uff0c\u4e3a\u53ef\u6301\u7eed\u4f20\u7c89\u5a92\u4ecb\u7ba1\u7406\u63d0\u4f9bAI\u9a71\u52a8\u7684\u76d1\u6d4b\u7cfb\u7edf\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u8702\u7bb1\u68c0\u67e5\u65b9\u5f0f\u52b3\u52a8\u5bc6\u96c6\u4e14\u5177\u6709\u5e72\u6270\u6027\uff0c\u800c\u57fa\u4e8e\u4e91\u7684\u76d1\u63a7\u65b9\u6848\u5728\u504f\u8fdc\u6216\u8d44\u6e90\u6709\u9650\u7684\u517b\u8702\u573a\u4e0d\u5b9e\u7528\u3002\u7269\u8054\u7f51\u548c\u5fae\u578b\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u53d1\u5c55\u4e3a\u4f4e\u529f\u8017\u3001\u5b9e\u65f6\u8fb9\u7f18\u76d1\u63a7\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u56f4\u7ed5\u56db\u4e2a\u5173\u952e\u529f\u80fd\u9886\u57df\uff08\u8702\u7fa4\u6761\u4ef6\u76d1\u6d4b\u3001\u871c\u8702\u884c\u4e3a\u8bc6\u522b\u3001\u75c5\u866b\u5bb3\u68c0\u6d4b\u3001\u5206\u8702\u9884\u6d4b\uff09\u7cfb\u7edf\u68b3\u7406TinyML\u5728\u517b\u8702\u4e1a\u4e2d\u7684\u521b\u65b0\u5e94\u7528\uff0c\u5e76\u8003\u5bdf\u76f8\u5173\u6570\u636e\u96c6\u3001\u8f7b\u91cf\u7ea7\u6a21\u578b\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u7b56\u7565\u3002", "result": "\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5305\u62ec\u6570\u636e\u7a00\u7f3a\u3001\u6cdb\u5316\u6311\u6218\u4ee5\u53ca\u5728\u79bb\u7f51\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u969c\u788d\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8d85\u9ad8\u6548\u63a8\u7406\u7ba1\u9053\u3001\u81ea\u9002\u5e94\u8fb9\u7f18\u5b66\u4e60\u548c\u6570\u636e\u96c6\u6807\u51c6\u5316\u7b49\u65b0\u5174\u673a\u9047\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u7814\u7a76\u548c\u5de5\u7a0b\u5b9e\u8df5\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001AI\u9a71\u52a8\u4e14\u751f\u6001\u4fe1\u606f\u5316\u7684\u76d1\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u652f\u6301\u53ef\u6301\u7eed\u7684\u4f20\u7c89\u5a92\u4ecb\u7ba1\u7406\u3002"}}
