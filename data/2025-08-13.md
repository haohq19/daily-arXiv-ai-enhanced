<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Evaluation of State-of-the-Art Deep Learning Techniques for Plant Disease and Pest Detection](https://arxiv.org/abs/2508.08317)
*Saptarshi Banerjee,Tausif Mallick,Amlan Chakroborty,Himadri Nath Saha,Nityananda T. Takur*

Main category: cs.CV

TL;DR: 该论文综述了基于AI、ML和DL的植物病虫害检测方法，分类为五种技术，并展示了现代AI方法在速度和准确性上的优势，尤其是视觉变换器（如HvT）表现突出。


<details>
  <summary>Details</summary>
Motivation: 提高作物产量和减少经济损失需要更精确和高效的植物病虫害检测方法。

Method: 论文将方法分为五类：高光谱成像、非可视化技术、可视化方法、改进的深度学习架构和变换器模型。

Result: 现代AI方法（如HvT）在检测速度和准确性上优于传统方法，HvT的准确率超过99.3%。

Conclusion: 论文总结了系统设计挑战，提出了解决方案，并指出了未来研究方向。

Abstract: Addressing plant diseases and pests is critical for enhancing crop production
and preventing economic losses. Recent advances in artificial intelligence
(AI), machine learning (ML), and deep learning (DL) have significantly improved
the precision and efficiency of detection methods, surpassing the limitations
of manual identification. This study reviews modern computer-based techniques
for detecting plant diseases and pests from images, including recent AI
developments. The methodologies are organized into five categories:
hyperspectral imaging, non-visualization techniques, visualization approaches,
modified deep learning architectures, and transformer models. This structured
taxonomy provides researchers with detailed, actionable insights for selecting
advanced state-of-the-art detection methods. A comprehensive survey of recent
work and comparative studies demonstrates the consistent superiority of modern
AI-based approaches, which often outperform older image analysis methods in
speed and accuracy. In particular, vision transformers such as the Hierarchical
Vision Transformer (HvT) have shown accuracy exceeding 99.3% in plant disease
detection, outperforming architectures like MobileNetV3. The study concludes by
discussing system design challenges, proposing solutions, and outlining
promising directions for future research.

</details>


### [2] [SelfHVD: Self-Supervised Handheld Video Deblurring for Mobile Phones](https://arxiv.org/abs/2508.08605)
*Honglei Xu,Zhilu Zhang,Junjie Fan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种自监督的手持视频去模糊方法，通过提取视频中的清晰线索作为训练标签，并结合自增强和空间一致性维护技术，显著提升了去模糊效果。


<details>
  <summary>Details</summary>
Motivation: 手持移动设备拍摄的视频常因抖动导致模糊，现有方法在真实场景中表现不佳，存在训练与测试数据的模糊域差距问题。

Method: 1. 提取视频中的清晰线索作为模糊帧的错位标签；2. 提出自增强视频去模糊方法生成高质量配对数据；3. 引入自约束空间一致性维护技术防止输出帧位置偏移。

Result: 在合成和真实数据集上的实验表明，该方法显著优于现有自监督方法。

Conclusion: 该方法通过自监督和增强技术有效解决了手持视频去模糊问题，代码和数据集已开源。

Abstract: Shooting video with a handheld mobile phone, the most common photographic
device, often results in blurry frames due to shaking hands and other
instability factors. Although previous video deblurring methods have achieved
impressive progress, they still struggle to perform satisfactorily on
real-world handheld video due to the blur domain gap between training and
testing data. To address the issue, we propose a self-supervised method for
handheld video deblurring, which is driven by sharp clues in the video. First,
to train the deblurring model, we extract the sharp clues from the video and
take them as misalignment labels of neighboring blurry frames. Second, to
improve the model's ability, we propose a novel Self-Enhanced Video Deblurring
(SEVD) method to create higher-quality paired video data. Third, we propose a
Self-Constrained Spatial Consistency Maintenance (SCSCM) method to regularize
the model, preventing position shifts between the output and input frames.
Moreover, we construct a synthetic and a real-world handheld video dataset for
handheld video deblurring. Extensive experiments on these two and other common
real-world datasets demonstrate that our method significantly outperforms
existing self-supervised ones. The code and datasets are publicly available at
https://github.com/cshonglei/SelfHVD.

</details>


### [3] [When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges](https://arxiv.org/abs/2508.09022)
*Zhiqiang Yang,Renshuai Tao,Xiaolong Zheng,Guodong Yang,Chunjie Zhang*

Main category: cs.CV

TL;DR: 论文提出DPGNet方法，通过双路径引导网络解决深度伪造检测中标注数据不足的问题，利用文本引导跨域对齐和课程驱动伪标签生成，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容越来越逼真，人工标注变得耗时且不可靠，亟需能有效利用大规模未标注数据的方法。

Method: DPGNet包含文本引导跨域对齐和课程驱动伪标签生成两个核心模块，结合跨域知识蒸馏防止灾难性遗忘。

Result: 在11个数据集上，DPGNet比现有方法性能提升6.3%。

Conclusion: DPGNet通过利用未标注数据有效解决了深度伪造检测中的标注挑战。

Abstract: Existing deepfake detection methods heavily depend on labeled training data.
However, as AI-generated content becomes increasingly realistic, even
\textbf{human annotators struggle to distinguish} between deepfakes and
authentic images. This makes the labeling process both time-consuming and less
reliable. Specifically, there is a growing demand for approaches that can
effectively utilize large-scale unlabeled data from online social networks.
Unlike typical unsupervised learning tasks, where categories are distinct,
AI-generated faces closely mimic real image distributions and share strong
similarities, causing performance drop in conventional strategies. In this
paper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key
challenges: (1) bridging the domain gap between faces from different generation
models, and (2) utilizing unlabeled image samples. The method features two core
modules: text-guided cross-domain alignment, which uses learnable prompts to
unify visual and textual embeddings into a domain-invariant feature space, and
curriculum-driven pseudo label generation, which dynamically exploit more
informative unlabeled samples. To prevent catastrophic forgetting, we also
facilitate bridging between domains via cross-domain knowledge distillation.
Extensive experiments on \textbf{11 popular datasets}, show that DPGNet
outperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectiveness
in leveraging unlabeled data to address the annotation challenges posed by the
increasing realism of deepfakes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants](https://arxiv.org/abs/2508.08266)
*Ryan Mioduski*

Main category: cs.LG

TL;DR: 研究评估了大型语言模型（LLMs）在将弗吉尼亚17-18世纪土地专利描述转换为地理坐标方面的表现，发现某些模型在准确性和成本效益上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 弗吉尼亚17-18世纪的土地专利描述多为叙述性文本，限制了空间分析，因此需要一种高效的方法将其转换为地理坐标。

Method: 研究测试了六种OpenAI模型（三种架构）在两种范式下的表现：直接生成坐标和工具增强的链式思考（调用外部地理编码API），并与GIS分析师等基准进行比较。

Result: 最佳模型（o3-2025-04-16）平均误差为23公里，优于其他模型和基准。五模型集成进一步降低误差至19公里，成本效益显著。

Conclusion: LLMs在历史地理坐标转换中具有可扩展性、准确性和成本效益的潜力。

Abstract: Virginia's seventeenth- and eighteenth-century land patents survive primarily
as narrative metes-and-bounds descriptions, limiting spatial analysis. This
study systematically evaluates current-generation large language models (LLMs)
in converting these prose abstracts into geographically accurate
latitude/longitude coordinates within a focused evaluation context. A digitized
corpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43
rigorously verified test cases serving as an initial, geographically focused
benchmark. Six OpenAI models across three architectures (o-series, GPT-4-class,
and GPT-3.5) were tested under two paradigms: direct-to-coordinate and
tool-augmented chain-of-thought invoking external geocoding APIs. Results were
compared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3,
and a county-centroid heuristic.
  The top single-call model, o3-2025-04-16, achieved a mean error of 23 km
(median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest
LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70%
(Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12
km) at minimal additional cost (approx. USD 0.20 per grant), outperforming the
median LLM by 48.6%. A patentee-name-redaction ablation increased error by
about 9%, indicating reliance on textual landmark and adjacency descriptions
rather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained
a 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong
cost-accuracy benchmark; external geocoding tools offered no measurable benefit
in this evaluation.
  These findings demonstrate the potential of LLMs for scalable, accurate, and
cost-effective historical georeferencing.

</details>


### [5] [Stationarity Exploration for Multivariate Time Series Forecasting](https://arxiv.org/abs/2508.08919)
*Hao Liu,Chun Yang,Zhang xiaoxing,Rui Ma,Xiaobin Zhu*

Main category: cs.LG

TL;DR: APRNet通过解耦振幅和相位，有效捕捉时间序列中的平稳信息，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从复杂的频率成分中提取平稳信息，APRNet旨在解决这一问题。

Method: 提出APRNet，利用振幅和相位的相互关系，结合KLC模块自适应拟合局部函数。

Result: 实验表明APRNet在捕捉时变模式上优于现有方法。

Conclusion: APRNet通过解耦信号特性，显著提升了时间序列预测的准确性。

Abstract: Deep learning-based time series forecasting has found widespread
applications. Recently, converting time series data into the frequency domain
for forecasting has become popular for accurately exploring periodic patterns.
However, existing methods often cannot effectively explore stationary
information from complex intertwined frequency components. In this paper, we
propose a simple yet effective Amplitude-Phase Reconstruct Network (APRNet)
that models the inter-relationships of amplitude and phase, which prevents the
amplitude and phase from being constrained by different physical quantities,
thereby decoupling the distinct characteristics of signals for capturing
stationary information. Specifically, we represent the multivariate time series
input across sequence and channel dimensions, highlighting the correlation
between amplitude and phase at multiple interaction frequencies. We propose a
novel Kolmogorov-Arnold-Network-based Local Correlation (KLC) module to
adaptively fit local functions using univariate functions, enabling more
flexible characterization of stationary features across different amplitudes
and phases. This significantly enhances the model's capability to capture
time-varying patterns. Extensive experiments demonstrate the superiority of our
APRNet against the state-of-the-arts (SOTAs).

</details>


### [6] [FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm](https://arxiv.org/abs/2508.09056)
*Shreya Ghosh,Abu Shafin Mohammad Mahdee Jameel,Aly El Gamal*

Main category: cs.LG

TL;DR: FetFIDS是一种基于特征嵌入的深度学习系统，用于提升基于Transformer的入侵检测性能，特别适合联邦学习环境。


<details>
  <summary>Details</summary>
Motivation: 近年来，深度学习显著提升了入侵检测系统（IDS）的性能，但如何在联邦学习环境中进一步优化性能并保护隐私仍是一个挑战。

Method: FetFIDS采用特征嵌入而非位置嵌入，结合Transformer架构，专为联邦学习环境设计，支持多轮通信以优化本地性能。

Result: FetFIDS在联邦学习环境中表现优于现有入侵检测系统，并显示出对联邦学习的高度适应性。

Conclusion: FetFIDS通过特征嵌入和联邦学习的结合，显著提升了入侵检测性能，同时保护了隐私。

Abstract: Intrusion Detection Systems (IDS) have an increasingly important role in
preventing exploitation of network vulnerabilities by malicious actors. Recent
deep learning based developments have resulted in significant improvements in
the performance of IDS systems. In this paper, we present FetFIDS, where we
explore the employment of feature embedding instead of positional embedding to
improve intrusion detection performance of a transformer based deep learning
system. Our model is developed with the aim of deployments in edge learning
scenarios, where federated learning over multiple communication rounds can
ensure both privacy and localized performance improvements. FetFIDS outperforms
multiple state-of-the-art intrusion detection systems in a federated
environment and demonstrates a high degree of suitability to federated
learning. The code for this work can be found at
https://github.com/ghosh64/fetfids.

</details>


### [7] [Causal Machine Learning for Patient-Level Intraoperative Opioid Dose Prediction from Electronic Health Records](https://arxiv.org/abs/2508.09059)
*Jonas Valbjørn Andersena,Anders Peder Højer Karlsen,Markus Harboe Olsen,Nikolaj Krebs Pedersen*

Main category: cs.LG

TL;DR: OPIAID算法是一种基于机器学习的个性化阿片类药物剂量预测与推荐方法，旨在优化疼痛管理并减少不良反应。


<details>
  <summary>Details</summary>
Motivation: 解决阿片类药物剂量个性化推荐问题，以平衡疼痛管理和不良反应的最小化。

Method: 利用观察性电子健康记录数据训练机器学习模型，采用因果机器学习方法分析剂量、患者特征与结果的关系。

Result: 算法能够根据患者特征和阿片类药物类型提供个性化剂量推荐。

Conclusion: OPIAID算法为个性化阿片类药物剂量管理提供了有效工具，需进一步验证其性能。

Abstract: This paper introduces the OPIAID algorithm, a novel approach for predicting
and recommending personalized opioid dosages for individual patients. The
algorithm optimizes pain management while minimizing opioid related adverse
events (ORADE) by employing machine learning models trained on observational
electronic health records (EHR) data. It leverages a causal machine learning
approach to understand the relationship between opioid dose, case specific
patient and intraoperative characteristics, and pain versus ORADE outcomes. The
OPIAID algorithm considers patient-specific characteristics and the influence
of different opiates, enabling personalized dose recommendations. This paper
outlines the algorithm's methodology and architecture, and discusses key
assumptions, and approaches to evaluating its performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games](https://arxiv.org/abs/2508.08501)
*Yuchen Li,Cong Lin,Muhammad Umair Nasir,Philip Bontrager,Jialin Liu,Julian Togelius*

Main category: cs.AI

TL;DR: GVGAI-LLM是一个基于视频游戏的基准测试，用于评估大型语言模型（LLMs）的推理和问题解决能力，揭示了其在空间推理和基础规划方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试大多未能涵盖多样化的任务需求，GVGAI-LLM旨在填补这一空白，通过游戏场景测试模型的通用能力。

Method: 利用游戏描述语言快速生成新游戏和关卡，避免过拟合；通过ASCII字符表示游戏场景，便于语言模型处理；定义可解释的评估指标。

Result: 零样本评估显示LLMs在空间推理和基础规划方面存在明显缺陷，结构化提示和空间接地技术仅带来部分改进。

Conclusion: GVGAI-LLM为语言模型能力研究提供了可复现的测试平台，尤其关注代理行为和上下文推理。

Abstract: We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning
and problem-solving capabilities of large language models (LLMs). Built on the
General Video Game AI framework, it features a diverse collection of
arcade-style games designed to test a model's ability to handle tasks that
differ from most existing LLM benchmarks. The benchmark leverages a game
description language that enables rapid creation of new games and levels,
helping to prevent overfitting over time. Each game scene is represented by a
compact set of ASCII characters, allowing for efficient processing by language
models. GVGAI-LLM defines interpretable metrics, including the meaningful step
ratio, step efficiency, and overall score, to assess model behavior. Through
zero-shot evaluations across a broad set of games and levels with diverse
challenges and skill depth, we reveal persistent limitations of LLMs in spatial
reasoning and basic planning. Current models consistently exhibit spatial and
logical errors, motivating structured prompting and spatial grounding
techniques. While these interventions lead to partial improvements, the
benchmark remains very far from solved. GVGAI-LLM provides a reproducible
testbed for advancing research on language model capabilities, with a
particular emphasis on agentic behavior and contextual reasoning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Real-time News Story Identification](https://arxiv.org/abs/2508.08272)
*Tadej Škvorc,Nikola Ivačič,Sebastjan Hribar,Marko Robnik-Šikonja*

Main category: cs.CL

TL;DR: 提出了一种实时新闻故事识别方法，结合文本表示、聚类和在线主题建模技术，用于将新闻文章分配到特定故事中。


<details>
  <summary>Details</summary>
Motivation: 提升新闻阅读体验，通过自动将新闻文章归类到特定故事中，帮助用户更高效地获取信息。

Method: 结合文本表示技术、聚类算法和在线主题建模方法（如BERTopic、DBStream、TextClust），提取特定事件和命名实体以实现故事识别。

Result: 在斯洛文尼亚媒体的一个月新闻数据集上验证，结果显示该方法能产生合理的结果。

Conclusion: 提出的实时故事识别方法有效，能够满足新闻监控系统的需求。

Abstract: To improve the reading experience, many news sites organize news into topical
collections, called stories. In this work, we present an approach for
implementing real-time story identification for a news monitoring system that
automatically collects news articles as they appear online and processes them
in various ways. Story identification aims to assign each news article to a
specific story that the article is covering. The process is similar to text
clustering and topic modeling, but requires that articles be grouped based on
particular events, places, and people, rather than general text similarity (as
in clustering) or general (predefined) topics (as in topic modeling). We
present an approach to story identification that is capable of functioning in
real time, assigning articles to stories as they are published online. In the
proposed approach, we combine text representation techniques, clustering
algorithms, and online topic modeling methods. We combine various text
representation methods to extract specific events and named entities necessary
for story identification, showing that a mixture of online topic-modeling
approaches such as BERTopic, DBStream, and TextClust can be adapted for story
discovery. We evaluate our approach on a news dataset from Slovene media
covering a period of 1 month. We show that our real-time approach produces
sensible results as judged by human evaluators.

</details>


### [10] [Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering](https://arxiv.org/abs/2508.08785)
*Yunfeng Ning,Mayi Xu,Jintao Wen,Qiankun Pi,Yuanyuan Zhu,Ming Zhong,Jiawei Jiang,Tieyun Qian*

Main category: cs.CL

TL;DR: 论文提出了一种隐私保护的RAG框架ARoG，通过关系中心抽象和结构导向抽象策略解决匿名实体检索问题，实验证明其高效性和隐私鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在RAG系统中因匿名实体导致的语义缺失和隐私风险问题。

Method: 提出ARoG框架，包含关系中心抽象和结构导向抽象策略，将匿名实体转化为可检索信息并优化检索过程。

Result: 在三个数据集上验证了ARoG的高效性和隐私保护能力。

Conclusion: ARoG在保护隐私的同时，有效提升了RAG系统的检索性能。

Abstract: LLMs often suffer from hallucinations and outdated or incomplete knowledge.
RAG is proposed to address these issues by integrating external knowledge like
that in KGs into LLMs. However, leveraging private KGs in RAG systems poses
significant privacy risks due to the black-box nature of LLMs and potential
insecure data transmission, especially when using third-party LLM APIs lacking
transparency and control. In this paper, we investigate the privacy-protected
RAG scenario for the first time, where entities in KGs are anonymous for LLMs,
thus preventing them from accessing entity semantics. Due to the loss of
semantics of entities, previous RAG systems cannot retrieve question-relevant
knowledge from KGs by matching questions with the meaningless identifiers of
anonymous entities. To realize an effective RAG system in this scenario, two
key challenges must be addressed: (1) How can anonymous entities be converted
into retrievable information. (2) How to retrieve question-relevant anonymous
entities. Hence, we propose a novel ARoG framework including relation-centric
abstraction and structure-oriented abstraction strategies. For challenge (1),
the first strategy abstracts entities into high-level concepts by dynamically
capturing the semantics of their adjacent relations. It supplements meaningful
semantics which can further support the retrieval process. For challenge (2),
the second strategy transforms unstructured natural language questions into
structured abstract concept paths. These paths can be more effectively aligned
with the abstracted concepts in KGs, thereby improving retrieval performance.
To guide LLMs to effectively retrieve knowledge from KGs, the two strategies
strictly protect privacy from being exposed to LLMs. Experiments on three
datasets demonstrate that ARoG achieves strong performance and
privacy-robustness.

</details>


### [11] [LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA](https://arxiv.org/abs/2508.09012)
*Adrián Gude,Roi Santos-Ríos,Francisco Prado-Valiño,Ana Ezquerro,Jesús Vilares*

Main category: cs.CL

TL;DR: 本文介绍了在SemEval 2025任务8中的参与情况，提出了一种基于大型语言模型的零样本代码生成方法，用于表格问答任务。


<details>
  <summary>Details</summary>
Motivation: 探索零样本代码生成在表格问答任务中的有效性，无需任务特定微调。

Method: 采用模块化流程，包括代码生成模块、列选择模块和数据类型分析模块，并通过迭代优化提高鲁棒性。

Result: 在测试阶段排名33/53，证明了零样本方法的可行性。

Conclusion: 零样本代码生成是表格问答任务的一种有效方法，具有潜力。

Abstract: This paper describes our participation in SemEval 2025 Task 8, focused on
Tabular Question Answering. We developed a zero-shot pipeline that leverages an
Large Language Model to generate functional code capable of extracting the
relevant information from tabular data based on an input question. Our approach
consists of a modular pipeline where the main code generator module is
supported by additional components that identify the most relevant columns and
analyze their data types to improve extraction accuracy. In the event that the
generated code fails, an iterative refinement process is triggered,
incorporating the error feedback into a new generation prompt to enhance
robustness. Our results show that zero-shot code generation is a valid approach
for Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of
task-specific fine-tuning.

</details>


### [12] [Link Prediction for Event Logs in the Process Industry](https://arxiv.org/abs/2508.09096)
*Anastasia Zhukova,Thomas Walton,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 论文提出了一种基于记录链接（RL）的方法，通过将问题建模为跨文档共指消解（CDCR）任务，并结合自然语言推理（NLI）和语义文本相似度（STS），以解决流程工业中事件日志碎片化的问题。


<details>
  <summary>Details</summary>
Motivation: 流程工业中的知识管理（KM）因事件日志的碎片化而受限，导致相关记录（如设备问题及解决方案）无法有效关联，阻碍了解决方案的推荐。

Method: 将记录链接（RL）问题建模为跨文档共指消解（CDCR）任务，结合NLI和STS，并引入因果推理（CI）进行优化。

Result: 提出的RL模型在性能上优于基于NLI和STS的基线方法，分别提升了28%（11.43分）和27%（11.21分）。

Conclusion: 通过领域适配和增强推理能力，CDCR模型可有效应用于流程工业，提升数据质量和日志记录的关联性。

Abstract: Knowledge management (KM) is vital in the process industry for optimizing
operations, ensuring safety, and enabling continuous improvement through
effective use of operational data and past insights. A key challenge in this
domain is the fragmented nature of event logs in shift books, where related
records, e.g., entries documenting issues related to equipment or processes and
the corresponding solutions, may remain disconnected. This fragmentation
hinders the recommendation of previous solutions to the users. To address this
problem, we investigate record linking (RL) as link prediction, commonly
studied in graph-based machine learning, by framing it as a cross-document
coreference resolution (CDCR) task enhanced with natural language inference
(NLI) and semantic text similarity (STS) by shifting it into the causal
inference (CI). We adapt CDCR, traditionally applied in the news domain, into
an RL model to operate at the passage level, similar to NLI and STS, while
accommodating the process industry's specific text formats, which contain
unstructured text and structured record attributes. Our RL model outperformed
the best versions of NLI- and STS-driven baselines by 28% (11.43 points) and
27% (11.21 points), respectively. Our work demonstrates how domain adaptation
of the state-of-the-art CDCR models, enhanced with reasoning capabilities, can
be effectively tailored to the process industry, improving data quality and
connectivity in shift logs.

</details>
