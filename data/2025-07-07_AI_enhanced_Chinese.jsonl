{"id": "2507.02200", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02200", "abs": "https://arxiv.org/abs/2507.02200", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6d41\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b\u6846\u67b6ESTR-CoT\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u589e\u5f3a\u8bc6\u522b\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u91ca\u6027\u548c\u903b\u8f91\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u3001\u5feb\u901f\u8fd0\u52a8\u7b49\u6781\u7aef\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528EVA-CLIP\u89c6\u89c9\u7f16\u7801\u5668\u548cVicuna-7B\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408Q-former\u5bf9\u9f50\u89c6\u89c9\u4e0e\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u89e3\u91ca\u6027\u3002", "conclusion": "ESTR-CoT\u5728\u4e8b\u4ef6\u6d41\u573a\u666f\u6587\u672c\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2507.02217", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02217", "abs": "https://arxiv.org/abs/2507.02217", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "title": "Understanding Trade offs When Conditioning Synthetic Data", "comment": null, "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5c11\u91cf\u56fe\u50cf\u6570\u636e\u4e0b\u5b66\u4e60\u9c81\u68d2\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u6311\u6218\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u7b56\u7565\uff08\u63d0\u793a\u548c\u5e03\u5c40\uff09\uff0c\u53d1\u73b0\u5e03\u5c40\u6761\u4ef6\u5728\u591a\u6837\u6027\u9ad8\u65f6\u66f4\u4f18\uff0c\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5de5\u4e1a\u89c6\u89c9\u7cfb\u7edf\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u8017\u65f6\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u5173\u952e\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u6162\u4e14\u4eff\u771f\u4e0e\u771f\u5b9e\u5dee\u8ddd\u5927\u3002\u6269\u6563\u6a21\u578b\u867d\u9ad8\u6548\uff0c\u4f46\u7cbe\u786e\u63a7\u5236\u56f0\u96be\u3002", "method": "\u7814\u7a7680\u4e2a\u89c6\u89c9\u6982\u5ff5\uff0c\u6bd4\u8f83\u63d0\u793a\u6761\u4ef6\u548c\u5e03\u5c40\u6761\u4ef6\u4e24\u79cd\u7b56\u7565\uff0c\u5206\u6790\u5176\u5bf9\u5408\u6210\u6570\u636e\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5e03\u5c40\u6761\u4ef6\u5728\u591a\u6837\u6027\u9ad8\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u5339\u914d\u5b8c\u6574\u8bad\u7ec3\u5206\u5e03\u65f6\uff0c\u5408\u6210\u6570\u636e\u4f7f\u5e73\u5747\u7cbe\u5ea6\u63d0\u534734%\uff0c\u6700\u9ad8\u8fbe177%\u3002", "conclusion": "\u5e03\u5c40\u6761\u4ef6\u5728\u591a\u6837\u6027\u9ad8\u65f6\u4f18\u4e8e\u63d0\u793a\u6761\u4ef6\uff0c\u5408\u6210\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.02197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02197", "abs": "https://arxiv.org/abs/2507.02197", "authors": ["Amogh Mannekote", "Adam Davies", "Guohao Li", "Kristy Elizabeth Boyer", "ChengXiang Zhai", "Bonnie J Dorr", "Francesco Pinto"], "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust", "comment": null, "summary": "As LLMs are increasingly studied as role-playing agents to generate synthetic\ndata for human behavioral research, ensuring that their outputs remain coherent\nwith their assigned roles has become a critical concern. In this paper, we\ninvestigate how consistently LLM-based role-playing agents' stated beliefs\nabout the behavior of the people they are asked to role-play (\"what they say\")\ncorrespond to their actual behavior during role-play (\"how they act\").\nSpecifically, we establish an evaluation framework to rigorously measure how\nwell beliefs obtained by prompting the model can predict simulation outcomes in\nadvance. Using an augmented version of the GenAgents persona bank and the Trust\nGame (a standard economic game used to quantify players' trust and\nreciprocity), we introduce a belief-behavior consistency metric to\nsystematically investigate how it is affected by factors such as: (1) the types\nof beliefs we elicit from LLMs, like expected outcomes of simulations versus\ntask-relevant attributes of individual characters LLMs are asked to simulate;\n(2) when and how we present LLMs with relevant information about Trust Game;\nand (3) how far into the future we ask the model to forecast its actions. We\nalso explore how feasible it is to impose a researcher's own theoretical priors\nin the event that the originally elicited beliefs are misaligned with research\nobjectives. Our results reveal systematic inconsistencies between LLMs' stated\n(or imposed) beliefs and the outcomes of their role-playing simulation, at both\nan individual- and population-level. Specifically, we find that, even when\nmodels appear to encode plausible beliefs, they may fail to apply them in a\nconsistent way. These findings highlight the need to identify how and when\nLLMs' stated beliefs align with their simulated behavior, allowing researchers\nto use LLM-based agents appropriately in behavioral studies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eLLM\u7684\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u5728\u751f\u6210\u5408\u6210\u6570\u636e\u65f6\uff0c\u5176\u9648\u8ff0\u7684\u4fe1\u5ff5\u4e0e\u5b9e\u9645\u884c\u4e3a\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u4e00\u81f4\u6027\u5ea6\u91cf\u3002", "motivation": "\u968f\u7740LLM\u4f5c\u4e3a\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u7528\u4e8e\u4eba\u7c7b\u884c\u4e3a\u7814\u7a76\uff0c\u786e\u4fdd\u5176\u8f93\u51fa\u4e0e\u89d2\u8272\u4e00\u81f4\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u589e\u5f3a\u7248GenAgents\u89d2\u8272\u5e93\u548c\u4fe1\u4efb\u6e38\u620f\uff0c\u5f15\u5165\u4fe1\u5ff5-\u884c\u4e3a\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u5206\u6790\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u53d1\u73b0LLM\u7684\u9648\u8ff0\u4fe1\u5ff5\u4e0e\u6a21\u62df\u884c\u4e3a\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\uff0c\u5373\u4f7f\u4fe1\u5ff5\u770b\u4f3c\u5408\u7406\uff0c\u4e5f\u53ef\u80fd\u65e0\u6cd5\u4e00\u81f4\u5e94\u7528\u3002", "conclusion": "\u9700\u660e\u786eLLM\u4fe1\u5ff5\u4e0e\u884c\u4e3a\u4f55\u65f6\u4e00\u81f4\uff0c\u4ee5\u5728\u884c\u4e3a\u7814\u7a76\u4e2d\u6b63\u786e\u4f7f\u7528LLM\u4ee3\u7406\u3002"}}
{"id": "2507.02270", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02270", "abs": "https://arxiv.org/abs/2507.02270", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "comment": "Accepted by IEEE SMC 2025", "summary": "Enhancing underwater images is crucial for exploration. These images face\nvisibility and color issues due to light changes, water turbidity, and bubbles.\nTraditional prior-based methods and pixel-based methods often fail, while deep\nlearning lacks sufficient high-quality datasets. We introduce the Multi-Axis\nConditional Lookup (MAC-Lookup) model, which enhances visual quality by\nimproving color accuracy, sharpness, and contrast. It includes Conditional 3D\nLookup Table Color Correction (CLTCC) for preliminary color and quality\ncorrection and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.\nThis model prevents over-enhancement and saturation while handling underwater\nchallenges. Extensive experiments show that MAC-Lookup excels in enhancing\nunderwater images by restoring details and colors better than existing methods.\nThe code is https://github.com/onlycatdoraemon/MAC-Lookup.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAC-Lookup\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u989c\u8272\u51c6\u786e\u6027\u3001\u6e05\u6670\u5ea6\u548c\u5bf9\u6bd4\u5ea6\u6765\u589e\u5f3a\u6c34\u4e0b\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u56e0\u5149\u7ebf\u53d8\u5316\u3001\u6c34\u4f53\u6d51\u6d4a\u548c\u6c14\u6ce1\u7b49\u95ee\u9898\u5b58\u5728\u53ef\u89c1\u6027\u548c\u989c\u8272\u5931\u771f\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u6df1\u5ea6\u5b66\u4e60\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u591a\u8f74\u6761\u4ef6\u67e5\u627e\uff08MAC-Lookup\uff09\u6a21\u578b\uff0c\u5305\u62ec\u6761\u4ef63D\u67e5\u627e\u8868\u989c\u8272\u6821\u6b63\uff08CLTCC\uff09\u548c\u591a\u8f74\u81ea\u9002\u5e94\u589e\u5f3a\uff08MAAE\uff09\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAC-Lookup\u5728\u6062\u590d\u7ec6\u8282\u548c\u989c\u8272\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAC-Lookup\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u95ee\u9898\uff0c\u907f\u514d\u4e86\u8fc7\u5ea6\u589e\u5f3a\u548c\u9971\u548c\u73b0\u8c61\u3002"}}
{"id": "2507.02227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02227", "abs": "https://arxiv.org/abs/2507.02227", "authors": ["Xinquan Huang", "Paris Perdikaris"], "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations", "comment": null, "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.", "AI": {"tldr": "PhysicsCorrect\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8ePDE\u6b8b\u5dee\u7684\u7ebf\u6027\u5316\u9006\u95ee\u9898\uff0c\u5728\u6bcf\u4e00\u6b65\u9884\u6d4b\u4e2d\u5f3a\u5236PDE\u4e00\u81f4\u6027\uff0c\u663e\u8457\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3aPDE\u6c42\u89e3\u5668\u7684\u66ff\u4ee3\u65b9\u6cd5\u5b58\u5728\u957f\u671f\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7ed3\u679c\u504f\u79bb\u7269\u7406\u6709\u6548\u89e3\u3002", "method": "\u63d0\u51faPhysicsCorrect\u6846\u67b6\uff0c\u5229\u7528\u79bb\u7ebf\u9884\u8ba1\u7b97Jacobian\u53ca\u5176\u4f2a\u9006\u7684\u9ad8\u6548\u7f13\u5b58\u7b56\u7565\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728Navier-Stokes\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u6ce2\u52a8\u65b9\u7a0b\u548c\u6df7\u6c8cKuramoto-Sivashinsky\u65b9\u7a0b\u4e2d\uff0c\u9884\u6d4b\u8bef\u5dee\u51cf\u5c11\u9ad8\u8fbe100\u500d\uff0c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u4e0d\u52305%\u3002", "conclusion": "PhysicsCorrect\u5c06\u4e0d\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u5de5\u5177\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u4eff\u771f\u5de5\u5177\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u6548\u8ba1\u7b97\u548c\u7269\u7406\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2507.02652", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.02652", "abs": "https://arxiv.org/abs/2507.02652", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yang Zhao", "Hongjin Qian", "Zhicheng Dou"], "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search", "comment": "9 pages", "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.", "AI": {"tldr": "HiRA\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u641c\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u590d\u6742\u4fe1\u606f\u9700\u6c42\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u5355\u4e00\u6a21\u578b\u540c\u65f6\u5904\u7406\u9ad8\u5c42\u89c4\u5212\u548c\u7ec6\u8282\u6267\u884c\u3002", "method": "HiRA\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u7531\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u673a\u5236\u534f\u8c03\u7ed3\u679c\u3002", "result": "\u5728\u56db\u4e2a\u590d\u6742\u8de8\u6a21\u6001\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHiRA\u663e\u8457\u4f18\u4e8e\u73b0\u6709RAG\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u7cfb\u7edf\u3002", "conclusion": "\u5206\u5c42\u89c4\u5212\u4e0e\u6267\u884c\u5206\u79bb\u6709\u6548\u63d0\u5347\u4e86\u591a\u6b65\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.02703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02703", "abs": "https://arxiv.org/abs/2507.02703", "authors": ["Robin Schm\u00f6cker", "Lennart Kampmann", "Alexander Dockhorn"], "title": "Time-critical and confidence-based abstraction dropping methods", "comment": "Accepted for Publication at the IEEE Conference on Games 2025", "summary": "One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and\nuse state and/or action abstractions during the tree search. Non-exact\nabstractions, however, introduce an approximation error making convergence to\nthe optimal action in the abstract space impossible. Hence, as proposed as a\ncomponent of Elastic Monte Carlo Tree Search by Xu et al., abstraction\nalgorithms should eventually drop the abstraction. In this paper, we propose\ntwo novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can\nyield clear performance improvements whilst being safe in the sense that the\ndropping never causes any notable performance degradations contrary to Xu's\ndropping method. OGA-IAAD is designed for time critical settings while OGA-CAD\nis designed to improve the MCTS performance with the same number of iterations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u62bd\u8c61\u4e22\u5f03\u65b9\u6848\uff08OGA-IAAD\u548cOGA-CAD\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6027\u80fd\uff0c\u4e14\u4e0d\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u975e\u7cbe\u786e\u62bd\u8c61\u5728MCTS\u4e2d\u5f15\u5165\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6536\u655b\u5230\u6700\u4f18\u52a8\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u5b89\u5168\u7684\u62bd\u8c61\u4e22\u5f03\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u62bd\u8c61\u4e22\u5f03\u65b9\u6848\uff1aOGA-IAAD\uff08\u9002\u7528\u4e8e\u65f6\u95f4\u5173\u952e\u573a\u666f\uff09\u548cOGA-CAD\uff08\u65e8\u5728\u76f8\u540c\u8fed\u4ee3\u6b21\u6570\u4e0b\u63d0\u5347MCTS\u6027\u80fd\uff09\u3002", "result": "\u4e24\u79cd\u65b9\u6848\u5747\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u4e0d\u4f1a\u5f15\u8d77\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "OGA-IAAD\u548cOGA-CAD\u662f\u5b89\u5168\u4e14\u6709\u6548\u7684\u62bd\u8c61\u4e22\u5f03\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002"}}
{"id": "2507.02354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02354", "abs": "https://arxiv.org/abs/2507.02354", "authors": ["Fei Yuhuan", "Wang Gengchen", "Liu Fenghao", "Zang Ran", "Sun Xufei", "Chang Hao"], "title": "Lightweight Shrimp Disease Detection Research Based on YOLOv8n", "comment": "in Chinese language", "summary": "Shrimp diseases are one of the primary causes of economic losses in shrimp\naquaculture. To prevent disease transmission and enhance intelligent detection\nefficiency in shrimp farming, this paper proposes a lightweight network\narchitecture based on YOLOv8n. First, by designing the RLDD detection head and\nC2f-EMCM module, the model reduces computational complexity while maintaining\ndetection accuracy, improving computational efficiency. Subsequently, an\nimproved SegNext_Attention self-attention mechanism is introduced to further\nenhance the model's feature extraction capability, enabling more precise\nidentification of disease characteristics. Extensive experiments, including\nablation studies and comparative evaluations, are conducted on a\nself-constructed shrimp disease dataset, with generalization tests extended to\nthe URPC2020 dataset. Results demonstrate that the proposed model achieves a\n32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5\nof 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms\nother lightweight YOLO-series models in mAP@0.5, parameter count, and model\nsize. Generalization experiments on the URPC2020 dataset further validate the\nmodel's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The\nproposed method achieves an optimal balance between accuracy and efficiency,\nproviding reliable technical support for intelligent disease detection in\nshrimp aquaculture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8n\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u867e\u75c5\u667a\u80fd\u68c0\u6d4b\uff0c\u901a\u8fc7\u4f18\u5316\u68c0\u6d4b\u5934\u548c\u5f15\u5165\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u867e\u75c5\u662f\u867e\u517b\u6b96\u4e2d\u7ecf\u6d4e\u635f\u5931\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u4e9f\u9700\u9ad8\u6548\u667a\u80fd\u7684\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u51cf\u5c11\u75be\u75c5\u4f20\u64ad\u3002", "method": "\u8bbe\u8ba1\u4e86RLDD\u68c0\u6d4b\u5934\u548cC2f-EMCM\u6a21\u5757\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u7684SegNext_Attention\u81ea\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u6a21\u578b\u53c2\u6570\u51cf\u5c1132.3%\uff0cmAP@0.5\u8fbe92.7%\uff08\u6bd4YOLOv8n\u63d0\u53473%\uff09\uff0c\u5e76\u5728\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u867e\u75c5\u667a\u80fd\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.02416", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.02416", "abs": "https://arxiv.org/abs/2507.02416", "authors": ["Subhasis Dasgupta", "Jaydip Sen", "Tuhina Halder"], "title": "Determination Of Structural Cracks Using Deep Learning Frameworks", "comment": "This is the accepted version of the paper presented in IEEE CONIT\n  2025 held on 20th June 2025. This is not the camera-ready version. There are\n  6 pages in this paper and it contains 7 figures and 1 table", "summary": "Structural crack detection is a critical task for public safety as it helps\nin preventing potential structural failures that could endanger lives. Manual\ndetection by inexperienced personnel can be slow, inconsistent, and prone to\nhuman error, which may compromise the reliability of assessments. The current\nstudy addresses these challenges by introducing a novel deep-learning\narchitecture designed to enhance the accuracy and efficiency of structural\ncrack detection. In this research, various configurations of residual U-Net\nmodels were utilized. These models, due to their robustness in capturing fine\ndetails, were further integrated into an ensemble with a meta-model comprising\nconvolutional blocks. This unique combination aimed to boost prediction\nefficiency beyond what individual models could achieve. The ensemble's\nperformance was evaluated against well-established architectures such as SegNet\nand the traditional U-Net. Results demonstrated that the residual U-Net models\noutperformed their predecessors, particularly with low-resolution imagery, and\nthe ensemble model exceeded the performance of individual models, proving it as\nthe most effective. The assessment was based on the Intersection over Union\n(IoU) metric and DICE coefficient. The ensemble model achieved the highest\nscores, signifying superior accuracy. This advancement suggests way for more\nreliable automated systems in structural defects monitoring tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6b8b\u5deeU-Net\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u7ed3\u6784\u88c2\u7f1d\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u624b\u52a8\u68c0\u6d4b\u7ed3\u6784\u88c2\u7f1d\u5b58\u5728\u6548\u7387\u4f4e\u3001\u4e00\u81f4\u6027\u5dee\u548c\u4eba\u4e3a\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6b8b\u5deeU-Net\u6a21\u578b\u7684\u4e0d\u540c\u914d\u7f6e\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u4e00\u4e2a\u5305\u542b\u5377\u79ef\u5757\u7684\u5143\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u6548\u7387\u3002", "result": "\u96c6\u6210\u6a21\u578b\u5728IoU\u548cDICE\u7cfb\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u4f18\u4e8eSegNet\u548c\u4f20\u7edfU-Net\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ed3\u6784\u7f3a\u9677\u76d1\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02619", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02619", "abs": "https://arxiv.org/abs/2507.02619", "authors": ["Hazal Mogultay Ozcan", "Sinan Kalkan", "Fatos T. Yarman-Vural"], "title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation", "comment": "The paper is under revision at Machine Vision and Applications", "summary": "In this paper, we propose a novel model called Learnable VAE (L-VAE), which\nlearns a disentangled representation together with the hyperparameters of the\ncost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein\nthe hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the\nlimitations of \\b{eta}-VAE by learning the relative weights of the terms in the\nloss function to control the dynamic trade-off between disentanglement and\nreconstruction losses. In the proposed model, the weight of the loss terms and\nthe parameters of the model architecture are learned concurrently. An\nadditional regularization term is added to the loss function to prevent bias\ntowards either reconstruction or disentanglement losses. Experimental analyses\nshow that the proposed L-VAE finds an effective balance between reconstruction\nfidelity and disentangling the latent dimensions. Comparisons of the proposed\nL-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on\ndatasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that\nL-VAE consistently provides the best or the second best performances measured\nby a set of disentanglement metrics. Moreover, qualitative experiments on\nCelebA dataset, confirm the success of the L-VAE model for disentangling the\nfacial attributes.", "AI": {"tldr": "L-VAE\u662f\u4e00\u79cd\u53ef\u5b66\u4e60\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\uff0c\u6709\u6548\u5e73\u8861\u4e86\u89e3\u7f20\u548c\u91cd\u6784\u635f\u5931\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u03b2-VAE\u4e2d\u8d85\u53c2\u6570\u03b7\u9700\u624b\u52a8\u8c03\u6574\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u52a8\u6001\u6743\u8861\u89e3\u7f20\u4e0e\u91cd\u6784\u635f\u5931\u3002", "method": "\u63d0\u51faL-VAE\u6a21\u578b\uff0c\u5b66\u4e60\u635f\u5931\u51fd\u6570\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u6dfb\u52a0\u6b63\u5219\u5316\u9879\u9632\u6b62\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u8861\u4e86\u89e3\u7f20\u548c\u91cd\u6784\u6027\u80fd\uff0c\u5b9a\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "L-VAE\u5728\u89e3\u7f20\u8868\u793a\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.02479", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02479", "abs": "https://arxiv.org/abs/2507.02479", "authors": ["Teng Fu", "Yuwen Chen", "Zhuofan Chen", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios", "comment": null, "summary": "Multi-object tracking is a classic field in computer vision. Among them,\npedestrian tracking has extremely high application value and has become the\nmost popular research category. Existing methods mainly use motion or\nappearance information for tracking, which is often difficult in complex\nscenarios. For the motion information, mutual occlusions between objects often\nprevent updating of the motion state; for the appearance information,\nnon-robust results are often obtained due to reasons such as only partial\nvisibility of the object or blurred images. Although learning how to perform\ntracking in these situations from the annotated data is the simplest solution,\nthe existing MOT dataset fails to satisfy this solution. Existing methods\nmainly have two drawbacks: relatively simple scene composition and\nnon-realistic scenarios. Although some of the video sequences in existing\ndataset do not have the above-mentioned drawbacks, the number is far from\nadequate for research purposes. To this end, we propose a difficult large-scale\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\nthere are numerous objects in most of the sequences. Our dataset consists of 33\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\na complete bounding box and a unique object ID. The dataset will provide a\nplatform to facilitate the development of algorithms that remain effective in\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\nSOTA models on our dataset. Besides, we analyzed the performance of the\nfoundation models on our dataset. The dataset and project code is released at:\nhttps://github.com/loseevaya/CrowdTrack .", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aCrowdTrack\u7684\u5927\u89c4\u6a21\u590d\u6742\u573a\u666f\u591a\u884c\u4eba\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u5f25\u8865\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u591a\u884c\u4eba\u8ddf\u8e2a\u6570\u636e\u96c6\u573a\u666f\u7b80\u5355\u4e14\u4e0d\u771f\u5b9e\uff0c\u96be\u4ee5\u652f\u6301\u590d\u6742\u573a\u666f\u4e0b\u7684\u7b97\u6cd5\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b33\u4e2a\u89c6\u9891\u30015,185\u6761\u8f68\u8ff9\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u5b8c\u6574\u8fb9\u754c\u6846\u548c\u552f\u4e00ID\u3002", "result": "\u6570\u636e\u96c6\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "CrowdTrack\u6570\u636e\u96c6\u586b\u8865\u4e86\u590d\u6742\u573a\u666f\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u591a\u884c\u4eba\u8ddf\u8e2a\u7b97\u6cd5\u7684\u7814\u7a76\u3002"}}
{"id": "2507.02687", "categories": ["cs.CV", "cs.AI", "60J60, 68T07", "I.2.6; I.2.10; I.4.9"], "pdf": "https://arxiv.org/pdf/2507.02687", "abs": "https://arxiv.org/abs/2507.02687", "authors": ["JungWoo Chae", "Jiyoon Kim", "JaeWoong Choi", "Kyungyul Kim", "Sangheum Hwang"], "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data", "comment": "CVPR 2025 camera ready. Project page: https://lgcnsai.github.io/apt", "summary": "Personalizing diffusion models using limited data presents significant\nchallenges, including overfitting, loss of prior knowledge, and degradation of\ntext alignment. Overfitting leads to shifts in the noise prediction\ndistribution, disrupting the denoising trajectory and causing the model to lose\nsemantic coherence. In this paper, we propose Adaptive Personalized Training\n(APT), a novel framework that mitigates overfitting by employing adaptive\ntraining strategies and regularizing the model's internal representations\nduring fine-tuning. APT consists of three key components: (1) Adaptive Training\nAdjustment, which introduces an overfitting indicator to detect the degree of\noverfitting at each time step bin and applies adaptive data augmentation and\nadaptive loss weighting based on this indicator; (2)Representation\nStabilization, which regularizes the mean and variance of intermediate feature\nmaps to prevent excessive shifts in noise prediction; and (3) Attention\nAlignment for Prior Knowledge Preservation, which aligns the cross-attention\nmaps of the fine-tuned model with those of the pretrained model to maintain\nprior knowledge and semantic coherence. Through extensive experiments, we\ndemonstrate that APT effectively mitigates overfitting, preserves prior\nknowledge, and outperforms existing methods in generating high-quality, diverse\nimages with limited reference data.", "AI": {"tldr": "APT\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\u548c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u4e2a\u6027\u5316\u8bad\u7ec3\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5148\u9a8c\u77e5\u8bc6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u4e2a\u6027\u5316\u8bad\u7ec3\u65f6\u51fa\u73b0\u7684\u8fc7\u62df\u5408\u3001\u5148\u9a8c\u77e5\u8bc6\u4e22\u5931\u548c\u6587\u672c\u5bf9\u9f50\u9000\u5316\u95ee\u9898\u3002", "method": "APT\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u8bad\u7ec3\u8c03\u6574\u3001\u8868\u793a\u7a33\u5b9a\u5316\u548c\u6ce8\u610f\u529b\u5bf9\u9f50\uff0c\u5206\u522b\u901a\u8fc7\u68c0\u6d4b\u8fc7\u62df\u5408\u3001\u6b63\u5219\u5316\u7279\u5f81\u56fe\u548c\u4fdd\u6301\u6ce8\u610f\u529b\u56fe\u4e00\u81f4\u6027\u6765\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAPT\u80fd\u6709\u6548\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u4fdd\u6301\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5728\u6709\u9650\u53c2\u8003\u6570\u636e\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u56fe\u50cf\u3002", "conclusion": "APT\u4e3a\u6709\u9650\u6570\u636e\u4e0b\u7684\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2507.02807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02807", "abs": "https://arxiv.org/abs/2507.02807", "authors": ["Thiti Suttaket", "Stanley Kok"], "title": "In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization", "comment": null, "summary": "Survival analysis is an important problem in healthcare because it models the\nrelationship between an individual's covariates and the onset time of an event\nof interest (e.g., death). It is important for survival models to be\nwell-calibrated (i.e., for their predicted probabilities to be close to\nground-truth probabilities) because badly calibrated systems can result in\nerroneous clinical decisions. Existing survival models are typically calibrated\nat the population level only, and thus run the risk of being poorly calibrated\nfor one or more minority subpopulations. We propose a model called GRADUATE\nthat achieves multicalibration by ensuring that all subpopulations are\nwell-calibrated too. GRADUATE frames multicalibration as a constrained\noptimization problem, and optimizes both calibration and discrimination\nin-training to achieve a good balance between them. We mathematically prove\nthat the optimization method used yields a solution that is both near-optimal\nand feasible with high probability. Empirical comparisons against\nstate-of-the-art baselines on real-world clinical datasets demonstrate\nGRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of\nthe baselines vis-a-vis GRADUATE's strengths.", "AI": {"tldr": "GRADUATE\u6a21\u578b\u901a\u8fc7\u591a\u6821\u51c6\u4f18\u5316\u89e3\u51b3\u751f\u5b58\u5206\u6790\u4e2d\u5c11\u6570\u5b50\u7fa4\u4f53\u6821\u51c6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e73\u8861\u6821\u51c6\u4e0e\u5224\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u5b58\u6a21\u578b\u901a\u5e38\u4ec5\u5728\u7fa4\u4f53\u5c42\u9762\u6821\u51c6\uff0c\u53ef\u80fd\u5bfc\u81f4\u5c11\u6570\u5b50\u7fa4\u4f53\u6821\u51c6\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "GRADUATE\u5c06\u591a\u6821\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u4e2d\u4f18\u5316\u6821\u51c6\u4e0e\u5224\u522b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGRADUATE\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u6570\u5b66\u4e0a\u8bc1\u660e\u4e86\u5176\u4f18\u5316\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u8fd1\u4f18\u6027\u3002", "conclusion": "GRADUATE\u5728\u591a\u6821\u51c6\u548c\u6027\u80fd\u5e73\u8861\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
