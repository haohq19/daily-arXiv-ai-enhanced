{"id": "2512.05136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05136", "abs": "https://arxiv.org/abs/2512.05136", "authors": ["Yujie Xiao", "Gongzhen Tang", "Deyun Zhang", "Jun Li", "Guangkun Nie", "Haoyu Wang", "Shun Huang", "Tong Liu", "Qinghao Zhao", "Kangyin Chen", "Shenda Hong"], "title": "Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes", "comment": null, "summary": "Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.", "AI": {"tldr": "\u5f00\u53d1\u53ef\u89e3\u91ca\u7684AI-ECG\u6a21\u578b\uff0c\u901a\u8fc7\u5fc3\u7535\u56fe\u9884\u6d4b\u51a0\u72b6\u52a8\u8109\u4e25\u91cd\u6216\u5b8c\u5168\u72ed\u7a84\uff0c\u5728\u5185\u5916\u9a8c\u8bc1\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u7a33\u5b9a\u4e14\u5177\u6709\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u5065\u5eb7\u8d1f\u62c5\uff0c\u9700\u8981\u51c6\u786e\u8bc6\u522b\u7f6a\u72af\u8840\u7ba1\u548c\u8bc4\u4f30\u72ed\u7a84\u7a0b\u5ea6\u3002\u867d\u7136\u51a0\u8109CTA\u662f\u9996\u9009\u65e0\u521b\u8bca\u65ad\u65b9\u6cd5\uff0c\u4f46\u5176\u4f9d\u8d56\u9ad8\u7aef\u8bbe\u5907\u3001\u6709\u8f90\u5c04\u66b4\u9732\u4e14\u9700\u8981\u60a3\u8005\u4e25\u683c\u914d\u5408\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u5e94\u7528\u3002\u968f\u7740AI\u53d1\u5c55\u548c\u5fc3\u7535\u56fe\u5e7f\u6cdb\u53ef\u7528\uff0cAI-ECG\u4e3aCAD\u7b5b\u67e5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u53ef\u89e3\u91ca\u7684AI-ECG\u6a21\u578b\uff0c\u9884\u6d4b\u56db\u5927\u4e3b\u8981\u51a0\u72b6\u52a8\u8109\uff08RCA\u3001LM\u3001LAD\u3001LCX\uff09\u5728\u51a0\u8109CTA\u4e0a\u7684\u4e25\u91cd\u6216\u5b8c\u5168\u72ed\u7a84\u3002\u5728\u5185\u90e8\u548c\u5916\u90e8\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u4e34\u5e8a\u6b63\u5e38\u5fc3\u7535\u56fe\u4e9a\u7ec4\u3001\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u91c7\u96c6\u65f6\u95f4\u4e9a\u7ec4\u4e2d\u8fdb\u884c\u5206\u6790\u3002\u901a\u8fc7\u98ce\u9669\u5206\u5c42\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u6a21\u578b\u51b3\u7b56\u7684\u5173\u952e\u7535\u751f\u7406\u533a\u57df\u3002", "result": "\u5185\u90e8\u9a8c\u8bc1\u96c6AUC\uff1aRCA 0.794\u3001LM 0.818\u3001LAD 0.744\u3001LCX 0.755\uff1b\u5916\u90e8\u9a8c\u8bc1\u96c6AUC\uff1aRCA 0.749\u3001LM 0.971\u3001LAD 0.667\u3001LCX 0.727\u3002\u5728\u4e34\u5e8a\u6b63\u5e38\u5fc3\u7535\u56fe\u4e9a\u7ec4\u4e2d\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002\u4e9a\u7ec4\u5206\u6790\u786e\u8ba4\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u7a33\u5b9a\u6027\u3002\u98ce\u9669\u5206\u5c42\u663e\u793a\u6821\u51c6\u548c\u7d2f\u79ef\u4e8b\u4ef6\u66f2\u7ebf\u4e00\u81f4\u5206\u79bb\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u9ad8\u4f4e\u98ce\u9669\u7ec4\u95f4\u6ce2\u5f62\u5dee\u5f02\uff0c\u8bc6\u522b\u51fa\u5f71\u54cd\u6a21\u578b\u51b3\u7b56\u7684\u5173\u952e\u7535\u751f\u7406\u533a\u57df\u3002", "conclusion": "\u53ef\u89e3\u91ca\u7684AI-ECG\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u51a0\u72b6\u52a8\u8109\u4e25\u91cd\u72ed\u7a84\uff0c\u5728\u5185\u5916\u9a8c\u8bc1\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u4e14\u7a33\u5b9a\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u63d0\u4f9b\u53ef\u9760\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u7684\u5fc3\u7535\u56fe\u76f8\u5173\u7279\u5f81\uff0c\u4e3aCAD\u7b5b\u67e5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u6f5c\u5728\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.05241", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.05241", "abs": "https://arxiv.org/abs/2512.05241", "authors": ["Bruno Jacob", "Amanda A. Howard", "Panos Stinis"], "title": "Bridging quantum and classical computing for partial differential equations through multifidelity machine learning", "comment": "19 pages, 12 figures", "summary": "Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4fdd\u771f\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u91cf\u5b50\u6c42\u89e3\u5668\u751f\u6210\u4f4e\u4fdd\u771f\u89e3\uff0c\u518d\u7528\u7a00\u758f\u7ecf\u5178\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6821\u6b63\uff0c\u514b\u670d\u91cf\u5b50\u786c\u4ef6\u9650\u5236\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6PDE\u6c42\u89e3\u3002", "motivation": "\u91cf\u5b50PDE\u6c42\u89e3\u5668\u9762\u4e34\u8fd1\u91cf\u5b50\u786c\u4ef6\u7684\u4e25\u91cd\u9650\u5236\uff1a\u91cf\u5b50\u6bd4\u7279\u6570\u9650\u5236\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u7535\u8def\u6df1\u5ea6\u9650\u5236\u957f\u65f6\u95f4\u79ef\u5206\u7cbe\u5ea6\uff0c\u8fd9\u4e9b\u786c\u4ef6\u74f6\u9888\u4f7f\u91cf\u5b50\u6c42\u89e3\u5668\u53ea\u80fd\u63d0\u4f9b\u4f4e\u4fdd\u771f\u89e3\uff0c\u5c3d\u7ba1\u7406\u8bba\u4e0a\u6709\u8ba1\u7b97\u52a0\u901f\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u591a\u4fdd\u771f\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a\u9996\u5148\u7528\u91cf\u5b50\u6c42\u89e3\u5668\u751f\u6210\u4e30\u5bcc\u7684\u4f4e\u4fdd\u771f\u89e3\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u4f7f\u7528\u7a00\u758f\u7684\u9ad8\u4fdd\u771f\u7ecf\u5178\u6570\u636e\u8bad\u7ec3\u591a\u4fdd\u771f\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5e73\u8861\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u53d8\u6362\u6765\u5b66\u4e60\u6821\u6b63\u6620\u5c04\u3002", "result": "\u5728\u7c98\u6027Burgers\u65b9\u7a0b\u548c\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u6d41\u52a8\u7b49\u975e\u7ebf\u6027PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u6210\u529f\u6821\u6b63\u4e86\u7c97\u7f51\u683c\u91cf\u5b50\u9884\u6d4b\uff0c\u5e76\u5b9e\u73b0\u4e86\u8d85\u51fa\u7ecf\u5178\u8bad\u7ec3\u7a97\u53e3\u7684\u65f6\u95f4\u5916\u63a8\uff0c\u9884\u6d4b\u7cbe\u5ea6\u53ef\u4e0e\u7ecf\u5178\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7\u5f25\u5408\u786c\u4ef6\u53d7\u9650\u7684\u91cf\u5b50\u6a21\u62df\u4e0e\u5e94\u7528\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8be5\u5de5\u4f5c\u4e3a\u4ece\u5f53\u524d\u91cf\u5b50\u8bbe\u5907\u4e2d\u63d0\u53d6\u8ba1\u7b97\u4ef7\u503c\u5efa\u7acb\u4e86\u9014\u5f84\uff0c\u63a8\u8fdb\u4e86\u8fd1\u91cf\u5b50\u8ba1\u7b97\u5728\u8ba1\u7b97\u7269\u7406\u4e2d\u7684\u7b97\u6cd5\u5f00\u53d1\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.05240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.05240", "abs": "https://arxiv.org/abs/2512.05240", "authors": ["Dmitrii Torbunov", "Onur Okuducu", "Yi Huang", "Odera Dim", "Rebecca Coles", "Yonggang Cui", "Yihui Ren"], "title": "IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction", "comment": null, "summary": "Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6355\u83b7\u8303\u5f0f\uff1a\u7ed3\u5408\u7a00\u758fRGB\u5173\u952e\u5e27\u4e0e\u8fde\u7eed\u4e8b\u4ef6\u6d41\uff0c\u79bb\u7ebf\u91cd\u5efa\u5b8c\u6574RGB\u89c6\u9891\uff0c\u964d\u4f4e\u529f\u8017\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u89c6\u9891\u8f93\u51fa", "motivation": "\u4f20\u7edfRGB\u6444\u50cf\u5934\u80fd\u8017\u9ad8\uff0c\u4e8b\u4ef6\u76f8\u673a\u529f\u8017\u4f4e\u4f46\u8f93\u51fa\u5f02\u6b65\u4e8b\u4ef6\u6d41\u800c\u975e\u6807\u51c6\u89c6\u9891\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u964d\u4f4e\u529f\u8017\u53c8\u80fd\u751f\u6210\u6807\u51c6\u89c6\u9891\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faIE2Video\u4efb\u52a1\uff1a\u4ece\u5355\u4e2a\u521d\u59cb\u5e27\u548c\u540e\u7eed\u4e8b\u4ef6\u6570\u636e\u91cd\u5efaRGB\u89c6\u9891\u3002\u7814\u7a76\u4e24\u79cd\u67b6\u6784\uff1a1\uff09\u81ea\u56de\u5f52\u6a21\u578b\uff08HyperE2VID\uff09\u9002\u914dRGB\u751f\u6210\uff1b2\uff09\u901a\u8fc7\u5b66\u4e60\u7f16\u7801\u5668\u548c\u4f4e\u79e9\u9002\u914d\u5c06\u4e8b\u4ef6\u8868\u793a\u6ce8\u5165\u9884\u8bad\u7ec3\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08LTX\uff09", "result": "\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u6bd4\u81ea\u56de\u5f52\u57fa\u7ebf\u611f\u77e5\u8d28\u91cf\u63d0\u534733%\uff08LPIPS 0.283 vs 0.422\uff09\u3002\u5728\u4e09\u4e2a\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff08BS-ERGB, HS-ERGB far/close\uff09\u4e0a\u9a8c\u8bc1\uff0c\u652f\u630132-128\u5e27\u5e8f\u5217\u957f\u5ea6\uff0c\u5c55\u793a\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "conclusion": "\u6df7\u5408\u6355\u83b7\u8303\u5f0f\u6709\u6548\u964d\u4f4e\u529f\u8017\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\uff0c\u6269\u6563\u65b9\u6cd5\u4f18\u4e8e\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2512.05323", "categories": ["cs.LG", "cs.AI", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.05323", "abs": "https://arxiv.org/abs/2512.05323", "authors": ["Adam Lizerbram", "Shane Stevenson", "Iman Khadir", "Matthew Tu", "Samuel S. P. Shen"], "title": "Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition", "comment": "26 pages, 12 figures", "summary": "Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.", "AI": {"tldr": "\u6d4b\u8bd5AI\u5929\u6c14\u9884\u62a5\u6a21\u578bFourCastNetv2\u5bf9\u8f93\u5165\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u548c\u5b8c\u5168\u968f\u673a\u521d\u59cb\u6761\u4ef6\u6765\u8bc4\u4f30\u5176\u5bf9\u98d3\u98ce\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u8bc4\u4f30AI\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5728\u8f93\u5165\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9c81\u68d2\u6027\u5bf9\u4e8e\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\uff08\u5982\u98d3\u98ce\uff09\u7684\u53ef\u9760\u6027\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e86\u89e3\u6a21\u578b\u5bf9\u521d\u59cb\u6761\u4ef6\u6270\u52a8\u7684\u654f\u611f\u5ea6\u3002", "method": "\u8fdb\u884c\u4e24\u4e2a\u5b9e\u9a8c\uff1a1) \u5728\u98d3\u98ceFlorence\u7684ERA5\u521d\u59cb\u6761\u4ef6\u4e2d\u6ce8\u5165\u4e0d\u540c\u6c34\u5e73\u7684\u9ad8\u65af\u566a\u58f0\uff0c\u5206\u6790\u5bf9\u9884\u6d4b\u8f68\u8ff9\u548c\u5f3a\u5ea6\u7684\u5f71\u54cd\uff1b2) \u4f7f\u7528\u5b8c\u5168\u968f\u673a\u521d\u59cb\u6761\u4ef6\u542f\u52a8\u6a21\u578b\uff0c\u89c2\u5bdf\u6a21\u578b\u5bf9\u65e0\u610f\u4e49\u8f93\u5165\u7684\u53cd\u5e94\u3002", "result": "FCNv2\u5728\u4f4e\u5230\u4e2d\u7b49\u566a\u58f0\u4e0b\u80fd\u51c6\u786e\u4fdd\u6301\u98d3\u98ce\u7279\u5f81\uff1b\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u4e0b\u4e5f\u80fd\u7ef4\u6301\u57fa\u672c\u98ce\u66b4\u8f68\u8ff9\u548c\u7ed3\u6784\uff0c\u4f46\u4f4d\u7f6e\u7cbe\u5ea6\u4e0b\u964d\uff1b\u6a21\u578b\u5728\u6240\u6709\u566a\u58f0\u6c34\u5e73\u4e0b\u90fd\u4f4e\u4f30\u98ce\u66b4\u5f3a\u5ea6\u548c\u6301\u7eed\u6027\uff1b\u5b8c\u5168\u968f\u673a\u521d\u59cb\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u5728\u51e0\u4e2a\u65f6\u95f4\u6b65\u540e\u80fd\u751f\u6210\u5e73\u6ed1\u8fde\u8d2f\u7684\u9884\u6d4b\u3002", "conclusion": "FCNv2\u8868\u73b0\u51fa\u5bf9\u8f93\u5165\u566a\u58f0\u7684\u826f\u597d\u9c81\u68d2\u6027\uff0c\u503e\u5411\u4e8e\u751f\u6210\u7a33\u5b9a\u5e73\u6ed1\u7684\u8f93\u51fa\uff0c\u4f46\u5b58\u5728\u4f4e\u4f30\u98ce\u66b4\u5f3a\u5ea6\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff1b\u8be5\u8bc4\u4f30\u65b9\u6cd5\u7b80\u5355\u4e14\u53ef\u79fb\u690d\u5230\u5176\u4ed6\u6570\u636e\u9a71\u52a8\u7684AI\u5929\u6c14\u9884\u62a5\u6a21\u578b\u3002"}}
{"id": "2512.05946", "categories": ["cs.AI", "cs.ET", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.05946", "abs": "https://arxiv.org/abs/2512.05946", "authors": ["Truong Thanh Hung Nguyen", "Truong Thinh Nguyen", "Hung Cao"], "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem", "comment": "Quantum Software Engineering Practices at The 41st ACM/SIGAPP Symposium On Applied Computing (SAC 2026)", "summary": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.", "AI": {"tldr": "VQR-DQN\u7ed3\u5408\u53d8\u5206\u91cf\u5b50\u7535\u8def\u4e0eRainbow DQN\uff0c\u5728\u4eba\u529b\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e0a\u6bd4\u7ecf\u5178\u65b9\u6cd5\u63d0\u53474.9-13.4%\u6027\u80fd", "motivation": "\u8d44\u6e90\u5206\u914d\u662fNP\u96be\u95ee\u9898\uff0c\u7ecf\u5178\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u51fd\u6570\u903c\u8fd1\u5668\u7684\u8868\u793a\u80fd\u529b\uff0c\u9700\u8981\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u7684\u53e0\u52a0\u548c\u7ea0\u7f20\u7279\u6027\u6765\u63d0\u5347\u6027\u80fd", "method": "\u63d0\u51fa\u53d8\u5206\u91cf\u5b50Rainbow DQN(VQR-DQN)\uff0c\u5c06\u73af\u62d3\u6251\u53d8\u5206\u91cf\u5b50\u7535\u8def\u96c6\u6210\u5230Rainbow DQN\u4e2d\uff0c\u5c06\u4eba\u529b\u8d44\u6e90\u5206\u914d\u95ee\u9898\u5efa\u6a21\u4e3a\u57fa\u4e8e\u4eba\u5458\u80fd\u529b\u3001\u4e8b\u4ef6\u8c03\u5ea6\u548c\u8f6c\u79fb\u65f6\u95f4\u7684MDP", "result": "\u5728\u56db\u4e2aHRAP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVQR-DQN\u76f8\u6bd4\u968f\u673a\u57fa\u7ebf\u51cf\u5c1126.8%\u5f52\u4e00\u5316makespan\uff0c\u6bd4Double DQN\u548c\u7ecf\u5178Rainbow DQN\u63d0\u53474.9-13.4%\u6027\u80fd", "conclusion": "\u7535\u8def\u8868\u8fbe\u80fd\u529b\u3001\u7ea0\u7f20\u4e0e\u7b56\u7565\u8d28\u91cf\u7684\u7406\u8bba\u8054\u7cfb\u9a8c\u8bc1\u4e86\u91cf\u5b50\u589e\u5f3aDRL\u5728\u5927\u89c4\u6a21\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u6f5c\u529b"}}
{"id": "2512.05403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05403", "abs": "https://arxiv.org/abs/2512.05403", "authors": ["Gyusam Chang", "Jeongyoon Yoon", "Shin han yi", "JaeHyeok Lee", "Sujin Jang", "Sangpil Kim"], "title": "RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design", "comment": null, "summary": "Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.", "AI": {"tldr": "RevoNAD\uff1a\u4e00\u79cd\u53cd\u5c04\u5f0f\u8fdb\u5316\u7f16\u6392\u5668\uff0c\u5c06LLM\u63a8\u7406\u4e0e\u53cd\u9988\u5bf9\u9f50\u7684\u67b6\u6784\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u591a\u8f6e\u591a\u4e13\u5bb6\u5171\u8bc6\u3001\u81ea\u9002\u5e94\u53cd\u5c04\u63a2\u7d22\u548c\u5e15\u7d2f\u6258\u8fdb\u5316\u9009\u62e9\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u7cfb\u7edf\u5b58\u5728\u6311\u6218\uff1a\u4ee4\u724c\u7ea7\u8bbe\u8ba1\u5faa\u73af\u662f\u79bb\u6563\u4e14\u4e0d\u53ef\u5fae\u5206\u7684\uff0c\u53cd\u9988\u65e0\u6cd5\u5e73\u6ed1\u6307\u5bfc\u67b6\u6784\u6539\u8fdb\uff0c\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u4e3a\u5197\u4f59\u7ed3\u6784\u6216\u6f02\u79fb\u5230\u4e0d\u53ef\u884c\u8bbe\u8ba1\u3002", "method": "1. \u591a\u8f6e\u591a\u4e13\u5bb6\u5171\u8bc6\uff1a\u5c06\u5b64\u7acb\u8bbe\u8ba1\u89c4\u5219\u8f6c\u5316\u4e3a\u6709\u610f\u4e49\u7684\u67b6\u6784\u7ebf\u7d22\uff1b2. \u81ea\u9002\u5e94\u53cd\u5c04\u63a2\u7d22\uff1a\u5229\u7528\u5956\u52b1\u65b9\u5dee\u8c03\u6574\u63a2\u7d22\u7a0b\u5ea6\uff0c\u4e0d\u786e\u5b9a\u6027\u65f6\u63a2\u7d22\uff0c\u7a33\u5b9a\u6027\u8fbe\u5230\u65f6\u7ec6\u5316\uff1b3. \u5e15\u7d2f\u6258\u5f15\u5bfc\u7684\u8fdb\u5316\u9009\u62e9\uff1a\u8054\u5408\u4f18\u5316\u51c6\u786e\u6027\u3001\u6548\u7387\u3001\u5ef6\u8fdf\u3001\u7f6e\u4fe1\u5ea6\u548c\u7ed3\u6784\u591a\u6837\u6027\u3002", "result": "\u5728CIFAR10\u3001CIFAR100\u3001ImageNet16-120\u3001COCO-5K\u548cCityscape\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6d88\u878d\u548c\u8fc1\u79fb\u7814\u7a76\u9a8c\u8bc1\u4e86RevoNAD\u5728\u5b9e\u9645\u53ef\u9760\u548c\u53ef\u90e8\u7f72\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "RevoNAD\u6709\u6548\u6865\u63a5LLM\u63a8\u7406\u4e0e\u53cd\u9988\u5bf9\u9f50\u7684\u67b6\u6784\u641c\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u9a71\u52a8\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u53ef\u9760\u4e14\u53ef\u90e8\u7f72\u7684\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2512.05442", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05442", "abs": "https://arxiv.org/abs/2512.05442", "authors": ["Hua Wang", "Jinghao Lu", "Fan Zhang"], "title": "IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?", "comment": "Accepted at AAAI 2026", "summary": "Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.", "AI": {"tldr": "IdealTSF\u6846\u67b6\u5229\u7528\u975e\u7406\u60f3\u8d1f\u6837\u672c\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u3001\u8bad\u7ec3\u548c\u4f18\u5316\u4e09\u9636\u6bb5\uff0c\u5728\u566a\u58f0\u6837\u672c\u6216\u4f4e\u8d28\u91cf\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u548c\u5f02\u5e38\u503c\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7279\u5f81\u63d0\u53d6\u6216\u5c06\u6b21\u4f18\u6570\u636e\u4f5c\u4e3a\u6b63\u6837\u672c\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u800c\u5229\u7528\u975e\u7406\u60f3\u8d1f\u6837\u672c\u6765\u589e\u5f3a\u4e8b\u4ef6\u9884\u6d4b\u662f\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faIdealTSF\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6e10\u8fdb\u6b65\u9aa4\uff1a1) \u9884\u8bad\u7ec3\u9636\u6bb5\u4ece\u8d1f\u6837\u672c\u6570\u636e\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff1b2) \u8bad\u7ec3\u9636\u6bb5\u5c06\u5e8f\u5217\u6570\u636e\u8f6c\u5316\u4e3a\u7406\u60f3\u6b63\u6837\u672c\uff1b3) \u5e94\u7528\u5e26\u6709\u5bf9\u6297\u6270\u52a8\u7684\u8d1f\u4f18\u5316\u673a\u5236\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8d1f\u6837\u672c\u6570\u636e\u5728\u57fa\u672c\u6ce8\u610f\u529b\u67b6\u6784\u4e2d\u91ca\u653e\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u663e\u8457\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u6837\u672c\u6216\u4f4e\u8d28\u91cf\u6570\u636e\u573a\u666f\u4e2d\u3002", "conclusion": "IdealTSF\u6846\u67b6\u7279\u522b\u9002\u5408\u5177\u6709\u566a\u58f0\u6837\u672c\u6216\u4f4e\u8d28\u91cf\u6570\u636e\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6574\u5408\u7406\u60f3\u6b63\u8d1f\u6837\u672c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.05868", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2512.05868", "abs": "https://arxiv.org/abs/2512.05868", "authors": ["Brian Ezinwoke", "Oliver Rhodes"], "title": "Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks", "comment": "9 pages, 5 figures, 8 tables", "summary": "Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u9ad8\u9891\u4ea4\u6613\u4e2d\u7684\u4ef7\u683c\u5c16\u5cf0\u9884\u6d4b\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u65b0\u7684\u60e9\u7f5a\u6027\u5c16\u5cf0\u51c6\u786e\u7387\u76ee\u6807\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u91d1\u878d\u6a21\u578b\u96be\u4ee5\u6355\u6349\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u4e2d\u7684\u5fae\u79d2\u7ea7\u65f6\u95f4\u7ed3\u6784\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u5904\u7406\u79bb\u6563\u4e8b\u4ef6\u548c\u4fdd\u6301\u6beb\u79d2\u7ea7\u65f6\u5e8f\u7684\u81ea\u7136\u4f18\u52bf\uff0c\u9002\u5408\u7528\u4e8e\u4ef7\u683c\u5c16\u5cf0\u9884\u6d4b\u3002", "method": "\u5c06\u9ad8\u9891\u80a1\u7968\u6570\u636e\u8f6c\u6362\u4e3a\u8109\u51b2\u5e8f\u5217\uff0c\u8bc4\u4f30\u4e09\u79cd\u67b6\u6784\uff1a\u65e0\u76d1\u7763STDP\u8bad\u7ec3\u7684SNN\u3001\u5177\u6709\u663e\u5f0f\u6291\u5236\u7ade\u4e89\u7684\u65b0\u578bSNN\u3001\u76d1\u7763\u53cd\u5411\u4f20\u64ad\u7f51\u7edc\u3002\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u65b0\u7684\u60e9\u7f5a\u6027\u5c16\u5cf0\u51c6\u786e\u7387\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u4f7f\u7528PSA\u4f18\u5316\u7684\u6a21\u578b\u5728\u6a21\u62df\u4ea4\u6613\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4f7f\u7528SA\u4f18\u5316\u7684\u6a21\u578b\u548c\u57fa\u7ebf\u3002\u6269\u5c55\u7684SNN\u6a21\u578b\u5728\u7b80\u5355\u56de\u6d4b\u4e2d\u83b7\u5f97\u6700\u9ad8\u7d2f\u8ba1\u56de\u62a5\uff0876.8%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u66ff\u4ee3\u65b9\u6848\uff0842.54%\u56de\u62a5\uff09\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u9c81\u68d2\u8c03\u4f18\u540e\uff0c\u5728\u9ad8\u9891\u4ea4\u6613\u4ef7\u683c\u5c16\u5cf0\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6f5c\u529b\u3002"}}
{"id": "2512.05893", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.05893", "abs": "https://arxiv.org/abs/2512.05893", "authors": ["Neha Gupta", "Aditya Maheshwari"], "title": "NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process", "comment": "12 pages", "summary": "In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $\u03bc>0$ and $\u03b2\\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLSTM\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u5206\u6570\u6cca\u677e\u8fc7\u7a0b\u7684\u53c2\u6570\uff0c\u76f8\u6bd4\u4f20\u7edf\u77e9\u4f30\u8ba1\u65b9\u6cd5\u51cf\u5c11\u7ea655.3%\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u5e76\u5728\u5b9e\u9645\u9ad8\u9891\u6570\u636e\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u5206\u6570\u6cca\u677e\u8fc7\u7a0b\u80fd\u591f\u5efa\u6a21\u5177\u6709\u8bb0\u5fc6\u6027\u548c\u957f\u7a0b\u4f9d\u8d56\u7684\u4e8b\u4ef6\u5230\u8fbe\uff0c\u4f46\u4f20\u7edf\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u77e9\u4f30\u8ba1\uff09\u53ef\u80fd\u4e0d\u591f\u51c6\u786e\u3002\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u6765\u5904\u7406\u5177\u6709\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u5b9e\u9645\u6570\u636e\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u4ece\u5230\u8fbe\u65f6\u95f4\u95f4\u9694\u5e8f\u5217\u4e2d\u4f30\u8ba1\u5206\u6570\u6cca\u677e\u8fc7\u7a0b\u7684\u4e24\u4e2a\u5173\u952e\u53c2\u6570\u03bc\u548c\u03b2\u3002LSTM\u80fd\u591f\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u901a\u8fc7\u5e8f\u5217\u6570\u636e\u5b66\u4e60\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\uff0c\u76f8\u6bd4\u4f20\u7edf\u77e9\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684LSTM\u65b9\u6cd5\u51cf\u5c11\u4e86\u7ea655.3%\u7684\u5747\u65b9\u8bef\u5dee\u3002\u5728\u5b9e\u9645\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8ddf\u8e2a\u8499\u54e5\u9a6c\u5229\u53bf\u7d27\u6025\u547c\u53eb\u8bb0\u5f55\u548cAAPL\u80a1\u7968\u4ea4\u6613\u6570\u636e\u7684\u6bcf\u65e5\u6a21\u5f0f\u548c\u53c2\u6570\u53d8\u5316\u3002", "conclusion": "\u57fa\u4e8eLSTM\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u5206\u6570\u6cca\u677e\u8fc7\u7a0b\u7684\u53c2\u6570\uff0c\u5728\u5408\u6210\u548c\u5b9e\u9645\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u9ad8\u9891\u6570\u636e\u3002"}}
{"id": "2512.05746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.05746", "abs": "https://arxiv.org/abs/2512.05746", "authors": ["Shizhuo Mao", "Hongtao Zou", "Qihu Xie", "Song Chen", "Yi Kang"], "title": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models", "comment": null, "summary": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.", "AI": {"tldr": "\u63d0\u51faHQ-DM\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u54c8\u8fbe\u739b\u53d8\u6362\u51cf\u5c11\u6269\u6563\u6a21\u578b\u6fc0\u6d3b\u77e9\u9635\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u4fdd\u6301\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6fc0\u6d3b\u77e9\u9635\u7684\u5f02\u5e38\u503c\uff0c\u5bfc\u81f4\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d", "method": "\u63d0\u51faHQ-DM\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u5355\u54c8\u8fbe\u739b\u53d8\u6362\u5904\u7406\u6fc0\u6d3b\u77e9\u9635\uff0c\u6709\u6548\u51cf\u5c11\u6fc0\u6d3b\u5f02\u5e38\u503c\uff0c\u540c\u65f6\u652f\u6301INT\u5377\u79ef\u64cd\u4f5c\u5e76\u9632\u6b62\u6743\u91cd\u5f02\u5e38\u503c\u653e\u5927", "result": "\u5728ImageNet 256x256\u6570\u636e\u96c6\u4e0a\u4f7f\u7528LDM-4\u6a21\u578b\uff0cW4A4\u548cW4A3\u91cf\u5316\u65b9\u6848\u5206\u522b\u5c06Inception Score\u63d0\u534712.8%\u548c467.73%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5", "conclusion": "HQ-DM\u6846\u67b6\u901a\u8fc7\u5355\u54c8\u8fbe\u739b\u53d8\u6362\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u91cf\u5316\u4e2d\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500"}}
{"id": "2512.05830", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05830", "abs": "https://arxiv.org/abs/2512.05830", "authors": ["Muhammet Cagri Yeke", "Samil Sirin", "Kivilcim Yuksel", "Abdurrahman Gumus"], "title": "Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning", "comment": "22 pages, 11 figures, 5 tables", "summary": "This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Phase-OTDR\u5149\u7ea4\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e00\u7ef4\u6570\u636e\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf\uff08\u4f7f\u7528GADF\u3001GASF\u548cRP\u6280\u672f\uff09\uff0c\u7ec4\u5408\u6210\u591a\u901a\u9053RGB\u8868\u793a\uff0c\u5e76\u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u516d\u79cd\u5149\u7ea4\u4e8b\u4ef6\u7684\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edfPhase-OTDR\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5149\u7ea4\u4f20\u611f\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u5206\u6790\u6280\u672f\u6765\u51c6\u786e\u68c0\u6d4b\u548c\u5206\u7c7b\u5149\u7ea4\u4e2d\u7684\u5404\u79cd\u4e8b\u4ef6\u3002", "method": "1. \u5c06\u4e00\u7ef4Phase-OTDR\u6570\u636e\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf\uff1a\u4f7f\u7528Gramian Angular Difference Field (GADF)\u3001Gramian Angular Summation Field (GASF)\u548cRecurrence Plot (RP)\u4e09\u79cd\u6280\u672f\uff1b2. \u5c06\u4e09\u79cd\u7070\u5ea6\u56fe\u50cf\u7ec4\u5408\u6210\u591a\u901a\u9053RGB\u8868\u793a\uff1b3. \u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08EfficientNetB0\u548cDenseNet121\uff09\u8fdb\u884c\u5206\u7c7b\uff1b4. \u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "EfficientNetB0\u6a21\u578b\u8fbe\u523098.84%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0cDenseNet121\u8fbe\u523098.24%\u7684\u51c6\u786e\u7387\u30025\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6d4b\u8bd5\u51c6\u786e\u7387\u5206\u522b\u4e3a99.07%\u548c98.68%\u3002\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u6570\u636e\u96c6\u5927\u5c0f\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5206\u6790\u6548\u7387\u3002", "conclusion": "\u56fe\u50cf\u5316\u5206\u6790\u65b9\u6cd5\u5728\u89e3\u91ca\u590d\u6742\u5149\u7ea4\u4f20\u611f\u6570\u636e\u65b9\u9762\u5177\u6709\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5149\u7ea4\u76d1\u6d4b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u7814\u7a76\u4ee3\u7801\u548c\u56fe\u50cf\u5316\u6570\u636e\u96c6\u5df2\u5728GitHub\u516c\u5f00\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
