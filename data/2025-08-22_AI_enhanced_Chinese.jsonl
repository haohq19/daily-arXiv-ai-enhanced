{"id": "2508.14994", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.14994", "abs": "https://arxiv.org/abs/2508.14994", "authors": ["Murilo Vinicius da Silva", "Matheus Hipolito Carvalho", "Juliano Negri", "Thiago Segreto", "Gustavo J. G. Lahr", "Ricardo V. Godoy", "Marcelo Becker"], "title": "A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot", "comment": null, "summary": "In hazardous and remote environments, robotic systems perform critical tasks\ndemanding improved safety and efficiency. Among these, quadruped robots with\nmanipulator arms offer mobility and versatility for complex operations.\nHowever, teleoperating quadruped robots is challenging due to the lack of\nintegrated obstacle detection and intuitive control methods for the robotic\narm, increasing collision risks in confined or dynamically changing workspaces.\nTeleoperation via joysticks or pads can be non-intuitive and demands a high\nlevel of expertise due to its complexity, culminating in a high cognitive load\non the operator. To address this challenge, a teleoperation approach that\ndirectly maps human arm movements to the robotic manipulator offers a simpler\nand more accessible solution. This work proposes an intuitive remote control by\nleveraging a vision-based pose estimation pipeline that utilizes an external\ncamera with a machine learning-based model to detect the operator's wrist\nposition. The system maps these wrist movements into robotic arm commands to\ncontrol the robot's arm in real-time. A trajectory planner ensures safe\nteleoperation by detecting and preventing collisions with both obstacles and\nthe robotic arm itself. The system was validated on the real robot,\ndemonstrating robust performance in real-time control. This teleoperation\napproach provides a cost-effective solution for industrial applications where\nsafety, precision, and ease of use are paramount, ensuring reliable and\nintuitive robotic control in high-risk environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u7684\u76f4\u89c2\u56db\u8db3\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u6444\u50cf\u5934\u68c0\u6d4b\u64cd\u4f5c\u5458\u624b\u8155\u4f4d\u7f6e\uff0c\u5b9e\u65f6\u6620\u5c04\u5230\u673a\u68b0\u81c2\u63a7\u5236\uff0c\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u5668\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c", "motivation": "\u5728\u5371\u9669\u548c\u8fdc\u7a0b\u73af\u5883\u4e2d\uff0c\u56db\u8db3\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7cfb\u7edf\u9700\u8981\u66f4\u5b89\u5168\u548c\u9ad8\u6548\u7684\u64cd\u4f5c\u3002\u4f20\u7edf\u9065\u64cd\u4f5c\u65b9\u6cd5\uff08\u5982\u64cd\u7eb5\u6746\uff09\u4e0d\u76f4\u89c2\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8ba4\u77e5\u8d1f\u8377\u9ad8\uff0c\u7f3a\u4e4f\u96c6\u6210\u7684\u969c\u788d\u7269\u68c0\u6d4b\u529f\u80fd\uff0c\u5728\u53d7\u9650\u6216\u52a8\u6001\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u78b0\u649e\u98ce\u9669\u9ad8", "method": "\u4f7f\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5916\u90e8\u6444\u50cf\u5934\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u7ba1\u9053\u68c0\u6d4b\u64cd\u4f5c\u5458\u624b\u8155\u4f4d\u7f6e\uff0c\u5c06\u8fd9\u4e9b\u624b\u8155\u8fd0\u52a8\u5b9e\u65f6\u6620\u5c04\u4e3a\u673a\u68b0\u81c2\u547d\u4ee4\uff0c\u5e76\u91c7\u7528\u8f68\u8ff9\u89c4\u5212\u5668\u68c0\u6d4b\u548c\u9632\u6b62\u4e0e\u969c\u788d\u7269\u53ca\u673a\u68b0\u81c2\u81ea\u8eab\u7684\u78b0\u649e", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5b9e\u65f6\u63a7\u5236\u7684\u7a33\u5065\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b89\u5168\u6027\u3001\u7cbe\u786e\u6027\u548c\u6613\u7528\u6027\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u786e\u4fdd\u53ef\u9760\u548c\u76f4\u89c2\u7684\u673a\u5668\u4eba\u63a7\u5236"}}
{"id": "2508.14926", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14926", "abs": "https://arxiv.org/abs/2508.14926", "authors": ["Dianzhao Li", "Ostap Okhrin"], "title": "Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving", "comment": null, "summary": "Autonomous vehicles hold great promise for reducing traffic fatalities and\nimproving transportation efficiency, yet their widespread adoption hinges on\nembedding robust ethical reasoning into routine and emergency maneuvers. Here,\nwe present a hierarchical Safe Reinforcement Learning (Safe RL) framework that\nexplicitly integrates moral considerations with standard driving objectives. At\nthe decision level, a Safe RL agent is trained using a composite ethical risk\ncost, combining collision probability and harm severity, to generate high-level\nmotion targets. A dynamic Prioritized Experience Replay mechanism amplifies\nlearning from rare but critical, high-risk events. At the execution level,\npolynomial path planning coupled with Proportional-Integral-Derivative (PID)\nand Stanley controllers translates these targets into smooth, feasible\ntrajectories, ensuring both accuracy and comfort. We train and validate our\napproach on rich, real-world traffic datasets encompassing diverse vehicles,\ncyclists, and pedestrians, and demonstrate that it outperforms baseline methods\nin reducing ethical risk and maintaining driving performance. To our knowledge,\nthis is the first study of ethical decision-making for autonomous vehicles via\nSafe RL in real-world scenarios. Our results highlight the potential of\ncombining formal control theory and data-driven learning to advance ethically\naccountable autonomy in complex, human-mixed traffic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u9053\u5fb7\u8003\u91cf\u4e0e\u6807\u51c6\u9a7e\u9a76\u76ee\u6807\u7ed3\u5408\uff0c\u901a\u8fc7\u4f26\u7406\u98ce\u9669\u6210\u672c\u548c\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4f26\u7406\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u51cf\u5c11\u4ea4\u901a\u4e8b\u6545\u548c\u63d0\u9ad8\u4ea4\u901a\u6548\u7387\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5e7f\u6cdb\u91c7\u7528\u53d6\u51b3\u4e8e\u80fd\u5426\u5728\u5e38\u89c4\u548c\u7d27\u6025\u64cd\u4f5c\u4e2d\u5d4c\u5165\u7a33\u5065\u7684\u4f26\u7406\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u51b3\u7b56\u5c42\u4f7f\u7528\u5305\u542b\u78b0\u649e\u6982\u7387\u548c\u4f24\u5bb3\u4e25\u91cd\u6027\u7684\u590d\u5408\u4f26\u7406\u98ce\u9669\u6210\u672c\u8bad\u7ec3Safe RL\u667a\u80fd\u4f53\uff1b\u6267\u884c\u5c42\u4f7f\u7528\u591a\u9879\u5f0f\u8def\u5f84\u89c4\u5212\u548cPID+Stanley\u63a7\u5236\u5668\u5c06\u76ee\u6807\u8f6c\u5316\u4e3a\u5e73\u6ed1\u53ef\u884c\u7684\u8f68\u8ff9\u3002\u91c7\u7528\u52a8\u6001\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u673a\u5236\u5f3a\u5316\u9ad8\u98ce\u9669\u4e8b\u4ef6\u7684\u5b66\u4e60\u3002", "result": "\u5728\u5305\u542b\u591a\u6837\u5316\u8f66\u8f86\u3001\u9a91\u884c\u8005\u548c\u884c\u4eba\u7684\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u4f26\u7406\u98ce\u9669\u548c\u4fdd\u6301\u9a7e\u9a76\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4f26\u7406\u51b3\u7b56\u7684\u5de5\u4f5c\uff0c\u7ed3\u5408\u5f62\u5f0f\u63a7\u5236\u7406\u8bba\u548c\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u6709\u671b\u63a8\u52a8\u590d\u6742\u4eba\u8f66\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u4f26\u7406\u95ee\u8d23\u81ea\u6cbb\u3002"}}
{"id": "2508.15189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15189", "abs": "https://arxiv.org/abs/2508.15189", "authors": ["Jiahao Xu", "Changchang Yin", "Odysseas Chatzipanagiotou", "Diamantis Tsilimigras", "Kevin Clear", "Bingsheng Yao", "Dakuo Wang", "Timothy Pawlik", "Ping Zhang"], "title": "SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis", "comment": null, "summary": "Surgical site infection (SSI) is one of the most common and costly\nhealthcare-associated infections and and surgical wound care remains a\nsignificant clinical challenge in preventing SSIs and improving patient\noutcomes. While recent studies have explored the use of deep learning for\npreliminary surgical wound screening, progress has been hindered by concerns\nover data privacy and the high costs associated with expert annotation.\nCurrently, no publicly available dataset or benchmark encompasses various types\nof surgical wounds, resulting in the absence of an open-source Surgical-Wound\nscreening tool. To address this gap: (1) we present SurgWound, the first\nopen-source dataset featuring a diverse array of surgical wound types. It\ncontains 697 surgical wound images annotated by 3 professional surgeons with\neight fine-grained clinical attributes. (2) Based on SurgWound, we introduce\nthe first benchmark for surgical wound diagnosis, which includes visual\nquestion answering (VQA) and report generation tasks to comprehensively\nevaluate model performance. (3) Furthermore, we propose a three-stage learning\nframework, WoundQwen, for surgical wound diagnosis. In the first stage, we\nemploy five independent MLLMs to accurately predict specific surgical wound\ncharacteristics. In the second stage, these predictions serve as additional\nknowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess\ninfection risk and guide subsequent interventions. In the third stage, we train\na MLLM that integrates the diagnostic results from the previous two stages to\nproduce a comprehensive report. This three-stage framework can analyze detailed\nsurgical wound characteristics and provide subsequent instructions to patients\nbased on surgical images, paving the way for personalized wound care, timely\nintervention, and improved patient outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90\u624b\u672f\u4f24\u53e3\u6570\u636e\u96c6SurgWound\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e09\u9636\u6bb5\u5b66\u4e60\u6846\u67b6WoundQwen\u7528\u4e8e\u624b\u672f\u4f24\u53e3\u8bca\u65ad\uff0c\u5305\u62ec\u7279\u5f81\u9884\u6d4b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u624b\u672f\u90e8\u4f4d\u611f\u67d3(SSI)\u662f\u5e38\u89c1\u4e14\u6602\u8d35\u7684\u533b\u7597\u76f8\u5173\u611f\u67d3\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u53d7\u9650\u4e8e\u6570\u636e\u9690\u79c1\u548c\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u548c\u5f00\u6e90\u7b5b\u67e5\u5de5\u5177\u3002", "method": "1) \u521b\u5efa\u5305\u542b697\u5f20\u624b\u672f\u4f24\u53e3\u56fe\u50cf\u7684SurgWound\u6570\u636e\u96c6\uff0c\u75313\u540d\u4e13\u4e1a\u5916\u79d1\u533b\u751f\u6807\u6ce88\u4e2a\u7ec6\u7c92\u5ea6\u4e34\u5e8a\u5c5e\u6027\uff1b2) \u5efa\u7acb\u5305\u542b\u89c6\u89c9\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1b3) \u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6WoundQwen\uff1a\u7b2c\u4e00\u9636\u6bb5\u75285\u4e2aMLLM\u9884\u6d4b\u4f24\u53e3\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u75282\u4e2aMLLM\u8fdb\u884c\u98ce\u9669\u8bc4\u4f30\uff0c\u7b2c\u4e09\u9636\u6bb5\u6574\u5408\u7ed3\u679c\u751f\u6210\u7efc\u5408\u62a5\u544a\u3002", "result": "\u5f00\u53d1\u4e86\u9996\u4e2a\u5f00\u6e90\u624b\u672f\u4f24\u53e3\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86\u80fd\u591f\u5206\u6790\u4f24\u53e3\u7279\u5f81\u3001\u8bc4\u4f30\u611f\u67d3\u98ce\u9669\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u62a4\u7406\u6307\u5bfc\u7684\u4e09\u9636\u6bb5\u8bca\u65ad\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u624b\u672f\u4f24\u53e3\u7b5b\u67e5\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u4e3a\u5b9e\u73b0\u4e2a\u6027\u5316\u4f24\u53e3\u62a4\u7406\u3001\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.15663", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15663", "abs": "https://arxiv.org/abs/2508.15663", "authors": ["Nikita Kachaev", "Andrei Spiridonov", "Andrey Gorodetsky", "Kirill Muravyev", "Nikita Oskolkov", "Aditya Narendra", "Vlad Shakhuro", "Dmitry Makarov", "Aleksandr I. Panov", "Polina Fedotova", "Alexey K. Kovalev"], "title": "Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation", "comment": null, "summary": "Benchmarks are crucial for evaluating progress in robotics and embodied AI.\nHowever, a significant gap exists between benchmarks designed for high-level\nlanguage instruction following, which often assume perfect low-level execution,\nand those for low-level robot control, which rely on simple, one-step commands.\nThis disconnect prevents a comprehensive evaluation of integrated systems where\nboth task planning and physical execution are critical. To address this, we\npropose Kitchen-R, a novel benchmark that unifies the evaluation of task\nplanning and low-level control within a simulated kitchen environment. Built as\na digital twin using the Isaac Sim simulator and featuring more than 500\ncomplex language instructions, Kitchen-R supports a mobile manipulator robot.\nWe provide baseline methods for our benchmark, including a task-planning\nstrategy based on a vision-language model and a low-level control policy based\non diffusion policy. We also provide a trajectory collection system. Our\nbenchmark offers a flexible framework for three evaluation modes: independent\nassessment of the planning module, independent assessment of the control\npolicy, and, crucially, an integrated evaluation of the whole system. Kitchen-R\nbridges a key gap in embodied AI research, enabling more holistic and realistic\nbenchmarking of language-guided robotic agents.", "AI": {"tldr": "Kitchen-R\u662f\u4e00\u4e2a\u65b0\u7684\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7edf\u4e00\u8bc4\u4f30\u4efb\u52a1\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\u5728\u6a21\u62df\u53a8\u623f\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u548c\u4f4e\u7ea7\u63a7\u5236\u4e4b\u95f4\u7684\u8bc4\u4f30\u9e3f\u6c9f\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u5047\u8bbe\u5b8c\u7f8e\u4f4e\u7ea7\u6267\u884c\uff0c\u800c\u4f4e\u7ea7\u63a7\u5236\u57fa\u51c6\u4f9d\u8d56\u7b80\u5355\u5355\u6b65\u547d\u4ee4\uff0c\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u89c4\u5212\u548c\u7269\u7406\u6267\u884c\u96c6\u6210\u7cfb\u7edf\u7684\u7efc\u5408\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Isaac Sim\u6a21\u62df\u5668\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u53a8\u623f\u73af\u5883\uff0c\u5305\u542b500\u591a\u4e2a\u590d\u6742\u8bed\u8a00\u6307\u4ee4\uff0c\u652f\u6301\u79fb\u52a8\u673a\u68b0\u81c2\u673a\u5668\u4eba\u3002\u63d0\u4f9b\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u89c4\u5212\u7b56\u7565\u548c\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u4f4e\u7ea7\u63a7\u5236\u7b56\u7565\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "Kitchen-R\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e09\u79cd\u8bc4\u4f30\u6a21\u5f0f\u7684\u7075\u6d3b\u6846\u67b6\uff1a\u72ec\u7acb\u8bc4\u4f30\u89c4\u5212\u6a21\u5757\u3001\u72ec\u7acb\u8bc4\u4f30\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u53ca\u5173\u952e\u7684\u7cfb\u7edf\u96c6\u6210\u8bc4\u4f30\u3002", "conclusion": "Kitchen-R\u586b\u8865\u4e86\u5177\u8eabAI\u7814\u7a76\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u4ee3\u7406\u66f4\u5168\u9762\u548c\u73b0\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2508.15086", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15086", "abs": "https://arxiv.org/abs/2508.15086", "authors": ["Yen-Lung Lai", "Zhe Jin"], "title": "Wormhole Dynamics in Deep Neural Networks", "comment": null, "summary": "This work investigates the generalization behavior of deep neural networks\n(DNNs), focusing on the phenomenon of \"fooling examples,\" where DNNs\nconfidently classify inputs that appear random or unstructured to humans. To\nexplore this phenomenon, we introduce an analytical framework based on maximum\nlikelihood estimation, without adhering to conventional numerical approaches\nthat rely on gradient-based optimization and explicit labels. Our analysis\nreveals that DNNs operating in an overparameterized regime exhibit a collapse\nin the output feature space. While this collapse improves network\ngeneralization, adding more layers eventually leads to a state of degeneracy,\nwhere the model learns trivial solutions by mapping distinct inputs to the same\noutput, resulting in zero loss. Further investigation demonstrates that this\ndegeneracy can be bypassed using our newly derived \"wormhole\" solution. The\nwormhole solution, when applied to arbitrary fooling examples, reconciles\nmeaningful labels with random ones and provides a novel perspective on shortcut\nlearning. These findings offer deeper insights into DNN generalization and\nhighlight directions for future research on learning dynamics in unsupervised\nsettings to bridge the gap between theory and practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fc7\u53c2\u6570\u5316\u72b6\u6001\u4e0b\u7684\u6cdb\u5316\u884c\u4e3a\uff0c\u53d1\u73b0\u7f51\u7edc\u4f1a\u51fa\u73b0\u7279\u5f81\u7a7a\u95f4\u574d\u7f29\u548c\u9000\u5316\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\"\u866b\u6d1e\"\u89e3\u51b3\u65b9\u6848\u6765\u7ed5\u8fc7\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u7406\u89e3DNN\u6cdb\u5316\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\"\u6b3a\u9a97\u6027\u6837\u672c\"\u73b0\u8c61\uff0c\u5373DNN\u5bf9\u770b\u4f3c\u968f\u673a\u7684\u8f93\u5165\u4e5f\u80fd\u7ed9\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\u5206\u7c7b\uff0c\u65e8\u5728\u63a2\u7d22DNN\u6cdb\u5316\u884c\u4e3a\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5f25\u8865\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u5206\u6790\u6846\u67b6\uff0c\u907f\u514d\u4f20\u7edf\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u663e\u5f0f\u6807\u7b7e\uff0c\u7814\u7a76\u8fc7\u53c2\u6570\u5316\u72b6\u6001\u4e0bDNN\u7684\u7279\u5f81\u7a7a\u95f4\u574d\u7f29\u548c\u9000\u5316\u73b0\u8c61\u3002", "result": "\u53d1\u73b0DNN\u5728\u8fc7\u53c2\u6570\u5316\u72b6\u6001\u4e0b\u4f1a\u51fa\u73b0\u7279\u5f81\u7a7a\u95f4\u574d\u7f29\uff08\u6539\u5584\u6cdb\u5316\uff09\u4f46\u6700\u7ec8\u5bfc\u81f4\u9000\u5316\uff08\u4e0d\u540c\u8f93\u5165\u6620\u5c04\u5230\u76f8\u540c\u8f93\u51fa\uff09\uff0c\u63d0\u51fa\u7684\"\u866b\u6d1e\"\u89e3\u51b3\u65b9\u6848\u80fd\u6709\u6548\u7ed5\u8fc7\u9000\u5316\u95ee\u9898\uff0c\u534f\u8c03\u968f\u673a\u6807\u7b7e\u4e0e\u6709\u610f\u4e49\u6807\u7b7e\u7684\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86DNN\u6cdb\u5316\u884c\u4e3a\u7684\u65b0\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u6377\u5f84\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6307\u660e\u4e86\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u7814\u7a76\u5b66\u4e60\u52a8\u529b\u5b66\u4ee5\u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u5dee\u8ddd\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.15274", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15274", "abs": "https://arxiv.org/abs/2508.15274", "authors": ["Lekshmi R Nair", "Arun Sankar", "Koninika Pal"], "title": "TComQA: Extracting Temporal Commonsense from Text", "comment": null, "summary": "Understanding events necessitates grasping their temporal context, which is\noften not explicitly stated in natural language. For example, it is not a\ntrivial task for a machine to infer that a museum tour may last for a few\nhours, but can not take months. Recent studies indicate that even advanced\nlarge language models (LLMs) struggle in generating text that require reasoning\nwith temporal commonsense due to its infrequent explicit mention in text.\nTherefore, automatically mining temporal commonsense for events enables the\ncreation of robust language models. In this work, we investigate the capacity\nof LLMs to extract temporal commonsense from text and evaluate multiple\nexperimental setups to assess their effectiveness. Here, we propose a temporal\ncommonsense extraction pipeline that leverages LLMs to automatically mine\ntemporal commonsense and use it to construct TComQA, a dataset derived from\nSAMSum and RealNews corpora. TComQA has been validated through crowdsourcing\nand achieves over 80\\% precision in extracting temporal commonsense. The model\ntrained with TComQA also outperforms an LLM fine-tuned on existing dataset of\ntemporal question answering task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6316\u6398\u65f6\u95f4\u5e38\u8bc6\u7684\u7ba1\u9053\uff0c\u6784\u5efa\u4e86TComQA\u6570\u636e\u96c6\uff0c\u5e76\u5728\u65f6\u95f4\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u4e2d\u5f80\u5f80\u4e0d\u4f1a\u660e\u786e\u8868\u8ff0\u4e8b\u4ef6\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u5373\u4f7f\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e5f\u96be\u4ee5\u751f\u6210\u9700\u8981\u65f6\u95f4\u5e38\u8bc6\u63a8\u7406\u7684\u6587\u672c\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u6316\u6398\u65f6\u95f4\u5e38\u8bc6\u6765\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u95f4\u5e38\u8bc6\u63d0\u53d6\u7ba1\u9053\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4eceSAMSum\u548cRealNews\u8bed\u6599\u5e93\u4e2d\u81ea\u52a8\u6316\u6398\u65f6\u95f4\u5e38\u8bc6\uff0c\u6784\u5efaTComQA\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4f17\u5305\u9a8c\u8bc1\u3002", "result": "TComQA\u6570\u636e\u96c6\u5728\u63d0\u53d6\u65f6\u95f4\u5e38\u8bc6\u65b9\u9762\u8fbe\u5230\u8d85\u8fc780%\u7684\u7cbe\u786e\u5ea6\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u65f6\u95f4\u95ee\u7b54\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u53d6\u65f6\u95f4\u5e38\u8bc6\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.15314", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15314", "abs": "https://arxiv.org/abs/2508.15314", "authors": ["Naen Xu", "Jinghuai Zhang", "Changjiang Li", "Zhi Chen", "Chunyi Zhou", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "VideoEraser: Concept Erasure in Text-to-Video Diffusion Models", "comment": "To appear in the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP)", "summary": "The rapid growth of text-to-video (T2V) diffusion models has raised concerns\nabout privacy, copyright, and safety due to their potential misuse in\ngenerating harmful or misleading content. These models are often trained on\nnumerous datasets, including unauthorized personal identities, artistic\ncreations, and harmful materials, which can lead to uncontrolled production and\ndistribution of such content. To address this, we propose VideoEraser, a\ntraining-free framework that prevents T2V diffusion models from generating\nvideos with undesirable concepts, even when explicitly prompted with those\nconcepts. Designed as a plug-and-play module, VideoEraser can seamlessly\nintegrate with representative T2V diffusion models via a two-stage process:\nSelective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise\nGuidance (ARNG). We conduct extensive evaluations across four tasks, including\nobject erasure, artistic style erasure, celebrity erasure, and explicit content\nerasure. Experimental results show that VideoEraser consistently outperforms\nprior methods regarding efficacy, integrity, fidelity, robustness, and\ngeneralizability. Notably, VideoEraser achieves state-of-the-art performance in\nsuppressing undesirable content during T2V generation, reducing it by 46% on\naverage across four tasks compared to baselines.", "AI": {"tldr": "VideoEraser\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5904\u7406\u9632\u6b62\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5305\u542b\u4e0d\u826f\u6982\u5ff5\u7684\u5185\u5bb9\uff0c\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u5e73\u5747\u51cf\u5c1146%\u7684\u4e0d\u826f\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u9690\u79c1\u3001\u7248\u6743\u548c\u5b89\u5168\u62c5\u5fe7\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u88ab\u6ee5\u7528\u6765\u751f\u6210\u6709\u5bb3\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u9632\u6b62\u751f\u6210\u4e0d\u826f\u6982\u5ff5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a\u9009\u62e9\u6027\u63d0\u793a\u5d4c\u5165\u8c03\u6574\uff08SPEA\uff09\u548c\u6297\u5e72\u6270\u566a\u58f0\u5f15\u5bfc\uff08ARNG\uff09\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u4e0e\u73b0\u6709T2V\u6269\u6563\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u5bf9\u8c61\u64e6\u9664\u3001\u827a\u672f\u98ce\u683c\u64e6\u9664\u3001\u540d\u4eba\u64e6\u9664\u548c\u663e\u5f0f\u5185\u5bb9\u64e6\u9664\u56db\u4e2a\u4efb\u52a1\u4e2d\uff0cVideoEraser\u5728\u6548\u80fd\u3001\u5b8c\u6574\u6027\u3001\u4fdd\u771f\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VideoEraser\u5728\u6291\u5236T2V\u751f\u6210\u4e2d\u7684\u4e0d\u826f\u5185\u5bb9\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u63a7\u5236\u6269\u6563\u6a21\u578b\u8f93\u51fa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15389", "abs": "https://arxiv.org/abs/2508.15389", "authors": ["Wenrui Li", "Wei Han", "Liang-Jian Deng", "Ruiqin Xiong", "Xiaopeng Fan"], "title": "Spiking Variational Graph Representation Inference for Video Summarization", "comment": "Accepted by IEEE TIP", "summary": "With the rise of short video content, efficient video summarization\ntechniques for extracting key information have become crucial. However,\nexisting methods struggle to capture the global temporal dependencies and\nmaintain the semantic coherence of video content. Additionally, these methods\nare also influenced by noise during multi-channel feature fusion. We propose a\nSpiking Variational Graph (SpiVG) Network, which enhances information density\nand reduces computational complexity. First, we design a keyframe extractor\nbased on Spiking Neural Networks (SNN), leveraging the event-driven computation\nmechanism of SNNs to learn keyframe features autonomously. To enable\nfine-grained and adaptable reasoning across video frames, we introduce a\nDynamic Aggregation Graph Reasoner, which decouples contextual object\nconsistency from semantic perspective coherence. We present a Variational\nInference Reconstruction Module to address uncertainty and noise arising during\nmulti-channel feature fusion. In this module, we employ Evidence Lower Bound\nOptimization (ELBO) to capture the latent structure of multi-channel feature\ndistributions, using posterior distribution regularization to reduce\noverfitting. Experimental results show that SpiVG surpasses existing methods\nacross multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes\nand pre-trained models are available at https://github.com/liwrui/SpiVG.", "AI": {"tldr": "\u63d0\u51faSpiVG\u7f51\u7edc\uff0c\u7ed3\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u53d8\u5206\u63a8\u7406\uff0c\u89e3\u51b3\u89c6\u9891\u6458\u8981\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u77ed\u89c6\u9891\u5185\u5bb9\u5174\u8d77\u9700\u8981\u9ad8\u6548\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u6280\u672f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5168\u5c40\u65f6\u5e8f\u4f9d\u8d56\u3001\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u4e14\u5728\u591a\u901a\u9053\u7279\u5f81\u878d\u5408\u65f6\u6613\u53d7\u566a\u58f0\u5f71\u54cd", "method": "\u8bbe\u8ba1\u57fa\u4e8eSNN\u7684\u5173\u952e\u5e27\u63d0\u53d6\u5668\uff0c\u5f15\u5165\u52a8\u6001\u805a\u5408\u56fe\u63a8\u7406\u5668\u89e3\u8026\u4e0a\u4e0b\u6587\u5bf9\u8c61\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u89c6\u89d2\u8fde\u8d2f\u6027\uff0c\u4f7f\u7528\u53d8\u5206\u63a8\u7406\u91cd\u5efa\u6a21\u5757\u5904\u7406\u591a\u901a\u9053\u7279\u5f81\u878d\u5408\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u566a\u58f0", "result": "\u5728SumMe\u3001TVSum\u3001VideoXum\u548cQFVS\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "SpiVG\u7f51\u7edc\u901a\u8fc7SNN\u7684\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u548c\u53d8\u5206\u63a8\u7406\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u6458\u8981\u7684\u4fe1\u606f\u5bc6\u5ea6\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6"}}
{"id": "2508.15748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15748", "abs": "https://arxiv.org/abs/2508.15748", "authors": ["Emma Rath", "Stuart Armstrong", "Rebecca Gorman"], "title": "Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots", "comment": null, "summary": "The development of parasocial relationships with AI agents has severe, and in\nsome cases, tragic effects for human well-being. Yet preventing such dynamics\nis challenging: parasocial cues often emerge gradually in private\nconversations, and not all forms of emotional engagement are inherently\nharmful. We address this challenge by introducing a simple response evaluation\nframework, created by repurposing a state-of-the-art language model, that\nevaluates ongoing conversations for parasocial cues in real time. To test the\nfeasibility of this approach, we constructed a small synthetic dataset of\nthirty dialogues spanning parasocial, sycophantic, and neutral conversations.\nIterative evaluation with five stage testing successfully identified all\nparasocial conversations while avoiding false positives under a tolerant\nunanimity rule, with detection typically occurring within the first few\nexchanges. These findings provide preliminary evidence that evaluation agents\ncan provide a viable solution for the prevention of parasocial relations.", "AI": {"tldr": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u5b9e\u65f6\u8bc4\u4f30\u5bf9\u8bdd\u4e2d\u7684\u5047\u793e\u4ea4\u5173\u7cfb\u7ebf\u7d22\uff0c\u901a\u8fc7\u5c0f\u578b\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u80fd\u5728\u524d\u51e0\u8f6e\u5bf9\u8bdd\u4e2d\u51c6\u786e\u68c0\u6d4b\u5047\u793e\u4ea4\u5173\u7cfb\u5e76\u907f\u514d\u8bef\u62a5", "motivation": "\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5f62\u6210\u7684\u5047\u793e\u4ea4\u5173\u7cfb\u5bf9\u4eba\u7c7b\u5065\u5eb7\u9020\u6210\u4e25\u91cd\u5371\u5bb3\uff0c\u4f46\u9632\u6b62\u8fd9\u79cd\u5173\u7cfb\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5b9e\u65f6\u76d1\u6d4b\u79c1\u5bc6\u5bf9\u8bdd\u4e2d\u7684\u5047\u793e\u4ea4\u7ebf\u7d22", "method": "\u91cd\u65b0\u8c03\u6574\u6700\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7b80\u5355\u7684\u54cd\u5e94\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u5b9e\u65f6\u5bf9\u8bdd\u4e2d\u8bc4\u4f30\u5047\u793e\u4ea4\u7ebf\u7d22\uff0c\u4f7f\u752830\u4e2a\u5408\u6210\u5bf9\u8bdd\u7684\u5c0f\u578b\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5", "result": "\u901a\u8fc7\u4e94\u9636\u6bb5\u8fed\u4ee3\u6d4b\u8bd5\uff0c\u5728\u5bbd\u677e\u4e00\u81f4\u6027\u89c4\u5219\u4e0b\u6210\u529f\u8bc6\u522b\u6240\u6709\u5047\u793e\u4ea4\u5bf9\u8bdd\u4e14\u907f\u514d\u4e86\u8bef\u62a5\uff0c\u68c0\u6d4b\u901a\u5e38\u5728\u5bf9\u8bdd\u524d\u51e0\u8f6e\u5c31\u53ef\u5b8c\u6210", "conclusion": "\u8bc4\u4f30\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9884\u9632\u4eba\u4e0eAI\u4ee3\u7406\u5f62\u6210\u6709\u5bb3\u7684\u5047\u793e\u4ea4\u5173\u7cfb"}}
{"id": "2508.15439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15439", "abs": "https://arxiv.org/abs/2508.15439", "authors": ["Yogesh Kumar", "Uday Agarwal", "Manish Gupta", "Anand Mishra"], "title": "Aligning Moments in Time using Video Queries", "comment": "11 pages, 4 figures", "summary": "Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen\nevents or moments in a target video using a query video. This task poses\nseveral challenges, such as the need for semantic frame-level alignment and\nmodeling complex dependencies between query and target videos. To tackle this\nchallenging problem, we introduce MATR (Moment Alignment TRansformer), a\ntransformer-based model designed to capture semantic context as well as the\ntemporal details necessary for precise moment localization. MATR conditions\ntarget video representations on query video features using dual-stage sequence\nalignment that encodes the required correlations and dependencies. These\nrepresentations are then used to guide foreground/background classification and\nboundary prediction heads, enabling the model to accurately identify moments in\nthe target video that semantically match with the query video. Additionally, to\nprovide a strong task-specific initialization for MATR, we propose a\nself-supervised pre-training technique that involves training the model to\nlocalize random clips within videos. Extensive experiments demonstrate that\nMATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU\non an absolute scale compared to state-of-the-art methods on the popular\nActivityNet-VRL dataset. Additionally, on our newly proposed dataset,\nSportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an\nabsolute scale over strong baselines.", "AI": {"tldr": "MATR\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u89c6\u9891\u5230\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u5e8f\u5217\u5bf9\u9f50\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5728ActivityNet-VRL\u548cSportsMoments\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u5230\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u4efb\u52a1\u9700\u8981\u8bed\u4e49\u5e27\u7ea7\u5bf9\u9f50\u548c\u5efa\u6a21\u67e5\u8be2\u89c6\u9891\u4e0e\u76ee\u6807\u89c6\u9891\u4e4b\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faMATR\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u53cc\u9636\u6bb5\u5e8f\u5217\u5bf9\u9f50\u6765\u6355\u83b7\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u7ec6\u8282\uff0c\u5e76\u91c7\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6280\u672f\u6765\u521d\u59cb\u5316\u6a21\u578b\u3002", "result": "\u5728ActivityNet-VRL\u6570\u636e\u96c6\u4e0aR@1\u63d0\u534713.1%\uff0cmIoU\u63d0\u53478.1%\uff1b\u5728SportsMoments\u6570\u636e\u96c6\u4e0aR@1\u63d0\u534714.7%\uff0cmIoU\u63d0\u534714.4%\u3002", "conclusion": "MATR\u901a\u8fc7\u6709\u6548\u7684\u5e8f\u5217\u5bf9\u9f50\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5230\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u8be5\u4efb\u52a1\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.15524", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15524", "abs": "https://arxiv.org/abs/2508.15524", "authors": ["Naama Rivlin-Angert", "Guy Mor-Lan"], "title": "The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech", "comment": null, "summary": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse.", "AI": {"tldr": "\u9996\u4e2a\u5927\u89c4\u6a21\u653f\u6cbb\u53bb\u5408\u6cd5\u5316\u8bdd\u8bed(PDD)\u8ba1\u7b97\u7814\u7a76\uff0c\u6784\u5efa\u5e0c\u4f2f\u6765\u8bed\u8bed\u6599\u5e93\u5e76\u5f00\u53d1\u4e24\u9636\u6bb5\u5206\u7c7b\u6a21\u578b\uff0c\u53d1\u73b0PDD\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u653f\u6cbb\u4e8b\u4ef6\u4e2d\u663e\u8457\u589e\u52a0", "motivation": "\u7814\u7a76\u653f\u6cbb\u53bb\u5408\u6cd5\u5316\u8bdd\u8bed(PDD)\u7684\u81ea\u52a8\u5206\u6790\u53ef\u884c\u6027\uff0c\u4ee5\u7406\u89e3\u6c11\u4e3b\u8bdd\u8bed\u4e2d\u7684\u89c4\u8303\u6027\u653b\u51fb\u73b0\u8c61", "method": "\u6784\u5efa\u5305\u542b10,410\u53e5\u5e0c\u4f2f\u6765\u8bed\u8bed\u6599\u5e93\uff0c\u624b\u52a8\u6807\u6ce81,812\u4e2aPDD\u5b9e\u4f8b\uff1b\u5f00\u53d1\u4e24\u9636\u6bb5\u5206\u7c7b\u7ba1\u9053\uff0c\u7ed3\u5408\u5fae\u8c03\u7f16\u7801\u5668\u6a21\u578b\u548c\u89e3\u7801\u5668LLM", "result": "\u6700\u4f73\u6a21\u578b(DictaLM 2.0)\u5728\u4e8c\u5143PDD\u68c0\u6d4b\u4e2dF1\u5f97\u52060.74\uff0c\u7279\u5f81\u5206\u7c7bmacro-F1\u5f97\u52060.67\uff1b\u53d1\u73b0PDD\u572830\u5e74\u95f4\u663e\u8457\u4e0a\u5347\uff0c\u793e\u4ea4\u5a92\u4f53\u6bd4\u8bae\u4f1a\u8fa9\u8bba\u66f4\u666e\u904d\uff0c\u53f3\u503e\u884c\u4e3a\u8005\u4f7f\u7528\u66f4\u591a", "conclusion": "\u81ea\u52a8\u5316PDD\u5206\u6790\u5bf9\u4e8e\u7406\u89e3\u6c11\u4e3b\u8bdd\u8bed\u5177\u6709\u53ef\u884c\u6027\u548c\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u63ed\u793a\u653f\u6cbb\u8bdd\u8bed\u4e2d\u7684\u89c4\u8303\u6027\u653b\u51fb\u6a21\u5f0f"}}
{"id": "2508.15451", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.15451", "abs": "https://arxiv.org/abs/2508.15451", "authors": ["H. I. Nurdin", "C. A. Nijhuis"], "title": "A Solvable Molecular Switch Model for Stable Temporal Information Processing", "comment": "21 pages, 6 figures, submitted for publication. Comments are welcome", "summary": "This paper studies an input-driven one-state differential equation model\ninitially developed for an experimentally demonstrated dynamic molecular switch\nthat switches like synapses in the brain do. The linear-in-the-state and\nnonlinear-in-the-input model is exactly solvable, and it is shown that it also\npossesses mathematical properties of convergence and fading memory that enable\nstable processing of time-varying inputs by nonlinear dynamical systems. Thus,\nthe model exhibits the co-existence of biologically-inspired behavior and\ndesirable mathematical properties for stable learning on sequential data. The\nresults give theoretical support for the use of the dynamic molecular switches\nas computational units in deep cascaded/layered feedforward and recurrent\narchitectures as well as other more general structures for neuromorphic\ncomputing. They could also inspire more general exactly solvable models that\ncan be fitted to emulate arbitrary physical devices which can mimic\nbrain-inspired behaviour and perform stable computation on input signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u8f93\u5165\u9a71\u52a8\u7684\u5355\u72b6\u6001\u5fae\u5206\u65b9\u7a0b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6700\u521d\u4e3a\u5b9e\u9a8c\u8bc1\u660e\u7684\u52a8\u6001\u5206\u5b50\u5f00\u5173\u5f00\u53d1\uff0c\u5177\u6709\u7c7b\u4f3c\u5927\u8111\u7a81\u89e6\u7684\u5207\u6362\u7279\u6027\u3002\u6a21\u578b\u5177\u6709\u7cbe\u786e\u53ef\u89e3\u6027\u3001\u6536\u655b\u6027\u548c\u8870\u51cf\u8bb0\u5fc6\u7b49\u6570\u5b66\u7279\u6027\uff0c\u80fd\u591f\u7a33\u5b9a\u5904\u7406\u65f6\u53d8\u8f93\u5165\u3002", "motivation": "\u7814\u7a76\u52a8\u6001\u5206\u5b50\u5f00\u5173\u7684\u8ba1\u7b97\u7279\u6027\uff0c\u63a2\u7d22\u5176\u4f5c\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5355\u5143\u7684\u6f5c\u529b\uff0c\u4e3a\u5927\u8111\u542f\u53d1\u7684\u7a33\u5b9a\u8ba1\u7b97\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u72b6\u6001\u3001\u975e\u7ebf\u6027\u8f93\u5165\u7684\u5fae\u5206\u65b9\u7a0b\u6a21\u578b\uff0c\u8fdb\u884c\u6570\u5b66\u5206\u6790\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u9a8c\u8bc1\u6a21\u578b\u7684\u6536\u655b\u6027\u3001\u8870\u51cf\u8bb0\u5fc6\u548c\u7cbe\u786e\u53ef\u89e3\u6027\u3002", "result": "\u6a21\u578b\u5c55\u73b0\u51fa\u751f\u7269\u5b66\u542f\u53d1\u884c\u4e3a\u4e0e\u7a33\u5b9a\u6570\u5b66\u7279\u6027\u7684\u5171\u5b58\uff0c\u80fd\u591f\u7a33\u5b9a\u5904\u7406\u65f6\u5e8f\u6570\u636e\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u524d\u9988\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u52a8\u6001\u5206\u5b50\u5f00\u5173\u4f5c\u4e3a\u8ba1\u7b97\u5355\u5143\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u7cbe\u786e\u53ef\u89e3\u6a21\u578b\u6765\u6a21\u62df\u7269\u7406\u8bbe\u5907\uff0c\u5b9e\u73b0\u5927\u8111\u542f\u53d1\u7684\u7a33\u5b9a\u8ba1\u7b97\u3002"}}
{"id": "2508.15641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15641", "abs": "https://arxiv.org/abs/2508.15641", "authors": ["Pengcheng Fang", "Yuxia Chen", "Rui Guo"], "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding", "comment": null, "summary": "Understanding videos requires more than answering open ended questions, it\ndemands the ability to pinpoint when events occur and how entities interact\nacross time. While recent Video LLMs have achieved remarkable progress in\nholistic reasoning, they remain coarse in temporal perception: timestamps are\nencoded only implicitly, frame level features are weak in capturing continuity,\nand language vision alignment often drifts from the entities of interest. In\nthis paper, we present Grounded VideoDiT, a Video LLM designed to overcome\nthese limitations by introducing three key innovations. First, a Diffusion\nTemporal Latent (DTL) encoder enhances boundary sensitivity and maintains\ntemporal consistency. Second, object grounded representations explicitly bind\nquery entities to localized visual evidence, strengthening alignment. Third, a\nmixed token scheme with discrete temporal tokens provides explicit timestamp\nmodeling, enabling fine grained temporal reasoning. Together, these designs\nequip Grounded VideoDiT with robust grounding capabilities, as validated by\nstate of the art results on Charades STA, NExT GQA, and multiple VideoQA\nbenchmarks.", "AI": {"tldr": "Grounded VideoDiT\u662f\u4e00\u4e2a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563\u65f6\u5e8f\u6f5c\u5728\u7f16\u7801\u5668\u3001\u5bf9\u8c61\u63a5\u5730\u8868\u793a\u548c\u6df7\u5408\u4ee4\u724c\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891LLM\u5728\u65f6\u95f4\u611f\u77e5\u7cbe\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891LLM\u5728\u6574\u4f53\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u65f6\u95f4\u611f\u77e5\u4e0a\u4ecd\u663e\u7c97\u7cd9\uff1a\u65f6\u95f4\u6233\u4ec5\u9690\u5f0f\u7f16\u7801\u3001\u5e27\u7ea7\u7279\u5f81\u8fde\u7eed\u6027\u6355\u6349\u80fd\u529b\u5f31\u3001\u8bed\u8a00\u89c6\u89c9\u5bf9\u9f50\u5bb9\u6613\u504f\u79bb\u5173\u6ce8\u5b9e\u4f53\u3002", "method": "1) \u6269\u6563\u65f6\u5e8f\u6f5c\u5728(DTL)\u7f16\u7801\u5668\u589e\u5f3a\u8fb9\u754c\u654f\u611f\u6027\u548c\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff1b2) \u5bf9\u8c61\u63a5\u5730\u8868\u793a\u663e\u5f0f\u7ed1\u5b9a\u67e5\u8be2\u5b9e\u4f53\u5230\u5c40\u90e8\u89c6\u89c9\u8bc1\u636e\uff1b3) \u6df7\u5408\u4ee4\u724c\u65b9\u6848\u63d0\u4f9b\u663e\u5f0f\u65f6\u95f4\u6233\u5efa\u6a21", "result": "\u5728Charades STA\u3001NExT GQA\u548c\u591a\u4e2aVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u5f3a\u5927\u63a5\u5730\u80fd\u529b", "conclusion": "Grounded VideoDiT\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u8bbe\u8ba1\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u89c6\u9891LLM\u7684\u65f6\u95f4\u611f\u77e5\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u63a8\u7406\u548c\u7a33\u5065\u7684\u63a5\u5730\u80fd\u529b"}}
{"id": "2508.15711", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15711", "abs": "https://arxiv.org/abs/2508.15711", "authors": ["Abhijit Paul", "Mashiat Amin Farin", "Sharif Md. Abdullah", "Ahmedul Kabir", "Zarif Masud", "Shebuti Rayana"], "title": "Stemming -- The Evolution and Current State with a Focus on Bangla", "comment": null, "summary": "Bangla, the seventh most widely spoken language worldwide with 300 million\nnative speakers, faces digital under-representation due to limited resources\nand lack of annotated datasets. Stemming, a critical preprocessing step in\nlanguage analysis, is essential for low-resource, highly-inflectional languages\nlike Bangla, because it can reduce the complexity of algorithms and models by\nsignificantly reducing the number of words the algorithm needs to consider.\nThis paper conducts a comprehensive survey of stemming approaches, emphasizing\nthe importance of handling morphological variants effectively. While exploring\nthe landscape of Bangla stemming, it becomes evident that there is a\nsignificant gap in the existing literature. The paper highlights the\ndiscontinuity from previous research and the scarcity of accessible\nimplementations for replication. Furthermore, it critiques the evaluation\nmethodologies, stressing the need for more relevant metrics. In the context of\nBangla's rich morphology and diverse dialects, the paper acknowledges the\nchallenges it poses. To address these challenges, the paper suggests directions\nfor Bangla stemmer development. It concludes by advocating for robust Bangla\nstemmers and continued research in the field to enhance language analysis and\nprocessing.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5b5f\u52a0\u62c9\u8bed\u8bcd\u5e72\u63d0\u53d6\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u6307\u51fa\u8be5\u9886\u57df\u5b58\u5728\u7814\u7a76\u65ad\u5c42\u3001\u5b9e\u73b0\u7a00\u7f3a\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u53d1\u5c55\u65b9\u5411\u5efa\u8bae\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u5168\u7403\u7b2c\u4e03\u5927\u8bed\u8a00\u62e5\u67093\u4ebf\u6bcd\u8bed\u8005\uff0c\u4f46\u5728\u6570\u5b57\u9886\u57df\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\u8d44\u6e90\u3002\u8bcd\u5e72\u63d0\u53d6\u4f5c\u4e3a\u8bed\u8a00\u5206\u6790\u7684\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5bf9\u4e8e\u5b5f\u52a0\u62c9\u8bed\u8fd9\u79cd\u4f4e\u8d44\u6e90\u3001\u9ad8\u5c48\u6298\u6027\u8bed\u8a00\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf9\u73b0\u6709\u5b5f\u52a0\u62c9\u8bed\u8bcd\u5e72\u63d0\u53d6\u65b9\u6cd5\u7684\u5168\u9762\u8c03\u67e5\u548c\u5206\u6790\uff0c\u6279\u5224\u6027\u5730\u8bc4\u4f30\u73b0\u6709\u7814\u7a76\u65b9\u6cd5\u3001\u5b9e\u73b0\u53ef\u7528\u6027\u548c\u8bc4\u4f30\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u8bcd\u5e72\u63d0\u53d6\u9886\u57df\u5b58\u5728\u663e\u8457\u7684\u7814\u7a76\u65ad\u5c42\uff0c\u7f3a\u4e4f\u53ef\u590d\u73b0\u7684\u5b9e\u73b0\u65b9\u6848\uff0c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u76f8\u5173\u548c\u6709\u6548\u3002", "conclusion": "\u5efa\u8bae\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5b5f\u52a0\u62c9\u8bed\u8bcd\u5e72\u63d0\u53d6\u5668\uff0c\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u7ee7\u7eed\u6df1\u5165\u7814\u7a76\u4ee5\u63d0\u5347\u5b5f\u52a0\u62c9\u8bed\u7684\u8bed\u8a00\u5206\u6790\u548c\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2508.15637", "categories": ["cs.LG", "cs.CL", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.15637", "abs": "https://arxiv.org/abs/2508.15637", "authors": ["Lucas Gautheron", "Evan Kidd", "Anton Malko", "Marvin Lavechin", "Alejandrina Cristia"], "title": "Classification errors distort findings in automated speech processing: examples and solutions from child-development research", "comment": null, "summary": "With the advent of wearable recorders, scientists are increasingly turning to\nautomated methods of analysis of audio and video data in order to measure\nchildren's experience, behavior, and outcomes, with a sizable literature\nemploying long-form audio-recordings to study language acquisition. While\nnumerous articles report on the accuracy and reliability of the most popular\nautomated classifiers, less has been written on the downstream effects of\nclassification errors on measurements and statistical inferences (e.g., the\nestimate of correlations and effect sizes in regressions). This paper proposes\na Bayesian approach to study the effects of algorithmic errors on key\nscientific questions, including the effect of siblings on children's language\nexperience and the association between children's production and their input.\nIn both the most commonly used \\gls{lena}, and an open-source alternative (the\nVoice Type Classifier from the ACLEW system), we find that classification\nerrors can significantly distort estimates. For instance, automated annotations\nunderestimated the negative effect of siblings on adult input by 20--80\\%,\npotentially placing it below statistical significance thresholds. We further\nshow that a Bayesian calibration approach for recovering unbiased estimates of\neffect sizes can be effective and insightful, but does not provide a fool-proof\nsolution. Both the issue reported and our solution may apply to any classifier\ninvolving event detection and classification with non-zero error rates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u7814\u7a76\u81ea\u52a8\u5206\u7c7b\u5668\u9519\u8bef\u5bf9\u79d1\u5b66\u6d4b\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u8bed\u8a00\u4e60\u5f97\u7814\u7a76\u4e2d\uff0c\u5206\u7c7b\u9519\u8bef\u4f1a\u663e\u8457\u626d\u66f2\u6548\u5e94\u5927\u5c0f\u4f30\u8ba1\uff0c\u5e76\u5c55\u793a\u4e86\u8d1d\u53f6\u65af\u6821\u51c6\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u5f55\u97f3\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u7814\u7a76\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u81ea\u52a8\u5206\u7c7b\u5668\u5206\u6790\u97f3\u9891\u6570\u636e\u6765\u7814\u7a76\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u5206\u7c7b\u5668\u7684\u51c6\u786e\u6027\uff0c\u4f46\u8f83\u5c11\u63a2\u8ba8\u5206\u7c7b\u9519\u8bef\u5bf9\u4e0b\u6e38\u7edf\u8ba1\u63a8\u65ad\uff08\u5982\u76f8\u5173\u6027\u548c\u6548\u5e94\u5927\u5c0f\u4f30\u8ba1\uff09\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u7814\u7a76\u7b97\u6cd5\u9519\u8bef\u5bf9\u5173\u952e\u79d1\u5b66\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5144\u5f1f\u59d0\u59b9\u5bf9\u513f\u7ae5\u8bed\u8a00\u7ecf\u9a8c\u7684\u5f71\u54cd\u4ee5\u53ca\u513f\u7ae5\u4ea7\u51fa\u4e0e\u8f93\u5165\u4e4b\u95f4\u7684\u5173\u8054\u3002\u5728LENA\u548cACLEW\u7cfb\u7edf\u7684Voice Type Classifier\u4e24\u79cd\u5206\u7c7b\u5668\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u5206\u7c7b\u9519\u8bef\u4f1a\u663e\u8457\u626d\u66f2\u4f30\u8ba1\u503c\uff0c\u4f8b\u5982\u81ea\u52a8\u6807\u6ce8\u4f4e\u4f30\u4e86\u5144\u5f1f\u59d0\u59b9\u5bf9\u6210\u4eba\u8f93\u5165\u7684\u8d1f\u9762\u6548\u5e94\u8fbe20-80%\uff0c\u53ef\u80fd\u5bfc\u81f4\u7ed3\u679c\u4f4e\u4e8e\u7edf\u8ba1\u663e\u8457\u6027\u9608\u503c\u3002\u8d1d\u53f6\u65af\u6821\u51c6\u65b9\u6cd5\u5728\u6062\u590d\u65e0\u504f\u6548\u5e94\u5927\u5c0f\u4f30\u8ba1\u65b9\u9762\u6709\u6548\u4f46\u4e0d\u5b8c\u7f8e\u3002", "conclusion": "\u5206\u7c7b\u9519\u8bef\u4f1a\u4e25\u91cd\u5f71\u54cd\u79d1\u5b66\u6d4b\u91cf\u7684\u51c6\u786e\u6027\uff0c\u8d1d\u53f6\u65af\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5e76\u975e\u4e07\u65e0\u4e00\u5931\u3002\u8fd9\u4e00\u95ee\u9898\u53ca\u89e3\u51b3\u65b9\u6848\u9002\u7528\u4e8e\u4efb\u4f55\u6d89\u53ca\u4e8b\u4ef6\u68c0\u6d4b\u548c\u5206\u7c7b\u4e14\u9519\u8bef\u7387\u975e\u96f6\u7684\u5206\u7c7b\u5668\u3002"}}
