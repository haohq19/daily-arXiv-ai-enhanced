{"id": "2509.15230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15230", "abs": "https://arxiv.org/abs/2509.15230", "authors": ["Rutger Hendrix", "Giovanni Patan\u00e8", "Leonardo G. Russo", "Simone Carnemolla", "Giovanni Bellitto", "Federica Proietto Salanitri", "Concetto Spampinato", "Matteo Pennisi"], "title": "Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning", "comment": "Accepted at ACM multimedia 2025 BNI track", "summary": "Foundation models have transformed multimedia analysis by enabling robust and\ntransferable representations across diverse modalities and tasks. However,\ntheir static deployment conflicts with growing societal and regulatory demands\n-- particularly the need to unlearn specific data upon request, as mandated by\nprivacy frameworks such as the GDPR. Traditional unlearning approaches,\nincluding retraining, activation editing, or distillation, are often\ncomputationally expensive, fragile, and ill-suited for real-time or\ncontinuously evolving systems. In this paper, we propose a paradigm shift:\nrethinking unlearning not as a retroactive intervention but as a built-in\ncapability. We introduce a prompt-based learning framework that unifies\nknowledge acquisition and removal within a single training phase. Rather than\nencoding information in model weights, our approach binds class-level semantics\nto dedicated prompt tokens. This design enables instant unlearning simply by\nremoving the corresponding prompt -- without retraining, model modification, or\naccess to original data. Experiments demonstrate that our framework preserves\npredictive performance on retained classes while effectively erasing forgotten\nones. Beyond utility, our method exhibits strong privacy and security\nguarantees: it is resistant to membership inference attacks, and prompt removal\nprevents any residual knowledge extraction, even under adversarial conditions.\nThis ensures compliance with data protection principles and safeguards against\nunauthorized access to forgotten information, making the framework suitable for\ndeployment in sensitive and regulated environments. Overall, by embedding\nremovability into the architecture itself, this work establishes a new\nfoundation for designing modular, scalable and ethically responsive AI models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u77e5\u8bc6\u83b7\u53d6\u548c\u79fb\u9664\u7edf\u4e00\u5728\u5355\u4e00\u8bad\u7ec3\u9636\u6bb5\u4e2d\uff0c\u901a\u8fc7\u5c06\u7c7b\u522b\u7ea7\u8bed\u4e49\u7ed1\u5b9a\u5230\u4e13\u7528\u63d0\u793a\u6807\u8bb0\u6765\u5b9e\u73b0\u5373\u65f6\u9057\u5fd8\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6a21\u578b\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u591a\u5a92\u4f53\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9759\u6001\u90e8\u7f72\u4e0e\u65e5\u76ca\u589e\u957f\u7684\u793e\u4f1a\u548c\u76d1\u7ba1\u9700\u6c42\uff08\u5982GDPR\u8981\u6c42\u7684\u6570\u636e\u9057\u5fd8\uff09\u5b58\u5728\u51b2\u7a81\u3002\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8106\u5f31\u4e14\u4e0d\u9002\u5408\u5b9e\u65f6\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4fe1\u606f\u7f16\u7801\u5728\u63d0\u793a\u6807\u8bb0\u800c\u975e\u6a21\u578b\u6743\u91cd\u4e2d\u3002\u901a\u8fc7\u79fb\u9664\u5bf9\u5e94\u63d0\u793a\u6807\u8bb0\u5373\u53ef\u5b9e\u73b0\u5373\u65f6\u9057\u5fd8\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u6a21\u578b\u4fee\u6539\u6216\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u4fdd\u7559\u7c7b\u522b\u4e0a\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u64e6\u9664\u9057\u5fd8\u7c7b\u522b\u3002\u5177\u6709\u5f3a\u5927\u7684\u9690\u79c1\u548c\u5b89\u5168\u4fdd\u8bc1\uff0c\u80fd\u62b5\u6297\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u9632\u6b62\u6b8b\u7559\u77e5\u8bc6\u63d0\u53d6\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53ef\u79fb\u9664\u6027\u5d4c\u5165\u67b6\u6784\u672c\u8eab\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u548c\u7b26\u5408\u4f26\u7406\u7684AI\u6a21\u578b\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\uff0c\u7279\u522b\u9002\u5408\u654f\u611f\u548c\u53d7\u76d1\u7ba1\u73af\u5883\u90e8\u7f72\u3002"}}
{"id": "2509.15412", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.15412", "abs": "https://arxiv.org/abs/2509.15412", "authors": ["Easop Lee", "Samuel A. Moore", "Boyuan Chen"], "title": "Sym2Real: Symbolic Dynamics with Residual Learning for Data-Efficient Adaptive Control", "comment": null, "summary": "We present Sym2Real, a fully data-driven framework that provides a principled\nway to train low-level adaptive controllers in a highly data-efficient manner.\nUsing only about 10 trajectories, we achieve robust control of both a quadrotor\nand a racecar in the real world, without expert knowledge or simulation tuning.\nOur approach achieves this data efficiency by bringing symbolic regression to\nreal-world robotics while addressing key challenges that prevent its direct\napplication, including noise sensitivity and model degradation that lead to\nunsafe control. Our key observation is that the underlying physics is often\nshared for a system regardless of internal or external changes. Hence, we\nstrategically combine low-fidelity simulation data with targeted real-world\nresidual learning. Through experimental validation on quadrotor and racecar\nplatforms, we demonstrate consistent data-efficient adaptation across six\nout-of-distribution sim2sim scenarios and successful sim2real transfer across\nfive real-world conditions. More information and videos can be found at at\nhttp://generalroboticslab.com/Sym2Real", "AI": {"tldr": "Sym2Real\u662f\u4e00\u4e2a\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u56de\u5f52\u548c\u4f4e\u7cbe\u5ea6\u4eff\u771f\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6b8b\u5dee\u5b66\u4e60\u7684\u7b56\u7565\u7ed3\u5408\uff0c\u4ec5\u9700\u7ea610\u6761\u8f68\u8ff9\u5373\u53ef\u5b9e\u73b0\u56db\u65cb\u7ffc\u548c\u8d5b\u8f66\u7684\u9c81\u68d2\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u56de\u5f52\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u566a\u58f0\u654f\u611f\u6027\u548c\u6a21\u578b\u9000\u5316\u7b49\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u65e0\u9700\u4e13\u5bb6\u77e5\u8bc6\u6216\u4eff\u771f\u8c03\u4f18\u7684\u6570\u636e\u9ad8\u6548\u81ea\u9002\u5e94\u63a7\u5236\u3002", "method": "\u5229\u7528\u7cfb\u7edf\u7269\u7406\u7279\u6027\u7684\u5171\u4eab\u6027\uff0c\u7ed3\u5408\u4f4e\u7cbe\u5ea6\u4eff\u771f\u6570\u636e\u548c\u6709\u9488\u5bf9\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6b8b\u5dee\u5b66\u4e60\uff0c\u901a\u8fc7\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u8bad\u7ec3\u4f4e\u5c42\u81ea\u9002\u5e94\u63a7\u5236\u5668\u3002", "result": "\u57286\u4e2a\u5206\u5e03\u5916\u4eff\u771f\u5230\u4eff\u771f\u573a\u666f\u4e2d\u5b9e\u73b0\u4e00\u81f4\u7684\u6570\u636e\u9ad8\u6548\u9002\u5e94\uff0c\u5e76\u57285\u4e2a\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u6210\u529f\u5b8c\u6210\u4eff\u771f\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "Sym2Real\u6846\u67b6\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u7406\u6027\u7684\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7b26\u53f7\u56de\u5f52\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.15423", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.15423", "abs": "https://arxiv.org/abs/2509.15423", "authors": ["Christopher Oeltjen", "Carson Sobolewski", "Saleh Faghfoorian", "Lorant Domokos", "Giancarlo Vidal", "Ivan Ruchkin"], "title": "Online Slip Detection and Friction Coefficient Estimation for Autonomous Racing", "comment": "Equal contribution by the first three authors", "summary": "Accurate knowledge of the tire-road friction coefficient (TRFC) is essential\nfor vehicle safety, stability, and performance, especially in autonomous\nracing, where vehicles often operate at the friction limit. However, TRFC\ncannot be directly measured with standard sensors, and existing estimation\nmethods either depend on vehicle or tire models with uncertain parameters or\nrequire large training datasets. In this paper, we present a lightweight\napproach for online slip detection and TRFC estimation. Our approach relies\nsolely on IMU and LiDAR measurements and the control actions, without special\ndynamical or tire models, parameter identification, or training data. Slip\nevents are detected in real time by comparing commanded and measured motions,\nand the TRFC is then estimated directly from observed accelerations under\nno-slip conditions. Experiments with a 1:10-scale autonomous racing car across\ndifferent friction levels demonstrate that the proposed approach achieves\naccurate and consistent slip detections and friction coefficients, with results\nclosely matching ground-truth measurements. These findings highlight the\npotential of our simple, deployable, and computationally efficient approach for\nreal-time slip monitoring and friction coefficient estimation in autonomous\ndriving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5728\u7ebf\u6ed1\u79fb\u68c0\u6d4b\u548c\u8f6e\u80ce-\u8def\u9762\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528IMU\u3001LiDAR\u548c\u63a7\u5236\u52a8\u4f5c\uff0c\u65e0\u9700\u590d\u6742\u6a21\u578b\u6216\u8bad\u7ec3\u6570\u636e", "motivation": "\u51c6\u786e\u7684\u8f6e\u80ce-\u8def\u9762\u6469\u64e6\u7cfb\u6570\u5bf9\u8f66\u8f86\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u6781\u9650\u5de5\u51b5\u4e0b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u53c2\u6570\u6216\u5927\u91cf\u8bad\u7ec3\u6570\u636e", "method": "\u901a\u8fc7\u6bd4\u8f83\u6307\u4ee4\u8fd0\u52a8\u4e0e\u5b9e\u9645\u8fd0\u52a8\u5b9e\u65f6\u68c0\u6d4b\u6ed1\u79fb\u4e8b\u4ef6\uff0c\u5728\u65e0\u6ed1\u79fb\u6761\u4ef6\u4e0b\u76f4\u63a5\u4ece\u89c2\u6d4b\u52a0\u901f\u5ea6\u4f30\u8ba1\u6469\u64e6\u7cfb\u6570", "result": "\u57281:10\u6bd4\u4f8b\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u68c0\u6d4b\u6ed1\u79fb\u5e76\u4f30\u8ba1\u6469\u64e6\u7cfb\u6570\uff0c\u7ed3\u679c\u4e0e\u771f\u5b9e\u6d4b\u91cf\u503c\u9ad8\u5ea6\u5339\u914d", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u53ef\u90e8\u7f72\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u5177\u6709\u5b9e\u65f6\u6ed1\u79fb\u76d1\u6d4b\u548c\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u7684\u6f5c\u529b"}}
{"id": "2509.15370", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15370", "abs": "https://arxiv.org/abs/2509.15370", "authors": ["Vicky Kouni"], "title": "Adversarial generalization of unfolding (model-based) networks", "comment": "Accepted in NeurIPS2025", "summary": "Unfolding networks are interpretable networks emerging from iterative\nalgorithms, incorporate prior knowledge of data structure, and are designed to\nsolve inverse problems like compressed sensing, which deals with recovering\ndata from noisy, missing observations. Compressed sensing finds applications in\ncritical domains, from medical imaging to cryptography, where adversarial\nrobustness is crucial to prevent catastrophic failures. However, a solid\ntheoretical understanding of the performance of unfolding networks in the\npresence of adversarial attacks is still in its infancy. In this paper, we\nstudy the adversarial generalization of unfolding networks when perturbed with\n$l_2$-norm constrained attacks, generated by the fast gradient sign method.\nParticularly, we choose a family of state-of-the-art overaparameterized\nunfolding networks and deploy a new framework to estimate their adversarial\nRademacher complexity. Given this estimate, we provide adversarial\ngeneralization error bounds for the networks under study, which are tight with\nrespect to the attack level. To our knowledge, this is the first theoretical\nanalysis on the adversarial generalization of unfolding networks. We further\npresent a series of experiments on real-world data, with results corroborating\nour derived theory, consistently for all data. Finally, we observe that the\nfamily's overparameterization can be exploited to promote adversarial\nrobustness, shedding light on how to efficiently robustify neural networks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5c55\u5f00\u7f51\u7edc\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u5bf9\u6297Rademacher\u590d\u6742\u5ea6\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u4e0e\u653b\u51fb\u6c34\u5e73\u76f8\u5173\u7684\u7d27\u81f4\u6cdb\u5316\u8bef\u5dee\u754c\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u53d1\u73b0\u8fc7\u53c2\u6570\u5316\u53ef\u4ee5\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u5c55\u5f00\u7f51\u7edc\u5728\u538b\u7f29\u611f\u77e5\u7b49\u9006\u95ee\u9898\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u7406\u8bba\u7406\u89e3\u5c1a\u4e0d\u6210\u719f\u3002\u7531\u4e8e\u8fd9\u4e9b\u7f51\u7edc\u5e94\u7528\u4e8e\u533b\u7597\u6210\u50cf\u548c\u5bc6\u7801\u5b66\u7b49\u5173\u952e\u9886\u57df\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9009\u62e9\u6700\u5148\u8fdb\u7684\u8fc7\u53c2\u6570\u5316\u5c55\u5f00\u7f51\u7edc\u5bb6\u65cf\uff0c\u91c7\u7528\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u65b9\u6cd5\u751f\u6210l2\u8303\u6570\u7ea6\u675f\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u90e8\u7f72\u65b0\u7684\u5bf9\u6297Rademacher\u590d\u6742\u5ea6\u4f30\u8ba1\u6846\u67b6\u6765\u5206\u6790\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u63a8\u5bfc\u51fa\u7d27\u81f4\u4e8e\u653b\u51fb\u6c34\u5e73\u7684\u5bf9\u6297\u6cdb\u5316\u8bef\u5dee\u754c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u7406\u8bba\u5206\u6790\u5728\u6240\u6709\u771f\u5b9e\u6570\u636e\u4e0a\u4e00\u81f4\u6210\u7acb\u3002\u53d1\u73b0\u8fc7\u53c2\u6570\u5316\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5bf9\u5c55\u5f00\u7f51\u7edc\u5bf9\u6297\u6cdb\u5316\u6027\u80fd\u7684\u7406\u8bba\u5206\u6790\uff0c\u4e3a\u5982\u4f55\u6709\u6548\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u6307\u5bfc\u3002"}}
{"id": "2509.15430", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15430", "abs": "https://arxiv.org/abs/2509.15430", "authors": ["Liuyuan Jiang", "Xiaodong Cui", "Brian Kingsbury", "Tianyi Chen", "Lisha Chen"], "title": "BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition", "comment": "5 pages including reference", "summary": "Speech is a rich signal, and labeled audio-text pairs are costly, making\nself-supervised learning essential for scalable representation learning. A core\nchallenge in speech SSL is generating pseudo-labels that are both informative\nand efficient: strong labels, such as those used in HuBERT, improve downstream\nperformance but rely on external encoders and multi-stage pipelines, while\nefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.\nWe propose BiRQ, a bilevel SSL framework that combines the efficiency of\nBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key\nidea is to reuse part of the model itself as a pseudo-label generator:\nintermediate representations are discretized by a random-projection quantizer\nto produce enhanced labels, while anchoring labels derived directly from the\nraw input stabilize training and prevent collapse. Training is formulated as an\nefficient first-order bilevel optimization problem, solved end-to-end with\ndifferentiable Gumbel-softmax selection. This design eliminates the need for\nexternal label encoders, reduces memory cost, and enables iterative label\nrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ\nwhile maintaining low complexity and computational efficiency. We validate our\nmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMI\nmeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.", "AI": {"tldr": "BiRQ\u662f\u4e00\u4e2a\u53cc\u5c42\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86BEST-RQ\u7684\u6548\u7387\u548cHuBERT\u98ce\u683c\u6807\u7b7e\u589e\u5f3a\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u91cd\u7528\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6807\u7b7e\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u4f2a\u6807\u7b7e\u751f\u6210\u7684\u6838\u5fc3\u6311\u6218\uff1a\u5f3a\u6807\u7b7e\uff08\u5982HuBERT\uff09\u6027\u80fd\u597d\u4f46\u4f9d\u8d56\u5916\u90e8\u7f16\u7801\u5668\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u800c\u9ad8\u6548\u65b9\u6cd5\uff08\u5982BEST-RQ\uff09\u7b80\u5355\u4f46\u6807\u7b7e\u8d28\u91cf\u8f83\u5f31\u3002", "method": "\u63d0\u51fa\u53cc\u5c42SSL\u6846\u67b6\uff0c\u4f7f\u7528\u968f\u673a\u6295\u5f71\u91cf\u5316\u5668\u5c06\u4e2d\u95f4\u8868\u793a\u79bb\u6563\u5316\u751f\u6210\u589e\u5f3a\u6807\u7b7e\uff0c\u540c\u65f6\u4ece\u539f\u59cb\u8f93\u5165\u76f4\u63a5\u6d3e\u751f\u7684\u951a\u5b9a\u6807\u7b7e\u7a33\u5b9a\u8bad\u7ec3\u3002\u91c7\u7528\u53ef\u5faeGumbel-softmax\u9009\u62e9\u7684\u4e00\u9636\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08960\u5c0f\u65f6LibriSpeech\u3001150\u5c0f\u65f6AMI\u4f1a\u8bae\u548c5000\u5c0f\u65f6YODAS\uff09\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4BEST-RQ\u83b7\u5f97\u6301\u7eed\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BiRQ\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u6807\u7b7e\u7f16\u7801\u5668\u7684\u9700\u6c42\uff0c\u964d\u4f4e\u4e86\u5185\u5b58\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u8fed\u4ee3\u6807\u7b7e\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.15807", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15807", "abs": "https://arxiv.org/abs/2509.15807", "authors": ["Yuyang Zhang", "Zhuoli Tian", "Jinsheng Wei", "Meng Guo"], "title": "FlyKites: Human-centric Interactive Exploration and Assistance under Limited Communication", "comment": null, "summary": "Fleets of autonomous robots have been deployed for exploration of unknown\nscenes for features of interest, e.g., subterranean exploration,\nreconnaissance, search and rescue missions. During exploration, the robots may\nencounter un-identified targets, blocked passages, interactive objects,\ntemporary failure, or other unexpected events, all of which require consistent\nhuman assistance with reliable communication for a time period. This however\ncan be particularly challenging if the communication among the robots is\nseverely restricted to only close-range exchange via ad-hoc networks,\nespecially in extreme environments like caves and underground tunnels. This\npaper presents a novel human-centric interactive exploration and assistance\nframework called FlyKites, for multi-robot systems under limited communication.\nIt consists of three interleaved components: (I) the distributed exploration\nand intermittent communication (called the \"spread mode\"), where the robots\ncollaboratively explore the environment and exchange local data among the fleet\nand with the operator; (II) the simultaneous optimization of the relay\ntopology, the operator path, and the assignment of robots to relay roles\n(called the \"relay mode\"), such that all requested assistance can be provided\nwith minimum delay; (III) the human-in-the-loop online execution, where the\nrobots switch between different roles and interact with the operator\nadaptively. Extensive human-in-the-loop simulations and hardware experiments\nare performed over numerous challenging scenes.", "AI": {"tldr": "FlyKites\u662f\u4e00\u4e2a\u9762\u5411\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6709\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u63a2\u7d22\u4e0e\u534f\u52a9\u6846\u67b6\uff0c\u5305\u542b\u5206\u5e03\u5f0f\u63a2\u7d22\u3001\u4e2d\u7ee7\u62d3\u6251\u4f18\u5316\u548c\u4eba\u673a\u5728\u7ebf\u6267\u884c\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6", "motivation": "\u89e3\u51b3\u5728\u6781\u7aef\u73af\u5883\uff08\u5982\u6d1e\u7a74\u3001\u5730\u4e0b\u96a7\u9053\uff09\u4e2d\uff0c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u56e0\u901a\u4fe1\u53d7\u9650\u800c\u96be\u4ee5\u83b7\u5f97\u6301\u7eed\u4eba\u7c7b\u534f\u52a9\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9047\u5230\u672a\u77e5\u76ee\u6807\u3001\u963b\u585e\u901a\u9053\u7b49\u610f\u5916\u4e8b\u4ef6\u65f6", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff1a(I)\u5206\u5e03\u5f0f\u63a2\u7d22\u4e0e\u95f4\u6b47\u901a\u4fe1\u6a21\u5f0f\uff1b(II)\u540c\u65f6\u4f18\u5316\u4e2d\u7ee7\u62d3\u6251\u3001\u64cd\u4f5c\u5458\u8def\u5f84\u548c\u673a\u5668\u4eba\u89d2\u8272\u5206\u914d\uff1b(III)\u4eba\u673a\u5728\u7ebf\u81ea\u9002\u5e94\u6267\u884c\uff0c\u673a\u5668\u4eba\u6839\u636e\u4e0d\u540c\u89d2\u8272\u4e0e\u64cd\u4f5c\u5458\u4ea4\u4e92", "result": "\u901a\u8fc7\u5927\u91cf\u5177\u6709\u6311\u6218\u6027\u573a\u666f\u7684\u4eba\u673a\u56de\u8def\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "FlyKites\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6709\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63a2\u7d22\u4e0e\u534f\u52a9\u95ee\u9898\uff0c\u786e\u4fdd\u6240\u6709\u8bf7\u6c42\u7684\u534f\u52a9\u90fd\u80fd\u4ee5\u6700\u5c0f\u5ef6\u8fdf\u63d0\u4f9b"}}
{"id": "2509.15620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15620", "abs": "https://arxiv.org/abs/2509.15620", "authors": ["Bofu Dong", "Pritesh Shah", "Sumedh Sonawane", "Tiyasha Banerjee", "Erin Brady", "Xinya Du", "Ming Jiang"], "title": "SciEvent: Benchmarking Multi-domain Scientific Event Extraction", "comment": "9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted\n  to EMNLP 2025 (Main Conference)", "summary": "Scientific information extraction (SciIE) has primarily relied on\nentity-relation extraction in narrow domains, limiting its applicability to\ninterdisciplinary research and struggling to capture the necessary context of\nscientific information, often resulting in fragmented or conflicting\nstatements. In this paper, we introduce SciEvent, a novel multi-domain\nbenchmark of scientific abstracts annotated via a unified event extraction (EE)\nschema designed to enable structured and context-aware understanding of\nscientific content. It includes 500 abstracts across five research domains,\nwith manual annotations of event segments, triggers, and fine-grained\narguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting\nabstracts into core scientific activities--Background, Method, Result, and\nConclusion; and (2) extracting the corresponding triggers and arguments.\nExperiments with fine-tuned EE models, large language models (LLMs), and human\nannotators reveal a performance gap, with current models struggling in domains\nsuch as sociology and humanities. SciEvent serves as a challenging benchmark\nand a step toward generalizable, multi-domain SciIE.", "AI": {"tldr": "SciEvent\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u9886\u57df\u79d1\u5b66\u4fe1\u606f\u63d0\u53d6\u57fa\u51c6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4e8b\u4ef6\u63d0\u53d6\u6a21\u5f0f\u5bf9\u79d1\u5b66\u6458\u8981\u8fdb\u884c\u6807\u6ce8\uff0c\u65e8\u5728\u5b9e\u73b0\u7ed3\u6784\u5316\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u79d1\u5b66\u5185\u5bb9\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u4fe1\u606f\u63d0\u53d6\u4e3b\u8981\u4f9d\u8d56\u7a84\u9886\u57df\u7684\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\uff0c\u9650\u5236\u4e86\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u79d1\u5b66\u4fe1\u606f\u7684\u5fc5\u8981\u4e0a\u4e0b\u6587\uff0c\u5e38\u5bfc\u81f4\u788e\u7247\u5316\u6216\u51b2\u7a81\u7684\u9648\u8ff0\u3002", "method": "\u5c06\u79d1\u5b66\u4fe1\u606f\u63d0\u53d6\u5b9a\u4e49\u4e3a\u591a\u9636\u6bb5\u4e8b\u4ef6\u63d0\u53d6\u6d41\u7a0b\uff1a(1) \u5c06\u6458\u8981\u5206\u5272\u4e3a\u6838\u5fc3\u79d1\u5b66\u6d3b\u52a8\uff08\u80cc\u666f\u3001\u65b9\u6cd5\u3001\u7ed3\u679c\u3001\u7ed3\u8bba\uff09\uff1b(2) \u63d0\u53d6\u76f8\u5e94\u7684\u89e6\u53d1\u8bcd\u548c\u7ec6\u7c92\u5ea6\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u5fae\u8c03\u7684\u4e8b\u4ef6\u63d0\u53d6\u6a21\u578b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u6807\u6ce8\u8005\u7684\u5b9e\u9a8c\u663e\u793a\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u5f53\u524d\u6a21\u578b\u5728\u793e\u4f1a\u5b66\u548c\u4eba\u6587\u79d1\u5b66\u7b49\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "SciEvent\u4f5c\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u662f\u671d\u7740\u53ef\u6cdb\u5316\u3001\u591a\u9886\u57df\u79d1\u5b66\u4fe1\u606f\u63d0\u53d6\u8fc8\u51fa\u7684\u4e00\u6b65\u3002"}}
{"id": "2509.15621", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15621", "abs": "https://arxiv.org/abs/2509.15621", "authors": ["Tomoya Yamashita", "Yuuki Yamanaka", "Masanori Yamada", "Takayuki Miura", "Toshiki Shibahara", "Tomoharu Iwata"], "title": "Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets", "comment": null, "summary": "Machine Unlearning (MU) has recently attracted considerable attention as a\nsolution to privacy and copyright issues in large language models (LLMs).\nExisting MU methods aim to remove specific target sentences from an LLM while\nminimizing damage to unrelated knowledge. However, these approaches require\nexplicit target sentences and do not support removing broader concepts, such as\npersons or events. To address this limitation, we introduce Concept Unlearning\n(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to\nrepresent the LLM's internal knowledge and define CU as removing the forgetting\ntarget nodes and associated edges. This graph-based formulation enables a more\nintuitive unlearning and facilitates the design of more effective methods. We\npropose a novel method that prompts the LLM to generate knowledge triplets and\nexplanatory sentences about the forgetting target and applies the unlearning\nprocess to these representations. Our approach enables more precise and\ncomprehensive concept removal by aligning the unlearning process with the LLM's\ninternal knowledge representations. Experiments on real-world and synthetic\ndatasets demonstrate that our method effectively achieves concept-level\nunlearning while preserving unrelated knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6982\u5ff5\u9057\u5fd8\uff08Concept Unlearning\uff09\u4f5c\u4e3aLLM\u9057\u5fd8\u7684\u65b0\u9700\u6c42\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8868\u793aLLM\u5185\u90e8\u77e5\u8bc6\uff0c\u5b9a\u4e49\u9057\u5fd8\u76ee\u6807\u8282\u70b9\u53ca\u5173\u8054\u8fb9\uff0c\u5b9e\u73b0\u66f4\u76f4\u89c2\u6709\u6548\u7684\u6982\u5ff5\u7ea7\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u9700\u8981\u660e\u786e\u7684\u76ee\u6807\u53e5\u5b50\uff0c\u4e0d\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\uff08\u5982\u4eba\u7269\u3001\u4e8b\u4ef6\uff09\u9057\u5fd8\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u8868\u793aLLM\u5185\u90e8\u77e5\u8bc6\uff0c\u901a\u8fc7\u751f\u6210\u77e5\u8bc6\u4e09\u5143\u7ec4\u548c\u89e3\u91ca\u6027\u53e5\u5b50\uff0c\u5c06\u9057\u5fd8\u8fc7\u7a0b\u4e0eLLM\u5185\u90e8\u77e5\u8bc6\u8868\u793a\u5bf9\u9f50\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u6982\u5ff5\u7ea7\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u65e0\u5173\u77e5\u8bc6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u6982\u5ff5\u79fb\u9664\u3002"}}
{"id": "2509.15558", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15558", "abs": "https://arxiv.org/abs/2509.15558", "authors": ["Mahesh Shakya", "Bijay Adhikari", "Nirsara Shrestha", "Bipin Koirala", "Arun Adhikari", "Prasanta Poudyal", "Luna Mathema", "Sarbagya Buddhacharya", "Bijay Khatri", "Bishesh Khanal"], "title": "From Development to Deployment of AI-assisted Telehealth and Screening for Vision- and Hearing-threatening diseases in resource-constrained settings: Field Observations, Challenges and Way Forward", "comment": "Accepted to MIRASOL (Medical Image Computing in Resource Constrained\n  Settings Workshop & KI) Workshop, 2025", "summary": "Vision- and hearing-threatening diseases cause preventable disability,\nespecially in resource-constrained settings(RCS) with few specialists and\nlimited screening setup. Large scale AI-assisted screening and telehealth has\npotential to expand early detection, but practical deployment is challenging in\npaper-based workflows and limited documented field experience exist to build\nupon. We provide insights on challenges and ways forward in development to\nadoption of scalable AI-assisted Telehealth and screening in such settings.\nSpecifically, we find that iterative, interdisciplinary collaboration through\nearly prototyping, shadow deployment and continuous feedback is important to\nbuild shared understanding as well as reduce usability hurdles when\ntransitioning from paper-based to AI-ready workflows. We find public datasets\nand AI models highly useful despite poor performance due to domain shift. In\naddition, we find the need for automated AI-based image quality check to\ncapture gradable images for robust screening in high-volume camps.\n  Our field learning stress the importance of treating AI development and\nworkflow digitization as an end-to-end, iterative co-design process. By\ndocumenting these practical challenges and lessons learned, we aim to address\nthe gap in contextual, actionable field knowledge for building real-world\nAI-assisted telehealth and mass-screening programs in RCS.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5f00\u53d1\u548c\u90e8\u7f72AI\u8f85\u52a9\u8fdc\u7a0b\u533b\u7597\u548c\u5927\u89c4\u6a21\u7b5b\u67e5\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u5f3a\u8c03\u9700\u8981\u5c06AI\u5f00\u53d1\u548c\u6d41\u7a0b\u6570\u5b57\u5316\u89c6\u4e3a\u7aef\u5230\u7aef\u7684\u8fed\u4ee3\u534f\u540c\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u548c\u542c\u89c9\u5a01\u80c1\u6027\u75be\u75c5\u5bfc\u81f4\u53ef\u9884\u9632\u7684\u6b8b\u75be\uff0c\u4f46\u7531\u4e8e\u4e13\u5bb6\u7a00\u7f3a\u548c\u7b5b\u67e5\u8bbe\u5907\u6709\u9650\uff0c\u5927\u89c4\u6a21AI\u8f85\u52a9\u7b5b\u67e5\u548c\u8fdc\u7a0b\u533b\u7597\u5177\u6709\u6269\u5c55\u65e9\u671f\u68c0\u6d4b\u7684\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u65e9\u671f\u539f\u578b\u8bbe\u8ba1\u3001\u5f71\u5b50\u90e8\u7f72\u548c\u6301\u7eed\u53cd\u9988\u7684\u8fed\u4ee3\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u5efa\u7acb\u5171\u4eab\u7406\u89e3\u5e76\u51cf\u5c11\u4ece\u7eb8\u8d28\u5de5\u4f5c\u6d41\u7a0b\u8fc7\u6e21\u5230AI\u5c31\u7eea\u5de5\u4f5c\u6d41\u7a0b\u65f6\u7684\u53ef\u7528\u6027\u969c\u788d\u3002", "result": "\u53d1\u73b0\u516c\u5171\u6570\u636e\u96c6\u548cAI\u6a21\u578b\u5c3d\u7ba1\u56e0\u9886\u57df\u504f\u79fb\u800c\u6027\u80fd\u8f83\u5dee\uff0c\u4f46\u975e\u5e38\u6709\u7528\uff1b\u9700\u8981\u81ea\u52a8\u5316\u7684\u57fa\u4e8eAI\u7684\u56fe\u50cf\u8d28\u91cf\u68c0\u67e5\u4ee5\u5728\u9ad8\u8d28\u91cf\u8425\u5730\u4e2d\u6355\u83b7\u53ef\u5206\u7ea7\u56fe\u50cf\u3002", "conclusion": "\u901a\u8fc7\u8bb0\u5f55\u8fd9\u4e9b\u5b9e\u9645\u6311\u6218\u548c\u7ecf\u9a8c\u6559\u8bad\uff0c\u65e8\u5728\u586b\u8865\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6784\u5efa\u771f\u5b9e\u4e16\u754cAI\u8f85\u52a9\u8fdc\u7a0b\u533b\u7597\u548c\u5927\u89c4\u6a21\u7b5b\u67e5\u9879\u76ee\u7684\u4e0a\u4e0b\u6587\u3001\u53ef\u64cd\u4f5c\u73b0\u573a\u77e5\u8bc6\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.15602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15602", "abs": "https://arxiv.org/abs/2509.15602", "authors": ["Zhongyuan Bao", "Lejun Zhang"], "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?", "comment": null, "summary": "Multimodal large language models (MLLMs) excel at general video understanding\nbut struggle with fast, high-frequency sports like tennis, where rally clips\nare short yet information-dense. To systematically evaluate MLLMs in this\nchallenging domain, we present TennisTV, the first and most comprehensive\nbenchmark for tennis video understanding. TennisTV models each rally as a\ntemporal-ordered sequence of consecutive stroke events, using automated\npipelines for filtering and question generation. It covers 8 tasks at rally and\nstroke levels and includes 2,500 human-verified questions. Evaluating 16\nrepresentative MLLMs, we provide the first systematic assessment of tennis\nvideo understanding. Results reveal substantial shortcomings and yield two key\ninsights: (i) frame-sampling density should be tailored and balanced across\ntasks, and (ii) improving temporal grounding is essential for stronger\nreasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TennisTV\u57fa\u51c6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u7f51\u7403\u89c6\u9891\u7406\u89e3\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f3016\u4e2a\u4ee3\u8868\u6027\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5728\u5feb\u901f\u9ad8\u9891\u8fd0\u52a8\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6311\u6218\u548c\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u822c\u89c6\u9891\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u50cf\u7f51\u7403\u8fd9\u6837\u7684\u5feb\u901f\u3001\u9ad8\u9891\u8fd0\u52a8\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u7f51\u7403\u56de\u5408\u7247\u6bb5\u77ed\u4f46\u4fe1\u606f\u5bc6\u96c6\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30MLLMs\u5728\u8fd9\u4e00\u6311\u6218\u6027\u9886\u57df\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efaTennisTV\u57fa\u51c6\uff0c\u5c06\u6bcf\u4e2a\u7f51\u7403\u56de\u5408\u5efa\u6a21\u4e3a\u65f6\u95f4\u6709\u5e8f\u7684\u8fde\u7eed\u51fb\u7403\u4e8b\u4ef6\u5e8f\u5217\uff0c\u4f7f\u7528\u81ea\u52a8\u5316\u7ba1\u9053\u8fdb\u884c\u8fc7\u6ee4\u548c\u95ee\u9898\u751f\u6210\u3002\u57fa\u51c6\u6db5\u76d68\u4e2a\u4efb\u52a1\uff0c\u5305\u542b2,500\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u9898\u3002", "result": "\u8bc4\u4f3016\u4e2a\u4ee3\u8868\u6027MLLMs\u53d1\u73b0\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u83b7\u5f97\u4e24\u4e2a\u5173\u952e\u89c1\u89e3\uff1a(i)\u5e27\u91c7\u6837\u5bc6\u5ea6\u5e94\u6839\u636e\u4efb\u52a1\u8fdb\u884c\u5b9a\u5236\u548c\u5e73\u8861\uff1b(ii)\u6539\u8fdb\u65f6\u95f4\u5b9a\u4f4d\u5bf9\u4e8e\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "TennisTV\u57fa\u51c6\u4e3a\u7f51\u7403\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86MLLMs\u5728\u5feb\u901f\u8fd0\u52a8\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5e27\u91c7\u6837\u7b56\u7565\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u6539\u8fdb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.15736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15736", "abs": "https://arxiv.org/abs/2509.15736", "authors": ["Gabriel Jarry", "Ramon Dalmau", "Philippe Very", "Junzi Sun"], "title": "Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks", "comment": null, "summary": "Accurate modelling of aircraft fuel-flow is crucial for both operational\nplanning and environmental impact assessment, yet standard parametric models\noften neglect performance deterioration that occurs as aircraft age. This paper\ninvestigates multiple approaches to integrate engine ageing effects into\nfuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of\napproximately nineteen thousand Quick Access Recorder flights from nine\ndistinct airframes with varying years in service. We systematically evaluate\nclassical physics-based models, empirical correction coefficients, and\ndata-driven neural network architectures that incorporate age either as an\ninput feature or as an explicit multiplicative bias. Results demonstrate that\nwhile baseline models consistently underestimate fuel consumption for older\naircraft, the use of age-dependent correction factors and neural models\nsubstantially reduces bias and improves prediction accuracy. Nevertheless,\nlimitations arise from the small number of airframes and the lack of detailed\nmaintenance event records, which constrain the representativeness and\ngeneralization of age-based corrections. This study emphasizes the importance\nof accounting for the effects of ageing in parametric and machine learning\nframeworks to improve the reliability of operational and environmental\nassessments. The study also highlights the need for more diverse datasets that\ncan capture the complexity of real-world engine deterioration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u53d1\u52a8\u673a\u8001\u5316\u6548\u5e94\u6574\u5408\u5230\u7a7a\u5ba2A320-214\u71c3\u6cb9\u6d41\u91cf\u9884\u6d4b\u4e2d\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u8003\u8651\u8001\u5316\u56e0\u7d20\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u89c4\u6a21\u548c\u7f3a\u4e4f\u8be6\u7ec6\u7ef4\u62a4\u8bb0\u5f55\u3002", "motivation": "\u6807\u51c6\u53c2\u6570\u6a21\u578b\u901a\u5e38\u5ffd\u7565\u98de\u673a\u8001\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\uff0c\u800c\u51c6\u786e\u7684\u71c3\u6cb9\u6d41\u91cf\u5efa\u6a21\u5bf9\u8fd0\u8425\u89c4\u5212\u548c\u73af\u5883\u5f71\u54cd\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u7ecf\u5178\u6a21\u578b\u3001\u7ecf\u9a8c\u4fee\u6b63\u7cfb\u6570\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5c06\u5e74\u9f84\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\u6216\u663e\u5f0f\u4e58\u6cd5\u504f\u5dee\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u6301\u7eed\u4f4e\u4f30\u8001\u65e7\u98de\u673a\u7684\u71c3\u6cb9\u6d88\u8017\uff0c\u800c\u4f7f\u7528\u5e74\u9f84\u76f8\u5173\u4fee\u6b63\u56e0\u5b50\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u663e\u8457\u51cf\u5c11\u4e86\u504f\u5dee\u5e76\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u53c2\u6570\u548c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e2d\u8003\u8651\u8001\u5316\u6548\u5e94\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u6307\u51fa\u9700\u8981\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u6355\u6349\u771f\u5b9e\u4e16\u754c\u53d1\u52a8\u673a\u9000\u5316\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2509.15642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15642", "abs": "https://arxiv.org/abs/2509.15642", "authors": ["Fangyuan Mao", "Shuo Wang", "Jilin Mei", "Chen Min", "Shun Lu", "Fuyang Liu", "Yu Hu"], "title": "UNIV: Unified Foundation Model for Infrared and Visible Modalities", "comment": null, "summary": "The demand for joint RGB-visible and infrared perception is growing rapidly,\nparticularly to achieve robust performance under diverse weather conditions.\nAlthough pre-trained models for RGB-visible and infrared data excel in their\nrespective domains, they often underperform in multimodal scenarios, such as\nautonomous vehicles equipped with both sensors. To address this challenge, we\npropose a biologically inspired UNified foundation model for Infrared and\nVisible modalities (UNIV), featuring two key innovations. First, we introduce\nPatch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided\ndistillation framework that mimics retinal horizontal cells' lateral\ninhibition, which enables effective cross-modal feature alignment while\nremaining compatible with any transformer-based architecture. Second, our\ndual-knowledge preservation mechanism emulates the retina's bipolar cell signal\nrouting - combining LoRA adapters (2% added parameters) with synchronous\ndistillation to prevent catastrophic forgetting, thereby replicating the\nretina's photopic (cone-driven) and scotopic (rod-driven) functionality. To\nsupport cross-modal learning, we introduce the MVIP dataset, the most\ncomprehensive visible-infrared benchmark to date. It contains 98,992 precisely\naligned image pairs spanning diverse scenarios. Extensive experiments\ndemonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in\nsemantic segmentation and +0.7 mAP in object detection) while maintaining 99%+\nof the baseline performance on visible RGB tasks. Our code is available at\nhttps://github.com/fangyuanmao/UNIV.", "AI": {"tldr": "UNIV\u662f\u4e00\u4e2a\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u6a21\u6001\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u53cc\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\uff0c\u5728\u4fdd\u6301RGB\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7ea2\u5916\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3RGB-\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u591a\u6a21\u6001\u611f\u77e5\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u9700\u8981\u591a\u79cd\u4f20\u611f\u5668\u534f\u540c\u5de5\u4f5c\u7684\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faPatch-wise Cross-modality Contrastive Learning (PCCL)\u6a21\u62df\u89c6\u7f51\u819c\u6c34\u5e73\u7ec6\u80de\u7684\u4fa7\u5411\u6291\u5236\u673a\u5236\uff0c\u4ee5\u53ca\u53cc\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\u6a21\u62df\u53cc\u6781\u7ec6\u80de\u4fe1\u53f7\u8def\u7531\uff0c\u7ed3\u5408LoRA\u9002\u914d\u5668\u548c\u540c\u6b65\u84b8\u998f\u3002", "result": "\u5728\u7ea2\u5916\u4efb\u52a1\u4e0a\u8bed\u4e49\u5206\u5272\u63d0\u53471.7 mIoU\uff0c\u76ee\u6807\u68c0\u6d4b\u63d0\u53470.7 mAP\uff0c\u540c\u65f6\u4fdd\u6301RGB\u4efb\u52a199%\u4ee5\u4e0a\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "UNIV\u6a21\u578b\u901a\u8fc7\u751f\u7269\u5b66\u542f\u53d1\u7684\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u611f\u77e5\u7684\u6311\u6218\uff0c\u4e3a\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u6570\u636e\u7684\u7edf\u4e00\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15740", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15740", "abs": "https://arxiv.org/abs/2509.15740", "authors": ["Jonathan Adam Rico", "Nagarajan Raghavan", "Senthilnath Jayavelu"], "title": "Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets", "comment": "The published version of this preprint can be accessed at\n  https://ieeexplore.ieee.org/abstract/document/10874675", "summary": "Data-driven models accurately perform early battery prognosis to prevent\nequipment failure and further safety hazards. Most existing machine learning\n(ML) models work in offline mode which must consider their retraining\npost-deployment every time new data distribution is encountered. Hence, there\nis a need for an online ML approach where the model can adapt to varying\ndistributions. However, existing online incremental multistep forecasts are a\ngreat challenge as there is no way to correct the model of its forecasts at the\ncurrent instance. Also, these methods need to wait for a considerable amount of\ntime to acquire enough streaming data before retraining. In this study, we\npropose iFSNet (incremental Fast and Slow learning Network) which is a modified\nversion of FSNet for a single-pass mode (sample-by-sample) to achieve multistep\nforecasting using pseudo targets. It uses a simple linear regressor of the\ninput sequence to extrapolate pseudo future samples (pseudo targets) and\ncalculate the loss from the rest of the forecast and keep updating the model.\nThe model benefits from the associative memory and adaptive structure\nmechanisms of FSNet, at the same time the model incrementally improves by using\npseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on\ndatasets with smooth degradation trajectories while it achieved 0.01588 RMSE\nand 0.01234 MAE on datasets having irregular degradation trajectories with\ncapacity regeneration spikes.", "AI": {"tldr": "\u63d0\u51faiFSNet\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u7ebf\u589e\u91cf\u591a\u6b65\u7535\u6c60\u5bff\u547d\u9884\u6d4b\uff0c\u901a\u8fc7\u4f2a\u76ee\u6807\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u65f6\u6821\u6b63\u548c\u9700\u8981\u5927\u91cf\u6570\u636e\u91cd\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u591a\u4e3a\u79bb\u7ebf\u6a21\u5f0f\uff0c\u9700\u8981\u5728\u65b0\u6570\u636e\u5206\u5e03\u51fa\u73b0\u65f6\u91cd\u65b0\u8bad\u7ec3\uff0c\u65e0\u6cd5\u9002\u5e94\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u9700\u6c42\uff0c\u4e14\u591a\u6b65\u9884\u6d4b\u96be\u4ee5\u5b9e\u65f6\u6821\u6b63\u3002", "method": "\u6539\u8fdbFSNet\u4e3a\u5355\u901a\u6a21\u5f0f\uff0c\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u5668\u4ece\u8f93\u5165\u5e8f\u5217\u5916\u63a8\u4f2a\u672a\u6765\u6837\u672c\u4f5c\u4e3a\u4f2a\u76ee\u6807\uff0c\u7ed3\u5408FSNet\u7684\u8054\u60f3\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u7ed3\u6784\u673a\u5236\u8fdb\u884c\u589e\u91cf\u5b66\u4e60\u3002", "result": "\u5728\u5e73\u6ed1\u9000\u5316\u8f68\u8ff9\u6570\u636e\u96c6\u4e0aRMSE\u4e3a0.00197\u3001MAE\u4e3a0.00154\uff1b\u5728\u5177\u6709\u5bb9\u91cf\u518d\u751f\u5c16\u5cf0\u7684\u4e0d\u89c4\u5219\u9000\u5316\u8f68\u8ff9\u6570\u636e\u96c6\u4e0aRMSE\u4e3a0.01588\u3001MAE\u4e3a0.01234\u3002", "conclusion": "iFSNet\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5728\u7ebf\u589e\u91cf\u591a\u6b65\u9884\u6d4b\uff0c\u5728\u4e0d\u540c\u9000\u5316\u6a21\u5f0f\u4e0b\u5747\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.15688", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15688", "abs": "https://arxiv.org/abs/2509.15688", "authors": ["Johann Schmidt", "Sebastian Stober", "Joachim Denzler", "Paul Bodesheim"], "title": "Saccadic Vision for Fine-Grained Visual Classification", "comment": null, "summary": "Fine-grained visual classification (FGVC) requires distinguishing between\nvisually similar categories through subtle, localized features - a task that\nremains challenging due to high intra-class variability and limited inter-class\ndifferences. Existing part-based methods often rely on complex localization\nnetworks that learn mappings from pixel to sample space, requiring a deep\nunderstanding of image content while limiting feature utility for downstream\ntasks. In addition, sampled points frequently suffer from high spatial\nredundancy, making it difficult to quantify the optimal number of required\nparts. Inspired by human saccadic vision, we propose a two-stage process that\nfirst extracts peripheral features (coarse view) and generates a sample map,\nfrom which fixation patches are sampled and encoded in parallel using a\nweight-shared encoder. We employ contextualized selective attention to weigh\nthe impact of each fixation patch before fusing peripheral and focus\nrepresentations. To prevent spatial collapse - a common issue in part-based\nmethods - we utilize non-maximum suppression during fixation sampling to\neliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks\n(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect\ndatasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method\nachieves comparable performance to state-of-the-art approaches while\nconsistently outperforming our baseline encoder.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u626b\u89c6\u89c6\u89c9\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u56f4\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u89c6\u70b9\u91c7\u6837\u6765\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u90e8\u4ef6\u65b9\u6cd5\u7684\u7a7a\u95f4\u5197\u4f59\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u90e8\u4ef6\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u5197\u4f59\u95ee\u9898\uff0c\u590d\u6742\u5b9a\u4f4d\u7f51\u7edc\u9650\u5236\u4e86\u7279\u5f81\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u96be\u4ee5\u91cf\u5316\u6700\u4f18\u90e8\u4ef6\u6570\u91cf\u3002\u53d7\u4eba\u7c7b\u626b\u89c6\u89c6\u89c9\u542f\u53d1\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7279\u5f81\u91c7\u6837\u673a\u5236\u3002", "method": "\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a1\uff09\u63d0\u53d6\u5916\u56f4\u7279\u5f81\u751f\u6210\u91c7\u6837\u56fe\uff1b2\uff09\u4f7f\u7528\u975e\u6781\u5927\u503c\u6291\u5236\u91c7\u6837\u6ce8\u89c6\u70b9\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u7f16\u7801\u5668\u5e76\u884c\u7f16\u7801\uff0c\u5e94\u7528\u4e0a\u4e0b\u6587\u9009\u62e9\u6027\u6ce8\u610f\u529b\u52a0\u6743\u878d\u5408\u5916\u56f4\u548c\u6ce8\u89c6\u8868\u793a\u3002", "result": "\u5728CUB-200-2011\u3001NABirds\u3001Food-101\u3001Stanford-Dogs\u7b49\u6807\u51c6\u6570\u636e\u96c6\u548cEU-Moths\u3001Ecuador-Moths\u3001AMI-Moths\u7b49\u6606\u866b\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u8fbe\u5230\u4e0eSOTA\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u7f16\u7801\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u53d7\u4eba\u7c7b\u89c6\u89c9\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u7684\u7a7a\u95f4\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4e0d\u589e\u52a0\u590d\u6742\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.15748", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.15748", "abs": "https://arxiv.org/abs/2509.15748", "authors": ["Tony Lindeberg"], "title": "Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields", "comment": "25 pages, 9 figures", "summary": "Because of the variabilities of real-world image structures under the natural\nimage transformations that arise when observing similar objects or\nspatio-temporal events under different viewing conditions, the receptive field\nresponses computed in the earliest layers of the visual hierarchy may be\nstrongly influenced by such geometric image transformations. One way of\nhandling this variability is by basing the vision system on covariant receptive\nfield families, which expand the receptive field shapes over the degrees of\nfreedom in the image transformations.\n  This paper addresses the problem of deriving relationships between spatial\nand spatio-temporal receptive field responses obtained for different values of\nthe shape parameters in the resulting multi-parameter families of receptive\nfields. For this purpose, we derive both (i) infinitesimal relationships,\nroughly corresponding to a combination of notions from semi-groups and Lie\ngroups, as well as (ii) macroscopic cascade smoothing properties, which\ndescribe how receptive field responses at coarser spatial and temporal scales\ncan be computed by applying smaller support incremental filters to the output\nfrom corresponding receptive fields at finer spatial and temporal scales,\nstructurally related to the notion of Lie algebras, although with directional\npreferences.\n  The presented results provide (i) a deeper understanding of the relationships\nbetween spatial and spatio-temporal receptive field responses for different\nvalues of the filter parameters, which can be used for both (ii) designing more\nefficient schemes for computing receptive field responses over populations of\nmulti-parameter families of receptive fields, as well as (iii)~formulating\nidealized theoretical models of the computations of simple cells in biological\nvision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u7cfb\u7edf\u4e2d\u611f\u53d7\u91ce\u54cd\u5e94\u4e0e\u56fe\u50cf\u53d8\u6362\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u534f\u53d8\u611f\u53d7\u91ce\u65cf\u7684\u65b9\u6cd5\u6765\u5904\u7406\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7ed3\u6784\u7684\u53d8\u5f02\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u7a7a\u95f4\u548c\u65f6\u7a7a\u611f\u53d7\u91ce\u54cd\u5e94\u5728\u4e0d\u540c\u5f62\u72b6\u53c2\u6570\u4e0b\u7684\u6570\u5b66\u5173\u7cfb\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7ed3\u6784\u5728\u81ea\u7136\u56fe\u50cf\u53d8\u6362\u4e0b\u5b58\u5728\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u89c6\u89c9\u5c42\u6b21\u65e9\u671f\u5c42\u7684\u611f\u53d7\u91ce\u54cd\u5e94\u53d7\u5230\u51e0\u4f55\u56fe\u50cf\u53d8\u6362\u7684\u5f3a\u70c8\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u53d8\u5f02\u6027\uff0c\u9700\u8981\u57fa\u4e8e\u534f\u53d8\u611f\u53d7\u91ce\u65cf\u6765\u6269\u5c55\u611f\u53d7\u91ce\u5f62\u72b6\u4ee5\u9002\u5e94\u56fe\u50cf\u53d8\u6362\u7684\u81ea\u7531\u5ea6\u3002", "method": "\u63a8\u5bfc\u4e86\u7a7a\u95f4\u548c\u65f6\u7a7a\u611f\u53d7\u91ce\u54cd\u5e94\u5728\u4e0d\u540c\u5f62\u72b6\u53c2\u6570\u4e0b\u7684\u5173\u7cfb\uff0c\u5305\u62ec\uff1a(i) \u65e0\u7a77\u5c0f\u5173\u7cfb\uff08\u7ed3\u5408\u534a\u7fa4\u548c\u674e\u7fa4\u6982\u5ff5\uff09\u548c(ii) \u5b8f\u89c2\u7ea7\u8054\u5e73\u6ed1\u7279\u6027\uff08\u63cf\u8ff0\u5982\u4f55\u901a\u8fc7\u5e94\u7528\u8f83\u5c0f\u652f\u6301\u589e\u91cf\u6ee4\u6ce2\u5668\u6765\u8ba1\u7b97\u7c97\u5c3a\u5ea6\u611f\u53d7\u91ce\u54cd\u5e94\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u611f\u53d7\u91ce\u54cd\u5e94\u5728\u4e0d\u540c\u6ee4\u6ce2\u5668\u53c2\u6570\u4e0b\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5173\u7cfb\u53ef\u7528\u4e8e\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u591a\u53c2\u6570\u611f\u53d7\u91ce\u65cf\u8ba1\u7b97\u65b9\u6848\uff0c\u5e76\u4e3a\u751f\u7269\u89c6\u89c9\u4e2d\u7b80\u5355\u7ec6\u80de\u8ba1\u7b97\u63d0\u4f9b\u7406\u60f3\u5316\u7406\u8bba\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6df1\u5316\u4e86\u5bf9\u7a7a\u95f4\u548c\u65f6\u7a7a\u611f\u53d7\u91ce\u54cd\u5e94\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u8ba1\u7b97\u65b9\u6848\u548c\u6784\u5efa\u751f\u7269\u89c6\u89c9\u7406\u8bba\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5904\u7406\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7ed3\u6784\u7684\u53d8\u5f02\u6027\u95ee\u9898\u3002"}}
{"id": "2509.16014", "categories": ["cs.LG", "cs.CY", "I.2.7; I.2.4; G.3; J.4; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.16014", "abs": "https://arxiv.org/abs/2509.16014", "authors": ["R. O. Lane", "W. J. Holmes", "C. J. Taylor", "H. M. State-Davey", "A. J. Wragge"], "title": "Predicting the descent into extremism and terrorism", "comment": "10 pages, 12 figures, presented at 6th IMA Conference on Mathematics\n  in Defence and Security, Online, 30 September 2023 (conference page at\n  https://ima.org.uk/12970/6th-ima-conference-on-mathematics-in-defence-and-security/).\n  arXiv admin note: text overlap with arXiv:2502.00013", "summary": "This paper proposes an approach for automatically analysing and tracking\nstatements in material gathered online and detecting whether the authors of the\nstatements are likely to be involved in extremism or terrorism. The proposed\nsystem comprises: online collation of statements that are then encoded in a\nform amenable to machine learning (ML), an ML component to classify the encoded\ntext, a tracker, and a visualisation system for analysis of results. The\ndetection and tracking concept has been tested using quotes made by terrorists,\nextremists, campaigners, and politicians, obtained from wikiquote.org. A set of\nfeatures was extracted for each quote using the state-of-the-art Universal\nSentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The\ndata were used to train and test a support vector machine (SVM) classifier\nusing 10-fold cross-validation. The system was able to correctly detect\nintentions and attitudes associated with extremism 81% of the time and\nterrorism 97% of the time, using a dataset of 839 quotes. This accuracy was\nhigher than that which was achieved for a simple baseline system based on\nn-gram text features. Tracking techniques were also used to perform a temporal\nanalysis of the data, with each quote considered to be a noisy measurement of a\nperson's state of mind. It was demonstrated that the tracking algorithms were\nable to detect both trends over time and sharp changes in attitude that could\nbe attributed to major events.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5206\u6790\u548c\u8ffd\u8e2a\u5728\u7ebf\u6750\u6599\u4e2d\u6781\u7aef\u4e3b\u4e49\u6216\u6050\u6016\u4e3b\u4e49\u76f8\u5173\u8a00\u8bba\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u548c\u65f6\u95f4\u8ffd\u8e2a\u6280\u672f", "motivation": "\u9700\u8981\u81ea\u52a8\u68c0\u6d4b\u548c\u8ffd\u8e2a\u5728\u7ebf\u6781\u7aef\u4e3b\u4e49\u548c\u6050\u6016\u4e3b\u4e49\u8a00\u8bba\uff0c\u4ee5\u5e2e\u52a9\u5b89\u5168\u5206\u6790", "method": "\u4f7f\u7528\u901a\u7528\u53e5\u5b50\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0cSVM\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u8ffd\u8e2a\u7b97\u6cd5\u5206\u6790\u6001\u5ea6\u53d8\u5316", "result": "\u6781\u7aef\u4e3b\u4e49\u68c0\u6d4b\u51c6\u786e\u738781%\uff0c\u6050\u6016\u4e3b\u4e49\u68c0\u6d4b\u51c6\u786e\u738797%\uff0c\u4f18\u4e8e\u57fa\u4e8en-gram\u7684\u57fa\u7ebf\u7cfb\u7edf", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u6781\u7aef\u4e3b\u4e49\u610f\u56fe\u548c\u6001\u5ea6\uff0c\u5e76\u80fd\u8ffd\u8e2a\u65f6\u95f4\u8d8b\u52bf\u548c\u91cd\u5927\u4e8b\u4ef6\u5f15\u8d77\u7684\u6001\u5ea6\u7a81\u53d8"}}
