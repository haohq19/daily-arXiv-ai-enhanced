<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Preventing Shortcuts in Adapter Training via Providing the Shortcuts](https://arxiv.org/abs/2510.20887)
*Anujraaj Argo Goyal,Guocheng Gordon Qian,Huseyin Coskun,Aarush Gupta,Himmy Tam,Daniil Ostashev,Ju Hu,Dhritiman Sagar,Sergey Tulyakov,Kfir Aberman,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出Shortcut-Rerouted Adapter Training方法，通过在适配器训练中为混淆因素建立辅助模块，避免目标属性与无关因素的纠缠，从而提高生成质量和文本提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于适配器的训练方法容易将目标属性与姿势、表情、光照等无关因素纠缠，限制了模型的泛化能力和文本提示遵循能力。

Method: 在适配器训练期间，为混淆因素建立辅助模块（如ControlNet或LoRA），让这些因素通过捷径传递，消除适配器学习它们的动机，推理时移除辅助模块。

Result: 在人脸和全身身份注入任务中，该方法提高了生成质量、多样性和提示遵循能力。

Conclusion: 在大型模型时代，寻求解耦表示的最有效路径可能是为不应学习的内容建立捷径。

Abstract: Adapter-based training has emerged as a key mechanism for extending the
capabilities of powerful foundation image generators, enabling personalized and
stylized text-to-image synthesis. These adapters are typically trained to
capture a specific target attribute, such as subject identity, using
single-image reconstruction objectives. However, because the input image
inevitably contains a mixture of visual factors, adapters are prone to entangle
the target attribute with incidental ones, such as pose, expression, and
lighting. This spurious correlation problem limits generalization and obstructs
the model's ability to adhere to the input text prompt. In this work, we
uncover a simple yet effective solution: provide the very shortcuts we wish to
eliminate during adapter training. In Shortcut-Rerouted Adapter Training,
confounding factors are routed through auxiliary modules, such as ControlNet or
LoRA, eliminating the incentive for the adapter to internalize them. The
auxiliary modules are then removed during inference. When applied to tasks like
facial and full-body identity injection, our approach improves generation
quality, diversity, and prompt adherence. These results point to a general
design principle in the era of large models: when seeking disentangled
representations, the most effective path may be to establish shortcuts for what
should NOT be learned.

</details>


### [2] [Video-As-Prompt: Unified Semantic Control for Video Generation](https://arxiv.org/abs/2510.20888)
*Yuxuan Bian,Xin Chen,Zenan Li,Tiancheng Zhi,Shen Sang,Linjie Luo,Qiang Xu*

Main category: cs.CV

TL;DR: Video-As-Prompt (VAP) 是一种新的视频生成范式，通过参考视频作为语义提示，使用冻结的Video Diffusion Transformer和插件式Mixture-of-Transformers专家实现统一的语义控制，无需特定条件微调。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成中统一、可泛化的语义控制难题，现有方法要么引入伪影，要么依赖非泛化的特定条件微调或任务特定架构。

Method: 将问题重构为上下文生成，使用参考视频作为直接语义提示，通过冻结的Video Diffusion Transformer和插件式Mixture-of-Transformers专家架构，结合时间偏置位置嵌入来消除虚假映射先验。

Result: 作为单一统一模型，VAP在开源方法中达到最先进水平，获得38.7%的用户偏好率，与领先的特定条件商业模型相当，并构建了包含10万对视频的VAP-Data数据集。

Conclusion: VAP的强零样本泛化能力和对各种下游应用的支持，标志着向通用可控视频生成迈出了重要一步。

Abstract: Unified, generalizable semantic control in video generation remains a
critical open challenge. Existing methods either introduce artifacts by
enforcing inappropriate pixel-wise priors from structure-based controls, or
rely on non-generalizable, condition-specific finetuning or task-specific
architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes
this problem as in-context generation. VAP leverages a reference video as a
direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via
a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture
prevents catastrophic forgetting and is guided by a temporally biased position
embedding that eliminates spurious mapping priors for robust context retrieval.
To power this approach and catalyze future research, we built VAP-Data, the
largest dataset for semantic-controlled video generation with over 100K paired
videos across 100 semantic conditions. As a single unified model, VAP sets a
new state-of-the-art for open-source methods, achieving a 38.7% user preference
rate that rivals leading condition-specific commercial models. VAP's strong
zero-shot generalization and support for various downstream applications mark a
significant advance toward general-purpose, controllable video generation.

</details>


### [3] [Deep learning-based automated damage detection in concrete structures using images from earthquake events](https://arxiv.org/abs/2510.21063)
*Abdullah Turer,Yongsheng Bai,Halil Sezen,Alper Yilmaz*

Main category: cs.CV

TL;DR: 使用深度学习自动检测地震后混凝土结构中暴露的钢筋，通过YOLO模型识别开裂、剥落和钢筋暴露等损伤，构建混合框架评估结构损伤等级。


<details>
  <summary>Details</summary>
Motivation: 地震后及时评估结构完整性对公共安全和应急响应至关重要，需要自动化方法检测混凝土结构中暴露的钢筋来评估损伤程度。

Method: 基于深度学习框架，使用YOLOv11模型检测开裂、剥落和暴露钢筋，通过微调、数据增强和公共数据集测试，构建自动分类框架识别建筑内外和结构构件。

Result: 开发了能够自动可靠地从输入图像确定损伤等级的混合框架，在2023年土耳其地震数据集上验证了方法的有效性。

Conclusion: 研究表明，通过图像数据收集、标注和深度学习方法，可以在不同损伤背景下实现快速自动化的灾后损伤检测。

Abstract: Timely assessment of integrity of structures after seismic events is crucial
for public safety and emergency response. This study focuses on assessing the
structural damage conditions using deep learning methods to detect exposed
steel reinforcement in concrete buildings and bridges after large earthquakes.
Steel bars are typically exposed after concrete spalling or large flexural or
shear cracks. The amount and distribution of exposed steel reinforcement is an
indication of structural damage and degradation. To automatically detect
exposed steel bars, new datasets of images collected after the 2023 Turkey
Earthquakes were labeled to represent a wide variety of damaged concrete
structures. The proposed method builds upon a deep learning framework, enhanced
with fine-tuning, data augmentation, and testing on public datasets. An
automated classification framework is developed that can be used to identify
inside/outside buildings and structural components. Then, a YOLOv11 (You Only
Look Once) model is trained to detect cracking and spalling damage and exposed
bars. Another YOLO model is finetuned to distinguish different categories of
structural damage levels. All these trained models are used to create a hybrid
framework to automatically and reliably determine the damage levels from input
images. This research demonstrates that rapid and automated damage detection
following disasters is achievable across diverse damage contexts by utilizing
image data collection, annotation, and deep learning approaches.

</details>


### [4] [Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding](https://arxiv.org/abs/2510.21356)
*Anupam Pani,Yanchao Yang*

Main category: cs.CV

TL;DR: 提出一种基于注视正则化的框架，通过将模型注意力与人类视觉注视对齐来增强视觉语言模型在自我中心理解任务中的表现，仅在训练时使用注视数据。


<details>
  <summary>Details</summary>
Motivation: 人类注视提供了关于注意力、短期意图和未来行动的有价值线索，是建模自我中心行为的强大信号。

Method: 引入注视正则化注意力机制，在训练期间将模型焦点与人类视觉注视对齐，该设计灵活模块化，可泛化到多种使用注意力的VLM架构。

Result: 实验结果显示，相比无注视正则化的基线模型，该方法在未来事件预测任务中语义预测分数提升高达11分，在当前活动理解任务中提升约7分。

Conclusion: 这项工作为使用人类注视增强VLM在现实场景（如辅助机器人和人机协作）中的预测能力奠定了基础。

Abstract: Eye gaze offers valuable cues about attention, short-term intent, and future
actions, making it a powerful signal for modeling egocentric behavior. In this
work, we propose a gaze-regularized framework that enhances VLMs for two key
egocentric understanding tasks: fine-grained future event prediction and
current activity understanding. Unlike prior approaches that rely solely on
visual inputs or use gaze as an auxiliary input signal , our method uses gaze
only during training. We introduce a gaze-regularized attention mechanism that
aligns model focus with human visual gaze. This design is flexible and modular,
allowing it to generalize across multiple VLM architectures that utilize
attention. Experimental results show that our approach improves semantic
prediction scores by up to 11 for future event prediction and around 7 for
current activity understanding, compared to the corresponding baseline models
trained without gaze regularization. These results highlight the value of
gaze-guided training in improving the accuracy and robustness of egocentric
VLMs. Overall, this work establishes a foundation for using human gaze to
enhance the predictive capabilities of VLMs in real-world scenarios like
assistive robots and human-machine collaboration. Code and additional
information is available at: https://github.com/anupampani/Gaze-VLM

</details>


### [5] [MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence](https://arxiv.org/abs/2510.21406)
*Yue Feng,Jinwei Hu,Qijia Lu,Jiawei Niu,Li Tan,Shuo Yuan,Ziyi Yan,Yizhen Jia,Qingzhi He,Shiping Ge,Ethan Q. Chen,Wentong Li,Limin Wang,Jie Qin*

Main category: cs.CV

TL;DR: 提出了多模态未修剪视频检索任务和MUVR基准，用于长视频平台的视频检索，支持多模态查询并评估检索模型和MLLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索方法主要针对短视频和单模态查询，无法满足长视频平台对未修剪视频和多模态查询的检索需求。

Method: 构建MUVR基准，包含53K未修剪视频、1,050个多模态查询和84K匹配项，支持视频中心的多模态查询（长文本、视频标签、掩码提示）和多级视觉对应关系。

Result: 评估了3个SOTA视频检索模型、6个基于图像的VLM和10个MLLM，揭示了现有方法在处理未修剪视频和多模态查询方面的局限性。

Conclusion: MUVR基准为长视频平台的多模态视频检索提供了新的评估标准，展示了当前方法的不足和未来改进方向。

Abstract: We propose the Multi-modal Untrimmed Video Retrieval task, along with a new
benchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims
to retrieve untrimmed videos containing relevant segments using multi-modal
queries. It has the following features: 1) Practical retrieval paradigm: MUVR
supports video-centric multi-modal queries, expressing fine-grained retrieval
needs through long text descriptions, video tag prompts, and mask prompts. It
adopts a one-to-many retrieval paradigm and focuses on untrimmed videos,
tailored for long-video platform applications. 2) Multi-level visual
correspondence: To cover common video categories (e.g., news, travel, dance)
and precisely define retrieval matching criteria, we construct multi-level
visual correspondence based on core video content (e.g., news events, travel
locations, dance moves) which users are interested in and want to retrieve. It
covers six levels: copy, event, scene, instance, action, and others. 3)
Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,
Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA
assesses MLLMs in a question-answering format. We also propose a Reranking
Score to evaluate the reranking ability of MLLMs. MUVR consists of 53K
untrimmed videos from the video platform Bilibili, with 1,050 multi-modal
queries and 84K matches. Extensive evaluations of 3 state-of-the-art video
retrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals
the limitations of retrieval methods in processing untrimmed videos and
multi-modal queries, as well as MLLMs in multi-video understanding and
reranking. Our code and benchmark is available at
https://github.com/debby-0527/MUVR.

</details>


### [6] [Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent](https://arxiv.org/abs/2510.21704)
*Christy Li,Josep Lopez Camuñas,Jake Thomas Touchet,Jacob Andreas,Agata Lapedriza,Antonio Torralba,Tamar Rott Shaham*

Main category: cs.CV

TL;DR: 提出了一种自动化框架，通过自反思代理系统性地生成和测试视觉属性依赖假设，以检测训练好的视觉模型对特定视觉特征的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 检测视觉模型对特定视觉特征的意外依赖对于确保模型鲁棒性、防止过拟合和避免虚假相关性至关重要。

Method: 使用自反思代理迭代生成和测试关于模型可能依赖的视觉属性的假设，基于实验结果精炼假设，并通过自评估协议评估发现是否准确解释模型行为。

Result: 在包含130个模型的基准测试中，代理性能随自反思持续提升，显著优于非反思基线，并能识别CLIP视觉编码器和YOLOv8等先进模型中的真实视觉属性依赖。

Conclusion: 该自反思框架能有效检测视觉模型对特定视觉特征的依赖，为模型分析和改进提供了有力工具。

Abstract: When a vision model performs image recognition, which visual attributes drive
its predictions? Detecting unintended reliance on specific visual features is
critical for ensuring model robustness, preventing overfitting, and avoiding
spurious correlations. We introduce an automated framework for detecting such
dependencies in trained vision models. At the core of our method is a
self-reflective agent that systematically generates and tests hypotheses about
visual attributes that a model may rely on. This process is iterative: the
agent refines its hypotheses based on experimental outcomes and uses a
self-evaluation protocol to assess whether its findings accurately explain
model behavior. When inconsistencies arise, the agent self-reflects over its
findings and triggers a new cycle of experimentation. We evaluate our approach
on a novel benchmark of 130 models designed to exhibit diverse visual attribute
dependencies across 18 categories. Our results show that the agent's
performance consistently improves with self-reflection, with a significant
performance increase over non-reflective baselines. We further demonstrate that
the agent identifies real-world visual attribute dependencies in
state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object
detector.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Safety Assessment in Reinforcement Learning via Model Predictive Control](https://arxiv.org/abs/2510.20955)
*Jeff Pflueger,Michael Everett*

Main category: cs.LG

TL;DR: 提出了一种基于可逆性的安全强化学习方法，通过模型预测路径积分控制来检查学习策略提出的动作安全性，无需显式动态模型或安全约束知识。


<details>
  <summary>Details</summary>
Motivation: 无模型强化学习缺乏形式化安全保证，现有方法通常需要详细的安全规范知识，而许多难以明确规范的安全问题可以通过不变性来表征。

Method: 利用可逆性防止训练过程中的安全问题，使用模型预测路径积分控制检查学习策略提出的动作安全性，仅需查询黑盒动态系统。

Result: 实验结果表明，该方法在所有不安全动作发生前成功中止，同时训练进度与允许违反安全性的基线PPO方法相当。

Conclusion: 基于可逆性的方法能够有效提供安全保证，仅需黑盒动态查询能力，无需显式动态模型或安全约束知识。

Abstract: Model-free reinforcement learning approaches are promising for control but
typically lack formal safety guarantees. Existing methods to shield or
otherwise provide these guarantees often rely on detailed knowledge of the
safety specifications. Instead, this work's insight is that many
difficult-to-specify safety issues are best characterized by invariance.
Accordingly, we propose to leverage reversibility as a method for preventing
these safety issues throughout the training process. Our method uses
model-predictive path integral control to check the safety of an action
proposed by a learned policy throughout training. A key advantage of this
approach is that it only requires the ability to query the black-box dynamics,
not explicit knowledge of the dynamics or safety constraints. Experimental
results demonstrate that the proposed algorithm successfully aborts before all
unsafe actions, while still achieving comparable training progress to a
baseline PPO approach that is allowed to violate safety.

</details>


### [8] [Exploring Spiking Neural Networks for Binary Classification in Multivariate Time Series at the Edge](https://arxiv.org/abs/2510.20997)
*James Ghawaly,Andrew Nicholson,Catherine Schuman,Dalton Diez,Aaron Young,Brett Witherspoon*

Main category: cs.LG

TL;DR: 提出了一种训练脉冲神经网络(SNN)进行多元时间序列二分类的通用框架，特别关注逐步预测和低误报率下的高精度。通过EONS算法联合优化SNN架构和参数，在放射性源检测和癫痫检测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用的SNN训练框架，用于多元时间序列的二进制分类，特别强调在低误报率下实现高精度，并解决传统方法在资源受限环境下的局限性。

Method: 使用EONS算法进化稀疏、有状态的SNN，联合优化架构和参数；将输入编码为脉冲序列，通过阈值化单个输出神经元的脉冲计数进行预测；采用简单投票集成方法提高性能和鲁棒性。

Result: 在放射性源检测任务中，仅用49个神经元和66个突触的SNN在1/小时误报率下达到51.8%真阳性率，优于PCA(42.7%)和深度学习(49.8%)基线；三模型集成将真阳性率提升至67.1%；硬件部署显示2mW功耗和20.2ms推理延迟。在癫痫检测任务中，集成方法达到95%真阳性率和16%假阳性率，与深度学习相当但参数大幅减少。

Conclusion: 该框架能够训练出高效、低功耗的SNN，在多个时间序列分类任务中表现出色，特别适合资源受限的边缘计算应用，展示了SNN在低功耗实时检测任务中的巨大潜力。

Abstract: We present a general framework for training spiking neural networks (SNNs) to
perform binary classification on multivariate time series, with a focus on
step-wise prediction and high precision at low false alarm rates. The approach
uses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm to
evolve sparse, stateful SNNs by jointly optimizing their architectures and
parameters. Inputs are encoded into spike trains, and predictions are made by
thresholding a single output neuron's spike counts. We also incorporate simple
voting ensemble methods to improve performance and robustness.
  To evaluate the framework, we apply it with application-specific
optimizations to the task of detecting low signal-to-noise ratio radioactive
sources in gamma-ray spectral data. The resulting SNNs, with as few as 49
neurons and 66 synapses, achieve a 51.8% true positive rate (TPR) at a false
alarm rate of 1/hr, outperforming PCA (42.7%) and deep learning (49.8%)
baselines. A three-model any-vote ensemble increases TPR to 67.1% at the same
false alarm rate. Hardware deployment on the microCaspian neuromorphic platform
demonstrates 2mW power consumption and 20.2ms inference latency.
  We also demonstrate generalizability by applying the same framework, without
domain-specific modification, to seizure detection in EEG recordings. An
ensemble achieves 95% TPR with a 16% false positive rate, comparable to recent
deep learning approaches with significant reduction in parameter count.

</details>


### [9] [Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference](https://arxiv.org/abs/2510.21017)
*Yuhong Luo,Austin Hoag,Xintong Wang,Philip S. Thomas,Przemyslaw A. Grabowicz*

Main category: cs.LG

TL;DR: 提出了FRG框架，通过对抗模型学习具有高置信度公平性保证的表示，确保下游预测中的人口统计差异不超过用户定义的误差阈值。


<details>
  <summary>Details</summary>
Motivation: 表示学习在多个下游任务中应用广泛，但需要确保公平性，防止对特定人口群体的不公平。现有方法缺乏高置信度的公平性保证。

Method: 提出FRG框架，利用优化的对抗模型来提供高置信度的公平性保证，确保下游预测的人口统计差异有界。

Result: 在三个真实世界数据集上的实验表明，FRG在多种下游模型和任务中始终能约束不公平性，优于六种最先进的公平表示学习方法。

Conclusion: FRG框架能够有效学习具有高置信度公平性保证的表示，为公平表示学习提供了可靠的理论和实践基础。

Abstract: Representation learning is increasingly applied to generate representations
that generalize well across multiple downstream tasks. Ensuring fairness
guarantees in representation learning is crucial to prevent unfairness toward
specific demographic groups in downstream tasks. In this work, we formally
introduce the task of learning representations that achieve high-confidence
fairness. We aim to guarantee that demographic disparity in every downstream
prediction remains bounded by a *user-defined* error threshold $\epsilon$, with
*controllable* high probability. To this end, we propose the ***F**air
**R**epresentation learning with high-confidence **G**uarantees (FRG)*
framework, which provides these high-confidence fairness guarantees by
leveraging an optimized adversarial model. We empirically evaluate FRG on three
real-world datasets, comparing its performance to six state-of-the-art fair
representation learning methods. Our results demonstrate that FRG consistently
bounds unfairness across a range of downstream models and tasks.

</details>


### [10] [Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study](https://arxiv.org/abs/2510.21389)
*Stefan Kraft,Andreas Theissler,Vera Wienhausen-Wilke,Gjergji Kasneci,Hendrik Lensch*

Main category: cs.LG

TL;DR: 本研究通过用户实验证明，在睡眠医学中，透明AI辅助作为质量控制步骤可显著提升专家评分性能30%，比黑盒AI更有效，且质量控制时机能进一步改善计数结果。


<details>
  <summary>Details</summary>
Motivation: AI系统在生物医学信号解释方面已能与人类专家匹敌，但临床实践需要医生能判断何时以及为何信任算法建议，因此研究AI辅助的类型和时机对临床工作流程的影响至关重要。

Method: 对8名专业睡眠医学从业者进行应用导向的用户研究，在三种条件下评分多导睡眠图数据：手动评分、黑盒AI辅助和透明白盒AI辅助，辅助时机分为起始辅助和质量控制后辅助。

Result: AI和人类-AI团队在临床标准下均显著优于未辅助专家，协作还减少了评分者间变异性。透明AI辅助作为质量控制步骤比黑盒辅助提升约30%的事件级性能，质量控制时机进一步改善计数结果。

Conclusion: 策略性定时的透明AI辅助能有效平衡准确性和临床效率，为临床工作流程中可信AI集成和用户接受提供了有前景的途径。

Abstract: Artificial intelligence (AI) systems increasingly match or surpass human
experts in biomedical signal interpretation. However, their effective
integration into clinical practice requires more than high predictive accuracy.
Clinicians must discern \textit{when} and \textit{why} to trust algorithmic
recommendations. This work presents an application-grounded user study with
eight professional sleep medicine practitioners, who score nocturnal arousal
events in polysomnographic data under three conditions: (i) manual scoring,
(ii) black-box (BB) AI assistance, and (iii) transparent white-box (WB) AI
assistance. Assistance is provided either from the \textit{start} of scoring or
as a post-hoc quality-control (\textit{QC}) review. We systematically evaluate
how the type and timing of assistance influence event-level and clinically most
relevant count-based performance, time requirements, and user experience. When
evaluated against the clinical standard used to train the AI, both AI and
human-AI teams significantly outperform unaided experts, with collaboration
also reducing inter-rater variability. Notably, transparent AI assistance
applied as a targeted QC step yields median event-level performance
improvements of approximately 30\% over black-box assistance, and QC timing
further enhances count-based outcomes. While WB and QC approaches increase the
time required for scoring, start-time assistance is faster and preferred by
most participants. Participants overwhelmingly favor transparency, with seven
out of eight expressing willingness to adopt the system with minor or no
modifications. In summary, strategically timed transparent AI assistance
effectively balances accuracy and clinical efficiency, providing a promising
pathway toward trustworthy AI integration and user acceptance in clinical
workflows.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems](https://arxiv.org/abs/2510.21027)
*Zhe Fei,Mehmet Yigit Turali,Shreyas Rajesh,Xinyang Dai,Huyen Pham,Pavan Holur,Yuhui Zhu,Larissa Mooney,Yih-Ing Hser,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出一个基于开源大语言模型的框架，从异构电子健康记录中提取阿片类药物使用障碍治疗处方信息，并计算标准化的药物覆盖天数指标。


<details>
  <summary>Details</summary>
Motivation: 解决不同电子健康记录系统中药物数据格式不统一的问题，特别是阿片类药物使用障碍治疗处方的标准化监测需求。

Method: 定制开源大语言模型（Llama、Qwen、Gemma、MedGemma）来提取处方属性，采用固定JSON模式处理记录，并进行轻量级标准化和跨字段一致性检查。

Result: 在25,605条记录上评估，Qwen2.5-32B达到93.4%覆盖率和93.0%精确匹配准确率，MedGemma-27B达到93.1%/92.2%。

Conclusion: 该方法消除了脆弱的站点特定ETL流程，支持本地隐私保护部署，为真实世界环境中MOUD暴露、依从性和保留的跨站点分析提供了可行方案。

Abstract: Harmonizing medication data across Electronic Health Record (EHR) systems is
a persistent barrier to monitoring medications for opioid use disorder (MOUD).
In heterogeneous EHR systems, key prescription attributes are scattered across
differently formatted fields and freetext notes. We present a practical
framework that customizes open source large language models (LLMs), including
Llama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription
attributes (prescription date, drug name, duration, total quantity, daily
quantity, and refills) from heterogeneous, site specific data and compute a
standardized metric of medication coverage, \emph{MOUD days}, per patient. Our
pipeline processes records directly in a fixed JSON schema, followed by
lightweight normalization and cross-field consistency checks. We evaluate the
system on prescription level EHR data from five clinics in a national OUD study
(25{,}605 records from 1{,}257 patients), using a previously annotated
benchmark of 10{,}369 records (776 patients) as the ground truth. Performance
is reported as coverage (share of records with a valid, matchable output) and
record-level exact-match accuracy. Larger models perform best overall:
Qwen2.5-32B achieves \textbf{93.4\%} coverage with \textbf{93.0\%} exact-match
accuracy across clinics, and MedGemma-27B attains
\textbf{93.1\%}/\textbf{92.2\%}. A brief error review highlights three common
issues and fixes: imputing missing dosage fields using within-drug norms,
handling monthly/weekly injectables (e.g., Vivitrol) by setting duration from
the documented schedule, and adding unit checks to prevent mass units (e.g.,
``250 g'') from being misread as daily counts. By removing brittle,
site-specific ETL and supporting local, privacy-preserving deployment, this
approach enables consistent cross-site analyses of MOUD exposure, adherence,
and retention in real-world settings.

</details>


### [12] [EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law](https://arxiv.org/abs/2510.21524)
*Ilija Lichkovski,Alexander Müller,Mariam Ibrahim,Tiwai Mhundwa*

Main category: cs.AI

TL;DR: EU-Agent-Bench是一个评估LLM代理在欧盟法律框架下合规性的基准测试，通过模拟可能引发非法行为的良性用户请求来测试模型的法律遵从性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在各种场景中的部署增加，它们可能表现出不可预测的行为，包括采取不良和/或不安全的行动。需要测量LLM代理在欧盟立法背景下采取非法行为的潜在倾向。

Method: 创建了一个可验证的人工策划基准，涵盖数据保护、偏见/歧视和科学诚信等多个类别。通过将模型的函数调用与详尽引用相关立法的评分标准进行比较来评估法律合规性，并研究在系统提示中提供相关立法摘录对合规性的影响。

Result: 评估了前沿LLM的法律合规性，并研究了提供立法摘录对合规性的影响。发布了公开预览集供研究社区使用，同时保留私有测试集以防止数据污染。

Conclusion: 鼓励未来工作将代理安全基准扩展到不同的法律管辖区，以及多轮和多语言交互。发布了代码供研究使用。

Abstract: Large language models (LLMs) are increasingly deployed as agents in various
contexts by providing tools at their disposal. However, LLM agents can exhibit
unpredictable behaviors, including taking undesirable and/or unsafe actions. In
order to measure the latent propensity of LLM agents for taking illegal actions
under an EU legislative context, we introduce EU-Agent-Bench, a verifiable
human-curated benchmark that evaluates an agent's alignment with EU legal norms
in situations where benign user inputs could lead to unlawful actions. Our
benchmark spans scenarios across several categories, including data protection,
bias/discrimination, and scientific integrity, with each user request allowing
for both compliant and non-compliant execution of the requested actions.
Comparing the model's function calls against a rubric exhaustively supported by
citations of the relevant legislature, we evaluate the legal compliance of
frontier LLMs, and furthermore investigate the compliance effect of providing
the relevant legislative excerpts in the agent's system prompt along with
explicit instructions to comply. We release a public preview set for the
research community, while holding out a private test set to prevent data
contamination in evaluating upcoming models. We encourage future work extending
agentic safety benchmarks to different legal jurisdictions and to multi-turn
and multilingual interactions. We release our code on
\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: TripTide是首个评估LLM在现实旅行中断情况下修订行程能力的基准，通过建模中断严重性和旅行者容忍度等维度，综合评估LLM对航班取消、天气关闭等事件的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划系统如TripCraft和TravelPlanner虽然能生成个性化行程，但无法有效应对现实旅行中的各种中断情况，需要评估LLM在动态环境下的适应能力。

Method: 采用三重评估方法：1) 引入自动指标（意图保持、响应性、适应性）；2) 使用LLM作为评判者自动评估修订质量；3) 进行人工专家评估验证语义、空间、顺序和响应方面的保持情况。

Result: 实验显示LLM在顺序一致性和语义稳定性方面表现良好，空间偏差在短途旅行中较大但随行程延长而减小。然而，随着计划长度增加，中断处理能力下降，表明LLM鲁棒性存在局限。

Conclusion: TripTide为评估LLM在现实不确定性下的旅行规划适应性、个性化和韧性建立了基准，揭示了当前模型在长行程中断处理方面的不足。

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [14] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 提出了一个专门针对瑞典相关人物和事件的手动编写问答基准，用于评估多语言模型在瑞典特定知识上的表现。


<details>
  <summary>Details</summary>
Motivation: 许多瑞典基准测试是翻译自美国中心的基准，不适合测试瑞典特有的相关知识。需要专门针对瑞典的基准来评估模型对本地知识的掌握程度。

Method: 手动编写问答数据集，灵感来自瑞典文化媒体名人参与的流行广播节目和主要体育赛事，包含英语翻译以测试跨语言事实一致性。

Result: 发现具有更强瑞典覆盖范围的小型模型在回忆瑞典相关事实方面与三倍大的多语言模型表现相当。继续在瑞典语上进行预训练通常能提高事实知识，但也会导致部分先前已知信息的遗忘。

Conclusion: 该数据集可作为研究多语言模型中语言适应和知识保留的诊断工具，展示了其在评估语言特定知识方面的潜力。

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [15] [PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees](https://arxiv.org/abs/2510.21438)
*Satheeshkumar Veeramani,Zhengxue Zhou,Francisco Munguia-Galeano,Hatem Fakhruldeen,Thomas Roddelkopf,Mohammed Faeik Ruzaij Al-Okby,Kerstin Thurow,Andrew Ian Cooper*

Main category: cs.RO

TL;DR: 提出了PREVENT系统，这是一个基于多模态行为树的移动机器人化学家系统，能够检测和避免工作流程中的异常情况，完全避免了误报和漏报。


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人化学家缺乏工作流程意识能力，小的异常（如样品瓶未正确盖好）可能中断整个工作流程，浪费时间和资源，并对研究人员构成安全风险。现有感知机制会产生过多误报。

Method: 采用基于多模态行为树的方法，包含分层感知机制，利用AI技术和通过灵巧视觉、导航视觉摄像头及物联网气体传感器的感官反馈进行执行相关决策。

Result: 实验评估显示该方法相对高效，在模拟风险场景中完全避免了误报和漏报。多模态感知技能在导航和操作方面的部署准确率均高于相应单模态技能的平均水平。

Conclusion: PREVENT系统能够有效集成到现有软件架构中，提供可靠的工作流程异常检测和避免能力，确保机器人化学家的自主操作安全性和效率。

Abstract: Mobile robotic chemists are a fast growing trend in the field of chemistry
and materials research. However, so far these mobile robots lack workflow
awareness skills. This poses the risk that even a small anomaly, such as an
improperly capped sample vial could disrupt the entire workflow. This wastes
time, and resources, and could pose risks to human researchers, such as
exposure to toxic materials. Existing perception mechanisms can be used to
predict anomalies but they often generate excessive false positives. This may
halt workflow execution unnecessarily, requiring researchers to intervene and
to resume the workflow when no problem actually exists, negating the benefits
of autonomous operation. To address this problem, we propose PREVENT a system
comprising navigation and manipulation skills based on a multimodal Behavior
Tree (BT) approach that can be integrated into existing software architectures
with minimal modifications. Our approach involves a hierarchical perception
mechanism that exploits AI techniques and sensory feedback through Dexterous
Vision and Navigational Vision cameras and an IoT gas sensor module for
execution-related decision-making. Experimental evaluations show that the
proposed approach is comparatively efficient and completely avoids both false
negatives and false positives when tested in simulated risk scenarios within
our robotic chemistry workflow. The results also show that the proposed
multi-modal perception skills achieved deployment accuracies that were higher
than the average of the corresponding uni-modal skills, both for navigation and
for manipulation.

</details>
