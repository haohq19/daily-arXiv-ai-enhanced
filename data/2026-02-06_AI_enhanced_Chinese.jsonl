{"id": "2602.05085", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05085", "abs": "https://arxiv.org/abs/2602.05085", "authors": ["Sidi Lu", "Zhenwen Liang", "Dongyang Ma", "Yan Wang", "Haitao Mi", "Dong Yu"], "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories", "comment": "Tencent AI Lab Technical Report", "summary": "In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.", "AI": {"tldr": "Locas\u662f\u4e00\u79cd\u5c40\u90e8\u652f\u6301\u7684\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u53ef\u4e0eTransformer\u7684FFN\u6a21\u5757\u5171\u4eab\u8bbe\u8ba1\uff0c\u80fd\u591f\u7075\u6d3b\u5730\u4ece\u6a21\u578b\u53c2\u6570\u4e2d\u5378\u8f7d\u6216\u5408\u5e76\uff0c\u652f\u6301\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u6865\u63a5\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0e\u65b0\u578b\u53c2\u6570\u5316\u8bb0\u5fc6\uff0c\u4f7f\u8bb0\u5fc6\u80fd\u591f\u7075\u6d3b\u5730\u4ece\u6a21\u578b\u53c2\u6570\u4e2d\u5378\u8f7d\u6216\u5408\u5e76\uff0c\u4ee5\u652f\u6301\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u5e76\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51faLocas\uff08\u5c40\u90e8\u652f\u6301\u7684\u53c2\u6570\u5316\u8bb0\u5fc6\uff09\uff0c\u91c7\u7528\u4e0eTransformer FFN\u6a21\u5757\u76f8\u4f3c\u7684\u8bbe\u8ba1\uff0c\u6709\u4e24\u79cd\u4e3b\u8981\u53d8\u4f53\uff1a\u4f20\u7edf\u4e24\u5c42MLP\u8bbe\u8ba1\uff08\u7406\u8bba\u4fdd\u8bc1\u66f4\u6e05\u6670\uff09\u548c\u4e0eSOTA LLMs\u76f8\u540c\u7684GLU-FFN\u7ed3\u6784\u3002\u5173\u952e\u662f\u901a\u8fc7\u91cd\u7528\u6a21\u578b\u53c2\u6570\u3001\u6fc0\u6d3b\u548c/\u6216\u68af\u5ea6\u6765\u6b63\u786e\u521d\u59cb\u5316\u8fd9\u79cd\u4f4e\u79e9\u4fa7\u5411FFN\u5f0f\u8bb0\u5fc6\u3002", "result": "\u5728PG-19\u6574\u4e66\u8bed\u8a00\u5efa\u6a21\u548cLoCoMo\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u95ee\u7b54\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u3002Locas-GLU\u4ec5\u97000.02%\u989d\u5916\u53c2\u6570\u5373\u53ef\u5b58\u50a8\u8fc7\u53bb\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u5c0f\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002MMLU\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u8bb0\u5fc6\u6574\u672c\u4e66\u540e\uff0c\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "Locas\u80fd\u591f\u5c06\u8fc7\u53bb\u4e0a\u4e0b\u6587\u6c38\u4e45\u5316\u4e3a\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6a21\u578b\u73b0\u6709\u5185\u90e8\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5c55\u793a\u4e86\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u53c2\u6570\u5316\u8bb0\u5fc6\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.05078", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.05078", "abs": "https://arxiv.org/abs/2602.05078", "authors": ["Gautham Vinod", "Fengqing Zhu"], "title": "Food Portion Estimation: From Pixels to Calories", "comment": null, "summary": "Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u4e2d\u98df\u7269\u4e09\u7ef4\u5c3a\u5bf8\u4f30\u8ba1\u7684\u4e0d\u540c\u7b56\u7565\uff0c\u5305\u62ec\u6df1\u5ea6\u56fe\u3001\u591a\u89c6\u89d2\u8f93\u5165\u3001\u6a21\u677f\u5339\u914d\u7b49\u8f85\u52a9\u8f93\u5165\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u5728\u5355\u76ee\u56fe\u50cf\u6216\u7ec4\u5408\u8f93\u5165\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u5bf9\u4e8e\u51c6\u786e\u4fbf\u6377\u5730\u76d1\u6d4b\u4e2a\u4f53\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u662f\u9884\u9632\u548c\u62a4\u7406\u6162\u6027\u75be\u75c5\u53ca\u80a5\u80d6\u7684\u91cd\u8981\u673a\u5236\u3002\u7136\u800c\uff0c\u4ece2D\u56fe\u50cf\u8f93\u5165\u4f30\u8ba1\u98df\u7269\u7684\u4e09\u7ef4\u5c3a\u5bf8\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63a2\u7d22\u4e86\u591a\u79cd\u7b56\u7565\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff1a1\uff09\u4f7f\u7528\u8f85\u52a9\u8f93\u5165\u5982\u6df1\u5ea6\u56fe\u548c\u591a\u89c6\u89d2\u8f93\u5165\uff1b2\uff09\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5982\u6a21\u677f\u5339\u914d\uff1b3\uff09\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528\u5355\u76ee\u56fe\u50cf\u6216\u56fe\u50cf\u4e0e\u8f85\u52a9\u8f93\u5165\u7684\u7ec4\u5408\u6765\u7cbe\u786e\u9884\u6d4b\u98df\u7269\u5206\u91cf\u3002", "result": "\u8bba\u6587\u4e3b\u8981\u5bf9\u5404\u79cd\u7b56\u7565\u8fdb\u884c\u4e86\u7efc\u8ff0\u5206\u6790\uff0c\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u800c\u662f\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u89e3\u51b3\u98df\u7269\u4e09\u7ef4\u5c3a\u5bf8\u4f30\u8ba1\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u9700\u8981\u51c6\u786e\u7684\u98df\u7269\u5206\u91cf\u4f30\u8ba1\uff0c\u591a\u79cd\u7b56\u7565\u5df2\u88ab\u5f00\u53d1\u6765\u89e3\u51b3\u4ece2D\u56fe\u50cf\u4f30\u8ba13D\u5c3a\u5bf8\u7684\u6311\u6218\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b49\u73b0\u4ee3\u65b9\u6cd5\u6b63\u5728\u5e2e\u52a9\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002"}}
{"id": "2602.05113", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05113", "abs": "https://arxiv.org/abs/2602.05113", "authors": ["Suvadip Sana", "Jinzhou Wu", "Martin T. Wells"], "title": "Democratic Preference Alignment via Sortition-Weighted RLHF", "comment": "16 pages, 5 figures", "summary": "Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.", "AI": {"tldr": "DemPO\u6846\u67b6\u901a\u8fc7\u7b97\u6cd5\u62bd\u7b7e\u6784\u5efa\u4ee3\u8868\u6027\u4eba\u7c7b\u8bc4\u5206\u8005\u5c0f\u7ec4\uff0c\u7528\u4e8eAI\u504f\u597d\u5bf9\u9f50\u8bad\u7ec3\uff0c\u76f8\u6bd4\u4f20\u7edf\u4fbf\u5229\u6837\u672c\u80fd\u66f4\u597d\u5730\u53cd\u6620\u591a\u5143\u4eba\u53e3\u4ef7\u503c\u89c2\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u504f\u597d\u7684AI\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\uff09\u4f9d\u8d56\u4eba\u7c7b\u8bc4\u5206\u8005\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u5206\u8005\u901a\u5e38\u662f\u4fbf\u5229\u6837\u672c\uff0c\u5b58\u5728\u4eba\u53e3\u4ee3\u8868\u6027\u504f\u5dee\uff0c\u65e0\u6cd5\u53cd\u6620\u591a\u5143\u7fa4\u4f53\u7684\u4ef7\u503c\u89c2\u3002", "method": "\u63d0\u51fa\u6c11\u4e3b\u504f\u597d\u4f18\u5316\uff08DemPO\uff09\u6846\u67b6\uff0c\u91c7\u7528\u7b97\u6cd5\u62bd\u7b7e\u673a\u5236\u6784\u5efa\u4ee3\u8868\u6027\u8bc4\u5206\u5c0f\u7ec4\u3002\u63d0\u4f9b\u4e24\u79cd\u8bad\u7ec3\u65b9\u6848\uff1a\u786c\u9762\u677f\uff08\u4ec5\u4f7f\u7528\u62bd\u7b7e\u9009\u51fa\u7684\u4ee3\u8868\u6027\u5c0f\u7ec4\u6570\u636e\uff09\u548c\u8f6f\u9762\u677f\uff08\u4fdd\u7559\u6240\u6709\u6570\u636e\u4f46\u6309\u62bd\u7b7e\u6982\u7387\u91cd\u65b0\u52a0\u6743\uff09\u3002", "result": "\u57281B\u52308B\u53c2\u6570\u7684Llama\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u786c\u9762\u677f\u65b9\u6848\u5728\u516d\u79cd\u805a\u5408\u65b9\u6cd5\u4e2d\u59cb\u7ec8\u6392\u540d\u7b2c\u4e00\uff0c\u8f6f\u9762\u677f\u59cb\u7ec8\u4f18\u4e8e\u672a\u52a0\u6743\u57fa\u7ebf\uff0c\u4e14\u6a21\u578b\u5bb9\u91cf\u8d8a\u5927\u6548\u679c\u8d8a\u660e\u663e\u3002", "conclusion": "\u5728\u504f\u597d\u6536\u96c6\u9636\u6bb5\u5f3a\u5236\u5b9e\u65bd\u4eba\u53e3\u4ee3\u8868\u6027\uff0c\u800c\u975e\u4e8b\u540e\u4fee\u6b63\uff0c\u80fd\u8bad\u7ec3\u51fa\u66f4\u597d\u5730\u53cd\u6620\u4ee3\u8868\u6027\u516c\u4f17\u4ef7\u503c\u89c2\u7684AI\u6a21\u578b\u3002"}}
{"id": "2602.05220", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.05220", "abs": "https://arxiv.org/abs/2602.05220", "authors": ["Jinchuan Tian", "Haoran Wang", "Bo-Hao Su", "Chien-yu Huang", "Qingzheng Wang", "Jiatong Shi", "William Chen", "Xun Gong", "Siddhant Arora", "Chin-Jou Li", "Masao Someki", "Takashi Maekaku", "Yusuke Shinohara", "Jin Sakuma", "Chao-Han Huck Yang", "Shinji Watanabe"], "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions", "comment": null, "summary": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.", "AI": {"tldr": "Bagpiper\u662f\u4e00\u4e2a80\u4ebf\u53c2\u6570\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff08\u5b57\u5e55\uff09\u6765\u7406\u89e3\u7269\u7406\u97f3\u9891\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u57fa\u7840\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u50f5\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\uff0c\u53ea\u80fd\u5904\u7406\u97f3\u9891\u7684\u5b64\u7acb\u56e0\u7d20\u800c\u975e\u6574\u4f53\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u667a\u80fd\u4ee5\u6574\u4f53\u65b9\u5f0f\u5904\u7406\u97f3\u9891\uff0c\u65e0\u7f1d\u8fde\u63a5\u7269\u7406\u4fe1\u53f7\u4e0e\u62bd\u8c61\u8ba4\u77e5\u6982\u5ff5\u6765\u6267\u884c\u590d\u6742\u4efb\u52a1\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u97f3\u9891\u6a21\u578b\u3002", "method": "1. \u4f7f\u7528\u4e30\u5bcc\u7684\u5b57\u5e55\uff08\u5168\u9762\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff09\u6765\u89e3\u91ca\u7269\u7406\u97f3\u9891\uff0c\u8fd9\u4e9b\u5b57\u5e55\u5c01\u88c5\u4e86\u4fe1\u53f7\u4e2d\u7684\u5173\u952e\u8ba4\u77e5\u6982\u5ff5\uff08\u5982\u8f6c\u5f55\u3001\u97f3\u9891\u4e8b\u4ef6\uff09\uff1b2. \u5728600B token\u7684\u5927\u89c4\u6a21\u8bed\u6599\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5efa\u7acb\u539f\u59cb\u97f3\u9891\u4e0e\u9ad8\u7ea7\u6982\u5ff5\u7a7a\u95f4\u4e4b\u95f4\u7684\u7a33\u5065\u53cc\u5411\u6620\u5c04\uff1b3. \u5fae\u8c03\u65f6\u91c7\u7528\"\u5148\u5b57\u5e55\u540e\u5904\u7406\"\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6a21\u62df\u4e2d\u95f4\u8ba4\u77e5\u63a8\u7406\u6b65\u9aa4\u6765\u89e3\u51b3\u591a\u6837\u5316\u4efb\u52a1\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u3002", "result": "1. \u5728\u97f3\u9891\u7406\u89e3\u65b9\u9762\uff0cBagpiper\u5728MMAU\u548cAIRBench\u4e0a\u8d85\u8d8a\u4e86Qwen-2.5-Omni\uff1b2. \u5728\u751f\u6210\u8d28\u91cf\u65b9\u9762\uff0c\u8d85\u8d8a\u4e86CosyVoice3\u548cTangoFlux\uff0c\u80fd\u591f\u5408\u6210\u8bed\u97f3\u3001\u97f3\u4e50\u548c\u97f3\u6548\u7684\u4efb\u610f\u7ec4\u5408\uff1b3. \u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6279\u5b9e\u73b0\u901a\u7528\u97f3\u9891\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u7684\u5de5\u4f5c\u4e4b\u4e00\u3002", "conclusion": "Bagpiper\u901a\u8fc7\u5c06\u7269\u7406\u97f3\u9891\u4fe1\u53f7\u6620\u5c04\u5230\u4e30\u5bcc\u7684\u8ba4\u77e5\u6982\u5ff5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\uff0c\u6a21\u62df\u4e86\u4eba\u7c7b\u5904\u7406\u97f3\u9891\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u97f3\u9891AI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.05202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05202", "abs": "https://arxiv.org/abs/2602.05202", "authors": ["Shivanshu Shekhar", "Uttaran Bhattacharya", "Raghavendra Addanki", "Mehrab Tanjim", "Somdeb Sarkhel", "Tong Zhang"], "title": "GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling", "comment": null, "summary": "Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \\modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\\times$ to $65\\times$ fewer than existing VLM-based approaches.", "AI": {"tldr": "\u63d0\u51faGenerative-Transformer-based Self-Supervised Video Judge (GTSVJ)\uff0c\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4f7f\u5176\u80fd\u591f\u7cbe\u786e\u8bc4\u4f30\u89c6\u9891\u8d28\u91cf\uff0c\u4ec5\u97003\u4e07\u4e2a\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u89c6\u9891\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002\u89c6\u9891\u751f\u6210\u6a21\u578b\u5929\u751f\u5177\u5907\u5efa\u6a21\u65f6\u95f4\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4f46\u5c1a\u672a\u88ab\u7528\u4e8e\u5956\u52b1\u5efa\u6a21\u3002", "method": "\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u91cd\u65b0\u6784\u5efa\u4e3a\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b(EBMs)\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u4f7f\u5176\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u5206\u914d\u4f4e\u80fd\u91cf\u3001\u4f4e\u8d28\u91cf\u89c6\u9891\u5206\u914d\u9ad8\u80fd\u91cf\u3002\u8bbe\u8ba1\u4e09\u79cd\u6f5c\u5728\u7a7a\u95f4\u6270\u52a8\u65b9\u6cd5\u751f\u6210\u5408\u6210\u8d1f\u6837\u672c\uff1a\u65f6\u95f4\u5207\u7247\u3001\u7279\u5f81\u4ea4\u6362\u548c\u5e27\u91cd\u6392\uff0c\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u6709\u610f\u4e49\u7684\u65f6\u7a7a\u7279\u5f81\u800c\u975e\u8868\u9762\u5dee\u5f02\u3002", "result": "\u5728GenAI-Bench\u548cMonteBench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ec5\u4f7f\u75283\u4e07\u4e2a\u4eba\u5de5\u6807\u6ce8\uff0c\u6bd4\u73b0\u6709VLM\u65b9\u6cd5\u5c116-65\u500d\u6570\u636e\u91cf\u3002", "conclusion": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u91cd\u65b0\u7528\u4f5c\u65f6\u95f4\u611f\u77e5\u7684\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8d1f\u6837\u672c\u751f\u6210\uff0c\u80fd\u591f\u4ee5\u6781\u5c11\u7684\u4eba\u5de5\u6807\u6ce8\u5b9e\u73b0\u5353\u8d8a\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2602.04917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04917", "abs": "https://arxiv.org/abs/2602.04917", "authors": ["Soshi Kakio", "Yasuko Matsubara", "Ren Fujiwara", "Yasushi Sakurai"], "title": "Multi-Aspect Mining and Anomaly Detection for Heterogeneous Tensor Streams", "comment": "Proceedings of the ACM Web Conference 2026 (WWW '26), April 13--17, 2026, Dubai, United Arab Emirates, 12 pages", "summary": "Analysis and anomaly detection in event tensor streams consisting of timestamps and multiple attributes - such as communication logs(time, IP address, packet length)- are essential tasks in data mining. While existing tensor decomposition and anomaly detection methods provide useful insights, they face the following two limitations. (i) They cannot handle heterogeneous tensor streams, which comprises both categorical attributes(e.g., IP address) and continuous attributes(e.g., packet length). They typically require either discretizing continuous attributes or treating categorical attributes as continuous, both of which distort the underlying statistical properties of the data.Furthermore, incorrect assumptions about the distribution family of continuous attributes often degrade the model's performance. (ii) They discretize timestamps, failing to track the temporal dynamics of streams(e.g., trends, abnormal events), which makes them ineffective for detecting anomalies at the group level, referred to as 'group anomalies' (e.g, DoS attacks). To address these challenges, we propose HeteroComp, a method for continuously summarizing heterogeneous tensor streams into 'components' representing latent groups in each attribute and their temporal dynamics, and detecting group anomalies. Our method employs Gaussian process priors to model unknown distributions of continuous attributes, and temporal dynamics, which directly estimate probability densities from data. Extracted components give concise but effective summarization, enabling accurate group anomaly detection. Extensive experiments on real datasets demonstrate that HeteroComp outperforms the state-of-the-art algorithms for group anomaly detection accuracy, and its computational time does not depend on the data stream length.", "AI": {"tldr": "HeteroComp\uff1a\u4e00\u79cd\u5904\u7406\u5f02\u6784\u5f20\u91cf\u6d41\uff08\u5305\u542b\u5206\u7c7b\u548c\u8fde\u7eed\u5c5e\u6027\uff09\u7684\u7ec4\u4ef6\u63d0\u53d6\u548c\u7fa4\u4f53\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u5efa\u6a21\u8fde\u7eed\u5c5e\u6027\u5206\u5e03\u548c\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u5206\u89e3\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5305\u542b\u5206\u7c7b\u5c5e\u6027\uff08\u5982IP\u5730\u5740\uff09\u548c\u8fde\u7eed\u5c5e\u6027\uff08\u5982\u6570\u636e\u5305\u957f\u5ea6\uff09\u7684\u5f02\u6784\u5f20\u91cf\u6d41\uff0c\u4e14\u901a\u5e38\u9700\u8981\u79bb\u6563\u5316\u65f6\u95f4\u6233\uff0c\u65e0\u6cd5\u6355\u6349\u6d41\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5bfc\u81f4\u7fa4\u4f53\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faHeteroComp\u65b9\u6cd5\uff0c\u5c06\u5f02\u6784\u5f20\u91cf\u6d41\u8fde\u7eed\u603b\u7ed3\u4e3a\u8868\u793a\u5404\u5c5e\u6027\u4e2d\u6f5c\u5728\u7fa4\u4f53\u53ca\u5176\u65f6\u95f4\u52a8\u6001\u7684\"\u7ec4\u4ef6\"\u3002\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u5efa\u6a21\u8fde\u7eed\u5c5e\u6027\u7684\u672a\u77e5\u5206\u5e03\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4f30\u8ba1\u6982\u7387\u5bc6\u5ea6\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHeteroComp\u5728\u7fa4\u4f53\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u4e14\u8ba1\u7b97\u65f6\u95f4\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u6d41\u957f\u5ea6\u3002", "conclusion": "HeteroComp\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u5f20\u91cf\u6d41\uff0c\u51c6\u786e\u68c0\u6d4b\u7fa4\u4f53\u5f02\u5e38\uff0c\u4e3a\u5305\u542b\u5206\u7c7b\u548c\u8fde\u7eed\u5c5e\u6027\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5206\u6790\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2602.05552", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05552", "abs": "https://arxiv.org/abs/2602.05552", "authors": ["Bessie Dominguez-Dager", "Sergio Suescun-Ferrandiz", "Felix Escalona", "Francisco Gomez-Donoso", "Miguel Cazorla"], "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator", "comment": null, "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.", "AI": {"tldr": "VLN-Pilot\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u7684\"\u4eba\u7c7b\u98de\u884c\u5458\"\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u89c6\u89c9\u611f\u77e5\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\uff0c\u65e0\u9700GPS\u6216\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u4f9d\u8d56\u89c4\u5219\u57fa\u7840\u6216\u51e0\u4f55\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5de5\u7a0b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528VLLM\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u8bed\u4e49\u7406\u89e3\u9a71\u52a8\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u51cf\u5c11\u64cd\u4f5c\u5458\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "VLN-Pilot\u6846\u67b6\u8ba9VLLM\u5145\u5f53\u65e0\u4eba\u673a\u98de\u884c\u5458\uff0c\u901a\u8fc7\u89e3\u91ca\u81ea\u7531\u5f62\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5e76\u5c06\u5176\u4e0e\u89c6\u89c9\u89c2\u5bdf\u76f8\u7ed3\u5408\u6765\u89c4\u5212\u548c\u6267\u884c\u8f68\u8ff9\u3002\u6846\u67b6\u6574\u5408\u4e86\u8bed\u8a00\u9a71\u52a8\u7684\u8bed\u4e49\u7406\u89e3\u548c\u89c6\u89c9\u611f\u77e5\uff0c\u652f\u6301\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u3001\u907f\u969c\u548c\u5bf9\u610f\u5916\u4e8b\u4ef6\u7684\u52a8\u6001\u54cd\u5e94\u3002", "result": "\u5728\u5b9a\u5236\u7684\u903c\u771f\u5ba4\u5185\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\uff0c\u5305\u62ec\u5177\u6709\u591a\u4e2a\u8bed\u4e49\u76ee\u6807\u7684\u957f\u671f\u5bfc\u822a\u3002\u7ed3\u679c\u8868\u660e\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u4ee3\u7406\u53ef\u4ee5\u66ff\u4ee3\u8fdc\u7a0b\u65e0\u4eba\u673a\u98de\u884c\u5458\u3002", "conclusion": "VLLM\u4e3a\u57fa\u7840\u7684\u98de\u884c\u5458\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u64cd\u4f5c\u5458\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u5728\u53d7\u9650\u5ba4\u5185\u73af\u5883\u4e2d\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u4efb\u52a1\u7075\u6d3b\u6027\uff0c\u4e3a\u68c0\u67e5\u3001\u641c\u6551\u548c\u8bbe\u65bd\u76d1\u63a7\u7b49\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u3001\u4eba\u6027\u5316\u5ba4\u5185\u65e0\u4eba\u673a\u63a7\u5236\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.05215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05215", "abs": "https://arxiv.org/abs/2602.05215", "authors": ["Jiahao Nie", "Wenbin An", "Gongjie Zhang", "Yicheng Xu", "Yap-Peng Tan", "Alex C. Kot", "Shijian Lu"], "title": "E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching", "comment": null, "summary": "Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.", "AI": {"tldr": "E.M.Ground\uff1a\u4e00\u79cd\u7528\u4e8e\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u7684\u65b0\u578b\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u4ee4\u724c\u3001\u5e73\u6ed1\u6280\u672f\u548c\u591a\u7c92\u5ea6\u7279\u5f81\u805a\u5408\u6765\u63d0\u5347\u4e8b\u4ef6\u611f\u77e5\u7684\u5b8c\u6574\u6027\u548c\u8fde\u7eed\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4e3b\u8981\u4f9d\u8d56\u7cbe\u786e\u65f6\u95f4\u6233\u5339\u914d\u8d77\u59cb\u548c\u7ed3\u675f\u5e27\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4e8b\u4ef6\u7684\u8bed\u4e49\u8fde\u7eed\u6027\u548c\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u6a21\u7cca\u3002", "method": "\u63d0\u51faE.M.Ground\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u7279\u6b8a<event>\u4ee4\u724c\u805a\u5408\u67e5\u8be2\u4e8b\u4ef6\u6240\u6709\u5e27\u7684\u4fe1\u606f\uff1b2) Savitzky-Golay\u5e73\u6ed1\u51cf\u5c11\u65f6\u95f4\u6233\u95f4\u4ee4\u724c-\u5e27\u76f8\u4f3c\u5ea6\u7684\u566a\u58f0\uff1b3) \u591a\u7c92\u5ea6\u5e27\u7279\u5f81\u805a\u5408\u589e\u5f3a\u5339\u914d\u53ef\u9760\u6027\u548c\u65f6\u5e8f\u7406\u89e3\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cE.M.Ground\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "E.M.Ground\u901a\u8fc7\u5173\u6ce8\u6574\u4f53\u548c\u8fde\u8d2f\u7684\u4e8b\u4ef6\u611f\u77e5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u4e2d\u7684\u8bed\u4e49\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4e8b\u4ef6\u5339\u914d\u3002"}}
{"id": "2602.05258", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05258", "abs": "https://arxiv.org/abs/2602.05258", "authors": ["Haoran Li", "Sucheng Ren", "Alan Yuille", "Feng Wang"], "title": "CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs", "comment": null, "summary": "Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.", "AI": {"tldr": "CoPE\u901a\u8fc7\u8f6f\u88c1\u526aRoPE\u7684\u4f4e\u9891\u5206\u91cf\uff0c\u7edf\u4e00\u4e86OOD\u7f13\u89e3\u548c\u8bed\u4e49\u5efa\u6a21\u4e24\u4e2a\u76ee\u6807\uff0c\u5728\u957f\u5ea6\u6cdb\u5316\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u652f\u6301256k\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684RoPE\u6269\u5c55\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u4e24\u7c7b\uff1aOOD\u7f13\u89e3\uff08\u8c03\u6574\u9891\u7387\u4ee5\u9002\u5e94\u672a\u89c1\u4f4d\u7f6e\uff09\u548c\u8bed\u4e49\u5efa\u6a21\uff08\u6ce8\u610f\u529b\u5206\u6570\u5e94\u4f18\u5148\u8bed\u4e49\u76f8\u4f3ctoken\uff09\u3002\u8fd9\u4e24\u79cd\u770b\u4f3c\u4e0d\u540c\u7684\u76ee\u6807\u9700\u8981\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51faCoPE\u65b9\u6cd5\uff1a\u5bf9RoPE\u7684\u4f4e\u9891\u5206\u91cf\u8fdb\u884c\u8f6f\u88c1\u526a\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u6d88\u9664\u4e86OOD\u5f02\u5e38\u503c\u5e76\u4f18\u5316\u4e86\u8bed\u4e49\u4fe1\u53f7\uff0c\u8fd8\u907f\u514d\u4e86\u786c\u88c1\u526a\u5f15\u8d77\u7684\u9891\u8c31\u6cc4\u6f0f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u8f6f\u88c1\u526a\u7b56\u7565\u5e94\u7528\u4e8eRoPE\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u53ef\u6269\u5c55\u5230256k\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u5e76\u786e\u7acb\u4e86CoPE\u5728\u957f\u5ea6\u6cdb\u5316\u4e0a\u7684SOTA\u5730\u4f4d\u3002", "conclusion": "CoPE\u901a\u8fc7\u7b80\u5355\u7684\u8f6f\u88c1\u526a\u5e72\u9884\uff0c\u7edf\u4e00\u4e86OOD\u7f13\u89e3\u548c\u8bed\u4e49\u5efa\u6a21\u4e24\u4e2a\u76ee\u6807\uff0c\u4e3aRoPE\u7684\u957f\u5ea6\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.05683", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05683", "abs": "https://arxiv.org/abs/2602.05683", "authors": ["Chuwei Wang", "Eduardo Sebasti\u00e1n", "Amanda Prorok", "Anastasia Bizyaeva"], "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking", "comment": null, "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5c94\u673a\u5236\u89e3\u51b3\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5bf9\u79f0\u6027\u7834\u7f3a\u95ee\u9898\uff0c\u5c06\u89c6\u89c9\u76ee\u6807\u6fc0\u52b1\u76f4\u63a5\u8f6c\u6362\u4e3a\u8fd0\u52a8\u6307\u4ee4", "motivation": "\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u53cd\u5e94\u5f0f\u4f20\u611f\u5668\u63a7\u5236\u4e0e\u6a21\u578b\u89c4\u5212\u5668\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u5f53\u591a\u4e2a\u76ee\u6807\u9009\u9879\u5747\u7b49\u65f6\u5bb9\u6613\u4ea7\u751f\u51b3\u7b56\u72b9\u8c6b\uff0c\u9700\u8981\u5728\u4e0d\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u89c4\u5212\u5668\u7684\u60c5\u51b5\u4e0b\u6253\u7834\u5bf9\u79f0\u6027", "method": "\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u673a\u8f7d\u6444\u50cf\u5934\u50cf\u7d20\u7f16\u7801\u4e3a\u52a8\u6001\u795e\u7ecf\u5143\u7fa4\u8f93\u5165\uff0c\u76f4\u63a5\u5c06\u89c6\u89c9\u76ee\u6807\u6fc0\u52b1\u8f6c\u6362\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u6307\u4ee4\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5c94\u673a\u5236\u5ef6\u8fdf\u51b3\u7b56\u76f4\u5230\u73af\u5883\u51e0\u4f55\u8bf1\u5bfc\u7684\u4e34\u754c\u70b9", "result": "\u5728\u4eff\u771f\u73af\u5883\u548c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5b9e\u9a8c\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\uff0c\u8ba1\u7b97\u8d1f\u62c5\u5c0f\uff0c\u53c2\u6570\u53ef\u89e3\u91ca\u6027\u5f3a", "conclusion": "\u8be5\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u80fd\u591f\u5f25\u5408\u53cd\u5e94\u5f0f\u63a7\u5236\u4e0e\u6a21\u578b\u89c4\u5212\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u89c6\u89c9\u5f15\u5bfc\u5bfc\u822a\u548c\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u7ea6\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5e94\u7528\u7279\u5b9a\u7684\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\u4e2d"}}
{"id": "2602.05051", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05051", "abs": "https://arxiv.org/abs/2602.05051", "authors": ["Songyuan Zhang", "Oswin So", "H. M. Sabbir Ahmad", "Eric Yang Yu", "Matthew Cleaveland", "Mitchell Black", "Chuchu Fan"], "title": "ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation", "comment": "24 pages, 17 figures; Accepted by the fourteenth International Conference on Learning Representations (ICLR 2026)", "summary": "Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.", "AI": {"tldr": "ReFORM\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u7b56\u7565\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u5f3a\u5236\u6267\u884c\u652f\u6301\u7ea6\u675f\u6765\u907f\u514d\u5206\u5e03\u5916\u9519\u8bef\uff0c\u540c\u65f6\u5728OGBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u5206\u5e03\u5916\u9519\u8bef\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u60e9\u7f5a\u7edf\u8ba1\u8ddd\u79bb\u9879\u6765\u7ea6\u675f\u7b56\u7565\u63a5\u8fd1\u884c\u4e3a\u7b56\u7565\uff0c\u4f46\u8fd9\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u4e14\u4e0d\u80fd\u5b8c\u5168\u9632\u6b62OOD\u52a8\u4f5c\uff1b2\uff09\u6700\u4f18\u7b56\u7565\u5206\u5e03\u53ef\u80fd\u662f\u591a\u6a21\u6001\u4e14\u96be\u4ee5\u8868\u793a\u7684\uff0c\u73b0\u6709\u6269\u6563\u6216\u6d41\u7b56\u7565\u65b9\u6cd5\u4e0d\u6e05\u695a\u5982\u4f55\u5728\u4fdd\u6301\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u907f\u514dOOD\u9519\u8bef\u3002", "method": "ReFORM\u57fa\u4e8e\u6d41\u7b56\u7565\uff0c\u901a\u8fc7\u6784\u9020\u5f3a\u5236\u6267\u884c\u652f\u6301\u7ea6\u675f\u3002\u9996\u5148\u5b66\u4e60\u4e00\u4e2a\u6709\u754c\u6e90\u5206\u5e03\u7684\u884c\u4e3a\u514b\u9686\u6d41\u7b56\u7565\u6765\u6355\u83b7\u52a8\u4f5c\u5206\u5e03\u7684\u652f\u6301\uff0c\u7136\u540e\u4f18\u5316\u4e00\u4e2a\u53cd\u5c04\u6d41\uff0c\u8be5\u6d41\u4e3aBC\u6d41\u751f\u6210\u6709\u754c\u566a\u58f0\u540c\u65f6\u4fdd\u6301\u652f\u6301\uff0c\u4ee5\u6700\u5927\u5316\u6027\u80fd\u3002", "result": "\u5728OGBench\u57fa\u51c6\u6d4b\u8bd5\u768440\u4e2a\u5177\u6709\u4e0d\u540c\u8d28\u91cf\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u6052\u5b9a\u8d85\u53c2\u6570\u96c6\u7684ReFORM\u5728\u6027\u80fd\u66f2\u7ebf\u56fe\u4e0a\u4f18\u4e8e\u6240\u6709\u624b\u52a8\u8c03\u4f18\u8d85\u53c2\u6570\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReFORM\u901a\u8fc7\u6d41\u7b56\u7565\u7684\u6784\u9020\u65b9\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u7684OOD\u9519\u8bef\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.05380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05380", "abs": "https://arxiv.org/abs/2602.05380", "authors": ["Xiaoxuan He", "Siming Fu", "Wanli Li", "Zhiyuan Li", "Dacheng Yin", "Kang Rong", "Fengyun Rao", "Bo Zhang"], "title": "SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback", "comment": null, "summary": "Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \\textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \\textbf{SAIL} (\\textbf{S}elf-\\textbf{A}mplified \\textbf{I}terative \\textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.", "AI": {"tldr": "SAIL\u662f\u4e00\u4e2a\u81ea\u589e\u5f3a\u8fed\u4ee3\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u5bf9\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u81ea\u6211\u751f\u6210\u6837\u672c\u3001\u81ea\u6211\u6807\u6ce8\u504f\u597d\u3001\u81ea\u6211\u4f18\u5316\u7684\u95ed\u73af\u65b9\u5f0f\u5b9e\u73b0\u5bf9\u9f50\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u9762\u4e34\u6311\u6218\uff1a\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u83b7\u53d6\uff0c\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u96c6\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002\u6838\u5fc3\u95ee\u9898\u662f\u80fd\u5426\u4ec5\u7528\u5c11\u91cf\u4eba\u5de5\u53cd\u9988\uff0c\u4e0d\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6316\u6398\u6269\u6563\u6a21\u578b\u81ea\u8eab\u6f5c\u529b\u5b9e\u73b0\u6709\u6548\u5bf9\u9f50\u3002", "method": "\u63d0\u51faSAIL\u6846\u67b6\uff1a\u4ece\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u5bf9\u5f00\u59cb\uff0c\u91c7\u7528\u95ed\u73af\u8fed\u4ee3\u5b66\u4e60\u3002\u6a21\u578b\u9010\u6b65\u751f\u6210\u591a\u6837\u6837\u672c\uff0c\u57fa\u4e8e\u81ea\u8eab\u6f14\u5316\u7406\u89e3\u81ea\u6211\u6807\u6ce8\u504f\u597d\uff0c\u5229\u7528\u81ea\u589e\u5f3a\u6570\u636e\u96c6\u8fdb\u884c\u4f18\u5316\u3002\u5f15\u5165\u6392\u5e8f\u504f\u597d\u6df7\u5408\u7b56\u7565\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u521d\u59cb\u4eba\u7c7b\u5148\u9a8c\uff0c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "SAIL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ec5\u9700\u73b0\u6709\u65b9\u6cd56%\u7684\u504f\u597d\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\u6269\u6563\u6a21\u578b\u5177\u6709\u663e\u8457\u7684\u81ea\u589e\u5f3a\u80fd\u529b\uff0c\u5f53\u88ab\u6070\u5f53\u5229\u7528\u65f6\uff0c\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u548c\u5916\u90e8\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u62e5\u6709\u5f3a\u5927\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\uff0c\u901a\u8fc7SAIL\u6846\u67b6\u53ef\u4ee5\u4ec5\u7528\u5c11\u91cf\u4eba\u5de5\u53cd\u9988\u5b9e\u73b0\u6709\u6548\u5bf9\u9f50\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05863", "categories": ["cs.LG", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05863", "abs": "https://arxiv.org/abs/2602.05863", "authors": ["Roger Girgis", "Rodrigue de Schaetzen", "Luke Rowe", "Azal\u00e9e Robitaille", "Christopher Pal", "Liam Paull"], "title": "Constrained Group Relative Policy Optimization", "comment": "16 pages, 6 figures", "summary": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Constrained GRPO\uff0c\u4e00\u79cd\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7684\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u7ec4\u4ef6\u4f18\u52bf\u4f30\u8ba1\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u91cf\u5316\u4f18\u52bf\u6784\u9020\u6062\u590d\u7ea6\u675f\u63a7\u5236\u3002", "motivation": "\u867d\u7136GRPO\u5df2\u6210\u4e3a\u65e0\u8bc4\u8bba\u8005\u7b56\u7565\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u4f46\u5728\u5177\u6709\u663e\u5f0f\u884c\u4e3a\u7ea6\u675f\u7684\u8bbe\u7f6e\u4e2d\u6269\u5c55\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u7ea6\u675f\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u4efb\u52a1\u548c\u4f9d\u8d56\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5177\u8eabAI\u9886\u57df\u3002", "method": "\u63d0\u51faConstrained GRPO\uff0c\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u7684GRPO\u6269\u5c55\u3002\u4f7f\u7528\u6307\u793a\u5668\u6210\u672c\u51fd\u6570\u6307\u5b9a\u7ea6\u675f\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u76f4\u63a5\u4f18\u5316\u8fdd\u53cd\u7387\u3002\u53d1\u73b0\u591a\u7ec4\u4ef6\u4f18\u52bf\u4f30\u8ba1\u4f1a\u7834\u574f\u7ea6\u675f\u5b66\u4e60\uff0c\u56e0\u6b64\u63d0\u51fa\u6807\u91cf\u5316\u4f18\u52bf\u6784\u9020\u6765\u4fdd\u6301\u5956\u52b1\u548c\u7ea6\u675f\u9879\u4e4b\u95f4\u7684\u9884\u671f\u6743\u8861\u3002", "result": "\u5728\u73a9\u5177\u7f51\u683c\u4e16\u754c\u5b9e\u9a8c\u4e2d\u8bc1\u5b9e\u4e86\u9884\u6d4b\u7684\u4f18\u5316\u75c5\u7406\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u6807\u91cf\u5316\u4f18\u52bf\u80fd\u591f\u6062\u590d\u7a33\u5b9a\u7684\u7ea6\u675f\u63a7\u5236\u3002\u5728\u673a\u5668\u4eba\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cConstrained GRPO\u63d0\u9ad8\u4e86\u7ea6\u675f\u6ee1\u8db3\u5ea6\u540c\u65f6\u589e\u52a0\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "Constrained GRPO\u4e3a\u5177\u8eabAI\u9886\u57df\u7684\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u65e5\u76ca\u4f9d\u8d56\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u573a\u666f\u4e2d\uff0c\u80fd\u591f\u540c\u65f6\u6539\u5584\u7ea6\u675f\u6ee1\u8db3\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.05472", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05472", "abs": "https://arxiv.org/abs/2602.05472", "authors": ["Yiwen Duan", "Jing Ye", "Xinpei Zhao"], "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation", "comment": null, "summary": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.", "AI": {"tldr": "ALIVE\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u6559\u5b66\u6027\u8bed\u8a00\u8bc4\u4f30\uff0c\u8ba9LLM\u5185\u90e8\u5316\u63a8\u7406\u903b\u8f91\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\u5b58\u5728\u6210\u672c\u9ad8\u3001\u9886\u57df\u8106\u5f31\u3001\u65e0\u6cd5\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u5e95\u5c42\u903b\u8f91\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86LLM\u83b7\u5f97\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "method": "ALIVE\u6846\u67b6\u57fa\u4e8e\"\u8ba4\u77e5\u534f\u540c\"\u539f\u5219\uff0c\u5c06\u95ee\u9898\u63d0\u51fa\u3001\u89e3\u51b3\u548c\u8bc4\u5224\u7edf\u4e00\u5728\u5355\u4e00\u7b56\u7565\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u6559\u5b66\u6027\u8bed\u8a00\u53cd\u9988\uff0c\u8ba9\u6a21\u578b\u4ece\u539f\u59cb\u8bed\u6599\u4e2d\u5185\u90e8\u5316\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALIVE\u663e\u8457\u7f13\u89e3\u4e86\u5956\u52b1\u4fe1\u53f7\u9650\u5236\uff0c\u5728\u76f8\u540c\u6570\u636e\u548c\u8ba1\u7b97\u6761\u4ef6\u4e0b\u83b7\u5f97\u51c6\u786e\u7387\u63d0\u5347\u3001\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u589e\u5f3a\u548c\u66f4\u9ad8\u7684\u81ea\u6211\u7ea0\u6b63\u7387\u3002", "conclusion": "ALIVE\u901a\u8fc7\u63a8\u7406\u4e09\u4f4d\u4e00\u4f53\uff08\u95ee\u9898\u63d0\u51fa\u3001\u89e3\u51b3\u3001\u8bc4\u5224\uff09\u4fc3\u8fdb\u4e86\u80fd\u529b\u81ea\u6211\u6301\u7eed\u589e\u957f\uff0c\u4e3a\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u7684\u901a\u7528\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u57fa\u7840\u3002"}}
{"id": "2602.06023", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06023", "abs": "https://arxiv.org/abs/2602.06023", "authors": ["Christopher A. McClurg", "Alan R. Wagner"], "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments", "comment": "Preprint under review for conference publication. 9 pages, 4 figures, 4 tables", "summary": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.", "AI": {"tldr": "\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u5b66\u6821\u5b89\u5168\u5e72\u9884\u63aa\u65bd\uff0c\u7279\u522b\u662f\u673a\u5668\u4eba\u5e72\u9884\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3VR\u7814\u7a76\u4e2d\u9700\u8981\u5927\u91cf\u53c2\u4e0e\u8005\u7684\u9650\u5236\u95ee\u9898\u3002", "motivation": "VR\u867d\u7136\u80fd\u6709\u6548\u8bc4\u4f30\u5b66\u6821\u5b89\u5168\u63aa\u65bd\uff0c\u4f46\u9700\u8981\u4e3a\u6bcf\u4e2a\u6761\u4ef6\u62db\u52df\u65b0\u53c2\u4e0e\u8005\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u6216\u8fed\u4ee3\u8bc4\u4f30\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u573a\u666f\u7684\u5e72\u9884\u7b56\u7565\u5b66\u4e60\u4e2d\u3002", "method": "\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u5668\uff08DES\uff09\uff0c\u5c06\u5c04\u51fb\u8005\u79fb\u52a8\u548c\u533a\u57df\u5185\u884c\u52a8\u5efa\u6a21\u4e3a\u4eceVR\u7814\u7a76\u4e2d\u53c2\u4e0e\u8005\u884c\u4e3a\u5b66\u4e60\u5230\u7684\u968f\u673a\u8fc7\u7a0b\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u5e72\u9884\u7b56\u7565\u3002", "result": "\u6a21\u62df\u5668\u80fd\u591f\u91cd\u73b0\u5173\u952e\u7ecf\u9a8c\u6a21\u5f0f\uff0c\u4f7f\u5f97\u80fd\u591f\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u5b66\u4e60\u90a3\u4e9b\u65e0\u6cd5\u76f4\u63a5\u901a\u8fc7\u4eba\u7c7b\u53d7\u8bd5\u8005\u8bad\u7ec3\u7684\u5e72\u9884\u7b56\u7565\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u4e2a\u9ad8\u5230\u4e2d\u4fdd\u771f\u5ea6\u7684\u6a21\u62df\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u81ea\u4e3b\u5b66\u6821\u5b89\u5168\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.05414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05414", "abs": "https://arxiv.org/abs/2602.05414", "authors": ["Ngoc Doan-Minh Huynh", "Duong Nguyen-Ngoc Tran", "Long Hoang Pham", "Tai Huu-Phuong Tran", "Hyung-Joon Jeon", "Huy-Hung Nguyen", "Duong Khac Vu", "Hyung-Min Jeon", "Son Hong Phan", "Quoc Pham-Nam Ho", "Chi Dai Tran", "Trinh Le Ba Khanh", "Jae Wook Jeon"], "title": "TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions", "comment": "This paper has been accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TSBOW\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u4ea4\u901a\u76d1\u63a7\u4e2d\u906e\u6321\u8f66\u8f86\u68c0\u6d4b\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc732\u5c0f\u65f6\u7684\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u548c\u6570\u767e\u4e07\u6807\u6ce8\u5e27\u3002", "motivation": "\u5168\u7403\u53d8\u6696\u52a0\u5267\u4e86\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\uff0c\u8fd9\u4f1a\u964d\u4f4eCCTV\u4fe1\u53f7\u548c\u89c6\u9891\u8d28\u91cf\uff0c\u540c\u65f6\u6270\u4e71\u4ea4\u901a\u6d41\uff0c\u4ece\u800c\u589e\u52a0\u4ea4\u901a\u4e8b\u6545\u7387\u3002\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u4ec5\u9650\u4e8e\u8f7b\u5ea6\u96fe\u973e\u3001\u96e8\u96ea\u7b49\u6761\u4ef6\uff0c\u65e0\u6cd5\u6355\u6349\u6781\u7aef\u5929\u6c14\u60c5\u51b5\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86TSBOW\uff08Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc732\u5c0f\u65f6\u6765\u81ea\u4eba\u53e3\u5bc6\u96c6\u57ce\u5e02\u5730\u533a\u7684\u771f\u5b9e\u4ea4\u901a\u6570\u636e\uff0c\u5305\u542b48,000\u591a\u4e2a\u4eba\u5de5\u6807\u6ce8\u5e27\u548c320\u4e07\u534a\u6807\u6ce8\u5e27\uff0c\u6db5\u76d68\u79cd\u4ea4\u901a\u53c2\u4e0e\u8005\u7c7b\u522b\u3002", "result": "\u5efa\u7acb\u4e86TSBOW\u7684\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\uff0c\u7a81\u51fa\u4e86\u906e\u6321\u548c\u6076\u52a3\u5929\u6c14\u5e26\u6765\u7684\u6311\u6218\u3002\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u5316\u7684\u9053\u8def\u7c7b\u578b\u3001\u5c3a\u5ea6\u548c\u89c6\u89d2\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002", "conclusion": "TSBOW\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8eCCTV\u7684\u4ea4\u901a\u76d1\u63a7\u6f5c\u529b\uff0c\u4e3a\u65b0\u7684\u7814\u7a76\u548c\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.05533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05533", "abs": "https://arxiv.org/abs/2602.05533", "authors": ["Zhengyi Guo", "Wenpin Tang", "Renyuan Xu"], "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach", "comment": null, "summary": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDoob's h-transform\u7684\u6269\u6563\u6a21\u578b\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6f02\u79fb\u4fee\u6b63\u5b9e\u73b0\u786c\u7ea6\u675f\u6ee1\u8db3\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u5206\u6570\u7f51\u7edc\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u548c\u7f55\u89c1\u4e8b\u4ef6\u6a21\u62df\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u751f\u6210\u6837\u672c\u4ee5\u6982\u73871\u6ee1\u8db3\u786c\u7ea6\u675f\uff0c\u800c\u73b0\u6709\u7684\u8f6f\u7ea6\u675f\u6216\u57fa\u4e8e\u5956\u52b1\u7684\u5f15\u5bfc\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u3002", "method": "\u57fa\u4e8eDoob's h-transform\u3001\u9785\u8868\u793a\u548c\u4e8c\u6b21\u53d8\u5dee\u8fc7\u7a0b\uff0c\u63d0\u51fa\u6761\u4ef6\u6269\u6563\u5f15\u5bfc\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u6dfb\u52a0\u5305\u542b\u6761\u4ef6\u51fd\u6570\u5bf9\u6570\u68af\u5ea6\u7684\u663e\u5f0f\u6f02\u79fb\u4fee\u6b63\uff1b2\uff09\u63d0\u51fa\u57fa\u4e8e\u9785\u635f\u5931\u548c\u9785\u534f\u53d8\u635f\u5931\u7684\u4e24\u79cd\u79bb\u7b56\u7565\u5b66\u4e60\u7b97\u6cd5\u4f30\u8ba1h\u53ca\u5176\u68af\u5ea6\u3002", "result": "\u5728\u603b\u53d8\u5dee\u548cWasserstein\u8ddd\u79bb\u4e0a\u63d0\u4f9b\u4e86\u975e\u6e10\u8fd1\u7406\u8bba\u4fdd\u8bc1\uff0c\u660e\u786e\u523b\u753b\u4e86\u5206\u6570\u8fd1\u4f3c\u548c\u5f15\u5bfc\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\u548c\u751f\u6210\u7f55\u89c1\u4e8b\u4ef6\u6837\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u751f\u6210\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u7684\u786c\u7ea6\u675f\u6ee1\u8db3\u65b9\u6cd5\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u548c\u7f55\u89c1\u4e8b\u4ef6\u6a21\u62df\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.05423", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.05423", "abs": "https://arxiv.org/abs/2602.05423", "authors": ["Pengcheng Chen", "Yue Hu", "Wenhao Li", "Nicole M Gunderson", "Andrew Feng", "Zhenglong Sun", "Peter Beerel", "Eric J Seibel"], "title": "NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks", "comment": null, "summary": "In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.", "AI": {"tldr": "NeVStereo\uff1a\u4e00\u4e2a\u7ed3\u5408NeRF\u6e32\u67d3\u4e0e\u7acb\u4f53\u89c6\u89c9\u7684\u8054\u5408\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u591a\u89c6\u89d2RGB\u8f93\u5165\u4e2d\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\u3001\u591a\u89c6\u89d2\u6df1\u5ea6\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8868\u9762\u91cd\u5efa\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u524d\u9988\u7cfb\u7edf\uff08\u5982VGGT\u3001pi3\uff09\u4e13\u6ce8\u4e8e\u7aef\u5230\u7aef\u5339\u914d\u548c\u51e0\u4f55\u9884\u6d4b\u4f46\u4e0d\u8f93\u51fa\u65b0\u89c6\u89d2\u5408\u6210\uff1b\u57fa\u4e8e\u795e\u7ecf\u6e32\u67d3\u7684\u65b9\u6cd5\uff08\u5982NeRF\uff09\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u8be6\u7ec6\u51e0\u4f55\uff0c\u4f46\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u76f8\u673a\u4f4d\u59ff\u4e14\u5bf9\u4f4d\u59ff\u8bef\u5dee\u654f\u611f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4ece\u968f\u610f\u62cd\u6444\u7684\u89c6\u56fe\u4e2d\u540c\u65f6\u83b7\u5f97\u51c6\u786e\u4f4d\u59ff\u3001\u53ef\u9760\u6df1\u5ea6\u3001\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u7cbe\u786e3D\u8868\u9762\u3002", "method": "NeVStereo\u7ed3\u5408\u4e86NeRF\u9a71\u52a8\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u7acb\u4f53\u89c6\u89c9\u67b6\u6784\uff1a1\uff09\u4f7f\u7528NeRF-based NVS\u751f\u6210\u9002\u5408\u7acb\u4f53\u5339\u914d\u7684\u6e32\u67d3\uff1b2\uff09\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u6df1\u5ea6\u4f30\u8ba1\uff1b3\uff09NeRF\u8026\u5408\u7684\u675f\u8c03\u6574\u8fdb\u884c\u4f4d\u59ff\u4f18\u5316\uff1b4\uff09\u8fed\u4ee3\u4f18\u5316\u9636\u6bb5\u540c\u65f6\u66f4\u65b0\u6df1\u5ea6\u548c\u8f90\u5c04\u573a\u4ee5\u63d0\u9ad8\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u8be5\u8bbe\u8ba1\u7f13\u89e3\u4e86NeRF\u5e38\u89c1\u95ee\u9898\u5982\u8868\u9762\u5806\u53e0\u3001\u4f2a\u5f71\u548c\u4f4d\u59ff-\u6df1\u5ea6\u8026\u5408\u3002", "result": "\u5728\u5ba4\u5185\u3001\u5ba4\u5916\u3001\u684c\u9762\u548c\u822a\u7a7a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeVStereo\u5b9e\u73b0\u4e86\u6301\u7eed\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff1a\u6df1\u5ea6\u8bef\u5dee\u964d\u4f4e\u8fbe36%\uff0c\u4f4d\u59ff\u7cbe\u5ea6\u63d0\u534710.4%\uff0cNVS\u4fdd\u771f\u5ea6\u63d0\u9ad84.5%\uff0c\u7f51\u683c\u8d28\u91cf\u8fbe\u5230SOTA\uff08F1 91.93%\uff0cChamfer 4.35 mm\uff09\u3002", "conclusion": "NeVStereo\u6210\u529f\u5730\u5c06NeRF\u6e32\u67d3\u4e0e\u7acb\u4f53\u89c6\u89c9\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u591a\u89c6\u89d2RGB\u8f93\u5165\u4e2d\u8054\u5408\u8f93\u51fa\u76f8\u673a\u4f4d\u59ff\u3001\u591a\u89c6\u89d2\u6df1\u5ea6\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8868\u9762\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.05060", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05060", "abs": "https://arxiv.org/abs/2602.05060", "authors": ["Heajun An", "Qi Zhang", "Minqian Liu", "Xinyi Zhang", "Sang Won Lee", "Lifu Huang", "Pamela J. Wisniewski", "Jin-Hee Cho"], "title": "StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation", "comment": null, "summary": "Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.", "AI": {"tldr": "StagePilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u4ee3\u7406\uff0c\u7528\u4e8e\u6a21\u62df\u7f51\u7edc\u8bf1\u9a97\u7684\u9636\u6bb5\u8fdb\u5c55\uff0c\u901a\u8fc7\u5e73\u8861\u7528\u6237\u60c5\u611f\u548c\u76ee\u6807\u63a5\u8fd1\u5ea6\u7684\u590d\u5408\u5956\u52b1\u9009\u62e9\u5bf9\u8bdd\u9636\u6bb5\uff0c\u5728LLM\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7f51\u7edc\u8bf1\u9a97\u662f\u5bf9\u9752\u5c11\u5e74\u7684\u6301\u7eed\u5a01\u80c1\uff0c\u9700\u8981\u4e3b\u52a8\u7684\u6559\u80b2\u5e72\u9884\u63aa\u65bd\u6765\u9884\u9632\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8bf1\u9a97\u884c\u4e3a\u9636\u6bb5\u8fdb\u5c55\u7684\u6a21\u62df\uff0c\u96be\u4ee5\u63d0\u4f9b\u6709\u6548\u7684\u9884\u9632\u8bad\u7ec3\u3002", "method": "\u63d0\u51faStagePilot\u79bb\u7ebfRL\u5bf9\u8bdd\u4ee3\u7406\uff0c\u4f7f\u7528\u590d\u5408\u5956\u52b1\u5e73\u8861\u7528\u6237\u60c5\u611f\u548c\u76ee\u6807\u63a5\u8fd1\u5ea6\u6765\u9009\u62e9\u5bf9\u8bdd\u9636\u6bb5\uff0c\u9636\u6bb5\u8f6c\u6362\u9650\u5236\u5728\u76f8\u90bb\u9636\u6bb5\u4ee5\u786e\u4fdd\u771f\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7LLM\u6a21\u62df\u8bc4\u4f30\u5bf9\u8bdd\u8d28\u91cf\u3002", "result": "StagePilot\u80fd\u751f\u6210\u771f\u5b9e\u4e14\u8fde\u8d2f\u7684\u5bf9\u8bdd\uff0c\u4e0e\u8bf1\u9a97\u52a8\u6001\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u6d4b\u8bd5\u65b9\u6cd5\u4e2d\uff0cIQL+AWAC\u4ee3\u7406\u5728\u6218\u7565\u89c4\u5212\u548c\u60c5\u611f\u8fde\u8d2f\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff0c\u6700\u7ec8\u9636\u6bb5\u5230\u8fbe\u9891\u7387\u6bd4\u57fa\u7ebf\u9ad843%\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc770%\u7684\u60c5\u611f\u5bf9\u9f50\u3002", "conclusion": "StagePilot\u4e3a\u7f51\u7edc\u8bf1\u9a97\u9884\u9632\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5bf9\u8bdd\u6a21\u62df\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u7b26\u5408\u8bf1\u9a97\u9636\u6bb5\u8fdb\u5c55\u7684\u771f\u5b9e\u5bf9\u8bdd\uff0c\u5728\u6218\u7565\u89c4\u5212\u548c\u60c5\u611f\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.05435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05435", "abs": "https://arxiv.org/abs/2602.05435", "authors": ["Donglin Yang", "Yongxing Zhang", "Xin Yu", "Liang Hou", "Xin Tao", "Pengfei Wan", "Xiaojuan Qi", "Renjie Liao"], "title": "Stable Velocity: A Variance Perspective on Flow Matching", "comment": null, "summary": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.", "AI": {"tldr": "\u63d0\u51faStable Velocity\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u5dee\u5206\u6790\u6539\u8fdb\u6d41\u5339\u914d\u8bad\u7ec3\u548c\u91c7\u6837\uff0c\u5728\u4f4e\u65b9\u5dee\u533a\u57df\u5b9e\u73b02\u500d\u4ee5\u4e0a\u52a0\u901f\u4e14\u4e0d\u635f\u5931\u8d28\u91cf", "motivation": "\u4f20\u7edf\u6d41\u5339\u914d\u4f9d\u8d56\u5355\u6837\u672c\u6761\u4ef6\u901f\u5ea6\uff0c\u5bfc\u81f4\u9ad8\u65b9\u5dee\u8bad\u7ec3\u76ee\u6807\uff0c\u4f7f\u4f18\u5316\u4e0d\u7a33\u5b9a\u4e14\u6536\u655b\u7f13\u6162\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u65b9\u5dee\u95ee\u9898\u6765\u6539\u8fdb\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u901f\u5ea6\u3002", "method": "1) \u5206\u6790\u6761\u4ef6\u901f\u5ea6\u65b9\u5dee\uff0c\u8bc6\u522b\u9ad8\u65b9\u5dee\uff08\u5148\u9a8c\u9644\u8fd1\uff09\u548c\u4f4e\u65b9\u5dee\uff08\u6570\u636e\u5206\u5e03\u9644\u8fd1\uff09\u533a\u57df\uff1b2) \u63d0\u51faStable Velocity Matching (StableVM)\u65e0\u504f\u65b9\u5dee\u51cf\u5c11\u76ee\u6807\uff1b3) \u63d0\u51faVariance-Aware Representation Alignment (VA-REPA)\u5728\u4f4e\u65b9\u5dee\u533a\u57df\u81ea\u9002\u5e94\u589e\u5f3a\u8f85\u52a9\u76d1\u7763\uff1b4) \u63d0\u51faStable Velocity Sampling (StableVS)\u5229\u7528\u4f4e\u65b9\u5dee\u533a\u57df\u95ed\u5f0f\u7b80\u5316\u5b9e\u73b0\u514d\u5fae\u8c03\u52a0\u901f", "result": "\u5728ImageNet 256\u00d7256\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u6a21\u578b\uff08SD3.5\u3001Flux\u3001Qwen-Image\u3001Wan2.2\uff09\u4e0a\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u5347\uff0c\u4f4e\u65b9\u5dee\u533a\u57df\u91c7\u6837\u901f\u5ea6\u63d0\u9ad82\u500d\u4ee5\u4e0a\u4e14\u4e0d\u964d\u4f4e\u6837\u672c\u8d28\u91cf", "conclusion": "\u901a\u8fc7\u65b9\u5dee\u5206\u6790\u63d0\u51fa\u7684Stable Velocity\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u7684\u65b9\u5dee\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.05636", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05636", "abs": "https://arxiv.org/abs/2602.05636", "authors": ["Benny Cheung"], "title": "Generative Ontology: When Structured Knowledge Learns to Create", "comment": "15 pages, 6 figures, 6 tables. Code available at https://github.com/bennycheung/GameGrammarCLI", "summary": "Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.\n  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional \"anxiety\" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.\n  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (\"bioluminescent fungi competing in a cave ecosystem\"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.\n  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.", "AI": {"tldr": "\u63d0\u51faGenerative Ontology\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edf\u672c\u4f53\u8bba\u7684\u7ed3\u6784\u4e25\u8c28\u6027\u548cLLM\u7684\u521b\u9020\u529b\uff0c\u901a\u8fc7\u53ef\u6267\u884c\u7684Pydantic\u6a21\u5f0f\u7ea6\u675fLLM\u751f\u6210\uff0c\u5e94\u7528\u4e8e\u6e38\u620f\u8bbe\u8ba1\u7b49\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u672c\u4f53\u8bba\u80fd\u63cf\u8ff0\u9886\u57df\u7ed3\u6784\u4f46\u65e0\u6cd5\u751f\u6210\u65b0\u5185\u5bb9\uff0c\u800cLLM\u80fd\u6d41\u7545\u751f\u6210\u4f46\u7f3a\u4e4f\u7ed3\u6784\u6709\u6548\u6027\uff0c\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff1a\u672c\u4f53\u8bba\u63d0\u4f9b\u8bed\u6cd5\uff0cLLM\u63d0\u4f9b\u521b\u9020\u529b\u3002", "method": "\u5c06\u9886\u57df\u77e5\u8bc6\u7f16\u7801\u4e3a\u53ef\u6267\u884c\u7684Pydantic\u6a21\u5f0f\uff0c\u901a\u8fc7DSPy\u7b7e\u540d\u7ea6\u675fLLM\u751f\u6210\u3002\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u4e3a\u4e0d\u540c\u672c\u4f53\u9886\u57df\u5206\u914d\u4e13\u95e8\u89d2\u8272\uff08\u5982\u673a\u5236\u67b6\u6784\u5e08\u3001\u4e3b\u9898\u7f16\u7ec7\u8005\u3001\u5e73\u8861\u6279\u8bc4\u5bb6\uff09\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5e26\u6709\u4e13\u4e1a\"\u7126\u8651\"\u9632\u6b62\u6d45\u8584\u8f93\u51fa\u3002\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u8fed\u4ee3\u9a8c\u8bc1\u786e\u4fdd\u673a\u5236\u4e0e\u7ec4\u4ef6\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u5f00\u53d1\u4e86GameGrammar\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u4e3b\u9898\u63d0\u793a\uff08\u5982\"\u6d1e\u7a74\u751f\u6001\u7cfb\u7edf\u4e2d\u53d1\u5149\u771f\u83cc\u7ade\u4e89\"\uff09\u751f\u6210\u7ed3\u6784\u5b8c\u6574\u3001\u53ef\u73a9\u7684\u684c\u9762\u6e38\u620f\u8bbe\u8ba1\uff0c\u5305\u62ec\u673a\u5236\u3001\u7ec4\u4ef6\u3001\u80dc\u5229\u6761\u4ef6\u548c\u8bbe\u7f6e\u8bf4\u660e\uff0c\u65e2\u6ee1\u8db3\u672c\u4f53\u7ea6\u675f\u53c8\u4fdd\u6301\u521b\u9020\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u63a8\u5e7f\u5230\u6e38\u620f\u4ee5\u5916\u7684\u9886\u57df\uff0c\u4efb\u4f55\u5177\u6709\u4e13\u5bb6\u8bcd\u6c47\u3001\u6709\u6548\u6027\u7ea6\u675f\u548c\u79ef\u7d2f\u8303\u4f8b\u7684\u9886\u57df\uff08\u5982\u97f3\u4e50\u521b\u4f5c\u3001\u8f6f\u4ef6\u67b6\u6784\u3001\u70f9\u996a\u827a\u672f\uff09\u90fd\u9002\u5408\u4f7f\u7528Generative Ontology\u3002\u7ea6\u675f\u4e0d\u662f\u9650\u5236\u521b\u9020\u529b\uff0c\u800c\u662f\u4f7f\u5176\u6210\u4e3a\u53ef\u80fd\uff0c\u6b63\u5982\u8bed\u6cd5\u4f7f\u8bd7\u6b4c\u6210\u4e3a\u53ef\u80fd\u4e00\u6837\u3002"}}
{"id": "2602.05454", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05454", "abs": "https://arxiv.org/abs/2602.05454", "authors": ["Yue Lu", "Xiangyu Zhou", "Shizhou Zhang", "Yinghui Xing", "Guoqiang Liang", "Wencong Zhang"], "title": "Attention Retention for Continual Learning with Vision Transformers", "comment": "AAAI-2026 Camera Ready", "summary": "Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u4fdd\u6301\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u63a9\u7801\u9632\u6b62Vision Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u6f02\u79fb\uff0c\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u662f\u6838\u5fc3\u6311\u6218\uff0c\u7814\u7a76\u53d1\u73b0Vision Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u6f02\u79fb\u662f\u5bfc\u81f4\u9057\u5fd8\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u53d7\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u9009\u62e9\u6027\u6ce8\u610f\u529b\u673a\u5236\u542f\u53d1\uff0c\u9700\u8981\u9632\u6b62\u6ce8\u610f\u529b\u4ece\u5df2\u5b66\u6982\u5ff5\u6f02\u79fb", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u4fdd\u6301\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5c42\u95f4rollout\u673a\u5236\u63d0\u53d6\u524d\u4e00\u4efb\u52a1\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u751f\u6210\u5b9e\u4f8b\u81ea\u9002\u5e94\u4e8c\u8fdb\u5236\u63a9\u7801\uff1b2\uff09\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\uff0c\u5e94\u7528\u63a9\u7801\u5c06\u524d\u4e00\u4efb\u52a1\u6ce8\u610f\u529b\u533a\u57df\u7684\u68af\u5ea6\u7f6e\u96f6\uff0c\u9632\u6b62\u5df2\u5b66\u89c6\u89c9\u6982\u5ff5\u88ab\u7834\u574f\uff1b\u4e3a\u517c\u5bb9\u73b0\u4ee3\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6309\u6bd4\u4f8b\u7f29\u653e\u53c2\u6570\u66f4\u65b0\u6765\u4fdd\u6301\u76f8\u5bf9\u5e45\u5ea6", "result": "\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u5e76\u4fdd\u6301\u89c6\u89c9\u6982\u5ff5\uff0c\u5728\u591a\u79cd\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u5e76\u5c55\u73b0\u9c81\u68d2\u6cdb\u5316\u80fd\u529b", "conclusion": "\u6ce8\u610f\u529b\u6f02\u79fb\u662fVision Transformer\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u4fdd\u6301\u6846\u67b6\u901a\u8fc7\u68af\u5ea6\u63a9\u7801\u673a\u5236\u6210\u529f\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.05125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05125", "abs": "https://arxiv.org/abs/2602.05125", "authors": ["William F. Shen", "Xinchi Qiu", "Chenxi Whitehouse", "Lisa Alazraki", "Shashwat Goel", "Francesco Barbieri", "Timon Willi", "Akhil Mathur", "Ilias Leontiadis"], "title": "Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks", "comment": null, "summary": "Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.", "AI": {"tldr": "\u63d0\u51faRRD\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3-\u8fc7\u6ee4\u5faa\u73af\u4f18\u5316\u8bc4\u5206\u6807\u51c6\uff0c\u63d0\u5347LLM\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4fe1\u53f7", "motivation": "\u73b0\u6709\u8bc4\u5206\u6807\u51c6\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u63a7\u5236\uff0c\u5b58\u5728\u8986\u76d6\u4e0d\u5168\u3001\u7ef4\u5ea6\u6df7\u6dc6\u3001\u504f\u597d\u65b9\u5411\u9519\u4f4d\u3001\u5197\u4f59\u548c\u9ad8\u5ea6\u76f8\u5173\u6807\u51c6\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc4\u4f30\u51c6\u786e\u6027\u4e0b\u964d\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73", "method": "\u63d0\u51faRRD\u6846\u67b6\uff0c\u91c7\u7528\u9012\u5f52\u5206\u89e3-\u8fc7\u6ee4\u5faa\u73af\uff1a1) \u5c06\u7c97\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u3001\u53ef\u533a\u5206\u7684\u6807\u51c6\uff1b2) \u8fc7\u6ee4\u673a\u5236\u79fb\u9664\u9519\u4f4d\u548c\u5197\u4f59\u6807\u51c6\uff1b3) \u76f8\u5173\u6027\u611f\u77e5\u52a0\u6743\u65b9\u6848\u9632\u6b62\u9ad8\u5ea6\u76f8\u5173\u6807\u51c6\u8fc7\u5ea6\u8868\u793a", "result": "\u5728\u8bc4\u4f30\u65b9\u9762\uff1a\u5728JudgeBench\u548cPPE\u4e0a\u663e\u8457\u63d0\u5347GPT-4o\u548cLlama3.1-405B\u7684\u504f\u597d\u5224\u65ad\u51c6\u786e\u6027\uff0c\u5728JudgeBench\u4e0a\u6700\u9ad8\u63d0\u534717.7\u5206\uff1b\u5728\u8bad\u7ec3\u65b9\u9762\uff1a\u4f5c\u4e3aWildChat\u4e0aRFT\u7684\u5956\u52b1\u6e90\uff0cQwen3-4B\u5956\u52b1\u63d0\u5347160%\uff0cLlama3.1-8B\u63d0\u534760%\uff0c\u8fdc\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u8fc1\u79fb\u5230\u5176\u4ed6\u57fa\u51c6", "conclusion": "RRD\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u9012\u5f52\u8bc4\u5206\u6807\u51c6\u4f18\u5316\u57fa\u7840\uff0c\u4e3a\u5f00\u653e\u9886\u57dfLLM\u8bc4\u4f30\u548c\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.05717", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05717", "abs": "https://arxiv.org/abs/2602.05717", "authors": ["Tianyi Wang", "Long Li", "Hongcan Guo", "Yibiao Chen", "Yixia Li", "Yong Wang", "Yun Chen", "Guanhua Chen"], "title": "Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification", "comment": "17 pages, 6 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAnchored Policy Optimization (APO)\u65b9\u6cd5\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5956\u52b1\u5bfc\u81f4\u7684\u9012\u5f52\u7a7a\u95f4\u6536\u7f29\u95ee\u9898\uff0c\u901a\u8fc7\u4ece\u5168\u5c40\u5f62\u72b6\u5339\u914d\u8f6c\u5411\u652f\u6301\u8986\u76d6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u6062\u590d\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\u5b58\u5728\u9012\u5f52\u7a7a\u95f4\u6536\u7f29\u95ee\u9898\uff0c\u5bfc\u81f4\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u7684\u91c7\u6837\u6982\u7387\u6d88\u5931\u3002KL\u6b63\u5219\u5316\u867d\u7136\u8bd5\u56fe\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5f3a\u52a0\u4e86\u50f5\u5316\u7684\u5f62\u72b6\u5339\u914d\u7ea6\u675f\uff0c\u4e0e\u6b63\u786e\u6027\u6240\u9700\u7684\u9510\u5316\u4ea7\u751f\u68af\u5ea6\u51b2\u7a81\u3002", "method": "\u63d0\u51faAnchored Policy Optimization (APO)\uff0c\u5c06\u8303\u5f0f\u4ece\u5168\u5c40\u5f62\u72b6\u5339\u914d\u8f6c\u5411\u652f\u6301\u8986\u76d6\u3002\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u9ad8\u7f6e\u4fe1\u5ea6\u652f\u6301\u5b9a\u4e49\u5b89\u5168\u6d41\u5f62\uff0c\u5141\u8bb8\u6fc0\u8fdb\u9510\u5316\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u5728\u9519\u8bef\u6821\u6b63\u65f6\u9009\u62e9\u6027\u8c03\u7528\u6062\u590d\u529b\u4ee5\u9632\u6b62\u5d29\u6e83\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPO\u6253\u7834\u4e86\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u7684\u6743\u8861\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Pass@1\u6027\u80fd\uff0c\u540c\u65f6\u6062\u590d\u4e86\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u901a\u5e38\u4e22\u5931\u7684Pass@K\u591a\u6837\u6027\u3002", "conclusion": "APO\u4f5c\u4e3a\u4e00\u79cd\u68af\u5ea6\u5bf9\u9f50\u673a\u5236\uff0c\u80fd\u591f\u6700\u5927\u5316\u652f\u6301\u8986\u76d6\uff0c\u5b9e\u73b0\u5f39\u6027\u6062\u590d\uff0c\u91cd\u65b0\u81a8\u80c0\u6709\u6548\u5206\u652f\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u4e2d\u7684\u9012\u5f52\u7a7a\u95f4\u6536\u7f29\u95ee\u9898\u3002"}}
{"id": "2602.05932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05932", "abs": "https://arxiv.org/abs/2602.05932", "authors": ["L\u00e9o Labat", "Etienne Ollion", "Fran\u00e7ois Yvon"], "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions", "comment": "17 pages, 5 figures (8 pages of references and appendices)", "summary": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u89c2\u9009\u62e9\u9898\u4e0a\u7684\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u5927\u578b\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u6574\u4f53\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u4f46\u67d0\u4e9b\u95ee\u9898\u4ecd\u5b58\u5728\u8bed\u8a00\u7279\u5f02\u6027\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u89c2\u9009\u62e9\u9898\u4e0a\u7684\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u63a2\u7a76\u6a21\u578b\u662f\u50cf\u7406\u8bba\u4e0a\u7684\u591a\u8bed\u8005\u90a3\u6837\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u8fd8\u662f\u50cf\u591a\u4e2a\u5355\u8bed\u6a21\u578b\u90a3\u6837\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4ef7\u503c\u89c2\u3002", "method": "\u521b\u5efa\u4e86\u591a\u8bed\u8a00\u6b27\u6d32\u4ef7\u503c\u89c2\u8c03\u67e5\uff08MEVS\uff09\u8bed\u6599\u5e93\uff0c\u5305\u542b8\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u4eba\u5de5\u7ffb\u8bd1\u8c03\u67e5\u95ee\u9898\u3002\u5bf930\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u3001\u5236\u9020\u5546\u548c\u5fae\u8c03\u72b6\u6001\u7684\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u63a7\u5236\u63d0\u793a\u53d8\u4f53\u5305\u62ec\u7b54\u6848\u987a\u5e8f\u3001\u7b26\u53f7\u7c7b\u578b\u548c\u5c3e\u90e8\u5b57\u7b26\u3002", "result": "\u5927\u578b\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u6574\u4f53\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u4f46\u54cd\u5e94\u7a33\u5065\u6027\u5728\u4e0d\u540c\u95ee\u9898\u95f4\u5dee\u5f02\u5f88\u5927\u3002\u67d0\u4e9b\u95ee\u9898\u5728\u6240\u6709\u6a21\u578b\u5185\u90e8\u548c\u8de8\u6a21\u578b\u95f4\u5b8c\u5168\u4e00\u81f4\uff0c\u800c\u53e6\u4e00\u4e9b\u95ee\u9898\u5219\u5bfc\u81f4\u6a21\u578b\u7b54\u6848\u5206\u88c2\u3002\u6240\u6709\u4e00\u81f4\u7684\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u67d0\u4e9b\u95ee\u9898\u4e0a\u90fd\u8868\u73b0\u51fa\u8bed\u8a00\u7279\u5f02\u6027\u884c\u4e3a\u3002", "conclusion": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u89c2\u9009\u62e9\u9898\u4e0a\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\u7684\u591a\u8bed\u8005\uff0c\u800c\u662f\u5728\u67d0\u4e9b\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8bed\u8a00\u7279\u5f02\u6027\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u504f\u597d\u5fae\u8c03\u7684\u9009\u62e9\u6027\u6548\u5e94\uff0c\u4ee5\u53ca\u8bed\u8a00\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u4ef7\u503c\u8868\u8fbe\u3002"}}
{"id": "2602.05723", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05723", "abs": "https://arxiv.org/abs/2602.05723", "authors": ["Taoye Yin", "Haoyuan Hu", "Yaxin Fan", "Xinhao Chen", "Xinya Wu", "Kai Deng", "Kezun Zhang", "Feng Wang"], "title": "Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification", "comment": "accepted by ICASSP 2026", "summary": "In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51faRLFKV\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u9a8c\u8bc1\u548c\u5f3a\u5316\u5b66\u4e60\u51cf\u5c11\u91d1\u878dRAG\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u91d1\u878dRAG\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u4f9d\u8d56\u68c0\u7d22\u6587\u6863\u751f\u6210\u51c6\u786e\u56de\u7b54\uff0c\u4f46\u7531\u4e8e\u91d1\u878d\u9886\u57df\u7684\u65f6\u95f4\u654f\u611f\u6027\uff0c\u751f\u6210\u7684\u56de\u7b54\u4ecd\u5b58\u5728\u4e0e\u68c0\u7d22\u4fe1\u606f\u77db\u76fe\u7684\u5e7b\u89c9\u95ee\u9898", "method": "\u63d0\u51faRLFKV\u6846\u67b6\uff1a1) \u5c06\u91d1\u878d\u56de\u7b54\u5206\u89e3\u4e3a\u539f\u5b50\u77e5\u8bc6\u5355\u5143\uff1b2) \u8bc4\u4f30\u6bcf\u4e2a\u5355\u5143\u7684\u6b63\u786e\u6027\u8ba1\u7b97\u7ec6\u7c92\u5ea6\u5fe0\u5b9e\u5ea6\u5956\u52b1\uff1b3) \u52a0\u5165\u4fe1\u606f\u6027\u5956\u52b1\u9632\u6b62\u5956\u52b1\u653b\u51fb\uff1b4) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b", "result": "\u5728\u516c\u5f00\u7684FDD\u4efb\u52a1\u548c\u65b0\u63d0\u51fa\u7684FDD-ANT\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u5747\u663e\u793a\u4e00\u81f4\u6027\u6539\u8fdb\uff0c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "RLFKV\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u9a8c\u8bc1\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u91d1\u878dRAG\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u56de\u7b54\u4e0e\u68c0\u7d22\u6587\u6863\u7684\u4e00\u81f4\u6027"}}
{"id": "2602.05146", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05146", "abs": "https://arxiv.org/abs/2602.05146", "authors": ["Wonjun Yi", "Rismaya Kumar Mishra", "Yong-Hwa Park"], "title": "Cross-talk based multi-task learning for fault classification of physically coupled machine system", "comment": "Submitted to 32th International Congress on Sound and Vibration (ICSV32)", "summary": "Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u5bf9\u8bdd\u7ed3\u6784\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6545\u969c\u6761\u4ef6\u548c\u76f8\u5173\u7269\u7406\u53d8\u91cf\u6765\u63d0\u5347\u6545\u969c\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u7cfb\u7edf\u4ea7\u751f\u7684\u4fe1\u53f7\u4e2d\uff0c\u6545\u969c\u6761\u4ef6\u4e0e\u5404\u79cd\u7269\u7406\u53d8\u91cf\u4e4b\u95f4\u5b58\u5728\u7269\u7406\u8026\u5408\u3002\u73b0\u6709\u6545\u969c\u5206\u7c7b\u7814\u7a76\u5927\u591a\u53ea\u4f9d\u8d56\u76f4\u63a5\u6545\u969c\u6807\u7b7e\uff0c\u5ffd\u7565\u4e86\u4fe1\u53f7\u4e2d\u81ea\u7136\u5d4c\u5165\u7684\u5176\u4ed6\u7269\u7406\u8026\u5408\u4fe1\u606f\u3002\u5229\u7528\u8fd9\u79cd\u8026\u5408\u5173\u7cfb\u53ef\u4ee5\u63d0\u5347\u6545\u969c\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u662f\u4ea4\u53c9\u5bf9\u8bdd\u7ed3\u6784\uff0c\u5728\u4e4b\u524d\u63d0\u51fa\u7684\u6b8b\u5dee\u795e\u7ecf\u964d\u7ef4\u6a21\u578b\u57fa\u7840\u4e0a\u6269\u5c55\u5e94\u7528\u3002\u4ea4\u53c9\u5bf9\u8bdd\u7ed3\u6784\u5141\u8bb8\u4efb\u52a1\u95f4\u901a\u8fc7\u4ea4\u53c9\u5bf9\u8bdd\u5c42\u8fdb\u884c\u53d7\u63a7\u4fe1\u606f\u4ea4\u6362\uff0c\u540c\u65f6\u9632\u6b62\u8d1f\u8fc1\u79fb\u3002\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u65e0\u4eba\u673a\u6545\u969c\u6570\u636e\u96c6\u548c\u7535\u673a\u590d\u5408\u6545\u969c\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6b8b\u5dee\u795e\u7ecf\u964d\u7ef4\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5355\u4efb\u52a1\u6a21\u578b\u3001\u5408\u5e76\u6240\u6709\u6807\u7b7e\u7ec4\u5408\u7684\u591a\u7c7b\u6a21\u578b\u4ee5\u53ca\u5171\u4eab\u4e3b\u5e72\u591a\u4efb\u52a1\u6a21\u578b\u3002\u4ea4\u53c9\u5bf9\u8bdd\u67b6\u6784\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u6545\u969c\u5206\u7c7b\u548c\u7269\u7406\u5c5e\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5206\u7c7b\u6545\u969c\u3002", "conclusion": "\u5229\u7528\u4fe1\u53f7\u4e2d\u81ea\u7136\u5b58\u5728\u7684\u7269\u7406\u8026\u5408\u4fe1\u606f\uff0c\u901a\u8fc7\u4ea4\u53c9\u5bf9\u8bdd\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6545\u969c\u5206\u7c7b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u6545\u969c\u548c\u7535\u673a\u590d\u5408\u6545\u969c\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2602.05204", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05204", "abs": "https://arxiv.org/abs/2602.05204", "authors": ["Changhoon Song", "Teng Yuan Chang", "Youngjoon Hong"], "title": "Extreme Weather Nowcasting via Local Precipitation Pattern Prediction", "comment": "10pages, 20 figures, The Fourteenth International Conference on Learning Representations, see https://github.com/tony890048/exPreCast", "summary": "Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.", "AI": {"tldr": "\u63d0\u51faexPreCast\u6846\u67b6\u548c\u5e73\u8861\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u7cbe\u7ec6\u7684\u96f7\u8fbe\u9884\u62a5\uff0c\u5728\u666e\u901a\u548c\u6781\u7aef\u964d\u96e8\u60c5\u51b5\u4e0b\u5747\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u6781\u7aef\u5929\u6c14\u51c6\u786e\u9884\u62a5\u5bf9\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u6602\u8d35\u4e0d\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff0c\u786e\u5b9a\u6027\u6a21\u578b\u504f\u5411\u666e\u901a\u964d\u96e8\uff0c\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "\u63d0\u51faexPreCast\u786e\u5b9a\u6027\u6846\u67b6\uff0c\u96c6\u6210\u5c40\u90e8\u65f6\u7a7a\u6ce8\u610f\u529b\u3001\u7eb9\u7406\u4fdd\u6301\u7acb\u65b9\u53cc\u4e0a\u91c7\u6837\u89e3\u7801\u5668\u548c\u65f6\u95f4\u63d0\u53d6\u5668\uff0c\u53ef\u7075\u6d3b\u8c03\u6574\u9884\u62a5\u65f6\u57df\uff1b\u540c\u65f6\u6784\u5efa\u5305\u542b\u666e\u901a\u548c\u6781\u7aef\u964d\u96e8\u7684\u5e73\u8861KMA\u96f7\u8fbe\u6570\u636e\u96c6", "result": "\u5728SEVIR\u3001MeteoNet\u548c\u5e73\u8861KMA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u666e\u901a\u548c\u6781\u7aef\u964d\u96e8\u60c5\u51b5\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63d0\u4f9b\u51c6\u786e\u53ef\u9760\u7684\u4e34\u8fd1\u9884\u62a5", "conclusion": "exPreCast\u6846\u67b6\u548c\u5e73\u8861\u6570\u636e\u96c6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.05638", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05638", "abs": "https://arxiv.org/abs/2602.05638", "authors": ["Jinlin Wu", "Felix Holm", "Chuxi Chen", "An Wang", "Yaxin Hu", "Xiaofan Ye", "Zelin Zang", "Miao Xu", "Lihua Zhou", "Huai Liao", "Danny T. M. Chan", "Ming Feng", "Wai S. Poon", "Hongliang Ren", "Dong Yi", "Nassir Navab", "Gaofeng Meng", "Jiebo Luo", "Hongbin Liu", "Zhen Lei"], "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos", "comment": null, "summary": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.", "AI": {"tldr": "UniSurg\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u539f\u751f\u67b6\u6784\u7684\u624b\u672f\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u50cf\u7d20\u7ea7\u91cd\u5efa\u8f6c\u5411\u6f5c\u5728\u8fd0\u52a8\u9884\u6d4b\uff0c\u4e13\u6ce8\u4e8e\u8bed\u4e49\u7ed3\u6784\u800c\u975e\u4f4e\u5c42\u6b21\u89c6\u89c9\u7ec6\u8282\uff0c\u5728\u591a\u4e2a\u624b\u672f\u89c6\u9891\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u624b\u672f\u89c6\u9891\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u50cf\u7d20\u7ea7\u91cd\u5efa\u76ee\u6807\uff0c\u6d6a\u8d39\u6a21\u578b\u5bb9\u91cf\u5728\u70df\u96fe\u3001\u955c\u9762\u53cd\u5c04\u3001\u6d41\u4f53\u8fd0\u52a8\u7b49\u4f4e\u5c42\u6b21\u89c6\u89c9\u7ec6\u8282\u4e0a\uff0c\u800c\u4e0d\u662f\u5173\u6ce8\u5bf9\u624b\u672f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u57fa\u4e8e\u89c6\u9891\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784(V-JEPA)\uff0c\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u6280\u672f\u521b\u65b0\uff1a1)\u8fd0\u52a8\u5f15\u5bfc\u7684\u6f5c\u5728\u9884\u6d4b\u4ee5\u4f18\u5148\u8003\u8651\u8bed\u4e49\u91cd\u8981\u533a\u57df\uff1b2)\u65f6\u7a7a\u4eb2\u548c\u6027\u81ea\u84b8\u998f\u4ee5\u5f3a\u5236\u5173\u7cfb\u4e00\u81f4\u6027\uff1b3)\u7279\u5f81\u591a\u6837\u6027\u6b63\u5219\u5316\u4ee5\u9632\u6b62\u7eb9\u7406\u7a00\u758f\u624b\u672f\u573a\u666f\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u3002\u4f7f\u7528UniSurg-15M\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002", "result": "\u572817\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff1a\u624b\u672f\u5de5\u4f5c\u6d41\u8bc6\u522b\uff08EgoSurgery\u4e0aF1\u63d0\u534714.6%\uff0cPitVis\u4e0a\u63d0\u534710.3%\uff09\u3001\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\uff08CholecT50\u4e0a39.54% mAP-IVT\uff09\u3001\u6280\u80fd\u8bc4\u4f30\u3001\u606f\u8089\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u3002", "conclusion": "UniSurg\u901a\u8fc7\u4ece\u50cf\u7d20\u7ea7\u91cd\u5efa\u8f6c\u5411\u8fd0\u52a8\u9884\u6d4b\u7684\u5b66\u4e60\u8303\u5f0f\u8f6c\u53d8\uff0c\u5efa\u7acb\u4e86\u901a\u7528\u3001\u8fd0\u52a8\u5bfc\u5411\u7684\u624b\u672f\u89c6\u9891\u7406\u89e3\u65b0\u6807\u51c6\uff0c\u4e13\u6ce8\u4e8e\u8bed\u4e49\u7ed3\u6784\u800c\u975e\u4f4e\u5c42\u6b21\u89c6\u89c9\u7ec6\u8282\u3002"}}
{"id": "2602.05737", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.05737", "abs": "https://arxiv.org/abs/2602.05737", "authors": ["Luca Ciampi", "Ludovico Iannello", "Fabrizio Tonelli", "Gabriele Lagani", "Angelo Di Garbo", "Federico Cremisi", "Giuseppe Amato"], "title": "Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing", "comment": null, "summary": "In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.", "AI": {"tldr": "\u5229\u7528\u4f53\u5916\u57f9\u517b\u7684\u76ae\u5c42\u795e\u7ecf\u5143\u7f51\u7edc\u4f5c\u4e3a\u7269\u7406\u50a8\u5c42\uff0c\u6784\u5efa\u751f\u7269\u50a8\u5c42\u8ba1\u7b97\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u5bc6\u5ea6\u5fae\u7535\u6781\u9635\u5217\u8fdb\u884c\u523a\u6fc0\u548c\u8bfb\u53d6\uff0c\u5b9e\u73b0\u9759\u6001\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u50a8\u5c42\u8ba1\u7b97\u4f9d\u8d56\u4eba\u5de5\u5faa\u73af\u6a21\u578b\u8fd1\u4f3c\u795e\u7ecf\u52a8\u529b\u5b66\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u771f\u5b9e\u751f\u7269\u795e\u7ecf\u56de\u8def\u4f5c\u4e3a\u8ba1\u7b97\u57fa\u5e95\uff0c\u5c06\u751f\u7269\u539f\u7406\u878d\u5165\u673a\u5668\u5b66\u4e60\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "method": "\u4f7f\u7528\u4f53\u5916\u57f9\u517b\u7684\u76ae\u5c42\u795e\u7ecf\u5143\u7f51\u7edc\u4f5c\u4e3a\u7269\u7406\u50a8\u5c42\uff0c\u901a\u8fc7\u9ad8\u5bc6\u5ea6\u5fae\u7535\u6781\u9635\u5217\u540c\u65f6\u8fdb\u884c\u523a\u6fc0\u548c\u8bfb\u53d6\uff0c\u8f93\u5165\u6a21\u5f0f\u901a\u8fc7\u9009\u5b9a\u7535\u6781\u4f20\u9012\uff0c\u5176\u4f59\u7535\u6781\u6355\u83b7\u9ad8\u7ef4\u795e\u7ecf\u54cd\u5e94\uff0c\u6700\u540e\u7528\u7ebf\u6027\u8bfb\u51fa\u5c42\uff08\u5355\u5c42\u611f\u77e5\u5668\uff09\u5bf9\u50a8\u5c42\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7cfb\u7edf\u5728\u4ece\u70b9\u5230\u5b9a\u5411\u6761\u3001\u65f6\u949f\u6570\u5b57\u5f62\u72b6\u5230MNIST\u624b\u5199\u6570\u5b57\u7b49\u96be\u5ea6\u9012\u589e\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u5c3d\u7ba1\u5b58\u5728\u751f\u7269\u795e\u7ecf\u54cd\u5e94\u7684\u56fa\u6709\u53d8\u5f02\u6027\uff0c\u4f46\u4ecd\u80fd\u751f\u6210\u652f\u6301\u51c6\u786e\u5206\u7c7b\u7684\u9ad8\u7ef4\u8868\u793a\u3002", "conclusion": "\u4f53\u5916\u76ae\u5c42\u7f51\u7edc\u53ef\u4f5c\u4e3a\u9759\u6001\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\u7684\u6709\u6548\u50a8\u5c42\uff0c\u4e3a\u5c06\u6d3b\u4f53\u795e\u7ecf\u57fa\u5e95\u6574\u5408\u5230\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u6846\u67b6\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u5982\u4f55\u4e3a\u9ad8\u6548\u4e14\u751f\u7269\u5b66\u57fa\u7840\u7684\u8ba1\u7b97\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2602.05882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05882", "abs": "https://arxiv.org/abs/2602.05882", "authors": ["Mubashir Noman", "Mustansar Fiaz", "Hiyam Debary", "Abdul Hannan", "Shah Nawaz", "Fahad Shahbaz Khan", "Salman Khan"], "title": "EoCD: Encoder only Remote Sensing Change Detection", "comment": null, "summary": "Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51faEoCD\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u65f6\u95f4\u6570\u636e\u5e76\u7528\u53c2\u6570\u514d\u8d39\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u66ff\u4ee3\u89e3\u7801\u5668\uff0c\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5728\u6027\u80fd\u548c\u901f\u5ea6\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u57fa\u4e8e\u5b6a\u751f\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u9700\u8981\u5206\u522b\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\u518d\u8fdb\u884c\u878d\u5408\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6a21\u578b\u590d\u6742\uff1b2\uff09\u65e9\u671f\u878d\u5408\u65b9\u6cd5\u867d\u7136\u907f\u514d\u4e86\u5b6a\u751f\u7f16\u7801\u5668\u7684\u5f00\u9500\uff0c\u4f46\u4ecd\u4f9d\u8d56\u590d\u6742\u7684\u89e3\u7801\u5668\uff0c\u4e14\u6027\u80fd\u4e0d\u5982\u540e\u671f\u878d\u5408\u65b9\u6cd5\u3002\u9700\u8981\u4e00\u79cd\u65e2\u7b80\u5355\u53c8\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7f16\u7801\u5668\u4ec5\u53d8\u5316\u68c0\u6d4b\uff08EoCD\uff09\u65b9\u6cd5\uff1a1\uff09\u5bf9\u65f6\u95f4\u6570\u636e\u8fdb\u884c\u65e9\u671f\u878d\u5408\uff1b2\uff09\u7528\u53c2\u6570\u514d\u8d39\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u66ff\u4ee3\u4f20\u7edf\u89e3\u7801\u5668\uff1b3\uff09\u8bc1\u660e\u6a21\u578b\u6027\u80fd\u4e3b\u8981\u53d6\u51b3\u4e8e\u7f16\u7801\u5668\u7f51\u7edc\uff0c\u89e3\u7801\u5668\u53ea\u662f\u989d\u5916\u7ec4\u4ef6\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cEoCD\u65b9\u6cd5\u5728\u5404\u79cd\u7f16\u7801\u5668\u67b6\u6784\u4e0b\u90fd\u8868\u73b0\u51fa\u53d8\u5316\u68c0\u6d4b\u6027\u80fd\u548c\u9884\u6d4b\u901f\u5ea6\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u6574\u4f53\u590d\u6742\u5ea6\u3002", "conclusion": "EoCD\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u548c\u53c2\u6570\u514d\u8d39\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u4e86\u7f16\u7801\u5668\u5728\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002"}}
{"id": "2602.06028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06028", "abs": "https://arxiv.org/abs/2602.06028", "authors": ["Shuo Chen", "Cong Wei", "Sun Sun", "Ping Nie", "Kai Zhou", "Ge Zhang", "Ming-Hsuan Yang", "Wenhu Chen"], "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context", "comment": null, "summary": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \\textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \\textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \\textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.", "AI": {"tldr": "\u63d0\u51faContext Forcing\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u6559\u5e08\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u5b66\u751f\uff0c\u89e3\u51b3\u73b0\u6709\u6d41\u5f0f\u8c03\u4f18\u4e2d\u7684\u5e08\u751f\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u8fc720\u79d2\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6", "motivation": "\u73b0\u6709\u5b9e\u65f6\u957f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u91c7\u7528\u6d41\u5f0f\u8c03\u4f18\u7b56\u7565\uff0c\u4f7f\u7528\u77ed\u4e0a\u4e0b\u6587\uff08\u65e0\u8bb0\u5fc6\uff09\u6559\u5e08\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u5b66\u751f\u3002\u8fd9\u79cd\u7ed3\u6784\u5dee\u5f02\u5bfc\u81f4\u5e08\u751f\u4e0d\u5339\u914d\uff1a\u6559\u5e08\u65e0\u6cd5\u8bbf\u95ee\u957f\u671f\u5386\u53f2\uff0c\u65e0\u6cd5\u6307\u5bfc\u5b66\u751f\u5904\u7406\u5168\u5c40\u65f6\u95f4\u4f9d\u8d56\uff0c\u9650\u5236\u4e86\u5b66\u751f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6", "method": "\u63d0\u51faContext Forcing\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u6559\u5e08\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u5b66\u751f\uff0c\u6d88\u9664\u76d1\u7763\u4e0d\u5339\u914d\u3002\u4e3a\u5b9e\u73b0\u6781\u7aef\u65f6\u957f\uff08\u59822\u5206\u949f\uff09\u7684\u8ba1\u7b97\u53ef\u884c\u6027\uff0c\u5f15\u5165\u4e0a\u4e0b\u6587\u7ba1\u7406\u7cfb\u7edf\uff0c\u5c06\u7ebf\u6027\u589e\u957f\u7684\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u6162-\u5feb\u8bb0\u5fc6\u67b6\u6784\uff0c\u663e\u8457\u51cf\u5c11\u89c6\u89c9\u5197\u4f59", "result": "\u65b9\u6cd5\u5b9e\u73b0\u8d85\u8fc720\u79d2\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u6bd4LongLive\u548cInfinite-RoPE\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u957f2-10\u500d\u3002\u5229\u7528\u6269\u5c55\u7684\u4e0a\u4e0b\u6587\uff0cContext Forcing\u5728\u957f\u65f6\u6bb5\u5185\u4fdd\u6301\u4f18\u8d8a\u7684\u4e00\u81f4\u6027\uff0c\u5728\u5404\u79cd\u957f\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u57fa\u7ebf", "conclusion": "Context Forcing\u901a\u8fc7\u89e3\u51b3\u5e08\u751f\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u957f\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u4e3a\u5b9e\u65f6\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.05639", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05639", "abs": "https://arxiv.org/abs/2602.05639", "authors": ["Amin Oji", "Paul Fieguth"], "title": "Joint Embedding Variational Bayes", "comment": null, "summary": "We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.", "AI": {"tldr": "VJE\u662f\u4e00\u4e2a\u7ed3\u5408\u8054\u5408\u5d4c\u5165\u548c\u53d8\u5206\u63a8\u65ad\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6982\u7387\u8868\u793a\uff0c\u65e0\u9700\u91cd\u5efa\u548c\u975e\u5bf9\u6bd4\u8bbe\u7f6e\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u975e\u5bf9\u6bd4\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u80fd\u91cf\u7684\u9884\u6d4b\u76ee\u6807\u4f18\u5316\u70b9\u95f4\u5dee\u5f02\uff0c\u800cVJE\u65e8\u5728\u901a\u8fc7\u53d8\u5206\u63a8\u65ad\u6846\u67b6\u5b66\u4e60\u6982\u7387\u8868\u793a\uff0c\u907f\u514d\u91cd\u5efa\u548c\u975e\u5bf9\u6bd4\u8bbe\u7f6e\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "VJE\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u79f0\u6761\u4ef6\u8bc1\u636e\u4e0b\u754c(ELBO)\u6765\u5b66\u4e60\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff0c\u4f7f\u7528\u91cd\u5c3eStudent-t\u5206\u5e03\u5efa\u6a21\u6761\u4ef6\u4f3c\u7136\uff0c\u901a\u8fc7\u6781\u5750\u6807\u5206\u89e3\u89e3\u8026\u65b9\u5411\u548c\u5f84\u5411\u56e0\u5b50\uff0c\u91c7\u7528\u644a\u9500\u63a8\u65ad\u7f51\u7edc\u53c2\u6570\u5316\u5bf9\u89d2\u9ad8\u65af\u53d8\u5206\u540e\u9a8c\u3002", "result": "\u5728ImageNet-1K\u3001CIFAR-10/100\u548cSTL-10\u6570\u636e\u96c6\u4e0a\uff0cVJE\u5728\u7ebf\u6027\u548ck-NN\u8bc4\u4f30\u4e2d\u8fbe\u5230\u4e0e\u975e\u5bf9\u6bd4\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff1b\u5728\u4e00\u7c7bCIFAR-10\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u57fa\u4e8e\u4f3c\u7136\u7684\u8bc4\u5206\u4f18\u4e8e\u5176\u4ed6\u81ea\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "VJE\u6210\u529f\u5730\u5c06\u53d8\u5206\u63a8\u65ad\u4e0e\u8054\u5408\u5d4c\u5165\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u91cd\u5efa\u81ea\u7531\u3001\u975e\u5bf9\u6bd4\u7684\u81ea\u76d1\u7763\u6982\u7387\u8868\u793a\u5b66\u4e60\uff0c\u80fd\u591f\u6355\u83b7\u5404\u5411\u5f02\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.05667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05667", "abs": "https://arxiv.org/abs/2602.05667", "authors": ["Ling Zhan", "Zhen Li", "Junjie Huang", "Tao Jia"], "title": "Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection", "comment": "33 pages, 8 figures, ICLR conference paper", "summary": "Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS", "AI": {"tldr": "\u63d0\u51faSCLCS\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u4ee3\u8868\u6027\u6838\u5fc3\u96c6\u6765\u4fdd\u7559\u529f\u80fd\u8fde\u63a5\u5efa\u6a21\u65b9\u6cd5\u7684\u76f8\u5bf9\u6027\u80fd\u6392\u540d\uff0c\u89e3\u51b3\u5927\u89c4\u6a21fMRI\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21fMRI\u6570\u636e\u96c6\u4e2d\u529f\u80fd\u8fde\u63a5\u5efa\u6a21\u65b9\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u53ef\u91cd\u590d\u795e\u7ecf\u79d1\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u578b-\u6570\u636e\u914d\u5bf9\u7684\u7ec4\u5408\u7206\u70b8\u4f7f\u5f97\u7a77\u4e3e\u8bc4\u4f30\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u963b\u788d\u4e86\u8fd9\u79cd\u8bc4\u4f30\u6210\u4e3a\u5e38\u89c4\u9884\u5206\u6790\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u7684\u6838\u5fc3\u96c6\u9009\u62e9\u6846\u67b6\uff1a1) \u4f7f\u7528\u81ea\u9002\u5e94Transformer\u5b66\u4e60\u6bcf\u4e2a\u6837\u672c\u7684\u72ec\u7279\u529f\u80fd\u8fde\u63a5\u7ed3\u6784\uff1b2) \u5f15\u5165\u7ed3\u6784\u6270\u52a8\u8bc4\u5206\u91cf\u5316\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u7ed3\u6784\u7684\u7a33\u5b9a\u6027\uff1b3) \u91c7\u7528\u5bc6\u5ea6\u5e73\u8861\u91c7\u6837\u7b56\u7565\u786e\u4fdd\u6838\u5fc3\u96c6\u7684\u7ed3\u6784\u9c81\u68d2\u6027\u548c\u5206\u5e03\u4ee3\u8868\u6027\u3002", "result": "\u5728REST-meta-MDD\u6570\u636e\u96c6\u4e0a\uff0cSCLCS\u4ec5\u4f7f\u752810%\u7684\u6570\u636e\u5c31\u80fd\u4fdd\u7559\u771f\u5b9e\u6a21\u578b\u6392\u540d\uff0c\u5728\u6392\u540d\u4e00\u81f4\u6027(nDCG@k)\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u9ad8\u51fa23.2%\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u6838\u5fc3\u96c6\u9009\u62e9\u5f62\u5f0f\u5316\u7528\u4e8e\u529f\u80fd\u8fde\u63a5\u7b97\u5b50\u57fa\u51c6\u6d4b\u8bd5\u7684\u5de5\u4f5c\uff0c\u4f7f\u5927\u89c4\u6a21\u7b97\u5b50\u6bd4\u8f83\u6210\u4e3a\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e2d\u53ef\u884c\u4e14\u4e0d\u53ef\u6216\u7f3a\u7684\u90e8\u5206\u3002"}}
{"id": "2602.05783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05783", "abs": "https://arxiv.org/abs/2602.05783", "authors": ["Shutong Ding", "Yimiao Zhou", "Ke Hu", "Mokai Pan", "Shan Zhong", "Yanwei Fu", "Jingya Wang", "Ye Shi"], "title": "Distributional Reinforcement Learning with Diffusion Bridge Critics", "comment": null, "summary": "Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6865\u6279\u8bc4\u5668\u7684\u65b0\u578b\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5DBC\uff0c\u9996\u6b21\u5c06\u6269\u6563\u6865\u6a21\u578b\u7528\u4f5c\u6279\u8bc4\u5668\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21Q\u503c\u7684\u9006\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u6765\u51c6\u786e\u6355\u6349\u4ef7\u503c\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6269\u6563\u7b56\u7565\uff0c\u800c\u5ffd\u7565\u4e86\u6269\u6563\u6279\u8bc4\u5668\u3002\u7531\u4e8e\u7b56\u7565\u4f18\u5316\u672c\u8d28\u4e0a\u4f9d\u8d56\u4e8e\u6279\u8bc4\u5668\uff0c\u51c6\u786e\u7684\u4ef7\u503c\u4f30\u8ba1\u6bd4\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u66f4\u91cd\u8981\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u968f\u673a\u6027\uff0c\u6279\u8bc4\u5668\u66f4\u9002\u5408\u7528\u5206\u5e03\u6a21\u578b\u6765\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u6269\u6563\u6865\u6279\u8bc4\u5668(DBC)\uff0c\u76f4\u63a5\u5efa\u6a21Q\u503c\u7684\u9006\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff0c\u5229\u7528\u6269\u6563\u6865\u7684\u5f3a\u5927\u5206\u5e03\u5339\u914d\u80fd\u529b\u9632\u6b62\u4ef7\u503c\u5206\u5e03\u574d\u7f29\u4e3a\u5e73\u51e1\u9ad8\u65af\u5206\u5e03\u3002\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u5904\u7406DBC\u4e2d\u79bb\u6563\u5316\u8bef\u5dee\u7684\u89e3\u6790\u79ef\u5206\u516c\u5f0f\u3002", "result": "\u5728MuJoCo\u673a\u5668\u4eba\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDBC\u76f8\u6bd4\u4e4b\u524d\u7684\u5206\u5e03\u6279\u8bc4\u5668\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002DBC\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u5927\u591a\u6570\u73b0\u6709RL\u6846\u67b6\u4e2d\u3002", "conclusion": "DBC\u662f\u9996\u4e2a\u5c06\u6269\u6563\u6865\u6a21\u578b\u7528\u4f5c\u6279\u8bc4\u5668\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u51c6\u786e\u6355\u6349\u4ef7\u503c\u5206\u5e03\u63d0\u5347\u4e86\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.06029", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06029", "abs": "https://arxiv.org/abs/2602.06029", "authors": ["Yingke Li", "Anjali Parashar", "Enlu Zhou", "Chuchu Fan"], "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference", "comment": null, "summary": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.", "AI": {"tldr": "\u8bba\u6587\u4e3a\u4e3b\u52a8\u63a8\u7406\u4e2d\u7684\u671f\u671b\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\"\u8db3\u591f\u7684\u597d\u5947\u5fc3\"\u80fd\u540c\u65f6\u786e\u4fdd\u8d1d\u53f6\u65af\u540e\u9a8c\u4e00\u81f4\u6027\u548c\u65e0\u9057\u61be\u4f18\u5316\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0e\u7ecf\u5178\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u7406\u8bba\u8054\u7cfb\u3002", "motivation": "\u4e3b\u52a8\u63a8\u7406\u901a\u8fc7\u671f\u671b\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u7edf\u4e00\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4f46\u4e00\u76f4\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u6765\u5e73\u8861\u8ba4\u77e5\u4ef7\u503c\uff08\u4fe1\u606f\u589e\u76ca\uff09\u548c\u5b9e\u7528\u4ef7\u503c\uff08\u4efb\u52a1\u6027\u80fd\uff09\u3002\u4e0d\u6e05\u695a\u4f55\u65f6\u8fd9\u79cd\u5e73\u8861\u80fd\u540c\u65f6\u5b9e\u73b0\u4e00\u81f4\u5b66\u4e60\u548c\u9ad8\u6548\u51b3\u7b56\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u8bc1\u660e\"\u8db3\u591f\u7684\u597d\u5947\u5fc3\"\u8fd9\u4e00\u5355\u4e00\u8981\u6c42\u80fd\u540c\u65f6\u4fdd\u8bc1\u8d1d\u53f6\u65af\u540e\u9a8c\u4e00\u81f4\u6027\u548c\u6709\u754c\u7d2f\u79ef\u9057\u61be\u3002\u5206\u6790\u8be5\u673a\u5236\u5982\u4f55\u4f9d\u8d56\u4e8e\u521d\u59cb\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u8bc6\u522b\u6027\u548c\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u83b7\u5f97\u4e86\u4e3b\u52a8\u63a8\u7406\u4e2d\u671f\u671b\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u4ee3\u7406\u7684\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff0c\u5c06\u4e3b\u52a8\u63a8\u7406\u4e0e\u7ecf\u5178\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7edf\u4e00\u5728\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u5185\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u8f6c\u5316\u4e3a\u5b9e\u9645\u8bbe\u8ba1\u6307\u5357\uff0c\u7528\u4e8e\u5728\u6df7\u5408\u5b66\u4e60-\u4f18\u5316\u95ee\u9898\u4e2d\u8c03\u6574\u8ba4\u77e5-\u5b9e\u7528\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
