{"id": "2506.22740", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22740", "abs": "https://arxiv.org/abs/2506.22740", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "title": "Explanations are a means to an end", "comment": null, "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u5e94\u9488\u5bf9\u5177\u4f53\u7528\u9014\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u7528\u4f8b\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u89e3\u91ca\u7684\u5b9e\u9645\u7528\u9014\uff0c\u5bfc\u81f4\u89e3\u91ca\u53ef\u80fd\u88ab\u8bef\u7528\u6216\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u89e3\u91ca\u8bbe\u8ba1\u4e0e\u5177\u4f53\u4efb\u52a1\u76ee\u6807\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u7406\u60f3\u5316\u51b3\u7b56\u8005\u6a21\u578b\u91cf\u5316\u89e3\u91ca\u7684\u6f5c\u5728\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u63d0\u4f9b\u8865\u6551\u63aa\u65bd\u548c\u8c03\u8bd5\u7b49\u591a\u6837\u5316\u7528\u4f8b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u91cf\u5316\u4e86\u89e3\u91ca\u7684\u6700\u5927\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u7684\u8bc4\u4f30\u5e94\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u9645\u7528\u9014\uff0c\u660e\u786e\u5177\u4f53\u7528\u4f8b\u4ee5\u907f\u514d\u6b67\u4e49\uff0c\u5e76\u63d0\u51fa\u8de8\u89c6\u89d2\u7684\u5b9a\u4e49\u3002"}}
{"id": "2506.22491", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2506.22491", "abs": "https://arxiv.org/abs/2506.22491", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPromptAug\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u51b2\u7a81\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u63d0\u5347\u4e862%\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u51b2\u7a81\u589e\u591a\uff0c\u4f46\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u540c\u65f6\u5e73\u53f0\u9650\u5236\u7814\u7a76\u6570\u636e\u8bbf\u95ee\uff0c\u6570\u636e\u589e\u5f3a\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faPromptAug\u65b9\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u589e\u5f3a\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6781\u7aef\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3001\u591a\u6837\u6027\u5206\u6790\u548c\u4e3b\u9898\u5206\u6790\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "PromptAug\u5728\u51b2\u7a81\u548c\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e862%\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "conclusion": "PromptAug\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u654f\u611f\u4efb\u52a1\u5982\u51b2\u7a81\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u793e\u4f1a\u79d1\u5b66\u65b9\u6cd5\u3002"}}
{"id": "2506.22498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22498", "abs": "https://arxiv.org/abs/2506.22498", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "comment": null, "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "AI": {"tldr": "\u901a\u8fc7\u4f4e\u6210\u672c\u8d1f\u8f7d\u4f20\u611f\u5668\u9884\u6d4b\u60a3\u8005\u79bb\u5e8a\u610f\u56fe\uff0c\u63d0\u51faViFusionTST\u6a21\u578b\uff0c\u878d\u5408\u591a\u6a21\u6001\u56fe\u50cf\u6570\u636e\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u533b\u9662\u548c\u957f\u671f\u62a4\u7406\u673a\u6784\u4e2d\u5e8a\u65c1\u8dcc\u5012\u95ee\u9898\uff0c\u73b0\u6709\u8b66\u62a5\u7cfb\u7edf\u53cd\u5e94\u6ede\u540e\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u8d1f\u8f7d\u4f20\u611f\u5668\u751f\u6210\u4fe1\u53f7\uff0c\u8f6c\u6362\u4e3a\u591a\u6a21\u6001\u56fe\u50cf\uff0c\u63d0\u51fa\u53cc\u6d41Swin Transformer\u6a21\u578bViFusionTST\u8fdb\u884c\u878d\u5408\u5206\u7c7b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.885\u51c6\u786e\u7387\u548c0.794 F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u878d\u5408\u7684\u8d1f\u8f7d\u4fe1\u53f7\u5206\u7c7b\u662f\u5b9e\u65f6\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8dcc\u5012\u9884\u9632\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2506.22444", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.22444", "abs": "https://arxiv.org/abs/2506.22444", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "comment": null, "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4e3b\u52a8\u6ce8\u610f\u529b\u7f51\u7edc\u7684PASC\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "PASC\u7684\u957f\u671f\u5f71\u54cd\u5bf9\u5168\u7403\u533b\u7597\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5176\u590d\u6742\u8fdb\u5c55\uff0c\u9700\u66f4\u7cbe\u51c6\u7684\u98ce\u9669\u9884\u6d4b\u548c\u4e8b\u4ef6\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Llama-3.1-70B-Instruct\u751f\u6210\u6587\u672c\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\uff0c\u7ed3\u5408\u4e34\u5e8a\u4e13\u5bb6\u6807\u6ce8\uff0c\u63d0\u51fa\u4e3b\u52a8\u6ce8\u610f\u529b\u7f51\u7edc\u9884\u6d4b\u98ce\u9669\u548c\u8fdb\u5c55\u4e8b\u4ef6\u3002", "result": "\u65b9\u6cd5\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u63d0\u5347\u98ce\u9669\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6807\u6ce8\u91cf\u3002", "conclusion": "\u7814\u7a76\u76ee\u6807\u662f\u901a\u8fc7\u6539\u8fdb\u98ce\u9669\u9884\u6d4b\u548c\u4e8b\u4ef6\u8bc6\u522b\uff0c\u4f18\u5316SARS-CoV-2\u60a3\u8005\u7684\u62a4\u7406\u548c\u51b3\u7b56\u3002"}}
{"id": "2506.22510", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22510", "abs": "https://arxiv.org/abs/2506.22510", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDGCL\u7684\u591a\u57df\u9884\u8bad\u7ec3\u548c\u8de8\u57df\u8fc1\u79fb\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u6570\u636e\u4e2d\u4e0d\u540c\u57df\u95f4\u8bed\u4e49\u548c\u5c5e\u6027\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u56fe\u6570\u636e\u5728\u4e0d\u540c\u57df\u95f4\u5b58\u5728\u5de8\u5927\u8bed\u4e49\u548c\u5c5e\u6027\u5dee\u5f02\uff0c\u4f20\u7edf\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u5438\u6536\u591a\u57df\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u80fd\u8bc6\u522b\u548c\u6355\u83b7\u57df\u5dee\u5f02\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5f15\u5165\u57df\u4ee4\u724c\u7f16\u7801\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f7f\u7528\u57df\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMDGCL\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6700\u9ad8\u63d0\u534719.33%\u7684\u51c6\u786e\u7387\u548c19.13%\u7684Macro-F1\u5206\u6570\u3002", "conclusion": "MDGCL\u6846\u67b6\u901a\u8fc7\u591a\u57df\u9884\u8bad\u7ec3\u548c\u8de8\u57df\u8fc1\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u7684\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.22500", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22500", "abs": "https://arxiv.org/abs/2506.22500", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u624b\u672f\u5ba4\u98ce\u9669\u68c0\u6d4b\u4e2d\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\uff08VS-KC\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6OR-VSKC\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u672f\u98ce\u9669\u8bc6\u522b\u5bf9\u60a3\u8005\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709MLLMs\u5728\u89c6\u89c9\u5b89\u5168\u8fdd\u89c4\u68c0\u6d4b\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u4e0e\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u621034,000\u591a\u5f20\u5408\u6210\u56fe\u50cf\uff0c\u6a21\u62df\u624b\u672f\u5ba4\u8fdd\u89c4\u573a\u666f\uff0c\u5e76\u8f85\u4ee5214\u5f20\u4eba\u5de5\u6807\u6ce8\u56fe\u50cf\u4f5c\u4e3a\u9a8c\u8bc1\u57fa\u51c6\u3002\u901a\u8fc7\u5fae\u8c03MLLMs\uff0c\u7814\u7a76\u5176\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u540e\u7684MLLMs\u5728\u8bad\u7ec3\u8fc7\u7684\u51b2\u7a81\u5b9e\u4f53\u68c0\u6d4b\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5bf9\u672a\u8bad\u7ec3\u5b9e\u4f53\u7c7b\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u663e\u793a\u5b66\u4e60\u7279\u5f02\u6027\u3002", "conclusion": "OR-VSKC\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u7814\u7a76VS-KC\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u4f46\u9700\u66f4\u5168\u9762\u7684\u8bad\u7ec3\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.22894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22894", "abs": "https://arxiv.org/abs/2506.22894", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "comment": null, "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6f02\u79fb\uff0c\u7ed3\u5408\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\u548c\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\uff08PSF\uff09\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u7a33\u5b9a\u6f02\u79fb\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9ad8\u901f\u6f02\u79fb\u65f6\u96be\u4ee5\u5e94\u5bf9\u4e0d\u7a33\u5b9a\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u63a2\u7d22\u80fd\u529b\u6709\u9650\u4e14\u672a\u6709\u6548\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u3002", "method": "\u7ed3\u5408RL\u4ee3\u7406\u548c\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\uff0c\u901a\u8fc7PSF\u5728\u7ebf\u8c03\u6574\u52a8\u4f5c\u4ee5\u907f\u514d\u4e0d\u5b89\u5168\u72b6\u6001\u3002", "result": "\u5728Matlab-Carsim\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u6f02\u79fb\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c11\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2506.23078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23078", "abs": "https://arxiv.org/abs/2506.23078", "authors": ["Zhaoxing Zhang", "Xiaoxiang Wang", "Chengliang Zhang", "Yangyang Guo", "Zikang Yuan", "Xin Yang"], "title": "Event-based Stereo Visual-Inertial Odometry with Voxel Map", "comment": null, "summary": "The event camera, renowned for its high dynamic range and exceptional\ntemporal resolution, is recognized as an important sensor for visual odometry.\nHowever, the inherent noise in event streams complicates the selection of\nhigh-quality map points, which critically determine the precision of state\nestimation. To address this challenge, we propose Voxel-ESVIO, an event-based\nstereo visual-inertial odometry system that utilizes voxel map management,\nwhich efficiently filter out high-quality 3D points. Specifically, our\nmethodology utilizes voxel-based point selection and voxel-aware point\nmanagement to collectively optimize the selection and updating of map points on\na per-voxel basis. These synergistic strategies enable the efficient retrieval\nof noise-resilient map points with the highest observation likelihood in\ncurrent frames, thereby ensureing the state estimation accuracy. Extensive\nevaluations on three public benchmarks demonstrate that our Voxel-ESVIO\noutperforms state-of-the-art methods in both accuracy and computational\nefficiency.", "AI": {"tldr": "Voxel-ESVIO\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f53\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u9ad8\u6548\u7b5b\u9009\u9ad8\u8d28\u91cf3D\u70b9\uff0c\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4f7f\u5176\u6210\u4e3a\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u91cd\u8981\u4f20\u611f\u5668\uff0c\u4f46\u4e8b\u4ef6\u6d41\u4e2d\u7684\u566a\u58f0\u5f71\u54cd\u4e86\u9ad8\u8d28\u91cf\u5730\u56fe\u70b9\u7684\u9009\u62e9\uff0c\u4ece\u800c\u5f71\u54cd\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u70b9\u9009\u62e9\u548c\u4f53\u7d20\u611f\u77e5\u7684\u70b9\u7ba1\u7406\u7b56\u7565\uff0c\u4f18\u5316\u5730\u56fe\u70b9\u7684\u9009\u62e9\u548c\u66f4\u65b0\uff0c\u9ad8\u6548\u63d0\u53d6\u6297\u566a\u58f0\u7684\u5730\u56fe\u70b9\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVoxel-ESVIO\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Voxel-ESVIO\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u6a21\u5f0f\u5d29\u6e83\u7684\u65b0\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u5168\u5173\u952e\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u56e0\u6a21\u5f0f\u5d29\u6e83\u4ec5\u9884\u6d4b\u6700\u53ef\u80fd\u6a21\u5f0f\uff0c\u5ffd\u89c6\u4ea4\u4e92\u591a\u6837\u6027\uff0c\u4e14\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u672a\u5b9a\u91cf\u8bc4\u4f30\u4ea4\u4e92\u6216\u6a21\u5f0f\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165\u6a21\u5f0f\u5d29\u6e83\u3001\u6a21\u5f0f\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\u7684\u6307\u6807\uff0c\u6d4b\u8bd5\u56db\u79cd\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u6a21\u5f0f\u5d29\u6e83\u786e\u5b9e\u5b58\u5728\uff0c\u5c3d\u7ba1\u9884\u6d4b\u51c6\u786e\u6027\u5728\u63a5\u8fd1\u4ea4\u4e92\u4e8b\u4ef6\u65f6\u63d0\u9ad8\uff0c\u4f46\u6a21\u578b\u4ecd\u53ef\u80fd\u65e0\u6cd5\u9884\u6d4b\u6b63\u786e\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u4e00\u81f4\u3001\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "AI": {"tldr": "VSRM\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u7a7a\u95f4Mamba\u5757\u63d0\u53d6\u957f\u8303\u56f4\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u548c\u9891\u7387\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u65b9\u6cd5\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u5c40\u90e8\u611f\u53d7\u91ce\u9650\u5236\u548c\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u800cMamba\u56e0\u5176\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVSRM\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u7a7a\u95f4Mamba\u5757\u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u52a8\u6001\u5bf9\u9f50\u5e27\uff0c\u5e76\u5f15\u5165\u9891\u7387\u635f\u5931\u51fd\u6570\u4f18\u5316\u9ad8\u9891\u5185\u5bb9\u3002", "result": "VSRM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "VSRM\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2506.22806", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22806", "abs": "https://arxiv.org/abs/2506.22806", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "comment": null, "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConcept Pinpoint Eraser (CPE)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6a21\u5757\u9009\u62e9\u6027\u64e6\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u62a4\u5176\u4ed6\u6982\u5ff5\uff0c\u5e76\u589e\u5f3a\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7ebf\u6027\u6a21\u5757\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u591a\u6837\u5316\u7684\u5269\u4f59\u6982\u5ff5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "CPE\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027Residual Attention Gates (ResAGs)\u548c\u6ce8\u610f\u529b\u951a\u5b9a\u635f\u5931\uff0c\u9009\u62e9\u6027\u64e6\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCPE\u5728\u64e6\u9664\u540d\u4eba\u3001\u827a\u672f\u98ce\u683c\u548c\u4e0d\u5f53\u5185\u5bb9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5269\u4f59\u6982\u5ff5\u7684\u591a\u6837\u6027\u3002", "conclusion": "CPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.23949", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23949", "abs": "https://arxiv.org/abs/2506.23949", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "comment": null, "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08GPAI\uff09\u548c\u57fa\u7840\u6a21\u578b\u7684\u98ce\u9669\u7ba1\u7406\u5b9e\u8df5\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522b\u3001\u5206\u6790\u548c\u51cf\u8f7b\u76f8\u5173\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u591a\u7528\u9014AI\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5176\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u65e5\u76ca\u663e\u8457\uff0c\u9700\u8981\u4e13\u95e8\u7684\u98ce\u9669\u7ba1\u7406\u63aa\u65bd\u3002", "method": "\u6587\u6863\u57fa\u4e8eNIST AI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\u548cISO/IEC 23894\u6807\u51c6\uff0c\u63d0\u4f9b\u4e86\u9488\u5bf9GPAI/\u57fa\u7840\u6a21\u578b\u7684\u72ec\u7279\u98ce\u9669\u7ba1\u7406\u5b9e\u8df5\u3002", "result": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u98ce\u9669\u63a7\u5236\u6307\u5357\uff0c\u6709\u52a9\u4e8e\u964d\u4f4eGPAI/\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3aGPAI/\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u98ce\u9669\u7ba1\u7406\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI\u6280\u672f\u7684\u5b89\u5168\u53d1\u5c55\u3002"}}
{"id": "2402.09146", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2402.09146", "abs": "https://arxiv.org/abs/2402.09146", "authors": ["Muhammad Kashif", "Muhammad Shafique"], "title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks", "comment": null, "summary": "In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u91cf\u5b50\u5377\u79ef\u5c42\u548c\u89e3\u51b3\u76f8\u5173\u6311\u6218\uff0c\u63d0\u5347\u4e86\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QuNNs\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u5377\u79ef\u5c42\u867d\u6709\u76ca\u4e8e\u7279\u5f81\u63d0\u53d6\uff0c\u4f46\u9759\u6001\u6027\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53ef\u8bad\u7ec3\u5c42\u63d0\u5347\u7075\u6d3b\u6027\u548c\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u6b8b\u5dee\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08ResQuNNs\uff09\uff0c\u5229\u7528\u6b8b\u5dee\u5b66\u4e60\u6982\u5ff5\uff0c\u901a\u8fc7\u8df3\u8dc3\u8fde\u63a5\u589e\u5f3a\u68af\u5ea6\u6d41\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6b8b\u5dee\u5757\u7684\u7b56\u7565\u6027\u653e\u7f6e\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6027\u80fd\uff0c\u68af\u5ea6\u6d41\u52a8\u66f4\u9ad8\u6548\u3002", "conclusion": "\u7814\u7a76\u4e3a\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.23293", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.23293", "abs": "https://arxiv.org/abs/2506.23293", "authors": ["P. Myles Eugenio"], "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "comment": "22 pages, 7 figures", "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u4e8b\u4ef6\u9a71\u52a8\u6d8c\u73b0\u5b66\u4e60\u7684\u795e\u7ecf\u7b26\u53f7\u751f\u6210\u8bed\u8a00\u5efa\u6a21\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5206\u5c42Hopfield\u8bb0\u5fc6\u94fe\uff0c\u7528\u4f5c\u7ec4\u5408\u77ed\u671f\u8bb0\u5fc6\u548c\u52a8\u6001\u5206\u8bcd\u5668\u3002\u6a21\u578b\u4ece\u96f6\u6784\u5efa\u7ed3\u6784\uff0c\u5b66\u4e60\u7b26\u53f7\u5e8f\u5217\u4f5c\u4e3a\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u5e76\u80fd\u4ece\u566a\u58f0\u4e2d\u8fc7\u6ee4\u81ea\u7136\u8bed\u8a00\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u4ece\u5c40\u90e8\u795e\u7ecf\u5b66\u4e60\u4e2d\u6d8c\u73b0\u51fa\u7b26\u53f7\u7ed3\u6784\uff0c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "method": "\u4f7f\u7528\u5206\u5c42Hopfield\u8bb0\u5fc6\u94fe\u4f5c\u4e3a\u52a8\u6001\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u6295\u5f71\u5f20\u91cf\u7ed1\u5b9a\u5171\u73b0\u7279\u5f81\u4e3a\u5206\u5c42\u6807\u8bb0\uff0c\u5f15\u5165\u5197\u4f59\u5e76\u538b\u7f29\u5c40\u90e8\u6fc0\u6d3b\u4e3a\u957f\u7a0b\u4f9d\u8d56\u3002", "result": "\u6a21\u578b\u80fd\u751f\u6210\u5177\u6709\u5185\u90e8\u5f62\u6001\u4e00\u81f4\u6027\u7684\u5408\u6210\u8bed\u8a00\uff0c\u4e0e\u4eba\u7c7b\u8bed\u8a00\u91cf\u5316\u76f8\u540c\uff0c\u5e76\u652f\u6301\u957f\u65f6\u8bb0\u5fc6\u548c\u7ec4\u5408\u63a8\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u7b26\u53f7\u7ed3\u6784\u5982\u4f55\u4ece\u5c40\u90e8\u795e\u7ecf\u5b66\u4e60\u4e2d\u6d8c\u73b0\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u5f62\u6001\u67b6\u6784\u53d1\u5c55\u3002"}}
{"id": "2506.23315", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23315", "abs": "https://arxiv.org/abs/2506.23315", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "comment": null, "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u57fa\u4e8eBERT\u7684\u96c6\u6210\u6a21\u578b\uff0c\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u68c0\u6d4b\u548c\u5206\u7c7b\u836f\u7269\u4e8b\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u8bc6\u522b\u5065\u5eb7\u8bb0\u5f55\u548c\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u5173\u952e\u53d8\u91cf\uff08\u5982\u836f\u7269\u3001\u75be\u75c5\u3001\u5173\u7cfb\uff09\u5728\u4e34\u5e8a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002n2c2 2022\u63d0\u4f9b\u4e86\u5171\u4eab\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528\u9884\u8bad\u7ec3BERT\u6a21\u578b\uff08\u57fa\u4e8eWikipedia\u548cMIMIC\u6570\u636e\uff09\uff0c\u5e76\u5728CMED\u8bad\u7ec3\u6570\u636e\u4e0a\u5fae\u8c03\uff0c\u901a\u8fc7\u6295\u7968\u7b56\u7565\u96c6\u6210\u591a\u4e2a\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBERT\u96c6\u6210\u6a21\u578b\u5c06\u4e25\u683cMicro-F\u5206\u6570\u548cMacro-F\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e86\u7ea65%\u548c6%\u3002", "conclusion": "\u57fa\u4e8eBERT\u7684\u96c6\u6210\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u836f\u7269\u4e8b\u4ef6\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23446", "abs": "https://arxiv.org/abs/2506.23446", "authors": ["Mohamed Elbasheer", "Adewale Akinfaderin"], "title": "Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders", "comment": null, "summary": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u7684Transformer\u65b9\u6cd5\uff08UBS-Transformer\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u5185\u90e8\u5a01\u80c1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u56e0\u6076\u610f\u884c\u4e3a\u8005\u7684\u6388\u6743\u8eab\u4efd\u548c\u5f02\u5e38\u884c\u4e3a\u7684\u9690\u853d\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u7528\u6237\u884c\u4e3a\u7684\u65f6\u5e8f\u4f9d\u8d56\u6027\u3002", "method": "\u5c06CERT\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u65f6\u5e8f\u5e8f\u5217\uff0c\u4f7f\u7528Transformer Encoder\u5efa\u6a21\u6b63\u5e38\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u8bef\u5dee\u4f5c\u4e3a\u5f02\u5e38\u5206\u6570\uff0c\u7ed3\u5408\u4e09\u79cd\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u56db\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u51c6\u786e\u738796.61%\uff0c\u53ec\u56de\u738799.43%\uff0cF1\u5206\u657096.38%\uff0cAUROC 95.00%\uff0c\u5047\u9634\u6027\u548c\u5047\u9633\u6027\u7387\u6781\u4f4e\u3002", "conclusion": "UBS-Transformer\u8bc1\u660e\u4e86\u65f6\u5e8f\u5efa\u6a21\u548c\u9ad8\u7ea7\u5f02\u5e38\u68c0\u6d4b\u5728\u5185\u90e8\u5a01\u80c1\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.23469", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.23469", "abs": "https://arxiv.org/abs/2506.23469", "authors": ["Chunjing Xiao", "Jiahui Lu", "Xovee Xu", "Fan Zhou", "Tianshu Xie", "Wei Lu", "Lifeng Xu"], "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection", "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS); DOI: https://doi.org/10.1109/TNNLS.2025.3561172", "summary": "Graph anomaly detection is critical in domains such as healthcare and\neconomics, where identifying deviations can prevent substantial losses.\nExisting unsupervised approaches strive to learn a single model capable of\ndetecting both attribute and structural anomalies. However, they confront the\ntug-of-war problem between two distinct types of anomalies, resulting in\nsuboptimal performance. This work presents TripleAD, a mutual\ndistillation-based triple-channel graph anomaly detection framework. It\nincludes three estimation modules to identify the attribute, structural, and\nmixed anomalies while mitigating the interference between different types of\nanomalies. In the first channel, we design a multiscale attribute estimation\nmodule to capture extensive node interactions and ameliorate the over-smoothing\nissue. To better identify structural anomalies, we introduce a link-enhanced\nstructure estimation module in the second channel that facilitates information\nflow to topologically isolated nodes. The third channel is powered by an\nattribute-mixed curvature, a new indicator that encapsulates both attribute and\nstructural information for discriminating mixed anomalies. Moreover, a mutual\ndistillation strategy is introduced to encourage communication and\ncollaboration between the three channels. Extensive experiments demonstrate the\neffectiveness of the proposed TripleAD model against strong baselines.", "AI": {"tldr": "TripleAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e92\u84b8\u998f\u7684\u4e09\u901a\u9053\u56fe\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u5206\u522b\u68c0\u6d4b\u5c5e\u6027\u3001\u7ed3\u6784\u548c\u6df7\u5408\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e24\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u540c\u65f6\u68c0\u6d4b\u5c5e\u6027\u548c\u7ed3\u6784\u5f02\u5e38\u65f6\u5b58\u5728\u6027\u80fd\u95ee\u9898\uff0cTripleAD\u65e8\u5728\u901a\u8fc7\u591a\u901a\u9053\u534f\u4f5c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TripleAD\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u591a\u5c3a\u5ea6\u5c5e\u6027\u4f30\u8ba1\u6a21\u5757\u3001\u94fe\u63a5\u589e\u5f3a\u7ed3\u6784\u4f30\u8ba1\u6a21\u5757\u548c\u5c5e\u6027\u6df7\u5408\u66f2\u7387\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u4e92\u84b8\u998f\u7b56\u7565\u4fc3\u8fdb\u6a21\u5757\u95f4\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTripleAD\u5728\u68c0\u6d4b\u5404\u7c7b\u5f02\u5e38\u65f6\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TripleAD\u901a\u8fc7\u591a\u901a\u9053\u534f\u4f5c\u548c\u4e92\u84b8\u998f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23596", "abs": "https://arxiv.org/abs/2506.23596", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA2P\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u4e8b\u4ef6\uff0c\u5305\u62ec\u5f02\u5e38\u611f\u77e5\u9884\u6d4b\uff08AAF\uff09\u548c\u5408\u6210\u5f02\u5e38\u63d0\u793a\uff08SAP\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u65f6\u95f4\u70b9\uff08Anomaly Prediction, AP\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u9884\u6d4b\u6216\u4ec5\u5173\u6ce8\u5373\u65f6\u5f02\u5e38\u3002", "method": "A2P\u6846\u67b6\u7ed3\u5408AAF\u548cSAP\uff0c\u901a\u8fc7\u5f02\u5e38\u5173\u7cfb\u5b66\u4e60\u548c\u53ef\u5b66\u4e60\u7684\u5f02\u5e38\u63d0\u793a\u6c60\uff08APP\uff09\u6a21\u62df\u591a\u6837\u5316\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cA2P\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u672a\u6765\u5f02\u5e38\u3002", "conclusion": "A2P\u4e3a\u89e3\u51b3\u672a\u6765\u5f02\u5e38\u9884\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23731", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23731", "abs": "https://arxiv.org/abs/2506.23731", "authors": ["Michel Meintz", "Jan Dubi\u0144ski", "Franziska Boenisch", "Adam Dziedzic"], "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "comment": null, "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u548c\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u6c34\u5370\u7684\u653e\u5c04\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u786e\u4fdd\u6c34\u5370\u5728\u8bad\u7ec3\u540e\u4ecd\u53ef\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u65e0\u6cd5\u4fdd\u6301\u653e\u5c04\u6027\uff0c\u4e14\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\u7f3a\u4e4f\u76f8\u5173\u6c34\u5370\u6280\u672f\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u9632\u6b62\u751f\u6210\u56fe\u50cf\u7684\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u501f\u9274\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6280\u672f\uff0c\u786e\u4fdd\u6c34\u5370\u5728\u8bad\u7ec3\u540e\u4ecd\u5177\u6709\u653e\u5c04\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u80fd\u6709\u6548\u4fdd\u7559\u6c34\u5370\u7684\u653e\u5c04\u6027\uff0c\u652f\u6301\u5f3a\u5927\u7684\u6765\u6e90\u8ffd\u8e2a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\u6c34\u5370\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4e3a\u751f\u6210\u56fe\u50cf\u7684\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.23776", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23776", "abs": "https://arxiv.org/abs/2506.23776", "authors": ["Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Model-driven Stochastic Trace Clustering", "comment": null, "summary": "Process discovery algorithms automatically extract process models from event\nlogs, but high variability often results in complex and hard-to-understand\nmodels. To mitigate this issue, trace clustering techniques group process\nexecutions into clusters, each represented by a simpler and more understandable\nprocess model. Model-driven trace clustering improves on this by assigning\ntraces to clusters based on their conformity to cluster-specific process\nmodels. However, most existing clustering techniques rely on either no process\nmodel discovery, or non-stochastic models, neglecting the frequency or\nprobability of activities and transitions, thereby limiting their capability to\ncapture real-world execution dynamics. We propose a novel model-driven trace\nclustering method that optimizes stochastic process models within each cluster.\nOur approach uses entropic relevance, a stochastic conformance metric based on\ndirectly-follows probabilities, to guide trace assignment. This allows\nclustering decisions to consider both structural alignment with a cluster's\nprocess model and the likelihood that a trace originates from a given\nstochastic process model. The method is computationally efficient, scales\nlinearly with input size, and improves model interpretability by producing\nclusters with clearer control-flow patterns. Extensive experiments on public\nreal-life datasets show that our method outperforms existing alternatives in\nrepresenting process behavior and reveals how clustering performance rankings\ncan shift when stochasticity is considered.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u7684\u8f68\u8ff9\u805a\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u71b5\u76f8\u5173\u6027\u5ea6\u91cf\u4f18\u5316\u805a\u7c7b\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u9ad8\u53d8\u5f02\u6027\u5bfc\u81f4\u8fc7\u7a0b\u6a21\u578b\u590d\u6742\u96be\u61c2\uff0c\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u5ffd\u7565\u6d3b\u52a8\u9891\u7387\u548c\u8f6c\u79fb\u6982\u7387\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u6267\u884c\u52a8\u6001\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u76f4\u63a5\u8ddf\u968f\u6982\u7387\u7684\u71b5\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u4f18\u5316\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\uff0c\u6307\u5bfc\u8f68\u8ff9\u5206\u914d\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8003\u8651\u968f\u673a\u6027\u65f6\u805a\u7c7b\u6027\u80fd\u6392\u540d\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u66f4\u51c6\u786e\u5730\u8868\u793a\u8fc7\u7a0b\u884c\u4e3a\u3002"}}
{"id": "2506.23157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23157", "abs": "https://arxiv.org/abs/2506.23157", "authors": ["Hanyu Zhou", "Haonan Wang", "Haoyue Liu", "Yuxing Duan", "Luxin Yan", "Gim Hee Lee"], "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene", "comment": null, "summary": "High-dynamic scene reconstruction aims to represent static background with\nrigid spatial features and dynamic objects with deformed continuous\nspatiotemporal features. Typically, existing methods adopt unified\nrepresentation model (e.g., Gaussian) to directly match the spatiotemporal\nfeatures of dynamic scene from frame camera. However, this unified paradigm\nfails in the potential discontinuous temporal features of objects due to frame\nimaging and the heterogeneous spatial features between background and objects.\nTo address this issue, we disentangle the spatiotemporal features into various\nlatent representations to alleviate the spatiotemporal mismatching between\nbackground and objects. In this work, we introduce event camera to compensate\nfor frame camera, and propose a spatiotemporal-disentangled Gaussian splatting\nframework for high-dynamic scene reconstruction. As for dynamic scene, we\nfigure out that background and objects have appearance discrepancy in\nframe-based spatial features and motion discrepancy in event-based temporal\nfeatures, which motivates us to distinguish the spatiotemporal features between\nbackground and objects via clustering. As for dynamic object, we discover that\nGaussian representations and event data share the consistent spatiotemporal\ncharacteristic, which could serve as a prior to guide the spatiotemporal\ndisentanglement of object Gaussians. Within Gaussian splatting framework, the\ncumulative scene-object disentanglement can improve the spatiotemporal\ndiscrimination between background and objects to render the time-continuous\ndynamic scene. Extensive experiments have been performed to verify the\nsuperiority of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u89e3\u8026\u7684\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u5e27\u76f8\u673a\uff0c\u7528\u4e8e\u9ad8\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u80cc\u666f\u4e0e\u52a8\u6001\u5bf9\u8c61\u65f6\u7a7a\u7279\u5f81\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u8868\u793a\u6a21\u578b\uff08\u5982\u9ad8\u65af\uff09\u76f4\u63a5\u5339\u914d\u52a8\u6001\u573a\u666f\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u5bf9\u8c61\u7684\u6f5c\u5728\u4e0d\u8fde\u7eed\u65f6\u95f4\u7279\u5f81\u548c\u80cc\u666f\u4e0e\u5bf9\u8c61\u7684\u5f02\u8d28\u7a7a\u95f4\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u8865\u507f\u5e27\u76f8\u673a\uff0c\u63d0\u51fa\u65f6\u7a7a\u89e3\u8026\u7684\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u5229\u7528\u805a\u7c7b\u533a\u5206\u80cc\u666f\u4e0e\u5bf9\u8c61\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5229\u7528\u9ad8\u65af\u8868\u793a\u4e0e\u4e8b\u4ef6\u6570\u636e\u7684\u4e00\u81f4\u6027\u6307\u5bfc\u5bf9\u8c61\u7684\u65f6\u7a7a\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u63d0\u9ad8\u80cc\u666f\u4e0e\u5bf9\u8c61\u7684\u65f6\u7a7a\u533a\u5206\u5ea6\uff0c\u5b9e\u73b0\u65f6\u95f4\u8fde\u7eed\u7684\u52a8\u6001\u573a\u666f\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u7a7a\u89e3\u8026\u548c\u4e8b\u4ef6\u76f8\u673a\u8f85\u52a9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u65f6\u7a7a\u7279\u5f81\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2506.23189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23189", "abs": "https://arxiv.org/abs/2506.23189", "authors": ["Mustafa Hakan Kara", "Aysegul Dundar", "U\u011fur G\u00fcd\u00fckbay"], "title": "Trident: Detecting Face Forgeries with Adversarial Triplet Learning", "comment": "11 pages, 3 figures, and 7 tables", "summary": "As face forgeries generated by deep neural networks become increasingly\nsophisticated, detecting face manipulations in digital media has posed a\nsignificant challenge, underscoring the importance of maintaining digital media\nintegrity and combating visual disinformation. Current detection models,\npredominantly based on supervised training with domain-specific data, often\nfalter against forgeries generated by unencountered techniques. In response to\nthis challenge, we introduce \\textit{Trident}, a face forgery detection\nframework that employs triplet learning with a Siamese network architecture for\nenhanced adaptability across diverse forgery methods. \\textit{Trident} is\ntrained on curated triplets to isolate nuanced differences of forgeries,\ncapturing fine-grained features that distinguish pristine samples from\nmanipulated ones while controlling for other variables. To further enhance\ngeneralizability, we incorporate domain-adversarial training with a forgery\ndiscriminator. This adversarial component guides our embedding model towards\nforgery-agnostic representations, improving its robustness to unseen\nmanipulations. In addition, we prevent gradient flow from the classifier head\nto the embedding model, avoiding overfitting induced by artifacts peculiar to\ncertain forgeries. Comprehensive evaluations across multiple benchmarks and\nablation studies demonstrate the effectiveness of our framework. We will\nrelease our code in a GitHub repository.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrident\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5143\u7ec4\u5b66\u4e60\u548cSiamese\u7f51\u7edc\u67b6\u6784\u63d0\u9ad8\u5bf9\u4e0d\u540c\u4f2a\u9020\u65b9\u6cd5\u7684\u9002\u5e94\u6027\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u7684\u4eba\u8138\u4f2a\u9020\u6280\u672f\u65e5\u76ca\u590d\u6742\uff0c\u68c0\u6d4b\u6570\u5b57\u5a92\u4f53\u4e2d\u7684\u4eba\u8138\u4f2a\u9020\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u7ef4\u62a4\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u548c\u6253\u51fb\u89c6\u89c9\u865a\u5047\u4fe1\u606f\u7684\u91cd\u8981\u6027\u51f8\u663e\u3002", "method": "Trident\u6846\u67b6\u91c7\u7528\u4e09\u5143\u7ec4\u5b66\u4e60\u548cSiamese\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548c\u68af\u5ea6\u63a7\u5236\u907f\u514d\u8fc7\u62df\u5408\uff0c\u4ee5\u6355\u6349\u4f2a\u9020\u6837\u672c\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6d88\u878d\u7814\u7a76\u4e2d\uff0cTrident\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Trident\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.23843", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23843", "abs": "https://arxiv.org/abs/2506.23843", "authors": ["Joris Bekkers"], "title": "EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment", "comment": null, "summary": "Understanding team formations and player positioning is crucial for tactical\nanalysis in football (soccer). This paper presents a flexible method for\nformation recognition and player position assignment in football using\npredefined static formation templates and cost minimization from spatiotemporal\ntracking data, called EFPI. Our approach employs linear sum assignment to\noptimally match players to positions within a set of template formations by\nminimizing the total distance between actual player locations and template\npositions, subsequently selecting the formation with the lowest assignment\ncost. To improve accuracy, we scale actual player positions to match the\ndimensions of these formation templates in both width and length. While the\nmethod functions effectively on individual frames, it extends naturally to\nlarger game segments such as complete periods, possession sequences or specific\nintervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we\nincorporate an optional stability parameter that prevents unnecessary formation\nchanges when assignment costs differ only marginally between time segments.\nEFPI is available as open-source code through the unravelsports Python package.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u9635\u578b\u6a21\u677f\u548c\u6210\u672c\u6700\u5c0f\u5316\u7684\u8db3\u7403\u9635\u578b\u8bc6\u522b\u65b9\u6cd5EFPI\uff0c\u901a\u8fc7\u7ebf\u6027\u5206\u914d\u4f18\u5316\u7403\u5458\u4f4d\u7f6e\u5339\u914d\uff0c\u5e76\u652f\u6301\u65f6\u95f4\u6bb5\u7684\u6269\u5c55\u5206\u6790\u3002", "motivation": "\u8db3\u7403\u6218\u672f\u5206\u6790\u9700\u8981\u51c6\u786e\u8bc6\u522b\u7403\u961f\u9635\u578b\u548c\u7403\u5458\u4f4d\u7f6e\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u9884\u5b9a\u4e49\u9759\u6001\u9635\u578b\u6a21\u677f\uff0c\u901a\u8fc7\u7ebf\u6027\u603b\u548c\u5206\u914d\u4f18\u5316\u7403\u5458\u4f4d\u7f6e\u5339\u914d\uff0c\u6700\u5c0f\u5316\u5b9e\u9645\u4f4d\u7f6e\u4e0e\u6a21\u677f\u4f4d\u7f6e\u7684\u8ddd\u79bb\uff0c\u5e76\u5f15\u5165\u7a33\u5b9a\u6027\u53c2\u6570\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u9635\u578b\u53d8\u5316\u3002", "result": "EFPI\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u9635\u578b\uff0c\u9002\u7528\u4e8e\u5355\u5e27\u6216\u66f4\u957f\u65f6\u95f4\u6bb5\u7684\u5206\u6790\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "conclusion": "EFPI\u4e3a\u8db3\u7403\u6218\u672f\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u51c6\u786e\u7684\u9635\u578b\u8bc6\u522b\u5de5\u5177\u3002"}}
{"id": "2506.23196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23196", "abs": "https://arxiv.org/abs/2506.23196", "authors": ["Mona Ahmadian", "Amir Shirian", "Frank Guerin", "Andrew Gilbert"], "title": "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding", "comment": null, "summary": "Real-world videos often contain overlapping events and complex temporal\ndependencies, making multimodal interaction modeling particularly challenging.\nWe introduce DEL, a framework for dense semantic action localization, aiming to\naccurately detect and classify multiple actions at fine-grained temporal\nresolutions in long untrimmed videos. DEL consists of two key modules: the\nalignment of audio and visual features that leverage masked self-attention to\nenhance intra-mode consistency and a multimodal interaction refinement module\nthat models cross-modal dependencies across multiple scales, enabling\nhigh-level semantics and fine-grained details. Our method achieves\nstate-of-the-art performance on multiple real-world Temporal Action\nLocalization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and\nEPIC-Kitchens-100, surpassing previous approaches with notable average mAP\ngains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.", "AI": {"tldr": "DEL\u6846\u67b6\u901a\u8fc7\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u53ca\u591a\u6a21\u6001\u4ea4\u4e92\u7ec6\u5316\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u957f\u672a\u526a\u8f91\u89c6\u9891\u4e2d\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u548c\u5206\u7c7b\u591a\u4e2a\u52a8\u4f5c\uff0c\u5e76\u5728\u591a\u4e2aTAL\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u7684\u91cd\u53e0\u4e8b\u4ef6\u548c\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u4f7f\u5f97\u591a\u6a21\u6001\u4ea4\u4e92\u5efa\u6a21\u5177\u6709\u6311\u6218\u6027\u3002", "method": "DEL\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u5229\u7528\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u6001\u5185\u4e00\u81f4\u6027\u7684\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u53ca\u5efa\u6a21\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\u7684\u591a\u5c3a\u5ea6\u591a\u6a21\u6001\u4ea4\u4e92\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5728UnAV-100\u3001THUMOS14\u3001ActivityNet 1.3\u548cEPIC-Kitchens-100\u6570\u636e\u96c6\u4e0a\uff0cDEL\u7684\u5e73\u5747mAP\u5206\u522b\u63d0\u5347\u4e86+3.3%\u3001+2.6%\u3001+1.2%\u3001+1.7%\uff08\u52a8\u8bcd\uff09\u548c+1.4%\uff08\u540d\u8bcd\uff09\u3002", "conclusion": "DEL\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u4ea4\u4e92\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u8bed\u4e49\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23923", "abs": "https://arxiv.org/abs/2506.23923", "authors": ["Miguel Camacho-S\u00e1nchez", "Fernando Garc\u00eda-Torres", "Jesper John Lisegaard", "Roc\u00edo del Amor", "Sankhya Mohanty", "Valery Naranjo"], "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u63a7\u5236\u6811\u8102\u6ce8\u5165\u8fc7\u7a0b\u4e2d\u7684\u6d41\u52a8\u52a8\u6001\uff0c\u4ee5\u63d0\u9ad8\u590d\u5408\u6750\u6599\u5236\u9020\u7684\u5747\u5300\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u6811\u8102\u6ce8\u5165\uff08RI\uff09\u548c\u6811\u8102\u4f20\u9012\u6a21\u5851\uff08RTM\uff09\u662f\u5236\u9020\u9ad8\u6027\u80fd\u7ea4\u7ef4\u589e\u5f3a\u590d\u5408\u6750\u6599\u7684\u5173\u952e\u5de5\u827a\uff0c\u4f46\u6811\u8102\u6d41\u52a8\u52a8\u6001\u7684\u63a7\u5236\u5bf9\u786e\u4fdd\u7ea4\u7ef4\u5747\u5300\u6d78\u6e0d\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u907f\u514d\u5b54\u9699\u548c\u5e72\u6591\u5f71\u54cd\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "method": "\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u7a0b\u6a21\u62df\u5efa\u7acb\u7b56\u7565\uff0c\u4ee5\u540c\u6b65\u4e24\u4e2a\u6811\u8102\u5165\u53e3\u548c\u5355\u4e2a\u51fa\u53e3\u7684\u6d41\u52a8\u524d\u6cbf\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cRL\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u6d41\u52a8\u6536\u655b\uff0c\u63d0\u9ad8\u5de5\u827a\u63a7\u5236\u548c\u4ea7\u54c1\u8d28\u91cf\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u5408\u6750\u6599\u5236\u9020\u4e2d\u5177\u6709\u6539\u5584\u5de5\u827a\u63a7\u5236\u548c\u4ea7\u54c1\u8d28\u91cf\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23209", "abs": "https://arxiv.org/abs/2506.23209", "authors": ["Chia-Wen Huang", "Haw Hwai", "Chien-Chang Lee", "Pei-Yuan Wu"], "title": "A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans", "comment": "8 pages, 1 figure, 3 tables. Published in IEEE ISBI 2025. This\n  version corrects citation numbering errors", "summary": "Timely and accurate diagnosis of appendicitis is critical in clinical\nsettings to prevent serious complications. While CT imaging remains the\nstandard diagnostic tool, the growing number of cases can overwhelm\nradiologists, potentially causing delays. In this paper, we propose a deep\nlearning model that leverages 3D CT scans for appendicitis classification,\nincorporating Slice Attention mechanisms guided by external 2D datasets to\nenhance small lesion detection. Additionally, we introduce a hierarchical\nclassification framework using pre-trained 2D models to differentiate between\nsimple and complicated appendicitis. Our approach improves AUC by 3% for\nappendicitis and 5.9% for complicated appendicitis, offering a more efficient\nand reliable diagnostic solution compared to previous work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D CT\u626b\u63cf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u5207\u7247\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u76842D\u6a21\u578b\uff0c\u7528\u4e8e\u9611\u5c3e\u708e\u7684\u51c6\u786e\u5206\u7c7b\u548c\u590d\u6742\u9611\u5c3e\u708e\u7684\u533a\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u9611\u5c3e\u708e\u7684\u53ca\u65f6\u51c6\u786e\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46CT\u626b\u63cf\u6570\u91cf\u589e\u52a0\u53ef\u80fd\u5bfc\u81f4\u653e\u5c04\u79d1\u533b\u751f\u8d85\u8d1f\u8377\uff0c\u5f15\u53d1\u5ef6\u8bef\u3002", "method": "\u5229\u75283D CT\u626b\u63cf\u548c\u5207\u7247\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u76842D\u6a21\u578b\u8fdb\u884c\u5206\u5c42\u5206\u7c7b\u3002", "result": "\u9611\u5c3e\u708e\u5206\u7c7b\u7684AUC\u63d0\u5347\u4e863%\uff0c\u590d\u6742\u9611\u5c3e\u708e\u5206\u7c7b\u7684AUC\u63d0\u5347\u4e865.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u53ef\u9760\uff0c\u4e3a\u9611\u5c3e\u708e\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23257", "abs": "https://arxiv.org/abs/2506.23257", "authors": ["Chongke Bi", "Xin Gao", "Baofeng Fu", "Yuheng Zhao", "Siming Chen", "Ying Zhao", "Yunhai Wang"], "title": "PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation", "comment": null, "summary": "Large-scale simulations on supercomputers have become important tools for\nusers. However, their scalability remains a problem due to the huge\ncommunication cost among parallel processes. Most of the existing communication\nlatency analysis methods rely on the physical link layer information, which is\nonly available to administrators. In this paper, a framework called PCLVis is\nproposed to help general users analyze process communication latency (PCL)\nevents. Instead of the physical link layer information, the PCLVis uses the MPI\nprocess communication data for the analysis. First, a spatial PCL event\nlocating method is developed. All processes with high correlation are\nclassified into a single cluster by constructing a process-correlation tree.\nSecond, the propagation path of PCL events is analyzed by constructing a\ncommunication-dependency-based directed acyclic graph (DAG), which can help\nusers interactively explore a PCL event from the temporal evolution of a\nlocated PCL event cluster. In this graph, a sliding window algorithm is\ndesigned to generate the PCL events abstraction. Meanwhile, a new glyph called\nthe communication state glyph (CS-Glyph) is designed for each process to show\nits communication states, including its in/out messages and load balance. Each\nleaf node can be further unfolded to view additional information. Third, a PCL\nevent attribution strategy is formulated to help users optimize their\nsimulations. The effectiveness of the PCLVis framework is demonstrated by\nanalyzing the PCL events of several simulations running on the TH-1A\nsupercomputer. By using the proposed framework, users can greatly improve the\nefficiency of their simulations.", "AI": {"tldr": "PCLVis\u6846\u67b6\u5e2e\u52a9\u7528\u6237\u5206\u6790\u8d85\u7ea7\u8ba1\u7b97\u673a\u6a21\u62df\u4e2d\u7684\u8fdb\u7a0b\u901a\u4fe1\u5ef6\u8fdf\uff08PCL\uff09\u4e8b\u4ef6\uff0c\u65e0\u9700\u7269\u7406\u94fe\u8def\u5c42\u4fe1\u606f\uff0c\u901a\u8fc7MPI\u6570\u636e\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u4f18\u5316\u6a21\u62df\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u8d85\u7ea7\u8ba1\u7b97\u673a\u6a21\u62df\u7684\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7ba1\u7406\u5458\u624d\u80fd\u83b7\u53d6\u7684\u7269\u7406\u94fe\u8def\u5c42\u4fe1\u606f\uff0c\u666e\u901a\u7528\u6237\u96be\u4ee5\u5206\u6790\u3002", "method": "PCLVis\u5229\u7528MPI\u901a\u4fe1\u6570\u636e\uff0c\u901a\u8fc7\u8fdb\u7a0b\u76f8\u5173\u6027\u6811\u805a\u7c7b\u3001\u901a\u4fe1\u4f9d\u8d56DAG\u5206\u6790\u548cCS-Glyph\u53ef\u89c6\u5316\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0fPCL\u4e8b\u4ef6\u5206\u6790\u3002", "result": "\u5728TH-1A\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\uff0cPCLVis\u80fd\u6709\u6548\u5b9a\u4f4d\u548c\u5206\u6790PCL\u4e8b\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u62df\u6548\u7387\u3002", "conclusion": "PCLVis\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7269\u7406\u94fe\u8def\u5c42\u4fe1\u606f\u7684PCL\u5206\u6790\u5de5\u5177\uff0c\u4f18\u5316\u4e86\u5927\u89c4\u6a21\u6a21\u62df\u7684\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2506.23271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23271", "abs": "https://arxiv.org/abs/2506.23271", "authors": ["Jinxing Zhou", "Zhihui Li", "Yongqiang Yu", "Yanghao Zhou", "Ruohao Guo", "Guangyao Li", "Yuxin Mao", "Mingfei Han", "Xiaojun Chang", "Meng Wang"], "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation", "comment": "Technical Report", "summary": "We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple\nand memory-efficient method for adapting large-scale pretrained transformer\nmodels to downstream audio-visual tasks. Instead of sequentially modifying the\noutput feature distribution of the transformer backbone, Mettle utilizes a\nlightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in\nparallel the intact audio or visual features embedded by each transformer layer\ninto compact meta-tokens. This distillation process considers both pretrained\nknowledge preservation and task-specific adaptation. The obtained meta-tokens\ncan be directly applied to classification tasks, such as audio-visual event\nlocalization and audio-visual video parsing. To further support fine-grained\nsegmentation tasks, such as audio-visual segmentation, we introduce a\n\\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual\nmeta-tokens distilled from the top transformer layer to guide feature\nadaptation in earlier layers. Extensive experiments on multiple audiovisual\nbenchmarks demonstrate that our method significantly reduces memory usage and\ntraining time while maintaining parameter efficiency and competitive accuracy.", "AI": {"tldr": "Mettle\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5185\u5b58\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u84b8\u998f\u97f3\u9891\u6216\u89c6\u89c9\u7279\u5f81\u4e3a\u5143\u6807\u8bb0\uff0c\u9002\u5e94\u5927\u89c4\u6a21\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u5230\u4e0b\u6e38\u89c6\u542c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9002\u5e94\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u5185\u5b58\u5360\u7528\u9ad8\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7Layer-Centric Distillation\u6a21\u5757\u5e76\u884c\u84b8\u998f\u7279\u5f81\u4e3a\u5143\u6807\u8bb0\uff0c\u5e76\u5f15\u5165Meta-Token Injection\u6a21\u5757\u652f\u6301\u7ec6\u7c92\u5ea6\u5206\u5272\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u542c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u548c\u7ade\u4e89\u529b\u51c6\u786e\u6027\u3002", "conclusion": "Mettle\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5185\u5b58\u53cb\u597d\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u542c\u4efb\u52a1\u3002"}}
{"id": "2506.23783", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23783", "abs": "https://arxiv.org/abs/2506.23783", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u590d\u6742\u5ea6Vision Mamba\u7f51\u7edc\u7684\u9ad8\u6548RGB-Event\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6Mamba-FETrack V2\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7Prompt Generator\u548cFEMamba\u4e3b\u5e72\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u8ddf\u8e2a\u7b97\u6cd5\u4f9d\u8d56\u9ad8\u590d\u6742\u5ea6Vision Transformer\u67b6\u6784\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u8de8\u6a21\u6001\u4ea4\u4e92\u6548\u679c\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7Prompt Generator\u751f\u6210\u6a21\u6001\u7279\u5b9a\u63d0\u793a\u5411\u91cf\uff0c\u7ed3\u5408Vision Mamba\u7684FEMamba\u4e3b\u5e72\u5b9e\u73b0\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2aRGB-Event\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "conclusion": "Mamba-FETrack V2\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u8ddf\u8e2a\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23575", "abs": "https://arxiv.org/abs/2506.23575", "authors": ["Nuo Chen", "Chao Xiao", "Yimian Dai", "Shiman He", "Miao Li", "Wei An"], "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline", "comment": null, "summary": "Small object detection (SOD) in anti-UAV task is a challenging problem due to\nthe small size of UAVs and complex backgrounds. Traditional frame-based cameras\nstruggle to detect small objects in complex environments due to their low frame\nrates, limited dynamic range, and data redundancy. Event cameras, with\nmicrosecond temporal resolution and high dynamic range, provide a more\neffective solution for SOD. However, existing event-based object detection\ndatasets are limited in scale, feature large targets size, and lack diverse\nbackgrounds, making them unsuitable for SOD benchmarks. In this paper, we\nintroduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),\nthe first large-scale, highly diverse benchmark for anti-UAV tasks. It includes\n147 sequences with over 2.3 million event-level annotations, featuring\nextremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse\nscenarios such as urban clutter and extreme lighting conditions. Furthermore,\nbased on the observation that small moving targets form continuous curves in\nspatiotemporal event point clouds, we propose Event based Sparse Segmentation\nNetwork (EV-SpSegNet), a novel baseline for event segmentation in point cloud\nspace, along with a Spatiotemporal Correlation (STC) loss that leverages motion\ncontinuity to guide the network in retaining target events. Extensive\nexperiments on the EV-UAV dataset demonstrate the superiority of our method and\nprovide a benchmark for future research in EVSOD. The dataset and code are at\nhttps://github.com/ChenYichen9527/Ev-UAV.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6EV-UAV\uff0c\u5e76\u63d0\u51fa\u4e86EV-SpSegNet\u65b9\u6cd5\u548cSTC\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u89e3\u51b3\u53cd\u65e0\u4eba\u673a\u4efb\u52a1\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u68c0\u6d4b\u5c0f\u76ee\u6807\uff08\u5982\u65e0\u4eba\u673a\uff09\u5b58\u5728\u56f0\u96be\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u5fae\u79d2\u7ea7\u5206\u8fa8\u7387\u66f4\u9002\u5408\u6b64\u7c7b\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u76ee\u6807\u5927\u4e14\u80cc\u666f\u5355\u4e00\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86EV-UAV\u6570\u636e\u96c6\uff0c\u5305\u542b147\u4e2a\u5e8f\u5217\u548c230\u4e07\u4e8b\u4ef6\u7ea7\u6807\u6ce8\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86EV-SpSegNet\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u7a7a\u4e8b\u4ef6\u70b9\u4e91\u4e2d\u7684\u8fd0\u52a8\u8fde\u7eed\u6027\uff0c\u7ed3\u5408STC\u635f\u5931\u51fd\u6570\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u3002", "result": "\u5728EV-UAV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "conclusion": "EV-UAV\u6570\u636e\u96c6\u548cEV-SpSegNet\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u53cd\u65e0\u4eba\u673a\u4efb\u52a1\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23972", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23972", "abs": "https://arxiv.org/abs/2506.23972", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "Visual and Memory Dual Adapter for Multi-Modal Object Tracking", "comment": null, "summary": "Prompt-learning-based multi-modal trackers have achieved promising progress\nby employing lightweight visual adapters to incorporate auxiliary modality\nfeatures into frozen foundation models. However, existing approaches often\nstruggle to learn reliable prompts due to limited exploitation of critical cues\nacross frequency and temporal domains. In this paper, we propose a novel visual\nand memory dual adapter (VMDA) to construct more robust and discriminative\nrepresentations for multi-modal tracking. Specifically, we develop a simple but\neffective visual adapter that adaptively transfers discriminative cues from\nauxiliary modality to dominant modality by jointly modeling the frequency,\nspatial, and channel-wise features. Additionally, we design the memory adapter\ninspired by the human memory mechanism, which stores global temporal cues and\nperforms dynamic update and retrieval operations to ensure the consistent\npropagation of reliable temporal information across video sequences. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,\nand RGB-Event tracking. Code and models are available at\nhttps://github.com/xuboyue1999/mmtrack.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u548c\u8bb0\u5fc6\u53cc\u91cd\u9002\u914d\u5668\uff08VMDA\uff09\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u9891\u7387\u3001\u7a7a\u95f4\u548c\u901a\u9053\u7279\u5f81\uff0c\u4ee5\u53ca\u5229\u7528\u8bb0\u5fc6\u673a\u5236\u5b58\u50a8\u5168\u5c40\u65f6\u95f4\u7ebf\u7d22\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8ddf\u8e2a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9891\u7387\u548c\u65f6\u95f4\u57df\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u5173\u952e\u7ebf\u7d22\uff0c\u5bfc\u81f4\u63d0\u793a\u5b66\u4e60\u4e0d\u53ef\u9760\u3002", "method": "\u8bbe\u8ba1\u4e86\u89c6\u89c9\u9002\u914d\u5668\u548c\u8bb0\u5fc6\u9002\u914d\u5668\uff0c\u524d\u8005\u81ea\u9002\u5e94\u5730\u8f6c\u79fb\u8f85\u52a9\u6a21\u6001\u7684\u5224\u522b\u7ebf\u7d22\uff0c\u540e\u8005\u5b58\u50a8\u5168\u5c40\u65f6\u95f4\u7ebf\u7d22\u5e76\u52a8\u6001\u66f4\u65b0\u3002", "result": "\u5728RGB-Thermal\u3001RGB-Depth\u548cRGB-Event\u7b49\u591a\u6a21\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VMDA\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bb0\u5fc6\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u5224\u522b\u6027\u3002"}}
{"id": "2506.24063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24063", "abs": "https://arxiv.org/abs/2506.24063", "authors": ["Deng Li", "Aming Wu", "Yang Li", "Yaowei Wang", "Yahong Han"], "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios", "comment": null, "summary": "In practice, environments constantly change over time and space, posing\nsignificant challenges for object detectors trained based on a closed-set\nassumption, i.e., training and test data share the same distribution. To this\nend, continual test-time adaptation has attracted much attention, aiming to\nimprove detectors' generalization by fine-tuning a few specific parameters,\ne.g., BatchNorm layers. However, based on a small number of test images,\nfine-tuning certain parameters may affect the representation ability of other\nfixed parameters, leading to performance degradation. Instead, we explore a new\nmechanism, i.e., converting the fine-tuning process to a specific-parameter\ngeneration. Particularly, we first design a dual-path LoRA-based domain-aware\nadapter that disentangles features into domain-invariant and domain-specific\ncomponents, enabling efficient adaptation. Additionally, a conditional\ndiffusion-based parameter generation mechanism is presented to synthesize the\nadapter's parameters based on the current environment, preventing the\noptimization from getting stuck in local optima. Finally, we propose a\nclass-centered optimal transport alignment method to mitigate catastrophic\nforgetting. Extensive experiments conducted on various continuous domain\nadaptive object detection tasks demonstrate the effectiveness. Meanwhile,\nvisualization results show that the representation extracted by the generated\nparameters can capture more object-related information and strengthen the\ngeneralization ability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u5fae\u8c03\u8fc7\u7a0b\u8f6c\u6362\u4e3a\u7279\u5b9a\u53c2\u6570\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u5c11\u91cf\u6d4b\u8bd5\u56fe\u50cf\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u6570\u636e\u7684\u5206\u5e03\u4f1a\u968f\u65f6\u95f4\u7a7a\u95f4\u53d8\u5316\uff0c\u4f20\u7edf\u57fa\u4e8e\u5c01\u95ed\u96c6\u5047\u8bbe\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u96be\u4ee5\u9002\u5e94\u8fd9\u79cd\u53d8\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u8def\u5f84LoRA\u57df\u611f\u77e5\u9002\u914d\u5668\u5206\u79bb\u7279\u5f81\uff0c\u63d0\u51fa\u6761\u4ef6\u6269\u6563\u53c2\u6570\u751f\u6210\u673a\u5236\uff0c\u5e76\u5f15\u5165\u7c7b\u4e2d\u5fc3\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u65b9\u6cd5\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u591a\u79cd\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u53c2\u6570\u63d0\u53d6\u7684\u8868\u5f81\u80fd\u6355\u83b7\u66f4\u591a\u76ee\u6807\u76f8\u5173\u4fe1\u606f\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u751f\u6210\u548c\u7279\u5f81\u5206\u79bb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
