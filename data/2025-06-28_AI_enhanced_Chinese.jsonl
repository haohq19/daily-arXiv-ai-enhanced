{"id": "2506.20954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20954", "abs": "https://arxiv.org/abs/2506.20954", "authors": ["Xueming Liu", "Lin Li", "Xiang Zhou", "Qingrui Zhang", "Tianjiang Hu"], "title": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard Sensing", "comment": "8 Pages, 7 figures. Accepted by RA-L", "summary": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65e0\u4eba\u673a\u534f\u540c\u73af\u7ed5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u5b9a\u4f4d\u7cfb\u7edf\u7684\u60c5\u51b5\u4e0b\u8ddf\u8e2a\u79fb\u52a8\u76ee\u6807\uff0c\u901a\u8fc7\u5f02\u6784\u611f\u77e5\u7b56\u7565\u548c\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e0b\u5bf9\u79fb\u52a8\u76ee\u6807\u7684\u534f\u540c\u8ddf\u8e2a\u95ee\u9898\uff0c\u63d0\u5347\u641c\u7d22\u4e0e\u6551\u63f4\u7b49\u5b9e\u9645\u5e94\u7528\u7684\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u6027\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u878d\u5408\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e0e\u8ddd\u79bb\u6d4b\u91cf\uff0c\u8bbe\u8ba1\u4e8b\u4ef6\u89e6\u53d1\u7684\u5206\u5e03\u5f0f\u5361\u5c14\u66fc\u6ee4\u6ce2\u4ee5\u5e94\u5bf9\u89c6\u89c9\u906e\u6321\uff0c\u5e76\u57fa\u4e8e\u632f\u8361\u5668\u5b9e\u73b0\u81ea\u4e3b\u7f16\u961f\u98de\u884c\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u5ba4\u5185\u5916\u906e\u6321\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5bb9\u9519\u6027\uff0c\u9002\u7528\u4e8e\u641c\u7d22\u4e0e\u6551\u63f4\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u65e0\u4eba\u673a\u534f\u540c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.20893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20893", "abs": "https://arxiv.org/abs/2506.20893", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "comment": null, "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8f93\u51fa\u91cd\u52a0\u6743\u9057\u5fd8\u65b9\u6cd5RWFT\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u4ece\u5206\u7c7b\u5668\u4e2d\u5220\u9664\u7279\u5b9a\u7c7b\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5728\u9884\u6d4b\u672a\u5b66\u4e60\u7c7b\u522b\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f3a\u5236\u6267\u884c\u7528\u6237\u5220\u9664\u6743\u5229\u548c\u51cf\u5c11\u6709\u5bb3\u6216\u504f\u89c1\u9884\u6d4b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u907f\u514d\u6602\u8d35\u7684\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u9884\u6d4b\u6982\u7387\u8d28\u91cf\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5bf9MIA-NN\u653b\u51fb\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u603b\u53d8\u5dee\u8ddd\u79bb\u7684\u65b0\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRWFT\u5728\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684TV\u8ddd\u79bb\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347\u4e862.79%\u548c111.45%\u3002", "conclusion": "RWFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u76f8\u5f53\u7684\u6548\u679c\u3002"}}
{"id": "2506.20979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20979", "abs": "https://arxiv.org/abs/2506.20979", "authors": ["Weichen Dai", "Kangcheng Ma", "Jiaxin Wang", "Kecen Pan", "Yuhang Ming", "Hua Zhang", "Wanzeng Kong"], "title": "3D Scene-Camera Representation with Joint Camera Photometric Optimization", "comment": null, "summary": "Representing scenes from multi-view images is a crucial task in computer\nvision with extensive applications. However, inherent photometric distortions\nin the camera imaging can significantly degrade image quality. Without\naccounting for these distortions, the 3D scene representation may inadvertently\nincorporate erroneous information unrelated to the scene, diminishing the\nquality of the representation. In this paper, we propose a novel 3D\nscene-camera representation with joint camera photometric optimization. By\nintroducing internal and external photometric model, we propose a full\nphotometric model and corresponding camera representation. Based on\nsimultaneously optimizing the parameters of the camera representation, the\nproposed method effectively separates scene-unrelated information from the 3D\nscene representation. Additionally, during the optimization of the photometric\nparameters, we introduce a depth regularization to prevent the 3D scene\nrepresentation from fitting scene-unrelated information. By incorporating the\ncamera model as part of the mapping process, the proposed method constructs a\ncomplete map that includes both the scene radiance field and the camera\nphotometric model. Experimental results demonstrate that the proposed method\ncan achieve high-quality 3D scene representations, even under conditions of\nimaging degradation, such as vignetting and dirt.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76f8\u673a\u5149\u5ea6\u4f18\u5316\u76843D\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5149\u5ea6\u6a21\u578b\u548c\u6df1\u5ea6\u6b63\u5219\u5316\uff0c\u6709\u6548\u5206\u79bb\u4e0e\u573a\u666f\u65e0\u5173\u7684\u4fe1\u606f\uff0c\u63d0\u5347\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u76f8\u673a\u6210\u50cf\u4e2d\u7684\u5149\u5ea6\u5931\u771f\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u8fdb\u800c\u5f71\u54cd3D\u573a\u666f\u8868\u793a\u7684\u51c6\u786e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u8fd9\u4e9b\u5931\u771f\uff0c\u5bfc\u81f4\u8868\u793a\u4e2d\u5305\u542b\u65e0\u5173\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u5b8c\u6574\u7684\u5149\u5ea6\u6a21\u578b\u548c\u76f8\u673a\u8868\u793a\uff0c\u8054\u5408\u4f18\u5316\u76f8\u673a\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u6df1\u5ea6\u6b63\u5219\u5316\uff0c\u9632\u6b623D\u573a\u666f\u8868\u793a\u62df\u5408\u65e0\u5173\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u50cf\u9000\u5316\uff08\u5982\u6e10\u6655\u548c\u6c61\u57a2\uff09\u6761\u4ef6\u4e0b\u4ecd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u8868\u793a\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76f8\u673a\u5149\u5ea6\u548c\u573a\u666f\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.20995", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20995", "abs": "https://arxiv.org/abs/2506.20995", "authors": ["Akio Hayakawa", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance", "comment": null, "summary": "We propose a novel step-by-step video-to-audio generation method that\nsequentially produces individual audio tracks, each corresponding to a specific\nsound event in the video. Our approach mirrors traditional Foley workflows,\naiming to capture all sound events induced by a given video comprehensively.\nEach generation step is formulated as a guided video-to-audio synthesis task,\nconditioned on a target text prompt and previously generated audio tracks. This\ndesign is inspired by the idea of concept negation from prior compositional\ngeneration frameworks. To enable this guided generation, we introduce a\ntraining framework that leverages pre-trained video-to-audio models and\neliminates the need for specialized paired datasets, allowing training on more\naccessible data. Experimental results demonstrate that our method generates\nmultiple semantically distinct audio tracks for a single input video, leading\nto higher-quality composite audio synthesis than existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9010\u6b65\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6b65\u751f\u6210\u7279\u5b9a\u58f0\u97f3\u4e8b\u4ef6\u7684\u97f3\u9891\u8f68\u9053\uff0c\u6a21\u62df\u4f20\u7edfFoley\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u65e8\u5728\u5168\u9762\u6355\u6349\u89c6\u9891\u4e2d\u6240\u6709\u58f0\u97f3\u4e8b\u4ef6\uff0c\u63d0\u5347\u5408\u6210\u97f3\u9891\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5206\u6b65\u89c6\u9891\u5230\u97f3\u9891\u5408\u6210\u4efb\u52a1\uff0c\u7ed3\u5408\u76ee\u6807\u6587\u672c\u63d0\u793a\u548c\u5148\u524d\u751f\u6210\u7684\u97f3\u9891\u8f68\u9053\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u907f\u514d\u4e13\u7528\u914d\u5bf9\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4e3a\u5355\u4e00\u89c6\u9891\u751f\u6210\u591a\u4e2a\u8bed\u4e49\u4e0d\u540c\u7684\u97f3\u9891\u8f68\u9053\uff0c\u5408\u6210\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u8f68\u9053\u5408\u6210\uff0c\u65e0\u9700\u4e13\u7528\u6570\u636e\u96c6\u3002"}}
{"id": "2506.21035", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21035", "abs": "https://arxiv.org/abs/2506.21035", "authors": ["Haodong Lu", "Chongyang Zhao", "Jason Xue", "Lina Yao", "Kristen Moore", "Dong Gong"], "title": "Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning", "comment": "Preprint", "summary": "Continual learning (CL) with large pre-trained models is challenged by\ncatastrophic forgetting and task interference. Existing LoRA-based\nMixture-of-Experts (MoE) approaches mitigate forgetting by assigning and\nfreezing task-specific adapters, but suffer from interference, redundancy, and\nambiguous routing due to coarse adapter-level selection. However, this design\nintroduces three key challenges: 1) Interference: Activating full LoRA experts\nper input leads to subspace interference and prevents selective reuse of useful\ncomponents across tasks. 2) Redundancy: Newly added experts often duplicate or\ncontradict existing knowledge due to unnecessary activation of unrelated ranks\nand insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features\nacross tasks confuse the router, resulting in unstable expert assignments. As\nmore experts accumulate, earlier task routing degrades, accelerating\nforgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with\nself-activated and sparse rank activation for CL. Unlike mixing multiple\nlow-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,\neach treated as an independent expert, enabling fine-grained mixture of rank-1\nexpert utilization while mitigating interference and redundancy. To avoid\nambiguous routing, we propose that each rank-1 expert can infer its own\nrelevance via intermediate activations. Coupled with our proposed rank pruning\nand activation budgets, MoRA adaptively selects a sparse mixture of ranks per\ninput. We validate MoRA on continual learning tasks with CLIP and large\nlanguage models (LLMs), analyzing both in-domain learning and out-of-domain\nforgetting/generalization during fine-tuning. MoRA shows significant\neffectiveness on enhancing CL with PTMs, and improving generalization while\nmitigating forgetting.", "AI": {"tldr": "MoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMixture-of-Rank Adaptive learning\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684rank-1\u4e13\u5bb6\u6df7\u5408\u548c\u81ea\u9002\u5e94\u7a00\u758f\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u5e72\u6270\u3001\u5197\u4f59\u548c\u8def\u7531\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTMs\uff09\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u548c\u4efb\u52a1\u5e72\u6270\u7684\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8eLoRA\u7684Mixture-of-Experts\uff08MoE\uff09\u65b9\u6cd5\u901a\u8fc7\u51bb\u7ed3\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u7f13\u89e3\u9057\u5fd8\uff0c\u4f46\u5b58\u5728\u5e72\u6270\u3001\u5197\u4f59\u548c\u8def\u7531\u6a21\u7cca\u95ee\u9898\u3002", "method": "MoRA\u5c06\u6bcf\u4e2arank-r\u66f4\u65b0\u5206\u89e3\u4e3ar\u4e2arank-1\u7ec4\u4ef6\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u4f5c\u4e3a\u72ec\u7acb\u4e13\u5bb6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6df7\u5408\u3002\u901a\u8fc7\u4e2d\u95f4\u6fc0\u6d3b\u63a8\u65ad\u4e13\u5bb6\u76f8\u5173\u6027\uff0c\u5e76\u7ed3\u5408rank\u526a\u679d\u548c\u6fc0\u6d3b\u9884\u7b97\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u7a00\u758frank\u7ec4\u5408\u3002", "result": "\u5728CLIP\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cMoRA\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u679c\uff0c\u51cf\u5c11\u4e86\u9057\u5fd8\uff0c\u5e76\u6539\u5584\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MoRA\u901a\u8fc7\u7ec6\u7c92\u5ea6rank\u6df7\u5408\u548c\u81ea\u9002\u5e94\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5e72\u6270\u548c\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.21103", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21103", "abs": "https://arxiv.org/abs/2506.21103", "authors": ["Tim Lawson", "Laurence Aitchison"], "title": "Learning to Skip the Middle Layers of Transformers", "comment": "11 pages, 2 figures", "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8df3\u8fc7Transformer\u4e2d\u95f4\u5c42\u7684\u65b0\u67b6\u6784\uff0c\u4f46\u672a\u80fd\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4f18\u4e8e\u5bc6\u96c6\u57fa\u7ebf\u7684\u5e73\u8861\u3002", "motivation": "\u57fa\u4e8e\u4e2d\u95f4\u5c42\u5197\u4f59\u6027\u548c\u65e9\u671f\u5c42\u4fe1\u606f\u805a\u5408\u7684\u89c2\u5bdf\uff0c\u8bd5\u56fe\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7\u4e2d\u95f4\u5c42\u6765\u63d0\u5347Transformer\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8df3\u8fc7\u5bf9\u79f0\u7684\u4e2d\u95f4\u5c42\u5757\uff0c\u5e76\u91c7\u7528\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u8df3\u8fc7\u4f4d\u7f6e\u7684\u5173\u6ce8\uff0c\u540c\u65f6\u901a\u8fc7\u6b63\u5219\u5316\u635f\u5931\u63a7\u5236\u7a00\u758f\u6027\u3002", "result": "\u5728\u7814\u7a76\u7684\u89c4\u6a21\u4e0b\uff0c\u672a\u80fd\u5b9e\u73b0\u9a8c\u8bc1\u4ea4\u53c9\u71b5\u4e0eFLOPs\u4e4b\u95f4\u7684\u66f4\u597d\u6743\u8861\u3002", "conclusion": "\u867d\u7136\u65b9\u6cd5\u65b0\u9896\uff0c\u4f46\u672a\u8fbe\u5230\u9884\u671f\u6548\u679c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.21042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21042", "abs": "https://arxiv.org/abs/2506.21042", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "title": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability", "comment": "Accepted by ICCV2025. arXiv admin note: text overlap with\n  arXiv:2503.02101", "summary": "Detectors often suffer from performance drop due to domain gap between\ntraining and testing data. Recent methods explore diffusion models applied to\ndomain generalization (DG) and adaptation (DA) tasks, but still struggle with\nlarge inference costs and have not yet fully leveraged the capabilities of\ndiffusion models. We propose to tackle these problems by extracting\nintermediate features from a single-step diffusion process, improving feature\ncollection and fusion to reduce inference time by 75% while enhancing\nperformance on source domains (i.e., Fitness). Then, we construct an\nobject-centered auxiliary branch by applying box-masked images with class\nprompts to extract robust and domain-invariant features that focus on object.\nWe also apply consistency loss to align the auxiliary and ordinary branch,\nbalancing fitness and generalization while preventing overfitting and improving\nperformance on target domains (i.e., Generalization). Furthermore, within a\nunified framework, standard detectors are guided by diffusion detectors through\nfeature-level and object-level alignment on source domains (for DG) and\nunlabeled target domains (for DA), thereby improving cross-domain detection\nperformance (i.e., Transferability). Our method achieves competitive results on\n3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO\ngeneralization benchmark demonstrate that our method maintains significant\nadvantages and show remarkable efficiency in large domain shifts and low-data\nscenarios. Our work shows the superiority of applying diffusion models to\ndomain generalized and adaptive detection tasks and offers valuable insights\nfor visual perception tasks across diverse domains. The code is available at\n\\href{https://github.com/heboyong/Fitness-Generalization-Transferability}{Fitness-Generalization-Transferability}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6269\u6563\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u63a8\u7406\u65f6\u95f475%\uff0c\u5e76\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u8f85\u52a9\u5206\u652f\u548c\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u8de8\u57df\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u68c0\u6d4b\u5668\u56e0\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u57df\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u63a8\u7406\u6210\u672c\u3002", "method": "\u63d0\u53d6\u5355\u6b65\u6269\u6563\u8fc7\u7a0b\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u6784\u5efa\u5bf9\u8c61\u4e2d\u5fc3\u8f85\u52a9\u5206\u652f\uff0c\u5e94\u7528\u4e00\u81f4\u6027\u635f\u5931\u5bf9\u9f50\u5206\u652f\uff0c\u5e76\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u901a\u8fc7\u7279\u5f81\u548c\u5bf9\u8c61\u7ea7\u5bf9\u9f50\u6307\u5bfc\u6807\u51c6\u68c0\u6d4b\u5668\u3002", "result": "\u57283\u4e2aDA\u548c5\u4e2aDG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u5e76\u5728COCO\u6cdb\u5316\u57fa\u51c6\u4e2d\u8868\u73b0\u9ad8\u6548\u3002", "conclusion": "\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u57df\u6cdb\u5316\u548c\u81ea\u9002\u5e94\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u8de8\u57df\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.21129", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21129", "abs": "https://arxiv.org/abs/2506.21129", "authors": ["Deepak Kumar Panda", "Adolfo Perrusquia", "Weisi Guo"], "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks", "comment": null, "summary": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6297\u8106\u5f31\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u6b65\u5bf9\u6297\u6270\u52a8\u8bad\u7ec3RL\u7b56\u7565\uff0c\u63d0\u5347\u5176\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684RL\u7b56\u7565\u6613\u53d7\u89c2\u6d4b\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\u5916\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\uff0c\u5bfc\u81f4\u51b3\u7b56\u4e0d\u5b89\u5168\u6216\u6b21\u4f18\u3002", "method": "\u5f15\u5165\u6a21\u62df\u653b\u51fb\u8005\u9010\u6b65\u589e\u52a0\u6270\u52a8\u5f3a\u5ea6\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u6700\u5c0f\u5316\u8fdb\u884c\u4e13\u5bb6\u6307\u5bfc\u7684\u6279\u8bc4\u5668\u5bf9\u9f50\u3002", "result": "\u5728\u65e0\u4eba\u673a\u907f\u969c\u573a\u666f\u4e2d\uff0c\u6297\u8106\u5f31\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u6807\u51c6RL\u57fa\u7ebf\uff0c\u7d2f\u79ef\u5956\u52b1\u63d0\u9ad815%\uff0c\u51b2\u7a81\u4e8b\u4ef6\u51cf\u5c1130%\u3002", "conclusion": "\u6297\u8106\u5f31\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u5a01\u80c1\u73af\u5883\u4e2d\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u53ef\u884c\u6027\u3002"}}
{"id": "2506.21277", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21277", "abs": "https://arxiv.org/abs/2506.21277", "authors": ["Qize Yang", "Shimin Yao", "Weixuan Chen", "Shenghao Fu", "Detao Bai", "Jiaxing Zhao", "Boyuan Sun", "Bowen Yin", "Xihan Wei", "Jingren Zhou"], "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context", "comment": null, "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u548c\u6377\u5f84\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5IntentBench\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u65f6\u9762\u4e34\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u548c\u6377\u5f84\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u5956\u52b1\u3001\u683c\u5f0f\u5956\u52b1\u3001\u51c6\u786e\u6027\u5956\u52b1\u548c\u903b\u8f91\u5956\u52b1\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u5168\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u610f\u56fe\u548c\u60c5\u611f\u7684\u7406\u89e3\u3002"}}
{"id": "2506.21142", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21142", "abs": "https://arxiv.org/abs/2506.21142", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks", "comment": null, "summary": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u80fd\u9003\u907f\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u7684\u9690\u853d\u5bf9\u6297\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u68c0\u6d4b\u8fd9\u4e9b\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5728\u6c11\u7528\u7a7a\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u65b0\u578b\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u9690\u853d\u5bf9\u6297\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u591a\u7c7bIDS\u5206\u7c7b\u5668\uff0c\u7136\u540e\u4f7f\u7528cGAN\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u4f7f\u5176\u88ab\u8bef\u5206\u7c7b\u4e3a\u826f\u6027\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u7edf\u8ba1\u76f8\u4f3c\u6027\u3002\u6700\u540e\u901a\u8fc7CVAE\u68c0\u6d4b\u8fd9\u4e9b\u5bf9\u6297\u6837\u672c\u3002", "result": "CVAE\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u65b9\u6cd5\u5728\u68c0\u6d4b\u9690\u853d\u5bf9\u6297\u653b\u51fb\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u9a6c\u6c0f\u8ddd\u79bb\u68c0\u6d4b\u5668\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5148\u8fdb\u7684\u6982\u7387\u5efa\u6a21\u5bf9\u63d0\u5347IDS\u5bf9\u6297\u751f\u6210\u6a21\u578b\u653b\u51fb\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.21209", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21209", "abs": "https://arxiv.org/abs/2506.21209", "authors": ["Louis Kerner", "Michel Meintz", "Bihe Zhao", "Franziska Boenisch", "Adam Dziedzic"], "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models", "comment": null, "summary": "State-of-the-art text-to-image models like Infinity generate photorealistic\nimages at an unprecedented speed. These models operate in a bitwise\nautoregressive manner over a discrete set of tokens that is practically\ninfinite in size. However, their impressive generative power comes with a\ngrowing risk: as their outputs increasingly populate the Internet, they are\nlikely to be scraped and reused as training data-potentially by the very same\nmodels. This phenomenon has been shown to lead to model collapse, where\nrepeated training on generated content, especially from the models' own\nprevious versions, causes a gradual degradation in performance. A promising\nmitigation strategy is watermarking, which embeds human-imperceptible yet\ndetectable signals into generated images-enabling the identification of\ngenerated content. In this work, we introduce BitMark, a robust bitwise\nwatermarking framework for Infinity. Our method embeds a watermark directly at\nthe bit level of the token stream across multiple scales (also referred to as\nresolutions) during Infinity's image generation process. Our bitwise watermark\nsubtly influences the bits to preserve visual fidelity and generation speed\nwhile remaining robust against a spectrum of removal techniques. Furthermore,\nit exhibits high radioactivity, i.e., when watermarked generated images are\nused to train another image generative model, this second model's outputs will\nalso carry the watermark. The radioactive traces remain detectable even when\nonly fine-tuning diffusion or image autoregressive models on images watermarked\nwith our BitMark. Overall, our approach provides a principled step toward\npreventing model collapse in image generative models by enabling reliable\ndetection of generated outputs.", "AI": {"tldr": "BitMark\u662f\u4e00\u79cd\u9488\u5bf9Infinity\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u9c81\u68d2\u4f4d\u7ea7\u6c34\u5370\u6846\u67b6\uff0c\u65e8\u5728\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u901a\u8fc7\u5728\u591a\u5c3a\u5ea6\u5d4c\u5165\u6c34\u5370\u5e76\u4fdd\u6301\u751f\u6210\u901f\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u8f93\u51fa\u5728\u4e92\u8054\u7f51\u4e0a\u7684\u5e7f\u6cdb\u4f20\u64ad\uff0c\u5176\u53ef\u80fd\u88ab\u91cd\u65b0\u7528\u4f5c\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u3002\u6c34\u5370\u6280\u672f\u53ef\u4ee5\u8bc6\u522b\u751f\u6210\u5185\u5bb9\uff0c\u4ece\u800c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BitMark\u5728Infinity\u7684\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u76f4\u63a5\u5728\u4f4d\u7ea7\u522b\u5d4c\u5165\u6c34\u5370\uff0c\u8986\u76d6\u591a\u5c3a\u5ea6\uff08\u5206\u8fa8\u7387\uff09\uff0c\u5e76\u4fdd\u6301\u5bf9\u53bb\u9664\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002", "result": "BitMark\u4e0d\u4ec5\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u548c\u751f\u6210\u901f\u5ea6\uff0c\u8fd8\u5177\u6709\u9ad8\u653e\u5c04\u6027\uff0c\u5373\u6c34\u5370\u4f1a\u5728\u540e\u7eed\u6a21\u578b\u7684\u751f\u6210\u5185\u5bb9\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "BitMark\u4e3a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u6709\u6548\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u5e76\u652f\u6301\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u3002"}}
{"id": "2506.21502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21502", "abs": "https://arxiv.org/abs/2506.21502", "authors": ["Francesco Vitale", "Nicola Dall'Ora", "Sebastiano Gaiardelli", "Enrico Fraccaroli", "Nicola Mazzocca", "Franco Fummi"], "title": "Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems", "comment": null, "summary": "Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring\nsystem dependability and operational efficiency by accurately detecting\nanomalies and identifying their root causes. However, the manual modeling of\nfaulty behaviors often demands extensive domain expertise and produces models\nthat are complex, error-prone, and difficult to interpret. To address this\nchallenge, we present a novel unsupervised fault diagnosis methodology that\nintegrates collective anomaly detection in multivariate time series, process\nmining, and stochastic simulation. Initially, collective anomalies are detected\nfrom low-level sensor data using multivariate time-series analysis. These\nanomalies are then transformed into structured event logs, enabling the\ndiscovery of interpretable process models through process mining. By\nincorporating timing distributions into the extracted Petri nets, the approach\nsupports stochastic simulation of faulty behaviors, thereby enhancing root\ncause analysis and behavioral understanding. The methodology is validated using\nthe Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart\nmanufacturing. Experimental results demonstrate its effectiveness in modeling,\nsimulating, and classifying faulty behaviors in CPSs. This enables the creation\nof comprehensive fault dictionaries that support predictive maintenance and the\ndevelopment of digital twins for industrial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3001\u8fc7\u7a0b\u6316\u6398\u548c\u968f\u673a\u6a21\u62df\uff0c\u7528\u4e8eCPS\u7684\u6545\u969c\u5efa\u6a21\u4e0e\u5206\u7c7b\u3002", "motivation": "\u624b\u52a8\u5efa\u6a21\u6545\u969c\u884c\u4e3a\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u4e14\u6a21\u578b\u590d\u6742\u96be\u89e3\uff0c\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u68c0\u6d4b\u5f02\u5e38\uff0c\u8f6c\u5316\u4e3a\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u63d0\u53d6\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u7ed3\u5408\u968f\u673a\u6a21\u62df\u589e\u5f3a\u5206\u6790\u3002", "result": "\u5728Robotic Arm Dataset\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u80fd\u5efa\u6a21\u3001\u6a21\u62df\u548c\u5206\u7c7b\u6545\u969c\u884c\u4e3a\u3002", "conclusion": "\u65b9\u6cd5\u652f\u6301\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\uff0c\u63d0\u5347CPS\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.21451", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21451", "abs": "https://arxiv.org/abs/2506.21451", "authors": ["Cyrus Addy", "Ajay Kumar Gurumadaiah", "Yixiang Gao", "Kwame Awuah-Offei"], "title": "A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario", "comment": null, "summary": "Underground mining operations face significant safety challenges that make\nemergency response capabilities crucial. While robots have shown promise in\nassisting with search and rescue operations, their effectiveness depends on\nreliable miner detection capabilities. Deep learning algorithms offer potential\nsolutions for automated miner detection, but require comprehensive training\ndatasets, which are currently lacking for underground mining environments. This\npaper presents a novel thermal imaging dataset specifically designed to enable\nthe development and validation of miner detection systems for potential\nemergency applications. We systematically captured thermal imagery of various\nmining activities and scenarios to create a robust foundation for detection\nalgorithms. To establish baseline performance metrics, we evaluated several\nstate-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,\nand RT-DETR on our dataset. While not exhaustive of all possible emergency\nsituations, this dataset serves as a crucial first step toward developing\nreliable thermal-based miner detection systems that could eventually be\ndeployed in real emergency scenarios. This work demonstrates the feasibility of\nusing thermal imaging for miner detection and establishes a foundation for\nfuture research in this critical safety application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u5730\u4e0b\u77ff\u5de5\u68c0\u6d4b\u7684\u65b0\u578b\u70ed\u6210\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4e3a\u672a\u6765\u5e94\u6025\u5e94\u7528\u4e2d\u7684\u77ff\u5de5\u68c0\u6d4b\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5730\u4e0b\u91c7\u77ff\u4f5c\u4e1a\u9762\u4e34\u91cd\u5927\u5b89\u5168\u6311\u6218\uff0c\u4e9f\u9700\u53ef\u9760\u7684\u77ff\u5de5\u68c0\u6d4b\u80fd\u529b\u4ee5\u652f\u6301\u5e94\u6025\u54cd\u5e94\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u5730\u4e0b\u73af\u5883\u7684\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u91c7\u96c6\u5404\u79cd\u91c7\u77ff\u6d3b\u52a8\u548c\u573a\u666f\u7684\u70ed\u6210\u50cf\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86YOLOv8\u3001YOLOv10\u3001YOLO11\u548cRT-DETR\u7b49\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u70ed\u6210\u50cf\u6280\u672f\u53ef\u7528\u4e8e\u77ff\u5de5\u68c0\u6d4b\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6027\u80fd\u6307\u6807\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u662f\u5f00\u53d1\u53ef\u9760\u70ed\u6210\u50cf\u77ff\u5de5\u68c0\u6d4b\u7cfb\u7edf\u7684\u5173\u952e\u7b2c\u4e00\u6b65\uff0c\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u5b9e\u9645\u5e94\u6025\u573a\u666f\u3002"}}
{"id": "2506.21486", "categories": ["cs.CV", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2506.21486", "abs": "https://arxiv.org/abs/2506.21486", "authors": ["Tobias J. Riedlinger", "Kira Maag", "Hanno Gottschalk"], "title": "Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection", "comment": "15 pages, 4 figures, 3 tables", "summary": "Deep neural networks have set the state-of-the-art in computer vision tasks\nsuch as bounding box detection and semantic segmentation. Object detectors and\nsegmentation models assign confidence scores to predictions, reflecting the\nmodel's uncertainty in object detection or pixel-wise classification. However,\nthese confidence estimates are often miscalibrated, as their architectures and\nloss functions are tailored to task performance rather than probabilistic\nfoundation. Even with well calibrated predictions, object detectors fail to\nquantify uncertainty outside detected bounding boxes, i.e., the model does not\nmake a probability assessment of whether an area without detected objects is\ntruly free of obstacles. This poses a safety risk in applications such as\nautomated driving, where uncertainty in empty areas remains unexplored. In this\nwork, we propose an object detection model grounded in spatial statistics.\nBounding box data matches realizations of a marked point process, commonly used\nto describe the probabilistic occurrence of spatial point events identified as\nbounding box centers, where marks are used to describe the spatial extension of\nbounding boxes and classes. Our statistical framework enables a\nlikelihood-based training and provides well-defined confidence estimates for\nwhether a region is drivable, i.e., free of objects. We demonstrate the\neffectiveness of our method through calibration assessments and evaluation of\nperformance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u7edf\u8ba1\u5b66\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u672a\u68c0\u6d4b\u533a\u57df\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u901a\u5e38\u4e0d\u51c6\u786e\uff0c\u4e14\u65e0\u6cd5\u91cf\u5316\u672a\u68c0\u6d4b\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7a7a\u95f4\u7edf\u8ba1\u5b66\u7684\u6807\u8bb0\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u5c06\u8fb9\u754c\u6846\u6570\u636e\u89c6\u4e3a\u7a7a\u95f4\u70b9\u4e8b\u4ef6\u7684\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u4f3c\u7136\u8bad\u7ec3\u63d0\u4f9b\u660e\u786e\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u6821\u51c6\u8bc4\u4f30\u548c\u6027\u80fd\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7edf\u8ba1\u6846\u67b6\u4e3a\u533a\u57df\u662f\u5426\u53ef\u884c\u9a76\uff08\u65e0\u7269\u4f53\uff09\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.21538", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21538", "abs": "https://arxiv.org/abs/2506.21538", "authors": ["Hani Alomari", "Anushka Sivakumar", "Andrew Zhang", "Chris Thomas"], "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval", "comment": "Accepted at the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025 Main)", "summary": "Cross-modal image-text retrieval is challenging because of the diverse\npossible associations between content from different modalities. Traditional\nmethods learn a single-vector embedding to represent semantics of each sample,\nbut struggle to capture nuanced and diverse relationships that can exist across\nmodalities. Set-based approaches, which represent each sample with multiple\nembeddings, offer a promising alternative, as they can capture richer and more\ndiverse relationships. In this paper, we show that, despite their promise,\nthese set-based representations continue to face issues including sparse\nsupervision and set collapse, which limits their effectiveness. To address\nthese challenges, we propose Maximal Pair Assignment Similarity to optimize\none-to-one matching between embedding sets which preserve semantic diversity\nwithin the set. We also introduce two loss functions to further enhance the\nrepresentations: Global Discriminative Loss to enhance distinction among\nembeddings, and Intra-Set Divergence Loss to prevent collapse within each set.\nOur method achieves state-of-the-art performance on MS-COCO and Flickr30k\nwithout relying on external data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u914d\u5bf9\u5206\u914d\u76f8\u4f3c\u5ea6\u548c\u4e24\u79cd\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4e2d\u96c6\u5408\u8868\u793a\u7684\u95ee\u9898\uff0c\u5e76\u5728MS-COCO\u548cFlickr30k\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u5411\u91cf\u5d4c\u5165\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8de8\u6a21\u6001\u7684\u591a\u6837\u5173\u8054\uff0c\u96c6\u5408\u8868\u793a\u867d\u80fd\u4e30\u5bcc\u5173\u7cfb\u8868\u8fbe\uff0c\u4f46\u4ecd\u9762\u4e34\u7a00\u758f\u76d1\u7763\u548c\u96c6\u5408\u574d\u7f29\u95ee\u9898\u3002", "method": "\u63d0\u51faMaximal Pair Assignment Similarity\u4f18\u5316\u4e00\u5bf9\u4e00\u5339\u914d\uff0c\u5e76\u5f15\u5165Global Discriminative Loss\u548cIntra-Set Divergence Loss\u589e\u5f3a\u8868\u793a\u3002", "result": "\u5728MS-COCO\u548cFlickr30k\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u96c6\u5408\u8868\u793a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002"}}
