{"id": "2512.04220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04220", "abs": "https://arxiv.org/abs/2512.04220", "authors": ["Wenlong Deng", "Yushu Li", "Boying Gong", "Yi Ren", "Christos Thrampoulidis", "Xiaoxiao Li"], "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral", "comment": null, "summary": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0GRPO\u65b9\u6cd5\u5728\u5de5\u5177\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u8bad\u7ec3\u5d29\u6e83\u95ee\u9898\uff0c\u6838\u5fc3\u539f\u56e0\u662f\"\u61d2\u60f0\u4f3c\u7136\u4f4d\u79fb\"(LLD)\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6b63\u5219\u5316\u65b9\u6cd5LLDS\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u5de5\u5177\u96c6\u6210\u5f3a\u5316\u5b66\u4e60(TI-RL)\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u591a\u6b65\u63a8\u7406\uff0c\u4f46GRPO\u65b9\u6cd5\u867d\u7136\u6536\u655b\u5feb\u4e14\u65e0\u9700\u4ef7\u503c\u51fd\u6570\uff0c\u5374\u7ecf\u5e38\u51fa\u73b0\u8bad\u7ec3\u5d29\u6e83\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u9996\u5148\u8bc6\u522b\u51fa\u5bfc\u81f4GRPO\u8bad\u7ec3\u5d29\u6e83\u7684\u6838\u5fc3\u673a\u5236\u2014\u2014\u61d2\u60f0\u4f3c\u7136\u4f4d\u79fb(LLD)\uff0c\u7136\u540e\u63d0\u51faLLDS\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ea\u5728\u8f68\u8ff9\u4f3c\u7136\u4e0b\u964d\u65f6\u6fc0\u6d3b\uff0c\u5e76\u4e14\u53ea\u5bf9\u76f8\u5173token\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u4ee5\u6700\u5c0f\u5316\u5bf9\u4f18\u5316\u7684\u5e72\u6270\u3002", "result": "\u57287\u4e2a\u5f00\u653e\u57df\u548c\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLDS\u65b9\u6cd5\u7a33\u5b9a\u4e86\u8bad\u7ec3\uff0c\u9632\u6b62\u4e86\u68af\u5ea6\u7206\u70b8\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5982\u5728Qwen2.5-3B\u4e0a\u63d0\u534737.8%\uff0c\u5728Qwen2.5-7B\u4e0a\u63d0\u534732.0%\u3002", "conclusion": "LLD\u662fGRPO\u57faTI-RL\u7684\u6839\u672c\u74f6\u9888\uff0c\u63d0\u51fa\u7684LLDS\u65b9\u6cd5\u4e3a\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\u96c6\u6210LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2512.04219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04219", "abs": "https://arxiv.org/abs/2512.04219", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur\\\\"], "title": "Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning", "comment": "16 pages, 7 figures, 3 tables. Under Review", "summary": "Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.", "AI": {"tldr": "PARSE\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u9012\u5f52\u9884\u6d4b\u5668\u4ece\u6d41\u5f0f\u89c6\u9891\u4e2d\u5b66\u4e60\u5c42\u6b21\u5316\u4e8b\u4ef6\u7ed3\u6784\uff0c\u9884\u6d4b\u8bef\u5dee\u5cf0\u503c\u81ea\u7136\u5f62\u6210\u4e8b\u4ef6\u8fb9\u754c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6d41\u5f0f\u65b9\u6cd5\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u81ea\u7136\u5730\u5c06\u8fde\u7eed\u7ecf\u9a8c\u611f\u77e5\u4e3a\u65f6\u95f4\u5d4c\u5957\u7684\u4e8b\u4ef6\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u8ba1\u7b97\u673a\u89c6\u89c9\u9700\u8981\u80fd\u591f\u9884\u6d4b\u6027\u548c\u5c42\u6b21\u6027\u5730\u5206\u5272\u89c6\u9891\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u56de\u987e\u6027\u5730\u5206\u5272\u3002", "method": "PARSE\u91c7\u7528\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u9012\u5f52\u9884\u6d4b\u5668\u5b66\u4e60\u89c6\u9891\u7684\u5c42\u6b21\u5316\u4e8b\u4ef6\u7ed3\u6784\uff1a\u4f4e\u5c42\u5efa\u6a21\u77ed\u671f\u52a8\u6001\uff0c\u9ad8\u5c42\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u53cd\u9988\u6574\u5408\u957f\u671f\u4e0a\u4e0b\u6587\uff0c\u9884\u6d4b\u8bef\u5dee\u7684\u77ac\u65f6\u5cf0\u503c\u81ea\u7136\u5f62\u6210\u4e8b\u4ef6\u8fb9\u754c\u3002", "result": "\u5728Breakfast Actions\u300150 Salads\u548cAssembly 101\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPARSE\u5728\u6d41\u5f0f\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u65f6\u95f4\u5bf9\u9f50\uff08H-GEBD\uff09\u548c\u7ed3\u6784\u4e00\u81f4\u6027\uff08TED, hF1\uff09\u65b9\u9762\u4e0e\u79bb\u7ebf\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "\u9884\u6d4b\u5b66\u4e60\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4e3a\u4eba\u7c7b\u7c7b\u4f3c\u7684\u65f6\u95f4\u62bd\u8c61\u548c\u7ec4\u5408\u4e8b\u4ef6\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u80fd\u591f\u4ea7\u751f\u4e0e\u4eba\u7c7b\u4e8b\u4ef6\u611f\u77e5\u76f8\u4f3c\u7684\u5c42\u6b21\u5316\u3001\u5d4c\u5957\u7684\u4e8b\u4ef6\u7ed3\u6784\u3002"}}
{"id": "2512.04492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04492", "abs": "https://arxiv.org/abs/2512.04492", "authors": ["Yuanshuo Zhang", "Aohua Li", "Bo Chen", "Jingbo Sun", "Xiaobing Zhao"], "title": "MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection", "comment": null, "summary": "LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.", "AI": {"tldr": "MSME\uff1a\u4e00\u4e2a\u591a\u9636\u6bb5\u3001\u591a\u4e13\u5bb6\u7684\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u51c6\u5907\u3001\u4e13\u5bb6\u63a8\u7406\u548c\u51b3\u7b56\u805a\u5408\u4e09\u9636\u6bb5\u89e3\u51b3\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7acb\u573a\u7406\u89e3\u95ee\u9898", "motivation": "\u73b0\u6709\u7684LLM\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u9519\uff0c\u4f46\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u52a8\u6001\u80cc\u666f\u77e5\u8bc6\u3001\u76ee\u6807\u5b9a\u4e49\u6d89\u53ca\u590d\u5408\u5b9e\u4f53/\u4e8b\u4ef6\u9700\u8981\u4e0e\u7acb\u573a\u6807\u7b7e\u660e\u786e\u5173\u8054\u3001\u4ee5\u53ca\u53cd\u8bbd\u7b49\u4fee\u8f9e\u624b\u6cd5\u4f1a\u63a9\u76d6\u4f5c\u8005\u771f\u5b9e\u610f\u56fe", "method": "\u63d0\u51faMSME\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u77e5\u8bc6\u51c6\u5907\u9636\u6bb5\uff1a\u68c0\u7d22\u76f8\u5173\u80cc\u666f\u77e5\u8bc6\u5e76\u6f84\u6e05\u7acb\u573a\u6807\u7b7e\uff1b2) \u4e13\u5bb6\u63a8\u7406\u9636\u6bb5\uff1a\u5305\u542b\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff08\u77e5\u8bc6\u4e13\u5bb6\u4ece\u77e5\u8bc6\u89d2\u5ea6\u63d0\u70bc\u5173\u952e\u4e8b\u5b9e\u548c\u63a8\u7406\uff0c\u6807\u7b7e\u4e13\u5bb6\u7ec6\u5316\u7acb\u573a\u6807\u7b7e\u5e76\u8fdb\u884c\u76f8\u5e94\u63a8\u7406\uff0c\u8bed\u7528\u4e13\u5bb6\u68c0\u6d4b\u53cd\u8bbd\u7b49\u4fee\u8f9e\u7ebf\u7d22\u4ece\u8bed\u7528\u89d2\u5ea6\u63a8\u65ad\u610f\u56fe\uff09\uff1b3) \u51b3\u7b56\u805a\u5408\u9636\u6bb5\uff1a\u5143\u6cd5\u5b98\u6574\u5408\u6240\u6709\u4e13\u5bb6\u5206\u6790\u4ea7\u751f\u6700\u7ec8\u7acb\u573a\u9884\u6d4b", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMSME\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "MSME\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u3001\u591a\u4e13\u5bb6\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd"}}
{"id": "2512.04518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04518", "abs": "https://arxiv.org/abs/2512.04518", "authors": ["Tianmai M. Zhang", "Zhaoyi Sun", "Sihang Zeng", "Chenxi Li", "Neil F. Abernethy", "Barbara D. Lam", "Fei Xia", "Meliha Yetisgen"], "title": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction", "comment": "To be published in Proceedings of the 7th Clinical Natural Language Processing Workshop", "summary": "The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86ChemoTimelines\u5171\u4eab\u4efb\u52a1\u4e2d\u4ece\u4e34\u5e8a\u8bb0\u5f55\u63d0\u53d6\u5316\u7597\u65f6\u95f4\u7ebf\u7684\u5b50\u4efb\u52a12\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u79cdLLM\u7b56\u7565\u63d0\u5347\u65f6\u95f4\u7ebf\u63d0\u53d6\u6548\u679c\uff0c\u6700\u4f73\u6a21\u578b\u8fbe\u52300.678\u5206\u3002", "motivation": "\u4ece\u764c\u75c7\u60a3\u8005\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u6784\u5efa\u7cfb\u7edf\u6027\u6297\u764c\u6cbb\u7597\u65f6\u95f4\u7ebf\u5bf9\u4e34\u5e8a\u51b3\u7b56\u548c\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u5316\u63d0\u53d6\u5316\u7597\u65f6\u95f4\u7ebf\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u539f\u59cb\u4e34\u5e8a\u8bb0\u5f55\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u5de5\u4f5c\u6d41\u7a0b\uff1a1\uff09\u4f7f\u7528LLM\u4ece\u5355\u4e2a\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u63d0\u53d6\u5316\u7597\u4e8b\u4ef6\uff1b2\uff09\u901a\u8fc7\u7b97\u6cd5\u5c06\u4e8b\u4ef6\u6807\u51c6\u5316\u5e76\u805a\u5408\u6210\u60a3\u8005\u7ea7\u522b\u7684\u65f6\u95f4\u7ebf\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u7b56\u7565\uff1a\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u76d1\u7763\u5fae\u8c03\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u57fa\u4e8e\u5b57\u5178\u7684\u67e5\u627e\u65b9\u6cd5\uff0c\u4e0d\u540c\u65b9\u6cd5\u4e3b\u8981\u5728LLM\u7684\u4f7f\u7528\u548c\u8bad\u7ec3\u65b9\u5f0f\u4e0a\u6709\u6240\u533a\u522b\u3002", "result": "\u591a\u79cd\u65b9\u6cd5\u5728\u6d4b\u8bd5\u96c6\u6392\u884c\u699c\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5176\u4e2d\u5fae\u8c03\u7684Qwen3-14B\u6a21\u578b\u83b7\u5f97\u4e86\u6700\u4f73\u5b98\u65b9\u5206\u65700.678\u3002\u5206\u6790\u7ed3\u679c\u4e3a\u672a\u6765\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u4e34\u5e8a\u8bb0\u5f55\u63d0\u53d6\u5316\u7597\u65f6\u95f4\u7ebf\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86LLM\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7c7b\u4f3c\u533b\u7597\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2512.04773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04773", "abs": "https://arxiv.org/abs/2512.04773", "authors": ["Giorgos Polychronis", "Foivos Pournaropoulos", "Christos D. Antonopoulos", "Spyros Lalis"], "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions", "comment": "19 pages, 3 figures, to appear in the proceedings of MobiQuitous 2025", "summary": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.", "AI": {"tldr": "\u65e0\u4eba\u673a\u5728\u6570\u636e\u9a71\u52a8\u4efb\u52a1\u4e2d\u9700\u8981\u5b9e\u65f6\u5904\u7406\u6570\u636e\u4ee5\u51b3\u5b9a\u662f\u5426\u91c7\u53d6\u540e\u7eed\u884c\u52a8\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5206\u652f\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u4efb\u52a1\u65f6\u95f4\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u6570\u636e\u91c7\u96c6\u4efb\u52a1\u4e2d\u9762\u4e34\u51b3\u7b56\u56f0\u5883\uff1a\u5982\u679c\u539f\u5730\u7b49\u5f85\u5904\u7406\u7ed3\u679c\u53ef\u80fd\u6d6a\u8d39\u65f6\u95f4\uff0c\u5982\u679c\u63d0\u524d\u79fb\u52a8\u800c\u9700\u8981\u8fd4\u56de\u5219\u589e\u52a0\u98de\u884c\u65f6\u95f4\u3002\u73b0\u6709\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4e8b\u4ef6\u6982\u7387\u968f\u65f6\u95f4\u53d8\u5316\u7684\u590d\u6742\u573a\u666f\u3002", "method": "\u63d0\u51fa\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u5206\u652f\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u6570\u636e\u91c7\u96c6\u70b9\u51b3\u5b9a\u662f\u5426\u7b49\u5f85\u5904\u7406\u7ed3\u679c\u6216\u76f4\u63a5\u79fb\u52a8\u5230\u4e0b\u4e00\u4e2a\u70b9\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e8b\u4ef6\u6982\u7387\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5404\u79cd\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u56de\u5f52\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6700\u574f\u60c5\u51b5\u4efb\u52a1\u65f6\u95f4\u6700\u591a\u6539\u55844.1\u500d\uff0c\u4e2d\u4f4d\u4efb\u52a1\u65f6\u95f4\u4ec5\u6bd4\u5b8c\u7f8e\u77e5\u8bc6\u65b9\u6cd5\u9ad82.7%\u3002", "conclusion": "\u57fa\u4e8e\u5206\u652f\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u65f6\u95f4\uff0c\u63a5\u8fd1\u7406\u60f3\u6027\u80fd\u3002"}}
{"id": "2512.04314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04314", "abs": "https://arxiv.org/abs/2512.04314", "authors": ["Jiashu Liao", "Pietro Li\u00f2", "Marc de Kamps", "Duygu Sarikaya"], "title": "DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision", "comment": null, "summary": "Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.", "AI": {"tldr": "DisentangleFormer\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\u7684\u89c6\u89c9Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u7a7a\u95f4token\u548c\u901a\u9053token\u6d41\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u673a\u5236\u540c\u65f6\u5904\u7406\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u8868\u793a\u7ea0\u7f20\uff0c\u65e0\u6cd5\u72ec\u7acb\u5efa\u6a21\u7ed3\u6784\u548c\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u5728\u591a\u901a\u9053\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u9ad8\u5149\u8c31\u6210\u50cf\uff09\u4e2d\u5c24\u4e3a\u7a81\u51fa\uff0c\u56e0\u4e3a\u4e0d\u540c\u901a\u9053\u6355\u83b7\u4e0d\u540c\u7684\u751f\u7269\u7269\u7406\u6216\u751f\u5316\u4fe1\u606f\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u53bb\u76f8\u5173\u8868\u793a\u5b66\u4e60\u539f\u5219\uff0c\u91c7\u7528\u5e76\u884c\u89e3\u8026\u8bbe\u8ba1\uff1a1) \u5e76\u884c\u89e3\u8026\uff1a\u72ec\u7acb\u5904\u7406\u7a7a\u95f4token\u6d41\u548c\u901a\u9053token\u6d41\uff1b2) \u538b\u7f29token\u589e\u5f3a\u5668\uff1a\u52a8\u6001\u878d\u5408\u7a7a\u95f4\u548c\u901a\u9053\u6d41\u7684\u81ea\u9002\u5e94\u6821\u51c6\u6a21\u5757\uff1b3) \u591a\u5c3a\u5ea6FFN\uff1a\u8865\u5145\u5168\u5c40\u6ce8\u610f\u529b\uff0c\u6355\u83b7\u7ec6\u7c92\u5ea6\u7ed3\u6784\u548c\u8bed\u4e49\u4f9d\u8d56\u3002", "result": "\u5728\u9ad8\u5149\u8c31\u57fa\u51c6\u6d4b\u8bd5\uff08Indian Pine\u3001Pavia University\u3001Houston\u3001BigEarthNet\u548c\u7ea2\u5916\u75c5\u7406\u6570\u636e\u96c6\uff09\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728ImageNet\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e17.8% FLOPs\u3002", "conclusion": "DisentangleFormer\u901a\u8fc7\u7a7a\u95f4-\u901a\u9053\u89e3\u8026\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u591a\u901a\u9053\u89c6\u89c9\u8868\u793a\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u591a\u901a\u9053\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04797", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04797", "abs": "https://arxiv.org/abs/2512.04797", "authors": ["SIMA team", "Adrian Bolton", "Alexander Lerchner", "Alexandra Cordell", "Alexandre Moufarek", "Andrew Bolt", "Andrew Lampinen", "Anna Mitenkova", "Arne Olav Hallingstad", "Bojan Vujatovic", "Bonnie Li", "Cong Lu", "Daan Wierstra", "Daniel P. Sawyer", "Daniel Slater", "David Reichert", "Davide Vercelli", "Demis Hassabis", "Drew A. Hudson", "Duncan Williams", "Ed Hirst", "Fabio Pardo", "Felix Hill", "Frederic Besse", "Hannah Openshaw", "Harris Chan", "Hubert Soyer", "Jane X. Wang", "Jeff Clune", "John Agapiou", "John Reid", "Joseph Marino", "Junkyung Kim", "Karol Gregor", "Kaustubh Sridhar", "Kay McKinney", "Laura Kampis", "Lei M. Zhang", "Loic Matthey", "Luyu Wang", "Maria Abi Raad", "Maria Loks-Thompson", "Martin Engelcke", "Matija Kecman", "Matthew Jackson", "Maxime Gazeau", "Ollie Purkiss", "Oscar Knagg", "Peter Stys", "Piermaria Mendolicchio", "Raia Hadsell", "Rosemary Ke", "Ryan Faulkner", "Sarah Chakera", "Satinder Singh Baveja", "Shane Legg", "Sheleem Kashem", "Tayfun Terzi", "Thomas Keck", "Tim Harley", "Tim Scholtes", "Tyson Roberts", "Volodymyr Mnih", "Yulan Liu", "Zhengdong Wang", "Zoubin Ghahramani"], "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds", "comment": null, "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", "AI": {"tldr": "SIMA 2\u662f\u57fa\u4e8eGemini\u57fa\u7840\u6a21\u578b\u6784\u5efa\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u5728\u591a\u79cd3D\u865a\u62df\u4e16\u754c\u4e2d\u7406\u89e3\u548c\u884c\u52a8\uff0c\u652f\u6301\u9ad8\u7ea7\u76ee\u6807\u63a8\u7406\u3001\u7528\u6237\u5bf9\u8bdd\u4ee5\u53ca\u8bed\u8a00\u548c\u56fe\u50cf\u7684\u590d\u6742\u6307\u4ee4\u5904\u7406\uff0c\u663e\u8457\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u6027\u80fd\u7684\u5dee\u8ddd\u5e76\u5c55\u793a\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\uff08\u5982SIMA 1\uff09\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u8bed\u8a00\u6307\u4ee4\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4e3b\u52a8\u3001\u76ee\u6807\u5bfc\u5411\u7684\u4ea4\u4e92\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u9ad8\u7ea7\u76ee\u6807\u3001\u4e0e\u7528\u6237\u5bf9\u8bdd\u3001\u5904\u7406\u590d\u6742\u6307\u4ee4\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u4e3a\u865a\u62df\u548c\u7269\u7406\u4e16\u754c\u521b\u9020\u591a\u529f\u80fd\u4e14\u6301\u7eed\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u3002", "method": "\u57fa\u4e8eGemini\u57fa\u7840\u6a21\u578b\u6784\u5efa\uff0c\u652f\u6301\u8bed\u8a00\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6307\u4ee4\u8f93\u5165\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u6e38\u620f\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u5b9e\u73b0\u9ad8\u7ea7\u76ee\u6807\u63a8\u7406\u548c\u7528\u6237\u5bf9\u8bdd\u80fd\u529b\u3002\u5229\u7528Gemini\u751f\u6210\u4efb\u52a1\u548c\u63d0\u4f9b\u5956\u52b1\uff0c\u5b9e\u73b0\u5f00\u653e\u5f0f\u7684\u81ea\u6211\u6539\u8fdb\uff0c\u80fd\u591f\u5728\u65b0\u73af\u5883\u4e2d\u4ece\u96f6\u5f00\u59cb\u81ea\u4e3b\u5b66\u4e60\u65b0\u6280\u80fd\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6e38\u620f\u7ec4\u5408\u4e2d\uff0cSIMA 2\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u4eba\u7c7b\u6027\u80fd\u7684\u5dee\u8ddd\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002\u80fd\u591f\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u673a\u5236\u5728\u65b0\u73af\u5883\u4e2d\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u65b0\u6280\u80fd\u3002", "conclusion": "SIMA 2\u4ee3\u8868\u4e86\u5728\u5177\u8eab\u73af\u5883\u4e2d\u5b9e\u73b0\u4e3b\u52a8\u3001\u76ee\u6807\u5bfc\u5411\u4ea4\u4e92\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u9a8c\u8bc1\u4e86\u521b\u5efa\u591a\u529f\u80fd\u4e14\u6301\u7eed\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u7684\u53ef\u884c\u8def\u5f84\uff0c\u4e3a\u672a\u6765\u865a\u62df\u548c\u7269\u7406\u4e16\u754c\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.04727", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04727", "abs": "https://arxiv.org/abs/2512.04727", "authors": ["Kuinan Hou", "Marco Zorzi", "Alberto Testolin"], "title": "Sequential Enumeration in Large Language Models", "comment": null, "summary": "Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.", "AI": {"tldr": "LLMs\u5728\u5e8f\u5217\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff1a\u867d\u7136\u80fd\u5728\u660e\u786e\u63d0\u793a\u4e0b\u6267\u884c\u8ba1\u6570\u7a0b\u5e8f\uff0c\u4f46\u4e0d\u4f1a\u81ea\u53d1\u8fdb\u884c\u8ba1\u6570\uff0c\u663e\u793a\u795e\u7ecf\u7f51\u7edc\u4e0e\u7b26\u53f7\u7cfb\u7edf\u5728\u7ec4\u5408\u6cdb\u5316\u4e0a\u7684\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u591f\u50cf\u57fa\u4e8e\u89c4\u5219\u7684\u7b26\u53f7\u7cfb\u7edf\u90a3\u6837\u53ef\u9760\u5730\u6267\u884c\u5e8f\u5217\u8ba1\u6570\u4efb\u52a1\uff0c\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5728\u83b7\u53d6\u7cfb\u7edf\u6027\u8ba1\u6570\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u6d4b\u8bd55\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff08\u5305\u62ec\u4e13\u6709\u3001\u5f00\u6e90\u548c\u63a8\u7406\u6a21\u578b\uff09\u5728\u5b57\u6bcd\u548c\u5355\u8bcd\u5217\u8868\u7684\u5e8f\u5217\u547d\u540d\u548c\u751f\u4ea7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u91c7\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\u63a2\u7d22\u601d\u7ef4\u94fe\u7684\u4f5c\u7528\uff0c\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u7684\u8ba1\u6570\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u5d4c\u5165\u52a8\u6001\u3002", "result": "\u4e00\u4e9bLLM\u5728\u660e\u786e\u63d0\u793a\u4e0b\u80fd\u591f\u6267\u884c\u8ba1\u6570\u7a0b\u5e8f\uff0c\u4f46\u6ca1\u6709\u6a21\u578b\u5728\u7b80\u5355\u8981\u6c42\u679a\u4e3e\u5e8f\u5217\u9879\u6570\u65f6\u4f1a\u81ea\u53d1\u8fdb\u884c\u8ba1\u6570\u3002\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u5e76\u672a\u5e26\u6765\u8ba1\u6570\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u63d0\u5347\u3002", "conclusion": "\u5c3d\u7ba1LLM\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6d8c\u73b0\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4ecd\u65e0\u6cd5\u7a33\u5065\u4e14\u7cfb\u7edf\u5730\u6267\u884c\u8ba1\u6570\u7a0b\u5e8f\uff0c\u7a81\u663e\u4e86\u795e\u7ecf\u7f51\u7edc\u4e0e\u7b26\u53f7\u65b9\u6cd5\u5728\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u7684\u6301\u7eed\u5dee\u8ddd\u3002"}}
{"id": "2512.04451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04451", "abs": "https://arxiv.org/abs/2512.04451", "authors": ["Yifei Wang", "Zhenkai Li", "Tianwen Qian", "Huanran Zheng", "Zheng Wang", "Yuqian Fu", "Xiaoling Wang"], "title": "StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios", "comment": null, "summary": "As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.", "AI": {"tldr": "StreamEQA\uff1a\u9996\u4e2a\u9762\u5411\u5177\u8eab\u573a\u666f\u7684\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\uff0c\u8bc4\u4f30MLLM\u5728\u5177\u8eab\u548c\u6d41\u5f0f\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u80fd\u529b\uff0c\u5305\u542b\u611f\u77e5\u3001\u4ea4\u4e92\u3001\u89c4\u5212\u4e09\u4e2a\u5c42\u6b21\u548c\u5411\u540e\u3001\u5b9e\u65f6\u3001\u5411\u524d\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740\u5177\u8eab\u667a\u80fd\u5411\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u53d1\u5c55\uff0c\u9700\u8981\u6301\u7eed\u611f\u77e5\u548c\u7406\u89e3\u6d41\u5f0f\u89c6\u89c9\u8f93\u5165\u7684\u80fd\u529b\u3002\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u5728\u5177\u8eab\u573a\u666f\u4e0b\u5bf9\u6d41\u5f0f\u89c6\u9891\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaStreamEQA\u57fa\u51c6\uff0c\u57fa\u4e8e156\u4e2a\u72ec\u7acb\u957f\u89c6\u9891\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u548c\u4eba\u5de5\u7cbe\u70bc\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u5b9a\u4e49\u4e8642\u4e2a\u4efb\u52a1\u5e76\u751f\u6210\u4e86\u7ea621K\u4e2a\u5e26\u7cbe\u786e\u65f6\u95f4\u6233\u7684\u95ee\u7b54\u5bf9\u3002\u8bc4\u4f30\u7ef4\u5ea6\u5305\u62ec\u5177\u8eab\uff08\u611f\u77e5\u3001\u4ea4\u4e92\u3001\u89c4\u5212\uff09\u548c\u6d41\u5f0f\uff08\u5411\u540e\u3001\u5b9e\u65f6\u3001\u5411\u524d\u63a8\u7406\uff09\u3002", "result": "\u8bc4\u4f30\u4e8613\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u9891-LLM\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5728\u4f20\u7edf\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u5177\u8eab\u573a\u666f\u4e0b\u7684\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "StreamEQA\u57fa\u51c6\u5c06\u4fc3\u8fdb\u5177\u8eab\u5e94\u7528\u4e2d\u7684\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.04643", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04643", "abs": "https://arxiv.org/abs/2512.04643", "authors": ["Chang-Hsun Wu", "Kai-Po Chang", "Yu-Yang Sheng", "Hung-Kai Chung", "Kuei-Chun Wang", "Yu-Chiang Frank Wang"], "title": "SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding", "comment": null, "summary": "Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.", "AI": {"tldr": "SEASON\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u6bd4\u89e3\u7801\u6765\u589e\u5f3aVideoLLMs\u7684\u65f6\u7a7a\u5fe0\u5b9e\u5ea6\uff0c\u51cf\u5c11\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5f53\u524d\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u89c6\u9891\u4e2d\u7684\u4e30\u5bcc\u65f6\u95f4\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7ecf\u5e38\u4ea7\u751f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6216\u56e0\u679c\u4e0d\u5408\u7406\u7684\u63cf\u8ff0\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u5e7b\u89c9\uff08\u5982\u7269\u4f53\u4e0d\u5339\u914d\uff09\uff0c\u800c\u65f6\u95f4\u63a8\u7406\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faSelf-Diagnostic Contrastive Decoding (SEASON)\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bca\u65ad\u6bcf\u4e2a\u8f93\u51fatoken\u7684\u5e7b\u89c9\u503e\u5411\uff0c\u5e76\u5bf9\u5176\u5bf9\u5e94\u7684\u65f6\u7a7a\u8d1f\u6837\u672c\u5e94\u7528\u81ea\u9002\u5e94\u5bf9\u6bd4\u89e3\u7801\uff0c\u6765\u589e\u5f3a\u6bcf\u4e2a\u8f93\u51fatoken\u7684\u65f6\u7a7a\u5fe0\u5b9e\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u5e7b\u89c9\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEASON\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u56db\u4e2a\u901a\u7528\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86VideoLLMs\u7684\u6027\u80fd\u3002", "conclusion": "SEASON\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u6bd4\u89e3\u7801\u6709\u6548\u89e3\u51b3\u4e86VideoLLMs\u4e2d\u7684\u65f6\u7a7a\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u65f6\u7a7a\u5fe0\u5b9e\u5ea6\u548c\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2512.04487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04487", "abs": "https://arxiv.org/abs/2512.04487", "authors": ["Eunjong Lee", "Eunhee Kim", "Sanghoon Hong", "Eunho Jung", "Jihoon Kim"], "title": "Controllable Long-term Motion Generation with Extended Joint Targets", "comment": "WACV 2026", "summary": "Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.", "AI": {"tldr": "COMET\uff1a\u5b9e\u65f6\u89d2\u8272\u52a8\u753b\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7Transformer\u6761\u4ef6VAE\u5b9e\u73b0\u7cbe\u7ec6\u5173\u8282\u63a7\u5236\uff0c\u5f15\u5165\u53c2\u8003\u5f15\u5bfc\u53cd\u9988\u673a\u5236\u786e\u4fdd\u957f\u671f\u7a33\u5b9a\u6027\uff0c\u652f\u6301\u5b9e\u65f6\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u5b9e\u65f6\u89d2\u8272\u52a8\u753b\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u65e0\u6cd5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u63a7\u5236\uff1b2\uff09\u957f\u5e8f\u5217\u4e2d\u8fd0\u52a8\u8d28\u91cf\u4f1a\u9010\u6e10\u9000\u5316\u3002\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u63d0\u51faCOMET\u81ea\u56de\u5f52\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9ad8\u6548\u7684Transformer\u6761\u4ef6VAE\u5b9e\u73b0\u7cbe\u786e\u7684\u5173\u8282\u7ea7\u63a7\u5236\uff1b2\uff09\u5f15\u5165\u53c2\u8003\u5f15\u5bfc\u53cd\u9988\u673a\u5236\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\uff0c\u786e\u4fdd\u957f\u671f\u7a33\u5b9a\u6027\uff1b3\uff09\u8be5\u673a\u5236\u8fd8\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u5b9e\u73b0\u5b9e\u65f6\u98ce\u683c\u8fc1\u79fb\u3002", "result": "COMET\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u8fd0\u52a8\uff0c\u5728\u590d\u6742\u8fd0\u52a8\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u8981\u6c42\u82db\u523b\u7684\u4ea4\u4e92\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "COMET\u89e3\u51b3\u4e86\u5b9e\u65f6\u89d2\u8272\u52a8\u753b\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u957f\u671f\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u53c2\u8003\u5f15\u5bfc\u53cd\u9988\u673a\u5236\u8fd8\u6269\u5c55\u4e86\u98ce\u683c\u8fc1\u79fb\u529f\u80fd\u3002"}}
{"id": "2512.04921", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04921", "abs": "https://arxiv.org/abs/2512.04921", "authors": ["Julien Benchek", "Rohit Shetty", "Benjamin Hunsberger", "Ajay Arun", "Zach Richards", "Brendan Foody", "Osvald Nitski", "Bertie Vidgen"], "title": "The AI Consumer Index (ACE)", "comment": null, "summary": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.", "AI": {"tldr": "AI Consumer Index (ACE) \u662f\u9996\u4e2a\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u6267\u884c\u9ad8\u4ef7\u503c\u6d88\u8d39\u8005\u4efb\u52a1\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b400\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6db5\u76d6\u8d2d\u7269\u3001\u996e\u98df\u3001\u6e38\u620f\u548cDIY\u56db\u4e2a\u9886\u57df\u3002GPT-5\u5728\u6392\u884c\u699c\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6574\u4f53\u6a21\u578b\u6027\u80fd\u4e0e\u6d88\u8d39\u8005\u9700\u6c42\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30AI\u6a21\u578b\u5728\u771f\u5b9e\u6d88\u8d39\u8005\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u6807\u51c6\u57fa\u51c6\uff0c\u9700\u8981\u8861\u91cf\u524d\u6cbf\u6a21\u578b\u662f\u5426\u80fd\u591f\u6ee1\u8db3\u6d88\u8d39\u8005\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u8d2d\u7269\u3001\u996e\u98df\u3001\u6e38\u620f\u548cDIY\u7b49\u9ad8\u4ef7\u503c\u9886\u57df\u3002", "method": "\u521b\u5efa\u5305\u542b400\u4e2a\u9690\u85cf\u6d4b\u8bd5\u7528\u4f8b\u7684ACE\u57fa\u51c6\uff0c\u6db5\u76d6\u56db\u4e2a\u6d88\u8d39\u8005\u6d3b\u52a8\u9886\u57df\u3002\u91c7\u7528\u65b0\u9896\u7684\u8bc4\u5206\u65b9\u6cd5\uff0c\u52a8\u6001\u68c0\u67e5\u6a21\u578b\u54cd\u5e94\u662f\u5426\u57fa\u4e8e\u68c0\u7d22\u5230\u7684\u7f51\u9875\u6765\u6e90\u3002\u5728\u6392\u884c\u699c\u4e0a\u8bc4\u4f30\u4e8610\u4e2a\u524d\u6cbf\u6a21\u578b\uff08\u5f00\u542f\u7f51\u7edc\u641c\u7d22\u529f\u80fd\uff09\u3002", "result": "GPT-5\uff08Thinking = High\uff09\u4ee556.1%\u7684\u5f97\u5206\u6392\u540d\u7b2c\u4e00\uff0c\u5176\u6b21\u662fo3 Pro\uff0855.2%\uff09\u548cGPT-5.1\uff0855.1%\uff09\u3002\u4e0d\u540c\u9886\u57df\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u8d2d\u7269\u9886\u57df\u6700\u4f73\u6a21\u578b\u5f97\u5206\u4f4e\u4e8e50%\u3002\u6a21\u578b\u5728\u63d0\u4f9b\u6b63\u786e\u4ef7\u683c\u6216\u6709\u6548\u94fe\u63a5\u7b49\u4efb\u52a1\u4e0a\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002", "conclusion": "ACE\u57fa\u51c6\u63ed\u793a\u4e86\u5373\u4f7f\u662f\u6700\u4f73AI\u6a21\u578b\u7684\u8868\u73b0\u4e0e\u6d88\u8d39\u8005\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u5177\u4f53\u7ec6\u8282\u51c6\u786e\u6027\u548c\u907f\u514d\u5e7b\u89c9\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2512.05013", "categories": ["cs.AI", "cs.MA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.05013", "abs": "https://arxiv.org/abs/2512.05013", "authors": ["Eric Bridgeford", "Hayden Helm"], "title": "Detecting Perspective Shifts in Multi-agent Systems", "comment": null, "summary": "Generative models augmented with external tools and update mechanisms (or \\textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.", "AI": {"tldr": "\u63d0\u51faTDKPS\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u6d4b\u9ed1\u76d2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u52a8\u6001\u53d8\u5316\uff0c\u901a\u8fc7\u65f6\u95f4\u8054\u5408\u5d4c\u5165\u548c\u5047\u8bbe\u68c0\u9a8c\u68c0\u6d4b\u667a\u80fd\u4f53\u53ca\u7fa4\u4f53\u5c42\u9762\u7684\u884c\u4e3a\u53d8\u5316\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u90e8\u7f72\u89c4\u6a21\u6269\u5927\uff0c\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u7136\u6d8c\u73b0\uff0c\u9700\u8981\u76d1\u63a7\u5176\u884c\u4e3a\u52a8\u6001\u53d8\u5316\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u5355\u65f6\u95f4\u70b9\u7684\u67e5\u8be2\u54cd\u5e94\u8fdb\u884c\u4f4e\u7ef4\u8868\u793a\uff0c\u7f3a\u4e4f\u65f6\u95f4\u7ef4\u5ea6\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51faTemporal Data Kernel Perspective Space (TDKPS)\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u8054\u5408\u5d4c\u5165\uff0c\u5e76\u8bbe\u8ba1\u591a\u79cd\u65b0\u9896\u7684\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u667a\u80fd\u4f53\u5c42\u9762\u548c\u7fa4\u4f53\u5c42\u9762\u7684\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u68c0\u9a8c\u65b9\u6cd5\u7684\u7ecf\u9a8c\u7279\u6027\uff0c\u5305\u62ec\u5bf9\u5173\u952e\u8d85\u53c2\u6570\u7684\u654f\u611f\u6027\u3002\u901a\u8fc7\u81ea\u7136\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u68c0\u6d4b\u5230\u4e0e\u771f\u5b9e\u5916\u90e8\u4e8b\u4ef6\u654f\u611f\u3001\u7279\u5f02\u4e14\u663e\u8457\u76f8\u5173\u7684\u884c\u4e3a\u53d8\u5316\u3002", "conclusion": "TDKPS\u662f\u9996\u4e2a\u7528\u4e8e\u76d1\u6d4b\u9ed1\u76d2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u884c\u4e3a\u52a8\u6001\u53d8\u5316\u7684\u539f\u7406\u6027\u6846\u67b6\uff0c\u4e3a\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u89c4\u6a21\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u5173\u952e\u80fd\u529b\u3002"}}
{"id": "2512.04678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04678", "abs": "https://arxiv.org/abs/2512.04678", "authors": ["Yunhong Lu", "Yanhong Zeng", "Haobo Li", "Hao Ouyang", "Qiuyu Wang", "Ka Leong Cheng", "Jiapeng Zhu", "Hengyuan Cao", "Zhipeng Zhang", "Xing Zhu", "Yujun Shen", "Min Zhang"], "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "comment": null, "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "AI": {"tldr": "\u63d0\u51faReward Forcing\u6846\u67b6\uff0c\u901a\u8fc7EMA-Sink\u548cRe-DMD\u4e24\u4e2a\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4e2d\u521d\u59cb\u5e27\u590d\u5236\u548c\u8fd0\u52a8\u52a8\u6001\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u5c06\u521d\u59cb\u5e27\u4f5c\u4e3asink tokens\u6765\u7ef4\u6301\u6ce8\u610f\u529b\u6027\u80fd\u548c\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff0c\u4f46\u8fd9\u5bfc\u81f4\u89c6\u9891\u5e27\u8fc7\u5ea6\u4f9d\u8d56\u9759\u6001tokens\uff0c\u9020\u6210\u521d\u59cb\u5e27\u590d\u5236\u548c\u8fd0\u52a8\u52a8\u6001\u51cf\u5f31\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faReward Forcing\u6846\u67b6\uff1a1) EMA-Sink\uff1a\u7ef4\u62a4\u56fa\u5b9a\u5927\u5c0f\u7684tokens\uff0c\u4ece\u521d\u59cb\u5e27\u521d\u59cb\u5316\u5e76\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u878d\u5408\u88ab\u6dd8\u6c70\u7684tokens\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u6355\u83b7\u957f\u671f\u4e0a\u4e0b\u6587\u548c\u8fd1\u671f\u52a8\u6001\uff1b2) Re-DMD\uff08Rewarded Distribution Matching Distillation\uff09\uff1a\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u52a8\u6001\u6027\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u5956\u52b1\u6837\u672c\uff0c\u5c06\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u504f\u5411\u9ad8\u52a8\u6001\u533a\u57df\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5355\u4e2aH100 GPU\u4e0a\u5b9e\u73b023.1 FPS\u7684\u9ad8\u8d28\u91cf\u6d41\u5f0f\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u8d28\u91cf\u540c\u65f6\u4fdd\u6301\u6570\u636e\u4fdd\u771f\u5ea6\u3002", "conclusion": "Reward Forcing\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684EMA-Sink\u548cRe-DMD\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u548c\u52a8\u6001\u4e16\u754c\u6a21\u62df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
