<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 15]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Sports Strategy with Video Analytics and Data Mining: Assessing the effectiveness of Multimodal LLMs in tennis video analysis](https://arxiv.org/abs/2507.02904)
*Charlton Teo*

Main category: cs.CV

TL;DR: 评估多模态大语言模型（MLLMs）在分析网球视频中的有效性，填补现有模型在识别网球回合事件序列中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究在网球分析中存在模型无法理解网球回合事件序列的空白，MLLMs有望填补这一空白。

Method: 评估MLLMs在分类网球动作及识别动作序列中的表现，探索改进方法如不同训练方式与传统模型结合。

Result: 未明确提及具体结果，但研究目标是提升MLLMs在网球视频分析中的性能。

Conclusion: MLLMs在网球视频分析中具有潜力，需进一步优化以填补现有技术空白。

Abstract: The use of Large Language Models (LLMs) in recent years has also given rise
to the development of Multimodal LLMs (MLLMs). These new MLLMs allow us to
process images, videos and even audio alongside textual inputs. In this
project, we aim to assess the effectiveness of MLLMs in analysing sports
videos, focusing mainly on tennis videos. Despite research done on tennis
analysis, there remains a gap in models that are able to understand and
identify the sequence of events in a tennis rally, which would be useful in
other fields of sports analytics. As such, we will mainly assess the MLLMs on
their ability to fill this gap - to classify tennis actions, as well as their
ability to identify these actions in a sequence of tennis actions in a rally.
We further looked into ways we can improve the MLLMs' performance, including
different training methods and even using them together with other traditional
models.

</details>


### [2] [Detection of Rail Line Track and Human Beings Near the Track to Avoid Accidents](https://arxiv.org/abs/2507.03040)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv5的铁路线检测和附近人员识别方法，旨在通过实时视频数据提升铁路安全。


<details>
  <summary>Details</summary>
Motivation: 减少铁路事故，通过实时检测轨道附近人员来增强安全措施。

Method: 利用YOLOv5深度学习模型，实时分析视频数据，检测轨道并识别一米范围内的人类。

Result: 方法在准确性上显著优于现有技术，有效识别人类并提供实时警报。

Conclusion: 该方法有望革新铁路安全措施，为事故预防提供重要贡献。

Abstract: This paper presents an approach for rail line detection and the
identification of human beings in proximity to the track, utilizing the YOLOv5
deep learning model to mitigate potential accidents. The technique incorporates
real-time video data to identify railway tracks with impressive accuracy and
recognizes nearby moving objects within a one-meter range, specifically
targeting the identification of humans. This system aims to enhance safety
measures in railway environments by providing real-time alerts for any detected
human presence close to the track. The integration of a functionality to
identify objects at a longer distance further fortifies the preventative
capabilities of the system. With a precise focus on real-time object detection,
this method is poised to deliver significant contributions to the existing
technologies in railway safety. The effectiveness of the proposed method is
demonstrated through a comprehensive evaluation, yielding a remarkable
improvement in accuracy over existing methods. These results underscore the
potential of this approach to revolutionize safety measures in railway
environments, providing a substantial contribution to accident prevention
strategies.

</details>


### [3] [DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition](https://arxiv.org/abs/2507.03339)
*Sheng Liu,Yiheng Yu,Yuan Feng,Min Xu,Zhelun Jin,Yining Jiang,Tiantian Yuan*

Main category: cs.CV

TL;DR: DESign框架通过动态上下文感知卷积（DCAC）和子网正则化CTC（SR-CTC）提升连续手语识别（CSLR）性能，解决了现有方法在时空建模和过拟合方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR方法难以处理多样样本，且动态卷积仅关注空间建模，忽略了时间动态和上下文依赖。

Method: 提出DESign框架，结合DCAC动态捕捉帧间运动信息，SR-CTC通过子网监督防止CTC过拟合，并采用分类器共享策略增强多尺度一致性。

Result: 在PHOENIX14、PHOENIX14-T和CSL-Daily数据集上达到最优性能。

Conclusion: DESign通过DCAC和SR-CTC有效提升CSLR的泛化能力和准确性，且SR-CTC无需额外推理开销。

Abstract: Current continuous sign language recognition (CSLR) methods struggle with
handling diverse samples. Although dynamic convolutions are ideal for this
task, they mainly focus on spatial modeling and fail to capture the temporal
dynamics and contextual dependencies. To address this, we propose DESign, a
novel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and
Subnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC
dynamically captures the inter-frame motion cues that constitute signs and
uniquely adapts convolutional weights in a fine-grained manner based on
contextual information, enabling the model to better generalize across diverse
signing behaviors and boost recognition accuracy. Furthermore, we observe that
existing methods still rely on only a limited number of frames for parameter
updates during training, indicating that CTC learning overfits to a dominant
path. To address this, SR-CTC regularizes training by applying supervision to
subnetworks, encouraging the model to explore diverse CTC alignment paths and
effectively preventing overfitting. A classifier-sharing strategy in SR-CTC
further strengthens multi-scale consistency. Notably, SR-CTC introduces no
inference overhead and can be seamlessly integrated into existing CSLR models
to boost performance. Extensive ablations and visualizations further validate
the effectiveness of the proposed methods. Results on mainstream CSLR datasets
(i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves
state-of-the-art performance.

</details>


### [4] [DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](https://arxiv.org/abs/2507.04447)
*Wenyao Zhang,Hongsi Liu,Zekun Qi,Yunnan Wang,XinQiang Yu,Jiazhao Zhang,Runpei Dong,Jiawei He,He Wang,Zhizheng Zhang,Li Yi,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: DreamVLA提出了一种新的视觉-语言-动作（VLA）框架，通过整合全面的世界知识预测，解决了现有方法在冗余信息和缺乏动态、空间及语义知识方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在图像预测中存在冗余信息和缺乏全面世界知识的问题，DreamVLA旨在通过整合动态、空间和语义信息来改进机器人操作任务。

Method: DreamVLA采用动态区域引导的世界知识预测，结合空间和语义线索，并使用块状结构化注意力机制防止信息泄漏。此外，采用基于扩散的Transformer解耦动作表示。

Result: 在真实和模拟环境中，DreamVLA在真实机器人任务中达到76.7%的成功率，在CALVIN ABC-D基准测试中平均长度为4.44。

Conclusion: DreamVLA通过整合全面的世界知识预测和解耦表示，显著提升了机器人操作的性能和泛化能力。

Abstract: Recent advances in vision-language-action (VLA) models have shown promise in
integrating image generation with action prediction to improve generalization
and reasoning in robot manipulation. However, existing methods are limited to
challenging image-based forecasting, which suffers from redundant information
and lacks comprehensive and critical world knowledge, including dynamic,
spatial and semantic information. To address these limitations, we propose
DreamVLA, a novel VLA framework that integrates comprehensive world knowledge
forecasting to enable inverse dynamics modeling, thereby establishing a
perception-prediction-action loop for manipulation tasks. Specifically,
DreamVLA introduces a dynamic-region-guided world knowledge prediction,
integrated with the spatial and semantic cues, which provide compact yet
comprehensive representations for action planning. This design aligns with how
humans interact with the world by first forming abstract multimodal reasoning
chains before acting. To mitigate interference among the dynamic, spatial and
semantic information during training, we adopt a block-wise structured
attention mechanism that masks their mutual attention, preventing information
leakage and keeping each representation clean and disentangled. Moreover, to
model the conditional distribution over future actions, we employ a
diffusion-based transformer that disentangles action representations from
shared latent features. Extensive experiments on both real-world and simulation
environments demonstrate that DreamVLA achieves 76.7% success rate on real
robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.

</details>


### [5] [Efficient Event-Based Semantic Segmentation via Exploiting Frame-Event Fusion: A Hybrid Neural Network Approach](https://arxiv.org/abs/2507.03765)
*Hebei Li,Yansong Peng,Jiahui Yuan,Peixi Wu,Jin Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出了一种高效的混合框架，结合事件和帧数据用于图像语义分割，通过三个专用模块优化信息融合，显著提升精度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用事件和帧的互补信息，导致训练复杂和计算成本高。

Method: 采用混合框架，包括SNN分支处理事件和ANN分支处理帧，引入ATW Injector、EDS Injector和CSF模块优化特征融合。

Result: 在多个数据集上达到最先进精度，能耗降低65%。

Conclusion: 该框架有效解决了事件和帧数据融合的挑战，显著提升了分割性能和效率。

Abstract: Event cameras have recently been introduced into image semantic segmentation,
owing to their high temporal resolution and other advantageous properties.
However, existing event-based semantic segmentation methods often fail to fully
exploit the complementary information provided by frames and events, resulting
in complex training strategies and increased computational costs. To address
these challenges, we propose an efficient hybrid framework for image semantic
segmentation, comprising a Spiking Neural Network branch for events and an
Artificial Neural Network branch for frames. Specifically, we introduce three
specialized modules to facilitate the interaction between these two branches:
the Adaptive Temporal Weighting (ATW) Injector, the Event-Driven Sparse (EDS)
Injector, and the Channel Selection Fusion (CSF) module. The ATW Injector
dynamically integrates temporal features from event data into frame features,
enhancing segmentation accuracy by leveraging critical dynamic temporal
information. The EDS Injector effectively combines sparse event data with rich
frame features, ensuring precise temporal and spatial information alignment.
The CSF module selectively merges these features to optimize segmentation
performance. Experimental results demonstrate that our framework not only
achieves state-of-the-art accuracy across the DDD17-Seg, DSEC-Semantic, and
M3ED-Semantic datasets but also significantly reduces energy consumption,
achieving a 65\% reduction on the DSEC-Semantic dataset.

</details>


### [6] [Zero Memory Overhead Approach for Protecting Vision Transformer Parameters](https://arxiv.org/abs/2507.03816)
*Fereshteh Baradaran,Mohsen Raji,Azadeh Baradaran,Arezoo Baradaran,Reihaneh Akbarifard*

Main category: cs.CV

TL;DR: 提出了一种零内存开销的容错技术，通过替换ViT参数的最低位为奇偶校验位来检测位翻转错误，并在检测到错误时屏蔽受影响参数，显著提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着ViT在自动驾驶等安全关键应用中的普及，确保其在参数位翻转错误下的正确功能变得至关重要。

Method: 将参数的最低位替换为奇偶校验位以检测错误，检测到错误时通过零屏蔽受影响参数。

Result: 该技术将ViT模型对位翻转错误的鲁棒性提高了三个数量级，且无需额外内存开销。

Conclusion: 该方法为零开销的容错解决方案，显著提升了ViT在关键应用中的可靠性。

Abstract: Vision Transformers (ViTs) have demonstrated superior performance over
Convolutional Neural Networks (CNNs) in various vision-related tasks such as
classification, object detection, and segmentation due to their use of
self-attention mechanisms. As ViTs become more popular in safety-critical
applications like autonomous driving, ensuring their correct functionality
becomes essential, especially in the presence of bit-flip faults in their
parameters stored in memory. In this paper, a fault tolerance technique is
introduced to protect ViT parameters against bit-flip faults with zero memory
overhead. Since the least significant bits of parameters are not critical for
model accuracy, replacing the LSB with a parity bit provides an error detection
mechanism without imposing any overhead on the model. When faults are detected,
affected parameters are masked by zeroing out, as most parameters in ViT models
are near zero, effectively preventing accuracy degradation. This approach
enhances reliability across ViT models, improving the robustness of parameters
to bit-flips by up to three orders of magnitude, making it an effective
zero-overhead solution for fault tolerance in critical applications.

</details>


### [7] [Deconfounding Causal Inference through Two-Branch Framework with Early-Forking for Sensor-Based Cross-Domain Activity Recognition](https://arxiv.org/abs/2507.03898)
*Di Xiong,Lei Zhang,Shuoyuan Wang,Dongzhou Cheng,Wenbo Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果推理的表示学习算法，用于跨域活动识别，通过分离因果和非因果特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法在传感器活动识别中仅关注统计依赖，忽略了内在因果机制的重要性。

Method: 设计了双分支框架，分别学习因果和非因果特征，并采用独立性准则进行解耦，同时引入不均匀域采样和类别感知扰动层。

Result: 在多个公开基准测试中，该方法显著优于11种现有方法，验证了其有效性。

Conclusion: 该方法通过揭示因果机制，在跨域活动识别中表现出高效性和普适性。

Abstract: Recently, domain generalization (DG) has emerged as a promising solution to
mitigate distribution-shift issue in sensor-based human activity recognition
(HAR) scenario. However, most existing DG-based works have merely focused on
modeling statistical dependence between sensor data and activity labels,
neglecting the importance of intrinsic casual mechanism. Intuitively, every
sensor input can be viewed as a mixture of causal (category-aware) and
non-causal factors (domain-specific), where only the former affects activity
classification judgment. In this paper, by casting such DG-based HAR as a
casual inference problem, we propose a causality-inspired representation
learning algorithm for cross-domain activity recognition. To this end, an
early-forking two-branch framework is designed, where two separate branches are
respectively responsible for learning casual and non-causal features, while an
independence-based Hilbert-Schmidt Information Criterion is employed to
implicitly disentangling them. Additionally, an inhomogeneous domain sampling
strategy is designed to enhance disentanglement, while a category-aware domain
perturbation layer is performed to prevent representation collapse. Extensive
experiments on several public HAR benchmarks demonstrate that our
causality-inspired approach significantly outperforms eleven related
state-of-the-art baselines under cross-person, cross-dataset, and
cross-position settings. Detailed ablation and visualizations analyses reveal
underlying casual mechanism, indicating its effectiveness, efficiency, and
universality in cross-domain activity recognition scenario.

</details>


### [8] [Learning Adaptive Node Selection with External Attention for Human Interaction Recognition](https://arxiv.org/abs/2507.03936)
*Chen Pang,Xuequan Lu,Qianyu Zhou,Lei Lyu*

Main category: cs.CV

TL;DR: 论文提出ASEA方法，通过动态捕捉交互关系，无需预定义假设，结合GCN和AT-NAC模块，显著提升交互建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有GCN方法将交互个体视为独立图，忽略其内在依赖关系；预定义交互矩阵无法适应动态和上下文特定的联合交互。

Method: 提出ASEA方法，结合GCN建模个体内部关系，AT-NAC模块动态选择相关节点，EA模块捕捉交互动态和语义关系。

Result: 实验表明，ASEA能更有效灵活地捕捉交互关系，达到最先进性能。

Conclusion: ASEA通过动态节点选择和外部注意力机制，显著改进了交互建模的灵活性和效果。

Abstract: Most GCN-based methods model interacting individuals as independent graphs,
neglecting their inherent inter-dependencies. Although recent approaches
utilize predefined interaction adjacency matrices to integrate participants,
these matrices fail to adaptively capture the dynamic and context-specific
joint interactions across different actions. In this paper, we propose the
Active Node Selection with External Attention Network (ASEA), an innovative
approach that dynamically captures interaction relationships without predefined
assumptions. Our method models each participant individually using a GCN to
capture intra-personal relationships, facilitating a detailed representation of
their actions. To identify the most relevant nodes for interaction modeling, we
introduce the Adaptive Temporal Node Amplitude Calculation (AT-NAC) module,
which estimates global node activity by combining spatial motion magnitude with
adaptive temporal weighting, thereby highlighting salient motion patterns while
reducing irrelevant or redundant information. A learnable threshold,
regularized to prevent extreme variations, is defined to selectively identify
the most informative nodes for interaction modeling. To capture interactions,
we design the External Attention (EA) module to operate on active nodes,
effectively modeling the interaction dynamics and semantic relationships
between individuals. Extensive evaluations show that our method captures
interaction relationships more effectively and flexibly, achieving
state-of-the-art performance.

</details>


### [9] [Towards Spatially-Varying Gain and Binning](https://arxiv.org/abs/2507.04190)
*Anqi Yang,Eunhee Kang,Wei Chen,Hyong-Euk Lee,Aswin C. Sankaranarayanan*

Main category: cs.CV

TL;DR: 论文提出通过空间变化的增益和像素合并技术，提升图像传感器的噪声性能和动态范围。


<details>
  <summary>Details</summary>
Motivation: 随着像素尺寸缩小，图像质量因光积累减少而下降，需解决分辨率、噪声和动态范围之间的权衡问题。

Method: 采用空间变化的增益和像素合并策略，根据局部场景亮度调整增益和合并尺寸，优化信噪比和动态范围。

Result: 动态范围扩展了一个数量级，信噪比显著提升，数字合并在高增益下优于模拟合并。

Conclusion: 结合空间变化的增益和合并技术，可有效提升图像质量，适用于高动态范围成像等应用。

Abstract: Pixels in image sensors have progressively become smaller, driven by the goal
of producing higher-resolution imagery. However, ceteris paribus, a smaller
pixel accumulates less light, making image quality worse. This interplay of
resolution, noise, and the dynamic range of the sensor and their impact on the
eventual quality of acquired imagery is a fundamental concept in photography.
In this paper, we propose spatially-varying gain and binning to enhance the
noise performance and dynamic range of image sensors. First, we show that by
varying gain spatially to local scene brightness, the read noise can be made
negligible, and the dynamic range of a sensor is expanded by an order of
magnitude. Second, we propose a simple analysis to find a binning size that
best balances resolution and noise for a given light level; this analysis
predicts a spatially-varying binning strategy, again based on local scene
brightness, to effectively increase the overall signal-to-noise ratio. %
without sacrificing resolution. We discuss analog and digital binning modes
and, perhaps surprisingly, show that digital binning outperforms its analog
counterparts when a larger gain is allowed. Finally, we demonstrate that
combining spatially-varying gain and binning in various applications, including
high dynamic range imaging, vignetting, and lens distortion.

</details>


### [10] [A Data-Driven Novelty Score for Diverse In-Vehicle Data Recording](https://arxiv.org/abs/2507.04529)
*Philipp Reis,Joshua Ransiek,David Petri,Jacob Langner,Eric Sax*

Main category: cs.CV

TL;DR: 提出一种实时数据选择方法，通过对象级新颖性检测构建更平衡多样的数据集，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现实数据收集中的偏见问题，特别是罕见事件的不足，以提升模型泛化能力和安全性。

Method: 使用动态Mean Shift算法为图像帧分配数据驱动的新颖性分数，识别并保留含新颖对象的帧。

Result: 减少训练数据集大小可提升模型性能，冗余数据增加时更激进的过滤方法更有效。

Conclusion: 该方法支持实时部署，通过持续更新正常内容定义，高效检测数据流中的新颖性。

Abstract: High-quality datasets are essential for training robust perception systems in
autonomous driving. However, real-world data collection is often biased toward
common scenes and objects, leaving novel cases underrepresented. This imbalance
hinders model generalization and compromises safety. The core issue is the
curse of rarity. Over time, novel events occur infrequently, and standard
logging methods fail to capture them effectively. As a result, large volumes of
redundant data are stored, while critical novel cases are diluted, leading to
biased datasets. This work presents a real-time data selection method focused
on object-level novelty detection to build more balanced and diverse datasets.
The method assigns a data-driven novelty score to image frames using a novel
dynamic Mean Shift algorithm. It models normal content based on mean and
covariance statistics to identify frames with novel objects, discarding those
with redundant elements. The main findings show that reducing the training
dataset size with this method can improve model performance, whereas higher
redundancy tends to degrade it. Moreover, as data redundancy increases, more
aggressive filtering becomes both possible and beneficial. While random
sampling can offer some gains, it often leads to overfitting and
unpredictability in outcomes. The proposed method supports real-time deployment
with 32 frames per second and is constant over time. By continuously updating
the definition of normal content, it enables efficient detection of novelties
in a continuous data stream.

</details>


### [11] [From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach](https://arxiv.org/abs/2507.04815)
*Mihai Masala,Marius Leordeanu*

Main category: cs.CV

TL;DR: 论文提出了一种基于时空事件图的共享表示方法，用于生成视频的长段落描述，并通过自监督神经分析系统验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视频描述数据集缺乏复杂语言描述，且现有方法难以解释视觉与语言之间的关系。

Method: 提出基于时空事件图的共享表示方法，结合多视觉任务生成自然语言描述，并作为自动教师训练端到端神经学生路径。

Result: 方法在多个数据集上生成连贯、丰富且相关的文本描述，并通过标准评估指标和人类标注验证。

Conclusion: 提出的可解释神经分析方法有效解决了视频长段落描述生成问题。

Abstract: The task of describing video content in natural language is commonly referred
to as video captioning. Unlike conventional video captions, which are typically
brief and widely available, long-form paragraph descriptions in natural
language are scarce. This limitation of current datasets is due to the
expensive human manual annotation required and to the highly challenging task
of explaining the language formation process from the perspective of the
underlying story, as a complex system of interconnected events in space and
time. Through a thorough analysis of recently published methods and available
datasets, we identify a general lack of published resources dedicated to the
problem of describing videos in complex language, beyond the level of
descriptions in the form of enumerations of simple captions. Furthermore, while
state-of-the-art methods produce impressive results on the task of generating
shorter captions from videos by direct end-to-end learning between the videos
and text, the problem of explaining the relationship between vision and
language is still beyond our reach. In this work, we propose a shared
representation between vision and language, based on graphs of events in space
and time, which can be obtained in an explainable and analytical way, to
integrate and connect multiple vision tasks to produce the final natural
language description. Moreover, we also demonstrate how our automated and
explainable video description generation process can function as a fully
automatic teacher to effectively train direct, end-to-end neural student
pathways, within a self-supervised neuro-analytical system. We validate that
our explainable neuro-analytical approach generates coherent, rich and relevant
textual descriptions on videos collected from multiple varied datasets, using
both standard evaluation metrics, human annotations and consensus from
ensembles of state-of-the-art VLMs.

</details>


### [12] [ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing](https://arxiv.org/abs/2507.04678)
*Zhenghui Zhao,Chen Wu,Di Wang,Hongruixuan Chen,Zhuo Zheng*

Main category: cs.CV

TL;DR: ChangeBridge是一种基于多模态控制的时空扩散模型，用于从给定场景图像生成未来场景模拟。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法未探索基于给定场景图像模拟未来场景的能力，而这一能力对城市规划等应用至关重要。

Method: ChangeBridge利用条件时空扩散模型，结合多模态控制（如文本提示、实例布局和语义图），通过布朗桥扩散直接建模事件前后的时空演化。

Result: 实验表明，ChangeBridge能生成高保真度的未来场景，符合给定条件，包括事件及背景变化。

Conclusion: ChangeBridge是首个支持多模态控制的时空生成模型，为遥感图像合成提供了新方法。

Abstract: Recent advancements in generative methods, especially diffusion models, have
made great progress in remote sensing image synthesis. Despite these
advancements, existing methods have not explored the simulation of future
scenarios based on given scenario images. This simulation capability has wide
applications for urban planning, land managementChangeBridge: Spatiotemporal
Image Generation with Multimodal Controls, and beyond. In this work, we propose
ChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event
images and conditioned on multimodal spatial controls (e.g., text prompts,
instance layouts, and semantic maps), ChangeBridge can synthesize post-event
images. The core idea behind ChangeBridge is to modeling the noise-to-image
diffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal
controls, ChangeBridge leverages a stochastic Brownian-bridge diffusion,
directly modeling the spatiotemporal evolution between pre-event and post-event
states. To the best of our knowledge, ChangeBridge is the first spatiotemporal
generative model with multimodal controls for remote sensing. Experimental
results demonstrate that ChangeBridge can simulate high-fidelity future
scenarios aligned with given conditions, including event and event-driven
background variations. Code will be available.

</details>


### [13] [Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning](https://arxiv.org/abs/2507.04702)
*Feng Yue,Zhaoxing Zhang,Junming Jiao,Zhengyu Liang,Shiwen Cao,Feifei Zhang,Rong Shen*

Main category: cs.CV

TL;DR: 提出了一种名为Tempo-R0的视频多模态大语言模型，通过多模态时间感知强化解决时间视频定位任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频信息量大且冗余，模型需全面理解视频以准确检索查询相关片段。

Method: 采用自适应注意力分配（SAA）和显式时间戳模态对齐（ETA）预处理，并在微调阶段应用部分无关拒绝策略（PIR-GRPO）。

Result: 在QVHighlights测试基准上优于现有方法约3.5%。

Conclusion: Tempo-R0通过多模态时间感知强化和时间推理优化，显著提升了时间视频定位的性能。

Abstract: Temporal Video Grounding (TVG), which requires pinpointing relevant temporal
segments from video based on language query, has always been a highly
challenging task in the field of video understanding. Videos often have a
larger volume of information and redundancy than texts or images. Models should
present comprehensive understanding of the whole video to accurately retrieve
query-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large
Language Model (Video-MLLM) for the temporal video grounding task via
multimodal temporal sensing reinforcement. Specifically, during the
preprocessing stage of our pipeline, we employ Self-adaptive Attention
Allocation (SAA) method based on frame content variation to efficiently use the
MLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is
also utilized to strengthen our model's capability to perceive the boundaries
of events in the video. In the fine-tuning part of our pipeline, we creatively
apply Partial Irrelevance Refusing-based Group Relative Policy Optimization
(PIR-GRPO) in TVG area to foster model's temporal reasoning from not only
accepting relevant video-query pairs but also refusing irrelevant ones.
Experiments demonstrate that our method accomplishes a notable advantage over
SOTA solutions by around 3.5% on both the original QVHighlights testbench and
its corrected version with more reasonable ground truth annotations.

</details>


### [14] [MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry](https://arxiv.org/abs/2507.04750)
*Zicheng Lin,Xiaoqiang Li,Yichao Wang,Chuan Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种新的大规模合成PIV基准数据集和一种名为MCFormer的深度学习架构，用于标准化评估光学流和PIV算法，并展示了MCFormer的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对光学流模型在PIV数据上性能的全面评估，阻碍了该领域的进展。

Method: 通过CFD模拟生成多样化的合成PIV数据集，并提出MCFormer网络架构，利用多帧时间信息和多成本体积。

Result: MCFormer显著优于现有方法，实现了最低的归一化端点误差（NEPE）。

Conclusion: 该研究为PIV领域提供了基准资源和先进方法，推动了未来研究的发展。

Abstract: Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep
learning applications face significant hurdles. A critical gap exists: the lack
of comprehensive evaluation of how diverse optical flow models perform
specifically on PIV data, largely due to limitations in available datasets and
the absence of a standardized benchmark. This prevents fair comparison and
hinders progress. To address this, our primary contribution is a novel,
large-scale synthetic PIV benchmark dataset generated from diverse CFD
simulations (JHTDB and Blasius). It features unprecedented variety in particle
densities, flow velocities, and continuous motion, enabling, for the first
time, a standardized and rigorous evaluation of various optical flow and PIV
algorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a
new deep network architecture leveraging multi-frame temporal information and
multiple cost volumes, specifically designed for PIV's sparse nature. Our
comprehensive benchmark evaluation, the first of its kind, reveals significant
performance variations among adapted optical flow models and demonstrates that
MCFormer significantly outperforms existing methods, achieving the lowest
overall normalized endpoint error (NEPE). This work provides both a
foundational benchmark resource essential for future PIV research and a
state-of-the-art method tailored for PIV challenges. We make our benchmark
dataset and code publicly available to foster future research in this area.

</details>


### [15] [Differential Attention for Multimodal Crisis Event Analysis](https://arxiv.org/abs/2507.05165)
*Nusrat Munia,Junfeng Zhu,Olfa Nasraoui,Abdullah-Al-Zubaer Imran*

Main category: cs.CV

TL;DR: 利用视觉语言模型（VLMs）和先进融合策略提升危机数据分类性能，结合LLaVA生成文本和CLIP嵌入，采用Guided CA和Differential Attention优化特征对齐。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在危机事件中提供多模态数据流，但从中提取有效信息并整合异构数据仍具挑战。

Method: 结合LLaVA生成文本改进图文对齐，利用CLIP嵌入，采用Guided CA和Differential Attention机制优化特征融合。

Result: 在CrisisMMD数据集上，该方法在分类准确率上优于现有模型，提升了灾难响应任务的可靠性。

Conclusion: 预训练VLMs、增强文本描述和自适应融合策略的组合显著提升了危机事件多模态数据的分类性能。

Abstract: Social networks can be a valuable source of information during crisis events.
In particular, users can post a stream of multimodal data that can be critical
for real-time humanitarian response. However, effectively extracting meaningful
information from this large and noisy data stream and effectively integrating
heterogeneous data remains a formidable challenge. In this work, we explore
vision language models (VLMs) and advanced fusion strategies to enhance the
classification of crisis data in three different tasks. We incorporate
LLaVA-generated text to improve text-image alignment. Additionally, we leverage
Contrastive Language-Image Pretraining (CLIP)-based vision and text embeddings,
which, without task-specific fine-tuning, outperform traditional models. To
further refine multimodal fusion, we employ Guided Cross Attention (Guided CA)
and combine it with the Differential Attention mechanism to enhance feature
alignment by emphasizing critical information while filtering out irrelevant
content. Our results show that while Differential Attention improves
classification performance, Guided CA remains highly effective in aligning
multimodal features. Extensive experiments on the CrisisMMD benchmark data set
demonstrate that the combination of pretrained VLMs, enriched textual
descriptions, and adaptive fusion strategies consistently outperforms
state-of-the-art models in classification accuracy, contributing to more
reliable and interpretable models for three different tasks that are crucial
for disaster response. Our code is available at
https://github.com/Munia03/Multimodal_Crisis_Event.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Harnessing Near-Infrared Spectroscopy and Machine Learning for Traceable Classification of Hanwoo and Holstein Beef](https://arxiv.org/abs/2507.02903)
*AMM Nurul Alam,Abdul Samad,AMM Shamsul Alam,Jahan Ara Monti,Ayesha Muazzam*

Main category: cs.LG

TL;DR: 研究利用近红外光谱（NIRS）结合机器学习（ML）技术区分韩牛（HNB）和荷斯坦牛肉（HLB），以解决食品真实性和标签问题。随机森林模型表现最佳，AUC为0.8826。


<details>
  <summary>Details</summary>
Motivation: 解决食品真实性、标签错误和掺假问题，提供快速、非侵入性的检测方法。

Method: 使用便携式NIRS采集光谱数据，结合PCA和多种ML模型（如LDA、SVM、随机森林等）进行分析和优化。

Result: 随机森林模型表现最佳（AUC 0.8826），SVM次之（AUC 0.8747）。NN模型召回率最高（0.7804）。

Conclusion: NIRS结合ML技术是检测肉类真实性的有效方法，对打击食品欺诈有重要意义。

Abstract: This study evaluates the use of Near-Infrared spectroscopy (NIRS) combined
with advanced machine learning (ML) techniques to differentiate Hanwoo beef
(HNB) and Holstein beef (HLB) to address food authenticity, mislabeling, and
adulteration. Rapid and non-invasive spectral data were attained by a portable
NIRS, recording absorbance data within the wavelength range of 700 to 1100 nm.
A total of 40 Longissimus lumborum samples, evenly split between HNB and HLB,
were obtained from a local hypermarket. Data analysis using Principal Component
Analysis (PCA) demonstrated distinct spectral patterns associated with chemical
changes, clearly separating the two beef varieties and accounting for 93.72% of
the total variance. ML models, including Linear Discriminant Analysis (LDA),
Support Vector Machine (SVM), Logistic Regression (LR), Random Forest, Gradient
Boosting (GB), K-Nearest Neighbors, Decision Tree (DT), Naive Bayes (NB), and
Neural Networks (NN), were implemented, optimized through hyperparameter
tuning, and validated by 5-fold cross-validation techniques to enhance model
robustness and prevent overfitting. Random Forest provided the highest
predictive accuracy with a Receiver Operating Characteristic (ROC) Area Under
the Curve (AUC) of 0.8826, closely followed by the SVM model at 0.8747.
Furthermore, GB and NN algorithms exhibited satisfactory performances, with
cross-validation scores of 0.752. Notably, the NN model achieved the highest
recall rate of 0.7804, highlighting its suitability in scenarios requiring
heightened sensitivity. DT and NB exhibited comparatively lower predictive
performance. The LR and SVM models emerged as optimal choices by effectively
balancing high accuracy, precision, and recall. This study confirms that
integrating NIRS with ML techniques offers a powerful and reliable method for
meat authenticity, significantly contributing to detecting food fraud.

</details>


### [17] [Predictive Maintenance Optimization for Smart Vending Machines Using IoT and Machine Learning](https://arxiv.org/abs/2507.02934)
*Md. Nisharul Hasan*

Main category: cs.LG

TL;DR: 提出了一种基于物联网传感器和机器学习算法的自动售货机预测性维护框架，显著提高了故障检测效率并减少了维护成本。


<details>
  <summary>Details</summary>
Motivation: 传统维护方法（反应性或定时预防性）无法有效预防机器故障，导致停机和服务成本增加。

Method: 利用物联网传感器实时监控机器状态，结合机器学习算法预测故障，实现及时维护。

Result: 通过模拟故障数据验证，分类算法显示早期故障检测显著提升，冗余维护减少。

Conclusion: 预测性维护系统可显著提升自动售货机的运营效率和服务可靠性。

Abstract: The increasing proliferation of vending machines in public and commercial
environments has placed a growing emphasis on operational efficiency and
customer satisfaction. Traditional maintenance approaches either reactive or
time-based preventive are limited in their ability to preempt machine failures,
leading to unplanned downtimes and elevated service costs. This research
presents a novel predictive maintenance framework tailored for vending machines
by leveraging Internet of Things (IoT) sensors and machine learning (ML)
algorithms. The proposed system continuously monitors machine components and
operating conditions in real time and applies predictive models to forecast
failures before they occur. This enables timely maintenance scheduling,
minimizing downtime and extending machine lifespan. The framework was validated
through simulated fault data and performance evaluation using classification
algorithms. Results show a significant improvement in early fault detection and
a reduction in redundant service interventions. The findings indicate that
predictive maintenance systems, when integrated into vending infrastructure,
can transform operational efficiency and service reliability.

</details>


### [18] [Deep Learning-Based Forecasting of Hotel KPIs: A Cross-City Analysis of Global Urban Markets](https://arxiv.org/abs/2507.03028)
*C. J. Atapattu,Xia Cui,N. R Abeynayake*

Main category: cs.LG

TL;DR: 研究使用LSTM网络预测五个城市的酒店KPIs（OCC、ADR、RevPAR），验证了模型的有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 探索LSTM在多元经济背景下的城市酒店业预测能力，为旅游业和城市规划提供数据支持。

Method: 采用LSTM模型，结合时间序列分解和机器学习技术，使用2018-2025年月度数据（80%训练，20%测试）。

Result: 曼彻斯特和孟买预测准确率最高，迪拜和曼谷因季节性和事件影响波动较大。

Conclusion: LSTM模型适用于全球城市酒店业预测，为决策提供框架。

Abstract: This study employs Long Short-Term Memory (LSTM) networks to forecast key
performance indicators (KPIs), Occupancy (OCC), Average Daily Rate (ADR), and
Revenue per Available Room (RevPAR), across five major cities: Manchester,
Amsterdam, Dubai, Bangkok, and Mumbai. The cities were selected for their
diverse economic profiles and hospitality dynamics. Monthly data from 2018 to
2025 were used, with 80% for training and 20% for testing. Advanced time series
decomposition and machine learning techniques enabled accurate forecasting and
trend identification. Results show that Manchester and Mumbai exhibited the
highest predictive accuracy, reflecting stable demand patterns, while Dubai and
Bangkok demonstrated higher variability due to seasonal and event-driven
influences. The findings validate the effectiveness of LSTM models for urban
hospitality forecasting and provide a comparative framework for data-driven
decision-making. The models generalisability across global cities highlights
its potential utility for tourism stakeholders and urban planners.

</details>


### [19] [Monitoring of Static Fairness](https://arxiv.org/abs/2507.03048)
*Thomas A. Henzinger,Mahyar Karimi,Konstantin Kueffner,Kaushik Mallik*

Main category: cs.LG

TL;DR: 提出了一种运行时验证算法公平性的框架，适用于模型未知但具有马尔可夫链结构的系统，支持多种公平性属性的监测。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统广泛用于人类决策，确保其公平性（不基于敏感属性偏袒个体）至关重要。

Method: 引入一种规范语言建模常见公平性属性（如人口统计平等、机会均等），构建监测器实时评估系统公平性，并提供统计保证。

Result: 监测器能在毫秒级更新评估结果，实验验证了在银行贷款和大学录取场景中的有效性。

Conclusion: 该框架为未知模型的系统提供了一种动态验证公平性的方法，具有实际应用潜力。

Abstract: Machine-learned systems are in widespread use for making decisions about
humans, and it is important that they are fair, i.e., not biased against
individuals based on sensitive attributes.
  We present a general framework of runtime verification of algorithmic
fairness for systems whose models are unknown, but are assumed to have a Markov
chain structure, with or without full observation of the state space.
  We introduce a specification language that can model many common algorithmic
fairness properties, such as demographic parity, equal opportunity, and social
burden.
  We build monitors that observe a long sequence of events as generated by a
given system, and output, after each observation, a quantitative estimate of
how fair or biased the system was on that run until that point in time.
  The estimate is proven to be correct modulo a variable error bound and a
given confidence level, where the error bound gets tighter as the observed
sequence gets longer.
  We present two categories of monitoring algorithms, namely ones with a
uniform error bound across all time points, and ones with weaker non-uniform,
pointwise error bounds at different time points.
  Our monitoring algorithms use statistical tools that are adapted to suit the
dynamic requirements of monitoring and the special needs of the fairness
specifications.
  Using a prototype implementation, we show how we can monitor if a bank is
fair in giving loans to applicants from different social backgrounds, and if a
college is fair in admitting students while maintaining a reasonable financial
burden on the society.
  In these experiments, our monitors took less than a millisecond to update
their verdicts after each observation.

</details>


### [20] [BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data](https://arxiv.org/abs/2507.03062)
*Hao Yang,Angela Yao,Christopher Whalen,Gengchen Mai*

Main category: cs.LG

TL;DR: BERT4Traj是一种基于Transformer的模型，用于从稀疏移动数据中重建完整的人类移动轨迹，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 人类移动数据常因收集方法限制（如低频GPS采样或CDR数据）而稀疏，影响应用效果。

Method: BERT4Traj结合空间嵌入、时间嵌入及上下文背景特征（如人口统计和锚点），利用BERT的掩码语言建模目标和自注意力机制。

Result: 在乌干达坎帕拉的CDR和GPS数据集上，BERT4Traj显著优于马尔可夫链、KNN、RNN和LSTM等传统模型。

Conclusion: BERT4Traj能有效重建详细连续的移动轨迹，提升对人类移动模式的理解。

Abstract: Understanding human mobility is essential for applications in public health,
transportation, and urban planning. However, mobility data often suffers from
sparsity due to limitations in data collection methods, such as infrequent GPS
sampling or call detail record (CDR) data that only capture locations during
communication events. To address this challenge, we propose BERT4Traj, a
transformer based model that reconstructs complete mobility trajectories by
predicting hidden visits in sparse movement sequences. Inspired by BERT's
masked language modeling objective and self_attention mechanisms, BERT4Traj
leverages spatial embeddings, temporal embeddings, and contextual background
features such as demographics and anchor points. We evaluate BERT4Traj on real
world CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our
approach significantly outperforms traditional models such as Markov Chains,
KNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs
detailed and continuous mobility trajectories, enhancing insights into human
movement patterns.

</details>


### [21] [Communication Efficient, Differentially Private Distributed Optimization using Correlation-Aware Sketching](https://arxiv.org/abs/2507.03545)
*Julien Nicolas,Mohamed Maouche,Sonia Ben Mokhtar,Mark Coates*

Main category: cs.LG

TL;DR: DOME框架通过将梯度投影到低维子空间并引入随机探针，减少了联邦学习中差分隐私的通信成本和噪声影响。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中差分隐私带来的高维梯度传输和噪声放大的问题。

Method: 使用紧凑草图将梯度投影到低维子空间，并引入随机探针防止草图退化。

Result: 通信成本从$d$降至$k$，梯度近似误差为$\sigma^2 k$，满足$(\epsilon,\delta)$-差分隐私。

Conclusion: DOME框架有效降低了联邦学习中差分隐私的通信和噪声成本。

Abstract: Federated learning with differential privacy suffers from two major costs:
each client must transmit $d$-dimensional gradients every round, and the
magnitude of DP noise grows with $d$. Yet empirical studies show that gradient
updates exhibit strong temporal correlations and lie in a $k$-dimensional
subspace with $k \ll d$. Motivated by this, we introduce DOME, a decentralized
DP optimization framework in which each client maintains a compact sketch to
project gradients into $\mathbb{R}^k$ before privatization and Secure
Aggregation. This reduces per-round communication from order $d$ to order $k$
and moves towards a gradient approximation mean-squared error of $\sigma^2 k$.
To allow the sketch to span new directions and prevent it from collapsing onto
historical gradients, we augment it with random probes orthogonal to historical
directions. We prove that our overall protocol satisfies
$(\epsilon,\delta)$-Differential Privacy.

</details>


### [22] [Accurate and Efficient World Modeling with Masked Latent Transformers](https://arxiv.org/abs/2507.04075)
*Maxime Burchi,Radu Timofte*

Main category: cs.LG

TL;DR: EMERALD提出了一种高效且准确的世界建模方法，通过空间潜在状态和MaskGIT预测生成潜在空间中的准确轨迹，提升了智能体性能。


<details>
  <summary>Details</summary>
Motivation: Dreamer算法的潜在空间压缩可能导致关键信息丢失，影响智能体性能。现有方法（如Δ-IRIS和DIAMOND）直接从像素训练，效率低且无法利用世界模型学习的内在表示。

Method: EMERALD使用空间潜在状态和MaskGIT预测生成潜在空间中的准确轨迹，优化世界模型。

Result: 在Crafter基准测试中，EMERALD实现了最先进的性能，首次在10M环境步数内超越人类专家表现，并解锁了所有22项成就。

Conclusion: EMERALD提供了一种高效且准确的世界建模方法，显著提升了智能体性能。

Abstract: The Dreamer algorithm has recently obtained remarkable performance across
diverse environment domains by training powerful agents with simulated
trajectories. However, the compressed nature of its world model's latent space
can result in the loss of crucial information, negatively affecting the agent's
performance. Recent approaches, such as $\Delta$-IRIS and DIAMOND, address this
limitation by training more accurate world models. However, these methods
require training agents directly from pixels, which reduces training efficiency
and prevents the agent from benefiting from the inner representations learned
by the world model. In this work, we propose an alternative approach to world
modeling that is both accurate and efficient. We introduce EMERALD (Efficient
MaskEd latent tRAnsformer worLD model), a world model using a spatial latent
state with MaskGIT predictions to generate accurate trajectories in latent
space and improve the agent performance. On the Crafter benchmark, EMERALD
achieves new state-of-the-art performance, becoming the first method to surpass
human experts performance within 10M environment steps. Our method also
succeeds to unlock all 22 Crafter achievements at least once during evaluation.

</details>


### [23] [Evaluating LLMs on Real-World Forecasting Against Human Superforecasters](https://arxiv.org/abs/2507.04562)
*Janna Lu*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在预测未来事件方面表现有所提升，但仍不及人类超级预测者。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在预测任务中的表现，并与人类超级预测者对比。

Method: 评估前沿LLMs在464个Metaculus预测问题上的表现，使用Brier分数作为指标。

Result: 前沿LLMs的Brier分数超过普通人群，但仍显著低于超级预测者。

Conclusion: LLMs在预测任务中取得进展，但仍有提升空间。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their ability to forecast future events remains
understudied. A year ago, large language models struggle to come close to the
accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting
questions from Metaculus, comparing their performance against human
superforecasters. Frontier models achieve Brier scores that ostensibly surpass
the human crowd but still significantly underperform a group of
superforecasters.

</details>


### [24] [Recovering Plasticity of Neural Networks via Soft Weight Rescaling](https://arxiv.org/abs/2507.04683)
*Seungwon Oh,Sangyeon Park,Isaac Han,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 论文提出Soft Weight Rescaling (SWR)方法，通过逐步缩放权重防止无界增长，解决神经网络塑性损失问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中权重无界增长导致塑性损失，影响泛化能力和优化动态，传统重初始化方法会丢失已学信息。

Method: 提出SWR方法，在每一步学习过程中缩放权重，避免无界增长并保持信息。

Result: 理论证明SWR能限制权重幅度并平衡层间权重，实验显示其在多种学习场景中提升性能。

Conclusion: SWR有效解决塑性损失问题，提升网络性能，适用于多种学习任务。

Abstract: Recent studies have shown that as training progresses, neural networks
gradually lose their capacity to learn new information, a phenomenon known as
plasticity loss. An unbounded weight growth is one of the main causes of
plasticity loss. Furthermore, it harms generalization capability and disrupts
optimization dynamics. Re-initializing the network can be a solution, but it
results in the loss of learned information, leading to performance drops. In
this paper, we propose Soft Weight Rescaling (SWR), a novel approach that
prevents unbounded weight growth without losing information. SWR recovers the
plasticity of the network by simply scaling down the weight at each step of the
learning process. We theoretically prove that SWR bounds weight magnitude and
balances weight magnitude between layers. Our experiment shows that SWR
improves performance on warm-start learning, continual learning, and
single-task learning setups on standard image classification benchmarks.

</details>


### [25] [Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness](https://arxiv.org/abs/2507.04690)
*Hanseon Joo,Hayoung Choi,Ook Lee,Minjong Cheon*

Main category: cs.LG

TL;DR: 提出了一种新型神经网络层MJKAN，结合FiLM机制和RBF激活函数，解决了KANs的高计算成本和性能问题，在函数回归任务中表现优异，但在图像和文本分类中需谨慎调整参数。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 虽然具有理论优势，但在实际应用中存在高计算成本和性能不足的问题，因此需要改进。

Method: MJKAN结合了FiLM机制和RBF激活函数，形成了一种混合架构，兼具KANs的非线性表达能力和MLPs的高效性。

Result: 在函数回归任务中表现优异，显著优于MLPs；在图像和文本分类中表现与MLPs相当，但需调整基础函数数量以避免过拟合。

Conclusion: MJKAN继承了KANs的理论优势，同时提高了计算效率和实用性，是一种灵活的架构。

Abstract: Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed
activation functions with learnable univariate functions, but they exhibit
practical limitations, including high computational costs and performance
deficits in general classification tasks. In this paper, we propose the
Modulation Joint KAN (MJKAN), a novel neural network layer designed to overcome
these challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like
mechanism with Radial Basis Function (RBF) activations, creating a hybrid
architecture that combines the non-linear expressive power of KANs with the
efficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's
performance across a diverse set of benchmarks, including function regression,
image classification (MNIST, CIFAR-10/100), and natural language processing (AG
News, SMS Spam). The results demonstrate that MJKAN achieves superior
approximation capabilities in function regression tasks, significantly
outperforming MLPs, with performance improving as the number of basis functions
increases. Conversely, in image and text classification, its performance was
competitive with MLPs but revealed a critical dependency on the number of basis
functions. We found that a smaller basis size was crucial for better
generalization, highlighting that the model's capacity must be carefully tuned
to the complexity of the data to prevent overfitting. In conclusion, MJKAN
offers a flexible architecture that inherits the theoretical advantages of KANs
while improving computational efficiency and practical viability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Limits of Safe AI Deployment: Differentiating Oversight and Control](https://arxiv.org/abs/2507.03525)
*David Manheim,Aidan Homewood*

Main category: cs.AI

TL;DR: 论文区分了AI系统中的监督与控制概念，提出了一个框架来明确其适用条件与局限性，并提出了成熟度模型以支持实践。


<details>
  <summary>Details</summary>
Motivation: 在AI领域，监督与控制常被混淆，导致设计和评估系统时难以实现有效的人类监督。

Method: 通过文献综述区分监督与控制，提出理论框架和成熟度模型。

Result: 明确了监督与控制的定义、适用条件及局限性，并提出了实践指南。

Conclusion: 论文为AI系统的监督与控制提供了清晰的理论和实践基础，支持监管和实践者识别局限与需求。

Abstract: Oversight and control (collectively, supervision) are often invoked as key
levers for ensuring that AI systems are accountable, reliable, and able to
fulfill governance and management requirements. However, the concepts are
frequently conflated or insufficiently distinguished in academic and policy
discourse, undermining efforts to design or evaluate systems that should remain
under meaningful human supervision.
  This paper undertakes a targeted critical review of literature on supervision
outside of AI, along with a brief summary of past work on the topic related to
AI. We then differentiate control as being ex-ante or real-time, and
operational rather than policy or governance. In contrast, oversight is either
a policy and governance function, or is ex-post. We suggest that control aims
to prevent failures. In contrast, oversight often focuses on detection,
remediation, or incentives for future prevention; all preventative oversight
strategies nonetheless necessitate control.
  Building on this foundation, we make three contributions. First, we propose a
theoretically-informed yet policy-grounded framework that articulates the
conditions under which each mechanism is possible, where they fall short, and
what is required to make them meaningful in practice. Second, we outline how
supervision methods should be documented and integrated into risk management,
and drawing on the Microsoft Responsible AI Maturity Model, we outline a
maturity model for AI supervision. Third, we explicitly highlight some
boundaries of these mechanisms, including where they apply, where they fail,
and where it is clear that no existing methods suffice. This foregrounds the
question of whether meaningful supervision is possible in a given deployment
context, and can support regulators, auditors, and practitioners in identifying
both present limitations and the need for new conceptual and technical
advances.

</details>


### [27] [Large Language Models for Combinatorial Optimization: A Systematic Review](https://arxiv.org/abs/2507.03637)
*Francesca Da Ros,Michael Soprano,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型（LLMs）在组合优化（CO）中的应用，基于PRISMA指南筛选了103篇研究，分类总结了LLMs的任务、架构、数据集及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在组合优化领域的应用现状，为研究者提供全面的领域概览和未来研究方向。

Method: 通过Scopus和Google Scholar检索2000多篇文献，依据语言、研究焦点、年份和类型筛选出103篇研究，并进行分类分析。

Result: 总结了LLMs在CO中的任务、架构、专用数据集及应用领域，并提出了未来研究方向。

Conclusion: LLMs在组合优化中具有潜力，未来研究需进一步探索其应用和优化方法。

Abstract: This systematic review explores the application of Large Language Models
(LLMs) in Combinatorial Optimization (CO). We report our findings using the
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
guidelines. We conduct a literature search via Scopus and Google Scholar,
examining over 2,000 publications. We assess publications against four
inclusion and four exclusion criteria related to their language, research
focus, publication year, and type. Eventually, we select 103 studies. We
classify these studies into semantic categories and topics to provide a
comprehensive overview of the field, including the tasks performed by LLMs, the
architectures of LLMs, the existing datasets specifically designed for
evaluating LLMs in CO, and the field of application. Finally, we identify
future directions for leveraging LLMs in this field.

</details>


### [28] [Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing](https://arxiv.org/abs/2507.04105)
*Jinwei Hu,Yi Dong,Zhengtao Ding,Xiaowei Huang*

Main category: cs.AI

TL;DR: 提出了一种用于增强大型语言模型（LLM）驱动的多智能体系统（MAS）在安全关键领域（如航空航天）中的安全性的防御框架。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，LLM驱动的MAS容易受到对抗性行为和幻觉的影响，需要一种实用且可扩展的方法来确保其安全性。

Method: 应用随机平滑技术，结合两阶段自适应采样机制，在无需传统验证方法的黑盒设置下提供概率保证。

Result: 模拟结果表明，该方法能有效防止对抗性行为和幻觉的传播，同时保持共识性能。

Conclusion: 为LLM驱动的MAS在现实高风险环境中的安全部署提供了实用且可扩展的解决方案。

Abstract: This paper presents a defense framework for enhancing the safety of large
language model (LLM) empowered multi-agent systems (MAS) in safety-critical
domains such as aerospace. We apply randomized smoothing, a statistical
robustness certification technique, to the MAS consensus context, enabling
probabilistic guarantees on agent decisions under adversarial influence. Unlike
traditional verification methods, our approach operates in black-box settings
and employs a two-stage adaptive sampling mechanism to balance robustness and
computational efficiency. Simulation results demonstrate that our method
effectively prevents the propagation of adversarial behaviors and
hallucinations while maintaining consensus performance. This work provides a
practical and scalable path toward safe deployment of LLM-based MAS in
real-world, high-stakes environments.

</details>


### [29] [Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems](https://arxiv.org/abs/2507.04338)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.AI

TL;DR: 提出了一种可配置的winner-take-all电路，支持k-winner和滞后特性，功耗低且延迟小，适用于空间滤波和分类。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算中的winner-take-all电路是关键学习单元，但现有设计缺乏灵活性和低功耗特性。

Method: 在IBM 65 nm工艺节点上模拟了一种可配置的winner-take-all电路，支持k-winner和滞后特性。

Result: 电路功耗为34.9 μW，延迟10.4 ns，可处理1000个输入，适用于空间滤波和分类任务。

Conclusion: 该电路在神经形态计算中具有潜力，能够高效支持多种学习任务。

Abstract: Recent advances in neuromorphic computing demonstrate on-device learning
capabilities with low power consumption. One of the key learning units in these
systems is the winner-take-all circuit. In this research, we propose a
winner-take-all circuit that can be configured to achieve k-winner and
hysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9
$\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The
utility of the circuit is demonstrated for spatial filtering and
classification.

</details>


### [30] [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246)
*Scott Emmons,Erik Jenner,David K. Elson,Rif A. Saurous,Senthooran Rajamanoharan,Heng Chen,Irhum Shafkat,Rohin Shah*

Main category: cs.AI

TL;DR: 论文探讨了链式思维（CoT）监控在AI安全防御中的可靠性问题，提出了监控性的重要性，并区分了CoT作为合理化与计算的不同用途。通过实验验证了复杂推理的必要性，并提出了压力测试方法。


<details>
  <summary>Details</summary>
Motivation: 针对CoT监控的不可靠性问题，特别是其在事后合理化中的局限性，研究旨在探索其在预防严重危害时的有效性，强调监控性的关键作用。

Method: 引入概念框架区分CoT的两种用途，通过增加行为难度强制模型暴露推理过程，并设计压力测试方法评估监控效果。

Result: 实验表明，模型在复杂推理任务中会暴露意图，但在人为干预下可能隐藏意图。CoT监控虽非完美，但仍具显著防御价值。

Conclusion: CoT监控需持续压力测试和主动保护，虽不完美，但在预防严重危害中具有重要防御作用。

Abstract: While chain-of-thought (CoT) monitoring is an appealing AI safety defense,
recent work on "unfaithfulness" has cast doubt on its reliability. These
findings highlight an important failure mode, particularly when CoT acts as a
post-hoc rationalization in applications like auditing for bias. However, for
the distinct problem of runtime monitoring to prevent severe harm, we argue the
key property is not faithfulness but monitorability. To this end, we introduce
a conceptual framework distinguishing CoT-as-rationalization from
CoT-as-computation. We expect that certain classes of severe harm will require
complex, multi-step reasoning that necessitates CoT-as-computation. Replicating
the experimental setups of prior work, we increase the difficulty of the bad
behavior to enforce this necessity condition; this forces the model to expose
its reasoning, making it monitorable. We then present methodology guidelines to
stress-test CoT monitoring against deliberate evasion. Applying these
guidelines, we find that models can learn to obscure their intentions, but only
when given significant help, such as detailed human-written strategies or
iterative optimization against the monitor. We conclude that, while not
infallible, CoT monitoring offers a substantial layer of defense that requires
active protection and continued stress-testing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [31] [Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models](https://arxiv.org/abs/2507.02928)
*Hao Yang,Haoxuan Li,Luyu Chen,Haoxiang Wang,Xu Chen,Mingming Gong*

Main category: cs.CL

TL;DR: 论文提出ProCI框架，利用大语言模型（LLMs）的语义推理和世界知识，逐步生成、填充和验证隐藏混杂因素，以解决观测数据中隐藏混杂导致的因果估计偏差问题。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的隐藏混杂因素会导致因果估计偏差，现有方法大多依赖无混杂假设，而LLMs的潜力尚未充分挖掘。

Method: 提出ProCI框架，通过LLMs的语义推理和世界知识，迭代生成、填充和验证隐藏混杂因素，并采用分布推理策略避免输出崩溃。

Result: 实验表明，ProCI能发现有意义混杂因素，显著提升多种数据集和LLMs下的治疗效果估计。

Conclusion: ProCI首次利用LLMs解决隐藏混杂问题，为因果推断提供了新思路。

Abstract: Hidden confounding remains a central challenge in estimating treatment
effects from observational data, as unobserved variables can lead to biased
causal estimates. While recent work has explored the use of large language
models (LLMs) for causal inference, most approaches still rely on the
unconfoundedness assumption. In this paper, we make the first attempt to
mitigate hidden confounding using LLMs. We propose ProCI (Progressive
Confounder Imputation), a framework that elicits the semantic and world
knowledge of LLMs to iteratively generate, impute, and validate hidden
confounders. ProCI leverages two key capabilities of LLMs: their strong
semantic reasoning ability, which enables the discovery of plausible
confounders from both structured and unstructured inputs, and their embedded
world knowledge, which supports counterfactual reasoning under latent
confounding. To improve robustness, ProCI adopts a distributional reasoning
strategy instead of direct value imputation to prevent the collapsed outputs.
Extensive experiments demonstrate that ProCI uncovers meaningful confounders
and significantly improves treatment effect estimation across various datasets
and LLMs.

</details>


### [32] [GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models](https://arxiv.org/abs/2507.02986)
*Seshu Tirupathi,Dhaval Salwala,Elizabeth Daly,Inge Vejsbjerg*

Main category: cs.CL

TL;DR: GAF-Guard是一个新型的LLM治理框架，专注于用户、用例和模型本身，通过自主代理检测风险并持续监控，以增强AI安全性和用户期望。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，需要严格监控以防止负面后果并确保稳健性，同时需与人类价值观对齐。当前监控系统缺乏对具体用例和用户需求的关注。

Method: GAF-Guard框架通过建模自主代理，识别风险并激活检测工具，在特定用例中实现持续监控和报告。

Result: GAF-Guard能够有效检测和监控LLM应用部署中的风险，提升AI安全性和用户满意度。

Conclusion: GAF-Guard为LLM治理提供了以用户和用例为中心的解决方案，填补了现有系统的不足。

Abstract: As Large Language Models (LLMs) continue to be increasingly applied across
various domains, their widespread adoption necessitates rigorous monitoring to
prevent unintended negative consequences and ensure robustness. Furthermore,
LLMs must be designed to align with human values, like preventing harmful
content and ensuring responsible usage. The current automated systems and
solutions for monitoring LLMs in production are primarily centered on
LLM-specific concerns like hallucination etc, with little consideration given
to the requirements of specific use-cases and user preferences. This paper
introduces GAF-Guard, a novel agentic framework for LLM governance that places
the user, the use-case, and the model itself at the center. The framework is
designed to detect and monitor risks associated with the deployment of LLM
based applications. The approach models autonomous agents that identify risks,
activate risk detection tools, within specific use-cases and facilitate
continuous monitoring and reporting to enhance AI safety, and user
expectations. The code is available at
https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.

</details>


### [33] [`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/abs/2507.02990)
*Annika M Schoene,Cansu Canca*

Main category: cs.CL

TL;DR: 论文提出两种新的测试案例，用于绕过大型语言模型的安全防护，生成有害内容，并呼吁更全面的AI安全措施。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已有安全协议，但仍易受对抗性提示攻击，尤其是在心理健康领域。

Method: 使用多步提示级越狱技术，绕过内置内容与安全过滤器，对六种广泛使用的LLM进行实证评估。

Result: 实验证明，用户意图被忽视，生成了可能导致现实伤害的详细有害内容。

Conclusion: 需更系统化的AI安全方法，强调持续对抗测试，并承认当前通用LLM在全面安全性上的技术挑战。

Abstract: Recent advances in large language models (LLMs) have led to increasingly
sophisticated safety protocols and features designed to prevent harmful,
unethical, or unauthorized outputs. However, these guardrails remain
susceptible to novel and creative forms of adversarial prompting, including
manually generated test cases. In this work, we present two new test cases in
mental health for (i) suicide and (ii) self-harm, using multi-step,
prompt-level jailbreaking and bypass built-in content and safety filters. We
show that user intent is disregarded, leading to the generation of detailed
harmful content and instructions that could cause real-world harm. We conduct
an empirical evaluation across six widely available LLMs, demonstrating the
generalizability and reliability of the bypass. We assess these findings and
the multilayered ethical tensions that they present for their implications on
prompt-response filtering and context- and task-specific model development. We
recommend a more comprehensive and systematic approach to AI safety and ethics
while emphasizing the need for continuous adversarial testing in
safety-critical AI deployments. We also argue that while certain clearly
defined safety measures and guardrails can and must be implemented in LLMs,
ensuring robust and comprehensive safety across all use cases and domains
remains extremely challenging given the current technical maturity of
general-purpose LLMs.

</details>


### [34] [Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large Language Models](https://arxiv.org/abs/2507.03933)
*Eva Vanmassenhove*

Main category: cs.CL

TL;DR: 多语言大语言模型（LLMs）改变了技术对语言的影响方式，但可能导致语言多样性丧失。


<details>
  <summary>Details</summary>
Motivation: 探讨模型崩溃（model collapse）如何通过翻译技术等导致语言形式、语法特征和文化细微差别的消失。

Method: 结合计算机视觉、自然语言处理（NLP）和机器翻译（MT）的最新研究，分析模型自我强化的训练循环对语言多样性的影响。

Result: 语言分布的尾部特征正在消失，随之而来的是相关叙事和身份的丧失。

Conclusion: 呼吁抵制语言扁平化，重新构想NLP领域，以鼓励和保护多语言表达的多样性和创造力。

Abstract: Multilingual Large Language Models (LLMs) considerably changed how
technologies can influence language. While previous technologies could mediate
or assist humans, there is now a tendency to \textit{offload} the task of
writing itself to these technologies, enabling them to change our linguistic
ecosystem more directly. While they provide us quick access to information and
impressively fluent output, beneath their apparent sophistication lies a
subtle, more insidious threat: the gradual decline and loss of linguistic
diversity. With this opinion piece, I explore how model collapse, with a
particular focus on translation technology, can lead to the loss of linguistic
forms, grammatical features, and cultural nuance. Model collapse refers to the
eventual consequence of self-consuming training loops, where models reinforce
their own biases and lose linguistic diversity. Drawing on recent work in
Computer Vision, Natural Language Processing (NLP) and Machine Translation
(MT), I argue that the tails of our linguistic distributions are vanishing, and
with them, the narratives and identities they carry. This is a call to resist
linguistic flattening and to reimagine NLP as a field that encourages, values
and protects expressive multilingual lexical and linguistic diversity and
creativity.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [35] [Label-Free Long-Horizon 3D UAV Trajectory Prediction via Motion-Aligned RGB and Event Cues](https://arxiv.org/abs/2507.03365)
*Hanfang Liang,Shenghai Yuan,Fen Liu,Yizhuo Yang,Bing Wang,Zhuyu Huang,Chenyang Shi,Jing Jin*

Main category: cs.RO

TL;DR: 提出了一种无监督视觉方法，用于预测无人机的三维轨迹，结合LiDAR点云和相机图像，通过自监督学习显著提升了长时轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 消费级无人机的广泛使用对空域安全和公共安全构成挑战，现有方法难以预测其未来轨迹，需要更有效的解决方案。

Method: 使用无监督技术从LiDAR点云提取轨迹，通过运动一致性对齐相机图像生成伪标签，结合运动估计和视觉Mamba神经网络进行自监督预测。

Result: 在MMAUD数据集上表现优异，5秒3D误差降低约40%，优于现有监督方法。

Conclusion: 该方法为实时反无人机部署提供了经济高效、可扩展的解决方案，代码将开源以支持可重复研究。

Abstract: The widespread use of consumer drones has introduced serious challenges for
airspace security and public safety. Their high agility and unpredictable
motion make drones difficult to track and intercept. While existing methods
focus on detecting current positions, many counter-drone strategies rely on
forecasting future trajectories and thus require more than reactive detection
to be effective. To address this critical gap, we propose an unsupervised
vision-based method for predicting the three-dimensional trajectories of
drones. Our approach first uses an unsupervised technique to extract drone
trajectories from raw LiDAR point clouds, then aligns these trajectories with
camera images through motion consistency to generate reliable pseudo-labels. We
then combine kinematic estimation with a visual Mamba neural network in a
self-supervised manner to predict future drone trajectories. We evaluate our
method on the challenging MMAUD dataset, including the V2 sequences that
feature wide-field-of-view multimodal sensors and dynamic UAV motion in urban
scenes. Extensive experiments show that our framework outperforms supervised
image-only and audio-visual baselines in long-horizon trajectory prediction,
reducing 5-second 3D error by around 40 percent without using any manual 3D
labels. The proposed system offers a cost-effective, scalable alternative for
real-time counter-drone deployment. All code will be released upon acceptance
to support reproducible research in the robotics community.

</details>


### [36] [Multi-robot Aerial Soft Manipulator For Floating Litter Collection](https://arxiv.org/abs/2507.03517)
*Antonio González-Morgado,Sander Smits,Guillermo Heredia,Anibal Ollero,Alexandre Krupa,François Chaumette,Fabien Spindler,Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: 提出了一种多机器人空中软操纵器系统，用于水面漂浮垃圾收集，通过双无人机和柔性绳索操纵器提高负载能力和飞行耐力，并减少下洗效应。


<details>
  <summary>Details</summary>
Motivation: 保护水生生态系统和防止环境污染需要有效的水面漂浮垃圾清除方法。

Method: 系统由两架无人机和柔性绳索操纵器组成，采用基于钩的工具收集垃圾，并利用优化绳索形状规划器和视觉伺服控制器。

Result: 户外实验验证了系统的有效性，自适应规划器提高了操作成功率，实际水道测试证明了其垃圾收集能力。

Conclusion: 该系统展示了无人机在自主清除水生环境垃圾中的潜力。

Abstract: Removing floating litter from water bodies is crucial to preserving aquatic
ecosystems and preventing environmental pollution. In this work, we present a
multi-robot aerial soft manipulator for floating litter collection, leveraging
the capabilities of aerial robots. The proposed system consists of two aerial
robots connected by a flexible rope manipulator, which collects floating litter
using a hook-based tool. Compared to single-aerial-robot solutions, the use of
two aerial robots increases payload capacity and flight endurance while
reducing the downwash effect at the manipulation point, located at the midpoint
of the rope. Additionally, we employ an optimization-based rope-shape planner
to compute the desired rope shape. The planner incorporates an adaptive
behavior that maximizes grasping capabilities near the litter while minimizing
rope tension when farther away. The computed rope shape trajectory is
controlled by a shape visual servoing controller, which approximates the rope
as a parabola. The complete system is validated in outdoor experiments,
demonstrating successful grasping operations. An ablation study highlights how
the planner's adaptive mechanism improves the success rate of the operation.
Furthermore, real-world tests in a water channel confirm the effectiveness of
our system in floating litter collection. These results demonstrate the
potential of aerial robots for autonomous litter removal in aquatic
environments.

</details>


### [37] [Hardware-Free Event Cameras Temporal Synchronization Based on Event Density Alignment](https://arxiv.org/abs/2507.04314)
*Wenxuan Li,Yan Dong,Shaoqiang Qiu,Bin Han*

Main category: cs.RO

TL;DR: 提出了一种无需硬件的多事件相机同步方法，通过最小化事件密度分布差异来校准时间偏移，实验误差小于10ms。


<details>
  <summary>Details</summary>
Motivation: 多事件相机因触发和传输延迟导致时间偏移，硬件同步方法受限，需软件解决方案。

Method: 通过最小化事件密度分布差异确定时间差，调整时间戳实现同步。

Result: 实验表明，该方法在不同场景和相机模型下同步误差小于10ms。

Conclusion: 该方法有效解决了硬件同步的限制，适用于多种事件相机模型。

Abstract: Event cameras are a novel type of sensor designed for capturing the dynamic
changes of a scene. Due to factors such as trigger and transmission delays, a
time offset exists in the data collected by multiple event cameras, leading to
inaccurate information fusion. Thus, the collected data needs to be
synchronized to overcome any potential time offset issue. Hardware
synchronization methods require additional circuits, while certain models of
event cameras (e.g., CeleX5) do not support hardware synchronization.
Therefore, this paper proposes a hardware-free event camera synchronization
method. This method determines differences between start times by minimizing
the dissimilarity of the event density distributions of different event cameras
and synchronizes the data by adjusting timestamps. The experiments demonstrate
that the method's synchronization error is less than 10ms under various senses
with multiple models of event cameras.

</details>


### [38] [Safe Bimanual Teleoperation with Language-Guided Collision Avoidance](https://arxiv.org/abs/2507.04791)
*Dionis Totsila,Clemente Donoso,Enrico Mingo Hoffman,Jean-Baptiste Mouret,Serena Ivaldi*

Main category: cs.RO

TL;DR: 提出了一种结合VR控制和语音避障的安全遥操作系统，用于在杂乱环境中提升操作安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决操作者在杂乱环境中遥操作时空间感知有限和距离估计困难的问题。

Method: 结合沉浸式VR控制和语音激活的避障功能，通过视觉分割和3D障碍物网格整合到全身控制器中。

Result: 实验表明系统显著提高了操作安全性，同时不影响任务效率。

Conclusion: 系统有效解决了遥操作中的碰撞问题，提升了操作安全性。

Abstract: Teleoperating precise bimanual manipulations in cluttered environments is
challenging for operators, who often struggle with limited spatial perception
and difficulty estimating distances between target objects, the robot's body,
obstacles, and the surrounding environment. To address these challenges, local
robot perception and control should assist the operator during teleoperation.
In this work, we introduce a safe teleoperation system that enhances operator
control by preventing collisions in cluttered environments through the
combination of immersive VR control and voice-activated collision avoidance.
Using HTC Vive controllers, operators directly control a bimanual mobile
manipulator, while spoken commands such as "avoid the yellow tool" trigger
visual grounding and segmentation to build 3D obstacle meshes. These meshes are
integrated into a whole-body controller to actively prevent collisions during
teleoperation. Experiments in static, cluttered scenes demonstrate that our
system significantly improves operational safety without compromising task
efficiency.

</details>
