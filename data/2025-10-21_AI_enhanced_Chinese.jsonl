{"id": "2510.15965", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15965", "abs": "https://arxiv.org/abs/2510.15965", "authors": ["Mohan Zhang", "Yihua Zhang", "Jinghan Jia", "Zhangyang Wang", "Sijia Liu", "Tianlong Chen"], "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model", "comment": "NeurIPS 2025", "summary": "Modern large reasoning models (LRMs) exhibit impressive multi-step\nproblem-solving via chain-of-thought (CoT) reasoning. However, this iterative\nthinking mechanism introduces a new vulnerability surface. We present the\nDeadlock Attack, a resource exhaustion method that hijacks an LRM's generative\ncontrol flow by training a malicious adversarial embedding to induce perpetual\nreasoning loops. Specifically, the optimized embedding encourages transitional\ntokens (e.g., \"Wait\", \"But\") after reasoning steps, preventing the model from\nconcluding its answer. A key challenge we identify is the\ncontinuous-to-discrete projection gap: na\\\"ive projections of adversarial\nembeddings to token sequences nullify the attack. To overcome this, we\nintroduce a backdoor implantation strategy, enabling reliable activation\nthrough specific trigger tokens. Our method achieves a 100% attack success rate\nacross four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three\nmath reasoning benchmarks, forcing models to generate up to their maximum token\nlimits. The attack is also stealthy (in terms of causing negligible utility\nloss on benign user inputs) and remains robust against existing strategies\ntrying to mitigate the overthinking issue. Our findings expose a critical and\nunderexplored security vulnerability in LRMs from the perspective of reasoning\n(in)efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6b7b\u9501\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6076\u610f\u5bf9\u6297\u5d4c\u5165\u8bf1\u5bfc\u6a21\u578b\u9677\u5165\u65e0\u9650\u63a8\u7406\u5faa\u73af\uff0c\u963b\u6b62\u5176\u8f93\u51fa\u6700\u7ec8\u7b54\u6848\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u673a\u5236\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fd9\u79cd\u8fed\u4ee3\u601d\u7ef4\u673a\u5236\u8fdb\u884c\u653b\u51fb\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684\u5bf9\u6297\u5d4c\u5165\u9f13\u52b1\u63a8\u7406\u6b65\u9aa4\u540e\u7684\u8fc7\u6e21\u6027\u6807\u8bb0\uff0c\u7ed3\u5408\u540e\u95e8\u690d\u5165\u7b56\u7565\u786e\u4fdd\u653b\u51fb\u53ef\u9760\u6fc0\u6d3b\uff0c\u514b\u670d\u8fde\u7eed\u5230\u79bb\u6563\u7684\u6295\u5f71\u95f4\u9699\u95ee\u9898\u3002", "result": "\u5728\u56db\u79cd\u5148\u8fdbLRM\u548c\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u8feb\u4f7f\u6a21\u578b\u751f\u6210\u8fbe\u5230\u6700\u5927\u6807\u8bb0\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u826f\u6027\u8f93\u5165\u7684\u6548\u7528\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u8fd9\u79cd\u653b\u51fb\u5bf9\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16235", "abs": "https://arxiv.org/abs/2510.16235", "authors": ["Vishal Manikanden", "Aniketh Bandlamudi", "Daniel Haehn"], "title": "Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection", "comment": null, "summary": "Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head\nand neck cancer. Due to the subtle nature of its early stages, deep and hidden\nareas of development, and slow growth, OCSCC often goes undetected, leading to\npreventable deaths. However, properly trained Convolutional Neural Networks\n(CNNs), with their precise image segmentation techniques and ability to apply\nkernel matrices to modify the RGB values of images for accurate image pattern\nrecognition, would be an effective means for early detection of OCSCC. Pairing\nthis neural network with image capturing and processing hardware would allow\nincreased efficacy in OCSCC detection. The aim of our project is to develop a\nConvolutional Neural Network trained to recognize OCSCC, as well as to design a\nphysical hardware system to capture and process detailed images, in order to\ndetermine the image quality required for accurate predictions. A CNN was\ntrained on 4293 training images consisting of benign and malignant tumors, as\nwell as negative samples, and was evaluated for its precision, recall, and Mean\nAverage Precision (mAP) in its predictions of OCSCC. A testing dataset of\nrandomly assorted images of cancerous, non-cancerous, and negative images was\nchosen, and each image was altered to represent 5 common resolutions. This test\ndata set was thoroughly analyzed by the CNN and predictions were scored on the\nbasis of accuracy. The designed enhancement hardware was used to capture\ndetailed images, and its impact was scored. An application was developed to\nfacilitate the testing process and bring open access to the CNN. Images of\nincreasing resolution resulted in higher-accuracy predictions on a logarithmic\nscale, demonstrating the diminishing returns of higher pixel counts.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684OCSCC\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7ed3\u5408\u56fe\u50cf\u91c7\u96c6\u786c\u4ef6\uff0c\u7814\u7a76\u56fe\u50cf\u5206\u8fa8\u7387\u5bf9\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u65e9\u671f\u75c7\u72b6\u9690\u853d\u3001\u751f\u957f\u7f13\u6162\u4e14\u53d1\u751f\u5728\u6df1\u90e8\u533a\u57df\uff0c\u5e38\u88ab\u6f0f\u8bca\u5bfc\u81f4\u53ef\u9884\u9632\u7684\u6b7b\u4ea1\uff0c\u9700\u8981\u6709\u6548\u7684\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3CNN\u6a21\u578b\u8bc6\u522bOCSCC\uff0c\u4f7f\u75284293\u5f20\u8bad\u7ec3\u56fe\u50cf\uff08\u826f\u6027/\u6076\u6027\u80bf\u7624\u548c\u9634\u6027\u6837\u672c\uff09\uff0c\u8bbe\u8ba1\u56fe\u50cf\u91c7\u96c6\u786c\u4ef6\uff0c\u6d4b\u8bd5\u4e0d\u540c\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u56fe\u50cf\u5206\u8fa8\u7387\u8d8a\u9ad8\uff0c\u68c0\u6d4b\u51c6\u786e\u6027\u8d8a\u9ad8\uff0c\u4f46\u5448\u73b0\u5bf9\u6570\u589e\u957f\u8d8b\u52bf\uff0c\u9ad8\u50cf\u7d20\u6570\u5e26\u6765\u7684\u6536\u76ca\u9012\u51cf\u3002", "conclusion": "CNN\u7ed3\u5408\u56fe\u50cf\u91c7\u96c6\u786c\u4ef6\u53ef\u6709\u6548\u68c0\u6d4bOCSCC\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u5bf9\u68c0\u6d4b\u51c6\u786e\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u9ad8\u5206\u8fa8\u7387\u5e26\u6765\u7684\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002"}}
{"id": "2510.17191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17191", "abs": "https://arxiv.org/abs/2510.17191", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving", "comment": "6 pages, 2 figures, 2 tables", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86SimpleVSF\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8f68\u8ff9\u8bc4\u5206\u6280\u672f\u6765\u589e\u5f3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6027\u80fd", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u51b3\u7b56\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u597d\u7684\u8ba4\u77e5\u80fd\u529b\u548c\u51b3\u7b56\u673a\u5236", "method": "\u7ed3\u5408\u4f20\u7edf\u8bc4\u5206\u5668\u548cVLM\u589e\u5f3a\u8bc4\u5206\u5668\uff0c\u4f7f\u7528\u6743\u91cd\u878d\u5408\u5668\u8fdb\u884c\u5b9a\u91cf\u805a\u5408\uff0c\u4ee5\u53ca\u57fa\u4e8eVLM\u7684\u878d\u5408\u5668\u8fdb\u884c\u5b9a\u6027\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56", "result": "\u5728ICCV 2025 NAVSIM v2\u7aef\u5230\u7aef\u9a7e\u9a76\u6311\u6218\u4e2d\u53d6\u5f97\u9886\u5148\u5730\u4f4d\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "SimpleVSF\u6846\u67b6\u5728\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861"}}
{"id": "2510.17203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17203", "abs": "https://arxiv.org/abs/2510.17203", "authors": ["Ryota Soga", "Masataka Kobayashi", "Tsukasa Shimizu", "Shintaro Shiba", "Quan Kong", "Shan Lu", "Takaya Yamazato"], "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera", "comment": "7pages, APCC2025", "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65b0\u578b\u81ea\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u96c6\u6210\u53ef\u89c1\u5149\u901a\u4fe1(VLC)\u548c\u53ef\u89c1\u5149\u5b9a\u4f4d(VLP)\uff0c\u53ef\u5728GPS\u5931\u6548\u73af\u5883\uff08\u5982\u96a7\u9053\uff09\u4e2d\u4e3a\u8f66\u8f86\u63d0\u4f9b\u4f4d\u7f6e\u4f30\u8ba1\u3002", "motivation": "\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u7279\u6027\uff0c\u5728GPS\u5931\u6548\u73af\u5883\u4e0b\u5b9e\u73b0\u8f66\u8f86\u5b9a\u4f4d\uff0c\u89e3\u51b3\u96a7\u9053\u7b49\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\u3002", "method": "\u4f7f\u7528Walsh-Hadamard\u7801\u4e3a\u591a\u4e2aLED\u5206\u914d\u552f\u4e00\u5bfc\u9891\u5e8f\u5217\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u8bc6\u522b\u5355\u4e2aLED\uff0c\u5b9e\u73b0VLC\u9ad8\u5bb9\u91cf\u901a\u4fe1\u548c\u57fa\u4e8e\u76f8\u4f4d\u76f8\u5173(POC)\u7684\u7cbe\u786e\u8ddd\u79bb\u4f30\u8ba1\u3002", "result": "\u572830km/h\u8f66\u901f\u4e0b\u8fdb\u884c\u5b9e\u5730\u6d4b\u8bd5\uff0c\u8ddd\u79bb\u4f30\u8ba1\u5747\u65b9\u6839\u8bef\u5dee\u5c0f\u4e8e0.75\u7c73\uff08\u8303\u56f4\u8fbe100\u7c73\uff09\uff0c\u8bef\u7801\u7387\u4f4e\u4e8e0.01\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4f7f\u7528\u5355\u4e2a\u4e8b\u4ef6\u76f8\u673a\u540c\u65f6\u5b9e\u73b0VLC\u548cVLP\u529f\u80fd\u7684\u8f66\u8f86\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2510.16014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16014", "abs": "https://arxiv.org/abs/2510.16014", "authors": ["Hanyin Cheng", "Ruitong Zhang", "Yuning Lu", "Peng Chen", "Meng Wang", "Yang Shu", "Bin Yang", "Chenjuan Guo"], "title": "STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter", "comment": null, "summary": "While Time Series Foundation Models (TSFMs) have demonstrated remarkable\nsuccess in Multivariate Time Series Anomaly Detection (MTSAD), however, in\nreal-world industrial scenarios, many time series comprise not only numerical\nvariables such as temperature and flow, but also numerous discrete state\nvariables that describe the system status, such as valve on/off or day of the\nweek. Existing TSFMs often overlook the distinct categorical nature of state\nvariables and their critical role as conditions, typically treating them\nuniformly with numerical variables. This inappropriate modeling approach\nprevents the model from fully leveraging state information and even leads to a\nsignificant degradation in detection performance after state variables are\nintegrated. To address this critical limitation, this paper proposes a novel\nSTate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance\nthe capability of TSFMs in modeling and leveraging state variables during the\nfine-tuning stage. Specifically, STAR comprisesthree core components: (1) We\ndesign an Identity-guided State Encoder, whicheffectively captures the complex\ncategorical semantics of state variables through a learnable State Memory. (2)\nWe propose a Conditional Bottleneck Adapter, which dynamically generates\nlow-rank adaptation parameters conditioned on the current state, thereby\nflexibly injecting the influence of state variables into the backbone model.\n(3) We also introduce a Numeral-State Matching module to more effectively\ndetect anomalies inherent to the state variables themselves. Extensive\nexperiments conducted on real-world datasets demonstrate that STAR can improve\nthe performance of existing TSFMs on MTSAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86STAR\uff08STate-aware AdapteR\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5bf9\u72b6\u6001\u53d8\u91cf\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5ffd\u89c6\u72b6\u6001\u53d8\u91cf\u5206\u7c7b\u7279\u6027\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\u65f6\u95f4\u5e8f\u5217\u5305\u542b\u6570\u503c\u53d8\u91cf\u548c\u79bb\u6563\u72b6\u6001\u53d8\u91cf\uff0c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u5e38\u5ffd\u89c6\u72b6\u6001\u53d8\u91cf\u7684\u5206\u7c7b\u7279\u6027\uff0c\u5c06\u5176\u4e0e\u6570\u503c\u53d8\u91cf\u7edf\u4e00\u5904\u7406\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u72b6\u6001\u4fe1\u606f\u751a\u81f3\u6027\u80fd\u4e0b\u964d\u3002", "method": "STAR\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8eab\u4efd\u5f15\u5bfc\u7684\u72b6\u6001\u7f16\u7801\u5668\u3001\u6761\u4ef6\u74f6\u9888\u9002\u914d\u5668\u548c\u6570\u503c-\u72b6\u6001\u5339\u914d\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u72b6\u6001\u8bb0\u5fc6\u6355\u83b7\u72b6\u6001\u53d8\u91cf\u7684\u5206\u7c7b\u8bed\u4e49\uff0c\u52a8\u6001\u751f\u6210\u6761\u4ef6\u9002\u914d\u53c2\u6570\uff0c\u6709\u6548\u68c0\u6d4b\u72b6\u6001\u53d8\u91cf\u672c\u8eab\u7684\u5f02\u5e38\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSTAR\u80fd\u591f\u663e\u8457\u63d0\u5347\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "STAR\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5bf9\u72b6\u6001\u53d8\u91cf\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.16685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16685", "abs": "https://arxiv.org/abs/2510.16685", "authors": ["Damin Zhang", "Julia Rayz"], "title": "Temporal Understanding under Deictic Frame of Reference", "comment": "Under review", "summary": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TUuD\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u53d8\u5316\u7684\"\u73b0\u5728\"\u53c2\u8003\u70b9\u4e0b\u5bf9\u65f6\u95f4-\u4e8b\u4ef6\u548c\u4e8b\u4ef6-\u4e8b\u4ef6\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u57fa\u4e8e\u611f\u5b98\u8fd0\u52a8\u7ecf\u9a8c\u7684\u7a7a\u95f4\u9690\u55bb\u6765\u6982\u5ff5\u5316\u65f6\u95f4\u4f53\u9a8c\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u5982\u4f55\u89e3\u91ca\u968f\u65f6\u95f4\u52a8\u6001\u53d8\u5316\u7684\u53c2\u8003\u6846\u67b6\u3002", "method": "\u5f15\u5165TUuD\u6846\u67b6\uff0c\u8ba9LLMs\u5bf9\u5f53\u524d\u65f6\u523b\u4e0e\u76ee\u6807\u4e8b\u4ef6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u8bc4\u5206\uff080.00-1.00\uff09\uff0c\u91cf\u5316\u4e24\u4e2a\u65f6\u95f4\u70b9\u4e4b\u95f4\u7684\u611f\u77e5\u65f6\u95f4\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u56db\u4e2a\u8bc4\u4f30\u7684LLMs\u663e\u793a\u51fa\u5bf9\u6307\u793a\u6027\u65f6\u95f4\u53c2\u8003\u6846\u67b6\u7684\u53ef\u6d4b\u91cf\u9002\u5e94\uff0c\u76f8\u4f3c\u6027\u8bc4\u5206\u5728\u73b0\u5728\u9644\u8fd1\u8fbe\u5230\u5cf0\u503c\uff0c\u5e76\u5411\u8fc7\u53bb\u548c\u672a\u6765\u4e8b\u4ef6\u9012\u51cf\uff0c\u4f46\u8fd9\u79cd\u9002\u5e94\u5728\u8d85\u51fa\u8fd1\u671f\u60c5\u5883\u65f6\u4f1a\u51cf\u5f31\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u90e8\u5206\u7c7b\u4eba\u7684\u65f6\u95f4\u8ba4\u77e5\uff0c\u4f46\u5b83\u4eec\u7684\u65f6\u5e8f\u63a8\u7406\u4ecd\u7136\u5bf9\u53c2\u8003\u6846\u67b6\u53d8\u5316\u548c\u65f6\u95f4\u8ddd\u79bb\u654f\u611f\u3002"}}
{"id": "2510.16708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16708", "abs": "https://arxiv.org/abs/2510.16708", "authors": ["Kailai Yang", "Yan Leng", "Xin Zhang", "Tianlin Zhang", "Paul Thompson", "Bernard Keavney", "Maciej Tomaszewski", "Sophia Ananiadou"], "title": "Natural Language Processing Applications in Cardiology: A Narrative Review", "comment": null, "summary": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u56de\u987e\u4e862014-2025\u5e74\u95f4\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\uff0c\u5206\u6790\u4e86265\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u4eceNLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u76f8\u5173\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u65e5\u76ca\u666e\u904d\u4e14\u590d\u6742\uff0c\u76f8\u5173\u4fe1\u606f\u5206\u6563\u5728\u60a3\u8005\u53d9\u8ff0\u3001\u533b\u7597\u8bb0\u5f55\u548c\u79d1\u5b66\u6587\u732e\u7b49\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u4e2d\u3002NLP\u6280\u672f\u80fd\u591f\u5206\u6790\u8fd9\u4e9b\u6570\u636e\uff0c\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u6df1\u5165\u4e86\u89e3\u5fc3\u810f\u75c5\u5b66\uff0c\u4ece\u800c\u9769\u65b0\u5fc3\u810f\u95ee\u9898\u7684\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u9884\u9632\u65b9\u6cd5\u3002", "method": "\u67e5\u8be2\u4e86\u516d\u4e2a\u6587\u732e\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u4e25\u683c\u7b5b\u9009\u8fc7\u7a0b\u8bc6\u522b\u4e86265\u7bc7\u76f8\u5173\u6587\u7ae0\uff0c\u4eceNLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u76f8\u5173\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u8fdb\u884c\u65f6\u95f4\u5206\u6790\u4ee5\u5c55\u793a\u65b9\u6cd5\u6f14\u53d8\u8d8b\u52bf\u3002", "result": "\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u7ef4\u5ea6\u5185\u5b58\u5728\u76f8\u5f53\u5927\u7684\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86NLP\u7814\u7a76\u5728\u8be5\u9886\u57df\u7684\u5e7f\u5ea6\u3002\u65f6\u95f4\u5206\u6790\u63ed\u793a\u4e86\u6240\u6db5\u76d6\u8fc7\u53bb\u5341\u5e74\u4e2dNLP\u65b9\u6cd5\u7684\u6f14\u53d8\u548c\u53d8\u5316\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u5bf9\u5fc3\u810f\u75c5\u5b66\u9886\u57dfNLP\u7814\u7a76\u6700\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5c55\u793a\u4e86NLP\u6280\u672f\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u53d1\u5c55\u8d8b\u52bf\u3002"}}
{"id": "2510.16022", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16022", "abs": "https://arxiv.org/abs/2510.16022", "authors": ["Changsheng Wang", "Xin Chen", "Sijia Liu", "Ke Ding"], "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization", "comment": null, "summary": "Adapting pretrained large language models (LLMs) to code domains via\nsupervised fine-tuning (FT) has been commonly used for code generation.\nHowever, we identify a previously underappreciated failure mode, the\nmemorization barrier, where strong memorization of downstream code data in the\nbase model could trap optimization and prevent the standard FT from effectively\nacquiring new, generalizable code knowledge. To overcome this barrier, we\npropose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which\napplies an IB penalty on hidden representations of the code data to compress\nspurious, memorized features while preserving task-relevant information.\nExtensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)\nshow that IB-FT substantially alleviates the memorization barrier, improves\ntop-1 performance (Pass@$1$), and yields far more stable gains under the\nstricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if\nat least $m$ of $k$ samples pass unit tests) compared with conventional FT.", "AI": {"tldr": "\u63d0\u51faIB-FT\u65b9\u6cd5\u89e3\u51b3LLMs\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u538b\u7f29\u8bb0\u5fc6\u7279\u5f81\uff0c\u63d0\u5347\u6cdb\u5316\u6027\u80fd", "motivation": "\u53d1\u73b0\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u9886\u57df\u5fae\u8c03\u65f6\u5b58\u5728\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5bf9\u4e0b\u6e38\u4ee3\u7801\u6570\u636e\u7684\u5f3a\u8bb0\u5fc6\u4f1a\u963b\u788d\u5176\u5b66\u4e60\u65b0\u7684\u53ef\u6cdb\u5316\u77e5\u8bc6", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u5fae\u8c03\u65b9\u6cd5(IB-FT)\uff0c\u5728\u4ee3\u7801\u6570\u636e\u7684\u9690\u85cf\u8868\u793a\u4e0a\u5e94\u7528IB\u60e9\u7f5a\uff0c\u538b\u7f29\u865a\u5047\u8bb0\u5fc6\u7279\u5f81\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f", "result": "\u5728\u4e24\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5(OriGen\u548cEvol-CodeAlpaca-V1)\u4e0a\uff0cIB-FT\u663e\u8457\u7f13\u89e3\u8bb0\u5fc6\u969c\u788d\uff0c\u63d0\u5347Top-1\u6027\u80fd\uff0c\u5e76\u5728\u66f4\u4e25\u683c\u7684\u591a\u6837\u672c\u6307\u6807Pass@k^(m)\u4e0b\u83b7\u5f97\u66f4\u7a33\u5b9a\u7684\u589e\u76ca", "conclusion": "IB-FT\u65b9\u6cd5\u80fd\u6709\u6548\u514b\u670dLLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8bb0\u5fc6\u969c\u788d\uff0c\u76f8\u6bd4\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u548c\u7a33\u5b9a\u6027"}}
{"id": "2510.16438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16438", "abs": "https://arxiv.org/abs/2510.16438", "authors": ["Aidyn Ubingazhibov", "R\u00e9mi Pautrat", "Iago Su\u00e1rez", "Shaohui Liu", "Marc Pollefeys", "Viktor Larsson"], "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching", "comment": "Accepted at ICCVW 2025", "summary": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick.", "AI": {"tldr": "LightGlueStick\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u70b9\u548c\u7ebf\u6bb5\u5339\u914d\u5668\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u7ebf\u6d88\u606f\u4f20\u9012(ALMP)\u7ec4\u4ef6\u663e\u5f0f\u66b4\u9732\u7ebf\u7684\u8fde\u901a\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u70b9\u548c\u7ebf\u5339\u914d\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0cGlueStick\u867d\u7136\u5b9e\u73b0\u4e86\u8054\u5408\u5339\u914d\u4f46\u67b6\u6784\u8f83\u91cd\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u6216\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5339\u914d\u5668LightGlueStick\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u6ce8\u610f\u529b\u7ebf\u6d88\u606f\u4f20\u9012(ALMP)\u7ec4\u4ef6\uff0c\u663e\u5f0f\u66b4\u9732\u7ebf\u7684\u8fde\u901a\u6027\uff0c\u5b9e\u73b0\u8282\u70b9\u95f4\u9ad8\u6548\u901a\u4fe1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "LightGlueStick\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u548cALMP\u7ec4\u4ef6\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u70b9\u7ebf\u8054\u5408\u5339\u914d\u3002"}}
{"id": "2510.16053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16053", "abs": "https://arxiv.org/abs/2510.16053", "authors": ["Chenyang Yu", "Xinpeng Xie", "Yan Huang", "Chenxi Qiu"], "title": "FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting", "comment": null, "summary": "Accurate traffic forecasting is a core technology for building Intelligent\nTransportation Systems (ITS), enabling better urban resource allocation and\nimproved travel experiences. With growing urbanization, traffic congestion has\nintensified, highlighting the need for reliable and responsive forecasting\nmodels. In recent years, deep learning, particularly Graph Neural Networks\n(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can\neffectively capture complex spatial dependencies in road network topology and\ndynamic temporal evolution patterns in traffic flow data. Foundational models\nsuch as STGCN and GraphWaveNet, along with more recent developments including\nSTWave and D2STGNN, have achieved impressive performance on standard traffic\ndatasets. These approaches incorporate sophisticated graph convolutional\nstructures and temporal modeling mechanisms, demonstrating particular\neffectiveness in capturing and forecasting traffic patterns characterized by\nperiodic regularities. To address this challenge, researchers have explored\nvarious ways to incorporate event information. Early attempts primarily relied\non manually engineered event features. For instance, some approaches introduced\nmanually defined incident effect scores or constructed specific subgraphs for\ndifferent event-induced traffic conditions. While these methods somewhat\nenhance responsiveness to specific events, their core drawback lies in a heavy\nreliance on domain experts' prior knowledge, making generalization to diverse\nand complex unknown events difficult, and low-dimensional manual features often\nlead to the loss of rich semantic details.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4ea4\u901a\u9884\u6d4b\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u6349\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u65f6\u95f4\u6f14\u5316\u6a21\u5f0f\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e8b\u4ef6\u4fe1\u606f\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\uff0c\u4ea4\u901a\u62e5\u5835\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u53ef\u9760\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u4ea4\u901a\u9884\u6d4b\u6a21\u578b\u6765\u652f\u6301\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5efa\u8bbe\uff0c\u6539\u5584\u57ce\u5e02\u8d44\u6e90\u5206\u914d\u548c\u51fa\u884c\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4f5c\u4e3a\u4e3b\u6d41\u65b9\u6cd5\uff0c\u901a\u8fc7\u590d\u6742\u7684\u56fe\u5377\u79ef\u7ed3\u6784\u548c\u65f6\u95f4\u5efa\u6a21\u673a\u5236\u6765\u6355\u6349\u9053\u8def\u7f51\u7edc\u62d3\u6251\u4e2d\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u4ea4\u901a\u6d41\u6570\u636e\u7684\u65f6\u95f4\u6f14\u5316\u6a21\u5f0f\u3002\u65e9\u671f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u4e8b\u4ef6\u7279\u5f81\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5982STGCN\u3001GraphWaveNet\u4ee5\u53ca\u66f4\u65b0\u7684STWave\u548cD2STGNN\u5728\u6807\u51c6\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u64c5\u957f\u6355\u6349\u5177\u6709\u5468\u671f\u6027\u89c4\u5f8b\u7684\u4ea4\u901a\u6a21\u5f0f\u3002", "conclusion": "\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u5728\u5468\u671f\u6027\u4ea4\u901a\u6a21\u5f0f\u9884\u6d4b\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5728\u5904\u7406\u4e8b\u4ef6\u4fe1\u606f\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u590d\u6742\u7684\u672a\u77e5\u4e8b\u4ef6\uff0c\u4e14\u624b\u52a8\u7279\u5f81\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u7ec6\u8282\u4e22\u5931\u3002"}}
{"id": "2510.16084", "categories": ["cs.LG", "cond-mat.quant-gas", "math-ph", "math.MP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16084", "abs": "https://arxiv.org/abs/2510.16084", "authors": ["Karol Sajnok", "Micha\u0142 Matuszewski"], "title": "Near-Equilibrium Propagation training in nonlinear wave systems", "comment": "7 figures", "summary": "Backpropagation learning algorithm, the workhorse of modern artificial\nintelligence, is notoriously difficult to implement in physical neural\nnetworks. Equilibrium Propagation (EP) is an alternative with comparable\nefficiency and strong potential for in-situ training. We extend EP learning to\nboth discrete and continuous complex-valued wave systems. In contrast to\nprevious EP implementations, our scheme is valid in the weakly dissipative\nregime, and readily applicable to a wide range of physical settings, even\nwithout well defined nodes, where trainable inter-node connections can be\nreplaced by trainable local potential. We test the method in driven-dissipative\nexciton-polariton condensates governed by generalized Gross-Pitaevskii\ndynamics. Numerical studies on standard benchmarks, including a simple logical\ntask and handwritten-digit recognition, demonstrate stable convergence,\nestablishing a practical route to in-situ learning in physical systems in which\nsystem control is restricted to local parameters.", "AI": {"tldr": "\u5c06\u5e73\u8861\u4f20\u64ad\u5b66\u4e60\u7b97\u6cd5\u6269\u5c55\u5230\u79bb\u6563\u548c\u8fde\u7eed\u590d\u503c\u6ce2\u7cfb\u7edf\uff0c\u5728\u5f31\u8017\u6563\u72b6\u6001\u4e0b\u6709\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7269\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u5c40\u90e8\u52bf\u80fd\u66ff\u4ee3\u8282\u70b9\u95f4\u8fde\u63a5\uff0c\u5728\u6fc0\u5b50-\u6781\u5316\u6fc0\u5b50\u51dd\u805a\u4f53\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u96be\u4ee5\u5728\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b9e\u73b0\uff0c\u5e73\u8861\u4f20\u64ad(EP)\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u76f8\u4f3c\u6548\u7387\u548c\u539f\u4f4d\u8bad\u7ec3\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u7269\u7406\u7cfb\u7edf\u3002", "method": "\u6269\u5c55EP\u5b66\u4e60\u5230\u79bb\u6563\u548c\u8fde\u7eed\u590d\u503c\u6ce2\u7cfb\u7edf\uff0c\u5728\u5f31\u8017\u6563\u72b6\u6001\u4e0b\u6709\u6548\uff0c\u7528\u53ef\u8bad\u7ec3\u5c40\u90e8\u52bf\u80fd\u66ff\u4ee3\u8282\u70b9\u95f4\u8fde\u63a5\uff0c\u5728\u6fc0\u5b50-\u6781\u5316\u6fc0\u5b50\u51dd\u805a\u4f53\u4e2d\u6d4b\u8bd5\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u903b\u8f91\u4efb\u52a1\u548c\u624b\u5199\u6570\u5b57\u8bc6\u522b\uff09\u5c55\u793a\u4e86\u7a33\u5b9a\u6536\u655b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e3a\u7cfb\u7edf\u63a7\u5236\u4ec5\u9650\u4e8e\u5c40\u90e8\u53c2\u6570\u7684\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u539f\u4f4d\u5b66\u4e60\u5efa\u7acb\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2510.17211", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17211", "abs": "https://arxiv.org/abs/2510.17211", "authors": ["Tingsong Xiao", "Yao An Lee", "Zelin Xu", "Yupu Zhang", "Zibo Liu", "Yu Huang", "Jiang Bian", "Serena Jingchuan Guo", "Zhe Jiang"], "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling", "comment": null, "summary": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases.", "AI": {"tldr": "\u63d0\u51fa\u4e86TD-HNODE\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u548c\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u57282\u578b\u7cd6\u5c3f\u75c5\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u57fa\u4e8e\u4e0d\u89c4\u5219\u65f6\u95f4\u4e8b\u4ef6\u6837\u672c\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u8fdb\u5c55\u52a8\u6001\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u60a3\u8005\u5f02\u8d28\u6027\uff08\u4e0d\u540c\u8fdb\u5c55\u901f\u7387\u548c\u8def\u5f84\uff09\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u8868\u793a\u4e34\u5e8a\u8ba4\u53ef\u7684\u8fdb\u5c55\u8f68\u8ff9\uff0c\u901a\u8fc7\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u8fdb\u5c55\u52a8\u6001\uff0c\u5305\u542b\u53ef\u5b66\u4e60\u7684TD-\u8d85\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u6355\u6349\u5e76\u53d1\u75c7\u6807\u8bb0\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTD-HNODE\u57282\u578b\u7cd6\u5c3f\u75c5\u548c\u76f8\u5173\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TD-HNODE\u80fd\u591f\u6709\u6548\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u6570\u636e\u548c\u60a3\u8005\u5f02\u8d28\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16161", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16161", "abs": "https://arxiv.org/abs/2510.16161", "authors": ["Ankitkumar Joshi", "Milos Hauskrecht"], "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction", "comment": null, "summary": "Modeling irregularly sampled multivariate time series is a persistent\nchallenge in domains like healthcare and sensor networks. While recent works\nhave explored a variety of complex learning architectures to solve the\nprediction problems for irregularly sampled time series, it remains unclear\nwhat are the true benefits of some of these architectures, and whether clever\nmodifications of simpler and more efficient RNN-based algorithms are still\ncompetitive, i.e. they are on par with or even superior to these methods. In\nthis work, we propose and study GRUwE: Gated Recurrent Unit with Exponential\nbasis functions, that builds upon RNN-based architectures for observations made\nat irregular times. GRUwE supports both regression-based and event-based\npredictions in continuous time. GRUwE works by maintaining a Markov state\nrepresentation of the time series that updates with the arrival of irregular\nobservations. The Markov state update relies on two reset mechanisms: (i)\nobservation-triggered reset, and (ii) time-triggered reset of the GRU state\nusing learnable exponential decays, to support the predictions in continuous\ntime. Our empirical evaluations across several real-world benchmarks on\nnext-observation and next-event prediction tasks demonstrate that GRUwE can\nindeed achieve competitive to superior performance compared to the recent\nstate-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers\ncompelling advantages: it is easy to implement, requires minimal\nhyper-parameter tuning efforts, and significantly reduces the computational\noverhead in the online deployment.", "AI": {"tldr": "GRUwE\uff1a\u57fa\u4e8eGRU\u7684\u7b80\u5355\u9ad8\u6548\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u5b9e\u73b0\u7b80\u5355\u3001\u8c03\u53c2\u5c11\u3001\u8ba1\u7b97\u5f00\u9500\u4f4e\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u590d\u6742\u67b6\u6784\u5728\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u65f6\u7684\u771f\u6b63\u4f18\u52bf\u4e0d\u660e\u786e\uff0c\u9700\u8981\u9a8c\u8bc1\u7b80\u5355\u9ad8\u6548\u7684RNN\u67b6\u6784\u662f\u5426\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002", "method": "\u57fa\u4e8eGRU\u67b6\u6784\uff0c\u5f15\u5165\u4e24\u79cd\u91cd\u7f6e\u673a\u5236\uff1a\u89c2\u6d4b\u89e6\u53d1\u91cd\u7f6e\u548c\u65f6\u95f4\u89e6\u53d1\u91cd\u7f6e\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6307\u6570\u8870\u51cf\u6765\u652f\u6301\u8fde\u7eed\u65f6\u95f4\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRUwE\u5728\u4e0b\u4e00\u89c2\u6d4b\u548c\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u751a\u81f3\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "GRUwE\u8bc1\u660e\u4e86\u7b80\u5355RNN\u67b6\u6784\u7684\u6301\u7eed\u7ade\u4e89\u529b\uff0c\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7b80\u5355\u3001\u8c03\u53c2\u5c11\u3001\u8ba1\u7b97\u5f00\u9500\u4f4e\u7684\u5b9e\u7528\u4f18\u52bf\u3002"}}
{"id": "2510.17247", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17247", "abs": "https://arxiv.org/abs/2510.17247", "authors": ["Zefan Cai", "Haoyi Qiu", "Haozhe Zhao", "Ke Wan", "Jiachen Li", "Jiuxiang Gu", "Wen Xiao", "Nanyun Peng", "Junjie Hu"], "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models", "comment": null, "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VideoBiasEval\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u5728\u5bf9\u9f50\u8c03\u4f18\u8fc7\u7a0b\u4e2d\u4f1a\u653e\u5927\u5e76\u7a33\u5b9a\u5316\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u901a\u8fc7\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\u8c03\u4f18\uff0c\u867d\u7136\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u4e2d\u7f16\u7801\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\u3002", "method": "\u5f15\u5165VideoBiasEval\u8bca\u65ad\u6846\u67b6\uff0c\u57fa\u4e8e\u793e\u4f1a\u504f\u89c1\u5206\u7c7b\u5b66\uff0c\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u6f14\u5458\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u591a\u7c92\u5ea6\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5bf9\u9f50\u8c03\u4f18\u4e0d\u4ec5\u5f3a\u5316\u4e86\u8868\u5f81\u504f\u89c1\uff0c\u8fd8\u4f7f\u5176\u5728\u65f6\u95f4\u4e0a\u66f4\u52a0\u7a33\u5b9a\uff0c\u4ea7\u751f\u66f4\u5e73\u6ed1\u4f46\u66f4\u523b\u677f\u7684\u63cf\u7ed8\u3002", "conclusion": "\u9700\u8981\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u504f\u89c1\u611f\u77e5\u7684\u8bc4\u4f30\u548c\u7f13\u89e3\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u548c\u793e\u4f1a\u8d23\u4efb\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.17252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17252", "abs": "https://arxiv.org/abs/2510.17252", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Ayesha Siddiqua", "Jungpil Shin"], "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design", "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)", "summary": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u60c5\u611f\u5206\u6790\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u5b58\u5728\u660e\u663e\u7684\u8d1f\u9762\u60c5\u7eea\u4e3b\u5bfc\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u65b0\u95fb\u5a92\u4f53\u5982\u4f55\u901a\u8fc7\u60c5\u611f\u6846\u67b6\u5f71\u54cd\u516c\u4f17\u60c5\u7eea\uff0c\u4ee5\u53ca\u8d1f\u9762\u6807\u9898\u5982\u4f55\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\u548c\u4f20\u64ad\uff0c\u4ece\u800c\u9f13\u52b1\u5a92\u4f53\u4f7f\u7528\u66f4\u5f3a\u60c5\u611f\u53cd\u5e94\u7684\u62a5\u9053\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Gemma-3 4B\u6a21\u578b\u5bf930\u4e07\u6761\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u548c\u5185\u5bb9\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u8bc6\u522b\u6bcf\u7bc7\u6587\u7ae0\u7684\u4e3b\u8981\u60c5\u7eea\u548c\u6574\u4f53\u57fa\u8c03\u3002", "result": "\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u660e\u663e\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u65b0\u95fb\u7684\u60c5\u611f\u63cf\u8ff0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u7406\u5ff5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u65e5\u5e38\u65b0\u95fb\u4e2d\u7684\u9690\u85cf\u60c5\u611f\u6846\u67b6\u3002"}}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5df2\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u4e8b\u4ef6\u53ec\u56de\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u7b49\u5173\u952e\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u5728\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7814\u7a76\u5229\u7528LLMs\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u672a\u6765\u4e8b\u4ef6\u7684\u6f5c\u529b\uff0c\u8fd9\u4e00\u65b0\u5174\u8303\u5f0f\u88ab\u79f0\u4e3a\"LLM-as-a-Prophet\"\u3002", "method": "\u6784\u5efa\u4e86Prophet Arena\u8bc4\u4f30\u57fa\u51c6\uff0c\u6301\u7eed\u6536\u96c6\u5b9e\u65f6\u9884\u6d4b\u4efb\u52a1\u5e76\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u6d41\u6c34\u7ebf\u9636\u6bb5\uff0c\u4ee5\u652f\u6301\u53d7\u63a7\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u663e\u793a\u8bb8\u591aLLMs\u5df2\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5982\u8f83\u5c0f\u7684\u6821\u51c6\u8bef\u5dee\u3001\u4e00\u81f4\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6709\u524d\u666f\u7684\u5e02\u573a\u56de\u62a5\u3002", "conclusion": "\u867d\u7136LLMs\u5177\u6709\u9884\u6d4b\u6f5c\u529b\uff0c\u4f46\u5728\u5b9e\u73b0\u5353\u8d8a\u9884\u6d4b\u667a\u80fd\u65b9\u9762\u5b58\u5728\u5173\u952e\u74f6\u9888\uff0c\u5305\u62ec\u4e8b\u4ef6\u53ec\u56de\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u4ee5\u53ca\u63a5\u8fd1\u89e3\u51b3\u65f6\u4fe1\u606f\u805a\u5408\u901f\u5ea6\u6162\u4e8e\u5e02\u573a\u7b49\u95ee\u9898\u3002"}}
{"id": "2510.16185", "categories": ["cs.LG", "cs.AI", "cs.FL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16185", "abs": "https://arxiv.org/abs/2510.16185", "authors": ["Daniel Donnelly", "Angelo Ferrando", "Francesco Belardinelli"], "title": "Expressive Reward Synthesis with the Runtime Monitoring Language", "comment": null, "summary": "A key challenge in reinforcement learning (RL) is reward (mis)specification,\nwhereby imprecisely defined reward functions can result in unintended, possibly\nharmful, behaviours. Indeed, reward functions in RL are typically treated as\nblack-box mappings from state-action pairs to scalar values. While effective in\nmany settings, this approach provides no information about why rewards are\ngiven, which can hinder learning and interpretability. Reward Machines address\nthis issue by representing reward functions as finite state automata, enabling\nthe specification of structured, non-Markovian reward functions. However, their\nexpressivity is typically bounded by regular languages, leaving them unable to\ncapture more complex behaviours such as counting or parametrised conditions. In\nthis work, we build on the Runtime Monitoring Language (RML) to develop a novel\nclass of language-based Reward Machines. By leveraging the built-in memory of\nRML, our approach can specify reward functions for non-regular, non-Markovian\ntasks. We demonstrate the expressiveness of our approach through experiments,\nhighlighting additional advantages in flexible event-handling and task\nspecification over existing Reward Machine-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00(RML)\u7684\u65b0\u578b\u8bed\u8a00\u5956\u52b1\u673a\uff0c\u80fd\u591f\u8868\u793a\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u673a\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u51fd\u6570\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\u6620\u5c04\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u4e14\u4f20\u7edf\u5956\u52b1\u673a\u53ea\u80fd\u8868\u793a\u6b63\u5219\u8bed\u8a00\uff0c\u65e0\u6cd5\u5904\u7406\u8ba1\u6570\u6216\u53c2\u6570\u5316\u6761\u4ef6\u7b49\u590d\u6742\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00(RML)\u6784\u5efa\u8bed\u8a00\u5956\u52b1\u673a\uff0c\u5229\u7528RML\u7684\u5185\u7f6e\u5185\u5b58\u673a\u5236\u6765\u6307\u5b9a\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u5728\u7075\u6d3b\u4e8b\u4ef6\u5904\u7406\u548c\u4efb\u52a1\u89c4\u8303\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5956\u52b1\u673a\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u5956\u52b1\u673a\u6269\u5c55\u4e86\u5956\u52b1\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u975e\u6b63\u5219\u884c\u4e3a\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u4efb\u52a1\u89c4\u8303\u5de5\u5177\u3002"}}
{"id": "2510.16776", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16776", "abs": "https://arxiv.org/abs/2510.16776", "authors": ["Mingzheng Zhang", "Jinfeng Gao", "Dan Xu", "Jiangrui Yu", "Yuhan Qiao", "Lan Chen", "Jin Tang", "Xiao Wang"], "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation", "comment": null, "summary": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence that can significantly reduce diagnostic burdens for\nclinicians and patient wait times. Existing MRG models predominantly rely on\nLarge Language Models (LLMs) to improve report generation, with limited\nexploration of pre-trained vision foundation models or advanced fine-tuning\ntechniques. Mainstream frameworks either avoid fine-tuning or utilize\nsimplistic methods like LoRA, often neglecting the potential of enhancing\ncross-attention mechanisms. Additionally, while Transformer-based models\ndominate vision-language tasks, non-Transformer architectures, such as the\nMamba network, remain underexplored for medical report generation, presenting a\npromising avenue for future research. In this paper, we propose EMRRG, a novel\nX-ray report generation framework that fine-tunes pre-trained Mamba networks\nusing parameter-efficient methods. Specifically, X-ray images are divided into\npatches, tokenized, and processed by an SSM-based vision backbone for feature\nextraction, with Partial LoRA yielding optimal performance. An LLM with a\nhybrid decoder generates the medical report, enabling end-to-end training and\nachieving strong results on benchmark datasets. Extensive experiments on three\nwidely used benchmark datasets fully validated the effectiveness of our\nproposed strategies for the X-ray MRG. The source code of this paper will be\nreleased on https://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86EMRRG\u6846\u67b6\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5fae\u8c03\u9884\u8bad\u7ec3\u7684Mamba\u7f51\u7edc\u8fdb\u884cX\u5c04\u7ebf\u62a5\u544a\u751f\u6210\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u62a5\u544a\u751f\u6210\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56LLM\uff0c\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u5148\u8fdb\u5fae\u8c03\u6280\u672f\u63a2\u7d22\u6709\u9650\uff0c\u4e3b\u6d41\u6846\u67b6\u8981\u4e48\u907f\u514d\u5fae\u8c03\u8981\u4e48\u4f7f\u7528\u7b80\u5355\u65b9\u6cd5\u5982LoRA\uff0c\u5ffd\u7565\u4e86\u589e\u5f3a\u8de8\u6ce8\u610f\u529b\u673a\u5236\u7684\u6f5c\u529b\u3002", "method": "\u5c06X\u5c04\u7ebf\u56fe\u50cf\u5206\u5272\u4e3a\u8865\u4e01\u5e76\u6807\u8bb0\u5316\uff0c\u901a\u8fc7SSM\u57fa\u7840\u7684\u89c6\u89c9\u9aa8\u5e72\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u4f7f\u7528Partial LoRA\u83b7\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u7ed3\u5408\u6df7\u5408\u89e3\u7801\u5668\u7684LLM\u751f\u6210\u533b\u7597\u62a5\u544a\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u5bf9X\u5c04\u7ebf\u533b\u7597\u62a5\u544a\u751f\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "EMRRG\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5fae\u8c03\u9884\u8bad\u7ec3Mamba\u7f51\u7edc\uff0c\u5728X\u5c04\u7ebf\u533b\u7597\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u533b\u7597AI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.16781", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16781", "abs": "https://arxiv.org/abs/2510.16781", "authors": ["Shihao Ji", "Zihui Song"], "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features", "comment": null, "summary": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u548c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u89c6\u9891\u7406\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9ad8\u7ef4\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u81ea\u76d1\u7763\u65f6\u7a7a\u805a\u7c7b\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u56fe\u50cf\u4e0a\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5b8c\u5168\u8f6c\u5316\u5230\u89c6\u9891\u9886\u57df\uff0c\u4f20\u7edf\u89c6\u9891\u7406\u89e3\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3VLM\u7684\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u5c06\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u8bed\u4e49\u7279\u5f81\u8f68\u8ff9\uff0c\u91c7\u7528Kernel Temporal Segmentation\u8fdb\u884c\u65f6\u5e8f\u5206\u5272\uff0c\u7136\u540e\u901a\u8fc7\u65e0\u76d1\u7763\u5bc6\u5ea6\u805a\u7c7b\u8bc6\u522b\u91cd\u590d\u573a\u666f\u548c\u4e3b\u9898\uff0c\u6700\u540e\u9009\u62e9\u4ee3\u8868\u6027\u5173\u952e\u5e27\u5e76\u5229\u7528VLM\u751f\u6210\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u89c6\u9891\u5185\u5bb9\u6458\u8981\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u81ea\u52a8\u5316\u89c6\u9891\u7ed3\u6784\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7684\u96f6\u6837\u672c\u81ea\u52a8\u5316\u7ed3\u6784\u5206\u6790\u3002"}}
{"id": "2510.17437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17437", "abs": "https://arxiv.org/abs/2510.17437", "authors": ["Manuela Daniela Danu", "George Marica", "Constantin Suciu", "Lucian Mihai Itu", "Oladimeji Farri"], "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings", "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)", "summary": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u591a\u79cd\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7684\u4e34\u5e8a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u4e2d\u63d0\u53d6\u75be\u75c5\u548c\u836f\u7269\u63d0\u53ca\uff0c\u5728BioASQ MultiCardioNER\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u4e2d\u89e3\u9501\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u4ee5\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u4e34\u5e8a\u7cfb\u7edf\u53d1\u5c55\u3002\u867d\u7136\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u8bed\u6599\u5e93\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u7684\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002", "method": "\u63a2\u7d22\u4e86\u5728\u901a\u7528\u9886\u57df\u6587\u672c\u4e0a\u8bad\u7ec3\u7684\u5355\u8bed\u548c\u591a\u8bedBERT\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u7528\u4e8e\u4ece\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u4e2d\u63d0\u53d6\u75be\u75c5\u548c\u836f\u7269\u63d0\u53ca\u3002", "result": "\u5728\u897f\u73ed\u7259\u8bed\u75be\u75c5\u8bc6\u522b\u4e2d\u83b7\u5f9777.88%\u7684F1\u5206\u6570\uff0c\u897f\u73ed\u7259\u8bed\u836f\u7269\u8bc6\u522b92.09%\uff0c\u82f1\u8bed\u836f\u7269\u8bc6\u522b91.74%\uff0c\u610f\u5927\u5229\u8bed\u836f\u7269\u8bc6\u522b88.9%\uff0c\u5728\u6240\u6709\u5b50\u4efb\u52a1\u7684\u6d4b\u8bd5\u6392\u884c\u699c\u4e2d\u5747\u4f18\u4e8e\u5e73\u5747\u548c\u4e2d\u4f4d\u6570F1\u5206\u6570\u3002", "conclusion": "\u5f00\u53d1\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\u6210\u529f\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u4e34\u5e8a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4e34\u5e8a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2510.16530", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16530", "abs": "https://arxiv.org/abs/2510.16530", "authors": ["Ashutosh Srivastava", "Lokesh Nagalapatti", "Gautam Jajoo", "Aniket Vashishtha", "Parameswari Krishnamurthy", "Amit Sharma"], "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks", "comment": null, "summary": "Recent claims of strong performance by Large Language Models (LLMs) on causal\ndiscovery are undermined by a key flaw: many evaluations rely on benchmarks\nlikely included in pretraining corpora. Thus, apparent success suggests that\nLLM-only methods, which ignore observational data, outperform classical\nstatistical approaches. We challenge this narrative by asking: Do LLMs truly\nreason about causal structure, and how can we measure it without memorization\nconcerns? Can they be trusted for real-world scientific discovery? We argue\nthat realizing LLMs' potential for causal analysis requires two shifts: (P.1)\ndeveloping robust evaluation protocols based on recent scientific studies to\nguard against dataset leakage, and (P.2) designing hybrid methods that combine\nLLM-derived knowledge with data-driven statistics. To address P.1, we encourage\nevaluating discovery methods on novel, real-world scientific studies. We\noutline a practical recipe for extracting causal graphs from recent\npublications released after an LLM's training cutoff, ensuring relevance and\npreventing memorization while capturing both established and novel relations.\nCompared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,\nthey perform far worse on our curated graphs, underscoring the need for\nstatistical grounding. Supporting P.2, we show that using LLM predictions as\npriors for the classical PC algorithm significantly improves accuracy over both\nLLM-only and purely statistical methods. We call on the community to adopt\nscience-grounded, leakage-resistant benchmarks and invest in hybrid causal\ndiscovery methods suited to real-world inquiry.", "AI": {"tldr": "\u672c\u6587\u8d28\u7591LLM\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u6307\u51fa\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u65b0\u79d1\u5b66\u7814\u7a76\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u7ed3\u5408LLM\u4e0e\u7edf\u8ba1\u65b9\u6cd5\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u6311\u6218LLM\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u865a\u5047\u8868\u73b0\uff0c\u63ed\u793a\u73b0\u6709\u8bc4\u4f30\u56e0\u9884\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u800c\u4e0d\u53ef\u9760\uff0c\u63a2\u7d22LLM\u5728\u771f\u5b9e\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u65b0\u79d1\u5b66\u7814\u7a76\u7684\u8bc4\u4f30\u534f\u8bae\u9632\u6b62\u6570\u636e\u6cc4\u9732\uff1b\u8bbe\u8ba1\u6df7\u5408\u65b9\u6cd5\u5c06LLM\u9884\u6d4b\u4f5c\u4e3a\u7ecf\u5178PC\u7b97\u6cd5\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728BNLearn\u57fa\u51c6\u4e0aLLM\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f46\u5728\u65b0\u6784\u5efa\u7684\u79d1\u5b66\u56fe\u8c31\u4e0a\u8868\u73b0\u8f83\u5dee\uff1b\u5c06LLM\u9884\u6d4b\u4f5c\u4e3aPC\u7b97\u6cd5\u5148\u9a8c\u53ef\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u547c\u5401\u91c7\u7528\u57fa\u4e8e\u79d1\u5b66\u3001\u6297\u6cc4\u9732\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6295\u8d44\u5f00\u53d1\u9002\u5408\u771f\u5b9e\u4e16\u754c\u7814\u7a76\u7684\u6df7\u5408\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2510.16988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16988", "abs": "https://arxiv.org/abs/2510.16988", "authors": ["Junhao Zhao", "Zishuai Liu", "Ruili Fang", "Jin Lu", "Linghan Zhang", "Fei Dou"], "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams", "comment": null, "summary": "The recognition of Activities of Daily Living (ADLs) from event-triggered\nambient sensors is an essential task in Ambient Assisted Living, yet existing\nmethods remain constrained by representation-level limitations. Sequence-based\napproaches preserve temporal order of sensor activations but are sensitive to\nnoise and lack spatial awareness, while image-based approaches capture global\npatterns and implicit spatial correlations but compress fine-grained temporal\ndynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)\nfail to enforce alignment between sequence- and image-based representation\nviews, underutilizing their complementary strengths. We propose Contrastive\nAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an\nend-to-end framework that jointly optimizes representation learning via\nSequence-Image Contrastive Alignment (SICA) and classification via\ncross-entropy, ensuring both cross-representation alignment and task-specific\ndiscriminability. CARE integrates (i) time-aware, noise-resilient sequence\nencoding with (ii) spatially-informed and frequency-sensitive image\nrepresentations, and employs (iii) a joint contrastive-classification objective\nfor end-to-end learning of aligned and discriminative embeddings. Evaluated on\nthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% on\nMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to\nsensor malfunctions and layout variability, highlighting its potential for\nreliable ADL recognition in smart homes.", "AI": {"tldr": "\u63d0\u51fa\u4e86CARE\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217-\u56fe\u50cf\u5bf9\u6bd4\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3ADL\u8bc6\u522b\u4e2d\u5e8f\u5217\u548c\u56fe\u50cf\u8868\u793a\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u4e09\u4e2aCASAS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ADL\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u7ea7\u9650\u5236\uff1a\u5e8f\u5217\u65b9\u6cd5\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u4f46\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\uff0c\u56fe\u50cf\u65b9\u6cd5\u6355\u83b7\u5168\u5c40\u6a21\u5f0f\u4f46\u538b\u7f29\u65f6\u95f4\u52a8\u6001\u548c\u626d\u66f2\u4f20\u611f\u5668\u5e03\u5c40\uff0c\u7b80\u5355\u878d\u5408\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u8868\u793a\u5bf9\u9f50\u3002", "method": "CARE\u6846\u67b6\u5305\u542b\uff1a(1)\u65f6\u95f4\u611f\u77e5\u3001\u566a\u58f0\u5f39\u6027\u7684\u5e8f\u5217\u7f16\u7801\uff1b(2)\u7a7a\u95f4\u611f\u77e5\u548c\u9891\u7387\u654f\u611f\u7684\u56fe\u50cf\u8868\u793a\uff1b(3)\u8054\u5408\u5bf9\u6bd4-\u5206\u7c7b\u76ee\u6807\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u901a\u8fc7\u5e8f\u5217-\u56fe\u50cf\u5bf9\u6bd4\u5bf9\u9f50\u786e\u4fdd\u8de8\u8868\u793a\u5bf9\u9f50\u548c\u4efb\u52a1\u7279\u5b9a\u53ef\u533a\u5206\u6027\u3002", "result": "\u5728\u4e09\u4e2aCASAS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aMilan 89.8%\u3001Cairo 88.9%\u3001Kyoto7 73.3%\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u4f20\u611f\u5668\u6545\u969c\u548c\u5e03\u5c40\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CARE\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u6709\u6548\u6574\u5408\u5e8f\u5217\u548c\u56fe\u50cf\u8868\u793a\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4e3a\u667a\u80fd\u5bb6\u5c45\u4e2d\u53ef\u9760\u7684ADL\u8bc6\u522b\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2510.17199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17199", "abs": "https://arxiv.org/abs/2510.17199", "authors": ["Nirai Hayakawa", "Kazumasa Shimari", "Kazuma Yamasaki", "Hirotatsu Hoshikawa", "Rikuto Tsuchida", "Kenichi Matsumoto"], "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis", "comment": "Accepted to IEEE 2025 Conference on Games", "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.", "AI": {"tldr": "\u57fa\u4e8eTimeSformer\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790VALORANT\u6e38\u620f\u5c0f\u5730\u56fe\u4e2d\u7684\u6218\u672f\u7279\u5f81\uff08\u89d2\u8272\u4f4d\u7f6e\u548c\u6e38\u620f\u4e8b\u4ef6\uff09\u6765\u9884\u6d4b\u56de\u5408\u7ed3\u679c\uff0c\u5728\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ea681%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7535\u7ade\u6bd4\u8d5b\u7ed3\u679c\u9884\u6d4b\u7814\u7a76\u591a\u57fa\u4e8e\u6bd4\u8d5b\u65e5\u5fd7\u548c\u7edf\u8ba1\u6570\u636e\uff0c\u800cVALORANT\u4f5c\u4e3a\u9700\u8981\u590d\u6742\u7b56\u7565\u7684FPS\u6e38\u620f\uff0c\u5176\u5c0f\u5730\u56fe\u4fe1\u606f\u5305\u542b\u4e30\u5bcc\u7684\u6218\u672f\u7279\u5f81\uff0c\u53ef\u7528\u4e8e\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528TimeSformer\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u4ece\u5c0f\u5730\u56fe\u4fe1\u606f\u4e2d\u63d0\u53d6\u8be6\u7ec6\u6218\u672f\u7279\u5f81\uff08\u89d2\u8272\u4f4d\u7f6e\u548c\u6e38\u620f\u4e8b\u4ef6\uff09\uff0c\u6784\u5efa\u56de\u5408\u7ed3\u679c\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u589e\u5f3a\u6218\u672f\u4e8b\u4ef6\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u8fbe\u5230\u7ea681%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5728\u56de\u5408\u4e2d\u671f\u9636\u6bb5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5c0f\u5730\u56fe\u4fe1\u606f\u7684\u6a21\u578b\u3002", "conclusion": "\u5229\u7528\u6bd4\u8d5b\u89c6\u9891\u4e2d\u7684\u6218\u672f\u7279\u5f81\u5bf9\u4e8e\u9884\u6d4bVALORANT\u56de\u5408\u7ed3\u679c\u975e\u5e38\u6709\u6548\uff0c\u5c0f\u5730\u56fe\u4e2d\u7684\u6218\u672f\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.16857", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16857", "abs": "https://arxiv.org/abs/2510.16857", "authors": ["Jiyan Qiu", "Lyulin Kuang", "Guan Wang", "Yichen Xu", "Leiyao Cui", "Shaotong Fu", "Yixin Zhu", "Ruihua Zhang"], "title": "DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization", "comment": null, "summary": "Vehicle aerodynamics optimization has become critical for automotive\nelectrification, where drag reduction directly determines electric vehicle\nrange and energy efficiency. Traditional approaches face an intractable\ntrade-off: computationally expensive Computational Fluid Dynamics (CFD)\nsimulations requiring weeks per design iteration, or simplified models that\nsacrifice production-grade accuracy. While machine learning offers\ntransformative potential, existing datasets exhibit fundamental limitations --\ninadequate mesh resolution, missing vehicle components, and validation errors\nexceeding 5% -- preventing deployment in industrial workflows. We present\nDrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations\ngenerated using $\\text{STAR-CCM+}^\\unicode{xAE}$ software. The dataset\nsystematically explores three vehicle configurations through 20 Computer Aided\nDesign (CAD) parameters via Free Form Deformation (FFD) algorithms, including\ncomplete engine compartments and cooling systems with realistic internal\nairflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a\nfive-fold improvement over existing datasets -- through refined mesh strategies\nwith strict wall $y^+$ control. Benchmarks demonstrate that models trained on\nthis data achieve production-ready accuracy while reducing computational costs\nfrom weeks to minutes. This represents the first dataset bridging academic\nmachine learning research and industrial CFD practice, establishing a new\nstandard for data-driven aerodynamic optimization in automotive development.\nBeyond automotive applications, DrivAerStar demonstrates a paradigm for\nintegrating high-fidelity physics simulations with Artificial Intelligence (AI)\nacross engineering disciplines where computational constraints currently limit\ninnovation.", "AI": {"tldr": "DrivAerStar\u662f\u4e00\u4e2a\u5305\u542b12,000\u4e2a\u5de5\u4e1a\u7ea7\u6c7d\u8f66CFD\u6a21\u62df\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u7f51\u683c\u7b56\u7565\u5b9e\u73b0\u98ce\u6d1e\u9a8c\u8bc1\u7cbe\u5ea6\u4f4e\u4e8e1.04%\uff0c\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u63d0\u53475\u500d\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u4f18\u5316\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "motivation": "\u4f20\u7edf\u6c7d\u8f66\u7a7a\u6c14\u52a8\u529b\u5b66\u4f18\u5316\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u4e0e\u7cbe\u5ea6\u4e0d\u8db3\u7684\u6743\u8861\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u5b58\u5728\u7f51\u683c\u5206\u8fa8\u7387\u4e0d\u8db3\u3001\u7ec4\u4ef6\u7f3a\u5931\u548c\u9a8c\u8bc1\u8bef\u5dee\u8d85\u8fc75%\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u5728\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u90e8\u7f72\u3002", "method": "\u4f7f\u7528STAR-CCM+\u8f6f\u4ef6\u751f\u621012,000\u4e2a\u5de5\u4e1a\u7ea7CFD\u6a21\u62df\uff0c\u901a\u8fc720\u4e2aCAD\u53c2\u6570\u548c\u81ea\u7531\u53d8\u5f62\u7b97\u6cd5\u7cfb\u7edf\u63a2\u7d22\u4e09\u79cd\u8f66\u8f86\u914d\u7f6e\uff0c\u5305\u62ec\u5b8c\u6574\u7684\u53d1\u52a8\u673a\u8231\u548c\u51b7\u5374\u7cfb\u7edf\uff0c\u91c7\u7528\u7cbe\u70bc\u7f51\u683c\u7b56\u7565\u548c\u4e25\u683c\u7684\u58c1\u9762y+\u63a7\u5236\u3002", "result": "\u6570\u636e\u96c6\u5b9e\u73b0\u98ce\u6d1e\u9a8c\u8bc1\u7cbe\u5ea6\u4f4e\u4e8e1.04%\uff0c\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u63d0\u53475\u500d\uff1b\u57fa\u4e8e\u8be5\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5b9e\u73b0\u751f\u4ea7\u7ea7\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u8ba1\u7b97\u6210\u672c\u4ece\u6570\u5468\u51cf\u5c11\u5230\u51e0\u5206\u949f\u3002", "conclusion": "DrivAerStar\u662f\u9996\u4e2a\u8fde\u63a5\u5b66\u672f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u548c\u5de5\u4e1aCFD\u5b9e\u8df5\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u6c7d\u8f66\u5f00\u53d1\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u7a7a\u6c14\u52a8\u529b\u5b66\u4f18\u5316\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5c55\u793a\u4e86\u5c06\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u4e0eAI\u6574\u5408\u5230\u5de5\u7a0b\u9886\u57df\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.16943", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16943", "abs": "https://arxiv.org/abs/2510.16943", "authors": ["Dania Refai", "Moataz Ahmed"], "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used to convert natural\nlanguage descriptions into mathematical optimization formulations. Current\nevaluations often treat formulations as a whole, relying on coarse metrics like\nsolution accuracy or runtime, which obscure structural or numerical errors. In\nthis study, we present a comprehensive, component-level evaluation framework\nfor LLM-generated formulations. Beyond the conventional optimality gap, our\nframework introduces metrics such as precision and recall for decision\nvariables and constraints, constraint and objective root mean squared error\n(RMSE), and efficiency indicators based on token usage and latency. We evaluate\nGPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of\nvarying complexity under six prompting strategies. Results show that GPT-5\nconsistently outperforms other models, with chain-of-thought, self-consistency,\nand modular prompting proving most effective. Analysis indicates that solver\nperformance depends primarily on high constraint recall and low constraint\nRMSE, which together ensure structural correctness and solution reliability.\nConstraint precision and decision variable metrics play secondary roles, while\nconcise outputs enhance computational efficiency. These findings highlight\nthree principles for NLP-to-optimization modeling: (i) Complete constraint\ncoverage prevents violations, (ii) minimizing constraint RMSE ensures\nsolver-level accuracy, and (iii) concise outputs improve computational\nefficiency. The proposed framework establishes a foundation for fine-grained,\ndiagnostic evaluation of LLMs in optimization modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ec4\u4ef6\u7ea7\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30LLM\u751f\u6210\u7684\u6570\u5b66\u4f18\u5316\u516c\u5f0f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6574\u4f53\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u7cbe\u7ec6\u6307\u6807\u5206\u6790\u7ed3\u6784\u6027\u548c\u6570\u503c\u6027\u9519\u8bef\u3002", "motivation": "\u5f53\u524dLLM\u5728\u81ea\u7136\u8bed\u8a00\u5230\u6570\u5b66\u4f18\u5316\u516c\u5f0f\u8f6c\u6362\u7684\u8bc4\u4f30\u5f80\u5f80\u91c7\u7528\u6574\u4f53\u65b9\u6cd5\uff0c\u4f9d\u8d56\u7c97\u7565\u6307\u6807\u5982\u89e3\u7cbe\u5ea6\u6216\u8fd0\u884c\u65f6\u95f4\uff0c\u65e0\u6cd5\u63ed\u793a\u7ed3\u6784\u6216\u6570\u503c\u9519\u8bef\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u51b3\u7b56\u53d8\u91cf\u548c\u7ea6\u675f\u7684\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u3001\u7ea6\u675f\u548c\u76ee\u6807\u51fd\u6570\u5747\u65b9\u6839\u8bef\u5dee\u3001\u57fa\u4e8e\u4ee4\u724c\u4f7f\u7528\u548c\u5ef6\u8fdf\u7684\u6548\u7387\u6307\u6807\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86GPT-5\u3001LLaMA 3.1 Instruct\u548cDeepSeek Math\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u4f18\u5316\u95ee\u9898\u4e0b\u7684\u516d\u79cd\u63d0\u793a\u7b56\u7565\u3002", "result": "GPT-5\u8868\u73b0\u6700\u4f18\uff0c\u601d\u7ef4\u94fe\u3001\u81ea\u4e00\u81f4\u6027\u548c\u6a21\u5757\u5316\u63d0\u793a\u6700\u6709\u6548\u3002\u6c42\u89e3\u5668\u6027\u80fd\u4e3b\u8981\u53d6\u51b3\u4e8e\u9ad8\u7ea6\u675f\u53ec\u56de\u7387\u548c\u4f4e\u7ea6\u675fRMSE\uff0c\u7ea6\u675f\u7cbe\u5ea6\u548c\u51b3\u7b56\u53d8\u91cf\u6307\u6807\u6b21\u4e4b\uff0c\u7b80\u6d01\u8f93\u51fa\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u4e86NLP\u5230\u4f18\u5316\u5efa\u6a21\u7684\u4e09\u4e2a\u539f\u5219\uff1a\u5b8c\u6574\u7ea6\u675f\u8986\u76d6\u9632\u6b62\u8fdd\u89c4\u3001\u6700\u5c0f\u5316\u7ea6\u675fRMSE\u786e\u4fdd\u6c42\u89e3\u5668\u7ea7\u7cbe\u5ea6\u3001\u7b80\u6d01\u8f93\u51fa\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aLLM\u5728\u4f18\u5316\u5efa\u6a21\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bca\u65ad\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17305", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17305", "abs": "https://arxiv.org/abs/2510.17305", "authors": ["ZhaoYang Han", "Qihan Lin", "Hao Liang", "Bowen Chen", "Zhou Liu", "Wentao Zhang"], "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding", "comment": "Submitted to ARR Rolling Review", "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.", "AI": {"tldr": "LongInsightBench\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u957f\u89c6\u9891\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4eba\u7c7b\u8bed\u8a00\u3001\u89c2\u70b9\u3001\u52a8\u4f5c\u7b49\u4e0a\u4e0b\u6587\u5143\u7d20\uff0c\u5e76\u6574\u5408\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u591a\u6a21\u6001\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u6a21\u578b\u5bf9\u957f\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4fe1\u606f\u5bc6\u96c6\u7684\u957f\u89c6\u9891\u548c\u591a\u6a21\u6001\u878d\u5408\u65b9\u9762\u3002", "method": "\u4eceFineVideo\u6570\u636e\u96c6\u4e2d\u7cbe\u9009\u7ea61000\u4e2a\u957f\u89c6\u9891\uff0c\u8bbe\u8ba16\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u573a\u666f\uff0c\u5f00\u53d1\u4e09\u6b65\u534a\u81ea\u52a8\u5316\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5168\u6a21\u6001\u6a21\u578b\u5728\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u8ddd\u79bb\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u591a\u6a21\u6001\u878d\u5408\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u5904\u7406\u504f\u5dee\u3002", "conclusion": "LongInsightBench\u4e3a\u8bc4\u4f30\u6a21\u578b\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5168\u6a21\u6001\u6a21\u578b\u5728\u591a\u6a21\u6001\u878d\u5408\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17347", "abs": "https://arxiv.org/abs/2510.17347", "authors": ["Jingqian Wu", "Shengpeng Xu", "Yunbo Jia", "Edmund Y. Lam"], "title": "Exploring The Missing Semantics In Event Modality", "comment": null, "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.", "AI": {"tldr": "\u63d0\u51fa\u4e86Semantic-E2VID\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\uff0c\u5c06\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u4ece\u5e27\u6a21\u6001\u8f6c\u79fb\u5230\u4e8b\u4ef6\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4fe1\u606f\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u867d\u7136\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u53ea\u6355\u6349\u5f3a\u5ea6\u53d8\u5316\u800c\u5ffd\u7565\u9759\u6001\u7269\u4f53\u548c\u80cc\u666f\uff0c\u5bfc\u81f4\u4e8b\u4ef6\u6a21\u6001\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u3002\u73b0\u6709\u7684\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u8bed\u4e49\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "method": "1. \u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff1a\u5c06Segment Anything Model\u7684\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e8b\u4ef6\u7f16\u7801\u5668\uff1b2. \u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u5757\uff1a\u6574\u5408\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u7279\u5f81\u5f62\u6210\u5bcc\u542b\u8bed\u4e49\u7684\u4e8b\u4ef6\u8868\u793a\uff1b3. \u8bed\u4e49\u611f\u77e5E2V\u76d1\u7763\uff1a\u5229\u7528SAM\u751f\u6210\u7684\u7c7b\u522b\u6807\u7b7e\u5e2e\u52a9\u6a21\u578b\u91cd\u5efa\u8bed\u4e49\u7ec6\u8282\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemantic-E2VID\u663e\u8457\u63d0\u5347\u4e86\u5e27\u8d28\u91cf\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\uff0cSemantic-E2VID\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6a21\u6001\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4e3a\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17372", "abs": "https://arxiv.org/abs/2510.17372", "authors": ["Pawe\u0142 Borsukiewicz", "Fadi Boutros", "Iyiola E. Olatunji", "Charles Beumier", "Wendk\u00fbuni C. Ouedraogo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise", "comment": null, "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f3025\u4e2a\u5408\u6210\u4eba\u8138\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u5408\u6210\u6570\u636e\u53ef\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u6700\u4f73\u5408\u6210\u6570\u636e\u96c6\u8bc6\u522b\u51c6\u786e\u7387\u8fbe95.67%\uff0c\u8d85\u8d8a\u771f\u5b9e\u6570\u636e\u96c6CASIA-WebFace\uff0c\u540c\u65f6\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u548c\u504f\u89c1\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\u5b58\u5728\u9690\u79c1\u4fb5\u72af\u548c\u6cd5\u5f8b\u8d23\u4efb\u98ce\u9669\uff0c\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\u7f3a\u4e4f\u5b9e\u8bc1\u8bc1\u636e\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u5173\u952e\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u6587\u732e\u56de\u987e\u8bc6\u522b25\u4e2a\u5408\u6210\u4eba\u8138\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e03\u4e2a\u9690\u79c1\u4fdd\u62a4\u5408\u6210\u6570\u636e\u5173\u952e\u8981\u6c42\uff0c\u6d89\u53ca\u8d85\u8fc71000\u4e07\u5408\u6210\u6837\u672c\u548c\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6700\u4f73\u5408\u6210\u6570\u636e\u96c6VariFace\u548cVIGFace\u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u8fbe95.67%\u548c94.91%\uff0c\u8d85\u8d8a\u771f\u5b9e\u6570\u636e\u96c6CASIA-WebFace(94.70%)\uff0c\u540c\u65f6\u786e\u4fdd\u7c7b\u5185\u53d8\u5f02\u6027\u548c\u8eab\u4efd\u53ef\u5206\u6027\uff0c\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u504f\u89c1\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u5408\u6210\u4eba\u8138\u6570\u636e\u662f\u79d1\u5b66\u53ef\u884c\u4e14\u4f26\u7406\u5fc5\u8981\u7684\u9762\u90e8\u8bc6\u522b\u7814\u7a76\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5e73\u8861\u9ad8\u7cbe\u5ea6\u4e0e\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u3002"}}
{"id": "2510.17409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17409", "abs": "https://arxiv.org/abs/2510.17409", "authors": ["Dmitrii Galimzianov", "Viacheslav Vyshegorodtsev", "Ivan Nezhivykh"], "title": "Monitoring Horses in Stalls: From Object to Event Detection", "comment": "12 pages, 4 figures, 4 tables", "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u9a6c\u5339\u884c\u4e3a\u76d1\u63a7\u7cfb\u7edf\u539f\u578b\uff0c\u4f7f\u7528YOLOv11\u548cBoT-SORT\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u9a6c\u53a9\u4e2d\u7684\u9a6c\u5339\u548c\u4eba\u5458\u884c\u4e3a\u4e8b\u4ef6\u3002", "motivation": "\u4f20\u7edf\u9a6c\u5339\u884c\u4e3a\u76d1\u63a7\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u65e9\u671f\u68c0\u6d4b\u5065\u5eb7\u4e0e\u798f\u5229\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u6280\u672f\uff0c\u5229\u7528YOLOv11\u548cBoT-SORT\uff0c\u57fa\u4e8e\u76ee\u6807\u8f68\u8ff9\u548c\u7a7a\u95f4\u5173\u7cfb\u63a8\u65ad\u4e8b\u4ef6\u72b6\u6001\uff0c\u4f7f\u7528CLIP\u548cGroundingDINO\u8f85\u52a9\u6784\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002", "result": "\u7cfb\u7edf\u80fd\u533a\u5206\u4e94\u79cd\u4e8b\u4ef6\u7c7b\u578b\u5e76\u8003\u8651\u6444\u50cf\u5934\u76f2\u533a\uff0c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u5bf9\u9a6c\u76f8\u5173\u4e8b\u4ef6\u68c0\u6d4b\u53ef\u9760\uff0c\u4f46\u4eba\u5458\u68c0\u6d4b\u56e0\u6570\u636e\u7a00\u7f3a\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u4e3a\u9a6c\u573a\u5b9e\u65f6\u884c\u4e3a\u76d1\u63a7\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5bf9\u52a8\u7269\u798f\u5229\u548c\u53a9\u820d\u7ba1\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.17088", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17088", "abs": "https://arxiv.org/abs/2510.17088", "authors": ["Zan Li", "Rui Fan"], "title": "Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing", "comment": null, "summary": "Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity\nfreezes, contagion cascades, regime shifts), but existing detectors treat all\nanomalies uniformly, producing scalar scores without revealing which mechanism\nis failing, where risks concentrate, or how to intervene. This opacity prevents\ntargeted regulatory responses. Three unsolved challenges persist: (1) static\ngraph structures cannot adapt when market correlations shift during regime\nchanges; (2) uniform detection mechanisms miss type-specific signatures across\nmultiple temporal scales while failing to integrate individual behaviors with\nnetwork contagion; (3) black-box outputs provide no actionable guidance on\nanomaly mechanisms or their temporal evolution.\n  We address these via adaptive graph learning with specialized expert networks\nthat provide built-in interpretability. Our framework captures multi-scale\ntemporal dependencies through BiLSTM with self-attention, fuses temporal and\nspatial information via cross-modal attention, learns dynamic graphs through\nneural multi-source interpolation, adaptively balances learned dynamics with\nstructural priors via stress-modulated fusion, routes anomalies to four\nmechanism-specific experts, and produces dual-level interpretable attributions.\nCritically, interpretability is embedded architecturally rather than applied\npost-hoc.\n  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events\nwith 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley\nBank case study demonstrates anomaly evolution tracking: Price-Shock expert\nweight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48\n(66% above baseline) one week later, revealing automatic temporal mechanism\nidentification without labeled supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u4e13\u5bb6\u7f51\u7edc\u5b9e\u73b0\u5185\u7f6e\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u91d1\u878d\u5f02\u5e38\u68c0\u6d4b\u4e2d\u673a\u5236\u4e0d\u900f\u660e\u3001\u9759\u6001\u56fe\u7ed3\u6784\u65e0\u6cd5\u9002\u5e94\u5e02\u573a\u53d8\u5316\u3001\u7edf\u4e00\u68c0\u6d4b\u673a\u5236\u65e0\u6cd5\u8bc6\u522b\u7279\u5b9a\u7c7b\u578b\u5f02\u5e38\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u5f02\u5e38\u68c0\u6d4b\u5668\u5c06\u6240\u6709\u5f02\u5e38\u7edf\u4e00\u5904\u7406\uff0c\u4ea7\u751f\u6807\u91cf\u5206\u6570\u800c\u4e0d\u63ed\u793a\u5177\u4f53\u5931\u6548\u673a\u5236\u3001\u98ce\u9669\u96c6\u4e2d\u70b9\u6216\u5e72\u9884\u65b9\u6cd5\uff0c\u8fd9\u79cd\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7ba1\u54cd\u5e94\u3002", "method": "\u901a\u8fc7BiLSTM\u4e0e\u81ea\u6ce8\u610f\u529b\u6355\u83b7\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\uff0c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u65f6\u7a7a\u4fe1\u606f\uff0c\u795e\u7ecf\u591a\u6e90\u63d2\u503c\u5b66\u4e60\u52a8\u6001\u56fe\uff0c\u5e94\u529b\u8c03\u5236\u878d\u5408\u81ea\u9002\u5e94\u5e73\u8861\u5b66\u4e60\u52a8\u6001\u4e0e\u7ed3\u6784\u5148\u9a8c\uff0c\u5c06\u5f02\u5e38\u8def\u7531\u5230\u56db\u4e2a\u673a\u5236\u7279\u5b9a\u4e13\u5bb6\uff0c\u4ea7\u751f\u53cc\u7ea7\u53ef\u89e3\u91ca\u5f52\u56e0\u3002", "result": "\u5728100\u53ea\u7f8e\u56fd\u80a1\u7968\uff082017-2024\uff09\u4e0a\u5b9e\u73b0\u4e8692.3%\u768413\u4e2a\u4e3b\u8981\u4e8b\u4ef6\u68c0\u6d4b\u7387\uff0c\u63d0\u524d3.8\u5929\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad830.8\u4e2a\u767e\u5206\u70b9\u3002\u7845\u8c37\u94f6\u884c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5f02\u5e38\u6f14\u5316\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u67b6\u6784\u5d4c\u5165\u800c\u975e\u4e8b\u540e\u5e94\u7528\u7684\u65b9\u5f0f\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u65f6\u95f4\u673a\u5236\u800c\u65e0\u9700\u6807\u6ce8\u76d1\u7763\uff0c\u4e3a\u91d1\u878d\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u548c\u53ef\u64cd\u4f5c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17458", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.17458", "abs": "https://arxiv.org/abs/2510.17458", "authors": ["Ayrat Abdullin", "Denis Anikiev", "Umair bin Waheed"], "title": "Explainable AI for microseismic event detection", "comment": "Submitted to Artificial Intelligence in Geosciences", "summary": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors.", "AI": {"tldr": "\u5e94\u7528\u53ef\u89e3\u91caAI\u6280\u672f\uff08Grad-CAM\u548cSHAP\uff09\u89e3\u91caPhaseNet\u5730\u9707\u68c0\u6d4b\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u57fa\u4e8eSHAP\u503c\u5f00\u53d1\u4e86\u95e8\u63a7\u63a8\u7406\u65b9\u6848\uff0c\u57289000\u4e2a\u6ce2\u5f62\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u8fbe\u52300.98\uff0c\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5982PhaseNet\u5728\u68c0\u6d4b\u5fae\u5730\u9707\u4e8b\u4ef6\u65f6\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u62c5\u5fe7\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528Grad-CAM\u548cSHAP\u7b49\u53ef\u89e3\u91caAI\u6280\u672f\u5206\u6790PhaseNet\u6a21\u578b\uff1b\u5f00\u53d1SHAP\u95e8\u63a7\u63a8\u7406\u65b9\u6848\uff0c\u5c06\u6a21\u578b\u8f93\u51fa\u4e0e\u57fa\u4e8e\u89e3\u91ca\u7684\u6307\u6807\u7ed3\u5408\u4ee5\u51cf\u5c11\u9519\u8bef\u3002", "result": "Grad-CAM\u663e\u793a\u7f51\u7edc\u6ce8\u610f\u529b\u4e0eP\u6ce2\u548cS\u6ce2\u5230\u8fbe\u65f6\u95f4\u4e00\u81f4\uff1bSHAP\u503c\u91cf\u5316\u7279\u5f81\u8d21\u732e\uff0c\u786e\u8ba4\u5782\u76f4\u5206\u91cf\u5e45\u5ea6\u9a71\u52a8P\u76f8\u4f4d\u62fe\u53d6\uff0c\u6c34\u5e73\u5206\u91cf\u4e3b\u5bfcS\u76f8\u4f4d\u62fe\u53d6\uff1bSHAP\u95e8\u63a7\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u65700.98\uff08\u7cbe\u5ea60.99\uff0c\u53ec\u56de\u73870.97\uff09\uff0c\u4f18\u4e8e\u57fa\u51c6PhaseNet\uff08F1\u5206\u65700.97\uff09\uff0c\u5bf9\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u53ef\u89e3\u91caAI\u4e0d\u4ec5\u80fd\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8fd8\u80fd\u76f4\u63a5\u63d0\u5347\u5176\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684\u81ea\u52a8\u5316\u5730\u9707\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u6a21\u677f\u3002"}}
{"id": "2510.17526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17526", "abs": "https://arxiv.org/abs/2510.17526", "authors": ["Wei Huang", "Andi Han", "Yujin Song", "Yilan Chen", "Denny Wu", "Difan Zou", "Taiji Suzuki"], "title": "How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?", "comment": "40 pages", "summary": "The capacity of deep learning models is often large enough to both learn the\nunderlying statistical signal and overfit to noise in the training set. This\nnoise memorization can be harmful especially for data with a low\nsignal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior\nobservations that label noise provides implicit regularization that improves\ngeneralization, in this work, we investigate whether introducing label noise to\nthe gradient updates can enhance the test performance of neural network (NN) in\nthe low SNR regime. Specifically, we consider training a two-layer NN with a\nsimple label noise gradient descent (GD) algorithm, in an idealized\nsignal-noise data setting. We prove that adding label noise during training\nsuppresses noise memorization, preventing it from dominating the learning\nprocess; consequently, label noise GD enjoys rapid signal growth while the\noverfitting remains controlled, thereby achieving good generalization despite\nthe low SNR. In contrast, we also show that NN trained with standard GD tends\nto overfit to noise in the same low SNR setting and establish a non-vanishing\nlower bound on its test error, thus demonstrating the benefit of introducing\nlabel noise in gradient-based training.", "AI": {"tldr": "\u5728\u4f4e\u4fe1\u566a\u6bd4(SNR)\u6570\u636e\u4e2d\uff0c\u901a\u8fc7\u5411\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fc7\u7a0b\u6dfb\u52a0\u6807\u7b7e\u566a\u58f0\u53ef\u4ee5\u6291\u5236\u566a\u58f0\u8bb0\u5fc6\u5316\uff0c\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u5728\u8bad\u7ec3\u4e2d\u8bb0\u5fc6\u566a\u58f0\uff0c\u7279\u522b\u662f\u5728\u4f4eSNR\u6570\u636e\u4e2d\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002\u53d7\u6807\u7b7e\u566a\u58f0\u5177\u6709\u6b63\u5219\u5316\u6548\u679c\u7684\u542f\u53d1\uff0c\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u6807\u7b7e\u566a\u58f0\u6765\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u5728\u4f4eSNR\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u7406\u60f3\u5316\u7684\u4fe1\u53f7-\u566a\u58f0\u6570\u636e\u8bbe\u7f6e\u4e2d\uff0c\u91c7\u7528\u5e26\u6807\u7b7e\u566a\u58f0\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6807\u7b7e\u566a\u58f0GD\u80fd\u591f\u6291\u5236\u566a\u58f0\u8bb0\u5fc6\u5316\uff0c\u4fc3\u8fdb\u4fe1\u53f7\u5feb\u901f\u5b66\u4e60\uff0c\u540c\u65f6\u63a7\u5236\u8fc7\u62df\u5408\uff0c\u5728\u4f4eSNR\u4e0b\u5b9e\u73b0\u826f\u597d\u6cdb\u5316\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6807\u51c6GD\u5bb9\u6613\u8fc7\u62df\u5408\u566a\u58f0\uff0c\u6d4b\u8bd5\u8bef\u5dee\u5b58\u5728\u975e\u96f6\u4e0b\u754c\u3002", "conclusion": "\u5728\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u4e2d\u5f15\u5165\u6807\u7b7e\u566a\u58f0\u53ef\u4ee5\u6709\u6548\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u5728\u4f4eSNR\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17584", "abs": "https://arxiv.org/abs/2510.17584", "authors": ["Ludi Li", "Junbin Mao", "Hanhe Lin", "Xu Tian", "Fang-Xiang Wu", "Jin Liu"], "title": "CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification", "comment": null, "summary": "Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical\npractice such as Alzheimer's disease diagnosis. To train a robust model for\nmulti-pulse MRI classification, it requires large and diverse data from various\nmedical institutions while protecting privacy by preventing raw data sharing\nacross institutions. Although federated learning (FL) is a feasible solution to\naddress this issue, it poses challenges of model convergence due to the effect\nof data heterogeneity and substantial communication overhead due to large\nnumbers of parameters transmitted within the model. To address these\nchallenges, we propose CEPerFed, a communication-efficient personalized FL\nmethod. It mitigates the effect of data heterogeneity by incorporating\nclient-side historical risk gradients and historical mean gradients to\ncoordinate local and global optimization. The former is used to weight the\ncontributions from other clients, enhancing the reliability of local updates,\nwhile the latter enforces consistency between local updates and the global\noptimization direction to ensure stable convergence across heterogeneous data\ndistributions. To address the high communication overhead, we propose a\nhierarchical SVD (HSVD) strategy that transmits only the most critical\ninformation required for model updates. Experiments on five classification\ntasks demonstrate the effectiveness of the CEPerFed method. The code will be\nreleased upon acceptance at https://github.com/LD0416/CEPerFed.", "AI": {"tldr": "\u63d0\u51faCEPerFed\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u5386\u53f2\u98ce\u9669\u68af\u5ea6\u548c\u5386\u53f2\u5e73\u5747\u68af\u5ea6\u534f\u8c03\u672c\u5730\u4e0e\u5168\u5c40\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u5206\u5c42SVD\u7b56\u7565\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u89e3\u51b3\u591a\u8109\u51b2MRI\u5206\u7c7b\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u591a\u8109\u51b2MRI\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u6570\u636e\u4e14\u9700\u4fdd\u62a4\u9690\u79c1\u3002\u8054\u90a6\u5b66\u4e60\u867d\u7136\u53ef\u884c\uff0c\u4f46\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u6a21\u578b\u6536\u655b\u95ee\u9898\u548c\u5927\u91cf\u53c2\u6570\u4f20\u8f93\u5e26\u6765\u7684\u901a\u4fe1\u5f00\u9500\u6311\u6218\u3002", "method": "CEPerFed\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5ba2\u6237\u7aef\u5386\u53f2\u98ce\u9669\u68af\u5ea6\u52a0\u6743\u5176\u4ed6\u5ba2\u6237\u7aef\u8d21\u732e\uff0c\u589e\u5f3a\u672c\u5730\u66f4\u65b0\u53ef\u9760\u6027\uff1b2) \u4f7f\u7528\u5386\u53f2\u5e73\u5747\u68af\u5ea6\u786e\u4fdd\u672c\u5730\u66f4\u65b0\u4e0e\u5168\u5c40\u4f18\u5316\u65b9\u5411\u4e00\u81f4\uff1b3) \u91c7\u7528\u5206\u5c42SVD\u7b56\u7565\u4ec5\u4f20\u8f93\u6a21\u578b\u66f4\u65b0\u6240\u9700\u7684\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5728\u4e94\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CEPerFed\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CEPerFed\u901a\u8fc7\u534f\u8c03\u672c\u5730\u4e0e\u5168\u5c40\u4f18\u5316\u4ee5\u53ca\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u591a\u8109\u51b2MRI\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2510.17661", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17661", "abs": "https://arxiv.org/abs/2510.17661", "authors": ["Vaishnavi Visweswaraiah", "Tanvi Banerjee", "William Romine"], "title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction", "comment": null, "summary": "Suicide prediction is the key for prevention, but real data with sufficient\npositive samples is rare and causes extreme class imbalance. We utilized\nmachine learning (ML) to build the model and deep learning (DL) techniques,\nlike Generative Adversarial Networks (GAN), to generate synthetic data samples\nto enhance the dataset. The initial dataset contained 656 samples, with only\nfour positive cases, prompting the need for data augmentation. A variety of\nmachine learning models, ranging from interpretable data models to black box\nalgorithmic models, were used. On real test data, Logistic Regression (LR)\nachieved a weighted precision of 0.99, a weighted recall of 0.85, and a\nweighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,\nrespectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.\nLR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and\nmisclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &\n0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)\nwith 0 false positives (specificity: 1.0). These results highlight the models'\neffectiveness, with GAN playing a key role in generating synthetic data to\nsupport suicide prevention modeling efforts.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u7279\u522b\u662fGAN\uff09\u6765\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u81ea\u6740\u9884\u6d4b\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u6740\u9884\u6d4b\u662f\u9884\u9632\u7684\u5173\u952e\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u4e2d\u9633\u6027\u6837\u672c\u7a00\u5c11\uff0c\u5bfc\u81f4\u6781\u7aef\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9700\u8981\u6570\u636e\u589e\u5f3a\u6765\u89e3\u51b3\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u751f\u6210\u5bf9\u6297\u7f51\u7edcGAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u6837\u672c\u6765\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\uff0c\u903b\u8f91\u56de\u5f52\u7684\u52a0\u6743\u7cbe\u5ea60.99\u3001\u53ec\u56de\u73870.85\u3001F1\u5206\u65700.91\uff1b\u968f\u673a\u68ee\u6797\u5206\u522b\u4e3a0.98\u30010.99\u30010.99\uff1b\u652f\u6301\u5411\u91cf\u673a\u4e3a0.99\u30010.76\u30010.86\u3002LR\u548cSVM\u80fd\u6b63\u786e\u8bc6\u522b\u81ea\u6740\u5c1d\u8bd5\u6848\u4f8b\uff0c\u4f46RF\u672a\u80fd\u8bc6\u522b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0cGAN\u5728\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u652f\u6301\u81ea\u6740\u9884\u9632\u5efa\u6a21\u5de5\u4f5c\u4e2d\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.17727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17727", "abs": "https://arxiv.org/abs/2510.17727", "authors": ["Ege Beyazit", "KL Navaneet", "Prashant Mathur", "Roi Blanco", "Vidit Bansal", "Karim Bouyarmane"], "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs", "comment": "35 pages", "summary": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5206\u7c7b\u5668\u7684\u64cd\u4f5c\u7c92\u5ea6\uff0c\u901a\u8fc7\u5206\u6790\u5176\u4f4e\u57fa\u6570\u6570\u503c\u8f93\u51fa\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u65b9\u6cd5\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u589e\u52a0\u53ef\u7528\u64cd\u4f5c\u70b9\u3002", "motivation": "\u9ed1\u76d2LLMs\u5728\u9700\u8981\u7279\u5b9a\u6307\u6807\u7ea6\u675f\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u6570\u503c\u8f93\u51fa\u57fa\u6570\u4f4e\uff0c\u9650\u5236\u4e86\u64cd\u4f5c\u70b9\u7684\u7cbe\u7ec6\u8c03\u6574\u80fd\u529b\u3002", "method": "\u9996\u5148\u5206\u6790LLMs\u4f4e\u57fa\u6570\u8f93\u51fa\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u5176\u504f\u5411\u751f\u6210\u56db\u820d\u4e94\u5165\u4f46\u6709\u4fe1\u606f\u91cf\u7684\u8bed\u8a00\u5316\u6982\u7387\uff1b\u7136\u540e\u5b9e\u9a8c\u6807\u51c6\u63d0\u793a\u5de5\u7a0b\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7f6e\u4fe1\u5ea6\u6fc0\u53d1\u6280\u672f\uff1b\u6700\u540e\u63d0\u51fa\u6709\u6548\u65b9\u6cd5\u6765\u663e\u8457\u589e\u52a0\u53ef\u7528\u64cd\u4f5c\u70b9\u7684\u6570\u91cf\u548c\u591a\u6837\u6027\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u572811\u4e2a\u6570\u636e\u96c6\u548c3\u4e2aLLMs\u4e0a\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u64cd\u4f5c\u70b9\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9ed1\u76d2LLMs\u4f5c\u4e3a\u5206\u7c7b\u5668\u7684\u64cd\u4f5c\u7c92\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u7ea6\u675f\u6307\u6807\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.15387", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15387", "abs": "https://arxiv.org/abs/2510.15387", "authors": ["Davide Basso", "Luca Bortolussi", "Mirjana Videnovic-Misic", "Husni Habal"], "title": "Advancing Routing-Awareness in Analog ICs Floorplanning", "comment": null, "summary": "The adoption of machine learning-based techniques for analog integrated\ncircuit layout, unlike its digital counterpart, has been limited by the\nstringent requirements imposed by electric and problem-specific constraints,\nalong with the interdependence of floorplanning and routing steps. In this\nwork, we address a prevalent concern among layout engineers regarding the need\nfor readily available routing-aware floorplanning solutions. To this extent, we\ndevelop an automatic floorplanning engine based on reinforcement learning and\nrelational graph convolutional neural network specifically tailored to\ncondition the floorplan generation towards more routable outcomes. A\ncombination of increased grid resolution and precise pin information\nintegration, along with a dynamic routing resource estimation technique, allows\nbalancing routing and area efficiency, eventually meeting industrial standards.\nWhen analyzing the place and route effectiveness in a simulated environment,\nthe proposed approach achieves a 13.8% reduction in dead space, a 40.6%\nreduction in wirelength and a 73.4% increase in routing success when compared\nto past learning-based state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u5173\u7cfb\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5e03\u5c40\u5f15\u64ce\uff0c\u4e13\u95e8\u9488\u5bf9\u6a21\u62df\u96c6\u6210\u7535\u8def\u5e03\u5c40\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u9ad8\u7f51\u683c\u5206\u8fa8\u7387\u3001\u7cbe\u786e\u5f15\u811a\u4fe1\u606f\u96c6\u6210\u548c\u52a8\u6001\u5e03\u7ebf\u8d44\u6e90\u4f30\u8ba1\u6280\u672f\uff0c\u5728\u5e03\u7ebf\u6210\u529f\u7387\u548c\u9762\u79ef\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6a21\u62df\u96c6\u6210\u7535\u8def\u5e03\u5c40\u5de5\u7a0b\u5e08\u5bf9\u5e03\u7ebf\u611f\u77e5\u7684\u5e03\u5c40\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u7535\u6c14\u7ea6\u675f\u3001\u7279\u5b9a\u95ee\u9898\u8981\u6c42\u548c\u5e03\u5c40\u5e03\u7ebf\u6b65\u9aa4\u76f8\u4e92\u4f9d\u8d56\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u5173\u7cfb\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u81ea\u52a8\u5e03\u5c40\u5f15\u64ce\uff0c\u7ed3\u5408\u9ad8\u7f51\u683c\u5206\u8fa8\u7387\u3001\u7cbe\u786e\u5f15\u811a\u4fe1\u606f\u96c6\u6210\u548c\u52a8\u6001\u5e03\u7ebf\u8d44\u6e90\u4f30\u8ba1\u6280\u672f\u3002", "result": "\u4e0e\u73b0\u6709\u5b66\u4e60\u578b\u6700\u5148\u8fdb\u6280\u672f\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e8613.8%\u7684\u6b7b\u533a\u51cf\u5c11\u300140.6%\u7684\u7ebf\u957f\u51cf\u5c11\u548c73.4%\u7684\u5e03\u7ebf\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5e73\u8861\u5e03\u7ebf\u548c\u9762\u79ef\u6548\u7387\uff0c\u6ee1\u8db3\u5de5\u4e1a\u6807\u51c6\uff0c\u4e3a\u6a21\u62df\u96c6\u6210\u7535\u8def\u5e03\u5c40\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e03\u7ebf\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
