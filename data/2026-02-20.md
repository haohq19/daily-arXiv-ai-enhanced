<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD是一个用于弱监督视频异常检测的双分支框架，通过交叉伪标签交换，结合异常定位和类别分类能力，在XD-Violence和UCF-Crime数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 弱监督视频异常检测需要在仅有视频级标签的情况下检测异常并识别异常类别。现有方法往往难以同时实现精确的异常定位和准确的类别识别。

Method: 提出CPL-VAD双分支框架：1）二元异常检测分支专注于片段级异常定位；2）类别分类分支利用视觉-语言对齐识别异常事件类别。通过交叉伪标签交换，两个分支相互传递互补优势，结合时间精度和语义判别能力。

Result: 在XD-Violence和UCF-Crime数据集上的实验表明，CPL-VAD在异常检测和异常类别分类两方面都达到了最先进的性能。

Conclusion: CPL-VAD通过双分支框架和交叉伪标签机制，有效解决了弱监督视频异常检测中异常定位和类别识别的双重挑战，实现了优越的性能。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [2] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: ComptonUNet是一种混合深度学习框架，用于在光子统计有限和背景噪声强的条件下，对伽马射线暴进行鲁棒定位。


<details>
  <summary>Details</summary>
Motivation: 微弱伽马射线暴（GRBs）能提供早期恒星形成的独特见解，但由于光子统计量低和背景噪声大，检测和定位这些弱源仍然具有挑战性。现有机器学习模型难以在统计鲁棒性和噪声抑制之间取得平衡。

Method: 提出ComptonUNet混合深度学习框架，联合处理原始数据并重建图像，结合直接重建模型的统计效率和基于图像架构的去噪能力。

Result: 在模拟低地球轨道任务背景环境中的GRB类事件评估中，ComptonUNet显著优于现有方法，在广泛的低统计量和高背景场景下实现了改进的定位精度。

Conclusion: ComptonUNet为在具有挑战性的观测条件下检测和定位微弱伽马射线暴提供了一种有效的解决方案，平衡了统计鲁棒性和噪声抑制的需求。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [3] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker：基于强化微调的方法，通过构建事件级场景图增强视觉基础，减少视频推理中的幻觉


<details>
  <summary>Details</summary>
Motivation: 视频推理需要理解事件间的因果关系，但这些关系通常是隐式的且标注成本高。现有的多模态大语言模型通过密集描述或视频摘要推断事件关系，缺乏因果结构建模，导致推理时产生幻觉。

Method: 1. 使用MLLM构建事件级视频场景图(EVSG)，显式建模事件内和事件间关系；2. 将场景图作为中间思考过程融入MLLM；3. 在强化微调中引入视觉注意力奖励，增强视频基础并减少幻觉。

Result: 在RexTime和VidHalluc数据集上评估，GraphThinker在捕捉对象和事件关系方面表现优异，具有更精确的事件定位能力，相比先前方法减少了视频推理中的幻觉。

Conclusion: 通过显式建模事件级场景图并增强视觉基础，GraphThinker有效减少了视频推理中的幻觉问题，提升了因果理解能力。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [4] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: ArtToMus：首个直接从艺术品生成音乐的框架，无需图像转文本翻译，使用视觉嵌入指导潜在扩散模型生成音乐。


<details>
  <summary>Details</summary>
Motivation: 现有图像条件音乐生成系统存在两个根本限制：1）通常在自然照片上训练，难以捕捉艺术品丰富的语义、风格和文化内容；2）大多依赖图像到文本转换阶段，使用语言作为语义捷径，阻碍了直接的视觉到音频学习。

Method: 提出ArtSound数据集（105,884个艺术品-音乐对，带双模态标注）和ArtToMus框架，将视觉嵌入投影到潜在扩散模型的调节空间，实现无需语言监督的直接艺术品到音乐生成。

Result: ArtToMus生成音乐连贯且风格一致，能反映源艺术品的显著视觉线索。虽然绝对对齐分数低于文本条件系统（因去除语言监督难度更大），但在感知质量和有意义的跨模态对应方面具有竞争力。

Conclusion: 这项工作确立了直接视觉到音乐生成作为一个独特且具有挑战性的研究方向，为多媒体艺术、文化遗产和AI辅助创意实践提供了资源支持。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文通过几何分析揭示grokking现象的本质：模型训练初期被限制在低维执行子空间中，当横向曲率积累到临界点后，模型才能逃逸该亚稳态区域实现泛化。


<details>
  <summary>Details</summary>
Motivation: 理解grokking现象——在小规模算法任务中从记忆到泛化的延迟转变机制。目前对这一现象的理解仍然有限，需要探究其背后的优化动力学和损失景观几何特性。

Method: 1. 在模运算任务上训练transformer模型；2. 对注意力权重轨迹进行PCA分析，识别低维执行子空间；3. 测量交换子缺陷（连续梯度步的非交换性）来探测损失景观曲率；4. 将曲率投影到学习到的子空间上；5. 进行因果干预实验，验证子空间运动和曲率积累的必要性。

Result: 1. 训练动态主要发生在低维执行子空间内（单个主成分解释68-83%方差）；2. 横向于执行子空间的方向上曲率急剧增长；3. 曲率增长始终先于泛化发生，且领先时间服从幂律关系；4. 因果实验表明：沿学习子空间的运动对grokking是必要的，但人为增加曲率不足以引发泛化。

Conclusion: grokking反映了从亚稳态区域的逃逸过程，该区域特征为低维限制和横向曲率积累。模型需要积累足够的横向曲率才能突破限制，实现从记忆到泛化的转变。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [6] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 该论文通过引入残差连接的理论分析，解决了因果Transformer中注意力机制位置偏差的理论与实践差异，解释了Lost-in-the-Middle现象。


<details>
  <summary>Details</summary>
Motivation: Transformer模型系统性地偏好某些token位置，但导致这种位置偏差的架构起源仍不清楚。先前理论分析预测在无限深度下注意力会崩溃到第一个token，但这与实际情况不符，需要解决这一差异。

Method: 提出残差感知的累积注意力展开理论，通过纳入残差连接来分析注意力机制。在有限深度下，证明因果Transformer会诱导U形位置偏差。

Result: 残差连接防止了注意力在实际条件下的崩溃。在有限深度下，因果Transformer的注意力集中在早期和晚期token上，形成U形位置偏差。

Conclusion: 该研究为Lost-in-the-Middle现象提供了基于架构原理的解释，表明残差连接是防止注意力崩溃的关键组件，并解释了Transformer中系统性的位置偏差模式。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [7] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 研究离线数据驱动报童问题中的需求截断问题，提出计算经典库存策略最坏情况后悔的通用方法，证明无限维非凸优化可简化为有限维问题，分析不同策略在需求截断下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统报童问题假设需求完全可观测，但实际中需求常被库存水平截断，只能观测到销售数据而非真实需求。现有研究缺乏对需求截断情况下数据驱动库存策略性能的系统分析，特别是当销售系统不记录缺货事件时，将销售视为需求的做法可能导致严重性能下降。

Method: 提出通用程序计算经典数据驱动库存策略的最坏情况后悔，将无限维非凸优化问题简化为有限维问题，实现对任意样本量和截断水平下策略性能的精确刻画。分析Kaplan-Meier策略和销售即需求启发式策略的性能。

Result: 需求截断从根本上限制了从被动销售数据中学习的能力，但在高库存水平进行少量有针对性的探索可显著改善最坏情况保证，即使在严重截断下也能实现接近最优性能。销售即需求启发式策略会随着截断数据积累而遭受严重性能下降。

Conclusion: 销售点信息的质量对离线学习能力具有关键影响，需求截断限制了被动学习，但通过少量探索可大幅改善性能。将销售视为需求的做法在截断数据积累时会导致性能严重下降，强调了设计考虑截断的数据驱动库存策略的重要性。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [8] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 该研究开发了一个用于巴西中老年人的痴呆症分类模型，结合变量选择和多元分析，使用低成本可干预变量，基于ELSI-Brazil数据（9,412人），发现随机森林模型表现优于逻辑回归（AUC=0.776）。


<details>
  <summary>Details</summary>
Motivation: 为巴西中老年人群建立痴呆症分类模型，使用低成本且具有干预潜力的变量，以识别高危人群并指导公共卫生政策。

Method: 采用观察性研究和预测建模方法，基于巴西老龄化纵向研究（ELSI-Brazil）的横断面数据（9,412名参与者）。使用随机森林和多元逻辑回归分析，结合神经心理学评估和知情者报告的认知功能确定痴呆状态。

Result: 痴呆患病率为9.6%。主要风险因素包括：文盲（OR=7.42）、90岁以上（OR=11.00）、低体重（OR=2.11）、低握力（OR=2.50）、自报黑人肤色（OR=1.47）、缺乏运动（OR=1.61）、听力损失（OR=1.65）和抑郁症状（OR=1.72）。保护因素包括：高等教育（OR=0.44）、高生活满意度（OR=0.72）和就业（OR=0.78）。随机森林模型表现优于逻辑回归（AUC=0.776）。

Conclusion: 研究结果强调了痴呆症的多维性质以及可及性因素在识别高危人群中的重要性。加强关注促进大脑健康的公共政策，有助于巴西初级保健资源的有效分配和痴呆症预防。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [9] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 该研究发现交换子缺陷（梯度更新的非交换性度量）是Transformer中延迟泛化的早期预警信号，在三个不同任务家族中具有普遍性。


<details>
  <summary>Details</summary>
Motivation: 研究Grokking现象（从记忆到泛化的突然转变）是否仅限于模运算任务，以及是否存在跨任务的通用早期预警信号。

Method: 使用两个序列学习基准：SCAN组合泛化和Dyck-1深度预测，分析交换子缺陷（梯度更新的非交换性度量），并通过权重空间PCA和因果干预实验验证其作用。

Result: 交换子缺陷在泛化前显著上升，遵循超线性幂律；放大非交换性可加速Grokking（SCAN约32%，Dyck约50%），抑制正交梯度流会延迟或阻止Grokking；三个任务家族形成因果敏感性谱系。

Conclusion: 交换子缺陷是Transformer中延迟泛化的稳健、架构无关、因果相关的早期预警信号，具有跨任务普遍性。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [10] [Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles](https://arxiv.org/abs/2602.17028)
*Hyeongwon Kang,Jinwoo Park,Seunghun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: FATE是一个无监督的时间序列异常预测框架，通过集成多个预测模型的预测不确定性来检测异常前兆，无需真实标签即可提供早期预警。


<details>
  <summary>Details</summary>
Motivation: 当前大多数异常检测方法都是反应式的，只能在异常发生后检测，缺乏提供主动早期预警信号的能力。在工业运营、金融和网络安全等领域，早期识别异常模式对确保系统可靠性和预防性维护至关重要。

Method: 提出FATE框架，通过集成多个时间序列预测模型的预测不确定性来量化异常前兆。不同于依赖重构误差或需要真实标签的方法，FATE预测未来值并利用集成模型之间的分歧作为早期异常信号，无需推理时的目标值。

Result: 在五个真实世界基准数据集上的实验表明，FATE在PTaPR AUC上平均提升19.9个百分点，在早期检测F1分数上提升20.02个百分点，优于基线方法且无需异常标签。

Conclusion: FATE展示了在复杂时间序列环境中进行实时无监督早期预警的有效性和实用性，通过预测不确定性和集成方法成功检测异常前兆。

Abstract: Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.

</details>


### [11] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: MPZCH是一种基于线性探测的新型索引机制，通过多探测和主动驱逐策略消除嵌入表中的碰撞问题，已在TorchRec开源库中发布。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统中唯一ID数量的增加，传统的基于哈希的索引方法会产生碰撞，导致模型性能下降和个性化质量降低。需要一种能有效缓解嵌入碰撞的索引机制。

Method: 提出Multi-Probe Zero Collision Hash (MPZCH)，基于线性探测的索引机制，使用辅助张量和高性能CUDA内核实现可配置的探测和主动驱逐策略，通过淘汰过时ID和重置重新分配的槽位来防止陈旧嵌入继承。

Result: MPZCH实现了用户嵌入的零碰撞，显著提高了项目嵌入的新鲜度和质量，同时保持了与现有方法相当的训练QPS和推理延迟。已在TorchRec开源库中发布。

Conclusion: MPZCH有效解决了大规模推荐系统中嵌入表的碰撞问题，在消除碰撞的同时保持了生产级效率，为推荐系统提供了更高质量的嵌入表示。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [12] [AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation](https://arxiv.org/abs/2602.17071)
*Rong Fu,Muge Qi,Chunlei Meng,Shuo Yin,Kun Liu,Zhaolu Kang,Simon Fong*

Main category: cs.LG

TL;DR: AdvSynGNN：通过多分辨率结构合成、对抗传播引擎和标签精炼，提升图神经网络在结构噪声和非同配拓扑下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 图神经网络在面对结构噪声或非同配拓扑时经常出现显著的性能下降，存在系统性脆弱性，需要更鲁棒的节点级表示学习方法

Method: 1. 多分辨率结构合成与对比目标建立几何敏感初始化；2. 基于Transformer的主干网络通过学习的拓扑信号调制注意力机制以适应异质性；3. 集成对抗传播引擎（生成器识别潜在连接变化，判别器强制全局一致性）；4. 基于节点置信度指标的残差校正方案实现标签精炼

Result: 经验评估表明，这种协同方法能有效优化不同图分布下的预测准确性，同时保持计算效率

Conclusion: AdvSynGNN系统在大规模环境中具有鲁棒部署的潜力，研究提供了实际实现协议以确保系统的稳健部署

Abstract: Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.

</details>


### [13] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 提出一个基于无服务器架构的MLOps框架，通过事件驱动管道管理完整的机器学习生命周期，应用于海关HS编码预测任务，实现98%准确率，同时确保可重现性、可审计性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 海关HS编码预测任务具有挑战性：产品描述简短、非结构化，频繁更新且存在歧义，分类错误会导致货运延迟和经济损失。传统方法难以满足合规性要求、可扩展性和成本效益的需求。

Method: 采用无服务器MLOps框架，包含自定义文本嵌入编码器和多种深度学习架构（Text-CNN表现最佳）。架构模型无关，支持标准化接口，通过事件驱动管道实现自动化训练、部署、监控和重训练，包含A/B测试和自动扩展功能。

Result: Text-CNN在真实数据上达到98%准确率。框架确保可重现性、可审计性和SLA遵守，在可变负载下通过自动扩展保持性能。相比Transformer模型，在保持相似准确率的同时显著降低长期运营成本。

Conclusion: 该工作提供了一个可复制的无服务器ML操作化蓝图，使企业能够在优化性能和经济效益的同时扩展机器学习应用，优先考虑确定性分类、可预测延迟和可解释性，同时保持对Transformer变体和LLM推理的可扩展性。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [14] [The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound](https://arxiv.org/abs/2602.17321)
*Christoph Balada,Aida Romano-Martinez,Payal Varshney,Vincent ten Cate,Katharina Geschke,Jonas Tesarz,Paul Claßen,Alexander K. Schuster,Dativa Tibyampansha,Karl-Patrik Kresoja,Philipp S. Wild,Sheraz Ahmed,Andreas Dengel*

Main category: cs.LG

TL;DR: 利用机器学习从颈动脉超声视频中提取血管损伤表征，作为心血管风险评估的非侵入性工具


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，但早期风险检测受限于现有诊断方法。颈动脉超声作为非侵入性、广泛可及的模态，蕴含丰富的结构和血流动力学信息未被充分利用。

Method: 开发机器学习框架，以高血压作为弱代理标签，从颈动脉超声视频中提取具有临床意义的血管损伤表征。模型学习到生物学上合理、可解释的特征。

Result: 高血管损伤评分能有效分层预测心肌梗死、心脏死亡和全因死亡率，性能匹配或优于传统风险模型（如SCORE2）。可解释AI分析显示模型依赖血管形态和血管周围组织特征。

Conclusion: 常规颈动脉超声包含比先前认识更多的预后信息。该方法为人群心血管风险评估提供了可扩展、非侵入性、成本效益高的工具，无需依赖实验室测试或复杂临床输入。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.

</details>


### [15] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 开源PyTorch库softdtw-cuda-torch，用于GPU上计算SoftDTW，解决了现有实现的三个关键限制：序列长度限制、数值不稳定性和内存消耗问题。


<details>
  <summary>Details</summary>
Motivation: 现有GPU实现存在三个主要问题：1) 硬性序列长度限制（1024），2) 小平滑参数下反向传播数值不稳定，3) 成对距离张量导致GPU内存消耗过大。需要开发一个更高效、稳定的SoftDTW GPU实现。

Method: 提出三种关键技术：1) 平铺反对角线核执行，消除序列长度限制；2) 对数空间反向传播，防止浮点溢出；3) 融合距离计算模式，消除O(BNM)中间距离张量，减少高达98%内存使用。

Result: 实现了一个支持任意序列长度、完整PyTorch自动微分集成和SoftDTW重心计算的库。相比先前工作，内存使用减少高达98%，解决了数值稳定性问题。

Conclusion: softdtw-cuda-torch是一个高效、稳定的SoftDTW GPU实现，解决了现有实现的关键限制，为时间序列分析提供了更好的工具支持。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [16] [LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy](https://arxiv.org/abs/2602.17312)
*Hsin-Jung Yang,Zhanhong Jiang,Prajwal Koirala,Qisai Liu,Cody Fleming,Soumik Sarkar*

Main category: cs.LG

TL;DR: LexiSafe是一个词典序离线强化学习框架，通过词典序优先级和结构偏置来保证安全对齐行为，在标准单成本和多成本安全需求下都提供了样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全强化学习方法通常通过约束松弛或联合优化来平衡奖励-安全权衡，但缺乏防止安全漂移的结构机制。在信息物理系统中，训练期间的安全违规是不可接受的，且只有预收集数据可用。

Method: 提出LexiSafe词典序离线RL框架：1) LexiSafe-SC单成本公式用于标准离线安全RL，推导安全违规和性能次优性边界；2) LexiSafe-MC扩展支持多安全成本的层次安全需求，并提供相应的样本复杂度分析。

Result: 经验上，LexiSafe相比约束离线基线方法减少了安全违规并提高了任务性能。框架通过词典序优先级与结构偏置的统一，为安全关键CPS决策提供了实用且理论可靠的方法。

Conclusion: LexiSafe通过词典序优先级和结构偏置机制，为离线安全强化学习提供了防止安全漂移的实用解决方案，在理论和实验上都表现出优越性。

Abstract: Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.

</details>


### [17] [From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection](https://arxiv.org/abs/2602.17342)
*Luzhi Wang,Xuanshuo Fu,He Zhang,Chuang Liu,Xiaobao Wang,Hongbo Liu*

Main category: cs.LG

TL;DR: SIGOOD：一种自改进的图OOD检测框架，通过提示增强图和能量偏好优化损失，在测试时训练中实现渐进式OOD信号放大


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法大多采用一次性推理范式，无法渐进式修正错误预测以放大OOD信号。需要一种能够持续自我学习并与测试时训练结合的框架

Method: 提出SIGOOD框架：1）生成提示构建提示增强图以放大OOD信号；2）引入能量偏好优化损失，利用原始测试图与提示增强图之间的能量差异；3）通过自改进循环迭代优化提示，最终使用最优提示增强图进行OOD检测

Result: 在21个真实世界数据集上的综合评估证实了SIGOOD的有效性和优越性能

Conclusion: SIGOOD通过集成持续自学习与测试时训练，提供了一种有效的无监督图OOD检测方法，能够渐进式修正预测并放大OOD信号

Abstract: Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \textbf{S}elf-\textbf{I}mproving \textbf{G}raph \textbf{O}ut-\textbf{o}f-\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态感知的安全监控框架，通过RNN架构追踪多轮对话中的用户意图演变，显著提升越狱攻击检测性能，同时保持实时推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护大多是无状态的，将多轮对话视为独立事件，导致无法检测跨轮次边界的渐进式恶意意图（如Crescendo和ActorAttack攻击），存在"安全漏洞"。

Method: 提出DeepContext状态监控框架，采用RNN架构处理经过微调的轮次级嵌入序列，通过隐藏状态在对话中传播，捕捉风险累积过程，替代孤立评估模型。

Result: 在多轮越狱检测中显著优于现有基线，达到最先进的F1分数0.84，远超云提供商防护和主流开源模型（Llama-Prompt-Guard-2和Granite-Guardian均为0.67），在T4 GPU上保持低于20ms的推理开销。

Conclusion: 建模意图的序列演化比部署大规模无状态模型更有效且计算效率更高，为实时应用中的LLM安全防护提供了可行方案。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [19] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过搜索-验证流程解决时间序列数据库的自然语言查询问题，使用SQL搜索候选窗口，再用Python程序验证原始信号，并提出了首个大规模基准NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据，需要新的解决方案来帮助非专业用户从海量时间记录中检索有意义的事件、区间和摘要。

Method: 提出Sonar-TS神经符号框架，采用类似主动声纳的搜索-验证流程：首先通过特征索引使用SQL搜索候选窗口，然后生成Python程序锁定并验证候选窗口与原始信号的匹配。

Result: 实验表明Sonar-TS能有效处理传统方法失败的复杂时间查询，并揭示了该领域的独特挑战。提出的NLQTSBench是首个为TSDB规模历史设计的NLQ大规模基准。

Conclusion: 这是对NLQ4TSDB的首次系统性研究，提供了一个通用框架和评估标准，有助于推动未来研究。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [20] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 提出PA-MoE方法，通过相位感知路由解决传统MoE在RL中token级路由导致相位一致性模式碎片化的问题，让专家能专注于特定相位任务。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单一策略网络导致简单任务占据大部分参数和梯度更新（简单性偏差），复杂任务能力不足。传统MoE的token级路由会将相位一致性模式碎片化，破坏专家专业化。

Method: 提出相位感知专家混合（PA-MoE）：1）轻量级相位路由器直接从RL目标学习潜在相位边界，无需预定义相位类别；2）相位路由器为相同专家分配时间一致的分配，让专家保持相位特定专业知识。

Result: 实验结果证明了PA-MoE的有效性。

Conclusion: PA-MoE通过相位感知路由解决了传统MoE在RL中的局限性，让专家能更好地专注于特定相位任务，提升复杂任务解决能力。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [21] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLM预测中的时间知识泄露，并开发TimeSPEC方法通过声明验证减少泄露


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测未来事件能力需要回溯测试，但模型可能无意中泄露训练时学到的截止日期后知识，影响评估有效性

Method: 1) 提出Shapley-DCLR指标：将模型推理分解为原子声明，按时间可验证性分类，用Shapley值衡量每个声明对预测的贡献；2) 开发TimeSPEC方法：在生成过程中插入声明验证和再生步骤，主动过滤时间污染

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上实验显示标准提示基线存在显著泄露，TimeSPEC能降低Shapley-DCLR同时保持任务性能

Conclusion: 显式的、可解释的声明级验证优于基于提示的时间约束，TimeSPEC方法能更可靠地进行回溯测试，减少时间知识泄露

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: ReIn是一种无需修改模型参数或系统提示的测试时干预方法，通过植入初始推理来帮助对话代理从用户诱导的错误中恢复，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的对话代理在固定任务导向数据集上表现良好，但对用户诱导的意外错误很脆弱。由于微调或修改提示的成本和时间要求很高，需要在保持模型参数和提示不变的情况下实现错误恢复。

Method: 提出Reasoning Inception (ReIn)方法：外部起始模块识别对话上下文中的预定义错误并生成恢复计划，然后将这些计划整合到代理的内部推理过程中，引导其采取纠正措施，而不修改参数或系统提示。

Result: ReIn显著提高了任务成功率，能够泛化到未见过的错误类型，并且在各种代理模型和起始模块组合中都表现良好。它始终优于显式提示修改方法，表明其作为高效即时方法的实用性。

Conclusion: ReIn是一种安全有效的策略，无需修改骨干模型或系统提示就能提高对话代理的韧性。通过联合定义恢复工具，可以改善代理的恢复能力，操作机制分析显示其在指令层次结构方面具有优势。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [23] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 该论文提出了一个修订的跨文档共指消解标注方案，将共指链视为话语元素，以分析新闻媒体中多样化和两极分化的报道。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注事件消解，采用狭窄的共指定义，限制了在词汇变化广泛的多样化和两极分化新闻报道中的分析效果。

Method: 提出修订的CDCR标注方案，将共指链视为话语元素，容纳身份和近身份关系；使用统一标注手册重新标注NewsWCL50和ECB+子集。

Result: 重新标注的数据集在词汇多样性指标和基线测试中表现一致，介于原始ECB+和NewsWCL50之间，支持平衡和话语感知的CDCR研究。

Conclusion: 修订的标注方案能够捕捉媒体话语中的词汇多样性和框架变化，为新闻领域的平衡和话语感知的CDCR研究提供了更好的数据集支持。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [24] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: 提出一种用于自主叉车在倾斜表面上卸货的控制方法，通过ICP算法实时追踪货盘与货叉的相对位置和姿态，使货叉与目标表面平行对齐，从而避免拖拽货盘。


<details>
  <summary>Details</summary>
Motivation: 自主叉车在倾斜表面（如卡车倾斜床板）上卸货时，传统方法容易导致货盘被拖拽，需要一种能够确保货叉与倾斜表面对齐的控制方法。

Method: 使用迭代最近点（ICP）算法处理货盘上部区域测量的点云数据，实时追踪货盘与货叉的相对位置和姿态角度差异，据此调整货叉使其与目标倾斜表面对齐，然后沿倾斜方向撤回货叉。

Result: 通过动态仿真和真实叉车实验验证了方法的有效性，成功在卡车倾斜床板上完成卸货操作，避免了货盘拖拽问题。

Conclusion: 提出的基于ICP算法的控制方法能够有效实现自主叉车在倾斜表面的无拖拽卸货操作，提高了卸货的可靠性和安全性。

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [25] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: 提出一种仅使用IMU和电机测量的纯本体感知状态估计器，通过接触腿作为运动学锚点抑制长期漂移，适用于双足、四足和轮腿机器人。


<details>
  <summary>Details</summary>
Motivation: 解决无相机或激光雷达的腿式机器人里程计问题，克服IMU漂移和关节速度噪声的挑战。

Method: 将接触腿视为运动学锚点，基于关节扭矩的足部力估计选择可靠接触点，足部落点位置提供间歇性世界坐标系约束；引入高度聚类和时间衰减校正防止高程漂移；应用逆运动学容积卡尔曼滤波器改善足端速度观测。

Result: 在四个四足平台上测试：Astrall点足机器人A完成约200米水平环路误差0.1638米，约15米垂直环路误差0.219米；轮腿机器人B相应误差为0.2264米和0.199米；轮腿机器人C约700米水平环路误差7.68米，约20米垂直环路误差0.540米；Unitree Go2 EDU约120米水平环路误差2.2138米，约8米垂直环路垂直误差小于0.1米。

Conclusion: 该方法为无外部传感器的腿式机器人提供了可靠的纯本体感知状态估计，通过接触约束有效抑制长期漂移，在不同机器人平台上表现出良好性能。

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [26] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow：基于最优传输条件流匹配的无监督安全关键异常检测框架，用于自动驾驶安全验证，通过低秩谱流形约束生成过程，实现稳定似然估计，能识别传统方法忽略的语义违规行为。


<details>
  <summary>Details</summary>
Motivation: L4级自动驾驶安全验证面临瓶颈，传统基于规则的方法难以扩展检测罕见的高风险长尾场景，需要数据驱动的统计安全验证方法。

Method: 1. 使用最优传输条件流匹配(OT-CFM)建模专家人类驾驶行为的连续概率密度；2. 通过PCA瓶颈将生成过程约束到低秩谱流形，确保运动学平滑性；3. 采用早期融合Transformer编码器处理复杂路口的多模态歧义，具有车道感知目标条件；4. 引入运动学复杂度加权方案，在无模拟训练中优先处理高能量机动。

Result: 在Waymo Open Motion Dataset上达到0.766的AUC-ROC；识别出运动学危险与语义违规之间的根本区别；发现传统安全过滤器忽略的分布外行为，如车道边界违规和非规范路口机动。

Conclusion: Deep-Flow为定义统计安全门提供了数学严谨的基础，实现了客观、数据驱动的自动驾驶车队安全部署验证，能够识别传统方法忽略的关键可预测性差距。

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>
