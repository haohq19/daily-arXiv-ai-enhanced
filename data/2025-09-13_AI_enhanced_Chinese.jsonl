{"id": "2509.08910", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08910", "abs": "https://arxiv.org/abs/2509.08910", "authors": ["Tung Vu", "Lam Nguyen", "Quynh Dao"], "title": "PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in real-world applications\nposes unprecedented risks of generating harmful, biased, or misleading\ninformation to vulnerable populations including LGBTQ+ individuals, single\nparents, and marginalized communities. While existing safety approaches rely on\npost-hoc filtering or generic alignment techniques, they fail to proactively\nprevent harmful outputs at the generation source. This paper introduces\nPromptGuard, a novel modular prompting framework with our breakthrough\ncontribution: VulnGuard Prompt, a hybrid technique that prevents harmful\ninformation generation using real-world data-driven contrastive learning.\nVulnGuard integrates few-shot examples from curated GitHub repositories,\nethical chain-of-thought reasoning, and adaptive role-prompting to create\npopulation-specific protective barriers. Our framework employs theoretical\nmulti-objective optimization with formal proofs demonstrating 25-30% analytical\nharm reduction through entropy bounds and Pareto optimality. PromptGuard\norchestrates six core modules: Input Classification, VulnGuard Prompting,\nEthical Principles Integration, External Tool Interaction, Output Validation,\nand User-System Interaction, creating an intelligent expert system for\nreal-time harm prevention. We provide comprehensive mathematical formalization\nincluding convergence proofs, vulnerability analysis using information theory,\nand theoretical validation framework using GitHub-sourced datasets,\nestablishing mathematical foundations for systematic empirical research.", "AI": {"tldr": "PromptGuard\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7VulnGuard Prompt\u6280\u672f\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u9632\u6b62LLM\u751f\u6210\u6709\u5bb3\u4fe1\u606f\uff0c\u7279\u522b\u4fdd\u62a4LGBTQ+\u7b49\u8106\u5f31\u7fa4\u4f53\uff0c\u7406\u8bba\u8bc1\u660e\u53ef\u51cf\u5c1125-30%\u7684\u5371\u5bb3\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u65b9\u6cd5\u4f9d\u8d56\u4e8b\u540e\u8fc7\u6ee4\u6216\u901a\u7528\u5bf9\u9f50\u6280\u672f\uff0c\u65e0\u6cd5\u5728\u751f\u6210\u6e90\u5934\u4e3b\u52a8\u9632\u6b62\u6709\u5bb3\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5bf9\u8106\u5f31\u7fa4\u4f53\u9020\u6210\u98ce\u9669\u3002", "method": "\u63d0\u51faPromptGuard\u6846\u67b6\uff0c\u5305\u542bVulnGuard Prompt\u6df7\u5408\u6280\u672f\uff0c\u6574\u5408GitHub\u6570\u636e\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u3001\u5c11\u6837\u672c\u793a\u4f8b\u3001\u4f26\u7406\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u81ea\u9002\u5e94\u89d2\u8272\u63d0\u793a\uff0c\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u7406\u8bba\u3002", "result": "\u901a\u8fc7\u71b5\u754c\u548c\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u7406\u8bba\u8bc1\u660e\u53ef\u5b9e\u73b025-30%\u7684\u5371\u5bb3\u51cf\u5c11\uff0c\u5efa\u7acb\u4e86\u4f7f\u7528GitHub\u6570\u636e\u96c6\u7684\u7406\u8bba\u9a8c\u8bc1\u6846\u67b6\u3002", "conclusion": "PromptGuard\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5b9e\u65f6\u5371\u5bb3\u9884\u9632\u4e13\u5bb6\u7cfb\u7edf\uff0c\u4e3a\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\u5960\u5b9a\u4e86\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2509.08972", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08972", "abs": "https://arxiv.org/abs/2509.08972", "authors": ["Soheil Zibakhsh Shabgahi", "Pedram Aghazadeh", "Azalia Mirhosseini", "Farinaz Koushanfar"], "title": "ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models", "comment": null, "summary": "The increasing reliance on generative AI models has accelerated the\ngeneration rate of synthetic data, with some projections suggesting that most\navailable new data for training could be machine-generated by 2030. This shift\nto a mainly synthetic content presents a critical challenge: repeated training\nin synthetic data leads to a phenomenon known as model collapse, where model\nperformance degrades over generations of training, eventually rendering the\nmodels ineffective. Although prior studies have explored the causes and\ndetection of model collapse, existing mitigation strategies remain limited.\n  In this paper, we identify model overconfidence in their self-generated data\nas a key driver of collapse. Building on this observation, we propose a\nconfidence-aware loss function that downweights high-confidence predictions\nduring training. We introduce a novel loss function we call Truncated Cross\nEntropy (TCE). We demonstrate that TCE significantly delays model collapse in\nrecursive training.\n  We provide a model-agnostic framework that links the loss function design to\nmodel collapse mitigation and validate our approach both theoretically and\nempirically, showing that it can extend the model's fidelity interval before\ncollapse by more than 2.3x. Finally, we show that our method generalizes across\nmodalities. These findings suggest that the design of loss functions provides a\nsimple yet powerful tool for preserving the quality of generative models in the\nera of increasing synthetic data.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u622a\u65ad\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570(TCE)\uff0c\u901a\u8fc7\u964d\u4f4e\u6a21\u578b\u5bf9\u81ea\u8eab\u751f\u6210\u6570\u636e\u7684\u8fc7\u9ad8\u4fe1\u5fc3\u5ea6\u6765\u5ef6\u7f13\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff0c\u5c06\u6a21\u578b\u4fdd\u771f\u5ea6\u6301\u7eed\u65f6\u95f4\u5ef6\u957f\u4e862.3\u500d\u4ee5\u4e0a\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8d8a\u6765\u8d8a\u591a\uff0c\u53cd\u590d\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\uff0c\u800c\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\u6709\u9650\u3002\u8bc6\u522b\u5230\u6a21\u578b\u5bf9\u81ea\u8eab\u751f\u6210\u6570\u636e\u7684\u8fc7\u9ad8\u4fe1\u5fc3\u5ea6\u662f\u5d29\u6e83\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u5fc3\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u964d\u4f4e\u9ad8\u4fe1\u5fc3\u5ea6\u9884\u6d4b\u7684\u6743\u91cd\u3002\u4ecb\u7ecd\u4e86\u65b0\u7684\u622a\u65ad\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570(TCE)\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\u3002", "result": "TCE\u663e\u8457\u5ef6\u8fdf\u4e86\u9012\u5f52\u8bad\u7ec3\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\uff0c\u5c06\u6a21\u578b\u4fdd\u771f\u5ea6\u6301\u7eed\u65f6\u95f4\u5ef6\u957f\u4e862.3\u500d\u4ee5\u4e0a\u3002\u65b9\u6cd5\u5728\u591a\u79cd\u6a21\u6001\u4e0a\u90fd\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u5728\u5408\u6210\u6570\u636e\u65e5\u76ca\u589e\u957f\u7684\u65f6\u4ee3\u4fdd\u6301\u751f\u6210\u5f0f\u6a21\u578b\u7684\u8d28\u91cf\u3002"}}
{"id": "2509.09030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09030", "abs": "https://arxiv.org/abs/2509.09030", "authors": ["Spencer King", "Zhilu Zhang", "Ruofan Yu", "Baris Coskun", "Wei Ding", "Qian Cui"], "title": "Deep Context-Conditioned Anomaly Detection for Tabular Data", "comment": "Submitted to WSDM 2026. 11 pages, 4 figures, 5 tables, 1 algorithm, 8\n  datasets, contextual anomaly detection framework for tabular data", "summary": "Anomaly detection is critical in domains such as cybersecurity and finance,\nespecially when working with large-scale tabular data. Yet, unsupervised\nanomaly detection -- where no labeled anomalies are available -- remains a\nsignificant challenge. Although various deep learning methods have been\nproposed to model a dataset's joint distribution, real-world tabular data often\ncontain heterogeneous contexts (e.g., different users), making globally rare\nevents normal under certain contexts. Consequently, relying on a single global\ndistribution can overlook these contextual nuances, degrading detection\nperformance. In this paper, we present a context-conditional anomaly detection\nframework tailored for tabular datasets. Our approach automatically identifies\ncontext features and models the conditional data distribution using a simple\ndeep autoencoder. Extensive experiments on multiple tabular benchmark datasets\ndemonstrate that our method outperforms state-of-the-art approaches,\nunderscoring the importance of context in accurately distinguishing anomalous\nfrom normal instances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u4e0a\u4e0b\u6587\u7279\u5f81\u548c\u4f7f\u7528\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u6761\u4ef6\u6570\u636e\u5206\u5e03\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u5305\u542b\u5f02\u6784\u4e0a\u4e0b\u6587\uff08\u5982\u4e0d\u540c\u7528\u6237\uff09\uff0c\u5168\u5c40\u7a00\u6709\u4e8b\u4ef6\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u53ef\u80fd\u662f\u6b63\u5e38\u7684\uff0c\u5355\u4e00\u5168\u5c40\u5206\u5e03\u4f1a\u5ffd\u7565\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u5dee\u5f02\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d", "method": "\u81ea\u52a8\u8bc6\u522b\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u6761\u4ef6\u6570\u636e\u5206\u5e03", "result": "\u5728\u591a\u4e2a\u8868\u683c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5", "conclusion": "\u4e0a\u4e0b\u6587\u5728\u51c6\u786e\u533a\u5206\u5f02\u5e38\u548c\u6b63\u5e38\u5b9e\u4f8b\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528"}}
{"id": "2509.09546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09546", "abs": "https://arxiv.org/abs/2509.09546", "authors": ["Yanhui Lu", "Zeyu Deng", "Stephen J. Redmond", "Efi Psomopoulou", "Benjamin Ward-Cherrier"], "title": "A Neuromorphic Incipient Slip Detection System using Papillae Morphology", "comment": "7 pages, 12 figures. Submitted to IEEE Robotics and Automation\n  Letters (RAL), under review", "summary": "Detecting incipient slip enables early intervention to prevent object\nslippage and enhance robotic manipulation safety. However, deploying such\nsystems on edge platforms remains challenging, particularly due to energy\nconstraints. This work presents a neuromorphic tactile sensing system based on\nthe NeuroTac sensor with an extruding papillae-based skin and a spiking\nconvolutional neural network (SCNN) for slip-state classification. The SCNN\nmodel achieves 94.33% classification accuracy across three classes (no slip,\nincipient slip, and gross slip) in slip conditions induced by sensor motion.\nUnder the dynamic gravity-induced slip validation conditions, after temporal\nsmoothing of the SCNN's final-layer spike counts, the system detects incipient\nslip at least 360 ms prior to gross slip across all trials, consistently\nidentifying incipient slip before gross slip occurs. These results demonstrate\nthat this neuromorphic system has stable and responsive incipient slip\ndetection capability.", "AI": {"tldr": "\u57fa\u4e8eNeuroTac\u611f\u5e94\u5668\u5488\u7c73\u76ae\u80a4\u5488\u7c73\u76ae\u80a4\u7684\u795e\u7ecf\u5143\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u95f4\u523b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b094.33%\u7684\u6ed1\u52a8\u72b6\u6001\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u80fd\u591f\u5728\u6ed1\u52a8\u53d1\u751f\u524d360ms\u68c0\u6d4b\u5230\u521d\u59cb\u6ed1\u52a8", "motivation": "\u89e3\u51b3\u5728\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u4e0a\u90e8\u7f72\u521d\u59cb\u6ed1\u52a8\u68c0\u6d4b\u7cfb\u7edf\u65f6\u9047\u5230\u7684\u80fd\u6e90\u7ea6\u675f\u6311\u6218\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u6027", "method": "\u4f7f\u7528NeuroTac\u611f\u5e94\u5668\u914d\u5907\u7a81\u51fa\u7684\u8089\u7f8e\u76ae\u80a4\uff0c\u6784\u5efa\u95f4\u523b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(SCNN)\u8fdb\u884c\u6ed1\u52a8\u72b6\u6001\u5206\u7c7b\uff0c\u5305\u62ec\u65e0\u6ed1\u52a8\u3001\u521d\u59cb\u6ed1\u52a8\u548c\u663e\u8457\u6ed1\u52a8\u4e09\u4e2a\u7c7b\u522b", "result": "SCNN\u6a21\u578b\u5728\u611f\u5e94\u5668\u79fb\u52a8\u5f15\u8d77\u7684\u6ed1\u52a8\u6761\u4ef6\u4e0b\u8fbe\u523194.33%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff1b\u5728\u52a8\u6001\u91cd\u529b\u5f15\u8d77\u7684\u6ed1\u52a8\u9a8c\u8bc1\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u80fd\u591f\u5728\u663e\u8457\u6ed1\u52a8\u53d1\u751f\u524d\u81f3\u5c11360ms\u68c0\u6d4b\u5230\u521d\u59cb\u6ed1\u52a8", "conclusion": "\u8fd9\u79cd\u795e\u7ecf\u5143\u5f62\u6001\u7cfb\u7edf\u5177\u6709\u7a33\u5b9a\u4e14\u54cd\u5e94\u5feb\u901f\u7684\u521d\u59cb\u6ed1\u52a8\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e9\u671f\u5e72\u9884\u624b\u6bb5"}}
{"id": "2509.09584", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09584", "abs": "https://arxiv.org/abs/2509.09584", "authors": ["Lingdong Kong", "Dongyue Lu", "Ao Liang", "Rong Li", "Yuhao Dong", "Tianshuai Hu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "Visual Grounding from Event Cameras", "comment": "Abstract Paper (Non-Archival) @ ICCV 2025 NeVi Workshop", "summary": "Event cameras capture changes in brightness with microsecond precision and\nremain reliable under motion blur and challenging illumination, offering clear\nadvantages for modeling highly dynamic scenes. Yet, their integration with\nnatural language understanding has received little attention, leaving a gap in\nmultimodal perception. To address this, we introduce Talk2Event, the first\nlarge-scale benchmark for language-driven object grounding using event data.\nBuilt on real-world driving scenarios, Talk2Event comprises 5,567 scenes,\n13,458 annotated objects, and more than 30,000 carefully validated referring\nexpressions. Each expression is enriched with four structured attributes --\nappearance, status, relation to the viewer, and relation to surrounding objects\n-- that explicitly capture spatial, temporal, and relational cues. This\nattribute-centric design supports interpretable and compositional grounding,\nenabling analysis that moves beyond simple object recognition to contextual\nreasoning in dynamic environments. We envision Talk2Event as a foundation for\nadvancing multimodal and temporally-aware perception, with applications\nspanning robotics, human-AI interaction, and so on.", "AI": {"tldr": "Talk2Event\u662f\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u9a71\u52a8\u7269\u4f53\u5b9a\u4f4d\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b5,567\u4e2a\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u300113,458\u4e2a\u6807\u6ce8\u7269\u4f53\u548c30,000+\u9a8c\u8bc1\u8fc7\u7684\u6307\u4ee3\u8868\u8fbe\u5f0f\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u7684\u7ec4\u5408\u5f0f\u7269\u4f53\u5b9a\u4f4d\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u7cbe\u5ea6\u548c\u6297\u8fd0\u52a8\u6a21\u7cca\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u591a\u6a21\u6001\u611f\u77e5\u7814\u7a76\u8f83\u5c11\uff0c\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6307\u4ee3\u8868\u8fbe\u5f0f\u5305\u542b\u5916\u89c2\u3001\u72b6\u6001\u3001\u4e0e\u89c2\u5bdf\u8005\u5173\u7cfb\u3001\u4e0e\u5468\u56f4\u7269\u4f53\u5173\u7cfb\u56db\u4e2a\u7ed3\u6784\u5316\u5c5e\u6027\uff0c\u663e\u5f0f\u6355\u6349\u65f6\u7a7a\u548c\u5173\u7cfb\u7ebf\u7d22\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b5,567\u4e2a\u573a\u666f\u300113,458\u4e2a\u6807\u6ce8\u7269\u4f53\u548c30,000+\u9a8c\u8bc1\u8868\u8fbe\u5f0f\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301\u8d85\u8d8a\u7b80\u5355\u7269\u4f53\u8bc6\u522b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "conclusion": "Talk2Event\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u548c\u65f6\u95f4\u611f\u77e5\u611f\u77e5\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5728\u673a\u5668\u4eba\u3001\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.09195", "categories": ["cs.LG", "cs.CV", "68T07, 86A10", "I.2.6; J.2"], "pdf": "https://arxiv.org/pdf/2509.09195", "abs": "https://arxiv.org/abs/2509.09195", "authors": ["Md Tanveer Hossain Munim"], "title": "Breaking the Statistical Similarity Trap in Extreme Convection Detection", "comment": "43 pages, 7 figures", "summary": "Current evaluation metrics for deep learning weather models create a\n\"Statistical Similarity Trap\", rewarding blurry predictions while missing rare,\nhigh-impact events. We provide quantitative evidence of this trap, showing\nsophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous\nconvection detection. We introduce DART (Dual Architecture for Regression\nTasks), a framework addressing the challenge of transforming coarse atmospheric\nforecasts into high-resolution satellite brightness temperature fields\noptimized for extreme convection detection (below 220 K). DART employs\ndual-decoder architecture with explicit background/extreme decomposition,\nphysically motivated oversampling, and task-specific loss functions. We present\nfour key findings: (1) empirical validation of the Statistical Similarity Trap\nacross multiple sophisticated baselines; (2) the \"IVT Paradox\", removing\nIntegrated Water Vapor Transport, widely regarded as essential for atmospheric\nriver analysis, improves extreme convection detection by 270%; (3)\narchitectural necessity demonstrated through operational flexibility (DART\nachieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent\nCSI), and (4) real-world validation with the August 2023 Chittagong flooding\ndisaster as a case study. To our knowledge, this is the first work to\nsystematically address this hybrid conversion-segmentation-downscaling task,\nwith no direct prior benchmarks identified in existing literature. Our\nvalidation against diverse statistical and deep learning baselines sufficiently\ndemonstrates DART's specialized design. The framework enables precise\noperational calibration through beta-tuning, trains in under 10 minutes on\nstandard hardware, and integrates seamlessly with existing meteorological\nworkflows, demonstrating a pathway toward trustworthy AI for extreme weather\npreparedness.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u5b58\u5728\"\u7edf\u8ba1\u76f8\u4f3c\u6027\u9677\u9631\"\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u6781\u7aef\u5bf9\u6d41\u4e8b\u4ef6\u3002DART\u6846\u67b6\u901a\u8fc7\u53cc\u89e3\u7801\u5668\u7ed3\u6784\u548c\u7269\u7406\u9a71\u52a8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u5bf9\u6d41\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u7684\u8bc4\u4f30\u6307\u6807\u5b58\u5728\"\u7edf\u8ba1\u76f8\u4f3c\u6027\u9677\u9631\"\uff0c\u5bfc\u81f4\u6a21\u578b\u8bc4\u5206\u9ad8\u4f46\u7f3a\u4e4f\u5bf9\u7a00\u7f55\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b\u3002\u9700\u8981\u4e13\u95e8\u4e3a\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u8bbe\u8ba1\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51faDART\u53cc\u89e3\u7801\u5668\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u660e\u786e\u7684\u80cc\u666f/\u6781\u7aef\u5206\u89e3\uff1b2\uff09\u7269\u7406\u9a71\u52a8\u7684\u8fc7\u91c7\u6837\u6280\u672f\uff1b3\uff09\u4efb\u52a1\u7279\u5b9a\u635f\u5931\u51fd\u6570\u3002\u8be5\u6846\u67b6\u5c06\u7c97\u7cd5\u5927\u6c14\u9884\u62a5\u8f6c\u6362\u4e3a\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u4eae\u5ea6\u6e29\u5ea6\u573a\u3002", "result": "\u56db\u9879\u5173\u952e\u53d1\u73b0\uff1a1\uff09\u5b9e\u8bc1\u7edf\u8ba1\u76f8\u4f3c\u6027\u9677\u9631\uff1b2\uff09IVT\u77db\u76fe\uff08\u79fb\u9664\u603b\u6c34\u6c14\u8fd0\u8f93\u53cd\u800c\u63d0\u5347270%\u68c0\u6d4b\u6548\u679c\uff09\uff1b3\uff09DART\u8fbe\u5230CSI=0.273\uff0c\u504f\u5dee\u4ec52.52\uff08\u57fa\u7ebf\u6a21\u578b\u4e3a6.72\uff09\uff1b4\uff092023\u5e748\u6708\u5409\u5927\u94a2\u6d2a\u6c34\u5b9e\u4f8b\u9a8c\u8bc1\u3002\u8bad\u7ec3\u65f6\u95f4\u4ec5\u970010\u5206\u949f\u3002", "conclusion": "DART\u6846\u67b6\u7cfb\u7edf\u6027\u89e3\u51b3\u4e86\u6df7\u5408\u8f6c\u6362-\u5206\u5272-\u964d\u7ef4\u4efb\u52a1\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u6781\u7aef\u5929\u6c14\u9884\u9632\u63d0\u4f9b\u4e86\u4fe1\u4efbAI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09226", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09226", "abs": "https://arxiv.org/abs/2509.09226", "authors": ["Haipeng Liu", "Ting Long", "Jing Fu"], "title": "Constructing a Question-Answering Simulator through the Distillation of LLMs", "comment": null, "summary": "The question-answering (QA) simulator is a model that mimics real student\nlearning behaviors and predicts their correctness of their responses to\nquestions. QA simulators enable educational recommender systems (ERS) to\ncollect large amounts of training data without interacting with real students,\nthereby preventing harmful recommendations made by an undertrained ERS from\nundermining actual student learning. Given the QA history, there are two\ncategories of solutions to predict the correctness, conducting the simulation:\n(1) LLM-free methods, which apply a traditional sequential model to transfer\nthe QA history into a vector representation first, and make predictions based\non the representation; (2) LLM-based methods, which leverage the domain\nknowledge and reasoning capability of LLM to enhence the prediction. LLM-free\nmethods offer fast inference but generally yield suboptimal performance. In\ncontrast, most LLM-based methods achieve better results, but at the cost of\nslower inference speed and higher GPU memory consumption. In this paper, we\npropose a method named LLM Distillation based Simulator (LDSim), which distills\ndomain knowledge and reasoning capability from an LLM to better assist\nprediction, thereby improving simulation performance. Extensive experiments\ndemonstrate that our LDSim achieves strong results on both the simulation task\nand the knowledge tracing (KT) task. Our code is publicly available at\nhttps://anonymous.4open.science/r/LDSim-05A9.", "AI": {"tldr": "\u63d0\u51faLDSim\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eceLLM\u84b8\u998f\u9886\u57df\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u63d0\u5347QA\u6a21\u62df\u5668\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u9884\u6d4b\u6548\u679c", "motivation": "\u89e3\u51b3\u73b0\u6709QA\u6a21\u62df\u5668\u4e2dLLM-free\u65b9\u6cd5\u6027\u80fd\u4e0d\u4f73\u3001LLM-based\u65b9\u6cd5\u63a8\u7406\u901f\u5ea6\u6162\u548cGPU\u5185\u5b58\u6d88\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u627e\u5230\u5e73\u8861", "method": "\u63d0\u51faLLM\u84b8\u998f\u6a21\u62df\u5668(LDSim)\uff0c\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u9886\u57df\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u8f85\u52a9\u9884\u6d4b\uff0c\u63d0\u5347\u6a21\u62df\u6027\u80fd", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eLDSim\u5728\u6a21\u62df\u4efb\u52a1\u548c\u77e5\u8bc6\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u7ed3\u679c", "conclusion": "LDSim\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u5e73\u8861\u4e86QA\u6a21\u62df\u5668\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6a21\u62df\u6570\u636e\u751f\u6210\u65b9\u6848"}}
{"id": "2509.09172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09172", "abs": "https://arxiv.org/abs/2509.09172", "authors": ["Chunxiao Li", "Xiaoxiao Wang", "Meiling Li", "Boming Miao", "Peng Sun", "Yunjian Zhang", "Xiangyang Ji", "Yao Zhu"], "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios", "comment": "ICCV2025", "summary": "With the rapid advancement of generative models, highly realistic image\nsynthesis has posed new challenges to digital security and media credibility.\nAlthough AI-generated image detection methods have partially addressed these\nconcerns, a substantial research gap remains in evaluating their performance\nunder complex real-world conditions. This paper introduces the Real-World\nRobustness Dataset (RRDataset) for comprehensive evaluation of detection models\nacross three dimensions: 1) Scenario Generalization: RRDataset encompasses\nhigh-quality images from seven major scenarios (War and Conflict, Disasters and\nAccidents, Political and Social Events, Medical and Public Health, Culture and\nReligion, Labor and Production, and everyday life), addressing existing dataset\ngaps from a content perspective. 2) Internet Transmission Robustness: examining\ndetector performance on images that have undergone multiple rounds of sharing\nacross various social media platforms. 3) Re-digitization Robustness: assessing\nmodel effectiveness on images altered through four distinct re-digitization\nmethods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on\nRRDataset and conducted a large-scale human study involving 192 participants to\ninvestigate human few-shot learning capabilities in detecting AI-generated\nimages. The benchmarking results reveal the limitations of current AI detection\nmethods under real-world conditions and underscore the importance of drawing on\nhuman adaptability to develop more robust detection algorithms.", "AI": {"tldr": "RRDataset\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d67\u4e2a\u573a\u666f\u3001\u7f51\u7edc\u4f20\u8f93\u9c81\u68d2\u6027\u548c\u91cd\u6570\u5b57\u5316\u9c81\u68d2\u6027\u4e09\u4e2a\u7ef4\u5ea6\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u501f\u9274\u4eba\u7c7b\u9002\u5e94\u80fd\u529b\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u7b97\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9ad8\u5ea6\u903c\u771f\u7684\u56fe\u50cf\u5408\u6210\u5bf9\u6570\u5b57\u5b89\u5168\u548c\u5a92\u4f53\u53ef\u4fe1\u5ea6\u63d0\u51fa\u4e86\u65b0\u6311\u6218\u3002\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faReal-World Robustness Dataset (RRDataset)\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\uff1a1)\u573a\u666f\u6cdb\u5316-7\u4e2a\u4e3b\u8981\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff1b2)\u7f51\u7edc\u4f20\u8f93\u9c81\u68d2\u6027-\u793e\u4ea4\u5a92\u4f53\u591a\u6b21\u5206\u4eab\u540e\u7684\u56fe\u50cf\uff1b3)\u91cd\u6570\u5b57\u5316\u9c81\u68d2\u6027-4\u79cd\u4e0d\u540c\u91cd\u6570\u5b57\u5316\u65b9\u6cd5\u5904\u7406\u7684\u56fe\u50cf\u3002\u5bf917\u4e2a\u68c0\u6d4b\u5668\u548c10\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c192\u4eba\u53c2\u4e0e\u7684\u5927\u89c4\u6a21\u4eba\u7c7b\u7814\u7a76\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dAI\u68c0\u6d4b\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u501f\u9274\u4eba\u7c7b\u9002\u5e94\u80fd\u529b\u5f00\u53d1\u66f4\u9c81\u68d2\u68c0\u6d4b\u7b97\u6cd5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6761\u4ef6\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4eba\u7c7b\u5728\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u9762\u7684\u80fd\u529b\u4e3a\u7b97\u6cd5\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.09242", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09242", "abs": "https://arxiv.org/abs/2509.09242", "authors": ["Mustafa Yurdakul", "Sakir Tasdemir"], "title": "CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification", "comment": null, "summary": "Background and objective Early diagnosis of gastric diseases is crucial to\nprevent fatal outcomes. Although histopathologic examination remains the\ndiagnostic gold standard, it is performed entirely manually, making evaluations\nlabor-intensive and prone to variability among pathologists. Critical findings\nmay be missed, and lack of standard procedures reduces consistency. These\nlimitations highlight the need for automated, reliable, and efficient methods\nfor gastric tissue analysis. Methods In this study, a novel hybrid model named\nCoAtNeXt was proposed for the classification of gastric tissue images. The\nmodel is built upon the CoAtNet architecture by replacing its MBConv layers\nwith enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block\nAttention Module (CBAM) is integrated to improve local feature extraction\nthrough channel and spatial attention mechanisms. The architecture was scaled\nto achieve a balance between computational efficiency and classification\nperformance. CoAtNeXt was evaluated on two publicly available datasets,\nHMU-GC-HE-30K for eight-class classification and GasHisSDB for binary\nclassification, and was compared against 10 Convolutional Neural Networks\n(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved\n96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%\nAUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%\nprecision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all\nCNN and ViT models tested and surpassed previous studies in the literature.\nConclusion Experimental results show that CoAtNeXt is a robust architecture for\nhistopathological classification of gastric tissue images, providing\nperformance on binary and multiclass. Its highlights its potential to assist\npathologists by enhancing diagnostic accuracy and reducing workload.", "AI": {"tldr": "\u63d0\u51faCoAtNeXt\u6df7\u5408\u6a21\u578b\u7528\u4e8e\u80c3\u7ec4\u7ec7\u56fe\u50cf\u5206\u7c7b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709CNN\u548cViT\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u5f02", "motivation": "\u80c3\u75c5\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7ec4\u7ec7\u75c5\u7406\u5b66\u68c0\u67e5\u5b8c\u5168\u624b\u52a8\u64cd\u4f5c\uff0c\u5de5\u4f5c\u91cf\u5927\u4e14\u5b58\u5728\u8bca\u65ad\u5dee\u5f02\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u53ef\u9760\u7684\u80c3\u7ec4\u7ec7\u5206\u6790\u65b9\u6cd5", "method": "\u57fa\u4e8eCoAtNet\u67b6\u6784\uff0c\u7528\u589e\u5f3a\u7684ConvNeXtV2\u5757\u66ff\u6362MBConv\u5c42\uff0c\u96c6\u6210CBAM\u6ce8\u610f\u529b\u673a\u5236\u6539\u5584\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u5206\u7c7b\u6027\u80fd", "result": "\u5728HMU-GC-HE-30K\u516b\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.47%\u51c6\u786e\u7387\uff0c\u5728GasHisSDB\u4e8c\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.29%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u6240\u6709\u6d4b\u8bd5\u7684CNN\u548cViT\u6a21\u578b", "conclusion": "CoAtNeXt\u662f\u80c3\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u9c81\u68d2\u67b6\u6784\uff0c\u5177\u6709\u534f\u52a9\u75c5\u7406\u5b66\u5bb6\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u51cf\u5c11\u5de5\u4f5c\u8d1f\u8377\u7684\u6f5c\u529b"}}
{"id": "2509.09263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09263", "abs": "https://arxiv.org/abs/2509.09263", "authors": ["Chao Yuan", "Yang Yang", "Yehui Yang", "Zach Cheng"], "title": "DATE: Dynamic Absolute Time Enhancement for Long Video Understanding", "comment": null, "summary": "Long video understanding remains a fundamental challenge for multimodal large\nlanguage models (MLLMs), particularly in tasks requiring precise temporal\nreasoning and event localization. Existing approaches typically adopt uniform\nframe sampling and rely on implicit position encodings to model temporal order.\nHowever, these methods struggle with long-range dependencies, leading to\ncritical information loss and degraded temporal comprehension. In this paper,\nwe propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal\nawareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a\nsemantically guided Temporal-Aware Similarity Sampling (TASS) strategy.\nSpecifically, we interleave video frame embeddings with textual timestamp\ntokens to construct a continuous temporal reference system. We further\nreformulate the video sampling problem as a vision-language retrieval task and\nintroduce a two-stage algorithm to ensure both semantic relevance and temporal\ncoverage: enriching each query into a descriptive caption to better align with\nthe vision feature, and sampling key event with a similarity-driven temporally\nregularized greedy strategy. Our method achieves remarkable improvements w.r.t.\nabsolute time understanding and key event localization, resulting in\nstate-of-the-art performance among 7B and 72B models on hour-long video\nbenchmarks. Particularly, our 7B model even exceeds many 72B models on some\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86DATE\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u6233\u6ce8\u5165\u673a\u5236\u548c\u65f6\u5e8f\u611f\u77e5\u76f8\u4f3c\u6027\u91c7\u6837\u7b56\u7565\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u89c6\u9891\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u57287B\u548c72B\u6a21\u578b\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u5747\u5300\u5e27\u91c7\u6837\u548c\u9690\u5f0f\u4f4d\u7f6e\u7f16\u7801\uff0c\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u548c\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u4e0b\u964d", "method": "DATE\u65b9\u6cd5\u5305\u542b\u65f6\u95f4\u6233\u6ce8\u5165\u673a\u5236(TIM)\u548c\u65f6\u5e8f\u611f\u77e5\u76f8\u4f3c\u6027\u91c7\u6837\u7b56\u7565(TASS)\uff1a1)\u5728\u89c6\u9891\u5e27\u5d4c\u5165\u4e2d\u63d2\u5165\u6587\u672c\u65f6\u95f4\u6233\u6807\u8bb0\u6784\u5efa\u8fde\u7eed\u65f6\u5e8f\u53c2\u8003\u7cfb\u7edf\uff1b2)\u5c06\u89c6\u9891\u91c7\u6837\u91cd\u6784\u4e3a\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7b97\u6cd5\u786e\u4fdd\u8bed\u4e49\u76f8\u5173\u6027\u548c\u65f6\u5e8f\u8986\u76d6", "result": "\u5728\u5c0f\u65f6\u7ea7\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c7B\u6a21\u578b\u5728\u67d0\u4e9b\u57fa\u51c6\u4e0a\u751a\u81f3\u8d85\u8d8a\u4e86\u8bb8\u591a72B\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7edd\u5bf9\u65f6\u95f4\u7406\u89e3\u548c\u5173\u952e\u4e8b\u4ef6\u5b9a\u4f4d\u6027\u80fd", "conclusion": "DATE\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u7684\u65f6\u95f4\u5efa\u6a21\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u91c7\u6837\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u65f6\u5e8f\u63a8\u7406\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u89c6\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09286", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09286", "abs": "https://arxiv.org/abs/2509.09286", "authors": ["Bohao Tang", "Yan Ma", "Fei Zhang", "Jiadi Su", "Ethan Chern", "Zhulin Hu", "Zhixin Wang", "Pengfei Liu", "Ya Zhang"], "title": "Visual Programmability: A Guide for Code-as-Thought in Chart Understanding", "comment": null, "summary": "Chart understanding presents a critical test to the reasoning capabilities of\nVision-Language Models (VLMs). Prior approaches face critical limitations: some\nrely on external tools, making them brittle and constrained by a predefined\ntoolkit, while others fine-tune specialist models that often adopt a single\nreasoning strategy, such as text-based chain-of-thought (CoT). The intermediate\nsteps of text-based reasoning are difficult to verify, which complicates the\nuse of reinforcement-learning signals that reward factual accuracy. To address\nthis, we propose a Code-as-Thought (CaT) approach to represent the visual\ninformation of a chart in a verifiable, symbolic format. Our key insight is\nthat this strategy must be adaptive: a fixed, code-only implementation\nconsistently fails on complex charts where symbolic representation is\nunsuitable. This finding leads us to introduce Visual Programmability: a\nlearnable property that determines if a chart-question pair is better solved\nwith code or direct visual analysis. We implement this concept in an adaptive\nframework where a VLM learns to choose between the CaT pathway and a direct\nvisual reasoning pathway. The selection policy of the model is trained with\nreinforcement learning using a novel dual-reward system. This system combines a\ndata-accuracy reward to ground the model in facts and prevent numerical\nhallucination, with a decision reward that teaches the model when to use each\nstrategy, preventing it from defaulting to a single reasoning mode. Experiments\ndemonstrate strong and robust performance across diverse chart-understanding\nbenchmarks. Our work shows that VLMs can be taught not only to reason but also\nhow to reason, dynamically selecting the optimal reasoning pathway for each\ntask.", "AI": {"tldr": "\u901a\u8fc7\u4ee3\u7801\u4f5c\u4e3a\u601d\u7ef4(CaT)\u65b9\u6cd5\uff0c\u8ba9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684\u63a8\u7406\u8def\u5f84\uff08\u4ee3\u7801\u8868\u793a\u6216\u76f4\u63a5\u89c6\u89c9\u5206\u6790\uff09\u6765\u63d0\u5347\u56fe\u8868\u7406\u89e3\u80fd\u529b", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u8868\u7406\u89e3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff1a\u5916\u90e8\u5de5\u5177\u4f9d\u8d56\u6027\u5f3a\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u4e13\u4e1a\u6a21\u578b\u53ea\u91c7\u7528\u5355\u4e00\u7684\u6587\u672c\u94fe\u5f0f\u601d\u7ef4\u7b56\u7565\uff0c\u4e2d\u95f4\u6b65\u9aa4\u96be\u4ee5\u9a8c\u8bc1", "method": "\u63d0\u51fa\u4ee3\u7801\u4f5c\u4e3a\u601d\u7ef4(CaT)\u65b9\u6cd5\uff0c\u5c06\u56fe\u8868\u89c6\u89c9\u4fe1\u606f\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u7684\u7b26\u53f7\u683c\u5f0f\uff1b\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u89c6\u89c9\u53ef\u7f16\u7a0b\u6027\u6982\u5ff5\uff0c\u8ba9\u6a21\u578b\u52a8\u6001\u9009\u62e9\u4ee3\u7801\u8def\u5f84\u6216\u76f4\u63a5\u89c6\u89c9\u63a8\u7406\u8def\u5f84\uff1b\u4f7f\u7528\u53cc\u91cd\u5956\u52b1\u7cfb\u7edf\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60", "result": "\u5728\u591a\u6837\u5316\u7684\u56fe\u8868\u7406\u89e3\u6d4b\u8bd5\u96c6\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u4e14\u7a33\u5065\u7684\u6027\u80fd", "conclusion": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u5b66\u4f1a\u5982\u4f55\u63a8\u7406\uff0c\u8fd8\u53ef\u4ee5\u5b66\u4f1a\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684\u63a8\u7406\u65b9\u5f0f\uff0c\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u7684\u89e3\u51b3\u7b56\u7565"}}
{"id": "2509.09512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09512", "abs": "https://arxiv.org/abs/2509.09512", "authors": ["Cynthia Moreira Maia", "Lucas B. V. de Amorim", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "PIPES: A Meta-dataset of Machine Learning Pipelines", "comment": null, "summary": "Solutions to the Algorithm Selection Problem (ASP) in machine learning face\nthe challenge of high computational costs associated with evaluating various\nalgorithms' performances on a given dataset. To mitigate this cost, the\nmeta-learning field can leverage previously executed experiments shared in\nonline repositories such as OpenML. OpenML provides an extensive collection of\nmachine learning experiments. However, an analysis of OpenML's records reveals\nlimitations. It lacks diversity in pipelines, specifically when exploring data\npreprocessing steps/blocks, such as scaling or imputation, resulting in limited\nrepresentation. Its experiments are often focused on a few popular techniques\nwithin each pipeline block, leading to an imbalanced sample. To overcome the\nobserved limitations of OpenML, we propose PIPES, a collection of experiments\ninvolving multiple pipelines designed to represent all combinations of the\nselected sets of techniques, aiming at diversity and completeness. PIPES stores\nthe results of experiments performed applying 9,408 pipelines to 300 datasets.\nIt includes detailed information on the pipeline blocks, training and testing\ntimes, predictions, performances, and the eventual error messages. This\ncomprehensive collection of results allows researchers to perform analyses\nacross diverse and representative pipelines and datasets. PIPES also offers\npotential for expansion, as additional data and experiments can be incorporated\nto support the meta-learning community further. The data, code, supplementary\nmaterial, and all experiments can be found at\nhttps://github.com/cynthiamaia/PIPES.git.", "AI": {"tldr": "PIPES\u662f\u4e00\u4e2a\u89e3\u51b3\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548cOpenML\u6570\u636e\u5c40\u9650\u6027\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u5305\u542b9,408\u4e2a\u7ba1\u9053\u5728300\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u5b9e\u9a8c\u7ed3\u679c", "motivation": "OpenML\u7b49\u5728\u7ebf\u5b58\u50a8\u5e93\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u8bb0\u5f55\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7ba1\u9053\u591a\u6837\u6027\u4e0d\u8db3\u3001\u6570\u636e\u9884\u5904\u7406\u6b65\u9aa4\u63a2\u7d22\u6709\u9650\u3001\u6837\u672c\u4e0d\u5e73\u8861\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5143\u5b66\u4e60\u9700\u6c42", "method": "\u6784\u5efaPIPES\u5b9e\u9a8c\u96c6\u5408\uff0c\u8bbe\u8ba1\u4ee3\u8868\u6240\u6709\u6280\u672f\u7ec4\u5408\u7684\u591a\u6837\u5316\u7ba1\u9053\uff0c\u5728300\u4e2a\u6570\u636e\u96c6\u4e0a\u6267\u884c9,408\u4e2a\u7ba1\u9053\u7684\u5b9e\u9a8c\uff0c\u8bb0\u5f55\u8be6\u7ec6\u7684\u7ba1\u9053\u5757\u4fe1\u606f\u3001\u8bad\u7ec3\u6d4b\u8bd5\u65f6\u95f4\u3001\u9884\u6d4b\u7ed3\u679c\u3001\u6027\u80fd\u6307\u6807\u548c\u9519\u8bef\u4fe1\u606f", "result": "\u521b\u5efa\u4e86\u5305\u542b\u5168\u9762\u5b9e\u9a8c\u7ed3\u679c\u7684PIPES\u6570\u636e\u96c6\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u591a\u6837\u5316\u548c\u4ee3\u8868\u6027\u7ba1\u9053\u7684\u5206\u6790\uff0c\u4e3a\u5143\u5b66\u4e60\u793e\u533a\u63d0\u4f9b\u6269\u5c55\u6f5c\u529b", "conclusion": "PIPES\u514b\u670d\u4e86OpenML\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u52a0\u591a\u6837\u548c\u5b8c\u6574\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\u5408\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\u548c\u5143\u5b66\u4e60\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55"}}
{"id": "2509.09599", "categories": ["cs.LG", "math.DS", "nlin.CD", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.09599", "abs": "https://arxiv.org/abs/2509.09599", "authors": ["Ira J. S. Shokar", "Rich R. Kerswell", "Peter H. Haynes"], "title": "Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics", "comment": null, "summary": "We present a deep learning emulator for stochastic and chaotic\nspatio-temporal systems, explicitly conditioned on the parameter values of the\nunderlying partial differential equations (PDEs). Our approach involves\npre-training the model on a single parameter domain, followed by fine-tuning on\na smaller, yet diverse dataset, enabling generalisation across a broad range of\nparameter values. By incorporating local attention mechanisms, the network is\ncapable of handling varying domain sizes and resolutions. This enables\ncomputationally efficient pre-training on smaller domains while requiring only\na small additional dataset to learn how to generalise to larger domain sizes.\nWe demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky\nequation and stochastically-forced beta-plane turbulence, showcasing its\nability to capture phenomena at interpolated parameter values. The emulator\nprovides significant computational speed-ups over conventional numerical\nintegration, facilitating efficient exploration of parameter space, while a\nprobabilistic variant of the emulator provides uncertainty quantification,\nallowing for the statistical study of rare events.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u968f\u673a\u548c\u6df7\u6c8c\u65f6\u7a7a\u7cfb\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u4eff\u771f\u5668\uff0c\u80fd\u591f\u6839\u636ePDE\u53c2\u6570\u503c\u8fdb\u884c\u6761\u4ef6\u5316\u5efa\u6a21\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5b9e\u73b0\u53c2\u6570\u7a7a\u95f4\u6cdb\u5316\uff0c\u5e76\u63d0\u4f9b\u8ba1\u7b97\u52a0\u901f\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u79ef\u5206\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u9ad8\u6548\u63a2\u7d22\u53c2\u6570\u7a7a\u95f4\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u4e0d\u540c\u53c2\u6570\u503c\u3001\u57df\u5927\u5c0f\u548c\u5206\u8fa8\u7387\u7684\u901a\u7528\u4eff\u771f\u5668\uff0c\u540c\u65f6\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u7814\u7a76\u7f55\u89c1\u4e8b\u4ef6\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3+\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u5355\u4e00\u53c2\u6570\u57df\u9884\u8bad\u7ec3\u540e\uff0c\u4f7f\u7528\u591a\u6837\u5316\u5c0f\u6570\u636e\u96c6\u5fae\u8c03\u4ee5\u5b9e\u73b0\u53c2\u6570\u6cdb\u5316\u3002\u5f15\u5165\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u53ef\u53d8\u57df\u5927\u5c0f\u548c\u5206\u8fa8\u7387\uff0c\u652f\u6301\u4ece\u5c0f\u57df\u9884\u8bad\u7ec3\u6269\u5c55\u5230\u66f4\u5927\u57df\u3002", "result": "\u5728\u6df7\u6c8cKuramoto-Sivashinsky\u65b9\u7a0b\u548c\u968f\u673a\u5f3a\u8febbeta-plane\u6e4d\u6d41\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u80fd\u591f\u6355\u6349\u63d2\u503c\u53c2\u6570\u4e0b\u7684\u73b0\u8c61\uff0c\u76f8\u6bd4\u4f20\u7edf\u6570\u503c\u79ef\u5206\u63d0\u4f9b\u663e\u8457\u8ba1\u7b97\u52a0\u901f\uff0c\u6982\u7387\u53d8\u4f53\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u53c2\u6570\u6761\u4ef6\u5316\u7684\u65f6\u7a7a\u7cfb\u7edf\u4eff\u771f\uff0c\u63d0\u4f9b\u9ad8\u6548\u53c2\u6570\u7a7a\u95f4\u63a2\u7d22\u80fd\u529b\uff0c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\u652f\u6301\u7075\u6d3b\u57df\u5904\u7406\uff0c\u6982\u7387\u7248\u672c\u652f\u6301\u7edf\u8ba1\u7814\u7a76\u548c\u7f55\u89c1\u4e8b\u4ef6\u5206\u6790\u3002"}}
{"id": "2509.09429", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09429", "abs": "https://arxiv.org/abs/2509.09429", "authors": ["Peisong Wen", "Qianqian Xu", "Siran Dai", "Runmin Cong", "Qingming Huang"], "title": "Semantic Concentration for Self-Supervised Dense Representations Learning", "comment": null, "summary": "Recent advances in image-level self-supervised learning (SSL) have made\nsignificant progress, yet learning dense representations for patches remains\nchallenging. Mainstream methods encounter an over-dispersion phenomenon that\npatches from the same instance/category scatter, harming downstream performance\non dense tasks. This work reveals that image-level SSL avoids over-dispersion\nby involving implicit semantic concentration. Specifically, the non-strict\nspatial alignment ensures intra-instance consistency, while shared patterns,\ni.e., similar parts of within-class instances in the input space, ensure\ninter-image consistency. Unfortunately, these approaches are infeasible for\ndense SSL due to their spatial sensitivity and complicated scene-centric data.\nThese observations motivate us to explore explicit semantic concentration for\ndense SSL. First, to break the strict spatial alignment, we propose to distill\nthe patch correspondences. Facing noisy and imbalanced pseudo labels, we\npropose a noise-tolerant ranking loss. The core idea is extending the Average\nPrecision (AP) loss to continuous targets, such that its decision-agnostic and\nadaptive focusing properties prevent the student model from being misled.\nSecond, to discriminate the shared patterns from complicated scenes, we propose\nthe object-aware filter to map the output space to an object-based space.\nSpecifically, patches are represented by learnable prototypes of objects via\ncross-attention. Last but not least, empirical studies across various tasks\nsoundly support the effectiveness of our method. Code is available in\nhttps://github.com/KID-7391/CoTAP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5bc6\u96c6\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u663e\u5f0f\u8bed\u4e49\u96c6\u4e2d\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u7a7a\u95f4\u5bf9\u9f50\u548c\u5bf9\u8c61\u611f\u77e5\u8fc7\u6ee4\u6765\u89e3\u51b3\u8865\u4e01\u7ea7\u8868\u793a\u4e2d\u7684\u8fc7\u5ea6\u5206\u6563\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u7ea7\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5bc6\u96c6\u4efb\u52a1\u4e2d\u5b58\u5728\u8865\u4e01\u8fc7\u5ea6\u5206\u6563\u95ee\u9898\uff0c\u5373\u6765\u81ea\u540c\u4e00\u5b9e\u4f8b/\u7c7b\u522b\u7684\u8865\u4e01\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5206\u6563\uff0c\u5f71\u54cd\u4e0b\u6e38\u5bc6\u96c6\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "1) \u901a\u8fc7\u84b8\u998f\u8865\u4e01\u5bf9\u5e94\u5173\u7cfb\u6253\u7834\u4e25\u683c\u7a7a\u95f4\u5bf9\u9f50\uff1b2) \u63d0\u51fa\u566a\u58f0\u5bb9\u5fcd\u6392\u5e8f\u635f\u5931\u5904\u7406\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u4f2a\u6807\u7b7e\uff1b3) \u8bbe\u8ba1\u5bf9\u8c61\u611f\u77e5\u8fc7\u6ee4\u5668\u5c06\u8f93\u51fa\u7a7a\u95f4\u6620\u5c04\u5230\u57fa\u4e8e\u5bf9\u8c61\u7684\u7a7a\u95f4\u3002", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u5145\u5206\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u8bed\u4e49\u96c6\u4e2d\u6210\u529f\u89e3\u51b3\u4e86\u5bc6\u96c6\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8fc7\u5ea6\u5206\u6563\u95ee\u9898\uff0c\u4e3a\u5bc6\u96c6\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
