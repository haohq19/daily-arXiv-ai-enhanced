{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u4e2d\u6587\u6587\u732e\u8bed\u6cd5\u7ea0\u9519\u6301\u7eed\u5b66\u4e60\u57fa\u51c6CL\u00b2GEC\uff0c\u5305\u542b10\u4e2a\u5b66\u79d1\u768410,000\u53e5\u6807\u6ce8\u6570\u636e\uff0c\u8bc4\u4f30LLM\u5728\u8de8\u5b66\u79d1\u8bed\u6cd5\u7ea0\u9519\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u7f3a\u4e4f\u591a\u5b66\u79d1\u5b66\u672f\u5199\u4f5c\u7684\u4e13\u95e8\u57fa\u51c6\uff0c\u5ffd\u89c6\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53d8\u5f02\u548c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b10\u4e2a\u5b66\u79d110,000\u53e5\u6807\u6ce8\u6570\u636e\u7684\u57fa\u51c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u8c03\u4f18\u3001\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u548c\u56db\u79cd\u4ee3\u8868\u6027\u6301\u7eed\u5b66\u4e60\u7b97\u6cd5\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u91cd\u653e\u6216\u7b80\u5355\u987a\u5e8f\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8de8\u5b66\u79d1\u5b66\u672f\u9886\u57df\u7684\u81ea\u9002\u5e94\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2509.13339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.", "AI": {"tldr": "\u8be5\u4f4d\u7f6e\u6587\u7ae0\u4e3b\u5f20\u91c7\u7528\u53cd\u810f\u8109\u89c6\u89d2\u6765\u5b9e\u73b0AI\u5b89\u5168\u6027\uff0c\u8ba9\u7cfb\u7edf\u5728\u5e94\u5bf9\u7a00\u7f55\u548c\u5206\u5e03\u5916\u4e8b\u4ef6\u65f6\u80fd\u529b\u6301\u7eed\u589e\u957f\uff0c\u800c\u975e\u4f9d\u8d56\u9759\u6001\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u6d4b\u8bd5\u548c\u5355\u6b21\u7c97\u7cd6\u6027\u6d4b\u8bd5\u5ffd\u89c6\u4e86\u73af\u5883\u6f14\u5316\u548c\u6a21\u578b\u6f0f\u6d1e\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u5982\u5956\u52b1\u7a83\u53d6\u3001\u8fc7\u5ea6\u4f18\u5316\u7b49\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9002\u5e94\u957f\u671f\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63a8\u8350\u53cd\u810f\u8109\u65b9\u6cd5\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6765\u9884\u9632\u672a\u6765\u66f4\u5927\u7684\u4e0d\u53ef\u9884\u6d4b\u98ce\u9669\uff0c\u91cd\u65b0\u8c03\u6574AI\u5b89\u5168\u6027\u7684\u6d4b\u91cf\u3001\u6807\u51c6\u5316\u548c\u6301\u7eed\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u5efa\u7acb\u53cd\u810f\u8109AI\u5b89\u5168\u793e\u533a\u7684\u4f26\u7406\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u8865\u5145\u73b0\u6709\u7c97\u7cd6\u6027\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "conclusion": "\u53cd\u810f\u8109\u65b9\u6cd5\u5bf9\u4e8e\u5f00\u653e\u5f0f\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u957f\u671f\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u662f\u5b9e\u73b0AI\u5b89\u5168\u6027\u6301\u7eed\u6539\u8fdb\u7684\u5173\u952e\u3002"}}
{"id": "2509.13386", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13386", "abs": "https://arxiv.org/abs/2509.13386", "authors": ["Hansol Lim", "Minhyeok Im", "Jonathan Boyack", "Jee Won Lee", "Jongseong Brad Choi"], "title": "VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization", "comment": "This work has been submitted to the 2026 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication", "summary": "Demands for software-defined vehicles (SDV) are rising and electric vehicles\n(EVs) are increasingly being equipped with powerful computers. This enables\nonboard AI systems to optimize charge-aware path optimization customized to\nreflect vehicle's current condition and environment. We present VEGA, a\ncharge-aware EV navigation agent that plans over a charger-annotated road graph\nusing Proximal Policy Optimization (PPO) with budgeted A* teacher-student\nguidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.\nFirst, a physics-informed neural operator (PINO), trained on real vehicle speed\nand battery-power logs, uses recent vehicle speed logs to estimate aerodynamic\ndrag, rolling resistance, mass, motor and regenerative-braking efficiencies,\nand auxiliary load by learning a vehicle-custom dynamics. Second, a\nReinforcement Learning (RL) agent uses these dynamics to optimize a path with\noptimal charging stops and dwell times under SoC constraints. VEGA requires no\nadditional sensors and uses only vehicle speed signals. It may serve as a\nvirtual sensor for power and efficiency to potentially reduce EV cost. In\nevaluation on long routes like San Francisco to New York, VEGA's stops, dwell\ntimes, SoC management, and total travel time closely track Tesla Trip Planner\nwhile being slightly more conservative, presumably due to real vehicle\nconditions such as vehicle parameter drift due to deterioration. Although\ntrained only in U.S. regions, VEGA was able to compute optimal charge-aware\npaths in France and Japan, demonstrating generalizability. It achieves\npractical integration of physics-informed learning and RL for EV eco-routing.", "AI": {"tldr": "VEGA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u611f\u77e5\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548cPPO\u7b97\u6cd5\u4f18\u5316\u8def\u5f84\u89c4\u5212\u548c\u5145\u7535\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u4e2a\u6027\u5316\u80fd\u6548\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\u9700\u6c42\u589e\u957f\u548c\u7535\u52a8\u6c7d\u8f66\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6839\u636e\u8f66\u8f86\u5f53\u524d\u72b6\u6001\u548c\u73af\u5883\u8fdb\u884c\u5145\u7535\u611f\u77e5\u8def\u5f84\u4f18\u5316\u7684\u8f66\u8f7dAI\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u4e24\u6a21\u5757\u8bbe\u8ba1\uff1a1)\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50(PINO)\u4ece\u8f66\u901f\u65e5\u5fd7\u5b66\u4e60\u8f66\u8f86\u5b9a\u5236\u5316\u52a8\u529b\u5b66\u53c2\u6570\uff1b2)\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4f7f\u7528PPO\u7b97\u6cd5\u5728\u5145\u7535\u7ad9\u6807\u6ce8\u7684\u9053\u8def\u56fe\u4e0a\u4f18\u5316\u8def\u5f84\u548c\u5145\u7535\u7b56\u7565\u3002", "result": "\u5728\u957f\u8ddd\u79bb\u8def\u7ebf(\u5982\u65e7\u91d1\u5c71\u5230\u7ebd\u7ea6)\u4e0a\uff0cVEGA\u7684\u505c\u8f66\u70b9\u3001\u505c\u7559\u65f6\u95f4\u3001\u7535\u91cf\u7ba1\u7406\u548c\u603b\u65c5\u884c\u65f6\u95f4\u4e0e\u7279\u65af\u62c9\u884c\u7a0b\u89c4\u5212\u5668\u63a5\u8fd1\u4f46\u66f4\u4fdd\u5b88\uff0c\u4e14\u5728\u6cd5\u56fd\u548c\u65e5\u672c\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VEGA\u6210\u529f\u5b9e\u73b0\u4e86\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u7528\u96c6\u6210\uff0c\u4e3a\u7535\u52a8\u6c7d\u8f66\u751f\u6001\u8def\u7531\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4f5c\u4e3a\u865a\u62df\u4f20\u611f\u5668\u964d\u4f4eEV\u6210\u672c\u3002"}}
{"id": "2509.13621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13621", "abs": "https://arxiv.org/abs/2509.13621", "authors": ["Antonin Sulc", "Thorsten Hellert", "Steven Hunt"], "title": "Unsupervised Anomaly Detection in ALS EPICS Event Logs", "comment": "6 pages, 5 figures, The 20th International Conference on Accelerator\n  and Large Experimental Physics Control Systems", "summary": "This paper introduces an automated fault analysis framework for the Advanced\nLight Source (ALS) that processes real-time event logs from its EPICS control\nsystem. By treating log entries as natural language, we transform them into\ncontextual vector representations using semantic embedding techniques. A\nsequence-aware neural network, trained on normal operational data, assigns a\nreal-time anomaly score to each event. This method flags deviations from\nbaseline behavior, enabling operators to rapidly identify the critical event\nsequences that precede complex system failures.", "AI": {"tldr": "\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5165\u548c\u5e8f\u5217\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u7684ALS\u6545\u969c\u81ea\u52a8\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u4e8b\u4ef6\u65e5\u5fd7\u5904\u7406\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b", "motivation": "\u4e3a\u5148\u8fdb\u5149\u6e90(ALS)\u63a7\u5236\u7cfb\u7edf\u5f00\u53d1\u81ea\u52a8\u5316\u6545\u969c\u5206\u6790\u65b9\u6cd5\uff0c\u5e2e\u52a9\u64cd\u4f5c\u5458\u5feb\u901f\u8bc6\u522b\u5bfc\u81f4\u590d\u6742\u7cfb\u7edf\u6545\u969c\u7684\u5173\u952e\u4e8b\u4ef6\u5e8f\u5217", "method": "\u5c06EPICS\u63a7\u5236\u7cfb\u7edf\u7684\u4e8b\u4ef6\u65e5\u5fd7\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u6280\u672f\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf\u8868\u793a\uff0c\u7528\u5e8f\u5217\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u5bf9\u6b63\u5e38\u64cd\u4f5c\u6570\u636e\u8bad\u7ec3\u5e76\u5b9e\u65f6\u8ba1\u7b97\u5f02\u5e38\u5206\u6570", "result": "\u80fd\u591f\u6807\u8bb0\u4e0e\u57fa\u7ebf\u884c\u4e3a\u7684\u504f\u5dee\uff0c\u5b9e\u73b0\u5bf9\u7cfb\u7edf\u6545\u969c\u524d\u5173\u952e\u4e8b\u4ef6\u5e8f\u5217\u7684\u5feb\u901f\u8bc6\u522b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u578b\u79d1\u5b66\u8bbe\u65bd\u7684\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u6545\u969c\u5206\u6790\u548c\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13666", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13666", "abs": "https://arxiv.org/abs/2509.13666", "authors": ["Zhenqi Wu", "Abhinav Modi", "Angelos Mavrogiannis", "Kaustubh Joshi", "Nikhil Chopra", "Yiannis Aloimonos", "Nare Karapetyan", "Ioannis Rekleitis", "Xiaomin Lin"], "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring", "comment": "submitted to ICRA 2026", "summary": "The ocean is warming and acidifying, increasing the risk of mass mortality\nevents for temperature-sensitive shellfish such as oysters. This motivates the\ndevelopment of long-term monitoring systems. However, human labor is costly and\nlong-duration underwater work is highly hazardous, thus favoring robotic\nsolutions as a safer and more efficient option. To enable underwater robots to\nmake real-time, environment-aware decisions without human intervention, we must\nequip them with an intelligent \"brain.\" This highlights the need for\npersistent,wide-area, and low-cost benthic monitoring. To this end, we present\nDREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term\nunderwater exploration and habitat monitoring. The results show that our\nframework is highly efficient in finding and exploring target objects (e.g.,\noysters, shipwrecks) without prior location information. In the\noyster-monitoring task, our framework takes 31.5% less time than the previous\nbaseline with the same amount of oysters. Compared to the vanilla VLM, it uses\n23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our\nframework successfully explores and maps the wreck without collisions,\nrequiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,\nwhile the vanilla model achieves 60.23% average coverage in our shipwreck\nenvironments.", "AI": {"tldr": "DREAM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u671f\u6c34\u4e0b\u63a2\u7d22\u548c\u6816\u606f\u5730\u76d1\u6d4b\uff0c\u5728\u5bfb\u627e\u548c\u63a2\u7d22\u76ee\u6807\u7269\u4f53\uff08\u5982\u7261\u86ce\u3001\u6c89\u8239\uff09\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8282\u7701\u65f6\u95f4\u4e14\u8986\u76d6\u66f4\u5e7f\u3002", "motivation": "\u6d77\u6d0b\u53d8\u6696\u548c\u9178\u5316\u589e\u52a0\u4e86\u6e29\u5ea6\u654f\u611f\u8d1d\u7c7b\uff08\u5982\u7261\u86ce\uff09\u5927\u89c4\u6a21\u6b7b\u4ea1\u4e8b\u4ef6\u7684\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u957f\u671f\u76d1\u6d4b\u7cfb\u7edf\u3002\u4eba\u7c7b\u6c34\u4e0b\u4f5c\u4e1a\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u56e0\u6b64\u9700\u8981\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u6765\u5b9e\u73b0\u5b9e\u65f6\u3001\u73af\u5883\u611f\u77e5\u7684\u81ea\u4e3b\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e86DREAM\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u7528\u4e8e\u957f\u671f\u6c34\u4e0b\u63a2\u7d22\u548c\u6816\u606f\u5730\u76d1\u6d4b\uff0c\u4e0d\u9700\u8981\u5148\u9a8c\u4f4d\u7f6e\u4fe1\u606f\u3002", "result": "\u5728\u7261\u86ce\u76d1\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8282\u770131.5%\u65f6\u95f4\uff1b\u76f8\u6bd4\u539f\u59cbVLM\uff0c\u6b65\u9aa4\u51cf\u5c1123%\u4e14\u7261\u86ce\u8986\u76d6\u589e\u52a08.88%\u3002\u5728\u6c89\u8239\u573a\u666f\u4e2d\uff0c\u5b9e\u73b0100%\u8986\u76d6\u4e14\u65e0\u78b0\u649e\uff0c\u6b65\u9aa4\u51cf\u5c1127.5%\uff0c\u800c\u539f\u59cbVLM\u5e73\u5747\u8986\u76d6\u7387\u4e3a60.23%\u3002", "conclusion": "DREAM\u6846\u67b6\u4e3a\u6c34\u4e0b\u957f\u671f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u76ee\u6807\u63a2\u7d22\u548c\u8986\u76d6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13839", "abs": "https://arxiv.org/abs/2509.13839", "authors": ["Motonari Kambara", "Komei Sugiura"], "title": "Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models", "comment": "Published in Advanced Robotics", "summary": "In this work, we address the problem of predicting the future success of\nopen-vocabulary object manipulation tasks. Conventional approaches typically\ndetermine success or failure after the action has been carried out. However,\nthey make it difficult to prevent potential hazards and rely on failures to\ntrigger replanning, thereby reducing the efficiency of object manipulation\nsequences. To overcome these challenges, we propose a model, which predicts the\nalignment between a pre-manipulation egocentric image with the planned\ntrajectory and a given natural language instruction. We introduce a Multi-Level\nTrajectory Fusion module, which employs a state-of-the-art deep state-space\nmodel and a transformer encoder in parallel to capture multi-level time-series\nself-correlation within the end effector trajectory. Our experimental results\nindicate that the proposed method outperformed existing methods, including\nfoundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u672a\u6765\u6210\u529f\u7387\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\u5206\u6790\u9884\u64cd\u4f5c\u56fe\u50cf\u3001\u89c4\u5212\u8f68\u8ff9\u548c\u8bed\u8a00\u6307\u4ee4\u7684\u5bf9\u9f50\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u64cd\u4f5c\u5b8c\u6210\u540e\u5224\u65ad\u6210\u529f\u4e0e\u5426\uff0c\u96be\u4ee5\u9884\u9632\u6f5c\u5728\u5371\u9669\u4e14\u4f9d\u8d56\u5931\u8d25\u89e6\u53d1\u91cd\u89c4\u5212\uff0c\u964d\u4f4e\u4e86\u64cd\u4f5c\u5e8f\u5217\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u5148\u8fdb\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548ctransformer\u7f16\u7801\u5668\u5e76\u884c\u6355\u83b7\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u4e2d\u7684\u591a\u7ea7\u65f6\u95f4\u5e8f\u5217\u81ea\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u5305\u62ec\u57fa\u7840\u6a21\u578b\u5728\u5185\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u63d0\u9ad8\u64cd\u4f5c\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.13784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13784", "abs": "https://arxiv.org/abs/2509.13784", "authors": ["Hanfang Liang", "Bing Wang", "Shizhen Zhang", "Wen Jiang", "Yizhuo Yang", "Weixiang Guo", "Shenghai Yuan"], "title": "CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling", "comment": "8 pages, 6 figures", "summary": "Event cameras capture asynchronous pixel-level brightness changes with\nmicrosecond temporal resolution, offering unique advantages for high-speed\nvision tasks. Existing methods often convert event streams into intermediate\nrepresentations such as frames, voxel grids, or point clouds, which inevitably\nrequire predefined time windows and thus introduce window latency. Meanwhile,\npointwise detection methods face computational challenges that prevent\nreal-time efficiency due to their high computational cost. To overcome these\nlimitations, we propose the Variable-Rate Spatial Event Mamba, a novel\narchitecture that directly processes raw event streams without intermediate\nrepresentations. Our method introduces a lightweight causal spatial\nneighborhood encoder to efficiently capture local geometric relations, followed\nby Mamba-based state space models for scalable temporal modeling with linear\ncomplexity. During inference, a controller adaptively adjusts the processing\nspeed according to the event rate, achieving an optimal balance between window\nlatency and inference latency.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5904\u7406\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u91c7\u7528Mamba\u6a21\u578b\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u901a\u8fc7\u53d8\u901f\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u65b9\u6cd5\u9700\u8981\u5c06\u4e8b\u4ef6\u6d41\u8f6c\u6362\u4e3a\u4e2d\u95f4\u8868\u793a\uff08\u5982\u5e27\u3001\u7acb\u65b9\u4f53\u7f51\u683c\u7b49\uff09\uff0c\u5f15\u5165\u4e86\u65f6\u95f4\u7a97\u53e3\u5ef6\u8fdf\uff1b\u70b9\u7ea7\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u65e0\u6cd5\u5b9e\u65f6\u3002", "method": "\u63d0\u51faVariable-Rate Spatial Event Mamba\u67b6\u6784\uff1a1\uff09\u8f7b\u91cf\u7ea7\u56e0\u679c\u7a7a\u95f4\u90bb\u57df\u7f16\u7801\u5668\u6355\u6349\u5c40\u90e8\u51e0\u4f55\u5173\u7cfb\uff1b2\uff09Mamba\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5efa\u6a21\uff1b3\uff09\u63a8\u7406\u65f6\u901a\u8fc7\u63a7\u5236\u5668\u6839\u636e\u4e8b\u4ef6\u7387\u52a8\u6001\u8c03\u6574\u5904\u7406\u901f\u5ea6\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u907f\u514d\u4e86\u4e2d\u95f4\u8868\u793a\u8f6c\u6362\u5e26\u6765\u7684\u7a97\u53e3\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u4f18\u52bf\uff0c\u901a\u8fc7\u53d8\u901f\u5904\u7406\u673a\u5236\u5b9e\u73b0\u4e86\u5ef6\u8fdf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u9ad8\u901f\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2509.13863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13863", "abs": "https://arxiv.org/abs/2509.13863", "authors": ["Chu Chen", "Ander Biguri", "Jean-Michel Morel", "Raymond H. Chan", "Carola-Bibiane Sch\u00f6nlieb", "Jizhou Li"], "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction", "comment": null, "summary": "X-ray Computed Laminography (CL) is essential for non-destructive inspection\nof plate-like structures in applications such as microchips and composite\nbattery materials, where traditional computed tomography (CT) struggles due to\ngeometric constraints. However, reconstructing high-quality volumes from\nlaminographic projections remains challenging, particularly under highly\nsparse-view acquisition conditions. In this paper, we propose a reconstruction\nalgorithm, namely LamiGauss, that combines Gaussian Splatting radiative\nrasterization with a dedicated detector-to-world transformation model\nincorporating the laminographic tilt angle. LamiGauss leverages an\ninitialization strategy that explicitly filters out common laminographic\nartifacts from the preliminary reconstruction, preventing redundant Gaussians\nfrom being allocated to false structures and thereby concentrating model\ncapacity on representing the genuine object. Our approach effectively optimizes\ndirectly from sparse projections, enabling accurate and efficient\nreconstruction with limited data. Extensive experiments on both synthetic and\nreal datasets demonstrate the effectiveness and superiority of the proposed\nmethod over existing techniques. LamiGauss uses only 3$\\%$ of full views to\nachieve superior performance over the iterative method optimized on a full\ndataset.", "AI": {"tldr": "LamiGauss\u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u5149\u6805\u5316\u7684CL\u91cd\u5efa\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e13\u7528\u63a2\u6d4b\u5668\u5230\u4e16\u754c\u53d8\u6362\u6a21\u578b\u548c\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5728\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa", "motivation": "\u4f20\u7edfCT\u5728\u5e73\u677f\u7ed3\u6784\u68c0\u6d4b\u4e2d\u5b58\u5728\u51e0\u4f55\u9650\u5236\uff0c\u800c\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u7684CL\u91cd\u5efa\u8d28\u91cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u5149\u6805\u5316\u548c\u5305\u542b\u5c42\u6790\u503e\u659c\u89d2\u7684\u4e13\u7528\u63a2\u6d4b\u5668\u5230\u4e16\u754c\u53d8\u6362\u6a21\u578b\uff0c\u91c7\u7528\u521d\u59cb\u5316\u7b56\u7565\u8fc7\u6ee4\u5e38\u89c1\u4f2a\u5f71", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u97003%\u7684\u5168\u89c6\u56fe\u5373\u53ef\u8fbe\u5230\u4f18\u4e8e\u5728\u5168\u6570\u636e\u96c6\u4e0a\u4f18\u5316\u7684\u8fed\u4ee3\u65b9\u6cd5\u7684\u6027\u80fd", "conclusion": "LamiGauss\u80fd\u591f\u76f4\u63a5\u4ece\u7a00\u758f\u6295\u5f71\u4e2d\u6709\u6548\u4f18\u5316\uff0c\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u51c6\u786e\u9ad8\u6548\u7684\u91cd\u5efa"}}
{"id": "2509.13883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13883", "abs": "https://arxiv.org/abs/2509.13883", "authors": ["Zhen Xu", "Guorui Lu", "Chang Gao", "Qinyu Chen"], "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View", "comment": "8 pages", "summary": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.", "AI": {"tldr": "EvHand-FPV\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5355\u4e8b\u4ef6\u76f8\u673a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d23D\u624b\u90e8\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u8155ROI\u5b9a\u4f4d\u3001\u7aef\u5230\u7aef\u6620\u5c04\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u91cf\u548c\u53c2\u6570\u6570\u91cf\uff0c\u9002\u5408XR\u8bbe\u5907\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u624b\u90e8\u8ffd\u8e2a\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u96be\u4ee5\u6ee1\u8db3XR\u8bbe\u5907\u7684\u9700\u6c42\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u6beb\u74e6\u7ea7\u529f\u8017\u7684\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u8bc4\u4f30\u6570\u636e\u76f8\u7ed3\u5408\u7684\u4e8b\u4ef6\u6570\u636e\u96c6\uff1b\u5f15\u5165\u57fa\u4e8e\u624b\u8155\u7684ROI\u5b9a\u4f4d\u548c\u7aef\u5230\u7aef\u6620\u5c04\u7b56\u7565\u51cf\u5c11\u8ba1\u7b97\uff1b\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u8f85\u52a9\u51e0\u4f55\u7279\u5f81\u5934\u63d0\u5347\u8868\u5f81\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u6d4b\u8bd5\u96c6\u4e0a2D-AUCp\u4ece0.77\u63d0\u5347\u52300.85\uff0c\u53c2\u6570\u51cf\u5c1189%\uff0811.2M\u21921.2M\uff09\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1189%\uff081.648G\u21920.185G FLOPs\uff09\uff0c\u5408\u6210\u6570\u636e\u4e0a\u4fdd\u63010.84\u76843D-AUCp\u3002", "conclusion": "EvHand-FPV\u5b9e\u73b0\u4e86\u51c6\u786e\u9ad8\u6548\u7684\u81ea\u6211\u4e2d\u5fc3\u4e8b\u4ef6\u624b\u90e8\u8ffd\u8e2a\uff0c\u9002\u5408\u8bbe\u5907\u7aefXR\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6027\u80fd\u9700\u6c42\u3002"}}
{"id": "2509.14097", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.14097", "abs": "https://arxiv.org/abs/2509.14097", "authors": ["Yaru Chen", "Ruohao Guo", "Liting Gao", "Yang Xiang", "Qingyu Luo", "Zhenbo Li", "Wenwu Wang"], "title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing", "comment": null, "summary": "Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.", "AI": {"tldr": "\u63d0\u51faEMA\u5f15\u5bfc\u7684\u4f2a\u76d1\u7763\u6846\u67b6\u548c\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7a33\u5b9a\u7247\u6bb5\u7ea7\u76d1\u7763\u548c\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u95ee\u9898", "method": "1) EMA\u5f15\u5bfc\u7684\u4f2a\u76d1\u7763\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u6216top-k\u9009\u62e9\u751f\u6210\u53ef\u9760\u7247\u6bb5\u7ea7\u63a9\u7801\uff1b2) \u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u635f\u5931\u5728\u53ef\u9760\u7247\u6bb5-\u7c7b\u522b\u5bf9\u4e0a\u5bf9\u9f50\u97f3\u9891\u548c\u89c6\u89c9\u5d4c\u5165", "result": "\u5728LLP\u548cUnAV-100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u63d0\u4f9b\u7a33\u5b9a\u7247\u6bb5\u7ea7\u76d1\u7763\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u7684\u6027\u80fd"}}
{"id": "2509.14142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14142", "abs": "https://arxiv.org/abs/2509.14142", "authors": ["Peng Xu", "Shengwu Xiong", "Jiajun Zhang", "Yaxiong Chen", "Bowen Zhou", "Chen Change Loy", "David A. Clifton", "Kyoung Mu Lee", "Luc Van Gool", "Ruiming He", "Ruilin Yao", "Xinwei Long", "Jirui Huang", "Kai Tian", "Sa Yang", "Yihua Shao", "Jin Feng", "Yue Zhong", "Jiakai Zhou", "Cheng Tang", "Tianyu Zou", "Yifang Zhang", "Junming Liang", "Guoyou Li", "Zhaoxiang Wang", "Qiang Zhou", "Yichen Zhao", "Shili Xiong", "Hyeongjin Nam", "Jaerin Lee", "Jaeyoung Chung", "JoonKyu Park", "Junghun Oh", "Kanggeon Lee", "Wooseok Lee", "Juneyoung Ro", "Turghun Osman", "Can Hu", "Chaoyang Liao", "Cheng Chen", "Chengcheng Han", "Chenhao Qiu", "Chong Peng", "Cong Xu", "Dailin Li", "Feiyu Wang", "Feng Gao", "Guibo Zhu", "Guopeng Tang", "Haibo Lu", "Han Fang", "Han Qi", "Hanxiao Wu", "Haobo Cheng", "Hongbo Sun", "Hongyao Chen", "Huayong Hu", "Hui Li", "Jiaheng Ma", "Jiang Yu", "Jianing Wang", "Jie Yang", "Jing He", "Jinglin Zhou", "Jingxuan Li", "Josef Kittler", "Lihao Zheng", "Linnan Zhao", "Mengxi Jia", "Muyang Yan", "Nguyen Thanh Thien", "Pu Luo", "Qi Li", "Shien Song", "Shijie Dong", "Shuai Shao", "Shutao Li", "Taofeng Xue", "Tianyang Xu", "Tianyi Gao", "Tingting Li", "Wei Zhang", "Weiyang Su", "Xiaodong Dong", "Xiao-Jun Wu", "Xiaopeng Zhou", "Xin Chen", "Xin Wei", "Xinyi You", "Xudong Kang", "Xujie Zhou", "Xusheng Liu", "Yanan Wang", "Yanbin Huang", "Yang Liu", "Yang Yang", "Yanglin Deng", "Yashu Kang", "Ye Yuan", "Yi Wen", "Yicen Tian", "Yilin Tao", "Yin Tang", "Yipeng Lin", "Yiqing Wang", "Yiting Xi", "Yongkang Yu", "Yumei Li", "Yuxin Qin", "Yuying Chen", "Yuzhe Cen", "Zhaofan Zou", "Zhaohong Liu", "Zhehao Shen", "Zhenglin Du", "Zhengyang Li", "Zhenni Huang", "Zhenwei Shao", "Zhilong Song", "Zhiyong Feng", "Zhiyu Wang", "Zhou Yu", "Ziang Li", "Zihan Zhai", "Zijian Zhang", "Ziyang Peng", "Ziyun Xiao", "Zongshu Li"], "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook", "comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''", "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.", "AI": {"tldr": "MARS2 2025\u6311\u6218\u8d5b\u7efc\u8ff0\uff0c\u805a\u7126\u591a\u6a21\u6001\u63a8\u7406\uff0c\u53d1\u5e03\u4e24\u4e2a\u5b9a\u5236\u6570\u636e\u96c6Lens\u548cAdsQA\uff0c\u8bc4\u4f3040+\u57fa\u7ebf\u6a21\u578b\uff0c\u8bbe\u7acb\u4e09\u4e2a\u7ade\u8d5b\u8d5b\u9053\uff0c\u5438\u5f1576\u4e2a\u56e2\u961f\u53c2\u4e0e\u3002", "motivation": "\u6574\u5408\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u548cLLM\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u8ba9\u7814\u7a76\u8005\u8ddf\u4e0a\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u4e13\u6ce8\u4e8e\u73b0\u5b9e\u4e16\u754c\u548c\u4e13\u4e1a\u5316\u573a\u666f\u4ee5\u62d3\u5bbdMLLM\u7684\u591a\u6a21\u6001\u63a8\u7406\u5e94\u7528\u3002", "method": "\u53d1\u5e03\u4e24\u4e2a\u5b9a\u5236\u6570\u636e\u96c6\uff08Lens\u652f\u630112\u4e2a\u65e5\u5e38\u573a\u666f\u7684\u901a\u7528\u63a8\u7406\uff0cAdsQA\u652f\u6301\u5e7f\u544a\u89c6\u9891\u7684\u9886\u57df\u7279\u5b9a\u63a8\u7406\uff09\uff0c\u8bc4\u4f3040+\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ec\u901a\u7528MLLM\u548c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff09\uff0c\u8bbe\u7acb\u4e09\u4e2a\u7ade\u8d5b\u8d5b\u9053\uff08VG-RS\u3001VQA-SA\u3001VR-Ads\uff09\u3002", "result": "76\u4e2a\u6765\u81ea\u77e5\u540d\u5b66\u672f\u548c\u5de5\u4e1a\u673a\u6784\u7684\u56e2\u961f\u6ce8\u518c\uff0c1200+\u63d0\u4ea4\u4e2d40+\u6709\u6548\u63d0\u4ea4\u88ab\u7eb3\u5165\u6392\u540d\u5217\u8868\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u96c6\u548c\u6392\u540d\u5df2\u516c\u5f00\u3002", "conclusion": "MARS2 2025\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
