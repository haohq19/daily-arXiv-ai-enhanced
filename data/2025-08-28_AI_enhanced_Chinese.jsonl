{"id": "2508.19257", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86Temporal Token Fusion (TTF)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5386\u53f2\u89c6\u89c9\u4fe1\u606f\u6765\u589e\u5f3aVLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8de8\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684Vision-Language-Action\u6a21\u578b\u9010\u5e27\u5904\u7406\u89c6\u89c9\u8f93\u5165\uff0c\u4e22\u5f03\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9d\u8d35\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u89c6\u89c9\u566a\u58f0\u5f71\u54cd\u5e76\u5ffd\u7565\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8fde\u8d2f\u6027\u3002", "method": "TTF\u91c7\u7528\u53cc\u7ef4\u5ea6\u68c0\u6d4b\uff08\u7070\u5ea6\u50cf\u7d20\u5dee\u5f02\u5206\u6790\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bed\u4e49\u76f8\u5173\u6027\u8bc4\u4f30\uff09\uff0c\u901a\u8fc7\u786c\u878d\u5408\u7b56\u7565\u548c\u5173\u952e\u5e27\u951a\u5b9a\u5b9e\u73b0\u9009\u62e9\u6027\u65f6\u95f4token\u878d\u5408\uff0c\u9632\u6b62\u9519\u8bef\u7d2f\u79ef\u3002", "result": "\u5728LIBERO\u4e0a\u5e73\u5747\u63d0\u53474.0\u4e2a\u767e\u5206\u70b9\uff0872.4% vs 68.4%\u57fa\u51c6\uff09\uff0cSimplerEnv\u4e0a\u76f8\u5bf9\u63d0\u53474.8%\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u76f8\u5bf9\u63d0\u53478.7%\uff0c\u4e14\u8bc1\u660e\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u3002", "conclusion": "TTF\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u9009\u62e9\u6027Query\u77e9\u9635\u91cd\u7528\u80fd\u591f\u63d0\u5347\u6027\u80fd\u800c\u975e\u635f\u5bb3\u6027\u80fd\uff0c\u4e3a\u76f4\u63a5KQV\u77e9\u9635\u91cd\u7528\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5728\u5b9e\u73b0\u8ba1\u7b97\u52a0\u901f\u7684\u540c\u65f6\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2508.19361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19361", "abs": "https://arxiv.org/abs/2508.19361", "authors": ["Yongbin Lee", "Ki H. Chon"], "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture", "comment": "4 pages, 2 figures, 4 table, IEEE-EMBS International Conference on\n  Body Sensor Networks (IEEE-EMBS BSN 2025)", "summary": "Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk\nof stroke, heart failure, and other cardiovascular complications. While AF\ndetection algorithms perform well in identifying persistent AF, early-stage\nprogression, such as paroxysmal AF (PAF), often goes undetected due to its\nsudden onset and short duration. However, undetected PAF can progress into\nsustained AF, increasing the risk of mortality and severe complications. Early\nprediction of AF offers an opportunity to reduce disease progression through\npreventive therapies, such as catecholamine-sparing agents or beta-blockers. In\nthis study, we propose a lightweight deep learning model using only RR\nIntervals (RRIs), combining a Temporal Convolutional Network (TCN) for\npositional encoding with Mamba, a selective state space model, to enable early\nprediction of AF through efficient parallel sequence modeling. In subject-wise\ntesting results, our model achieved a sensitivity of 0.908, specificity of\n0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our\nmethod demonstrates high computational efficiency, with only 73.5 thousand\nparameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural\nNetwork-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and\nmodel compactness. Notably, the model can predict AF up to two hours in advance\nusing just 30 minutes of input data, providing enough lead time for preventive\ninterventions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528RR\u95f4\u9694\u6570\u636e\u901a\u8fc7\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\u548cMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u7ec4\u5408\uff0c\u80fd\u591f\u63d0\u524d2\u5c0f\u65f6\u9884\u6d4b\u623f\u98a4\uff0c\u4e14\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u623f\u98a4\u662f\u6700\u5e38\u89c1\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u7279\u522b\u662f\u53d1\u4f5c\u6027\u623f\u98a4(PAF)\u56e0\u5176\u7a81\u7136\u53d1\u4f5c\u548c\u77ed\u6682\u6301\u7eed\u65f6\u95f4\u800c\u96be\u4ee5\u68c0\u6d4b\uff0c\u4f46\u672a\u88ab\u53d1\u73b0\u7684PAF\u53ef\u80fd\u8fdb\u5c55\u4e3a\u6301\u7eed\u6027\u623f\u98a4\uff0c\u589e\u52a0\u6b7b\u4ea1\u98ce\u9669\u3002\u65e9\u671f\u9884\u6d4b\u623f\u98a4\u53ef\u4ee5\u901a\u8fc7\u9884\u9632\u6027\u6cbb\u7597\u51cf\u5c11\u75be\u75c5\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528RR\u95f4\u9694(RRIs)\u6570\u636e\uff0c\u7ed3\u5408\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc(TCN)\u8fdb\u884c\u4f4d\u7f6e\u7f16\u7801\u548cMamba\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u5728\u4e3b\u4f53\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u8fbe\u5230\u4e86\u654f\u611f\u5ea60.908\u3001\u7279\u5f02\u5ea60.933\u3001F1\u5206\u65700.930\u3001AUROC\u4e3a0.972\u3001AUPRC\u4e3a0.932\u3002\u6a21\u578b\u4ec5\u670973.5\u5343\u53c2\u6570\u548c38.3 MFLOPs\u8ba1\u7b97\u91cf\uff0c\u80fd\u591f\u4ec5\u4f7f\u752830\u5206\u949f\u8f93\u5165\u6570\u636e\u9884\u6d4b\u672a\u67652\u5c0f\u65f6\u7684\u623f\u98a4\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6a21\u578b\u7d27\u51d1\u6027\u65b9\u9762\u90fd\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684CNN-RNN\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u5145\u8db3\u7684\u9884\u8b66\u65f6\u95f4\u8fdb\u884c\u9884\u9632\u6027\u5e72\u9884\uff0c\u4e3a\u65e9\u671f\u623f\u98a4\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19359", "abs": "https://arxiv.org/abs/2508.19359", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "\u63d0\u51faARIS\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u6df7\u5408\u4ee3\u7406\u548c\u5224\u522b\u5f0f\u5e8f\u5217\u6807\u6ce8\u5668\uff0c\u901a\u8fc7\u6a21\u578b\u5171\u8bc6\u3001\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548cLLM\u53cd\u601d\u63a8\u7406\u6765\u63d0\u5347\u4e8b\u4ef6\u62bd\u53d6\u6027\u80fd\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u5224\u522b\u6a21\u578b\u7cbe\u5ea6\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u7279\u522b\u662f\u5bf9\u7ec6\u5fae\u6216\u7f55\u89c1\u4e8b\u4ef6\uff1b\u751f\u6210\u5f0fLLM\u65b9\u6cd5\u8bed\u4e49\u7075\u6d3b\u6027\u9ad8\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u9884\u6d4b\u95ee\u9898", "method": "ARIS\u6df7\u5408\u65b9\u6cd5\uff1a\u81ea\u6df7\u5408\u4ee3\u7406+\u5224\u522b\u5f0f\u5e8f\u5217\u6807\u6ce8\u5668\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6a21\u578b\u5171\u8bc6\u3001\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548cLLM\u53cd\u601d\u63a8\u7406\u6a21\u5757\u6765\u53ef\u9760\u89e3\u51b3\u6b67\u4e49\uff1b\u8fd8\u7814\u7a76\u4e86\u5206\u89e3\u6307\u4ee4\u5fae\u8c03\u4ee5\u589e\u5f3aLLM\u4e8b\u4ef6\u62bd\u53d6\u7406\u89e3", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u4e8b\u4ef6\u62bd\u53d6\u65b9\u6cd5", "conclusion": "ARIS\u901a\u8fc7\u7ed3\u5408\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u7684\u7cbe\u5ea6-\u53ec\u56de\u6743\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u4e8b\u4ef6\u9884\u6d4b\u8d28\u91cf"}}
{"id": "2508.19376", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.19376", "abs": "https://arxiv.org/abs/2508.19376", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments", "comment": null, "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.", "AI": {"tldr": "\u57fa\u4e8eLLaMA 3.2\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u80fd\u7269\u7406\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfCNN\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u5bf9\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u50cf\u7d20\u5316\u63a2\u6d4b\u5668\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b", "method": "\u4f7f\u7528\u57fa\u4e8eLLaMA 3.2\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4e0eNOvA\u548cDUNE\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u7684CNN\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u8bc4\u4f30", "result": "VLM\u4e0d\u4ec5\u8fbe\u5230\u6216\u8d85\u8fc7CNN\u6027\u80fd\uff0c\u8fd8\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u80fd\u529b\u548c\u66f4\u597d\u7684\u8f85\u52a9\u6587\u672c/\u8bed\u4e49\u4e0a\u4e0b\u6587\u6574\u5408", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u9ad8\u80fd\u7269\u7406\u4e8b\u4ef6\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u901a\u7528\u9aa8\u5e72\u7f51\u7edc\uff0c\u4e3a\u5b9e\u9a8c\u6027\u4e2d\u5fae\u5b50\u7269\u7406\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2508.19427", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19427", "abs": "https://arxiv.org/abs/2508.19427", "authors": ["Evandro L. T. P. Cunha"], "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4eba\u5de5\u667a\u80fd\u6587\u672c\u751f\u6210\u5de5\u5177\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u9000\u5316\u7684\u98ce\u9669\uff0c\u7c7b\u4f3c\u4e8e\u53e4\u5e0c\u814a\u9ed1\u6697\u65f6\u4ee3\u6587\u5b57\u80fd\u529b\u7684\u4e27\u5931", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u7c7b\u53ef\u80fd\u8fc7\u5ea6\u4f9d\u8d56\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u5bfc\u81f4\u81ea\u8eab\u5199\u4f5c\u80fd\u529b\u9000\u5316\uff0c\u9700\u8981\u63a2\u8ba8\u8fd9\u79cd\u5386\u53f2\u91cd\u6f14\u7684\u53ef\u80fd\u6027", "method": "\u901a\u8fc7\u5386\u53f2\u7c7b\u6bd4\u5206\u6790\uff0c\u5c06\u5f53\u524dAI\u6587\u672c\u751f\u6210\u5de5\u5177\u7684\u666e\u53ca\u4e0e\u53e4\u5e0c\u814a\u9ed1\u6697\u65f6\u4ee3\u6587\u5b57\u80fd\u529b\u4e27\u5931\u7684\u73b0\u8c61\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76", "result": "\u8bc6\u522b\u51faAI\u5de5\u5177\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u9010\u6e10\u9000\u5316\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u8fd9\u79cd\u6a21\u5f0f\u4e0e\u5386\u53f2\u4e0a\u5176\u4ed6\u6587\u5b57\u80fd\u529b\u4e27\u5931\u65f6\u671f\u6709\u76f8\u4f3c\u4e4b\u5904", "conclusion": "\u9700\u8981\u8b66\u60d5\u8fc7\u5ea6\u4f9d\u8d56AI\u6587\u672c\u751f\u6210\u5de5\u5177\u53ef\u80fd\u5e26\u6765\u7684\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u4fdd\u6301\u4eba\u7c7b\u81ea\u8eab\u7684\u6587\u5b57\u521b\u4f5c\u80fd\u529b\u81f3\u5173\u91cd\u8981"}}
{"id": "2508.19325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19325", "abs": "https://arxiv.org/abs/2508.19325", "authors": ["Haoyang Su", "Jin-Yi Xiang", "Shaohao Rui", "Yifan Gao", "Xingyu Chen", "Tingxuan Yin", "Xiaosong Wang", "Lian-Ming Wu"], "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI", "comment": null, "summary": "Accurate prediction of major adverse cardiac events (MACE) remains a central\nchallenge in cardiovascular prognosis. We present PRISM (Prompt-guided\nRepresentation Integration for Survival Modeling), a self-supervised framework\nthat integrates visual representations from non-contrast cardiac cine magnetic\nresonance imaging with structured electronic health records (EHRs) for survival\nanalysis. PRISM extracts temporally synchronized imaging features through\nmotion-aware multi-view distillation and modulates them using medically\ninformed textual prompts to enable fine-grained risk prediction. Across four\nindependent clinical cohorts, PRISM consistently surpasses classical survival\nprediction models and state-of-the-art (SOTA) deep learning baselines under\ninternal and external validation. Further clinical findings demonstrate that\nthe combined imaging and EHR representations derived from PRISM provide\nvaluable insights into cardiac risk across diverse cohorts. Three distinct\nimaging signatures associated with elevated MACE risk are uncovered, including\nlateral wall dyssynchrony, inferior wall hypersensitivity, and anterior\nelevated focus during diastole. Prompt-guided attribution further identifies\nhypertension, diabetes, and smoking as dominant contributors among clinical and\nphysiological EHR factors.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u6574\u5408\u5fc3\u810f\u78c1\u5171\u632f\u6210\u50cf\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8fdb\u884c\u751f\u5b58\u5206\u6790\uff0c\u5728\u56db\u4e2a\u72ec\u7acb\u4e34\u5e8a\u961f\u5217\u4e2d\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u548cSOTA\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u53d1\u73b0\u4e86\u4e09\u4e2a\u4e0e\u5fc3\u810f\u98ce\u9669\u76f8\u5173\u7684\u6210\u50cf\u7279\u5f81\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e3b\u8981\u4e0d\u826f\u5fc3\u810f\u4e8b\u4ef6\uff08MACE\uff09\u662f\u5fc3\u8840\u7ba1\u9884\u540e\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u6765\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u591a\u89c6\u56fe\u84b8\u998f\u63d0\u53d6\u65f6\u95f4\u540c\u6b65\u7684\u6210\u50cf\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u533b\u5b66\u4fe1\u606f\u6587\u672c\u63d0\u793a\u8fdb\u884c\u8c03\u5236\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u98ce\u9669\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u72ec\u7acb\u4e34\u5e8a\u961f\u5217\u4e2d\uff0cPRISM\u5728\u5185\u90e8\u548c\u5916\u90e8\u9a8c\u8bc1\u4e2d\u5747\u8d85\u8d8a\u7ecf\u5178\u751f\u5b58\u9884\u6d4b\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u53d1\u73b0\u4e86\u4e09\u4e2a\u4e0eMACE\u98ce\u9669\u76f8\u5173\u7684\u6210\u50cf\u7279\u5f81\u3002", "conclusion": "PRISM\u6574\u5408\u6210\u50cf\u548cEHR\u8868\u5f81\u4e3a\u4e0d\u540c\u961f\u5217\u7684\u5fc3\u810f\u98ce\u9669\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u63d0\u793a\u5f15\u5bfc\u5f52\u56e0\u8bc6\u522b\u4e86\u9ad8\u8840\u538b\u3001\u7cd6\u5c3f\u75c5\u548c\u5438\u70df\u662f\u4e3b\u8981\u7684\u4e34\u5e8a\u548c\u751f\u7406\u98ce\u9669\u56e0\u7d20\u3002"}}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86CASE\uff08\u8bc8\u9a97\u89e3\u91ca\u5bf9\u8bdd\u673a\u5668\u4eba\uff09\u6846\u67b6\uff0c\u901a\u8fc7AI\u5bf9\u8bdd\u673a\u5668\u8c03\u67e5\u6f5c\u5728\u53d7\u5bb3\u8005\uff0c\u6536\u96c6\u7ed3\u6784\u5316\u8bc8\u9a97\u60c5\u62a5\uff0c\u5728Google Pay\u5e73\u53f0\u5b9e\u73b0\u4e8621%\u7684\u8bc8\u9a97\u5904\u7f6e\u91cf\u63d0\u5347\u3002", "motivation": "\u6570\u5b57\u652f\u4ed8\u5e73\u53f0\u7684\u666e\u53ca\u5bfc\u81f4\u793e\u5de5\u8bc8\u9a97\u589e\u591a\uff0c\u4f46\u4f20\u7edf\u7684\u7528\u6237\u548c\u4ea4\u6613\u4fe1\u53f7\u65e0\u6cd5\u5168\u9762\u8bc6\u522b\u8bc8\u9a97\u6a21\u5f0f\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u53ca\u65f6\u9632\u8303\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e3b\u52a8\u5bf9\u8bdd\u673a\u5668\u673a\u5236\uff0c\u4f7f\u7528Google Gemini LLMs\u4e3b\u52a8\u8c03\u67e5\u6f5c\u5728\u53d7\u5bb3\u8005\uff0c\u5c06\u5bf9\u8bdd\u5185\u5bb9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u548c\u624b\u52a8\u6267\u6cd5\u673a\u5236\u3002", "result": "\u5728Google Pay\u5370\u5ea6\u5e73\u53f0\u5b9e\u73b0\u540e\uff0c\u8bc8\u9a97\u5904\u7f6e\u91cf\u63d0\u5347\u4e8621%\uff0c\u6846\u67b6\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CASE\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u6536\u96c6\u8bc8\u9a97\u60c5\u62a5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bc8\u9a97\u9632\u8303\u7684\u4fe1\u606f\u7f3a\u53e3\u95ee\u9898\uff0c\u4e3a\u5176\u4ed6\u654f\u611f\u9886\u57df\u7684AI\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2508.19542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19542", "abs": "https://arxiv.org/abs/2508.19542", "authors": ["Nannan Zhu", "Yonghao Dong", "Teng Wang", "Xueqian Li", "Shengjun Deng", "Yijia Wang", "Zheng Hong", "Tiantian Geng", "Guo Niu", "Hanyan Huang", "Xiongfei Yao", "Shuaiwei Jiao"], "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning", "comment": null, "summary": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs.The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.", "AI": {"tldr": "CVBench\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u591a\u89c6\u9891\u5173\u7cfb\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1000\u4e2aQA\u5bf9\uff0c\u6db5\u76d6\u4e09\u4e2a\u5c42\u6b21\uff1a\u8de8\u89c6\u9891\u5bf9\u8c61\u5173\u8054\u3001\u4e8b\u4ef6\u5173\u8054\u548c\u590d\u6742\u63a8\u7406\u3002\u6d4b\u8bd5\u53d1\u73b0\u5f53\u524d\u9876\u7ea7MLLM\u6a21\u578b\u5728\u591a\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u89c6\u9891\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u89c6\u9891\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u8fd9\u79cd\u80fd\u529b\u5bf9\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u591a\u6444\u50cf\u5934\u76d1\u63a7\u3001\u8de8\u89c6\u9891\u7a0b\u5e8f\u5b66\u4e60\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efaCVBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u89c6\u9891\u96c6\u7fa4\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u5c42\u6b21\u7684\u63a8\u7406\u4efb\u52a1\uff1a\u5bf9\u8c61\u5173\u8054\u3001\u4e8b\u4ef6\u5173\u8054\u548c\u590d\u6742\u63a8\u7406\u3002\u8bc4\u4f3010+\u4e2a\u9886\u5148MLLM\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e3b\u8981\u6a21\u578b\u5982GPT-4o\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u523060%\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768491%\u8868\u73b0\u3002\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524dMLLM\u67b6\u6784\u7684\u6839\u672c\u74f6\u9888\uff1a\u8de8\u89c6\u9891\u4e0a\u4e0b\u6587\u4fdd\u6301\u80fd\u529b\u4e0d\u8db3\u548c\u91cd\u53e0\u5b9e\u4f53\u6d88\u6b67\u80fd\u529b\u5dee\u3002", "conclusion": "CVBench\u4e3a\u8bca\u65ad\u548c\u63a8\u8fdb\u591a\u89c6\u9891\u63a8\u7406\u63d0\u4f9b\u4e86\u4e25\u8c28\u6846\u67b6\uff0c\u4e3a\u4e0b\u4e00\u4ee3MLLM\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002\u57fa\u51c6\u6570\u636e\u548c\u8bc4\u4f30\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.19533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19533", "abs": "https://arxiv.org/abs/2508.19533", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u672a\u89c1\u60c5\u611f\u5bf9\u8bdd\u8bc6\u522b(UERC)\u4efb\u52a1\u548cProEmoTrans\u539f\u578b\u8fc1\u79fb\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5c01\u95ed\u57df\u60c5\u611f\u8bc6\u522b\u65e0\u6cd5\u5904\u7406\u672a\u77e5\u60c5\u611f\u7684\u95ee\u9898\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u63cf\u8ff0\u3001\u53c2\u6570\u65e0\u5173\u7f16\u7801\u673a\u5236\u548c\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u7ef4\u7279\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u60c5\u611f\u5bf9\u8bdd\u8bc6\u522b(ERC)\u7814\u7a76\u57fa\u4e8e\u5c01\u95ed\u57df\u5047\u8bbe\uff0c\u4f46\u5fc3\u7406\u5b66\u4e2d\u60c5\u611f\u5206\u7c7b\u6ca1\u6709\u660e\u786e\u5171\u8bc6\uff0c\u73b0\u5b9e\u5e94\u7528\u4e2d\u6a21\u578b\u96be\u4ee5\u8bc6\u522b\u672a\u89c1\u8fc7\u7684\u60c5\u611f\u7c7b\u578b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u672a\u77e5\u60c5\u611f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faProEmoTrans\u539f\u578b\u60c5\u611f\u8fc1\u79fb\u6846\u67b6\uff1a1\uff09\u4f7f\u7528LLM\u589e\u5f3a\u63cf\u8ff0\u89e3\u51b3\u9690\u5f0f\u8868\u8fbe\u95ee\u9898\uff1b2\uff09\u53c2\u6570\u65e0\u5173\u673a\u5236\u5904\u7406\u957f\u5bf9\u8bdd\u7f16\u7801\u548c\u8fc7\u62df\u5408\uff1b3\uff09\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u7ef4\u7279\u6bd4\u89e3\u7801(AVD)\u65b9\u6cd5\u8fc1\u79fb\u5df2\u77e5\u60c5\u611f\u8f6c\u79fb\u6a21\u5f0f\u5230\u672a\u77e5\u60c5\u611f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u8fd9\u4e00\u65b0\u9886\u57df\u7684\u5f3a\u57fa\u7ebf\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u672a\u89c1\u8fc7\u7684\u60c5\u611f\u7c7b\u578b\uff0c\u8bc1\u660e\u4e86\u539f\u578b\u8fc1\u79fb\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684UERC\u4efb\u52a1\u548cProEmoTrans\u6846\u67b6\u4e3a\u5f00\u653e\u57df\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u539f\u578b\u8fc1\u79fb\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u672a\u89c1\u60c5\u611f\u8bc6\u522b\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19573", "abs": "https://arxiv.org/abs/2508.19573", "authors": ["Luhu Li", "Bowen Lin", "Mukhtiar Khan", "Shujun Fu"], "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection", "comment": null, "summary": "Anomaly detection in medical images is challenging due to limited annotations\nand a domain gap compared to natural images. Existing reconstruction methods\noften rely on frozen pre-trained encoders, which limits adaptation to\ndomain-specific features and reduces localization accuracy. Prototype-based\nlearning offers interpretability and clustering benefits but suffers from\nprototype collapse, where few prototypes dominate training, harming diversity\nand generalization. To address this, we propose a unified framework combining a\ntrainable encoder with prototype-guided reconstruction and a novel\nDiversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum\nbranch, enables stable domain-adaptive feature learning. A lightweight\nPrototype Extractor mines informative normal prototypes to guide the decoder\nvia attention for precise reconstruction. Our loss enforces balanced prototype\nuse through diversity constraints and per-prototype normalization, effectively\npreventing collapse. Experiments on multiple medical imaging benchmarks show\nsignificant improvements in representation quality and anomaly localization,\noutperforming prior methods. Visualizations and prototype assignment analyses\nfurther validate the effectiveness of our anti-collapse mechanism and enhanced\ninterpretability.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u539f\u578b\u5f15\u5bfc\u91cd\u5efa\u548c\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u539f\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8868\u793a\u8d28\u91cf\u548c\u5f02\u5e38\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6709\u9650\u548c\u9886\u57df\u5dee\u5f02\u6311\u6218\u3002\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u51bb\u7ed3\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u9650\u5236\u4e86\u9886\u57df\u9002\u5e94\u80fd\u529b\uff1b\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u539f\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5c11\u6570\u539f\u578b\u4e3b\u5bfc\u8bad\u7ec3\uff0c\u5f71\u54cd\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u4f7f\u7528\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\uff08\u901a\u8fc7\u52a8\u91cf\u5206\u652f\u589e\u5f3a\uff09\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60\uff1b\u8f7b\u91cf\u7ea7\u539f\u578b\u63d0\u53d6\u5668\u6316\u6398\u4fe1\u606f\u4e30\u5bcc\u7684\u6b63\u5e38\u539f\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6307\u5bfc\u89e3\u7801\u5668\u8fdb\u884c\u7cbe\u786e\u91cd\u5efa\uff1b\u63d0\u51fa\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\uff0c\u901a\u8fc7\u591a\u6837\u6027\u7ea6\u675f\u548c\u9010\u539f\u578b\u5f52\u4e00\u5316\u5b9e\u73b0\u5e73\u8861\u7684\u539f\u578b\u4f7f\u7528", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u8868\u793a\u8d28\u91cf\u548c\u5f02\u5e38\u5b9a\u4f4d\u7684\u663e\u8457\u6539\u8fdb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u53ef\u89c6\u5316\u548c\u539f\u578b\u5206\u914d\u5206\u6790\u9a8c\u8bc1\u4e86\u6297\u5d29\u6e83\u673a\u5236\u7684\u6709\u6548\u6027\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u539f\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60\u548c\u591a\u6837\u6027\u7ea6\u675f\u673a\u5236\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027"}}
{"id": "2508.19758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19758", "abs": "https://arxiv.org/abs/2508.19758", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "NEWSCOPE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u65b0\u95fb\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u53e5\u5b50\u7ea7\u8bed\u4e49\u53d8\u5316\u5efa\u6a21\u6765\u63d0\u5347\u4e8b\u4ef6\u62a5\u9053\u7684\u591a\u6837\u6027\uff0c\u5728\u4fdd\u6301\u76f8\u5173\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u7ed3\u679c\u7684\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b0\u95fb\u68c0\u7d22\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u5197\u4f59\u4e14\u89c6\u89d2\u6709\u9650\uff0c\u9700\u8981\u83b7\u53d6\u591a\u6837\u5316\u7684\u89c2\u70b9\u6765\u66f4\u597d\u5730\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5bc6\u96c6\u68c0\u7d22\u83b7\u53d6\u4e3b\u9898\u76f8\u5173\u5185\u5bb9\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u53e5\u5b50\u7ea7\u805a\u7c7b\u548c\u591a\u6837\u6027\u611f\u77e5\u91cd\u6392\u5e8f\u6765\u53d1\u73b0\u8865\u5145\u4fe1\u606f\u3002", "result": "NEWSCOPE\u5728\u6784\u5efa\u7684\u4e24\u4e2a\u6bb5\u843d\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff08LocalNews\u548cDSGlobal\uff09\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u591a\u6837\u6027\u800c\u4e0d\u635f\u5bb3\u76f8\u5173\u6027\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u5efa\u6a21\u5728\u51cf\u5c11\u5197\u4f59\u548c\u4fc3\u8fdb\u5168\u9762\u4e8b\u4ef6\u7406\u89e3\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684\u4e09\u4e2a\u53ef\u89e3\u91ca\u6307\u6807\uff08\u5e73\u5747\u6210\u5bf9\u8ddd\u79bb\u3001\u6b63\u805a\u7c7b\u8986\u76d6\u7387\u548c\u4fe1\u606f\u5bc6\u5ea6\u6bd4\uff09\u80fd\u6709\u6548\u8bc4\u4f30\u68c0\u7d22\u591a\u6837\u6027\u3002"}}
{"id": "2508.19649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19649", "abs": "https://arxiv.org/abs/2508.19649", "authors": ["Dongjin Kim", "Jaekyun Ko", "Muhammad Kashif Ali", "Tae Hyun Kim"], "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising", "comment": "ICCV 2025. Project Page: https://dongjinkim9.github.io/projects/idf/", "summary": "Image denoising is a fundamental challenge in computer vision, with\napplications in photography and medical imaging. While deep learning-based\nmethods have shown remarkable success, their reliance on specific noise\ndistributions limits generalization to unseen noise types and levels. Existing\napproaches attempt to address this with extensive training data and high\ncomputational resources but they still suffer from overfitting. To address\nthese issues, we conduct image denoising by utilizing dynamically generated\nkernels via efficient operations. This approach helps prevent overfitting and\nimproves resilience to unseen noise. Specifically, our method leverages a\nFeature Extraction Module for robust noise-invariant features, Global\nStatistics and Local Correlation Modules to capture comprehensive noise\ncharacteristics and structural correlations. The Kernel Prediction Module then\nemploys these cues to produce pixel-wise varying kernels adapted to local\nstructures, which are then applied iteratively for denoising. This ensures both\nefficiency and superior restoration quality. Despite being trained on\nsingle-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse\nnoise types and levels, demonstrating the promise of iterative dynamic\nfiltering for practical image denoising.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u751f\u6210\u5185\u6838\u7684\u8fed\u4ee3\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u5168\u5c40\u7edf\u8ba1\u548c\u5c40\u90e8\u76f8\u5173\u7279\u5f81\u6765\u9884\u6d4b\u50cf\u7d20\u7ea7\u53d8\u5316\u7684\u53bb\u566a\u5185\u6838\uff0c\u5e76\u5728\u8bad\u7ec3\u6570\u636e\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u566a\u58f0\u7c7b\u578b\u548c\u7ea7\u522b\u7684\u826f\u597d\u6f14\u7eed\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\u5bf9\u7279\u5b9a\u566a\u58f0\u5206\u5e03\u4f9d\u8d56\u6027\u5f3a\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u7efc\u5408\u6027\u80fd\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u826f\u597d\u6f14\u7eed\u6027\u7684\u9ad8\u6548\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3001\u5168\u5c40\u7edf\u8ba1\u6a21\u5757\u3001\u5c40\u90e8\u76f8\u5173\u6a21\u5757\u548c\u5185\u6838\u9884\u6d4b\u6a21\u5757\u7684\u7cfb\u7edf\u3002\u901a\u8fc7\u52a8\u6001\u751f\u6210\u50cf\u7d20\u7ea7\u53d8\u5316\u7684\u53bb\u566a\u5185\u6838\uff0c\u5e76\u8fed\u4ee3\u5e94\u7528\u8fd9\u4e9b\u5185\u6838\u6765\u8fdb\u884c\u56fe\u50cf\u6062\u590d\u3002", "result": "\u8be5\u7b80\u6d01\u6a21\u578b\uff08\u7ea6 0.04M \u53c2\u6570\uff09\u4ec5\u5728\u5355\u4e00\u6b63\u6001\u566a\u58f0\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5728\u591a\u79cd\u566a\u58f0\u7c7b\u578b\u548c\u7ea7\u522b\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u793a\u4e86\u826f\u597d\u7684\u6f14\u7eed\u6027\u80fd\u3002", "conclusion": "\u8fed\u4ee3\u52a8\u6001\u8fc7\u6ee4\u65b9\u6cd5\u4e3a\u5b9e\u9645\u56fe\u50cf\u53bb\u566a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u6f14\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u826f\u597d\u7684\u53bb\u566a\u6548\u679c\u3002"}}
{"id": "2508.19654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19654", "abs": "https://arxiv.org/abs/2508.19654", "authors": ["Matthias H\u00f6fflin", "J\u00fcrgen Wassner"], "title": "Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications", "comment": "Accepted for the IAA-SPAICE 2025 conference", "summary": "Spiking Neural Networks (SNNs), inspired by biological intelligence, have\nlong been considered inherently energy-efficient, making them attractive for\nresource-constrained domains such as space applications. However, recent\ncomparative studies with conventional Artificial Neural Networks (ANNs) have\nbegun to question this reputation, especially for digital implementations. This\nwork investigates SNNs for multi-output regression, specifically 3-D satellite\nposition estimation from monocular images, and compares hardware-aware and\nhardware-agnostic energy estimation methods. The proposed SNN, trained using\nthe membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the\nfinal layer, achieves comparable Mean Squared Error (MSE) to a reference\nConvolutional Neural Network (CNN) on a photorealistic satellite dataset.\nEnergy analysis shows that while hardware-agnostic methods predict a consistent\n50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals\nthat significant energy savings are realized only on neuromorphic hardware and\nwith high input sparsity. The influence of dark pixel ratio on energy\nconsumption is quantified, emphasizing the impact of data characteristics and\nhardware assumptions. These findings highlight the need for transparent\nevaluation methods and explicit disclosure of underlying assumptions to ensure\nfair comparisons of neural network energy efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9SNN\u5728\u536b\u661f\u4f4d\u7f6e\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u80fd\u6548\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0\u786c\u4ef6\u65e0\u5173\u65b9\u6cd5\u9884\u6d4bSNN\u6bd4CNN\u8282\u80fd50-60%\uff0c\u4f46\u786c\u4ef6\u611f\u77e5\u5206\u6790\u663e\u793a\u53ea\u6709\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u548c\u9ad8\u8f93\u5165\u7a00\u758f\u5ea6\u4e0b\u624d\u80fd\u5b9e\u73b0\u663e\u8457\u8282\u80fd\u3002", "motivation": "\u4f20\u7edf\u8ba4\u4e3aSNN\u5177\u6709\u56fa\u6709\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u6570\u5b57\u5b9e\u73b0\u4e2d\u8fd9\u79cd\u4f18\u52bf\u53ef\u80fd\u88ab\u9ad8\u4f30\uff0c\u9700\u8981\u66f4\u900f\u660e\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u516c\u5e73\u6bd4\u8f83\u795e\u7ecf\u7f51\u7edc\u80fd\u6548\u3002", "method": "\u4f7f\u7528LIF\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u8fdb\u884c\u591a\u8f93\u51fa\u56de\u5f52\u8bad\u7ec3\uff0c\u5728\u903c\u771f\u536b\u661f\u6570\u636e\u96c6\u4e0a\u4e0e\u53c2\u8003CNN\u6bd4\u8f83MSE\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u786c\u4ef6\u65e0\u5173\u548c\u786c\u4ef6\u611f\u77e5\u4e24\u79cd\u80fd\u91cf\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "SNN\u8fbe\u5230\u4e0eCNN\u76f8\u5f53\u7684MSE\u6027\u80fd\uff0c\u4f46\u80fd\u91cf\u5206\u6790\u663e\u793aSNN\u7684\u8282\u80fd\u4f18\u52bf\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u786c\u4ef6\u5e73\u53f0\u548c\u8f93\u5165\u6570\u636e\u7a00\u758f\u5ea6\uff08\u6697\u50cf\u7d20\u6bd4\u4f8b\uff09\u3002", "conclusion": "\u9700\u8981\u900f\u660e\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u660e\u786e\u62ab\u9732\u5e95\u5c42\u5047\u8bbe\uff0c\u4ee5\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u80fd\u6548\u6bd4\u8f83\u7684\u516c\u5e73\u6027\uff0c\u6570\u636e\u7279\u6027\u548c\u786c\u4ef6\u5047\u8bbe\u5bf9\u80fd\u6548\u8bc4\u4f30\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2508.19705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19705", "abs": "https://arxiv.org/abs/2508.19705", "authors": ["Qiang Hu", "Ying Zhou", "Gepeng Ji", "Nick Barnes", "Qiang Li", "Zhiwei Wang"], "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation", "comment": null, "summary": "Existing video polyp segmentation (VPS) paradigms usually struggle to balance\nbetween spatiotemporal modeling and domain generalization, limiting their\napplicability in real clinical scenarios. To embrace this challenge, we recast\nthe VPS task as a track-by-detect paradigm that leverages the spatial contexts\ncaptured by the image polyp segmentation (IPS) model while integrating the\ntemporal modeling capabilities of segment anything model 2 (SAM2). However,\nduring long-term polyp tracking in colonoscopy videos, SAM2 suffers from error\naccumulation, resulting in a snowball effect that compromises segmentation\nstability. We mitigate this issue by repurposing SAM2 as a video polyp\nsegmenter with two training-free modules. In particular, the intra-association\nfiltering module eliminates spatial inaccuracies originating from the detecting\nstage, reducing false positives. The inter-association refinement module\nadaptively updates the memory bank to prevent error propagation over time,\nenhancing temporal coherence. Both modules work synergistically to stabilize\nSAM2, achieving cutting-edge performance in both in-domain and out-of-domain\nscenarios. Furthermore, we demonstrate the robust tracking capabilities of\nFreeVPS in long-untrimmed colonoscopy videos, underscoring its potential\nreliable clinical analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u89c6\u9891\u6062\u808c\u5206\u5272\u65b9\u6cd5FreeVPS\uff0c\u901a\u8fc7\u7ed3\u5408IPS\u6a21\u578b\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548cSAM2\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u8ddf\u8e2a\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u5728\u5185\u5916\u57df\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6062\u808c\u5206\u5272\u65b9\u6cd5\u5728\u7a7a\u65f6\u95f4\u5efa\u6a21\u548c\u57df\u9006\u5411\u6027\u4e4b\u95f4\u96be\u4ee5\u53d6\u5f97\u5e73\u8861\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u7279\u522b\u662fSAM2\u5728\u957f\u671f\u8ddf\u8e2a\u4e2d\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u7684\u96ea\u7403\u6548\u5e94\u95ee\u9898\u3002", "method": "\u91cd\u6784VPS\u4efb\u52a1\u4e3a\u8ddf\u8e2a-\u68c0\u6d4b\u8303\u5f0f\uff0c\u5229\u7528IPS\u6a21\u578b\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548cSAM2\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u3002\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u514d\u8bad\u7ec3\u6a21\u5757\uff1a\u5185\u8054\u5408\u7b5b\u9009\u6a21\u5757\u6d88\u9664\u7a7a\u95f4\u4e0d\u51c6\u786e\u6027\uff0c\u5916\u8054\u5408\u7cbe\u7ec6\u6a21\u5757\u9632\u6b62\u9519\u8bef\u4f20\u64ad\uff0c\u63d0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u5185\u57df\u548c\u5916\u57df\u573a\u666f\u4e2d\u90fd\u8fbe\u5230\u4e86\u9886\u5148\u6027\u80fd\uff0c\u5e76\u5728\u957f\u65f6\u95f4\u672a\u526a\u8fde\u7eed\u7ec6\u80a0\u955c\u89c6\u9891\u4e2d\u5c55\u73b0\u4e86\u7a33\u5065\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "FreeVPS\u901a\u8fc7\u521b\u65b0\u5730\u91cd\u6784\u95ee\u9898\u548c\u514d\u8bad\u7ec3\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6062\u808c\u5206\u5272\u4e2d\u7684\u7a7a\u65f6\u95f4\u5e73\u8861\u548c\u9519\u8bef\u7d2f\u79ef\u6311\u6218\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.19746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19746", "abs": "https://arxiv.org/abs/2508.19746", "authors": ["Qiyao Xu", "Qiming Wu", "Xiaowei Li"], "title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection", "comment": null, "summary": "Segment Anything Model (SAM) has demonstrated remarkable capabilities in\nsolving light field salient object detection (LF SOD). However, most existing\nmodels tend to neglect the extraction of prompt information under this task.\nMeanwhile, traditional models ignore the analysis of frequency-domain\ninformation, which leads to small objects being overwhelmed by noise. In this\npaper, we put forward a novel model called self-prompting light field segment\nanything model (SPLF-SAM), equipped with unified multi-scale feature embedding\nblock (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is\ncapable of identifying multiple objects of varying sizes, while MAFA, by\nlearning frequency features, effectively prevents small objects from being\noverwhelmed by noise. Extensive experiments have demonstrated the superiority\nof our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be\navailable at https://github.com/XucherCH/splfsam.", "AI": {"tldr": "SPLF-SAM\u662f\u4e00\u4e2a\u81ea\u63d0\u793a\u5149\u573a\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u591a\u5c3a\u5ea6\u7279\u5f81\u5d4c\u5165\u5757\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u6ee4\u6ce2\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u63d0\u793a\u4fe1\u606f\u63d0\u53d6\u548c\u9891\u57df\u4fe1\u606f\u5206\u6790\u7684\u95ee\u9898\uff0c\u5728\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e10\u4e2aSOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5f80\u5f80\u5ffd\u7565\u63d0\u793a\u4fe1\u606f\u7684\u63d0\u53d6\uff0c\u540c\u65f6\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u9891\u57df\u4fe1\u606f\u5206\u6790\uff0c\u5bfc\u81f4\u5c0f\u76ee\u6807\u88ab\u566a\u58f0\u6df9\u6ca1\u3002", "method": "\u63d0\u51faSPLF-SAM\u6a21\u578b\uff0c\u5305\u542b\u7edf\u4e00\u591a\u5c3a\u5ea6\u7279\u5f81\u5d4c\u5165\u5757(UMFEB)\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u6ee4\u6ce2\u9002\u914d\u5668(MAFA)\u3002UMFEB\u80fd\u8bc6\u522b\u4e0d\u540c\u5927\u5c0f\u7684\u591a\u4e2a\u76ee\u6807\uff0cMAFA\u901a\u8fc7\u5b66\u4e60\u9891\u57df\u7279\u5f81\u6709\u6548\u9632\u6b62\u5c0f\u76ee\u6807\u88ab\u566a\u58f0\u6df9\u6ca1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u572810\u4e2a\u6700\u5148\u8fdb\u7684\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "SPLF-SAM\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5d4c\u5165\u548c\u9891\u57df\u6ee4\u6ce2\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5c0f\u76ee\u6807\u548c\u566a\u58f0\u6291\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.19769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19769", "abs": "https://arxiv.org/abs/2508.19769", "authors": ["Shu Shen", "C. L. Philip Chen", "Tong Zhang"], "title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning", "comment": "13pages,7 figures", "summary": "Multimodal learning has significantly enhanced machine learning performance\nbut still faces numerous challenges and limitations. Imbalanced multimodal\nlearning is one of the problems extensively studied in recent works and is\ntypically mitigated by modulating the learning of each modality. However, we\nfind that these methods typically hinder the dominant modality's learning to\npromote weaker modalities, which affects overall multimodal performance. We\nanalyze the cause of this issue and highlight a commonly overlooked problem:\noptimization bias within networks. To address this, we propose Adaptive\nIntra-Network Modulation (AIM) to improve balanced modality learning. AIM\naccounts for differences in optimization state across parameters and depths\nwithin the network during modulation, achieving balanced multimodal learning\nwithout hindering either dominant or weak modalities for the first time.\nSpecifically, AIM decouples the dominant modality's under-optimized parameters\ninto Auxiliary Blocks and encourages reliance on these performance-degraded\nblocks for joint training with weaker modalities. This approach effectively\nprevents suppression of weaker modalities while enabling targeted optimization\nof under-optimized parameters to improve the dominant modality. Additionally,\nAIM assesses modality imbalance level across network depths and adaptively\nadjusts modulation strength at each depth. Experimental results demonstrate\nthat AIM outperforms state-of-the-art imbalanced modality learning methods\nacross multiple benchmarks and exhibits strong generalizability across\ndifferent backbones, fusion strategies, and optimizers.", "AI": {"tldr": "\u63d0\u51fa\u4e86AIM\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u4f18\u5316\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f51\u7edc\u5185\u8c03\u5236\u5b9e\u73b0\u5e73\u8861\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4e0d\u727a\u7272\u4efb\u4f55\u6a21\u6001\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u4e3b\u5bfc\u6a21\u6001\u6765\u4fc3\u8fdb\u5f31\u52bf\u6a21\u6001\uff0c\u5f71\u54cd\u4e86\u6574\u4f53\u591a\u6a21\u6001\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u6839\u672c\u539f\u56e0\u662f\u7f51\u7edc\u5185\u90e8\u7684\u4f18\u5316\u504f\u5dee\u95ee\u9898", "method": "AIM\u65b9\u6cd5\u5c06\u4e3b\u5bfc\u6a21\u6001\u7684\u672a\u4f18\u5316\u53c2\u6570\u89e3\u8026\u5230\u8f85\u52a9\u5757\u4e2d\uff0c\u9f13\u52b1\u4e0e\u5f31\u52bf\u6a21\u6001\u8054\u5408\u8bad\u7ec3\u65f6\u4f9d\u8d56\u8fd9\u4e9b\u6027\u80fd\u4e0b\u964d\u7684\u5757\uff0c\u540c\u65f6\u6839\u636e\u7f51\u7edc\u6df1\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u8c03\u5236\u5f3a\u5ea6", "result": "AIM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e0d\u5e73\u8861\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u3001\u878d\u5408\u7b56\u7565\u548c\u4f18\u5316\u5668\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "AIM\u9996\u6b21\u5b9e\u73b0\u4e86\u4e0d\u727a\u7272\u4efb\u4f55\u6a21\u6001\u6027\u80fd\u7684\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f51\u7edc\u5185\u90e8\u4f18\u5316\u504f\u5dee\u95ee\u9898"}}
{"id": "2508.19997", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19997", "abs": "https://arxiv.org/abs/2508.19997", "authors": ["Boheng Mao"], "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9009\u62e9\u6027\u68c0\u7d22\u589e\u5f3a(SRA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u4f4e\u9891\u6807\u7b7e\u6837\u672c\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\uff0c\u89e3\u51b3\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u4e2d\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u7684\u7f55\u89c1\u7c7b\u522b\u6027\u80fd\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u957f\u5c3e\u6807\u7b7e\u5206\u5e03\uff0c\u8bb8\u591a\u6807\u7b7e\u6837\u672c\u4e0d\u8db3\u5bfc\u81f4\u6a21\u578b\u5728\u7f55\u89c1\u7c7b\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "\u9009\u62e9\u6027\u68c0\u7d22\u589e\u5f3a(SRA)\u65b9\u6cd5\uff0c\u4ec5\u5bf9\u4f4e\u9891\u6807\u7b7e\u7684\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\uff0c\u907f\u514d\u4e3a\u5145\u5206\u8868\u793a\u7684\u7c7b\u522b\u5f15\u5165\u566a\u58f0\uff0c\u4e14\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\uff0c\u4ec5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u68c0\u7d22\u4ee5\u907f\u514d\u4fe1\u606f\u6cc4\u9732\u3002", "result": "\u5728LEDGAR(\u5355\u6807\u7b7e)\u548cUNFAIR-ToS(\u591a\u6807\u7b7e)\u4e24\u4e2a\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSRA\u65b9\u6cd5\u76f8\u6bd4\u6240\u6709\u5f53\u524dLexGLUE\u57fa\u7ebf\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684micro-F1\u548cmacro-F1\u5206\u6570\u3002", "conclusion": "SRA\u65b9\u6cd5\u5728\u6cd5\u5f8b\u6587\u672c\u957f\u5c3e\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u89e3\u51b3\u6cd5\u5f8b\u9886\u57df\u7f55\u89c1\u7c7b\u522b\u5206\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19806", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19806", "abs": "https://arxiv.org/abs/2508.19806", "authors": ["Shenqi Wang", "Guangzhi Tang"], "title": "Context-aware Sparse Spatiotemporal Learning for Event-based Vision", "comment": "Accepted at IROS 2025", "summary": "Event-based camera has emerged as a promising paradigm for robot perception,\noffering advantages with high temporal resolution, high dynamic range, and\nrobustness to motion blur. However, existing deep learning-based event\nprocessing methods often fail to fully leverage the sparse nature of event\ndata, complicating their integration into resource-constrained edge\napplications. While neuromorphic computing provides an energy-efficient\nalternative, spiking neural networks struggle to match of performance of\nstate-of-the-art models in complex event-based vision tasks, like object\ndetection and optical flow. Moreover, achieving high activation sparsity in\nneural networks is still difficult and often demands careful manual tuning of\nsparsity-inducing loss terms. Here, we propose Context-aware Sparse\nSpatiotemporal Learning (CSSL), a novel framework that introduces context-aware\nthresholding to dynamically regulate neuron activations based on the input\ndistribution, naturally reducing activation density without explicit sparsity\nconstraints. Applied to event-based object detection and optical flow\nestimation, CSSL achieves comparable or superior performance to\nstate-of-the-art methods while maintaining extremely high neuronal sparsity.\nOur experimental results highlight CSSL's crucial role in enabling efficient\nevent-based vision for neuromorphic processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86CSSL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u9608\u503c\u52a8\u6001\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u5728\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u7a00\u758f\u6027\u548c\u4f18\u5f02\u6027\u80fd", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u7684\u7a00\u758f\u6027\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u4e14\u9ad8\u6fc0\u6d3b\u7a00\u758f\u6027\u96be\u4ee5\u5b9e\u73b0", "method": "Context-aware Sparse Spatiotemporal Learning (CSSL)\u6846\u67b6\uff0c\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u9608\u503c\u673a\u5236\uff0c\u6839\u636e\u8f93\u5165\u5206\u5e03\u52a8\u6001\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u65e0\u9700\u663e\u5f0f\u7a00\u758f\u7ea6\u675f", "result": "\u5728\u4e8b\u4ef6\u76f8\u673a\u76ee\u6807\u68c0\u6d4b\u548c\u5149\u6d41\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u9ad8\u7684\u795e\u7ecf\u5143\u7a00\u758f\u6027", "conclusion": "CSSL\u4e3a\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5b9e\u73b0\u9ad8\u6548\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u5173\u952e\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.20066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20066", "abs": "https://arxiv.org/abs/2508.20066", "authors": ["Zheng Li", "Yanming Guo", "WenZhe Liu", "Xueyi Zhang", "Zhaoyun Ding", "Long Xu", "Mingrui Lao"], "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence", "comment": "10 pages", "summary": "Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PAUL\u6846\u67b6\u6765\u89e3\u51b3\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u548c\u9009\u62e9\u6027\u589e\u5f3a\u6765\u5904\u7406GPS\u6f02\u79fb\u5bfc\u81f4\u7684\u56fe\u50cf\u5bf9\u672a\u5bf9\u9f50\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u56fe\u50cf\u5bf9\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2dGPS\u6f02\u79fb\u7b49\u56e0\u7d20\u5bfc\u81f4\u7cfb\u7edf\u6027\u7684\u5bf9\u9f50\u504f\u79fb\uff0c\u53ea\u6709\u90e8\u5206\u5bf9\u5e94\u5173\u7cfb\u5b58\u5728\uff0c\u8fd9\u79cd\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\u5728\u5f53\u524d\u7814\u7a76\u4e2d\u5173\u6ce8\u4e0d\u8db3", "method": "\u63d0\u51faPAUL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u534f\u540c\u589e\u5f3a\u548c\u8bc1\u636e\u534f\u540c\u8bad\u7ec3\u6765\u4f30\u8ba1\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff0c\u57fa\u4e8e\u6b64\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5206\u533a\u548c\u589e\u5f3a\u3002\u9009\u62e9\u6027\u5730\u589e\u5f3a\u5177\u6709\u9ad8\u5bf9\u5e94\u7f6e\u4fe1\u5ea6\u7684\u533a\u57df\uff0c\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u7ec6\u5316\u7279\u5f81\u5b66\u4e60", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PAUL\u5404\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c\u5728\u5404\u79cd\u566a\u58f0\u6bd4\u4f8b\u4e0b\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u7ade\u4e89\u6027\u566a\u58f0\u5bf9\u5e94\u9a71\u52a8\u65b9\u6cd5\u7684\u6027\u80fd", "conclusion": "PAUL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u4e3a\u7406\u60f3\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u642d\u5efa\u4e86\u6865\u6881\uff0c\u63d0\u4f9b\u4e86\u5bf9\u566a\u58f0\u6837\u672c\u7684\u9c81\u68d2\u76d1\u7763"}}
{"id": "2508.20088", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20088", "abs": "https://arxiv.org/abs/2508.20088", "authors": ["Yuxin Guo", "Teng Wang", "Yuying Ge", "Shijie Ma", "Yixiao Ge", "Wei Zou", "Ying Shan"], "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language Models", "comment": null, "summary": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory", "AI": {"tldr": "AudioStory\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548cTTA\u7cfb\u7edf\u6765\u751f\u6210\u957f\u7bc7\u53d9\u4e8b\u97f3\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u5e8f\u8fde\u8d2f\u6027\u548c\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6280\u672f\u5728\u5408\u6210\u77ed\u97f3\u9891\u7247\u6bb5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u957f\u7bc7\u53d9\u4e8b\u97f3\u9891\u65f6\u5b58\u5728\u65f6\u5e8f\u8fde\u8d2f\u6027\u548c\u7ec4\u5408\u63a8\u7406\u7684\u56f0\u96be\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faAudioStory\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u590d\u6742\u53d9\u4e8b\u67e5\u8be2\u5206\u89e3\u4e3a\u65f6\u5e8f\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u89e3\u8026\u7684\u6865\u63a5\u673a\u5236\uff08\u8bed\u4e49\u5bf9\u9f50\u6865\u63a5\u67e5\u8be2\u548c\u8fde\u8d2f\u6027\u4fdd\u6301\u6b8b\u5dee\u67e5\u8be2\uff09\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAudioStory\u5728\u5355\u97f3\u9891\u751f\u6210\u548c\u53d9\u4e8b\u97f3\u9891\u751f\u6210\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u97f3\u9891\u4fdd\u771f\u5ea6\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AudioStory\u901a\u8fc7\u6574\u5408LLM\u548cTTA\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u97f3\u9891\u53d9\u4e8b\u7684\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6AudioStory-10K\uff0c\u4e3a\u957f\u683c\u5f0f\u97f3\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20096", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20096", "abs": "https://arxiv.org/abs/2508.20096", "authors": ["Zeyi Sun", "Yuhang Cao", "Jianze Liang", "Qiushi Sun", "Ziyu Liu", "Zhixiong Zhang", "Yuhang Zang", "Xiaoyi Dong", "Kai Chen", "Dahua Lin", "Jiaqi Wang"], "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning", "comment": "code available at this url: https://github.com/OpenIXCLab/CODA", "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.", "AI": {"tldr": "CODA\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u6574\u5408\u901a\u7528\u89c4\u5212\u5668\u548c\u4e13\u4e1a\u6267\u884c\u5668\uff0c\u5728\u79d1\u5b66\u8ba1\u7b97GUI\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6267\u884c\u7cbe\u5ea6\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3GUI\u81ea\u4e3b\u4ee3\u7406\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\uff1a\u901a\u7528\u4ee3\u7406\u89c4\u5212\u80fd\u529b\u5f3a\u4f46\u6267\u884c\u5dee\uff0c\u4e13\u4e1a\u4ee3\u7406\u6267\u884c\u5f3a\u4f46\u89c4\u5212\u5f31\uff0c\u73b0\u6709\u7ec4\u5408\u6846\u67b6\u9759\u6001\u4e0d\u53ef\u8bad\u7ec3\u65e0\u6cd5\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51faCODA\u6846\u67b6\uff0c\u5305\u542b\u901a\u7528\u89c4\u5212\u5668Cerebrum\u548c\u4e13\u4e1a\u6267\u884c\u5668Cerebellum\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1)\u4e13\u4e1a\u5316\u9636\u6bb5-\u4f7f\u7528decoupled GRPO\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u79d1\u5b66\u5e94\u7528\u5355\u72ec\u8bad\u7ec3\u4e13\u5bb6\u89c4\u5212\u5668\uff1b2)\u6cdb\u5316\u9636\u6bb5-\u805a\u5408\u6210\u529f\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728ScienceBenchmark\u7684\u56db\u4e2a\u6311\u6218\u6027\u5e94\u7528\u4e0a\u8bc4\u4f30\uff0cCODA\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CODA\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u7ec4\u5408\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u79d1\u5b66\u8ba1\u7b97GUI\u4efb\u52a1\u4e2d\u7684\u89c4\u5212-\u6267\u884c\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6267\u884c\u80fd\u529b\u548c\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u3002"}}
