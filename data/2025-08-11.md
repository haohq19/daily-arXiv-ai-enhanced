<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD提出了一种自监督框架，通过3D高斯溅射引导的扩散方法从单张图像估计法线，解决了多视角几何不一致性和数据依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据驱动的统计先验，缺乏对光-表面交互的显式建模，导致多视角法线方向冲突，且扩散模型的离散采样机制阻碍了3D几何误差的反向传播。

Method: 结合物理驱动的光交互建模和可微分渲染重投影策略，构建光交互驱动的3DGS重参数化模型，设计跨域特征融合模块和可微分3D重投影损失策略。

Result: 在Google Scanned Objects数据集上，SINGAD在多个指标上优于现有方法。

Conclusion: SINGAD通过自监督优化和几何误差传播，显著提升了单图像法线估计的准确性和一致性。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [2] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene是一个统一框架，解决了3D场景重建与4D人体动画无缝集成的挑战，包括位置放置、风格对齐和相机轨迹插入。


<details>
  <summary>Details</summary>
Motivation: 将重建的3D场景与4D人体动画无缝集成存在位置、光照和相机轨迹的挑战，需要一种统一的解决方案。

Method: 设计了位置放置模块、无训练风格对齐方法和联合后重建方法，以实现几何细节和时空一致性。

Result: 实验表明，AnimateScene能生成具有高几何细节和时空一致性的动态场景视频。

Conclusion: AnimateScene有效解决了3D场景与4D动画的集成问题，实现了视觉上吸引人的结果。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [3] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive提出了一种基于门控Mamba融合的端到端自动驾驶框架，通过几何增强的LiDAR表示和高效的空间感知状态空间模型，克服了传统Transformer的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的自动驾驶系统依赖Transformer融合，存在计算复杂度高和缺乏空间先验的问题，限制了高分辨率特征的使用和对BEV表示的有效建模。

Method: 1. 使用几何增强的LiDAR表示替代传统直方图表示；2. 提出门控Mamba融合架构（GM-Fusion），用高效的状态空间模型替代Transformer。

Result: 在NAVSIM基准测试中，GMF-Drive表现优于DiffusionDrive，达到新的SOTA性能。

Conclusion: 任务特定的状态空间模型在自动驾驶中性能和效率上均优于通用Transformer。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [4] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet是一个轻量级的两阶段网络，通过状态空间增强的交叉注意力处理事件像素修复和去马赛克，显著提升了HybridEVS相机的图像质量。


<details>
  <summary>Details</summary>
Motivation: HybridEVS相机结合Quad Bayer CFA传感器和事件像素时，缺乏颜色信息导致去马赛克过程中出现伪影和混叠问题，现有方法难以在资源有限的移动设备上解决这些问题。

Method: TSANet采用两阶段网络，分别处理事件像素修复和去马赛克，并引入轻量级的Cross-Swin State Block，利用位置先验和状态空间模型增强全局依赖。

Result: TSANet在模拟和真实HybridEVS数据上表现优异，PSNR和SSIM均优于现有方法DemosaicFormer，同时参数和计算成本分别降低1.86倍和3.29倍。

Conclusion: TSANet为移动设备上的高效图像去马赛克提供了新可能，代码已公开。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [5] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 该研究比较了PCA、CAE和预训练残差网络在卫星图像天气事件分类中的表现，发现CAE效果最佳，但缺乏物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索不同表示学习算法在卫星图像天气事件分类中的效果，并评估其潜在空间的性能。

Method: 使用PCA、CAE和预训练残差网络（PT）对卫星图像进行表示学习，并评估分类任务的表现。

Result: CAE在所有分类任务中表现最佳，PCA命中率高但误报率也高，PT在热带气旋识别中表现突出。高分辨率数据集对深度学习算法更有利。

Conclusion: CAE高效但缺乏物理可解释性，未来可开发物理信息增强的CAE版本。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [6] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 研究提出了一种多方向架构框架，结合合成数据生成和检测分割算法，用于自动化结肠镜图像中的息肉检测，显著提升了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜是结直肠癌早期诊断的关键工具，但医疗数据集有限且标注复杂，研究旨在解决这些问题并提升自动化检测效率。

Method: 采用Faster R-CNN进行初始目标定位，结合Segment Anything Model（SAM）优化分割掩码，并评估了五种分割模型（U-Net、PSPNet、FPN、LinkNet、MANet）。

Result: Faster R-CNN的召回率为93.08%，精确率为88.97%，F1分数为90.98%。FPN在PSNR和SSIM上表现最佳，U-Net在召回率上领先，LinkNet在IoU和Dice分数上表现均衡。

Conclusion: 该框架在息肉检测和分割任务中表现出色，为结直肠癌的早期诊断提供了高效工具。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [7] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: 提出了一种基于多智能体异步协作的犯罪行为预测框架（MA-CBP），通过实时视频流分析和高低语义融合，实现潜在犯罪行为的早期预警。


<details>
  <summary>Details</summary>
Motivation: 城市化加速导致公共场景犯罪行为增多，传统方法难以捕捉高级语义或满足实时需求。

Method: 将视频流转为语义描述，构建因果一致的历史摘要，融合相邻帧进行长短时上下文联合推理。

Result: 在多个数据集上表现优异，为城市公共安全提供有效风险预警。

Conclusion: MA-CBP框架在犯罪行为预测和公共安全预警方面具有潜力。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [8] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: LiLoRA是一种高效的架构扩展方法，用于解决多模态大语言模型在持续视觉指令调整中的灾难性遗忘问题，通过共享LoRA矩阵和低秩分解减少参数冗余，同时引入余弦正则化损失保持表示一致性。


<details>
  <summary>Details</summary>
Motivation: 持续视觉指令调整（CVIT）中，灾难性遗忘和参数效率低下是主要挑战，现有方法扩展整个层导致参数开销大且扩展性差。

Method: LiLoRA共享LoRA矩阵A，对矩阵B进行低秩分解以减少任务特定参数，并引入余弦正则化稳定性损失。

Result: 在多样化CVIT基准测试中，LiLoRA在顺序任务学习中表现优异，显著提高了参数效率。

Conclusion: LiLoRA为CVIT提供了一种高效且可扩展的解决方案，优于现有方法。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [9] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 论文提出了一种基于高斯散射引导的专家混合模型（GS-MoE），用于弱监督视频异常检测（WSVAD），通过专家模型和时序高斯散射损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前弱监督视频异常检测模型在处理复杂异常事件时表现不佳，主要因为模型无法区分异常类型且弱监督信号缺乏精确时序信息。

Method: 提出GS-MoE框架，使用多个专家模型分别捕捉特定异常类型，并通过时序高斯散射损失增强弱监督信号。

Result: 在UCF-Crime数据集上达到91.58%的AUC，并在XD-Violence和MSAD数据集上表现优异。

Conclusion: GS-MoE通过类别特定专家和时序引导，为弱监督视频异常检测设定了新基准。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [10] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT是一种新型多模态大语言模型，专注于视频中异常事件的总结与定位，通过空间和时间有效令牌选择模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理视频异常事件时因信息冗余表现不佳，需要改进以更好地捕捉稀疏的时空异常信息。

Method: 提出VA-GPT模型，包含空间有效令牌选择（SETS）和时间有效令牌生成（TETG）模块，优化视觉编码器与大语言模型的对齐。

Result: 在多个基准测试中表现优于现有方法，并构建了用于微调的指令跟随数据集和跨域评估基准。

Conclusion: VA-GPT通过改进的令牌选择机制显著提升了异常事件分析的准确性，为视频理解领域提供了新思路。

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 该研究提出了一种基于马尔可夫决策过程（MDP）和强化学习策略梯度（RL-PG）的框架，用于优化碰撞避免机动（CAM）决策，以最小化燃料消耗并保持碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 传统CAM策略通常在接近碰撞时间（TCA）前24小时启动机动，可能导致燃料浪费。本研究旨在通过早期决策优化燃料消耗和碰撞风险。

Method: 将CAM建模为连续状态、离散动作和有限时间范围的MDP，结合风险、燃料消耗和轨道几何的解析模型，使用RL-PG算法训练策略。

Result: 在合成和历史碰撞事件中，训练策略显著降低了平均燃料消耗，同时保持了与常规策略相同或更高的碰撞风险保障。

Conclusion: 该框架有效平衡了机动延迟和燃料消耗，为CAM决策提供了更优的解决方案。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [12] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 论文提出了MCRE框架和MCRQ算法，用于平衡离线强化学习中的保守性和性能提升，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，学习策略与行为策略的分布偏移导致OOD动作和过高估计，需要平衡保守性与性能。

Method: 提出MCRE框架，结合TD误差和行为克隆项，并开发MCRQ算法，将其融入离线actor-critic框架。

Result: MCRQ在基准数据集上优于现有基线方法和最先进的离线RL算法。

Conclusion: MCRE和MCRQ有效解决了离线RL中的保守性与性能平衡问题，具有显著优势。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [13] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出一种新型多输出脉冲神经元模型，结合线性状态转移和非线性反馈机制，提升性能。


<details>
  <summary>Details</summary>
Motivation: 结合SNN的低延迟、高效能与深度SSM的高性能，解决其缺乏重置机制和高精度激活的问题。

Method: 设计多输出脉冲神经元模型，明确区分脉冲功能、重置条件和重置动作。

Result: 在多项任务中表现与现有SNN基准相当，重置机制克服了不稳定性。

Conclusion: 提出的重置机制扩展了深度SSM的应用范围，突破了线性动态的稳定性限制。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 论文提出了一种自适应探索策略优化（AEPO）框架，通过多答案生成策略和自适应探索奖励（AER）函数，解决了多模态大语言模型（MLLMs）在图形用户界面（GUI）中语义对齐的探索瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在GUI操作中面临语义对齐的挑战，现有方法（如RLVR）在空间对齐上有效，但在语义对齐上因探索效率低而受限。

Method: 提出AEPO框架，结合多答案生成策略和基于效率理论的自适应探索奖励（AER）函数，以提升语义对齐能力。

Result: AEPO训练的模型（InfiGUI-G1-3B和InfiGUI-G1-7B）在多个GUI基准测试中取得新SOTA，相对RLVR基线提升高达9.0%。

Conclusion: AEPO有效解决了语义对齐的探索瓶颈，显著提升了MLLMs在GUI任务中的性能。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: 在Transformer模型中，自注意力机制的二次计算成本是主要挑战。本文发现，通过在后处理中引入结构化稀疏性，模型精度反而显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究注意力稀疏性是否能在不牺牲模型精度的情况下提升计算效率，并探索其潜在的正则化效果。

Method: 在DistilBERT模型的微调过程中，引入结构化后处理稀疏性，应用于SST-2情感分析任务。

Result: 80%稀疏性的模型验证精度达到91.59%，比密集基线提升0.97%。

Conclusion: 注意力稀疏性不仅是计算效率工具，还能通过正则化提升模型的泛化能力和性能。

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts](https://arxiv.org/abs/2508.05937)
*Gen Sako,Takuya Kiyokawa,Kensuke Harada,Tomoki Ishikura,Naoya Miyaji,Genichiro Matsuda*

Main category: cs.RO

TL;DR: 提出了一种基于直觉人机协作的双臂拆卸系统，通过虚拟环境和混合控制器提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人非破坏性拆卸中因内部结构不可见和操作灵活性不足带来的挑战。

Method: 采用基于几何的虚拟环境可视化抓取位姿和拆卸方向，并结合位置与阻抗控制的混合控制器。

Result: 实验验证了系统的有效性，任务成功率提高且物体位姿偏差减少。

Conclusion: 该系统为复杂拆卸任务提供了一种高效且直观的解决方案。

Abstract: Robotic non-destructive disassembly of mating parts remains challenging due
to the need for flexible manipulation and the limited visibility of internal
structures. This study presents an affordance-guided teleoperation system that
enables intuitive human demonstrations for dual-arm fix-and-disassemble tasks
for mating parts. The system visualizes feasible grasp poses and disassembly
directions in a virtual environment, both derived from the object's geometry,
to address occlusions and structural complexity. To prevent excessive position
tracking under load when following the affordance, we integrate a hybrid
controller that combines position and impedance control into the teleoperated
disassembly arm. Real-world experiments validate the effectiveness of the
proposed system, showing improved task success rates and reduced object pose
deviation.

</details>


### [17] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 论文提出了一种基于变分自编码器的新颖性检测方法，以提高模型规划算法对学习世界模型质量的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉世界模型对训练质量敏感，需要近乎完整的动作和状态空间覆盖以防止推理时发散。

Method: 使用变分自编码器作为新颖性检测器，确保规划中的动作轨迹不偏离训练数据分布。

Result: 在模拟机器人环境中实验表明，该方法在数据效率上优于现有解决方案。

Conclusion: 提出的方法显著提升了模型规划算法的鲁棒性和数据效率。

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [18] [Situationally-aware Path Planning Exploiting 3D Scene Graphs](https://arxiv.org/abs/2508.06283)
*Saad Ejaz,Marco Giberna,Muhammad Shaheer,Jose Andres Millan-Romera,Ali Tourani,Paul Kremer,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: S-Path利用3D场景图的语义结构，通过两阶段规划和子问题分解，显著提升路径规划效率，同时保持路径最优性。


<details>
  <summary>Details</summary>
Motivation: 3D场景图的语义结构未被充分利用，S-Path旨在通过利用这一结构提升路径规划的效率和可解释性。

Method: 采用两阶段规划：先在语义图上搜索高层路径，再分解为并行子问题；引入重规划机制以复用信息。

Result: 实验表明，S-Path平均减少5.7倍规划时间，路径最优性与传统方法相当，复杂场景表现更优。

Conclusion: S-Path是一种高效且可解释的路径规划方法，适用于3D场景图表示的环境。

Abstract: 3D Scene Graphs integrate both metric and semantic information, yet their
structure remains underutilized for improving path planning efficiency and
interpretability. In this work, we present S-Path, a situationally-aware path
planner that leverages the metric-semantic structure of indoor 3D Scene Graphs
to significantly enhance planning efficiency. S-Path follows a two-stage
process: it first performs a search over a semantic graph derived from the
scene graph to yield a human-understandable high-level path. This also
identifies relevant regions for planning, which later allows the decomposition
of the problem into smaller, independent subproblems that can be solved in
parallel. We also introduce a replanning mechanism that, in the event of an
infeasible path, reuses information from previously solved subproblems to
update semantic heuristics and prioritize reuse to further improve the
efficiency of future planning attempts. Extensive experiments on both
real-world and simulated environments show that S-Path achieves average
reductions of 5.7x in planning time while maintaining comparable path
optimality to classical sampling-based planners and surpassing them in complex
scenarios, making it an efficient and interpretable path planner for
environments represented by indoor 3D Scene Graphs.

</details>
