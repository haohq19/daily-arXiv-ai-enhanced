{"id": "2511.20696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20696", "abs": "https://arxiv.org/abs/2511.20696", "authors": ["Dan Li", "Hye-Bin Shin", "Yeon-Woo Choi"], "title": "Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding", "comment": "4 pages, 2 figures, 14th IEEE International Winter Conference on Brain-Computer Interface Conference 2026", "summary": "Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.", "AI": {"tldr": "\u63d0\u51faProNECL\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b\u5f15\u5bfc\u7684\u975e\u793a\u4f8b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8de8\u4e2a\u4f53EEG\u89e3\u7801\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u5b58\u50a8\u5386\u53f2EEG\u6570\u636e\u3002", "motivation": "\u7531\u4e8eEEG\u4fe1\u53f7\u7684\u4e2a\u4f53\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u5728\u6301\u7eedEEG\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u5f15\u5165\u65b0\u4e2a\u4f53\u65f6\u4f1a\u8986\u76d6\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b58\u50a8\u5386\u53f2\u6570\u636e\u4f5c\u4e3a\u56de\u653e\u7f13\u51b2\u533a\uff0c\u4f46\u5b58\u5728\u9690\u79c1\u548c\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "method": "\u6784\u5efa\u7c7b\u7ea7\u522b\u539f\u578b\u6765\u603b\u7ed3\u6bcf\u4e2a\u4e2a\u4f53\u7684\u5224\u522b\u6027\u8868\u793a\uff0c\u901a\u8fc7\u8de8\u4e2a\u4f53\u7279\u5f81\u5bf9\u9f50\u548c\u77e5\u8bc6\u84b8\u998f\u9010\u6b65\u5c06\u65b0\u7279\u5f81\u7a7a\u95f4\u4e0e\u5168\u5c40\u539f\u578b\u8bb0\u5fc6\u5bf9\u9f50\u3002", "result": "\u5728BCI Competition IV 2a\u548c2b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u6709\u6548\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u548c\u9002\u5e94\u6027\uff0c\u5728\u8de8\u4e2a\u4f53\u6301\u7eedEEG\u89e3\u7801\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ProNECL\u6846\u67b6\u80fd\u591f\u5728\u65e0\u9700\u8bbf\u95ee\u5386\u53f2EEG\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e3a\u6301\u7eedEEG\u89e3\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20695", "categories": ["cs.AI", "cs.CY", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.20695", "abs": "https://arxiv.org/abs/2511.20695", "authors": ["Yunqi Zhang", "Kuangyu Shi", "Biao Li"], "title": "A Brief History of Digital Twin Technology", "comment": "21 pages, 1 figure, 1 table", "summary": "Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.", "AI": {"tldr": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4eceNASA\u822a\u5929\u5668\u6a21\u62df\u53d1\u5c55\u800c\u6765\uff0c\u73b0\u6b63\u63a8\u52a8\u533b\u7597\u5065\u5eb7\u8f6c\u578b\uff0c\u901a\u8fc7\u521b\u5efa\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u6a21\u578b\u6765\u652f\u6301\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u836f\u7269\u5f00\u53d1\uff0c\u4f46\u4ecd\u9762\u4e34\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u9690\u79c1\u7b49\u6311\u6218\u3002", "motivation": "\u5c06\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4ece\u5de5\u4e1a\u9886\u57df\u5f15\u5165\u533b\u7597\u5065\u5eb7\uff0c\u65e8\u5728\u5b9e\u73b0\u4ece\u88ab\u52a8\u6cbb\u7597\u5411\u9884\u6d4b\u6027\u3001\u9884\u9632\u6027\u548c\u4e2a\u6027\u5316\u533b\u7597\u7684\u8f6c\u53d8\uff0c\u901a\u8fc7\u521b\u5efa\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u6a21\u578b\u6765\u4f18\u5316\u533b\u7597\u51b3\u7b56\u3002", "method": "\u6574\u5408\u533b\u5b66\u5f71\u50cf\u3001\u751f\u7269\u4f20\u611f\u5668\u548c\u8ba1\u7b97\u6a21\u578b\uff0c\u6784\u5efa\u52a8\u6001\u6570\u636e\u9a71\u52a8\u7684\u60a3\u8005\u865a\u62df\u5bf9\u5e94\u4f53\uff0c\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u6d41\u6301\u7eed\u66f4\u65b0\uff0c\u652f\u6301\u53cc\u5411\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u6a21\u62df\u3002", "result": "\u4ee3\u8868\u6027\u5e94\u7528\u5305\u62ec\uff1a\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u5fc3\u5f8b\u5931\u5e38\u6cbb\u7597\u7ed3\u679c\u3001\u80bf\u7624\u5b66\u6570\u5b57\u5b6a\u751f\u8ddf\u8e2a\u80bf\u7624\u8fdb\u5c55\u4f18\u5316\u653e\u7597\u3001\u836f\u7406\u5b66\u6570\u5b57\u5b6a\u751f\u52a0\u901f\u836f\u7269\u53d1\u73b0\u3002", "conclusion": "\u867d\u7136\u9762\u4e34\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u9690\u79c1\u548c\u6a21\u578b\u4fdd\u771f\u5ea6\u7b49\u6311\u6218\uff0c\u4f46\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u548c\u7edf\u4e00\u76d1\u7ba1\u6846\u67b6\u7b49\u65b0\u5174\u89e3\u51b3\u65b9\u6848\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\uff0c\u591a\u5668\u5b98\u6570\u5b57\u5b6a\u751f\u3001\u57fa\u56e0\u7ec4\u5b66\u6574\u5408\u548c\u4f26\u7406\u6cbb\u7406\u5c06\u662f\u5173\u952e\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2511.20718", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20718", "abs": "https://arxiv.org/abs/2511.20718", "authors": ["Chenliang Li", "Adel Elmahdy", "Alex Boyd", "Zhongruo Wang", "Alfredo Garcia", "Parminder Bhatia", "Taha Kass-Hout", "Cao Xiao", "Mingyi Hong"], "title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training", "comment": null, "summary": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7a33\u5b9a\u591a\u8f6e\u5bf9\u8bdd\u4e2dPPO\u8bad\u7ec3\u7684\u6280\u672f\uff1a\u8f6e\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u548c\u88c1\u526a\u504f\u5dee\u6821\u6b63\uff0c\u89e3\u51b3\u4e86token\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u4e0e\u591a\u8f6e\u73af\u5883\u7ed3\u6784\u4e0d\u5339\u914d\u4ee5\u53ca\u79bb\u7b56\u7565\u6837\u672c\u4f18\u52bf\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "PPO\u5728\u591a\u8f6e\u5bf9\u8bdd\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u6027\u80fd\u4e0d\u7a33\u5b9a\u4e14\u5bb9\u6613\u5d29\u6e83\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ectoken\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u4e0e\u591a\u8f6e\u73af\u5883\u7ed3\u6784\u4e0d\u5339\u914d\uff0c\u4ee5\u53ca\u79bb\u7b56\u7565\u6837\u672c\u5bfc\u81f4\u7684\u9ad8\u65b9\u5dee\u68af\u5ea6\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u4e92\u8865\u7684\u7a33\u5b9a\u6280\u672f\uff1a1) \u8f6e\u7ea7\u91cd\u8981\u6027\u91c7\u6837\uff0c\u4f7f\u4f18\u5316\u4e0e\u591a\u8f6e\u63a8\u7406\u7684\u81ea\u7136\u7ed3\u6784\u5bf9\u9f50\uff1b2) \u88c1\u526a\u504f\u5dee\u6821\u6b63\uff0c\u901a\u8fc7\u964d\u4f4e\u4e0d\u53ef\u9760\u7684\u79bb\u7b56\u7565\u6837\u672c\u6743\u91cd\u6765\u5f52\u4e00\u5316\u68af\u5ea6\u3002\u7ec4\u5408\u5f97\u5230\u4e09\u79cd\u53d8\u4f53\uff1aTurn-PPO\u3001S-PPO\u548cST-PPO\u3002", "result": "\u5728\u591a\u8f6e\u641c\u7d22\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cST-PPO\u548cS-PPO\u80fd\u9632\u6b62\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u5d29\u6e83\uff0c\u4fdd\u6301\u8f83\u4f4e\u7684\u88c1\u526a\u6bd4\u7387\uff0c\u5e76\u83b7\u5f97\u6bd4\u6807\u51c6token\u7ea7PPO\u66f4\u9ad8\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u8f6e\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u548c\u88c1\u526a\u504f\u5dee\u6821\u6b63\u4e3a\u7a33\u5b9a\u591a\u8f6eLLM\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20726", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20726", "abs": "https://arxiv.org/abs/2511.20726", "authors": ["Yuhang Wang", "Heye Huang", "Zhenhua Xu", "Kailai Sun", "Baoshen Guo", "Jinhua Zhao"], "title": "Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge", "comment": "24 pages, 6 figures", "summary": "Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7f55\u89c1\u957f\u5c3e\u4e8b\u4ef6\u548c\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u957f\u5c3e\u4e8b\u4ef6\u548c\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7a00\u7f3a\u4f46\u5bf9\u5b89\u5168\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528CVAE\u7f16\u7801\u5386\u53f2\u8f68\u8ff9\u548c\u5730\u56fe\u4fe1\u606f\u5b66\u4e60\u6f5c\u5728\u4ea4\u901a\u7ed3\u6784\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u57fa\u7840\u573a\u666f\uff1b\u5229\u7528LLM\u4f5c\u4e3a\u5bf9\u6297\u63a8\u7406\u5f15\u64ce\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u89e3\u6790\u4e3a\u9886\u57df\u7279\u5b9a\u635f\u5931\u51fd\u6570\uff0c\u52a8\u6001\u6307\u5bfc\u4e0d\u540c\u98ce\u9669\u7ea7\u522b\u7684\u573a\u666f\u751f\u6210\u3002", "result": "\u5728CARLA\u548cSMARTS\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u589e\u52a0\u4e86\u9ad8\u98ce\u9669\u548c\u957f\u5c3e\u4e8b\u4ef6\u7684\u8986\u76d6\u8303\u56f4\uff0c\u6539\u5584\u4e86\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e16\u754c\u4ea4\u901a\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u66b4\u9732\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u6311\u6218\u6027\u7684\u4ea4\u4e92\u573a\u666f\u3002", "conclusion": "\u4e3a\u5b89\u5168\u9a8c\u8bc1\u5efa\u7acb\u4e86\u65b0\u9014\u5f84\uff0c\u80fd\u591f\u5728\u7f55\u89c1\u4f46\u91cd\u8981\u7684\u4e8b\u4ef6\u4e0b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fdb\u884c\u539f\u5219\u6027\u538b\u529b\u6d4b\u8bd5\u3002"}}
{"id": "2511.21005", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.21005", "abs": "https://arxiv.org/abs/2511.21005", "authors": ["Jinpeng Wang", "Chao Li", "Ting Ye", "Mengyuan Zhang", "Wei Liu", "Jian Luan"], "title": "ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.", "AI": {"tldr": "\u63d0\u51faICPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u751f\u6210\u4e0d\u540c\u54cd\u5e94\u7684\u6982\u7387\u6765\u53cd\u6620\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u81ea\u6211\u8bc4\u4f30\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u89e3\u51b3\u73b0\u6709RLVR\u65b9\u6cd5\u4e2d\u7684\u7c97\u7c92\u5ea6\u5956\u52b1\u3001\u5956\u52b1\u566a\u58f0\u548c\u4f4e\u6548\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u7c97\u7c92\u5ea6\u5956\u52b1\u3001\u5956\u52b1\u566a\u58f0\u548c\u4f4e\u6548\u63a2\u7d22\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u71b5\u5d29\u6e83\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "ICPO\u65b9\u6cd5\u8ba1\u7b97\u6bcf\u4e2a\u54cd\u5e94\u7684\u504f\u597d\u4f18\u52bf\u5206\u6570\uff0c\u901a\u8fc7\u6bd4\u8f83\u540c\u4e00\u8f93\u5165\u63d0\u793a\u4e0b\u591a\u4e2a\u54cd\u5e94\u7684\u76f8\u5bf9\u751f\u6210\u6982\u7387\uff0c\u5e76\u5c06\u8be5\u5206\u6570\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7ed3\u5408\u6765\u6307\u5bfc\u63a2\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u901a\u7528\u9886\u57df\u57fa\u51c6\u548c\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cICPO\u76f8\u6bd4GRPO\u80fd\u7a33\u5b9a\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ICPO\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7c97\u7c92\u5ea6\u5956\u52b1\u548c\u5956\u52b1\u566a\u58f0\u95ee\u9898\uff0c\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\u9519\u8bef\uff0c\u589e\u5f3a\u88ab\u4f4e\u4f30\u9ad8\u8d28\u91cf\u54cd\u5e94\u7684\u76f8\u5bf9\u4f18\u52bf\uff0c\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408\u7279\u5b9a\u7b56\u7565\uff0c\u4fc3\u8fdb\u66f4\u5f7b\u5e95\u7684\u63a2\u7d22\u3002"}}
{"id": "2511.20795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20795", "abs": "https://arxiv.org/abs/2511.20795", "authors": ["Souradeep Dutta", "Keshav Bulia", "Neena S Nair"], "title": "Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models", "comment": "7 pages , 4 figures", "summary": "Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.", "AI": {"tldr": "\u672c\u6587\u5bf9KRISP\u6a21\u578b\u8fdb\u884c\u4e86\u8f7b\u91cf\u7ea7\u590d\u73b0\uff0c\u53c2\u6570\u5927\u5e45\u51cf\u5c11\u4f46\u6027\u80fd\u8fbe\u5230\u539f\u7248\u768475%\uff0c\u63ed\u793a\u4e86\u539f\u6a21\u578b\u7684\u8bbe\u8ba1\u7f3a\u9677\u548c\u73b0\u5b9e\u95ee\u9898\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u77e5\u8bc6\u589e\u5f3aVQA\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u539f\u7248KRISP\u6a21\u578b\u867d\u7136\u6709\u6548\uff0c\u4f46\u9700\u8981\u5de5\u4e1a\u7ea7\u8bad\u7ec3\u89c4\u6a21\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u4e14\u4e0e\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\u7d27\u5bc6\u8026\u5408\u3002\u672c\u6587\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6KRISP\uff0c\u63d0\u4f9b\u53c2\u6570\u66f4\u5c11\u7684\u8f7b\u91cf\u7ea7\u590d\u73b0\u7248\u672c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u8fdb\u884c\u8f7b\u91cf\u7ea7\u590d\u73b0\uff0c\u5305\u62ec\u5728\u5408\u6210VQA\u6570\u636e\u4e0a\u7684\u6982\u5ff5\u9a8c\u8bc1\u548c\u5728DAQUAR\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u3002\u6a21\u578b\u91c7\u7528\u4f4e\u53c2\u6570\u914d\u7f6e\uff0c\u53d7\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u9886\u57df\u7ea6\u675f\u3002", "result": "\u590d\u73b0\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u539f\u7248\u768475%\uff0c\u63ed\u793a\u4e86\u539f\u6a21\u578b\u672a\u5145\u5206\u8986\u76d6\u7684\u8bbe\u8ba1\u7f3a\u9677\u548c\u5b9e\u9645\u95ee\u9898\u3002\u6a21\u578b\u80fd\u9632\u6b62AI\u5e7b\u89c9\uff0c\u4ec5\u5728\u77e5\u8bc6\u56fe\u8c31\u9886\u57df\u5185\u751f\u6210\u8f93\u51fa\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u590d\u73b0\u6a21\u578b\u53c2\u6570\u5c11\uff0c\u53ef\u5728\u667a\u80fd\u624b\u673a\u548cAR-VR\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u79bb\u7ebf\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.20804", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20804", "abs": "https://arxiv.org/abs/2511.20804", "authors": ["Kriti Ghosh", "Devjyoti Chakraborty", "Lakshmish Ramaswamy", "Suchendra M. Bhandarkar", "In Kee Kim", "Nancy O'Hare", "Deepak Mishra"], "title": "$\u0394$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer", "comment": null, "summary": "Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $\u0394$-NeRF, a unique modular residual framework for incremental NeRF refinement. $\u0394$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\\% of original size). Experiments on satellite imagery demonstrate that $\u0394$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\\%. $\u0394$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.", "AI": {"tldr": "\u0394-NeRF\u662f\u4e00\u4e2a\u7528\u4e8e\u589e\u91cf\u5f0fNeRF\u4f18\u5316\u7684\u6a21\u5757\u5316\u6b8b\u5dee\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u63a7\u5236\u5668\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u95e8\u63a7\u673a\u5236\u548c\u89c6\u56fe\u9009\u62e9\u7b56\u7565\uff0c\u5b9e\u73b0\u5728\u4e0d\u8bbf\u95ee\u8fc7\u53bb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4f18\u5316NeRF\u6a21\u578b\uff0c\u540c\u65f6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u538b\u7f29\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u73b0\u6709NeRF\u6846\u67b6\u5728\u5f15\u5165\u65b0\u89c6\u56fe\u65f6\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5728\u6570\u636e\u987a\u5e8f\u5230\u8fbe\u573a\u666f\uff08\u5982\u536b\u661f\u5730\u5f62\u5206\u6790\uff09\u4e2d\u7684\u5e94\u7528\u3002\u589e\u91cf\u5f0fNeRF\u4f18\u5316\u7814\u7a76\u4e0d\u8db3\uff0c\u7b80\u5355\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u0394-NeRF\u6846\u67b6\uff1a1\uff09\u6b8b\u5dee\u63a7\u5236\u5668\u5411\u51bb\u7ed3\u7684\u57fa\u7840NeRF\u6ce8\u5165\u9010\u5c42\u4fee\u6b63\uff1b2\uff09\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u7ed3\u5408\u57fa\u7840\u548c\u4f18\u5316\u9884\u6d4b\uff1b3\uff09\u89c6\u56fe\u9009\u62e9\u7b56\u7565\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\uff1b4\uff09\u77e5\u8bc6\u84b8\u998f\u538b\u7f29\u6a21\u578b\u3002", "result": "\u5728\u536b\u661f\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u0394-NeRF\u6027\u80fd\u4e0e\u8054\u5408\u8bad\u7ec3\u76f8\u5f53\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1130-42%\uff0cPSNR\u6bd4\u7b80\u5355\u5fae\u8c03\u63d0\u534743.5%\uff0c\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u8d85\u8fc7\u8054\u5408\u8bad\u7ec3\u3002", "conclusion": "\u0394-NeRF\u6709\u6548\u89e3\u51b3\u4e86NeRF\u589e\u91cf\u4f18\u5316\u95ee\u9898\uff0c\u5728\u4e0d\u8bbf\u95ee\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u6570\u636e\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7d27\u51d1\u6027\u3002"}}
{"id": "2511.21444", "categories": ["cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.21444", "abs": "https://arxiv.org/abs/2511.21444", "authors": ["Zhe Jiang", "Jiong Wang", "Xiaoyu Yue", "Zijie Guo", "Wenlong Zhang", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "title": "EWE: An Agentic Framework for Extreme Weather Analysis", "comment": null, "summary": "Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.", "AI": {"tldr": "EWE\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u6781\u7aef\u5929\u6c14\u81ea\u52a8\u8bca\u65ad\u63a8\u7406\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u6c14\u8c61\u6570\u636e\u81ea\u4e3b\u751f\u6210\u548c\u89e3\u91ca\u591a\u6a21\u6001\u53ef\u89c6\u5316\uff0c\u5b9e\u73b0\u5168\u9762\u8bca\u65ad\u5206\u6790\u3002", "motivation": "\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u5bf9\u5168\u7403\u793e\u4f1a\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u98ce\u9669\uff0c\u4f46\u5f53\u524d\u4e13\u5bb6\u9a71\u52a8\u3001\u52b3\u52a8\u5bc6\u96c6\u7684\u8bca\u65ad\u8303\u5f0f\u9020\u6210\u4e86\u5173\u952e\u5206\u6790\u74f6\u9888\uff0c\u963b\u788d\u4e86\u79d1\u5b66\u8fdb\u5c55\u3002\u867d\u7136AI\u5728\u5730\u7403\u79d1\u5b66\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u81ea\u52a8\u8bca\u65ad\u63a8\u7406\u8fd9\u4e00\u540c\u7b49\u91cd\u8981\u7684\u6311\u6218\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "EWE\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u89c4\u5212\u3001\u95ed\u73af\u63a8\u7406\u548c\u9886\u57df\u5b9a\u5236\u7684\u6c14\u8c61\u5de5\u5177\u5305\u6765\u6a21\u62df\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\u3002\u5b83\u80fd\u591f\u4ece\u539f\u59cb\u6c14\u8c61\u6570\u636e\u81ea\u4e3b\u751f\u6210\u548c\u89e3\u91ca\u591a\u6a21\u6001\u53ef\u89c6\u5316\uff0c\u5b9e\u73b0\u5168\u9762\u8bca\u65ad\u5206\u6790\u3002", "result": "\u4e3a\u4fc3\u8fdb\u8be5\u65b0\u5174\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u9996\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b103\u4e2a\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u7684\u7cbe\u9009\u6570\u636e\u96c6\u548c\u65b0\u7684\u9010\u6b65\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "EWE\u6807\u5fd7\u7740\u5411\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5e76\u6709\u671b\u6c11\u4e3b\u5316\u4e13\u4e1a\u77e5\u8bc6\u548c\u667a\u529b\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5bf9\u6613\u53d7\u6781\u7aef\u5929\u6c14\u5f71\u54cd\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u3002"}}
{"id": "2511.20889", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20889", "abs": "https://arxiv.org/abs/2511.20889", "authors": ["Taehoon Kim", "Henry Gouk", "Timothy Hospedales"], "title": "Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation", "comment": null, "summary": "Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.", "AI": {"tldr": "\u63d0\u51faNull-Text Test-Time Alignment (Null-TTA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u4e2d\u7684\u65e0\u6761\u4ef6\u5d4c\u5165\u6765\u5bf9\u9f50\u6269\u6563\u6a21\u578b\uff0c\u907f\u514d\u5956\u52b1\u7834\u89e3\u95ee\u9898\uff0c\u5728\u8bed\u4e49\u7a7a\u95f4\u5b9e\u73b0\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u5956\u52b1\u6b20\u4f18\u5316\u6216\u8fc7\u4f18\u5316\uff08\u5956\u52b1\u7834\u89e3\uff09\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5bf9\u9f50\u76ee\u6807\u5956\u52b1\u540c\u65f6\u4fdd\u6301\u8de8\u5956\u52b1\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u4f18\u5316\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u4e2d\u7684\u65e0\u6761\u4ef6\u5d4c\u5165\u800c\u975e\u6f5c\u5728\u53d8\u91cf\u6216\u566a\u58f0\u53d8\u91cf\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u8bed\u4e49\u7279\u6027\uff0c\u5728\u8bed\u4e49\u76f8\u5e72\u6d41\u5f62\u4e0a\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "Null-TTA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76ee\u6807\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u8de8\u5956\u52b1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bed\u4e49\u7a7a\u95f4\u4f18\u5316\u662f\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u7684\u6709\u6548\u4e14\u539f\u5219\u6027\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.20983", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20983", "abs": "https://arxiv.org/abs/2511.20983", "authors": ["Al Amin", "Kamrul Hasan", "Liang Hong", "Sharif Ullah"], "title": "Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI", "comment": "7 pages, 4 figures", "summary": "Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408Vision Transformers\u548c\u540c\u6001\u52a0\u5bc6\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u52a0\u5bc6CLS token\u5b9e\u73b0\u5b89\u5168\u805a\u5408\uff0c\u76f8\u6bd4\u68af\u5ea6\u52a0\u5bc6\u51cf\u5c1130\u500d\u901a\u4fe1\u91cf\uff0c\u9632\u6b62\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u3002", "motivation": "\u533b\u7597\u673a\u6784\u7684\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u80fd\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f46\u9690\u79c1\u6cd5\u89c4\u7981\u6b62\u76f4\u63a5\u5171\u4eab\u60a3\u8005\u6570\u636e\u3002\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u6a21\u578b\u68af\u5ea6\u4ecd\u6613\u53d7\u91cd\u5efa\u653b\u51fb\uff0c\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u533b\u7597\u4fe1\u606f\u3002", "method": "\u4f7f\u7528Vision Transformers\u7684CLS token\u4f5c\u4e3a768\u7ef4\u7279\u5f81\u8868\u793a\uff0c\u91c7\u7528CKKS\u540c\u6001\u52a0\u5bc6\u5728\u4f20\u8f93\u524d\u52a0\u5bc6\u8fd9\u4e9btoken\uff0c\u5b9e\u73b0\u5b89\u5168\u805a\u5408\u3002", "result": "\u68af\u5ea6\u6613\u53d7\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\uff08PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741\uff09\uff0c\u800cCLS\u4fdd\u62a4\u7684HE\u65b9\u6cd5\u80fd\u9632\u6b62\u6b64\u7c7b\u653b\u51fb\uff0c\u6bcf\u8f6e\u805a\u5408\u4ec5\u9700326 KB\u52a0\u5bc6\u6570\u636e\u4f20\u8f93\uff0c\u5728\u672a\u52a0\u5bc6\u57df\u8fbe\u523096.12%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u52a0\u5bc6\u57df\u8fbe\u523090.02%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u5e76\u9632\u6b62\u4e86\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u3002"}}
{"id": "2511.21032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21032", "abs": "https://arxiv.org/abs/2511.21032", "authors": ["Yuxuan Zhu", "Cong Fu", "Yabo Ni", "Anxiang Zeng", "Yuan Fang"], "title": "A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems", "comment": null, "summary": "Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.", "AI": {"tldr": "ELBO\u209c\u1d05s\u662f\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u56e0\u679c\u56fe\u5efa\u6a21\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u65f6\u95f4\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5df2\u5728Shopee\u4ea7\u54c1\u641c\u7d22\u4e2d\u6210\u529f\u90e8\u7f72\u3002", "motivation": "\u65f6\u95f4\u5206\u5e03\u504f\u79fb\u4f1a\u4fb5\u8680\u63a8\u8350\u7cfb\u7edf\u7684\u957f\u671f\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u589e\u91cf\u8bad\u7ec3\u3001\u4e0d\u53d8\u6027\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u65f6\u95f4\u6cdb\u5316\u7a33\u5b9a\u6027\u3001\u8868\u793a\u5d29\u6e83\u6216\u6570\u636e\u5229\u7528\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u8bc6\u522b\u5173\u952e\u504f\u79fb\u56e0\u5b50\u5e76\u8bbe\u8ba1\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6269\u5c55\u8bad\u7ec3\u652f\u6301\uff1b\u5176\u6b21\u4f7f\u7528\u56e0\u679c\u56fe\u5efa\u6a21\u65f6\u95f4\u63a8\u8350\u573a\u666f\uff0c\u63a8\u5bfc\u51fa\u81ea\u76d1\u7763\u53d8\u5206\u76ee\u6807ELBO\u209c\u1d05s\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u65f6\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u7528\u6237GMV\u63d0\u53472.33%\uff0c\u5e76\u5df2\u5728Shopee\u4ea7\u54c1\u641c\u7d22\u4e2d\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "ELBO\u209c\u1d05s\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u5de5\u4e1a\u89c4\u6a21\u589e\u91cf\u5b66\u4e60\u7ba1\u9053\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.21054", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21054", "abs": "https://arxiv.org/abs/2511.21054", "authors": ["Jiaming Guo", "Rui Zhang", "Zerun Li", "Yunkai Gao", "Shaohui Peng", "Siming Lan", "Xing Hu", "Zidong Du", "Xishan Zhang", "Ling Li"], "title": "Efficient Diffusion Planning with Temporal Diffusion", "comment": "Accepted by the AAAI26 Conference Main Track", "summary": "Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.", "AI": {"tldr": "\u63d0\u51faTemporal Diffusion Planner (TDP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u53bb\u566a\u6b65\u9aa4\u5206\u5e03\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\uff0c\u63d0\u9ad8\u6269\u6563\u89c4\u5212\u7684\u51b3\u7b56\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u89c4\u5212\u65b9\u6cd5\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u65b0\u8ba1\u5212\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u51b3\u7b56\u9891\u7387\u4f4e\u3002\u53d7\u4eba\u7c7b\u89c4\u5212\u65b9\u5f0f\u542f\u53d1\uff0c\u5e0c\u671b\u521b\u5efa\u8be6\u7ec6\u77ed\u671f\u8ba1\u5212\u548c\u6a21\u7cca\u957f\u671f\u8ba1\u5212\uff0c\u5e76\u968f\u65f6\u95f4\u8c03\u6574\u3002", "method": "TDP\u9996\u5148\u751f\u6210\u968f\u65f6\u95f4\u9010\u6e10\u6a21\u7cca\u7684\u521d\u59cb\u8ba1\u5212\uff0c\u968f\u540e\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4ec5\u7528\u5c11\u91cf\u53bb\u566a\u6b65\u9aa4\u66f4\u65b0\u524d\u4e00\u4e2a\u8ba1\u5212\uff0c\u800c\u975e\u751f\u6210\u5168\u65b0\u8ba1\u5212\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u91cd\u65b0\u89c4\u5212\u673a\u5236\u9632\u6b62\u8ba1\u5212\u4e0e\u73b0\u5b9e\u504f\u5dee\u8fc7\u5927\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6bcf\u6b65\u751f\u6210\u65b0\u8ba1\u5212\u7684\u65b9\u6cd5\uff0cTDP\u5c06\u51b3\u7b56\u9891\u7387\u63d0\u9ad811-24.8\u500d\uff0c\u540c\u65f6\u8fbe\u5230\u66f4\u9ad8\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "TDP\u901a\u8fc7\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u53bb\u566a\u6b65\u9aa4\u5206\u5e03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u6563\u89c4\u5212\u7684\u51b3\u7b56\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.21075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21075", "abs": "https://arxiv.org/abs/2511.21075", "authors": ["Zhenchao Tang", "Fang Wang", "Haohuai He", "Jiale Zhou", "Tianxu Lv", "Jun Zhu", "Shouzhi Chen", "Minghao Yang", "Yu Wang", "Jiayang Wu", "Yidong Song", "Jianhua Yao"], "title": "Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning", "comment": null, "summary": "Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses \"minimum group confidence\" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5e73\u8861\u5fae\u8c03(BFT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u5c42\u52a0\u6743\u673a\u5236\u4ece\u7a00\u758f\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u63a8\u7406\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4e13\u4e1a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u5bf9\u9f50\u65f6\u9762\u4e34\u6311\u6218\uff1a\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u9762\u6307\u4ee4\u6a21\u5f0f\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u56e0\u9700\u8981\u5b9e\u9a8c\u9a8c\u8bc1\u5956\u52b1\u800c\u4e0d\u5b9e\u7528\u3002", "method": "BFT\u91c7\u7528\u4e24\u5c42\u52a0\u6743\u673a\u5236\uff1a1) \u5728token\u7ea7\u522b\u901a\u8fc7\u9884\u6d4b\u6982\u7387\u7f29\u653e\u635f\u5931\u4ee5\u7a33\u5b9a\u68af\u5ea6\uff1b2) \u5728\u6837\u672c\u7ea7\u522b\u4f7f\u7528\"\u6700\u5c0f\u7ec4\u7f6e\u4fe1\u5ea6\"\u81ea\u9002\u5e94\u589e\u5f3a\u56f0\u96be\u6837\u672c\u7684\u5b66\u4e60\u3002", "result": "BFT\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u8ba9LLMs\u5b66\u5230\u4e86SFT\u9057\u6f0f\u7684\u77e5\u8bc6\uff0c\u5728\u751f\u7269\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86GeneAgent\uff0c\u5176\u751f\u6210\u7684\u6587\u672c\u5d4c\u5165\u53ef\u76f4\u63a5\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u5982\u57fa\u56e0\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u3002", "conclusion": "BFT\u4fc3\u8fdb\u4e86LLMs\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u7a00\u758f\u6570\u636e\u4e2d\u7684\u590d\u6742\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2511.21105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21105", "abs": "https://arxiv.org/abs/2511.21105", "authors": ["Pushkal Mishra", "Kshitiz Bansal", "Dinesh Bharadia"], "title": "Scaling Foundation Models for Radar Scene Understanding", "comment": null, "summary": "Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.", "AI": {"tldr": "RadarFM\u662f\u4e00\u4e2a\u96f7\u8fbe\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7a7a\u95f4\u8bed\u8a00\u76d1\u7763\u5b66\u4e60\u7edf\u4e00\u7684\u573a\u666f\u7ea7\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u96f7\u8fbe\u65b9\u6cd5\u788e\u7247\u5316\u548c\u4efb\u52a1\u7279\u5b9a\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u96f7\u8fbe\u4f20\u611f\u5668\u5728\u6076\u52a3\u5929\u6c14\u3001\u5149\u7167\u548c\u8fdc\u8ddd\u79bb\u6761\u4ef6\u4e0b\u63d0\u4f9b\u53ef\u9760\u7684\u611f\u77e5\uff0c\u4f46\u73b0\u6709\u96f7\u8fbe\u65b9\u6cd5\u788e\u7247\u5316\u4e14\u4efb\u52a1\u7279\u5b9a\uff0c\u963b\u788d\u4e86\u8de8\u4efb\u52a1\u8fc1\u79fb\u3002\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4e0e\u96f7\u8fbe\u611f\u77e5\u7684\u6574\u5408\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u6807\u9898\u6846\u67b6\u5728\u539f\u751f\u96f7\u8fbe\u5750\u6807\u4e2d\u7f16\u7801\u8f66\u8f86\u5206\u5e03\uff0c\u4ee5\u53ca\u54c8\u5e0c\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u91cf\u5316\u8fde\u7eed\u573a\u666f\u76f8\u4f3c\u6027\u800c\u975e\u4e8c\u5143\u5339\u914d\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u3002\u5229\u7528CARLA\u6a21\u62df\u5668\u751f\u6210\u5927\u89c4\u6a21\u3001\u6807\u6ce8\u826f\u597d\u7684\u96f7\u8fbe\u6570\u636e\u96c6\u3002", "result": "\u5f00\u53d1\u4e86RadarFM\u96f7\u8fbe\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5b66\u4e60\u7edf\u4e00\u7684\u573a\u666f\u7ea7\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u5b9a\u4f4d\u611f\u77e5\u6307\u6807\u8bc4\u4f30\u7a7a\u95f4\u51c6\u786e\u6027\u3002", "conclusion": "RadarFM\u901a\u8fc7\u7ed3\u6784\u5316\u7a7a\u95f4\u8bed\u8a00\u76d1\u7763\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4e3a\u96f7\u8fbe\u611f\u77e5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u652f\u6301\u8de8\u4efb\u52a1\u8fc1\u79fb\u548c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u3002"}}
{"id": "2511.21118", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21118", "abs": "https://arxiv.org/abs/2511.21118", "authors": ["Pius Onobhayedo", "Paul Osemudiame Oamen"], "title": "Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination", "comment": null, "summary": "Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u805a\u5408\u5668\u7f3a\u4e4f\u95ee\u8d23\u3001\u6fc0\u52b1\u673a\u5236\u6613\u88ab\u64cd\u7eb5\u3001\u534f\u8c03\u53ef\u6269\u5c55\u6027\u5dee\u548c\u6cbb\u7406\u5b58\u5728\u8ffd\u6eaf\u6027\u64cd\u7eb5\u7b49\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u5bc6\u6536\u636e\u3001\u51e0\u4f55\u65b0\u9896\u6027\u6d4b\u91cf\u3001\u5e76\u884c\u5bf9\u8c61\u6240\u6709\u6743\u548c\u65f6\u95f4\u9501\u5b9a\u7b56\u7565\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6c11\u4e3b\u5316AI\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u6b63\u4ece\u96c6\u4e2d\u5f0f\u63d0\u4f9b\u5411\u5206\u5e03\u5f0f\u521b\u5efa\u8f6c\u53d8\uff0c\u4f46\u5f53\u524d\u7684\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u805a\u5408\u5668\u7f3a\u4e4f\u95ee\u8d23\u3001\u7ecf\u6d4e\u673a\u5236\u6613\u88ab\u64cd\u7eb5\u3001\u534f\u8c03\u53ef\u6269\u5c55\u6027\u5dee\u548c\u6cbb\u7406\u5b58\u5728\u8ffd\u6eaf\u6027\u64cd\u7eb5\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u6c11\u4e3b\u5316AI\u613f\u666f\u7684\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528\u52a0\u5bc6\u6536\u636e\u8bc1\u660e\u805a\u5408\u6b63\u786e\u6027\uff0c\u51e0\u4f55\u65b0\u9896\u6027\u6d4b\u91cf\u9632\u6b62\u6fc0\u52b1\u64cd\u7eb5\uff0c\u5e76\u884c\u5bf9\u8c61\u6240\u6709\u6743\u5b9e\u73b0\u7ebf\u6027\u53ef\u6269\u5c55\u6027\uff0c\u65f6\u95f4\u9501\u5b9a\u7b56\u7565\u68c0\u67e5\u8ffd\u6eaf\u6027\u64cd\u7eb5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0fAI\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u5bc6\u7801\u5b66\u548c\u7ecf\u6d4e\u673a\u5236\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5b89\u5168\u3001\u6c11\u4e3b\u7684\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u63a8\u52a8AI\u4ece\u96c6\u4e2d\u5f0f\u5411\u5206\u5e03\u5f0f\u7684\u8f6c\u53d8\u3002"}}
{"id": "2511.21181", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21181", "abs": "https://arxiv.org/abs/2511.21181", "authors": ["Dogukan Aksu", "Jesus Martinez del Rincon", "Ihsen Alouani"], "title": "Privacy in Federated Learning with Spiking Neural Networks", "comment": null, "summary": "Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.", "AI": {"tldr": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5177\u6709\u5929\u7136\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\uff0c\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u96be\u4ee5\u4eceSNN\u68af\u5ea6\u4e2d\u91cd\u5efa\u6709\u610f\u4e49\u7684\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u7814\u7a76SNNs\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u63a2\u7d22\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u5bf9SNNs\u7684\u6709\u6548\u6027\uff0c\u56e0\u4e3aSNNs\u7684\u975e\u53ef\u5fae\u6027\u548c\u66ff\u4ee3\u68af\u5ea6\u8bad\u7ec3\u53ef\u80fd\u51cf\u5c11\u68af\u5ea6\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u5c06\u4e0d\u540c\u68af\u5ea6\u6cc4\u6f0f\u653b\u51fb\u65b9\u6cd5\u9002\u914d\u5230\u8109\u51b2\u57df\uff0c\u5728\u591a\u79cd\u6570\u636e\u57df\u4e0a\u5bf9SNNs\u8fdb\u884c\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u4e0e\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u4e0e\u4f20\u7edfANNs\u4e0d\u540c\uff0cSNN\u68af\u5ea6\u4ea7\u751f\u7684\u91cd\u5efa\u7ed3\u679c\u566a\u58f0\u5927\u3001\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u6062\u590d\u6709\u610f\u4e49\u7684\u7a7a\u95f4\u6216\u65f6\u95f4\u7ed3\u6784\uff0c\u8868\u660e\u4e8b\u4ef6\u9a71\u52a8\u52a8\u6001\u548c\u66ff\u4ee3\u68af\u5ea6\u8bad\u7ec3\u663e\u8457\u964d\u4f4e\u4e86\u68af\u5ea6\u4fe1\u606f\u91cf\u3002", "conclusion": "SNNs\u5177\u6709\u56fa\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u6f5c\u529b\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u6bd4\u4f20\u7edfANNs\u66f4\u80fd\u62b5\u5fa1\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\uff0c\u7a81\u663e\u4e86\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.21265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21265", "abs": "https://arxiv.org/abs/2511.21265", "authors": ["Juncheng Chen", "Chao Xu", "Yanjun Cao"], "title": "Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting", "comment": null, "summary": "Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.", "AI": {"tldr": "MatchGS\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u6821\u6b63\u76843DGS\u6570\u636e\u751f\u6210\u548c2D-3D\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u56fe\u50cf\u5339\u914d\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u57fa\u4e8e\u5b66\u4e60\u7684\u56fe\u50cf\u5339\u914d\u9700\u8981\u5927\u89c4\u6a21\u3001\u591a\u6837\u4e14\u51e0\u4f55\u51c6\u786e\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f463DGS\u5b58\u5728\u51e0\u4f55\u4e0d\u51c6\u786e\u6027\u548c\u6df1\u5ea6\u6e32\u67d3\u504f\u5dee\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bf9\u5e94\u5173\u7cfb\u6807\u6ce8\u4e2d\u7684\u5e94\u7528\u3002", "method": "1) \u51e0\u4f55\u5fe0\u5b9e\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u7cbe\u70bc3DGS\u51e0\u4f55\u4ee5\u4ea7\u751f\u7cbe\u786e\u5bf9\u5e94\u6807\u7b7e\uff1b2) 2D-3D\u8868\u793a\u5bf9\u9f50\u7b56\u7565\uff0c\u5c063DGS\u7684\u663e\u5f0f3D\u77e5\u8bc6\u6ce8\u51652D\u5339\u914d\u5668\u3002", "result": "\u751f\u6210\u7684\u5bf9\u5e94\u5173\u7cfb\u5c06\u6781\u7ebf\u8bef\u5dee\u964d\u4f4e\u8fbe40\u500d\uff0c\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u96f6\u6837\u672c\u6027\u80fd\u63d0\u5347\u8fbe17.7%\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u51e0\u4f55\u7cbe\u70bc\uff0c3DGS\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u4fdd\u771f\u4e14\u7ed3\u6784\u4e30\u5bcc\u7684\u6570\u636e\u6e90\uff0c\u4e3a\u65b0\u4e00\u4ee3\u9c81\u68d2\u96f6\u6837\u672c\u56fe\u50cf\u5339\u914d\u5668\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.21337", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21337", "abs": "https://arxiv.org/abs/2511.21337", "authors": ["Munish Rathee", "Boris Ba\u010di\u0107", "Maryam Doborjeh"], "title": "Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure", "comment": "8 pages, 6 figures. This is a preprint of a paper accepted for presentation at the 2025 International Conference on Image and Vision Computing New Zealand (IVCNZ). The final version will appear in IEEE Xplore", "summary": "This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.", "AI": {"tldr": "SIFT-SNN\u6846\u67b6\uff1a\u4e00\u79cd\u4f4e\u5ef6\u8fdf\u795e\u7ecf\u5f62\u6001\u4fe1\u53f7\u5904\u7406\u7ba1\u9053\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u7ed3\u6784\u5f02\u5e38\uff0c\u7ed3\u5408SIFT\u7a7a\u95f4\u7279\u5f81\u7f16\u7801\u548cSNN\u5206\u7c7b\uff0c\u5728\u5965\u514b\u5170\u6d77\u6e2f\u5927\u6865\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.3%\u51c6\u786e\u7387\u548c9.5ms\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5f00\u53d1\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7684\u8fb9\u7f18\u90e8\u7f72\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u7684\u7ed3\u6784\u5b89\u5168\u76d1\u6d4b\uff0c\u514b\u670d\u4f20\u7edfCNN\u65b9\u6cd5\u5728\u5ef6\u8fdf\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u96c6\u6210\u5c3a\u5ea6\u4e0d\u53d8\u7279\u5f81\u53d8\u6362(SIFT)\u8fdb\u884c\u7a7a\u95f4\u7279\u5f81\u7f16\u7801\uff0c\u7ed3\u5408\u5ef6\u8fdf\u9a71\u52a8\u7684\u8109\u51b2\u8f6c\u6362\u5c42\u548c\u6cc4\u6f0f\u79ef\u5206\u70b9\u706b(LIF)\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u5965\u514b\u5170\u6d77\u6e2f\u5927\u6865\u6570\u636e\u96c6(6000\u6807\u8bb0\u5e27)\u4e0a\u8fbe\u523092.3%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6bcf\u5e27\u63a8\u7406\u65f6\u95f49.5ms\uff0c\u8109\u51b2\u7a00\u758f\u5ea68.1%\uff0c\u652f\u6301\u5b9e\u65f6\u4f4e\u529f\u8017\u8fb9\u7f18\u90e8\u7f72\u3002", "conclusion": "SIFT-SNN\u6846\u67b6\u5728\u4fdd\u6301\u7a7a\u95f4\u7279\u5f81\u57fa\u7840\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u7684\u5b9e\u65f6\u7ed3\u6784\u5b89\u5168\u76d1\u6d4b\uff0c\u4e3a\u79fb\u52a8\u6df7\u51dd\u571f\u62a4\u680f\u7b49\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21590", "abs": "https://arxiv.org/abs/2511.21590", "authors": ["Muhammad Siddique", "Sohaib Zafar"], "title": "An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids", "comment": "16 pages, 11 figures, IEEEaccess journal", "summary": "Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u667a\u80fd\u7535\u7f51\u6570\u5b57\u53d6\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u6570\u636e\u91c7\u96c6\u3001\u8ba4\u8bc1\u901a\u4fe1\u3001\u4e91\u5b58\u50a8\u548c\u81ea\u52a8\u5316\u53d6\u8bc1\u5206\u6790\uff0c\u7528\u4e8e\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u548c\u5165\u4fb5\u5206\u6790\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u878d\u5408\u4e86\u4f20\u7edf\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u548c\u5148\u8fdb\u901a\u4fe1\u7f51\u7edc\uff0c\u8fd9\u79cd\u96c6\u6210\u5e26\u6765\u4e86\u53ef\u80fd\u7834\u574f\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u7684\u6f0f\u6d1e\uff0c\u9700\u8981\u6709\u6548\u7684\u6570\u5b57\u53d6\u8bc1\u65b9\u6cd5\u6765\u8bc6\u522b\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u5b89\u5168\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u90e8\u7f72\u5728\u4e91\u7aef\u7684\u673a\u5668\u5b66\u4e60\u6570\u5b57\u53d6\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u68af\u5ea6\u63d0\u5347\u6811\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3001\u4e8b\u4ef6\u91cd\u5efa\u548c\u5b9e\u65f6\u5165\u4fb5\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5bf9\u5b9e\u65f6\u667a\u80fd\u7535\u8868\u6570\u636e\u6d41\u7684\u6a21\u62df\u548c\u5b9e\u9a8c\u7814\u7a76\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u7f51\u7edc\u653b\u51fb\uff08\u5305\u62ec\u6570\u636e\u7be1\u6539\u3001\u865a\u5047\u6570\u636e\u6ce8\u5165\u548c\u534f\u8c03\u63a7\u5236\u56de\u8def\u64cd\u7eb5\uff09\u7684\u97e7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u4e91\u670d\u52a1\u662f\u5927\u6570\u636e\u9a71\u52a8\u53d6\u8bc1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6700\u4f73\u9aa8\u5e72\uff0c\u4f7f\u80fd\u6e90\u516c\u7528\u4e8b\u4e1a\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u6001\u52bf\u611f\u77e5\u548c\u667a\u80fd\u4e8b\u4ef6\u54cd\u5e94\u3002"}}
{"id": "2511.21420", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21420", "abs": "https://arxiv.org/abs/2511.21420", "authors": ["Futian Wang", "Mengqi Wang", "Xiao Wang", "Haowen Wang", "Jin Tang"], "title": "SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning", "comment": null, "summary": "Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528SAM\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u9065\u611f\u53d8\u5316\u63cf\u8ff0\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u533a\u57df\u7ea7\u8868\u793a\u548c\u6ce8\u5165\u611f\u5174\u8da3\u533a\u57df\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u533a\u57df\u611f\u77e5\u5f31\u548c\u65f6\u95f4\u5bf9\u9f50\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u53d8\u5316\u63cf\u8ff0\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528CNNs/Transformers\u63d0\u53d6\u89c6\u89c9\u8868\u793a\u6216\u7ed3\u5408\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u7ed3\u679c\uff0c\u4f46\u5b58\u5728\u533a\u57df\u611f\u77e5\u5f31\u548c\u65f6\u95f4\u5bf9\u9f50\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528CNN/Transformer\u63d0\u53d6\u5168\u5c40\u89c6\u89c9\u7279\u5f81\uff0c\u5229\u7528SAM\u57fa\u7840\u6a21\u578b\u63cf\u7ed8\u8bed\u4e49\u548c\u8fd0\u52a8\u7ea7\u53d8\u5316\u533a\u57df\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u611f\u5174\u8da3\u5bf9\u8c61\u4fe1\u606f\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u5f02\u6784\u4fe1\u606f\uff0c\u4f7f\u7528Transformer\u89e3\u7801\u5668\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408SAM\u57fa\u7840\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u53d8\u5316\u63cf\u8ff0\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u533a\u57df\u611f\u77e5\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u6311\u6218\u3002"}}
{"id": "2511.21422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21422", "abs": "https://arxiv.org/abs/2511.21422", "authors": ["Adeela Islam", "Stefano Fiorini", "Manuel Lecha", "Theodore Tsesmelis", "Stuart James", "Pietro Morerio", "Alessio Del Bue"], "title": "E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework", "comment": null, "summary": "3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.", "AI": {"tldr": "E-M3RF\u662f\u4e00\u4e2a\u7b49\u53d8\u591a\u6a21\u60013D\u91cd\u7ec4\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u7279\u5f81\u548c\u989c\u8272\u7279\u5f81\uff0c\u4f7f\u7528SE(3)\u6d41\u5339\u914d\u6765\u9884\u6d4b\u788e\u7247\u91cd\u7ec4\u6240\u9700\u7684\u53d8\u6362\uff0c\u5728\u51e0\u4f55\u7279\u5f81\u4e0d\u8db3\u6216\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76843D\u91cd\u7ec4\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u51e0\u4f55\u7279\u5f81\uff0c\u5728\u51e0\u4f55\u4fe1\u606f\u4e0d\u8db3\uff08\u5982\u5c0f\u788e\u7247\u3001\u4fb5\u8680\u788e\u7247\u6216\u5bf9\u79f0\u788e\u7247\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u9632\u6b62\u91cd\u53e0\u7ec4\u88c5\u7684\u7269\u7406\u7ea6\u675f\u3002", "method": "\u4f7f\u7528\u65cb\u8f6c\u7b49\u53d8\u7f16\u7801\u5668\u63d0\u53d6\u51e0\u4f55\u7279\u5f81\uff0c\u7ed3\u5408Transformer\u63d0\u53d6\u989c\u8272\u7279\u5f81\uff0c\u5f62\u6210\u591a\u6a21\u6001\u8868\u793a\uff0c\u901a\u8fc7SE(3)\u6d41\u5339\u914d\u9884\u6d4b\u788e\u7247\u91cd\u7ec4\u53d8\u6362\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cE-M3RF\u5728RePAIR\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u7ade\u4e89\u65b9\u6cd5\uff0c\u65cb\u8f6c\u8bef\u5dee\u964d\u4f4e23.1%\uff0c\u5e73\u79fb\u8bef\u5dee\u964d\u4f4e13.2%\uff0cChamfer\u8ddd\u79bb\u51cf\u5c1118.4%\u3002", "conclusion": "E-M3RF\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u548c\u989c\u8272\u591a\u6a21\u6001\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u51e0\u4f55\u4fe1\u606f\u4e0d\u8db3\u65f6\u7684\u91cd\u7ec4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u7ec4\u7cbe\u5ea6\u3002"}}
{"id": "2511.21669", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21669", "abs": "https://arxiv.org/abs/2511.21669", "authors": ["Fengze Yu", "Leshu Li", "Brad McDanel", "Saiqian Zhang"], "title": "DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving", "comment": null, "summary": "Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.", "AI": {"tldr": "DSD\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u8349\u7a3f-\u76ee\u6807\u6267\u884c\u5728\u591a\u8bbe\u5907\u90e8\u7f72\u4e2d\u6269\u5c55\u63a8\u6d4b\u89e3\u7801\uff0c\u89e3\u51b3\u4e86LLM\u63a8\u7406\u5728\u5f02\u6784\u8fb9\u7f18-\u4e91\u73af\u5883\u4e2d\u7684\u9ad8\u5ef6\u8fdf\u548c\u6709\u9650\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5728\u5f02\u6784\u8fb9\u7f18-\u4e91\u73af\u5883\u4e2d\u9762\u4e34\u9ad8\u89e3\u7801\u5ef6\u8fdf\u548c\u6709\u9650\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u4ec5\u9650\u4e8e\u5355\u8282\u70b9\u6267\u884c\u3002", "method": "\u63d0\u51faDSD\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u5f15\u5165DSD-Sim\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u5668\u6355\u6349\u7f51\u7edc\u3001\u6279\u5904\u7406\u548c\u8c03\u5ea6\u52a8\u6001\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u7a97\u53e3\u63a7\u5236\u7b56\u7565\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u7a97\u53e3\u5927\u5c0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDSD\u76f8\u6bd4\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad81.1\u500d\u52a0\u901f\u548c9.7%\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728\u8fb9\u7f18\u548c\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u654f\u6377\u53ef\u6269\u5c55\u7684LLM\u670d\u52a1\u3002", "conclusion": "DSD\u6846\u67b6\u6210\u529f\u5c06\u63a8\u6d4b\u89e3\u7801\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u73af\u5883\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7a97\u53e3\u63a7\u5236\u4f18\u5316\u4e86\u5f02\u6784\u8fb9\u7f18-\u4e91\u90e8\u7f72\u4e2d\u7684LLM\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2511.21439", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21439", "abs": "https://arxiv.org/abs/2511.21439", "authors": ["Futian Wang", "Fan Zhang", "Xiao Wang", "Mengqi Wang", "Dexing Huang", "Jin Tang"], "title": "EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation", "comment": null, "summary": "Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u65f6\u7a7a\u4e8b\u4ef6\u6d41\u8865\u5168\u673a\u5236\uff0c\u901a\u8fc7\u8d85\u56fe\u8fde\u63a5\u4e0d\u540c\u65f6\u95f4\u548c\u7a7a\u95f4\u4f4d\u7f6e\u7684\u4e8b\u4ef6\u6807\u8bb0\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f20\u9012\u6765\u8865\u5168\u7a00\u758f\u4e8b\u4ef6\uff0c\u5e76\u80fd\u7075\u6d3b\u878d\u5408RGB\u6807\u8bb0\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u606f\u8865\u5168\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4ea7\u751f\u7684\u4e8b\u4ef6\u6d41\u5177\u6709\u7a7a\u95f4\u7a00\u758f\u4f46\u65f6\u95f4\u5bc6\u96c6\u7684\u7279\u70b9\uff0c\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u4e8b\u4ef6\u5e27\u3001\u4f53\u7d20\u6216\u5f20\u91cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f46\u96be\u4ee5\u89e3\u51b3\u7a7a\u95f4\u7a00\u758f\u5bfc\u81f4\u7684\u6b20\u91c7\u6837\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8d85\u56fe\u5f15\u5bfc\u7684\u65f6\u7a7a\u4e8b\u4ef6\u6d41\u8865\u5168\u673a\u5236\uff0c\u901a\u8fc7\u8d85\u56fe\u8fde\u63a5\u4e8b\u4ef6\u6807\u8bb0\u5e76\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f20\u9012\u8fdb\u884c\u8865\u5168\uff0c\u53ef\u878d\u5408RGB\u6807\u8bb0\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u606f\u8865\u5168\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u805a\u5408\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u8d85\u56fe\u8282\u70b9\u4fe1\u606f\u3002", "result": "\u5728\u5355\u6807\u7b7e\u548c\u591a\u6807\u7b7e\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d85\u56fe\u5f15\u5bfc\u4e8b\u4ef6\u6d41\u8865\u5168\u673a\u5236\u80fd\u6709\u6548\u89e3\u51b3\u4e8b\u4ef6\u6570\u636e\u7a7a\u95f4\u7a00\u758f\u95ee\u9898\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u5728\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
