{"id": "2601.22197", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22197", "abs": "https://arxiv.org/abs/2601.22197", "authors": ["Jathurshan Pradeepkumar", "Zheng Chen", "Jimeng Sun"], "title": "Neural Signals Generate Clinical Notes in the Wild", "comment": null, "summary": "Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\\%$--$95\\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL]."}
{"id": "2601.22369", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22369", "abs": "https://arxiv.org/abs/2601.22369", "authors": ["Yujie Hui", "Xiaoyi Lu", "Andrew Perrault", "Yang Wang"], "title": "Learning Provably Correct Distributed Protocols Without Human Knowledge", "comment": null, "summary": "Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods."}
{"id": "2601.22259", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22259", "abs": "https://arxiv.org/abs/2601.22259", "authors": ["Da In Kim", "Wei Siang Lai", "Kelly W. Zhang"], "title": "Tabular Foundation Models Can Do Survival Analysis", "comment": null, "summary": "While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics."}
{"id": "2601.22410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22410", "abs": "https://arxiv.org/abs/2601.22410", "authors": ["Imene Kolli", "Kai-Robin Lange", "Jonas Rieger", "Carsten Jentsch"], "title": "Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking", "comment": "20 pages, 16 figures", "summary": "We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories."}
{"id": "2601.22439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22439", "abs": "https://arxiv.org/abs/2601.22439", "authors": ["Galim Turumtaev"], "title": "Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss", "comment": "Accepted at LoResLM 2025 (COLING 2025 workshop). Oral presentation", "summary": "Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages."}
{"id": "2601.22501", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22501", "abs": "https://arxiv.org/abs/2601.22501", "authors": ["Renjie Lu", "Xulong Zhang", "Xiaoyang Qu", "Jianzong Wang", "Shangfei Wang"], "title": "MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control", "comment": "Accepted to 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation."}
{"id": "2601.22586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22586", "abs": "https://arxiv.org/abs/2601.22586", "authors": ["Qian Hong", "Siyuan Chang", "Xiao Zhou"], "title": "WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction", "comment": "The ACM on Web Conference 2026 (WWW'26)", "summary": "Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net."}
{"id": "2601.22302", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22302", "abs": "https://arxiv.org/abs/2601.22302", "authors": ["Amirhossein Taherpour", "Xiaodong Wang"], "title": "ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning", "comment": "Accepted for publication in IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "summary": "Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments."}
{"id": "2601.22580", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22580", "abs": "https://arxiv.org/abs/2601.22580", "authors": ["Chao Wang", "Bei Li", "Jiaqi Zhang", "Xinyu Liu", "Yuchun Fan", "Linkun Lyu", "Xin Chen", "Jingang Wang", "Tong Xiao", "Peng Pei", "Xunliang Cai"], "title": "SpanNorm: Reconciling Training Stability and Performance in Deep Transformers", "comment": null, "summary": "The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures."}
{"id": "2601.22522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22522", "abs": "https://arxiv.org/abs/2601.22522", "authors": ["Zhou Tang", "Jin Wang", "Angelo De Castro", "Yuxi Zhang", "Victoria Bastos Primo", "Ana Beatriz Montevecchio Bernardino", "Gota Morota", "Xu Wang", "Ricardo C Chebel", "Haipeng Yu"], "title": "Can 3D point cloud data improve automated body condition score prediction in dairy cattle?", "comment": null, "summary": "Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions."}
{"id": "2601.22688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22688", "abs": "https://arxiv.org/abs/2601.22688", "authors": ["Doyoung Kim", "Jaehyeok Doo", "Minjoon Seo"], "title": "TSLM: Tree-Structured Language Modeling for Divergent Thinking", "comment": null, "summary": "Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models."}
{"id": "2601.22692", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.22692", "abs": "https://arxiv.org/abs/2601.22692", "authors": ["Yiheng Liu", "Junhao Ning", "Sichen Xia", "Haiyang Sun", "Yang Yang", "Hanyang Chi", "Xiaohui Gao", "Ning Qiang", "Bao Ge", "Junwei Han", "Xintao Hu"], "title": "FNF: Functional Network Fingerprint for Large Language Models", "comment": "13 pages, 4 figures", "summary": "The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION."}
{"id": "2601.22758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22758", "abs": "https://arxiv.org/abs/2601.22758", "authors": ["Libin Qiu", "Zhirong Gao", "Junfu Chen", "Yuhang Ye", "Weizhi Huang", "Xiaobo Xue", "Wenkai Qiu", "Shuo Tang"], "title": "AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement", "comment": "8 pages, 3 figures, 3 tables", "summary": "Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination."}
{"id": "2601.22997", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22997", "abs": "https://arxiv.org/abs/2601.22997", "authors": ["Roham Koohestani", "Ateş Görpelioğlu", "Egor Klimov", "Burcu Kulahcioglu Ozkan", "Maliheh Izadi"], "title": "TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI", "comment": null, "summary": "Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal."}
{"id": "2601.22359", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22359", "abs": "https://arxiv.org/abs/2601.22359", "authors": ["Hsiang Hsu", "Pradeep Niroula", "Zichang He", "Ivan Brugere", "Freddy Lecue", "Chun-Fu Chen"], "title": "The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples", "comment": "Presented at NeurIPS 2025", "summary": "Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge."}
{"id": "2601.23001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23001", "abs": "https://arxiv.org/abs/2601.23001", "authors": ["Afrozah Nadeem", "Agrima", "Mehwish Nasim", "Usman Naseem"], "title": "Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs", "comment": "PrePrint", "summary": "Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity."}
{"id": "2601.22197", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22197", "abs": "https://arxiv.org/abs/2601.22197", "authors": ["Jathurshan Pradeepkumar", "Zheng Chen", "Jimeng Sun"], "title": "Neural Signals Generate Clinical Notes in the Wild", "comment": null, "summary": "Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\\%$--$95\\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL]."}
{"id": "2601.22444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22444", "abs": "https://arxiv.org/abs/2601.22444", "authors": ["Nikos I. Bosse", "Peter Mühlbacher", "Jack Wildman", "Lawrence Phillips", "Dan Schwarz"], "title": "Automating Forecasting Question Generation and Resolution for AI Evaluation", "comment": "41 pages, 4 figures", "summary": "Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141)."}
{"id": "2601.22359", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22359", "abs": "https://arxiv.org/abs/2601.22359", "authors": ["Hsiang Hsu", "Pradeep Niroula", "Zichang He", "Ivan Brugere", "Freddy Lecue", "Chun-Fu Chen"], "title": "The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples", "comment": "Presented at NeurIPS 2025", "summary": "Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge."}
{"id": "2601.23159", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.23159", "abs": "https://arxiv.org/abs/2601.23159", "authors": ["Seungjun Lee", "Gim Hee Lee"], "title": "Segment Any Events with Language", "comment": "ICLR 2026. Project Page: https://0nandon.github.io/SEAL", "summary": "Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL"}
{"id": "2601.22444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22444", "abs": "https://arxiv.org/abs/2601.22444", "authors": ["Nikos I. Bosse", "Peter Mühlbacher", "Jack Wildman", "Lawrence Phillips", "Dan Schwarz"], "title": "Automating Forecasting Question Generation and Resolution for AI Evaluation", "comment": "41 pages, 4 figures", "summary": "Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141)."}
{"id": "2601.22539", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.22539", "abs": "https://arxiv.org/abs/2601.22539", "authors": ["Babak Shahbaba", "Zahra Moslemi"], "title": "Neural-Inspired Posterior Approximation (NIPA)", "comment": "13 pages, 4 tables", "summary": "Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification."}
{"id": "2601.22580", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22580", "abs": "https://arxiv.org/abs/2601.22580", "authors": ["Chao Wang", "Bei Li", "Jiaqi Zhang", "Xinyu Liu", "Yuchun Fan", "Linkun Lyu", "Xin Chen", "Jingang Wang", "Tong Xiao", "Peng Pei", "Xunliang Cai"], "title": "SpanNorm: Reconciling Training Stability and Performance in Deep Transformers", "comment": null, "summary": "The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures."}
{"id": "2601.22692", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.22692", "abs": "https://arxiv.org/abs/2601.22692", "authors": ["Yiheng Liu", "Junhao Ning", "Sichen Xia", "Haiyang Sun", "Yang Yang", "Hanyang Chi", "Xiaohui Gao", "Ning Qiang", "Bao Ge", "Junwei Han", "Xintao Hu"], "title": "FNF: Functional Network Fingerprint for Large Language Models", "comment": "13 pages, 4 figures", "summary": "The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION."}
{"id": "2601.22820", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22820", "abs": "https://arxiv.org/abs/2601.22820", "authors": ["Arya Hadizadeh Moghaddam", "Mohsen Nayebi Kerdabadi", "Dongjie Wang", "Mei Liu", "Zijun Yao"], "title": "User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering", "comment": "IEEE International Conference on Data Engineering (ICDE) 2026 accepted paper", "summary": "Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients."}
{"id": "2601.22820", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22820", "abs": "https://arxiv.org/abs/2601.22820", "authors": ["Arya Hadizadeh Moghaddam", "Mohsen Nayebi Kerdabadi", "Dongjie Wang", "Mei Liu", "Zijun Yao"], "title": "User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering", "comment": "IEEE International Conference on Data Engineering (ICDE) 2026 accepted paper", "summary": "Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients."}
{"id": "2601.22891", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22891", "abs": "https://arxiv.org/abs/2601.22891", "authors": ["Jacques Cloete", "Mathias Jackermeier", "Ioannis Havoutis", "Alessandro Abate"], "title": "PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL", "comment": "11 pages, 3 figures (main paper). 14 pages, 10 figures (appendix)", "summary": "A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or \"symbols\"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments."}
{"id": "2601.23001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23001", "abs": "https://arxiv.org/abs/2601.23001", "authors": ["Afrozah Nadeem", "Agrima", "Mehwish Nasim", "Usman Naseem"], "title": "Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs", "comment": "PrePrint", "summary": "Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity."}
{"id": "2601.23147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23147", "abs": "https://arxiv.org/abs/2601.23147", "authors": ["Saeid Jamshidi", "Omar Abdul Wahab", "Rolando Herrero", "Foutse Khomh"], "title": "Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures", "comment": null, "summary": "The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies."}
{"id": "2601.23155", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23155", "abs": "https://arxiv.org/abs/2601.23155", "authors": ["Powei Chang", "Jinpeng Zhang", "Bowen Chen", "Chenyu Wang", "Chenlu Guo", "Yixing Zhang", "Yukang Gao", "JianXiang Xiang", "Yue Gao", "Chaoqun Sun", "Yiyi Chen", "Dongying Kong"], "title": "SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training", "comment": "39 pages, 9 figures, 15 tables (including appendices)", "summary": "Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost."}
{"id": "2601.23147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23147", "abs": "https://arxiv.org/abs/2601.23147", "authors": ["Saeid Jamshidi", "Omar Abdul Wahab", "Rolando Herrero", "Foutse Khomh"], "title": "Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures", "comment": null, "summary": "The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies."}
{"id": "2601.23155", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23155", "abs": "https://arxiv.org/abs/2601.23155", "authors": ["Powei Chang", "Jinpeng Zhang", "Bowen Chen", "Chenyu Wang", "Chenlu Guo", "Yixing Zhang", "Yukang Gao", "JianXiang Xiang", "Yue Gao", "Chaoqun Sun", "Yiyi Chen", "Dongying Kong"], "title": "SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training", "comment": "39 pages, 9 figures, 15 tables (including appendices)", "summary": "Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost."}
{"id": "2601.22580", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22580", "abs": "https://arxiv.org/abs/2601.22580", "authors": ["Chao Wang", "Bei Li", "Jiaqi Zhang", "Xinyu Liu", "Yuchun Fan", "Linkun Lyu", "Xin Chen", "Jingang Wang", "Tong Xiao", "Peng Pei", "Xunliang Cai"], "title": "SpanNorm: Reconciling Training Stability and Performance in Deep Transformers", "comment": null, "summary": "The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures."}
