{"id": "2509.12247", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12247", "abs": "https://arxiv.org/abs/2509.12247", "authors": ["Abigail R. Cohen", "Yuming Sun", "Zhihao Qin", "Harsh S. Muriki", "Zihao Xiao", "Yeonju Lee", "Matthew Housley", "Andrew F. Sharkey", "Rhuanito S. Ferrarezi", "Jing Li", "Lu Gan", "Yongsheng Chen"], "title": "Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture", "comment": null, "summary": "Efficient nutrient management is critical for crop growth and sustainable\nresource consumption (e.g., nitrogen, energy). Current approaches require\nlengthy analyses, preventing real-time optimization; similarly, imaging\nfacilitates rapid phenotyping but can be computationally intensive, preventing\ndeployment under resource constraints. This study proposes a flexible, tiered\npipeline for anomaly detection and status estimation (fresh weight, dry mass,\nand tissue nutrients), including a comprehensive energy analysis of approaches\nthat span the efficiency-accuracy spectrum. Using a nutrient depletion\nexperiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer\nstrength) and multispectral imaging (MSI), we developed a hierarchical pipeline\nusing an autoencoder (AE) for early warning. Further, we compared two status\nestimation modules of different complexity for more detailed analysis:\nvegetation index (VI) features with machine learning (Random Forest, RF) and\nraw whole-image deep learning (Vision Transformer, ViT). Results demonstrated\nhigh-efficiency anomaly detection (73% net detection of T3 samples 9 days after\ntransplanting) at substantially lower energy than embodied energy in wasted\nnitrogen. The state estimation modules show trade-offs, with ViT outperforming\nRF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at\nhigher energy cost. With our modular pipeline, this work opens opportunities\nfor edge diagnostics and practical opportunities for agricultural\nsustainability.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u5149\u8c31\u6210\u50cf\u548c\u673a\u5668\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4f53\u79cd\u5f02\u5e38\u68c0\u6d4b\u548c\u4f5c\u7269\u72b6\u6001\u4f30\u8ba1\uff0c\u5728\u4f4e\u80fd\u8017\u4e0b\u5b8c\u6210\u519c\u4e1a\u8425\u517b\u7ba1\u7406\u7684\u5b9e\u65f6\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u519c\u4e1a\u8425\u517b\u7ba1\u7406\u9700\u8981\u957f\u65f6\u95f4\u5206\u6790\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4f18\u5316\uff1b\u6210\u50cf\u6280\u672f\u867d\u80fd\u5feb\u901f\u8868\u578b\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u96be\u4ee5\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u6548\u7387\u4e0e\u51c6\u786e\u6027\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u80a5\u6599\u6fc0\u6d4b\u5904\u7406\uff08T1-100%\u3001T2-50%\u3001T3-25%\uff09\u8fdb\u884c\u8425\u517b\u6d88\u8017\u5b9e\u9a8c\uff0c\u4f7f\u7528\u591a\u5149\u8c31\u6210\u50cf\u6280\u672f\u3002\u5efa\u7acb\u5206\u5c42\u5904\u7406\u6d41\u7a0b\uff1a\u9996\u5148\u7528\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u8fdb\u884c\u65e9\u671f\u5f02\u5e38\u8b66\u62a5\uff0c\u7136\u540e\u4f7f\u7528\u690d\u88ab\u6307\u6570\u7279\u5f81\u7ed3\u5408\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c\u6574\u5f20\u56fe\u50cf\u6df1\u5ea6\u5b66\u4e60\uff08ViT\uff09\u4e24\u79cd\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u72b6\u6001\u4f30\u8ba1\u6a21\u5757\u3002", "result": "\u9ad8\u6548\u7387\u5f02\u5e38\u68c0\u6d4b\uff08\u79fb\u690d\u540e9\u5929\u68c0\u6d4b\u523073%\u7684T3\u6837\u672c\uff09\uff0c\u80fd\u8017\u8fdc\u4f4e\u4e8e\u6d6a\u8d39\u6c2e\u7684\u4f53\u73b0\u80fd\u91cf\u3002ViT\u5728\u78f7\u548c\u9499\u4f30\u8ba1\u4e0a\u8868\u73b0\u66f4\u597d\uff08R2 0.61 vs 0.58\uff0c0.48 vs 0.35\uff09\uff0c\u4f46\u80fd\u8017\u6210\u672c\u66f4\u9ad8\u3002RF\u4e0eViT\u5728\u4e0d\u540c\u6307\u6807\u4e0a\u5404\u6709\u4f18\u52bf\uff0c\u5b58\u5728\u6548\u7387-\u51c6\u786e\u6027\u4ea4\u6362\u3002", "conclusion": "\u8be5\u6a21\u5757\u5316\u5904\u7406\u6d41\u7a0b\u4e3a\u8fb9\u7f18\u8bbe\u5907\u7684\u519c\u4e1a\u8bca\u65ad\u5f00\u542f\u4e86\u65b0\u673a\u4f1a\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u519c\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4f4e\u80fd\u8017\u4e0b\u5b9e\u73b0\u8425\u517b\u7ba1\u7406\u7684\u5b9e\u65f6\u4f18\u5316\u3002"}}
{"id": "2509.12263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12263", "abs": "https://arxiv.org/abs/2509.12263", "authors": ["Gautam Sreekumar", "Vishnu Naresh Boddeti"], "title": "InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning", "comment": "35 pages including appendix", "summary": "Large multimodal models (LMMs) encode universal physical laws observed during\ntraining, such as momentum conservation, as parametric knowledge. It allows\nLMMs to answer physical reasoning queries, such as the outcome of a potential\ncollision event from visual input. However, since parametric knowledge includes\nonly the physical laws seen during training, it is insufficient for reasoning\nwhen the inference scenario violates these physical laws. In contrast, humans\npossess the skill to adapt their physical reasoning to unseen physical\nenvironments from a few visual examples. This ability, which we refer to as\ninductive physical reasoning, is indispensable for LMMs if they are to replace\nhuman agents in safety-critical applications. Despite its importance, existing\nvisual benchmarks evaluate only the parametric knowledge in LMMs, and not\ninductive physical reasoning. To this end, we propose InPhyRe, the first visual\nquestion answering benchmark to measure inductive physical reasoning in LMMs.\nInPhyRe evaluates LMMs on their ability to predict the outcome of collision\nevents in algorithmically generated synthetic collision videos. By inspecting\n13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited\nparametric knowledge about universal physical laws to reasoning, (2) inductive\nphysical reasoning in LMMs is weak when demonstration samples violate universal\nphysical laws, and (3) inductive physical reasoning in LMMs suffers from\nlanguage bias and largely ignores the visual inputs, questioning the\ntrustworthiness of LMMs regarding visual inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86InPhyRe\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8fdd\u53cd\u8bad\u7ec3\u7269\u7406\u5b9a\u5f8b\u573a\u666f\u4e0b\u7684\u5f52\u7eb3\u7269\u7406\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u53c2\u6570\u77e5\u8bc6\u5e94\u7528\u56f0\u96be\u3001\u8bed\u8a00\u504f\u89c1\u548c\u89c6\u89c9\u8f93\u5165\u5ffd\u7565\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u65f6\u5b66\u5230\u7684\u53c2\u6570\u5316\u7269\u7406\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5904\u7406\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u7684\u63a8\u7406\u573a\u666f\uff0c\u800c\u4eba\u7c7b\u5177\u5907\u4ece\u5c11\u91cf\u89c6\u89c9\u793a\u4f8b\u4e2d\u9002\u5e94\u65b0\u7269\u7406\u73af\u5883\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u521b\u5efaInPhyRe\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u901a\u8fc7\u7b97\u6cd5\u751f\u6210\u7684\u5408\u6210\u78b0\u649e\u89c6\u9891\u8bc4\u4f30\u6a21\u578b\u5728\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e8613\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u5e94\u7528\u53c2\u6570\u5316\u7269\u7406\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b\u5f53\u6f14\u793a\u6837\u672c\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u65f6\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u8f83\u5f31\uff1b\u5b58\u5728\u8bed\u8a00\u504f\u89c1\u4e14\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u89c6\u89c9\u8f93\u5165\u3002", "conclusion": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5f52\u7eb3\u7269\u7406\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u4fe1\u4efb\u5ea6\u503c\u5f97\u8d28\u7591\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u4ee5\u589e\u5f3a\u5728\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12390", "abs": "https://arxiv.org/abs/2509.12390", "authors": ["Evangelos Psomiadis", "Panagiotis Tsiotras"], "title": "Distributed Event-Triggered Distance-Based Formation Control for Multi-Agent Systems", "comment": "8 pages, 7 figures", "summary": "This paper addresses the problem of collaborative formation control for\nmulti-agent systems with limited resources. We consider a team of robots tasked\nwith achieving a desired formation from arbitrary initial configurations. To\nreduce unnecessary control updates and conserve resources, we propose a\ndistributed event-triggered formation controller that relies on inter-agent\ndistance measurements. Control updates are triggered only when the measurement\nerror exceeds a predefined threshold, ensuring system stability. The proposed\ncontroller is validated through extensive simulations and real-world\nexperiments involving different formations, communication topologies,\nscalability tests, and variations in design parameters, while also being\ncompared against periodic triggering strategies. Results demonstrate that the\nevent-triggered approach significantly reduces control efforts while preserving\nformation performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u89e6\u53d1\u7684\u5206\u5e03\u5f0f\u7f16\u961f\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u8ddd\u79bb\u6d4b\u91cf\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f16\u961f\u63a7\u5236\uff0c\u4ec5\u5728\u6d4b\u91cf\u8bef\u5dee\u8d85\u8fc7\u9608\u503c\u65f6\u66f4\u65b0\u63a7\u5236\uff0c\u663e\u8457\u51cf\u5c11\u63a7\u5236\u8d44\u6e90\u6d88\u8017", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6709\u9650\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u7f16\u961f\u63a7\u5236\u95ee\u9898\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u63a7\u5236\u66f4\u65b0\u4ee5\u8282\u7701\u8d44\u6e90", "method": "\u5206\u5e03\u5f0f\u4e8b\u4ef6\u89e6\u53d1\u7f16\u961f\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u667a\u80fd\u4f53\u95f4\u8ddd\u79bb\u6d4b\u91cf\uff0c\u5f53\u6d4b\u91cf\u8bef\u5dee\u8d85\u8fc7\u9884\u8bbe\u9608\u503c\u65f6\u89e6\u53d1\u63a7\u5236\u66f4\u65b0", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e8b\u4ef6\u89e6\u53d1\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u63a7\u5236\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7f16\u961f\u6027\u80fd", "conclusion": "\u4e8b\u4ef6\u89e6\u53d1\u7b56\u7565\u76f8\u6bd4\u5468\u671f\u6027\u89e6\u53d1\u7b56\u7565\u80fd\u6709\u6548\u8282\u7701\u63a7\u5236\u8d44\u6e90\uff0c\u5728\u4fdd\u8bc1\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387"}}
{"id": "2509.12458", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12458", "abs": "https://arxiv.org/abs/2509.12458", "authors": ["\u00c0lmos Veres-Vit\u00e0lyos", "Genis Castillo Gomez-Raya", "Filip Lemic", "Daniel Johannes Bugelnig", "Bernhard Rinner", "Sergi Abadal", "Xavier Costa-P\u00e9rez"], "title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles", "comment": "13 pages, 16 figures, 3 tables, 45 references", "summary": "Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for\nnavigating indoor and hard-to-reach areas, yet their significant constraints in\npayload and autonomy have largely prevented their use for complex tasks like\nhigh-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we\nintroduce a novel system architecture that enables fully autonomous,\nhigh-fidelity 3D scanning of static objects using UAVs weighing under 100\ngrams. Our core innovation lies in a dual-reconstruction pipeline that creates\na real-time feedback loop between data capture and flight control. A\nnear-real-time (near-RT) process uses Structure from Motion (SfM) to generate\nan instantaneous pointcloud of the object. The system analyzes the model\nquality on the fly and dynamically adapts the UAV's trajectory to intelligently\ncapture new images of poorly covered areas. This ensures comprehensive data\nacquisition. For the final, detailed output, a non-real-time (non-RT) pipeline\nemploys a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)\napproach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)\nlocation data to achieve superior accuracy. We implemented and validated this\narchitecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both\nsingle- and multi-UAV configurations, conclusively show that dynamic trajectory\nadaptation consistently improves reconstruction quality over static flight\npaths. This work demonstrates a scalable and autonomous solution that unlocks\nthe potential of miniaturized UAVs for fine-grained 3D reconstruction in\nconstrained environments, a capability previously limited to much larger\nplatforms.", "AI": {"tldr": "\u4f7f\u7528\u8d85\u8f7b\u5782\u76f4\u8d77\u98de\u5668\u5b9e\u73b0\u81ea\u4e3b\u9ad8\u7cbe\u5ea63D\u91cd\u5efa\u7684\u65b0\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u91cd\u5efa\u6d41\u6c34\u7ebf\u548c\u52a8\u6001\u8f68\u8ff9\u8c03\u6574\u63d0\u5347\u626b\u63cf\u8d28\u91cf", "motivation": "\u89e3\u51b3\u8f7b\u578b\u65e0\u4eba\u673a\u5728\u8d1f\u8f7d\u548c\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u4ee5\u652f\u6301\u590d\u6742\u4efb\u52a1\u5982\u9ad8\u8d28\u91cf3D\u91cd\u5efa", "method": "\u53cc\u91cd\u5efa\u6d41\u6c34\u7ebf\u7cfb\u7edf\uff1a\u8fd1\u5b9e\u65f6SfM\u70b9\u4e91\u751f\u6210\u4e0e\u52a8\u6001\u8f68\u8ff9\u8c03\u6574\u5f62\u6210\u53cd\u9988\u5faa\u73af\uff0c\u975e\u5b9e\u65f6N3DR\u7ba1\u9053\u7ed3\u5408SfM\u548cUWB\u6570\u636e\u8fdb\u884c\u7cbe\u786e\u91cd\u5efa", "result": "\u52a8\u6001\u8f68\u8ff9\u8c03\u6574\u5728\u5355\u673a\u548c\u591a\u673a\u914d\u7f6e\u4e2d\u5747\u80fd\u6301\u7eed\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u8d85\u8fc7100\u514b\u7684Crazyflie 2.1\u65e0\u4eba\u673a\u6210\u529f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u626b\u63cf", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5c0f\u578b\u5316\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u542f\u4e86\u5728\u7a7a\u95f4\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea63D\u91cd\u5efa\u7684\u65b0\u53ef\u80fd\u6027"}}
{"id": "2509.12524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12524", "abs": "https://arxiv.org/abs/2509.12524", "authors": ["Rohit Chakraborty", "Subasish Das"], "title": "A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights", "comment": "This is the author's preprint version of a paper accepted for\n  presentation at HICSS 59 (Hawaii International Conference on System\n  Sciences), 2026, Hawaii, USA. The final published version will appear in the\n  official conference proceedings. Conference site: https://hicss.hawaii.edu/", "summary": "Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This\nstudy analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable\nworkflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors\nand yields four crash patterns. A tree-based severity model is then interpreted\nwith SHAP to quantify drivers of injury within and across patterns. Results\nshow higher severity when darkness, wet surfaces, and higher posted speeds\ncoincide with fixed-object or angle events, and lower severity in clear,\nlow-speed settings. Pattern-specific explanations highlight mechanisms at\nentries (fail-to-yield, gap acceptance), within multi-lane circulation\n(improper maneuvers), and during slow-downs (rear-end). The workflow links\npattern discovery with case-level explanations, supporting site screening,\ncountermeasure selection, and audit-ready reporting. The contribution to\nInformation Systems is a practical template for usable XAI in public safety\nanalytics.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6d41\u7a0b\u5206\u6790\u4e86\u4e27\u751f\u5706\u76f8\u4e92\u76f8\u5173\u7684\u4e25\u91cd\u4ea4\u901a\u4e8b\u6545\u6a21\u5f0f\uff0c\u8bc6\u522b\u4e86\u56db\u79cd\u4e8b\u6545\u6a21\u5f0f\u5e76\u91cf\u5316\u4e86\u4e25\u91cd\u7a0b\u5ea6\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u867d\u7136\u5706\u76f8\u4e92\u80fd\u51cf\u5c11\u4e25\u91cd\u4ea4\u901a\u4e8b\u6545\uff0c\u4f46\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u98ce\u9669\u6a21\u5f0f\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u6df1\u5165\u5206\u6790\u4ee5\u652f\u6301\u6709\u6548\u7684\u5b89\u5168\u7ba1\u7406\u7b56\u7565\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u53ef\u89e3\u91ca\u6027\u5de5\u4f5c\u6d41\u7a0b\uff1a\u9996\u5148\u7528\u805a\u7c7b\u5bf9\u5e94\u5206\u6790(CCA)\u8bc6\u522b\u56db\u79cd\u4e8b\u6545\u6a21\u5f0f\uff0c\u7136\u540e\u7528\u6811\u57fa\u6a21\u578b\u7ed3\u5408SHAP\u65b9\u6cd5\u91cf\u5316\u4e25\u91cd\u7a0b\u5ea6\u7684\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u9ed1\u6697\u3001\u6e7f\u6ed1\u5730\u9762\u3001\u9ad8\u901f\u5ea6\u4e0e\u56fa\u5b9a\u7269\u4f53\u78b0\u649e\u6216\u89d2\u5ea6\u4e8b\u6545\u540c\u65f6\u51fa\u73b0\u65f6\u4e25\u91cd\u7a0b\u5ea6\u9ad8\uff1b\u660e\u4eae\u3001\u4f4e\u901f\u73af\u5883\u4e0b\u4e25\u91cd\u7a0b\u5ea6\u4f4e\u3002\u4e0d\u540c\u6a21\u5f0f\u6709\u7279\u5b9a\u7684\u4e8b\u6545\u673a\u5236\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u5c06\u6a21\u5f0f\u53d1\u73b0\u4e0e\u6848\u4f8b\u7ea7\u89e3\u91ca\u76f8\u7ed3\u5408\uff0c\u4e3a\u516c\u5171\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u7528\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6a21\u677f\uff0c\u652f\u6301\u573a\u666f\u7b5b\u9009\u3001\u63aa\u65bd\u9009\u62e9\u548c\u5ba1\u8ba1\u62a5\u544a\u3002"}}
{"id": "2509.12453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12453", "abs": "https://arxiv.org/abs/2509.12453", "authors": ["Yiran Song", "Yikai Zhang", "Silvia Orengo-Nania", "Nian Wang", "Fenglong Ma", "Rui Zhang", "Yifan Peng", "Mingquan Lin"], "title": "Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis", "comment": "11 pages.2 figures, 4 tables", "summary": "Glaucoma is one of the leading causes of irreversible blindness worldwide.\nGlaucoma prognosis is essential for identifying at-risk patients and enabling\ntimely intervention to prevent blindness. Many existing approaches rely on\nhistorical sequential data but are constrained by fixed-length inputs, limiting\ntheir flexibility. Additionally, traditional glaucoma prognosis methods often\nemploy end-to-end models, which struggle with the limited size of glaucoma\ndatasets. To address these challenges, we propose a Two-Stage Decoupling\nFramework (TSDF) for variable-length glaucoma prognosis. In the first stage, we\nemploy a feature representation module that leverages self-supervised learning\nto aggregate multiple glaucoma datasets for training, disregarding differences\nin their supervisory information. This approach enables datasets of varying\nsizes to learn better feature representations. In the second stage, we\nintroduce a temporal aggregation module that incorporates an attention-based\nmechanism to process sequential inputs of varying lengths, ensuring flexible\nand efficient utilization of all available data. This design significantly\nenhances model performance while maintaining a compact parameter size.\nExtensive experiments on two benchmark glaucoma datasets:the Ocular\nHypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal\nProgression Ensemble (GRAPE),which differ significantly in scale and clinical\nsettings,demonstrate the effectiveness and robustness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u89e3\u8026\u6846\u67b6TSDF\u7528\u4e8e\u53ef\u53d8\u957f\u5ea6\u9752\u5149\u773c\u9884\u540e\u9884\u6d4b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6574\u5408\u591a\u6570\u636e\u96c6\u7279\u5f81\u8868\u793a\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u53d8\u957f\u5e8f\u5217\u8f93\u5165\uff0c\u5728\u4e24\u4e2a\u4e0d\u540c\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u9752\u5149\u773c\u9884\u540e\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u957f\u5ea6\u8f93\u5165\u4e14\u7aef\u5230\u7aef\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u5904\u7406\u53d8\u957f\u5e8f\u5217\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6574\u5408\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u7279\u5f81\u8868\u793a\u6a21\u5757\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65f6\u5e8f\u805a\u5408\u6a21\u5757\u5904\u7406\u53d8\u957f\u5e8f\u5217\u8f93\u5165", "result": "\u5728OHTS\u548cGRAPE\u4e24\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u4e34\u5e8a\u8bbe\u7f6e\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u7d27\u51d1", "conclusion": "TSDF\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u9752\u5149\u773c\u9884\u540e\u4e2d\u7684\u53d8\u957f\u5e8f\u5217\u6570\u636e\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u7279\u5f81\u5b66\u4e60\u548c\u65f6\u5e8f\u805a\u5408\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2509.12652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12652", "abs": "https://arxiv.org/abs/2509.12652", "authors": ["Paul Kr\u00f6ger", "Emilio Barkett"], "title": "Don't Change My View: Ideological Bias Auditing in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in products used\nby millions, their outputs may influence individual beliefs and, cumulatively,\nshape public opinion. If the behavior of LLMs can be intentionally steered\ntoward specific ideological positions, such as political or religious views,\nthen those who control these systems could gain disproportionate influence over\npublic discourse. Although it remains an open question whether LLMs can\nreliably be guided toward coherent ideological stances and whether such\nsteering can be effectively prevented, a crucial first step is to develop\nmethods for detecting when such steering attempts occur. In this work, we adapt\na previously proposed statistical method to the new context of ideological bias\nauditing. Our approach carries over the model-agnostic design of the original\nframework, which does not require access to the internals of the language\nmodel. Instead, it identifies potential ideological steering by analyzing\ndistributional shifts in model outputs across prompts that are thematically\nrelated to a chosen topic. This design makes the method particularly suitable\nfor auditing proprietary black-box systems. We validate our approach through a\nseries of experiments, demonstrating its practical applicability and its\npotential to support independent post hoc audits of LLM behavior.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u610f\u8bc6\u5f62\u6001\u504f\u5411\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5ba1\u8ba1\u4e13\u6709\u9ed1\u76d2\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea7\u54c1\u4e2d\u7684\u666e\u53ca\uff0c\u5176\u8f93\u51fa\u53ef\u80fd\u5f71\u54cd\u516c\u4f17\u610f\u89c1\u3002\u5982\u679c\u6a21\u578b\u80fd\u591f\u88ab\u5076\u7136\u5730\u5411\u7279\u5b9a\u610f\u8bc6\u5f62\u6001\u504f\u5411\uff0c\u63a7\u5236\u8005\u5c06\u83b7\u5f97\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\u529b\u3002\u9700\u8981\u68c0\u6d4b\u8fd9\u79cd\u504f\u5411\u5c1d\u8bd5\u3002", "method": "\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u4e0e\u7279\u5b9a\u4e3b\u9898\u76f8\u5173\u63d0\u793a\u4e0b\u8f93\u51fa\u7684\u5206\u5e03\u79fb\u52a8\u6765\u8bc6\u522b\u610f\u8bc6\u5f62\u6001\u504f\u5411\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u652f\u6301\u5bf9LLM\u884c\u4e3a\u7684\u72ec\u7acb\u540e\u7f6e\u5ba1\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u610f\u8bc6\u5f62\u6001\u504f\u5411\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5ba1\u8ba1\u4e13\u6709\u9ed1\u76d2\u7cfb\u7edf\uff0c\u6709\u52a9\u4e8e\u7ef4\u62a4\u516c\u5171\u8bae\u7a0b\u7684\u516c\u6b63\u6027\u3002"}}
{"id": "2509.12611", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12611", "abs": "https://arxiv.org/abs/2509.12611", "authors": ["Anmol Singhal Navya Singhal"], "title": "Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis", "comment": "IEEE AIxB 2025", "summary": "Financial news sentiment analysis is crucial for anticipating market\nmovements. With the rise of AI techniques such as Large Language Models (LLMs),\nwhich demonstrate strong text understanding capabilities, there has been\nrenewed interest in enhancing these systems. Existing methods, however, often\nstruggle to capture the complex economic context of news and lack transparent\nreasoning, which undermines their reliability. We propose Analogy-Driven\nFinancial Chain-of-Thought (AD-FCoT), a prompting framework that integrates\nanalogical reasoning with chain-of-thought (CoT) prompting for sentiment\nprediction on historical financial news. AD-FCoT guides LLMs to draw parallels\nbetween new events and relevant historical scenarios with known outcomes,\nembedding these analogies into a structured, step-by-step reasoning chain. To\nour knowledge, this is among the first approaches to explicitly combine\nanalogical examples with CoT reasoning in finance. Operating purely through\nprompting, AD-FCoT requires no additional training data or fine-tuning and\nleverages the model's internal financial knowledge to generate rationales that\nmirror human analytical reasoning. Experiments on thousands of news articles\nshow that AD-FCoT outperforms strong baselines in sentiment classification\naccuracy and achieves substantially higher correlation with market returns. Its\ngenerated explanations also align with domain expertise, providing\ninterpretable insights suitable for real-world financial analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86AD-FCoT\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u6bd4\u63a8\u7406\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u5e02\u573a\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7ecf\u6d4e\u80cc\u666f\u4e14\u7f3a\u4e4f\u900f\u660e\u63a8\u7406\uff0c\u5f71\u54cd\u4e86\u53ef\u9760\u6027\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5907\u5f3a\u5927\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5904\u7406\u91d1\u878d\u9886\u57df\u7684\u7279\u5b9a\u9700\u6c42\u3002", "method": "AD-FCoT\u6846\u67b6\u7ed3\u5408\u7c7b\u6bd4\u63a8\u7406\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u5f15\u5bfcLLM\u5728\u65b0\u4e8b\u4ef6\u4e0e\u5df2\u77e5\u7ed3\u679c\u7684\u5386\u53f2\u60c5\u666f\u4e4b\u95f4\u5efa\u7acb\u5e73\u884c\u5173\u7cfb\uff0c\u5e76\u5d4c\u5165\u5230\u7ed3\u6784\u5316\u63a8\u7406\u94fe\u4e2d\u3002\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u6216\u5fae\u8c03\u3002", "result": "\u5728\u6570\u5343\u7bc7\u65b0\u95fb\u6587\u7ae0\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cAD-FCoT\u5728\u60c5\u611f\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5e02\u573a\u56de\u62a5\u76f8\u5173\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u89e3\u91ca\u4e5f\u4e0e\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e00\u81f4\u3002", "conclusion": "AD-FCoT\u662f\u9996\u6279\u5728\u91d1\u878d\u9886\u57df\u660e\u786e\u5c06\u7c7b\u6bd4\u793a\u4f8b\u4e0e\u601d\u7ef4\u94fe\u63a8\u7406\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4e3a\u5b9e\u9645\u91d1\u878d\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\uff0c\u5c55\u73b0\u4e86\u7eaf\u63d0\u793a\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.12720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12720", "abs": "https://arxiv.org/abs/2509.12720", "authors": ["Biswadip Mandal", "Anant Khandelwal", "Manish Gupta"], "title": "HistoryBankQA: Multilingual Temporal Question Answering on Historical Events", "comment": null, "summary": "Temporal reasoning about historical events is a critical skill for NLP tasks\nlike event extraction, historical entity linking, temporal question answering,\ntimeline summarization, temporal event clustering and temporal natural language\ninference. Yet efforts on benchmarking temporal reasoning capabilities of large\nlanguage models (LLMs) are rather limited. Existing temporal reasoning datasets\nare limited in scale, lack multilingual coverage and focus more on contemporary\nevents. To address these limitations, we present HistoryBank, a multilingual\ndatabase of 10M+ historical events extracted from Wikipedia timeline pages and\narticle infoboxes. Our database provides unprecedented coverage in both\nhistorical depth and linguistic breadth with 10 languages. Additionally, we\nconstruct a comprehensive question answering benchmark for temporal reasoning\nacross all languages. This benchmark covers a diverse set of 6 temporal QA\nreasoning tasks, and we evaluate a suite of popular language models\n(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their\nperformance on these tasks. As expected GPT4o performs best across all answer\ntypes and languages; Gemma-2 outperforms the other small language models. Our\nwork aims to provide a comprehensive resource for advancing multilingual and\ntemporally-aware natural language understanding of historical events. To\nfacilitate further research, we will make our code and datasets publicly\navailable upon acceptance of this paper.", "AI": {"tldr": "HistoryBank\u662f\u4e00\u4e2a\u5305\u542b1000\u591a\u4e07\u4e2a\u5386\u53f2\u4e8b\u4ef6\u7684\u591a\u8bed\u8a00\u6570\u636e\u5e93\uff0c\u6765\u81ea\u7ef4\u57fa\u767e\u79d1\u65f6\u95f4\u7ebf\u9875\u9762\u548c\u4fe1\u606f\u6846\uff0c\u6db5\u76d610\u79cd\u8bed\u8a00\uff0c\u5e76\u6784\u5efa\u4e86\u5168\u9762\u7684\u65f6\u95f4\u63a8\u7406\u95ee\u7b54\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u63a8\u7406\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u3001\u7f3a\u4e4f\u591a\u8bed\u8a00\u8986\u76d6\u4e14\u4e3b\u8981\u5173\u6ce8\u5f53\u4ee3\u4e8b\u4ef6\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8d44\u6e90\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u4e8b\u4ef6\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4ece\u7ef4\u57fa\u767e\u79d1\u65f6\u95f4\u7ebf\u9875\u9762\u548c\u6587\u7ae0\u4fe1\u606f\u6846\u4e2d\u63d0\u53d6\u5386\u53f2\u4e8b\u4ef6\uff0c\u6784\u5efa\u5305\u542b10M+\u4e8b\u4ef6\u7684\u591a\u8bed\u8a00\u6570\u636e\u5e93\uff0c\u5e76\u8bbe\u8ba1\u6db5\u76d66\u79cd\u65f6\u95f4QA\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "GPT4o\u5728\u6240\u6709\u7b54\u6848\u7c7b\u578b\u548c\u8bed\u8a00\u4e2d\u8868\u73b0\u6700\u4f73\uff1bGemma-2\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\u3002\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u5386\u53f2\u6df1\u5ea6\u548c\u8bed\u8a00\u5e7f\u5ea6\u8986\u76d6\u3002", "conclusion": "HistoryBank\u4e3a\u63a8\u8fdb\u591a\u8bed\u8a00\u548c\u65f6\u95f4\u611f\u77e5\u7684\u5386\u53f2\u4e8b\u4ef6\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u8d44\u6e90\uff0c\u5c06\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.12876", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.12876", "abs": "https://arxiv.org/abs/2509.12876", "authors": ["Fuyu Xing", "Zimu Wang", "Wei Wang", "Haiyang Zhang"], "title": "Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents", "comment": "Accepted at INLG 2025. Camera-ready version", "summary": "The proliferation of multimedia content necessitates the development of\neffective Multimedia Event Extraction (M2E2) systems. Though Large\nVision-Language Models (LVLMs) have shown strong cross-modal capabilities,\ntheir utility in the M2E2 task remains underexplored. In this paper, we present\nthe first systematic evaluation of representative LVLMs, including DeepSeek-VL2\nand the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,\nimage-only, and cross-media subtasks, assessed under both few-shot prompting\nand fine-tuning settings. Our key findings highlight the following valuable\ninsights: (1) Few-shot LVLMs perform notably better on visual tasks but\nstruggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA\nsubstantially enhances model performance; and (3) LVLMs exhibit strong synergy\nwhen combining modalities, achieving superior performance in cross-modal\nsettings. We further provide a detailed error analysis to reveal persistent\nchallenges in areas such as semantic precision, localization, and cross-modal\ngrounding, which remain critical obstacles for advancing M2E2 capabilities.", "AI": {"tldr": "\u5bf9DeepSeek-VL2\u548cQwen-VL\u7cfb\u5217LVLM\u6a21\u578b\u5728M2E2\u6570\u636e\u96c6\u4e0a\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u5c11\u6837\u672c\u63d0\u793a\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\uff0c\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u8868\u73b0\u826f\u597d\u4f46\u6587\u672c\u4efb\u52a1\u56f0\u96be\uff0c\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u591a\u6a21\u6001\u7ed3\u5408\u5c55\u73b0\u5f3a\u534f\u540c\u6548\u5e94", "motivation": "\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u6fc0\u589e\u9700\u8981\u6709\u6548\u7684\u591a\u5a92\u4f53\u4e8b\u4ef6\u62bd\u53d6\u7cfb\u7edf\uff0c\u4f46\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728M2E2\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22", "method": "\u5728M2E2\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4ee3\u8868\u6027LVLM\u6a21\u578b\uff0c\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u548c\u8de8\u5a92\u4f53\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u548cLoRA\u5fae\u8c03\u4e24\u79cd\u8bbe\u7f6e", "result": "\u5c11\u6837\u672cLVLM\u5728\u89c6\u89c9\u4efb\u52a1\u8868\u73b0\u66f4\u597d\u4f46\u6587\u672c\u4efb\u52a1\u56f0\u96be\uff1bLoRA\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b\u591a\u6a21\u6001\u7ed3\u5408\u5c55\u73b0\u5f3a\u534f\u540c\u6548\u5e94\uff0c\u5728\u8de8\u6a21\u6001\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LVLM\u5728M2E2\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u6307\u51fa\u4e86\u8bed\u4e49\u7cbe\u5ea6\u3001\u5b9a\u4f4d\u548c\u8de8\u6a21\u6001\u57fa\u7840\u7b49\u6301\u7eed\u6311\u6218\uff0c\u4e3a\u63a8\u8fdbM2E2\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3"}}
{"id": "2509.12372", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12372", "abs": "https://arxiv.org/abs/2509.12372", "authors": ["Konstantinos Vasili", "Zachery T. Dahm", "Stylianos Chatzidakis"], "title": "Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder", "comment": null, "summary": "The nuclear industry is advancing toward more new reactor designs, with\nnext-generation reactors expected to be smaller in scale and power output.\nThese systems have the potential to produce large volumes of information in the\nform of multivariate time-series data, which could be used for enhanced\nreal-time monitoring and control. In this context, the development of remote\nautonomous or semi-autonomous control systems for reactor operation has gained\nsignificant interest. A critical first step toward such systems is an accurate\ndiagnostics module capable of detecting and localizing anomalies within the\nreactor system. Recent studies have proposed various ML and DL approaches for\nanomaly detection in the nuclear domain. Despite promising results, key\nchallenges remain, including limited to no explainability, lack of access to\nreal-world data, and scarcity of abnormal events, which impedes benchmarking\nand characterization. Most existing studies treat these methods as black boxes,\nwhile recent work highlights the need for greater interpretability of ML/DL\noutputs in safety-critical domains. Here, we propose an unsupervised\nmethodology based on an LSTM autoencoder with a dual attention mechanism for\ncharacterization of abnormal events in a real-world reactor radiation area\nmonitoring system. The framework includes not only detection but also\nlocalization of the event and was evaluated using real-world datasets of\nincreasing complexity from the PUR-1 research reactor. The attention mechanisms\noperate in both the feature and temporal dimensions, where the feature\nattention assigns weights to radiation sensors exhibiting abnormal patterns,\nwhile time attention highlights the specific timesteps where irregularities\noccur, thus enabling localization. By combining the results, the framework can\nidentify both the affected sensors and the duration of each anomaly within a\nsingle unified network.", "AI": {"tldr": "\u57fa\u4e8e\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236LSTM\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6838\u53cd\u5e94\u5806\u76d1\u6d4b\u4e2d\u8fdb\u884c\u5f02\u5e38\u4e8b\u4ef6\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u5b9a\u65f6", "motivation": "\u4e0b\u4e00\u4ee3\u6838\u53cd\u5e94\u5806\u4ea7\u751f\u5927\u91cf\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u9700\u8981\u8fdc\u7a0b\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u3002\u73b0\u6709ML/DL\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3001\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u548c\u5f02\u5e38\u4e8b\u4ef6\u7a00\u7f3a\u7b49\u6311\u6218", "method": "\u4f7f\u7528LSTM\u81ea\u52a8\u7f16\u7801\u5668\u7ed3\u5408\u7279\u5f81\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7279\u5f81\u6ce8\u610f\u529b\u5206\u914d\u6743\u91cd\u7ed9\u5f02\u5e38\u6a21\u5f0f\u7684\u8f90\u5c04\u4f20\u611f\u5668\uff0c\u65f6\u95f4\u6ce8\u610f\u529b\u7a81\u51fa\u53d1\u751f\u4e0d\u5e38\u7684\u5177\u4f53\u65f6\u523b", "result": "\u5728PUR-1\u7814\u7a76\u53cd\u5e94\u5806\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6846\u67b6\u80fd\u591f\u5728\u5355\u4e00\u7edf\u4e00\u7f51\u7edc\u4e2d\u8bc6\u522b\u53d7\u5f71\u54cd\u4f20\u611f\u5668\u548c\u5f02\u5e38\u6301\u7eed\u65f6\u95f4", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5b9e\u73b0\u5f02\u5e38\u4e8b\u4ef6\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u5b9a\u65f6\uff0c\u4e3a\u8fdc\u7a0b\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u57fa\u7840"}}
{"id": "2509.13047", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.13047", "abs": "https://arxiv.org/abs/2509.13047", "authors": ["Nolan Platt", "Pragyansmita Nayak"], "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models", "comment": "8 pages. Accepted as a full paper to the 3rd International Conference\n  on Foundation and Large Language Models (IEEE FLLM) 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.", "AI": {"tldr": "\u4f7f\u7528LLMs\u4f5c\u4e3a\u4e00\u6b21\u6027\u6559\u5e08\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u800c\u975e\u76f4\u63a5\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86261\u500d\u6210\u672c\u964d\u4f4e\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u4f3c\u7684\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u5e94\u7528\u4e2d\u7684\u9ad8\u6602\u63a8\u7406\u6210\u672c", "method": "\u5229\u7528GPT-4o\u548co3-mini\u591a\u6a21\u578b\u751f\u621032\u4ebf\u6761AIS\u8239\u8236\u8ddf\u8e2a\u8bb0\u5f55\u768421,543\u4e2a\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u5e76\u786e\u4fdd\u51c6\u786e\u63a8\u7406\uff0c\u7136\u540e\u5fae\u8c03Qwen2.5-7B\u6a21\u578b", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u6d77\u4e8b\u4efb\u52a1\u4e0a\u8fbe\u523075%\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4f7f\u7528\u66f4\u5927\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u6210\u672c\u5927\u5e45\u964d\u4f4e", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9002\u5f53\u5fae\u8c03\uff0c\u5c0f\u578b\u5ec9\u4ef7\u6a21\u578b\u53ef\u4ee5\u5728\u4e13\u4e1a\u9886\u57df\u63d0\u4f9b\u4e0e\u6602\u8d35\u5927\u6a21\u578b\u76f8\u4f3c\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u624b\u52a8\u6807\u6ce8\u4e0d\u53ef\u884c\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6846\u67b6"}}
{"id": "2509.13011", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13011", "abs": "https://arxiv.org/abs/2509.13011", "authors": ["Yuyang Tian", "Shunqiang Mao", "Wenchang Gao", "Lanlan Qiu", "Tianxing He"], "title": "A Visualized Framework for Event Cooperation with Generative Agents", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants.", "AI": {"tldr": "MiniAgentPro\u662f\u4e00\u4e2a\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u4e8b\u4ef6\u7ec4\u7ec7\u548c\u534f\u8c03\u80fd\u529b\uff0c\u5305\u542b\u5730\u56fe\u7f16\u8f91\u5668\u548c\u6a21\u62df\u64ad\u653e\u5668\uff0c\u901a\u8fc78\u4e2a\u4e0d\u540c\u4e8b\u4ef6\u573a\u666f\u7684\u6d4b\u8bd5\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u793e\u4f1a\u6a21\u62df\u6846\u67b6\u7f3a\u4e4f\u7cfb\u7edf\u7684\u4e8b\u4ef6\u7ec4\u7ec7\u8bc4\u4f30\u548c\u7269\u7406\u73af\u5883\u53ef\u89c6\u5316\u96c6\u6210\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u5728\u7a7a\u95f4\u4e2d\u5bfc\u822a\u548c\u4e0e\u73b0\u5b9e\u7269\u54c1\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1MiniAgentPro\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u5305\u542b\u76f4\u89c2\u7684\u5730\u56fe\u7f16\u8f91\u5668\u7528\u4e8e\u5b9a\u5236\u73af\u5883\uff0c\u4ee5\u53ca\u5e26\u6709\u6d41\u7545\u52a8\u753b\u7684\u6a21\u62df\u64ad\u653e\u5668\u3002\u57fa\u4e8e\u8be5\u5de5\u5177\u6784\u5efa\u5305\u542b8\u4e2a\u4e0d\u540c\u4e8b\u4ef6\u573a\u666f\uff08\u57fa\u7840\u7248\u548c\u56f0\u96be\u7248\u53d8\u4f53\uff09\u7684\u7efc\u5408\u6d4b\u8bd5\u96c6\u3002", "result": "\u4f7f\u7528GPT-4o\u8fdb\u884c\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u57fa\u7840\u8bbe\u7f6e\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u56f0\u96be\u53d8\u4f53\u4e2d\u7a81\u663e\u4e86\u534f\u8c03\u6311\u6218\u3002", "conclusion": "MiniAgentPro\u5e73\u53f0\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u7269\u7406\u73af\u5883\u96c6\u6210\u548c\u7cfb\u7edf\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u73af\u5883\u3002"}}
{"id": "2509.12982", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12982", "abs": "https://arxiv.org/abs/2509.12982", "authors": ["Erblin Isaku", "Hassan Sartaj", "Shaukat Ali", "Beatriz Sanguino", "Tongtong Wang", "Guoyuan Li", "Houxiang Zhang", "Thomas Peyrucain"], "title": "Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins", "comment": "15 pages, 4 figures, 3 tables", "summary": "Self-adaptive robots (SARs) in complex, uncertain environments must\nproactively detect and address abnormal behaviors, including\nout-of-distribution (OOD) cases. To this end, digital twins offer a valuable\nsolution for OOD detection. Thus, we present a digital twin-based approach for\nOOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to\nforecast SAR states and employs reconstruction error and Monte Carlo dropout\nfor uncertainty quantification. By combining reconstruction error with\npredictive variance, the digital twin effectively detects OOD behaviors, even\nin previously unseen conditions. The digital twin also includes an\nexplainability layer that links potential OOD to specific SAR states, offering\ninsights for self-adaptation. We evaluated ODiSAR by creating digital twins of\ntwo industrial robots: one navigating an office environment, and another\nperforming maritime ship navigation. In both cases, ODiSAR forecasts SAR\nbehaviors (i.e., robot trajectories and vessel motion) and proactively detects\nOOD events. Our results showed that ODiSAR achieved high detection performance\n-- up to 98\\% AUROC, 96\\% TNR@TPR95, and 95\\% F1-score -- while providing\ninterpretable insights to support self-adaptation.", "AI": {"tldr": "ODiSAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u72b6\u6001\uff0c\u7ed3\u5408\u91cd\u6784\u8bef\u5dee\u548c\u8499\u7279\u5361\u6d1bdropout\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe98%\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u673a\u5668\u4eba\u9700\u8981\u4e3b\u52a8\u68c0\u6d4b\u548c\u5904\u7406\u5f02\u5e38\u884c\u4e3a\uff08\u5305\u62ec\u5206\u5e03\u5916\u60c5\u51b5\uff09\uff0c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u673a\u5668\u4eba\u72b6\u6001\uff0c\u91c7\u7528\u91cd\u6784\u8bef\u5dee\u548c\u8499\u7279\u5361\u6d1bdropout\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u7ed3\u5408\u91cd\u6784\u8bef\u5dee\u548c\u9884\u6d4b\u65b9\u5dee\u6765\u68c0\u6d4bOOD\u884c\u4e3a\uff0c\u5e76\u5305\u542b\u53ef\u89e3\u91ca\u6027\u5c42\u5173\u8054\u7279\u5b9a\u72b6\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u5de5\u4e1a\u673a\u5668\u4eba\u6848\u4f8b\uff08\u529e\u516c\u5ba4\u5bfc\u822a\u548c\u6d77\u4e0a\u8239\u8236\u5bfc\u822a\uff09\u4e2d\uff0cODiSAR\u5b9e\u73b0\u4e86\u9ad8\u8fbe98% AUROC\u300196% TNR@TPR95\u548c95% F1-score\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002", "conclusion": "ODiSAR\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u81ea\u9002\u5e94\u673a\u5668\u4eba\u7684\u5206\u5e03\u5916\u884c\u4e3a\uff0c\u5373\u4f7f\u5728\u672a\u89c1\u8fc7\u7684\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5de5\u4f5c\u826f\u597d\uff0c\u5e76\u4e3a\u81ea\u6211\u9002\u5e94\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.13164", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13164", "abs": "https://arxiv.org/abs/2509.13164", "authors": ["Jiawei Wang", "Haowei Sun", "Xintao Yan", "Shuo Feng", "Jun Gao", "Henry X. Liu"], "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving", "comment": "8 pages, 6 figures. Codes and videos are available at\n  https://wjiawei.com/terasim-world-web/", "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems.", "AI": {"tldr": "TeraSim-World\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u5728\u5168\u7403\u4efb\u4f55\u5730\u65b9\u5408\u6210\u903c\u771f\u4e14\u5730\u7406\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u6570\u636e\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u4e3b\u8981\u6765\u81ea\u6a21\u62df\u5668\uff08\u5b58\u5728\u663e\u8457\u7684\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\uff09\u6216\u9053\u8def\u6d4b\u8bd5\uff08\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff09\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u5b89\u5168\u5173\u952e\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u4ece\u4efb\u610f\u4f4d\u7f6e\u68c0\u7d22\u771f\u5b9e\u4e16\u754c\u5730\u56fe\u548c\u4ea4\u901a\u9700\u6c42\uff0c\u6a21\u62df\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u7684\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u7f16\u6392\u591a\u6837\u5316\u9006\u5883\u521b\u5efa\u6781\u7aef\u60c5\u51b5\uff0c\u5e76\u57fa\u4e8e\u8857\u666f\u5b9e\u73b0\u903c\u771f\u7684\u5730\u7406\u63a5\u5730\u4f20\u611f\u5668\u6e32\u67d3\u3002", "result": "\u901a\u8fc7\u6865\u63a5\u667a\u80fd\u4f53\u548c\u4f20\u611f\u5668\u6a21\u62df\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5173\u952e\u6570\u636e\u5408\u6210\u6846\u67b6\u3002", "conclusion": "TeraSim-World\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12727", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12727", "abs": "https://arxiv.org/abs/2509.12727", "authors": ["Jie Yin", "Ke Sun", "Han Wu"], "title": "Unbiased Online Curvature Approximation for Regularized Graph Continual Learning", "comment": "9 pages", "summary": "Graph continual learning (GCL) aims to learn from a continuous sequence of\ngraph-based tasks. Regularization methods are vital for preventing catastrophic\nforgetting in GCL, particularly in the challenging replay-free,\nclass-incremental setting, where each task consists of a set of unique classes.\nIn this work, we first establish a general regularization framework for GCL\nbased on the curved parameter space induced by the Fisher information matrix\n(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its\nvariants are a special case within this framework, using a diagonal\napproximation of the empirical FIM based on parameters from previous tasks. To\novercome their limitations, we propose a new unbiased online curvature\napproximation of the full FIM based on the model's current learning state. Our\nmethod directly estimates the regularization term in an online manner without\nexplicitly evaluating and storing the FIM itself. This enables the model to\nbetter capture the loss landscape during learning new tasks while retaining the\nknowledge learned from previous tasks. Extensive experiments on three graph\ndatasets demonstrate that our method significantly outperforms existing\nregularization-based methods, achieving a superior trade-off between stability\n(retaining old knowledge) and plasticity (acquiring new knowledge).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u56fe\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u6b27\u6c0f\u66f2\u7387\u7a7a\u95f4\u6765\u9632\u6b62\u574f\u5fd8\uff0c\u5728\u4e09\u4e2a\u56fe\u6570\u636e\u96c6\u4e0a\u8f83\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u574f\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u91cd\u653e\u3001\u7c7b\u589e\u91cf\u8bbe\u7f6e\u4e0b\uff0c\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u5bf9\u6b27\u6c0f\u4fe1\u606f\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u6027\u8fd1\u4f3c\u5b58\u5728\u9650\u5236\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u6b27\u6c0f\u4fe1\u606f\u77e9\u9635\u5f15\u5bfc\u7684\u66f2\u7387\u53c2\u6570\u7a7a\u95f4\u6b63\u5219\u5316\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u504f\u7684\u5728\u7ebf\u66f2\u7387\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u76f4\u63a5\u4f30\u8ba1\u6b63\u5219\u5316\u9879\u800c\u4e0d\u663e\u5f0f\u8ba1\u7b97\u548c\u5b58\u50a8FIM\u3002", "result": "\u5728\u4e09\u4e2a\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u8d85\u8fc7\u4e86\u73b0\u6709\u6b63\u5219\u5316\u57fa\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6027\uff08\u4fdd\u7559\u65e7\u77e5\u8bc6\uff09\u548c\u53ef\u5851\u6027\uff08\u83b7\u53d6\u65b0\u77e5\u8bc6\uff09\u4e4b\u95f4\u7684\u4f18\u79f0\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u5b8c\u6574FIM\u7684\u66f2\u7387\u7a7a\u95f4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u63cf\u8ff0\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u7684\u635f\u5931\u5730\u5f62\uff0c\u540c\u65f6\u4fdd\u7559\u4e4b\u524d\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u4e3a\u56fe\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.12878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12878", "abs": "https://arxiv.org/abs/2509.12878", "authors": ["Qianguang Zhao", "Dongli Wang", "Yan Zhou", "Jianxun Li", "Richard Irampa"], "title": "Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation", "comment": null, "summary": "Few-shot 3D point cloud semantic segmentation aims to segment novel\ncategories using a minimal number of annotated support samples. While existing\nprototype-based methods have shown promise, they are constrained by two\ncritical challenges: (1) Intra-class Diversity, where a prototype's limited\nrepresentational capacity fails to cover a class's full variations, and (2)\nInter-set Inconsistency, where prototypes derived from the support set are\nmisaligned with the query feature space. Motivated by the powerful generative\ncapability of diffusion model, we re-purpose its pre-trained conditional\nencoder to provide a novel source of generalizable features for expanding the\nprototype's representational range. Under this setup, we introduce the\nPrototype Expansion Network (PENet), a framework that constructs big-capacity\nprototypes from two complementary feature sources. PENet employs a dual-stream\nlearner architecture: it retains a conventional fully supervised Intrinsic\nLearner (IL) to distill representative features, while introducing a novel\nDiffusion Learner (DL) to provide rich generalizable features. The resulting\ndual prototypes are then processed by a Prototype Assimilation Module (PAM),\nwhich adopts a novel push-pull cross-guidance attention block to iteratively\nalign the prototypes with the query space. Furthermore, a Prototype Calibration\nMechanism (PCM) regularizes the final big capacity prototype to prevent\nsemantic drift. Extensive experiments on the S3DIS and ScanNet datasets\ndemonstrate that PENet significantly outperforms state-of-the-art methods\nacross various few-shot settings.", "AI": {"tldr": "PENet\u662f\u4e00\u4e2a\u7528\u4e8e\u5c11\u6837\u672c3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u901a\u7528\u7279\u5f81\u6765\u6269\u5c55\u539f\u578b\u8868\u793a\u80fd\u529b\uff0c\u89e3\u51b3\u7c7b\u5185\u591a\u6837\u6027\u548c\u96c6\u5408\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u539f\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1)\u7c7b\u5185\u591a\u6837\u6027\u95ee\u9898 - \u539f\u578b\u7684\u6709\u9650\u8868\u793a\u80fd\u529b\u65e0\u6cd5\u8986\u76d6\u7c7b\u7684\u5168\u90e8\u53d8\u5316\uff1b(2)\u96c6\u5408\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898 - \u652f\u6301\u96c6\u751f\u6210\u7684\u539f\u578b\u4e0e\u67e5\u8be2\u7279\u5f81\u7a7a\u95f4\u4e0d\u5bf9\u9f50", "method": "\u63d0\u51fa\u539f\u578b\u6269\u5c55\u7f51\u7edc(PENet)\uff0c\u91c7\u7528\u53cc\u6d41\u5b66\u4e60\u5668\u67b6\u6784\uff1a\u56fa\u6709\u5b66\u4e60\u5668(IL)\u63d0\u53d6\u4ee3\u8868\u6027\u7279\u5f81\uff0c\u6269\u6563\u5b66\u4e60\u5668(DL)\u63d0\u4f9b\u4e30\u5bcc\u7684\u901a\u7528\u7279\u5f81\u3002\u901a\u8fc7\u539f\u578b\u540c\u5316\u6a21\u5757(PAM)\u548c\u539f\u578b\u6821\u51c6\u673a\u5236(PCM)\u5904\u7406\u53cc\u539f\u578b", "result": "\u5728S3DIS\u548cScanNet\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPENet\u5728\u5404\u79cd\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u6765\u6269\u5c55\u539f\u578b\u8868\u793a\u8303\u56f4\uff0cPENet\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd"}}
{"id": "2509.12959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12959", "abs": "https://arxiv.org/abs/2509.12959", "authors": ["Yuqi Xie", "Shuhan Ye", "Chong Wang", "Jiazhen Xu", "Le Shen", "Yuanbin Qian", "Jiangbo Qian"], "title": "Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain", "comment": null, "summary": "The integration of event cameras and spiking neural networks holds great\npromise for energy-efficient visual processing. However, the limited\navailability of event data and the sparse nature of DVS outputs pose challenges\nfor effective training. Although some prior work has attempted to transfer\nsemantic knowledge from RGB datasets to DVS, they often overlook the\nsignificant distribution gap between the two modalities. In this paper, we\npropose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing\nstrategy that exploits the asynchronous nature of SNNs by interpolating RGB and\nDVS inputs at various time-steps. To enable label mixing in cross-modal\nscenarios, we further introduce modality-aware auxiliary learning objectives.\nThese objectives support the time-step mixup process and enhance the model's\nability to discriminate effectively across different modalities. Our approach\nenables smoother knowledge transfer, alleviates modality shift during training,\nand achieves superior performance in spiking image classification tasks.\nExtensive experiments demonstrate the effectiveness of our method across\nmultiple datasets. The code will be released after the double-blind review\nprocess.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.13049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13049", "abs": "https://arxiv.org/abs/2509.13049", "authors": ["Yukun Chen", "Zhaoxi Mu", "Andong Li", "Peilin Li", "Xinyu Yang"], "title": "Spiking Vocos: An Energy-Efficient Neural Vocoder", "comment": null, "summary": "Despite the remarkable progress in the synthesis speed and fidelity of neural\nvocoders, their high energy consumption remains a critical barrier to practical\ndeployment on computationally restricted edge devices. Spiking Neural Networks\n(SNNs), widely recognized for their high energy efficiency due to their\nevent-driven nature, offer a promising solution for low-resource scenarios. In\nthis paper, we propose Spiking Vocos, a novel spiking neural vocoder with\nultra-low energy consumption, built upon the efficient Vocos framework. To\nmitigate the inherent information bottleneck in SNNs, we design a Spiking\nConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate\nan amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to\nbridge the performance gap with its Artificial Neural Network (ANN)\ncounterpart, we introduce a self-architectural distillation strategy to\neffectively transfer knowledge. A lightweight Temporal Shift Module is also\nintegrated to enhance the model's ability to fuse information across the\ntemporal dimension with negligible computational overhead. Experiments\ndemonstrate that our model achieves performance comparable to its ANN\ncounterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while\nconsuming only 14.7% of the energy. The source code is available at\nhttps://github.com/pymaster17/Spiking-Vocos.", "AI": {"tldr": "\u63d0\u51faSpiking Vocos\uff0c\u4e00\u79cd\u57fa\u4e8eSNN\u7684\u8d85\u4f4e\u80fd\u8017\u795e\u7ecf\u58f0\u7801\u5668\uff0c\u901a\u8fc7Spiking ConvNeXt\u6a21\u5757\u548c\u632f\u5e45\u6377\u5f84\u8def\u5f84\u89e3\u51b3\u4fe1\u606f\u74f6\u9888\uff0c\u6027\u80fd\u63a5\u8fd1ANN\u7248\u672c\u4f46\u80fd\u8017\u4ec514.7%", "motivation": "\u4f20\u7edf\u795e\u7ecf\u58f0\u7801\u5668\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u80fd\u8017\u8fc7\u9ad8\uff0cSNN\u56e0\u5176\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u5177\u6709\u9ad8\u80fd\u6548\u4f18\u52bf\uff0c\u9002\u5408\u4f4e\u8d44\u6e90\u573a\u666f", "method": "\u57fa\u4e8eVocos\u6846\u67b6\u6784\u5efaSNN\u58f0\u7801\u5668\uff0c\u8bbe\u8ba1Spiking ConvNeXt\u51cf\u5c11MAC\u64cd\u4f5c\uff0c\u52a0\u5165\u632f\u5e45\u6377\u5f84\u8def\u5f84\u4fdd\u6301\u4fe1\u53f7\u52a8\u6001\uff0c\u91c7\u7528\u81ea\u67b6\u6784\u84b8\u998f\u7b56\u7565\u548c\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u79fb\u4f4d\u6a21\u5757", "result": "\u6027\u80fd\u4e0eANN\u76f8\u5f53\uff08UTMOS 3.74\uff0cPESQ 3.45\uff09\uff0c\u80fd\u8017\u4ec5\u4e3aANN\u768414.7%", "conclusion": "Spiking Vocos\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u8d85\u4f4e\u80fd\u8017\u7684\u5e73\u8861\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13053", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.13053", "abs": "https://arxiv.org/abs/2509.13053", "authors": ["Lorenzo Pes", "Bojian Yin", "Sander Stuijk", "Federico Corradi"], "title": "Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks", "comment": null, "summary": "Spiking Neural Networks (SNNs) provide an efficient framework for processing\ndynamic spatio-temporal signals and for investigating the learning principles\nunderlying biological neural systems. A key challenge in training SNNs is to\nsolve both spatial and temporal credit assignment. The dominant approach for\ntraining SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.\nHowever, BPTT is in stark contrast with the spatial and temporal locality\nobserved in biological neural systems and leads to high computational and\nmemory demands, limiting efficient training strategies and on-device learning.\nAlthough existing local learning rules achieve local temporal credit assignment\nby leveraging eligibility traces, they fail to address the spatial credit\nassignment without resorting to auxiliary layer-wise matrices, which increase\nmemory overhead and hinder scalability, especially on embedded devices. In this\nwork, we propose Traces Propagation (TP), a forward-only, memory-efficient,\nscalable, and fully local learning rule that combines eligibility traces with a\nlayer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP\noutperforms other fully local learning rules on NMNIST and SHD datasets. On\nmore complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases\ncompetitive performance and scales effectively to deeper SNN architectures such\nas VGG-9, while providing favorable memory scaling compared to prior fully\nlocal scalable rules, for datasets with a significant number of classes.\nFinally, we show that TP is well suited for practical fine-tuning tasks, such\nas keyword spotting on the Google Speech Commands dataset, thus paving the way\nfor efficient learning at the edge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTraces Propagation (TP)\u7684\u524d\u5411\u4f20\u64ad\u3001\u5185\u5b58\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b8c\u5168\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff0c\u7528\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u65f6\u7a7a\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u65e0\u9700\u8f85\u52a9\u77e9\u9635\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684SNN\u8bad\u7ec3\u65b9\u6cd5\u5982BPTT\u4e0e\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7684\u65f6\u7a7a\u5c40\u90e8\u6027\u4e0d\u7b26\uff0c\u4e14\u8ba1\u7b97\u5185\u5b58\u9700\u6c42\u9ad8\u3002\u73b0\u6709\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\u867d\u7136\u5b9e\u73b0\u4e86\u65f6\u95f4\u4fe1\u7528\u5206\u914d\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u7a7a\u95f4\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u9700\u8981\u8f85\u52a9\u77e9\u9635\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927\u548c\u53ef\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51faTP\u5b66\u4e60\u89c4\u5219\uff0c\u5c06\u8d44\u683c\u8ff9\u4e0e\u5206\u5c42\u5bf9\u6bd4\u635f\u5931\u76f8\u7ed3\u5408\uff0c\u65e0\u9700\u8f85\u52a9\u5206\u5c42\u77e9\u9635\uff0c\u5b9e\u73b0\u5b8c\u5168\u5c40\u90e8\u7684\u524d\u5411\u4f20\u64ad\u5b66\u4e60\u3002", "result": "\u5728NMNIST\u548cSHD\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5b8c\u5168\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff1b\u5728DVS-GESTURE\u548cDVS-CIFAR10\u7b49\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\uff1b\u53ef\u6709\u6548\u6269\u5c55\u5230VGG-9\u7b49\u6df1\u5c42SNN\u67b6\u6784\uff1b\u5185\u5b58\u6269\u5c55\u6027\u4f18\u4e8e\u73b0\u6709\u5b8c\u5168\u5c40\u90e8\u53ef\u6269\u5c55\u89c4\u5219\uff1b\u9002\u7528\u4e8eGoogle Speech Commands\u7b49\u5b9e\u9645\u5fae\u8c03\u4efb\u52a1\u3002", "conclusion": "TP\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u662f\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b8c\u5168\u5c40\u90e8\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12997", "abs": "https://arxiv.org/abs/2509.12997", "authors": ["Anton Eldeborg Lundin", "Rasmus Winzell", "Hanna Hamrell", "David Gustafsson", "Hannes Ovr\u00e9n"], "title": "Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire", "comment": null, "summary": "Small drones are an increasing threat to both military personnel and civilian\ninfrastructure, making early and automated detection crucial. In this work we\ndevelop a system that uses spiking neural networks and neuromorphic cameras\n(event cameras) to detect drones. The detection model is deployed on a\nneuromorphic chip making this a fully neuromorphic system. Multiple detection\nunits can be deployed to create a virtual tripwire which detects when and where\ndrones enter a restricted zone. We show that our neuromorphic solution is\nseveral orders of magnitude more energy efficient than a reference solution\ndeployed on an edge GPU, allowing the system to run for over a year on battery\npower. We investigate how synthetically generated data can be used for\ntraining, and show that our model most likely relies on the shape of the drone\nrather than the temporal characteristics of its propellers. The small size and\nlow power consumption allows easy deployment in contested areas or locations\nthat lack power infrastructure.", "AI": {"tldr": "\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u76f8\u673a\u548c\u75af\u75b2\u795e\u7ecf\u7f51\u7edc\u7684\u4f4e\u529f\u8017\u65e0\u4eba\u673a\u68c0\u6d4b\u7cfb\u7edf\uff0c\u80fd\u5728\u7f51\u683c\u5316\u90e8\u7f72\u4e2d\u5b9e\u73b0\u8d85\u8fc7\u4e00\u5e74\u7684\u7535\u6c60\u6301\u7eed\u65f6\u95f4", "motivation": "\u5c0f\u578b\u65e0\u4eba\u673a\u5bf9\u519b\u4e8b\u548c\u6c11\u7528\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u9700\u8981\u65e9\u671f\u81ea\u52a8\u5316\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u795e\u7ecf\u79d1\u5b66\u76f8\u673a\uff08\u4e8b\u4ef6\u76f8\u673a\uff09\u548c\u75af\u75b2\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u68c0\u6d4b\u6a21\u578b\u90e8\u7f72\u5728\u795e\u7ecf\u79d1\u5b66\u82af\u7247\u4e0a\uff0c\u652f\u6301\u591a\u4e2a\u68c0\u6d4b\u5355\u5143\u7ec4\u6210\u865a\u62df\u62a4\u680f", "result": "\u7cfb\u7edf\u80fd\u8017\u6548\u7387\u6bd4\u8fb9\u7f18GPU\u89e3\u51b3\u65b9\u6848\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u53ef\u4ee5\u8fd0\u884c\u8d85\u8fc7\u4e00\u5e74\uff0c\u6a21\u578b\u4e3b\u8981\u4f9d\u9760\u65e0\u4eba\u673a\u5f62\u72b6\u800c\u975e\u53f6\u8f6e\u65f6\u5e8f\u7279\u5f81", "conclusion": "\u8be5\u795e\u7ecf\u79d1\u5b66\u89e3\u51b3\u65b9\u6848\u5177\u6709\u6781\u4f4e\u7684\u529f\u8017\u548c\u5c0f\u5f62\u5316\u7279\u6027\uff0c\u9002\u5408\u5728\u7f3a\u4e4f\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u7684\u5730\u533a\u6216\u4e89\u8bae\u533a\u57df\u90e8\u7f72"}}
