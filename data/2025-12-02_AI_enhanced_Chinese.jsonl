{"id": "2512.00234", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00234", "abs": "https://arxiv.org/abs/2512.00234", "authors": ["Sai Koneru", "Matthias Huck", "Jan Niehues"], "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion", "comment": "Preprint for ACL 2026", "summary": "There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.", "AI": {"tldr": "OmniFusion\uff1a\u878d\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u7ffb\u8bd1LLM\u7684\u7aef\u5230\u7aef\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u652f\u6301\u8bed\u97f3\u3001\u56fe\u50cf\u5230\u6587\u672c\u7684\u7ffb\u8bd1\uff0c\u76f8\u6bd4\u7ea7\u8054\u6d41\u6c34\u7ebf\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6587\u672c\u7ffb\u8bd1LLM\u53ea\u80fd\u7528\u4e8e\u7ea7\u8054\u8bed\u97f3\u7ffb\u8bd1\u6d41\u6c34\u7ebf\uff0c\u5bfc\u81f4\u989d\u5916\u5ef6\u8fdf\uff08\u5bf9\u540c\u4f20\u5c24\u5176\u5173\u952e\uff09\u4e14\u65e0\u6cd5\u5229\u7528\u56fe\u50cf\u7b49\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8fdb\u884c\u6d88\u6b67\u3002\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u867d\u5177\u5907\u8de8\u6a21\u6001\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u4e13\u4e1a\u7ffb\u8bd1LLM\u7684\u591a\u8bed\u8a00\u8986\u76d6\u548c\u7ffb\u8bd1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u878d\u5408\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08Omni 2.5-7B\uff09\u4e0e\u7ffb\u8bd1LLM\uff08SeedX PPO-7B\uff09\u8fde\u63a5\u3002\u521b\u65b0\u878d\u5408\u7b56\u7565\uff1a\u5c06MMFM\u591a\u4e2a\u9690\u85cf\u5c42\u72b6\u6001\u8fde\u63a5\u5230\u7ffb\u8bd1LLM\uff0c\u5b9e\u73b0\u8054\u5408\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "OmniFusion\u80fd\u6267\u884c\u8bed\u97f3\u5230\u6587\u672c\u3001\u8bed\u97f3+\u56fe\u50cf\u5230\u6587\u672c\u3001\u6587\u672c+\u56fe\u50cf\u5230\u6587\u672c\u7ffb\u8bd1\u3002\u5b9e\u9a8c\u8868\u660e\uff1a\u6709\u6548\u5229\u7528\u97f3\u9891\u548c\u89c6\u89c9\u8f93\u5165\uff0c\u5728\u540c\u4f20\u4e2d\u76f8\u6bd4\u7ea7\u8054\u6d41\u6c34\u7ebf\u51cf\u5c111\u79d2\u5ef6\u8fdf\uff0c\u540c\u65f6\u63d0\u5347\u6574\u4f53\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u4e13\u4e1a\u7ffb\u8bd1LLM\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u7ea7\u8054\u6d41\u6c34\u7ebf\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u80fd\u5229\u7528\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2512.00075", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00075", "abs": "https://arxiv.org/abs/2512.00075", "authors": ["Jun Jia", "Hongyi Miao", "Yingjie Zhou", "Wangqiu Zhou", "Jianbo Zhang", "Linhan Cao", "Dandan Zhu", "Hua Yang", "Xiongkuo Min", "Wei Sun", "Guangtao Zhai"], "title": "Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation", "comment": null, "summary": "With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdapter Shield\uff0c\u9996\u4e2a\u9488\u5bf9\u96f6\u6837\u672c\u56fe\u50cf\u751f\u6210\u573a\u666f\u7684\u901a\u7528\u8ba4\u8bc1\u96c6\u6210\u9632\u5fa1\u65b9\u6848\uff0c\u901a\u8fc7\u53ef\u9006\u52a0\u5bc6\u7cfb\u7edf\u548c\u591a\u76ee\u6807\u5bf9\u6297\u6270\u52a8\u4fdd\u62a4\u4e2a\u4eba\u56fe\u50cf\u514d\u906d\u672a\u7ecf\u6388\u6743\u7684\u8eab\u4efd\u514b\u9686\u548c\u98ce\u683c\u6a21\u4eff\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u96f6\u6837\u672c\u56fe\u50cf\u751f\u6210\u6280\u672f\u80fd\u591f\u4ec5\u7528\u4e00\u5f20\u8096\u50cf\u6216\u827a\u672f\u4f5c\u54c1\u5c31\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u8eab\u4efd\u590d\u5236\u6216\u98ce\u683c\u6a21\u4eff\uff0c\u8fd9\u867d\u7136\u589e\u5f3a\u4e86\u521b\u4f5c\u53ef\u80fd\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u98ce\u9669\uff0c\u5305\u62ec\u672a\u7ecf\u6388\u6743\u7684\u8eab\u4efd\u514b\u9686\u548c\u98ce\u683c\u6a21\u4eff\u3002", "method": "\u9996\u5148\u7814\u7a76\u5f53\u524d\u96f6\u6837\u672c\u65b9\u6cd5\u5982\u4f55\u901a\u8fc7\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u5d4c\u5165\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u8f93\u5165\u6269\u6563\u6a21\u578bUNet\u3002\u57fa\u4e8e\u6b64\u673a\u5236\u6784\u5efa\u53ef\u9006\u52a0\u5bc6\u7cfb\u7edf\uff0c\u5c06\u539f\u59cb\u5d4c\u5165\u6620\u5c04\u4e3a\u4e0d\u540c\u5bc6\u94a5\u5bf9\u5e94\u7684\u52a0\u5bc6\u8868\u793a\u3002\u6388\u6743\u7528\u6237\u53ef\u901a\u8fc7\u89e3\u5bc6\u6a21\u5757\u548c\u6b63\u786e\u5bc6\u94a5\u6062\u590d\u539f\u59cb\u5d4c\u5165\u3002\u4e3a\u4fdd\u62a4\u76ee\u7684\uff0c\u8bbe\u8ba1\u591a\u76ee\u6807\u5bf9\u6297\u6270\u52a8\u65b9\u6cd5\uff0c\u4e3b\u52a8\u5c06\u539f\u59cb\u5d4c\u5165\u5411\u6307\u5b9a\u52a0\u5bc6\u6a21\u5f0f\u504f\u79fb\uff0c\u4f7f\u53d7\u4fdd\u62a4\u56fe\u50cf\u5d4c\u5165\u9632\u5fa1\u5c42\uff0c\u786e\u4fdd\u672a\u7ecf\u6388\u6743\u7528\u6237\u53ea\u80fd\u751f\u6210\u626d\u66f2\u6216\u52a0\u5bc6\u7684\u8f93\u51fa\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u963b\u6b62\u672a\u7ecf\u6388\u6743\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u4e14\u5b89\u5168\u7684\u8bbf\u95ee\u63a7\u5236\u4f9b\u5df2\u9a8c\u8bc1\u7528\u6237\u4f7f\u7528\u3002", "conclusion": "Adapter Shield\u4e3a\u4fdd\u62a4\u4e2a\u4eba\u56fe\u50cf\u514d\u906d\u96f6\u6837\u672c\u751f\u6210\u573a\u666f\u7684\u6ee5\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9632\u5fa1\u4e0e\u6388\u6743\u4f7f\u7528\u7684\u5e73\u8861\u3002"}}
{"id": "2512.00080", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00080", "abs": "https://arxiv.org/abs/2512.00080", "authors": ["Andr\u00e9 Dehne", "Juri Zach", "Peer Stelldinger"], "title": "Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels", "comment": null, "summary": "The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.", "AI": {"tldr": "MARWIN\u673a\u5668\u4eba\u91c7\u7528\u6df1\u5ea6\u89c6\u89c9\u7acb\u4f53\u91cc\u7a0b\u8ba1(DVSO)\u66ff\u4ee3\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u5728\u52a0\u901f\u5668\u96a7\u9053\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u672a\u77e5\u51e0\u4f55\u548c\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u6b27\u6d32XFEL\u7684MARWIN\u673a\u5668\u4eba\u5f53\u524d\u5bfc\u822a\u7cfb\u7edf\u7ed3\u5408\u6fc0\u5149\u96f7\u8fbe\u8fb9\u7f18\u68c0\u6d4b\u3001\u8f6e\u5f0f/\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u548cQR\u7801\u53c2\u8003\uff0c\u5728\u9884\u5b9a\u4e49\u533a\u57df\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u672a\u77e5\u51e0\u4f55\u7ed3\u6784\u548c\u969c\u788d\u7269\u7684\u7075\u6d3b\u6027\uff0c\u9700\u8981\u66f4\u81ea\u4e3b\u7684\u5bfc\u822a\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u91c7\u7528\u6df1\u5ea6\u89c6\u89c9\u7acb\u4f53\u91cc\u7a0b\u8ba1(DVSO)\uff0c\u5229\u7528\u7acb\u4f53\u89c6\u5dee\u3001\u5149\u6d41\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u548c\u81ea\u6211\u8fd0\u52a8\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u53ef\u540e\u7eed\u4e0e\u7edd\u5bf9\u53c2\u8003\u6216\u5176\u4ed6\u4f20\u611f\u5668\u878d\u5408\u4ee5\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u6982\u5ff5\u6027\u8bc4\u4f30\u663e\u793aDVSO\u5728\u52a0\u901f\u5668\u96a7\u9053\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9884\u671f\u4f18\u52bf\u5305\u62ec\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u51cf\u5c11\u5c3a\u5ea6\u6f02\u79fb\u3001\u4f4e\u6210\u672c\u4f20\u611f\u548c\u53ef\u6269\u5c55\u6570\u636e\u6536\u96c6\uff0c\u4f46\u4ecd\u9762\u4e34\u4f4e\u7eb9\u7406\u8868\u9762\u3001\u5149\u7167\u53d8\u5316\u3001\u8ba1\u7b97\u8d1f\u8f7d\u548c\u8f90\u5c04\u73af\u5883\u4e0b\u9c81\u68d2\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3aMARWIN\u673a\u5668\u4eba\u5728\u53d7\u9650\u3001\u5b89\u5168\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u4e2d\u5b9e\u73b0\u66f4\u81ea\u4e3b\u5bfc\u822a\u5b9a\u4e49\u4e86\u7814\u7a76\u8bae\u7a0b\uff0cDVSO\u4f5c\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2512.00074", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00074", "abs": "https://arxiv.org/abs/2512.00074", "authors": ["Qiwei Liang", "Boyang Cai", "Minghao Lai", "Sitong Zhuang", "Tao Lin", "Yan Qin", "Yixuan Ye", "Jiaming Liang", "Renjing Xu"], "title": "Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning", "comment": null, "summary": "Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We attribute this gap to two factors: the lack of state-action-state dynamics modeling and the unnecessary redundancy of explicit geometric reconstruction. We introduce AFRO, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. To prevent feature leakage in action learning, we employ feature differencing and inverse-consistency supervision, improving the quality and stability of visual features. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches. The framework also scales favorably with data volume and task complexity. Qualitative visualizations indicate that AFRO learns semantically rich, discriminative features, offering an effective pre-training solution for 3D representation learning in robotics. Project page: https://kolakivy.github.io/AFRO/", "AI": {"tldr": "AFRO\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u76843D\u89c6\u89c9\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u72b6\u6001\u9884\u6d4b\uff0c\u8054\u5408\u5efa\u6a21\u524d\u5411\u548c\u9006\u5411\u52a8\u529b\u5b66\uff0c\u65e0\u9700\u52a8\u4f5c\u6216\u91cd\u5efa\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u89c6\u89c9\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u72b6\u6001-\u52a8\u4f5c-\u72b6\u6001\u52a8\u6001\u5efa\u6a21\uff0c\u4ee5\u53ca\u663e\u5f0f\u51e0\u4f55\u91cd\u5efa\u5e26\u6765\u7684\u4e0d\u5fc5\u8981\u5197\u4f59\u3002", "method": "AFRO\u5c06\u72b6\u6001\u9884\u6d4b\u6784\u5efa\u4e3a\u751f\u6210\u6269\u6563\u8fc7\u7a0b\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8054\u5408\u5efa\u6a21\u524d\u5411\u548c\u9006\u5411\u52a8\u529b\u5b66\u4ee5\u6355\u6349\u56e0\u679c\u8f6c\u79fb\u7ed3\u6784\u3002\u91c7\u7528\u7279\u5f81\u5dee\u5206\u548c\u9006\u5411\u4e00\u81f4\u6027\u76d1\u7763\u9632\u6b62\u52a8\u4f5c\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u6cc4\u9732\u3002", "result": "\u4e0eDiffusion Policy\u7ed3\u5408\u65f6\uff0cAFRO\u572816\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u64cd\u4f5c\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e14\u80fd\u968f\u6570\u636e\u91cf\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u826f\u597d\u6269\u5c55\u3002", "conclusion": "AFRO\u5b66\u4e60\u5230\u8bed\u4e49\u4e30\u5bcc\u3001\u5224\u522b\u6027\u5f3a\u7684\u7279\u5f81\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u76843D\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00729", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00729", "abs": "https://arxiv.org/abs/2512.00729", "authors": ["Yuxiang Chen", "Zuohan Wu", "Ziwei Wang", "Xiangning Yu", "Xujia Li", "Linyi Yang", "Mengyue Yang", "Jun Wang", "Lei Chen"], "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens", "comment": "13 pages", "summary": "Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u5fc3\u7406\u8fc7\u7a0b\u7684\u5206\u7c7b\u6cd5\u6765\u7406\u89e3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u6807\u6ce8\u6846\u67b6CAPO\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524dLRMs\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u53d1\u73b0\u81ea\u6211\u76d1\u63a7\u8bc4\u4f30\u5927\u591a\u6d41\u4e8e\u8868\u9762\uff0c\u5efa\u8bae\u91c7\u7528\u591a\u6b65\u53cd\u601d\u6765\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5206\u6790\u6846\u67b6\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4ece\u8de8\u5b66\u79d1\u7684\u4eba\u7c7b\u5fc3\u7406\u8fc7\u7a0b\u89d2\u5ea6\uff0c\u6df1\u5165\u7406\u89e3LRMs\u7684\"\u5fc3\u7406\"\u7279\u5f81\uff0c\u4e3a\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "1) \u63d0\u51fa\u5305\u542b5\u7ec417\u7c7b\u7684\u4eba\u7c7b\u5fc3\u7406\u8fc7\u7a0b\u5206\u7c7b\u6cd5\uff1b2) \u5e94\u7528\u8be5\u5206\u7c7b\u6cd5\u5206\u6790\u5f53\u524dLRMs\uff0c\u521b\u5efa\u5305\u542b277,534\u4e2a\u539f\u5b50\u63a8\u7406\u6b65\u9aa4\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff1b3) \u5f00\u53d1\u81ea\u52a8\u6807\u6ce8\u6846\u67b6CAPO\uff0c\u5229\u7528LLMs\u751f\u6210\u5206\u7c7b\u6cd5\u6807\u6ce8\uff1b4) \u5206\u6790\u5f53\u4ee3LRMs\u5e76\u63d0\u70bc\u6539\u8fdb\u5efa\u8bae\u3002", "result": "1) CAPO\u6846\u67b6\u5728\u6807\u6ce8\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u66f4\u4e00\u81f4\uff1b2) \u5206\u6790\u53d1\u73b0\u5f53\u524d\u6d41\u884c\u7684\u540e\u7b54\u6848\"\u53cc\u91cd\u68c0\u67e5\"\uff08\u81ea\u6211\u76d1\u63a7\u8bc4\u4f30\uff09\u5927\u591a\u6d41\u4e8e\u8868\u9762\uff0c\u5f88\u5c11\u4ea7\u751f\u5b9e\u8d28\u6027\u4fee\u8ba2\uff1b3) \u6fc0\u52b1\u5168\u9762\u7684\u591a\u6b65\u53cd\u601d\u6bd4\u7b80\u5355\u81ea\u6211\u76d1\u63a7\u66f4\u6709\u6548\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u3001CAPO\u6846\u67b6\u548c\u6240\u5f97\u89c1\u89e3\u4e3a\u7406\u89e3\u548c\u63a8\u8fdbLRM\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002\u591a\u6b65\u53cd\u601d\u800c\u975e\u7b80\u5355\u81ea\u6211\u76d1\u63a7\u662f\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u7684\u66f4\u6709\u6548\u65b9\u5411\u3002"}}
{"id": "2512.00129", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00129", "abs": "https://arxiv.org/abs/2512.00129", "authors": ["Jayan Adhikari", "Prativa Joshi", "Susish Baral"], "title": "Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation", "comment": null, "summary": "Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\\% general accuracy with immaculate 100\\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ResNet50 OOD\u8fc7\u6ee4\u4e0eYOLO\u67b6\u6784\u7684\u4e73\u817a\u764c\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e25\u683c\u7b5b\u9009\u975e\u4e73\u817a\u56fe\u50cf\u786e\u4fdd\u68c0\u6d4b\u53ef\u9760\u6027\uff0c\u5728OOD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u68c0\u6d4b\u6027\u80fdmAP@0.5\u4e3a0.947\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e73\u817a\u764c\u68c0\u6d4b\u4e2d\u9762\u4e34OOD\u8f93\u5165\uff08\u5982\u5176\u4ed6\u5f71\u50cf\u6a21\u6001\u6216\u8bbe\u5907\u5dee\u5f02\uff09\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u8bef\u8bca\u98ce\u9669\u3002\u9700\u8981\u89e3\u51b3\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u7684\u53ef\u9760\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u7efc\u5408\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8eResNet50\u7684OOD\u8fc7\u6ee4\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5efa\u7acb\u57df\u5185\u56fe\u5e93\uff0c\u4e25\u683c\u62d2\u7edd\u975e\u4e73\u817a\u56fe\u50cf\uff1b2\uff09\u4f7f\u7528YOLO\u67b6\u6784\uff08YOLOv8/v11/v12\uff09\u8fdb\u884c\u4e73\u817a\u764c\u68c0\u6d4b\uff1b3\uff09\u7ed3\u5408Grad-CAM\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002\u7ecf\u8fc712\u79cdCNN\u67b6\u6784\u641c\u7d22\u9009\u62e9ResNet50\u4f5c\u4e3a\u6700\u4f73\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "OOD\u68c0\u6d4b\u7ec4\u4ef6\u8fbe\u523099.77%\u603b\u4f53\u51c6\u786e\u7387\uff0c\u5728OOD\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0100%\u51c6\u786e\u7387\u3002\u8054\u5408\u6846\u67b6\u68c0\u6d4b\u6027\u80fdmAP@0.5\u4e3a0.947\u3002\u5b9e\u9a8c\u9a8c\u8bc1OOD\u8fc7\u6ee4\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u9632\u6b62OOD\u8f93\u5165\u8bef\u62a5\uff0c\u540c\u65f6\u4fdd\u6301\u4e73\u817a\u6570\u636e\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u6570\u636e\u5f02\u8d28\u6027\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u9760\u7684AI\u4e73\u817a\u764c\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u901a\u8fc7OOD\u8fc7\u6ee4\u786e\u4fdd\u7cfb\u7edf\u4ec5\u5904\u7406\u76f8\u5173\u5f71\u50cf\uff0c\u7ed3\u5408\u9ad8\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.00366", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00366", "abs": "https://arxiv.org/abs/2512.00366", "authors": ["Wenshuo Wang", "Yaomin Shen", "Yingjie Tan", "Yihao Chen"], "title": "S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting", "comment": null, "summary": "Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.", "AI": {"tldr": "S^2-KD\uff1a\u7ed3\u5408\u8bed\u4e49\u5148\u9a8c\u4e0e\u9891\u8c31\u8868\u793a\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u7684\u8bed\u4e49\u7406\u89e3\u548c\u9891\u8c31\u5206\u89e3\uff0c\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u89c6\u89c9\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u63d0\u5347\u65f6\u7a7a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u50cf\u7d20\u7ea7\u4fe1\u53f7\u7684\u9891\u8c31\u7279\u6027\uff08\u9ad8\u9891\u7ec6\u8282\u548c\u4f4e\u9891\u8d8b\u52bf\uff09\uff0c\u4f46\u5ffd\u7565\u4e86\u89c6\u89c9\u6a21\u5f0f\u80cc\u540e\u7684\u4e30\u5bcc\u8bed\u4e49\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faS^2-KD\u6846\u67b6\uff1a1\uff09\u8bad\u7ec3\u7279\u6743\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u6587\u672c\u53d9\u8ff0\u8fdb\u884c\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u63a8\u7406\uff0c\u540c\u65f6\u5728\u6f5c\u5728\u7a7a\u95f4\u89e3\u8026\u9891\u8c31\u5206\u91cf\uff1b2\uff09\u8bbe\u8ba1\u65b0\u7684\u84b8\u998f\u76ee\u6807\uff0c\u5c06\u7edf\u4e00\u7684\u8bed\u4e49-\u9891\u8c31\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u7eaf\u89c6\u89c9\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728WeatherBench\u548cTaxiBJ+\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cS^2-KD\u663e\u8457\u63d0\u5347\u4e86\u7b80\u5355\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u957f\u65f6\u57df\u548c\u590d\u6742\u975e\u5e73\u7a33\u573a\u666f\u4e0b\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "S^2-KD\u901a\u8fc7\u7edf\u4e00\u8bed\u4e49\u5148\u9a8c\u548c\u9891\u8c31\u8868\u793a\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u4f7f\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u65e2\u9891\u8c31\u51c6\u786e\u53c8\u8bed\u4e49\u8fde\u8d2f\u7684\u9884\u6d4b\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u7684\u6587\u672c\u8f93\u5165\u6216\u67b6\u6784\u5f00\u9500\uff0c\u4e3a\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00261", "abs": "https://arxiv.org/abs/2512.00261", "authors": ["Yuzhen Hu", "Saurabh Prasad"], "title": "UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations", "comment": "Camera-ready for WACV 2026", "summary": "Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.", "AI": {"tldr": "UniDiff\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u5c06ImageNet\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u9002\u914d\u5230\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u6807\u6ce8\u4e0b\u7684\u6570\u636e\u878d\u5408\u95ee\u9898", "motivation": "\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\uff08\u5982\u9ad8\u5149\u8c31\u6210\u50cf\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff09\u9762\u4e34\u7a00\u758f\u6807\u6ce8\u7684\u6311\u6218\uff0c\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u5982MSFMamba\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u53ef\u7528\u6027\uff0c\u800cImageNet\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u9002\u914d\u5230\u5f02\u6784\u6a21\u6001", "method": "\u63d0\u51faUniDiff\u6846\u67b6\uff1a1) \u4f7f\u7528FiLM-based timestep-modality conditioning\u8fdb\u884c\u6a21\u6001\u6761\u4ef6\u5316\uff1b2) \u4ec5\u9002\u914d\u7ea65%\u53c2\u6570\u7684\u53c2\u6570\u9ad8\u6548\u9002\u5e94\uff1b3) \u91c7\u7528pseudo-RGB anchoring\u4fdd\u6301\u9884\u8bad\u7ec3\u8868\u793a\u5e76\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "result": "\u5728\u4e24\u4e2a\u5df2\u5efa\u7acb\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65e0\u76d1\u7763\u9002\u5e94\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6709\u6548\u7f13\u89e3\u4e86\u6807\u6ce8\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7684\u6709\u6548\u878d\u5408", "conclusion": "UniDiff\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u5c06\u5355\u4e2aImageNet\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u9002\u914d\u5230\u591a\u79cd\u9065\u611f\u6a21\u6001\uff0c\u4ec5\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\uff0c\u5728\u7a00\u758f\u6807\u6ce8\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408"}}
{"id": "2512.00327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00327", "abs": "https://arxiv.org/abs/2512.00327", "authors": ["Chenqi Zhu", "Levi Burner", "Yiannis Aloimonos"], "title": "Odometry Without Correspondence from Inertially Constrained Ruled Surfaces", "comment": "14 pages, 13 figures, 5 tables", "summary": "Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u76f4\u7ebf\u7279\u5f81\u548c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u76f4\u7eb9\u66f2\u9762\u6765\u4f30\u8ba1\u8fd0\u52a8\uff0c\u907f\u514d\u4f20\u7edf\u70b9\u5bf9\u70b9\u5339\u914d\u7684\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4f9d\u8d56\u7279\u5f81\u70b9\u63d0\u53d6\u548c\u5149\u6d41\u8ba1\u7b97\uff0c\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5c1d\u8bd5\u4f7f\u7528\u76f4\u7ebf\u7279\u5f81\u6216\u878d\u5408\u5176\u4ed6\u4f20\u611f\u5668\uff0c\u4f46\u4ecd\u4e25\u91cd\u4f9d\u8d56\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u7ed5\u8fc7\u5bf9\u5e94\u95ee\u9898\uff0c\u5229\u7528\u76f4\u7ebf\u5728\u56fe\u50cf\u7a7a\u95f4\u5f62\u6210\u7684\u76f4\u7eb9\u66f2\u9762\u6765\u83b7\u53d6\u8fd0\u52a8\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u76f8\u673a\u8fd0\u52a8\u65f6\u76f4\u7ebf\u5728\u56fe\u50cf\u7a7a\u95f4\u5f62\u6210\u7684\u76f4\u7eb9\u66f2\u9762\u6765\u91cd\u5efa3D\u573a\u666f\u548c\u4f30\u8ba1\u89c6\u89c9\u91cc\u7a0b\u8ba1\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u70b9\u4e0e\u7ebf\u7684\u5173\u8054\u8fdb\u884c\u5fae\u5206\u8ba1\u7b97\u66f4\u65b0\uff0c\u5e76\u5229\u7528IMU\u7684\u60ef\u6027\u6d4b\u91cf\u7ea6\u675f\u89e3\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u7eb9\u66f2\u9762\u5206\u6790\u548cIMU\u7ea6\u675f\uff0c\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u70b9\u5bf9\u70b9\u5339\u914d\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u4e0d\u7a33\u5b9a\u6027\u3002\u7279\u522b\u9002\u5408\u4e8b\u4ef6\u76f8\u673a\u7b49\u64c5\u957f\u8fb9\u7f18\u68c0\u6d4b\u7684\u4f20\u611f\u5668\u3002", "conclusion": "\u57fa\u4e8e\u76f4\u7eb9\u66f2\u9762\u5206\u6790\u548cIMU\u7ea6\u675f\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed5\u8fc7\u4f20\u7edf\u7279\u5f81\u5bf9\u5e94\u95ee\u9898\u7684\u65b0\u9014\u5f84\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7b49\u8fb9\u7f18\u68c0\u6d4b\u4f18\u52bf\u660e\u663e\u7684\u4f20\u611f\u5668\u7cfb\u7edf\u3002"}}
{"id": "2512.01331", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01331", "abs": "https://arxiv.org/abs/2512.01331", "authors": ["Saman Ahmadi", "Mahdi Jalili"], "title": "A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles", "comment": "13 pages, 5 Figures, 1 table, To appear as part of AAAI 2026 Proceedings", "summary": "We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u76ee\u6807A*\u641c\u7d22\u7684\u80fd\u91cf\u6700\u4f18\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u5728\u4e0d\u786e\u5b9a\u521d\u59cb\u80fd\u91cf\u60c5\u51b5\u4e0b\u7684\u8def\u5f84\u89c4\u5212\uff0c\u907f\u514d\u5904\u7406\u590d\u6742\u80fd\u91cf\u5256\u9762", "motivation": "\u4f20\u7edf\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u5047\u8bbe\u5df2\u77e5\u521d\u59cb\u80fd\u91cf\u6c34\u5e73\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u521d\u59cb\u80fd\u91cf\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\uff0c\u9700\u8981\u4e3a\u6240\u6709\u53ef\u80fd\u7684\u521d\u59cb\u80fd\u91cf\u6c34\u5e73\u89c4\u5212\u6700\u4f18\u8def\u5f84\uff08\u80fd\u91cf\u6700\u4f18\u5256\u9762\u641c\u7d22\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u5256\u9762\u5408\u5e76\u8fc7\u7a0b\uff0c\u5904\u7406\u590d\u6742\u5256\u9762\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u76ee\u6807A*\u641c\u7d22\u7684\u6807\u7b7e\u8bbe\u7f6e\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5256\u9762\u652f\u914d\u89c4\u5219\uff0c\u907f\u514d\u751f\u6210\u548c\u5904\u7406\u590d\u6742\u5256\u9762\u3002\u5f00\u53d1\u4e86\u56db\u79cd\u53d8\u4f53\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u9053\u8def\u7f51\u7edc\u4e0a\u4f7f\u7528\u5b9e\u9645\u80fd\u8017\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u80fd\u91cf\u5256\u9762A*\u641c\u7d22\u7684\u6027\u80fd\u4e0e\u5df2\u77e5\u521d\u59cb\u80fd\u91cf\u6c34\u5e73\u7684\u80fd\u91cf\u6700\u4f18A*\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u80fd\u591f\u5904\u7406\u7535\u52a8\u6c7d\u8f66\u5728\u4e0d\u786e\u5b9a\u521d\u59cb\u80fd\u91cf\u60c5\u51b5\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u907f\u514d\u4e86\u590d\u6742\u5256\u9762\u7684\u5904\u7406\u3002"}}
{"id": "2512.00598", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00598", "abs": "https://arxiv.org/abs/2512.00598", "authors": ["Yining Yuan", "J. Ben Tamo", "Wenqi Shi", "Yishan Zhong", "Micky C. Nnamdi", "B. Randall Brenn", "Steven W. Hwang", "May D. Wang"], "title": "Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction", "comment": null, "summary": "Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.\n  Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.\n  Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.", "AI": {"tldr": "FAIR-MTL\uff1a\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u4e9a\u7ec4\u63a8\u65ad\u673a\u5236\uff0c\u5728\u810a\u67f1\u878d\u5408\u624b\u672f\u5e76\u53d1\u75c7\u9884\u6d4b\u4e2d\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u7ec6\u7c92\u5ea6\u7684\u9884\u6d4b", "motivation": "\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u6301\u7eed\u5b58\u5728\uff0c\u7279\u522b\u662f\u5728\u810a\u67f1\u4fa7\u51f8\u810a\u67f1\u878d\u5408\u624b\u672f\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002\u73b0\u6709\u516c\u5e73\u6027\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7565\u7684\u4eba\u53e3\u7edf\u8ba1\u8c03\u6574\u6216\u4e8b\u540e\u6821\u6b63\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u4eba\u7fa4\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u504f\u89c1\u3002", "method": "\u63d0\u51faFAIR-MTL\u6846\u67b6\uff0c\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u4e9a\u7ec4\u63a8\u65ad\u673a\u5236\uff1a\u63d0\u53d6\u7d27\u51d1\u7684\u4eba\u53e3\u7edf\u8ba1\u5d4c\u5165\uff0c\u5e94\u7528k-means\u805a\u7c7b\u53d1\u73b0\u6f5c\u5728\u60a3\u8005\u4e9a\u7ec4\uff1b\u5728\u5171\u4eab\u591a\u4efb\u52a1\u67b6\u6784\u4e2d\u6839\u636e\u63a8\u65ad\u7684\u4e9a\u7ec4\u6807\u7b7e\u8fdb\u884c\u4efb\u52a1\u8def\u7531\uff1b\u901a\u8fc7\u9006\u9891\u7387\u52a0\u6743\u7f13\u89e3\u4e9a\u7ec4\u4e0d\u5e73\u8861\uff0c\u6b63\u5219\u5316\u9632\u6b62\u5bf9\u5c0f\u7fa4\u4f53\u8fc7\u62df\u5408\u3002", "result": "\u5728\u672f\u540e\u5e76\u53d1\u75c7\u9884\u6d4b\uff08\u56db\u4e2a\u4e25\u91cd\u7a0b\u5ea6\u7b49\u7ea7\uff09\u4e2d\uff0cFAIR-MTL\u8fbe\u5230AUC 0.86\u548c\u51c6\u786e\u738775%\uff0c\u4f18\u4e8e\u5355\u4efb\u52a1\u57fa\u7ebf\u5e76\u663e\u8457\u51cf\u5c11\u504f\u89c1\u3002\u6027\u522b\u65b9\u9762\uff1a\u4eba\u53e3\u7edf\u8ba1\u5947\u5076\u5dee\u5f02\u964d\u81f30.055\uff0c\u5747\u8861\u51e0\u7387\u964d\u81f30.094\uff1b\u5e74\u9f84\u65b9\u9762\uff1a\u5206\u522b\u964d\u81f30.056\u548c0.148\u3002SHAP\u548cGini\u91cd\u8981\u6027\u5206\u6790\u786e\u4fdd\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c06\u65e0\u76d1\u7763\u4e9a\u7ec4\u53d1\u73b0\u7eb3\u5165\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u624b\u672f\u98ce\u9669\u5206\u5c42\u63d0\u4f9b\u66f4\u516c\u5e73\u3001\u53ef\u89e3\u91ca\u4e14\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u3002"}}
{"id": "2512.01440", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01440", "abs": "https://arxiv.org/abs/2512.01440", "authors": ["Sylvain Mari\u00e9", "Pablo Knecht"], "title": "A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale", "comment": null, "summary": "Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u72b6\u6001\u8f6c\u79fb\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff08STE-ts\uff09\u548c\u9009\u62e9\u6027\u65f6\u5e8f\u6c49\u660e\u8ddd\u79bb\uff08STH\uff09\uff0c\u907f\u514d\u5bf9\u5927\u578b\u6570\u636e\u5e93\u8fdb\u884c\u4ee3\u4ef7\u9ad8\u6602\u4e14\u5931\u771f\u7684\u91cd\u91c7\u6837\uff0c\u540c\u65f6\u5229\u7528\u8f6c\u79fb\u65f6\u95f4\u548c\u72b6\u6001\u6301\u7eed\u65f6\u95f4\u4fe1\u606f\u3002", "motivation": "\u79bb\u6563\u4e8b\u4ef6\u7cfb\u7edf\u666e\u904d\u5b58\u5728\u4e8e\u81ea\u7136\u89c2\u5bdf\u3001\u793e\u4f1a\u7ecf\u6d4e\u79d1\u5b66\u548c\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u3002\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u901a\u5e38\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u4e8b\u4ef6/\u72b6\u6001\u53cc\u91cd\u7279\u6027\uff1a\u4fe1\u53f7\u8981\u4e48\u88ab\u5efa\u6a21\u4e3a\u8f6c\u79fb\u4e8b\u4ef6\u5e8f\u5217\uff08\u5f3a\u8c03\u4e8b\u4ef6\u987a\u5e8f\u5bf9\u9f50\uff09\uff0c\u8981\u4e48\u88ab\u5efa\u6a21\u4e3a\u5206\u7c7b\u6216\u6709\u5e8f\u72b6\u6001\u65f6\u95f4\u5e8f\u5217\uff08\u901a\u5e38\u9700\u8981\u91cd\u91c7\u6837\uff0c\u968f\u7740\u89c2\u6d4b\u5468\u671f\u548c\u4e8b\u4ef6\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8fd9\u79cd\u64cd\u4f5c\u65e2\u5931\u771f\u53c8\u4ee3\u4ef7\u9ad8\u6602\uff09\u3002", "method": "\u5b9a\u4e49\u72b6\u6001\u8f6c\u79fb\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff08STE-ts\uff09\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u9009\u62e9\u6027\u65f6\u5e8f\u6c49\u660e\u8ddd\u79bb\uff08STH\uff09\uff0c\u8be5\u8ddd\u79bb\u540c\u65f6\u5229\u7528\u8f6c\u79fb\u65f6\u95f4\u548c\u72b6\u6001\u6301\u7eed\u65f6\u95f4\u4fe1\u606f\uff0c\u907f\u514d\u5bf9\u5927\u578b\u6570\u636e\u5e93\u8fdb\u884c\u4ee3\u4ef7\u9ad8\u6602\u4e14\u5931\u771f\u7684\u91cd\u91c7\u6837\u3002", "result": "STH\u80fd\u591f\u6cdb\u5316\u91cd\u91c7\u6837\u6c49\u660e\u8ddd\u79bb\u548cJaccard\u5ea6\u91cf\uff0c\u5177\u6709\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u4e14\u80fd\u591f\u805a\u7126\u4e8e\u591a\u4e2a\u611f\u5174\u8da3\u7684\u72b6\u6001\u3002\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684STE-ts\u548cSTH\u65b9\u6cd5\u4e3a\u79bb\u6563\u4e8b\u4ef6\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u91cd\u91c7\u6837\u65b9\u6cd5\u7684\u5931\u771f\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u4e86\u7cfb\u7edf\u7684\u53cc\u91cd\u7279\u6027\u3002"}}
{"id": "2512.01280", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.01280", "abs": "https://arxiv.org/abs/2512.01280", "authors": ["Longji Yin", "Yunfan Ren", "Fangcheng Zhu", "Liuyu Shi", "Fanze Kong", "Benxu Tang", "Wenyi Liu", "Ximin Lyu", "Fu Zhang"], "title": "Visibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms", "comment": null, "summary": "Autonomous aerial tracking with drones offers vast potential for surveillance, cinematography, and industrial inspection applications. While single-drone tracking systems have been extensively studied, swarm-based target tracking remains underexplored, despite its unique advantages of distributed perception, fault-tolerant redundancy, and multidirectional target coverage. To bridge this gap, we propose a novel decentralized LiDAR-based swarm tracking framework that enables visibility-aware, cooperative target tracking in complex environments, while fully harnessing the unique capabilities of swarm systems. To address visibility, we introduce a novel Spherical Signed Distance Field (SSDF)-based metric for 3-D environmental occlusion representation, coupled with an efficient algorithm that enables real-time onboard SSDF updating. A general Field-of-View (FOV) alignment cost supporting heterogeneous LiDAR configurations is proposed for consistent target observation. Swarm coordination is enhanced through cooperative costs that enforce inter-robot safe clearance, prevent mutual occlusions, and notably facilitate 3-D multidirectional target encirclement via a novel electrostatic-potential-inspired distribution metric. These innovations are integrated into a hierarchical planner, combining a kinodynamic front-end searcher with a spatiotemporal $SE(3)$ back-end optimizer to generate collision-free, visibility-optimized trajectories.Deployed on heterogeneous LiDAR swarms, our fully decentralized implementation features collaborative perception, distributed planning, and dynamic swarm reconfigurability. Validated through rigorous real-world experiments in cluttered outdoor environments, the proposed system demonstrates robust cooperative tracking of agile targets (drones, humans) while achieving superior visibility maintenance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684LiDAR\u65e0\u4eba\u673a\u7fa4\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7403\u5f62\u7b26\u53f7\u8ddd\u79bb\u573a(SSDF)\u8868\u793a\u73af\u5883\u906e\u6321\uff0c\u7ed3\u5408\u534f\u540c\u89c4\u5212\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u89c1\u6027\u611f\u77e5\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u3002", "motivation": "\u5355\u65e0\u4eba\u673a\u8ddf\u8e2a\u7cfb\u7edf\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u57fa\u4e8e\u7fa4\u4f53\u7684\u76ee\u6807\u8ddf\u8e2a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7fa4\u4f53\u7cfb\u7edf\u5177\u6709\u5206\u5e03\u5f0f\u611f\u77e5\u3001\u5bb9\u9519\u5197\u4f59\u548c\u591a\u65b9\u5411\u76ee\u6807\u8986\u76d6\u7b49\u72ec\u7279\u4f18\u52bf\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u89c1\u6027\u611f\u77e5\u7684\u534f\u540c\u8ddf\u8e2a\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "method": "1) \u63d0\u51faSSDF-based\u76843D\u73af\u5883\u906e\u6321\u8868\u793a\u65b9\u6cd5\u53ca\u5b9e\u65f6\u66f4\u65b0\u7b97\u6cd5\uff1b2) \u652f\u6301\u5f02\u6784LiDAR\u914d\u7f6e\u7684FOV\u5bf9\u9f50\u6210\u672c\u51fd\u6570\uff1b3) \u57fa\u4e8e\u9759\u7535\u52bf\u542f\u53d1\u7684\u5206\u5e03\u5ea6\u91cf\u5b9e\u73b03D\u591a\u65b9\u5411\u76ee\u6807\u5305\u56f4\uff1b4) \u5206\u5c42\u89c4\u5212\u5668\u7ed3\u5408\u8fd0\u52a8\u5b66\u524d\u7aef\u641c\u7d22\u5668\u548c\u65f6\u7a7aSE(3)\u540e\u7aef\u4f18\u5316\u5668\u751f\u6210\u65e0\u78b0\u649e\u3001\u53ef\u89c1\u6027\u4f18\u5316\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u6742\u4e71\u5ba4\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u80fd\u591f\u9c81\u68d2\u5730\u534f\u540c\u8ddf\u8e2a\u654f\u6377\u76ee\u6807\uff08\u65e0\u4eba\u673a\u3001\u4eba\u7c7b\uff09\uff0c\u5e76\u5b9e\u73b0\u4f18\u8d8a\u7684\u53ef\u89c1\u6027\u7ef4\u62a4\u3002\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5b9e\u73b0\u5177\u6709\u534f\u4f5c\u611f\u77e5\u3001\u5206\u5e03\u5f0f\u89c4\u5212\u548c\u52a8\u6001\u7fa4\u4f53\u53ef\u91cd\u6784\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7fa4\u4f53\u76ee\u6807\u8ddf\u8e2a\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316LiDAR\u7fa4\u4f53\u8ddf\u8e2a\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u89c1\u6027\u611f\u77e5\u7684\u534f\u540c\u76ee\u6807\u8ddf\u8e2a\uff0c\u4e3a\u65e0\u4eba\u673a\u7fa4\u4f53\u5728\u76d1\u89c6\u3001\u7535\u5f71\u6444\u5f71\u548c\u5de5\u4e1a\u68c0\u6d4b\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00425", "abs": "https://arxiv.org/abs/2512.00425", "authors": ["Minh-Quan Le", "Yuanzhi Zhu", "Vicky Kalogeiton", "Dimitris Samaras"], "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards", "comment": "Project page: https://cvlab-stonybrook.github.io/NewtonRewards", "summary": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\\texttt{NewtonRewards}$ extracts $\\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.", "AI": {"tldr": "NewtonRewards\uff1a\u9996\u4e2a\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u7269\u7406\u57fa\u7840\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u5149\u5b66\u6d41\u548c\u5916\u89c2\u7279\u5f81\u4f5c\u4e3a\u901f\u5ea6\u548c\u8d28\u91cf\u4ee3\u7406\uff0c\u5f3a\u5236\u6267\u884c\u725b\u987f\u8fd0\u52a8\u5b66\u548c\u8d28\u91cf\u5b88\u6052\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u6269\u6563\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u7247\u6bb5\uff0c\u4f46\u7ecf\u5e38\u8fdd\u53cd\u57fa\u672c\u7269\u7406\u5b9a\u5f8b\uff08\u7269\u4f53\u6f02\u6d6e\u3001\u52a0\u901f\u5ea6\u6f02\u79fb\u3001\u78b0\u649e\u4e0d\u4e00\u81f4\uff09\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u771f\u5b9e\u611f\u4e0e\u7269\u7406\u771f\u5b9e\u611f\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\u3002", "method": "\u63d0\u51faNewtonRewards\u6846\u67b6\uff1a1) \u4f7f\u7528\u51bb\u7ed3\u7684\u5b9e\u7528\u6a21\u578b\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u63d0\u53d6\u53ef\u6d4b\u91cf\u4ee3\u7406\uff08\u5149\u5b66\u6d41\u4f5c\u4e3a\u901f\u5ea6\u4ee3\u7406\uff0c\u9ad8\u7ea7\u5916\u89c2\u7279\u5f81\u4f5c\u4e3a\u8d28\u91cf\u4ee3\u7406\uff09\uff1b2) \u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u5956\u52b1\u5f3a\u5236\u6267\u884c\u725b\u987f\u7ed3\u6784\uff1a\u725b\u987f\u8fd0\u52a8\u5b66\u7ea6\u675f\uff08\u5f3a\u5236\u6052\u5b9a\u52a0\u901f\u5ea6\u52a8\u529b\u5b66\uff09\u548c\u8d28\u91cf\u5b88\u6052\u5956\u52b1\uff08\u9632\u6b62\u9000\u5316\u89e3\uff09\u3002", "result": "\u5728NewtonBench-60K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNewtonRewards\u5728\u6240\u6709\u725b\u987f\u8fd0\u52a8\u57fa\u672c\u7c7b\u578b\uff08\u81ea\u7531\u843d\u4f53\u3001\u6c34\u5e73/\u629b\u7269\u7ebf\u6295\u63b7\u3001\u659c\u5761\u6ed1\u4e0b/\u4e0a\uff09\u7684\u89c6\u89c9\u548c\u7269\u7406\u6307\u6807\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u540e\u8bad\u7ec3\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u7269\u7406\u5408\u7406\u6027\u3001\u8fd0\u52a8\u5e73\u6ed1\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u5728\u9ad8\u5ea6\u3001\u901f\u5ea6\u548c\u6469\u64e6\u529b\u7684\u5206\u5e03\u5916\u504f\u79fb\u4e0b\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "\u7269\u7406\u57fa\u7840\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u4e3a\u7269\u7406\u611f\u77e5\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u80fd\u591f\u6709\u6548\u5f25\u5408\u89c6\u89c9\u771f\u5b9e\u611f\u4e0e\u7269\u7406\u771f\u5b9e\u611f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2512.00708", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2512.00708", "abs": "https://arxiv.org/abs/2512.00708", "authors": ["Ming-Hsiu Wu", "Ziqian Xie", "Shuiwang Ji", "Degui Zhi"], "title": "Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset", "comment": null, "summary": "Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete", "AI": {"tldr": "\u7814\u7a76\u8005\u521b\u5efa\u4e86DAVIS-complete\u6570\u636e\u96c6\uff0c\u5305\u542b4,032\u4e2a\u6d89\u53ca\u86cb\u767d\u8d28\u4fee\u9970\u7684\u6fc0\u9176-\u914d\u4f53\u5bf9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u86cb\u767d\u8d28\u4fee\u9970\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5728\u836f\u7269\u53d1\u73b0\u4efb\u52a1\uff08\u5982\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\uff09\u4e2d\u8fc7\u5ea6\u62df\u5408\u4e8e\u7b80\u5316\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0d\u80fd\u4ee3\u8868\u81ea\u7136\u53d1\u751f\u4e14\u5177\u6709\u751f\u7269\u5b66\u76f8\u5173\u6027\u7684\u86cb\u767d\u8d28\u4fee\u9970\u3002\u9700\u8981\u521b\u5efa\u66f4\u771f\u5b9e\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u751f\u7269\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "1. \u521b\u5efaDAVIS-complete\u6570\u636e\u96c6\uff1a\u6574\u5408\u4e864,032\u4e2a\u6d89\u53ca\u66ff\u4ee3\u3001\u63d2\u5165\u3001\u7f3a\u5931\u548c\u78f7\u9178\u5316\u4e8b\u4ef6\u7684\u6fc0\u9176-\u914d\u4f53\u5bf9\u30022. \u8bbe\u8ba1\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff1a\u589e\u5f3a\u6570\u636e\u96c6\u9884\u6d4b\u3001\u91ce\u751f\u578b\u5230\u4fee\u9970\u7684\u6cdb\u5316\u3001\u5c11\u6837\u672c\u4fee\u9970\u6cdb\u5316\u30023. \u5e7f\u6cdb\u8bc4\u4f30\u5bf9\u63a5\u81ea\u7531\u548c\u5bf9\u63a5\u4f9d\u8d56\u65b9\u6cd5\u3002", "result": "\u5bf9\u63a5\u4f9d\u8d56\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6cdb\u5316\u66f4\u597d\uff1b\u5bf9\u63a5\u81ea\u7531\u6a21\u578b\u503e\u5411\u4e8e\u8fc7\u5ea6\u62df\u5408\u91ce\u751f\u578b\u86cb\u767d\u8d28\uff0c\u96be\u4ee5\u5904\u7406\u672a\u89c1\u8fc7\u7684\u4fee\u9970\uff0c\u4f46\u5728\u5c11\u91cf\u4fee\u9970\u6837\u672c\u4e0a\u5fae\u8c03\u540e\u8868\u73b0\u663e\u8457\u6539\u5584\u3002", "conclusion": "DAVIS-complete\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u5f00\u53d1\u80fd\u66f4\u597d\u6cdb\u5316\u5230\u86cb\u767d\u8d28\u4fee\u9970\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u7cbe\u51c6\u533b\u7597\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u8fdb\u5c55\u3002"}}
{"id": "2512.01723", "categories": ["cs.AI", "cs.GT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.01723", "abs": "https://arxiv.org/abs/2512.01723", "authors": ["Saba Kublashvili"], "title": "Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation", "comment": "Preprint. Code and simulation notebooks available at the GitHub repository: https://github.com/Saba-Kublashvili/bayesian-computational-modeling.-", "summary": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.", "AI": {"tldr": "HistoricalML\uff1a\u4e00\u4e2a\u6982\u7387\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3001\u5408\u4f5c\u535a\u5f08\u8bba\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u5386\u53f2\u4e8b\u4ef6\u5efa\u6a21\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u5f02\u8d28\u6d4b\u91cf\u3001\u53cd\u4e8b\u5b9e\u7f3a\u5931\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002", "motivation": "\u5386\u53f2\u4e8b\u4ef6\u5efa\u6a21\u9762\u4e34\u6781\u7aef\u6570\u636e\u7a00\u7f3a\uff08N<100\uff09\u3001\u5f02\u8d28\u566a\u58f0\u6d4b\u91cf\u3001\u53cd\u4e8b\u5b9e\u7f3a\u5931\u4ee5\u53ca\u9700\u8981\u4eba\u7c7b\u53ef\u89e3\u91ca\u89e3\u91ca\u7b49\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7684\u5386\u53f2\u5206\u6790\u95ee\u9898\u3002", "method": "\u63d0\u51faHistoricalML\u6846\u67b6\uff0c\u6574\u5408\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5206\u79bb\u8ba4\u77e5\u4e0e\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff1b2\uff09\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5904\u7406\u6df7\u6742\u4e0b\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\uff1b3\uff09\u5408\u4f5c\u535a\u5f08\u8bba\uff08Shapley\u503c\uff09\u8fdb\u884c\u516c\u5e73\u5206\u914d\u5efa\u6a21\uff1b4\uff09\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u56e0\u5b50\u52a0\u6743\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u5728\u7a00\u758f\u6570\u636e\u4e0b\uff0c\u5f53\u6709\u5f3a\u9886\u57df\u5148\u9a8c\u65f6\uff0c\u65b9\u6cd5\u80fd\u5b9e\u73b0\u4e00\u81f4\u4f30\u8ba1\uff1bShapley\u5206\u914d\u6ee1\u8db3\u516c\u7406\u516c\u5e73\u6027\u4fdd\u8bc1\u3002\u5728\u4e24\u4e2a\u5386\u53f2\u6848\u4f8b\u4e2d\uff1a19\u4e16\u7eaa\u975e\u6d32\u6b96\u6c11\uff087\u4e2a\u6b96\u6c11\u56fd\u5bb6\uff09\u8bc6\u522b\u51fa\u5fb7\u56fd+107.9%\u5dee\u5f02\u4f5c\u4e3a\u4e00\u6218\u524d\u7ed3\u6784\u6027\u7d27\u5f20\uff1b\u7b2c\u4e8c\u6b21\u5e03\u533f\u6218\u4e89\uff082\u4e2a\u6d3e\u7cfb\uff09\u6a21\u62df\u663e\u793a\u8fe6\u592a\u57fa\u5728\u574e\u5c3c\u670957.3%\u80dc\u7387\uff0c\u7f57\u9a6c\u5728\u624e\u9a6c\u670957.8%\u80dc\u7387\u3002", "conclusion": "HistoricalML\u6210\u529f\u89e3\u51b3\u4e86\u5386\u53f2\u5efa\u6a21\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u6982\u7387\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u5386\u53f2\u5206\u6790\u3002\u53cd\u4e8b\u5b9e\u5206\u6790\u63ed\u793a\u8fe6\u592a\u57fa\u7684\u653f\u6cbb\u652f\u6301\uff08\u800c\u975e\u519b\u4e8b\u5b9e\u529b\uff09\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5728\u8bc6\u522b\u5386\u53f2\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.00475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00475", "abs": "https://arxiv.org/abs/2512.00475", "authors": ["Xin Gu", "Congcong Li", "Xinyao Wang", "Dexiang Hong", "Libo Zhang", "Tiejian Luo", "Longyin Wen", "Heng Fan"], "title": "Structured Context Learning for Generic Event Boundary Detection", "comment": null, "summary": "Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60(SCL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5e8f\u5217\u5206\u5272(SPoS)\u4e3aGEBD\u4efb\u52a1\u63d0\u4f9b\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u901f\u5ea6-\u51c6\u786e\u7387\u5e73\u8861", "motivation": "\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b(GEBD)\u65e8\u5728\u8bc6\u522b\u4eba\u7c7b\u611f\u77e5\u7684\u4e8b\u4ef6\u8fb9\u754c\u65f6\u523b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6-\u51c6\u786e\u7387\u5e73\u8861\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u89c6\u9891\u5e8f\u5217\u7684\u65f6\u5e8f\u4fe1\u606f\u3002", "method": "1. \u63d0\u51fa\u7ed3\u6784\u5316\u5e8f\u5217\u5206\u5272(SPoS)\u5c06\u8f93\u5165\u5e27\u5e8f\u5217\u5206\u5272\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff1b2. \u8ba1\u7b97\u7ec4\u95f4\u76f8\u4f3c\u5ea6\u6355\u6349\u5e27\u95f4\u5dee\u5f02\uff1b3. \u4f7f\u7528\u8f7b\u91cf\u7ea7\u5168\u5377\u79ef\u7f51\u7edc\u57fa\u4e8e\u5206\u7ec4\u76f8\u4f3c\u5ea6\u56fe\u786e\u5b9a\u4e8b\u4ef6\u8fb9\u754c\uff1b4. \u91c7\u7528\u9ad8\u65af\u6838\u9884\u5904\u7406\u5730\u9762\u771f\u503c\u8fb9\u754c\u4ee5\u89e3\u51b3\u6807\u6ce8\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5728Kinetics-GEBD\u3001TAPOS\u548c\u955c\u5934\u8f6c\u6362\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002SPoS\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u89c6\u9891\u957f\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u5e8f\u5217\u5206\u5272\u4e3aGEBD\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u7075\u6d3b\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.01661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01661", "abs": "https://arxiv.org/abs/2512.01661", "authors": ["Dengyun Peng", "Qiguang Chen", "Bofei Liu", "Jiannan Guan", "Libo Qin", "Zheng Yan", "Jinhao Liu", "Jianshu Zhang", "Wanxiang Che"], "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems", "comment": "preprint", "summary": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel \"Reverse Construction\" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUnsolvableQA\u6570\u636e\u96c6\u548cUnsolvableRL\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u533a\u5206\u53ef\u89e3\u95ee\u9898\u4e0e\u4e0d\u53ef\u89e3\u95ee\u9898\uff0c\u9632\u6b62\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u548c\u5e7b\u89c9\u3002", "motivation": "\u5f53\u524dLLM\u96be\u4ee5\u533a\u5206\u5ba2\u89c2\u4e0d\u53ef\u89e3\u6027\uff08\u95ee\u9898\u5185\u5728\u77db\u76fe\uff09\u548c\u4e3b\u89c2\u80fd\u529b\u9650\u5236\uff08\u8d85\u51fa\u6a21\u578b\u80fd\u529b\u7684\u95ee\u9898\uff09\uff0c\u5bfc\u81f4\u5e7b\u89c9\u548c\u8fc7\u5ea6\u81ea\u4fe1\u3002\u9700\u8981\u8ba9\u6a21\u578b\u65e2\u80fd\u89e3\u51b3\u53ef\u89e3\u95ee\u9898\uff0c\u53c8\u80fd\u8bc6\u522b\u4e0d\u53ef\u89e3\u95ee\u9898\u5e76\u8c28\u614e\u62d2\u7edd\u8d85\u51fa\u80fd\u529b\u8303\u56f4\u7684\u4efb\u52a1\u3002", "method": "1) \u6784\u5efaUnsolvableQA\u6570\u636e\u96c6\uff1a\u901a\u8fc7\u53cc\u8f68\u65b9\u6cd5\u521b\u5efa\u53ef\u89e3\u548c\u4e0d\u53ef\u89e3\u5b9e\u4f8b\u5bf9\uff0c\u5305\u62ec\u7a0b\u5e8f\u5316\u751f\u6210\u903b\u8f91\u8c1c\u9898\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\"\u53cd\u5411\u6784\u9020\"\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u94fe\u4e2d\u6ce8\u5165\u77db\u76fe\u30022) \u63d0\u51faUnsolvableRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u5305\u542b\u4e09\u4e2a\u5956\u52b1\u7ec4\u4ef6\uff0c\u5171\u540c\u8003\u8651\u51c6\u786e\u6027\u3001\u4e0d\u53ef\u89e3\u6027\u548c\u96be\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u4e0d\u53ef\u89e3\u6027\u68c0\u6d4b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002\u5173\u952e\u53d1\u73b0\u662f\"\u80fd\u529b\u5d29\u6e83\"\u73b0\u8c61\uff0c\u8868\u660e\u660e\u786e\u66b4\u9732\u4e8e\u4e0d\u53ef\u89e3\u6570\u636e\u5bf9\u4e8e\u9632\u6b62\u6a21\u578b\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002", "conclusion": "\u901a\u8fc7UnsolvableQA\u6570\u636e\u96c6\u548cUnsolvableRL\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u8bad\u7ec3LLM\u533a\u5206\u53ef\u89e3\u4e0e\u4e0d\u53ef\u89e3\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\uff0c\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u548c\u5e7b\u89c9\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.01710", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.01710", "abs": "https://arxiv.org/abs/2512.01710", "authors": ["Stefano Zeppieri"], "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.", "AI": {"tldr": "\u63d0\u51faMMAG\u6a21\u5f0f\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u6784\u5efa\u4e94\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u63d0\u5347\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\u3001\u4e2a\u6027\u5316\u548c\u6301\u7eed\u6027", "motivation": "LLM\u5728\u5355\u6b21\u63d0\u793a\u4e2d\u80fd\u751f\u6210\u8fde\u8d2f\u6587\u672c\uff0c\u4f46\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u76f8\u5173\u6027\u3001\u4e2a\u6027\u5316\u548c\u8fde\u7eed\u6027\u3002\u4eba\u7c7b\u6c9f\u901a\u4f9d\u8d56\u591a\u79cd\u8bb0\u5fc6\u5f62\u5f0f\uff0c\u9700\u8981\u4e3aLLM\u667a\u80fd\u4f53\u6784\u5efa\u7c7b\u4f3c\u7684\u591a\u5c42\u8bb0\u5fc6\u7cfb\u7edf", "method": "\u5f15\u5165\u6df7\u5408\u8bb0\u5fc6\u589e\u5f3a\u751f\u6210(MMAG)\u6a21\u5f0f\uff0c\u5c06\u8bb0\u5fc6\u7ec4\u7ec7\u4e3a\u4e94\u5c42\uff1a\u5bf9\u8bdd\u8bb0\u5fc6\u3001\u957f\u671f\u7528\u6237\u8bb0\u5fc6\u3001\u60c5\u666f\u4e0e\u4e8b\u4ef6\u5173\u8054\u8bb0\u5fc6\u3001\u611f\u77e5\u4e0e\u60c5\u5883\u8bb0\u5fc6\u3001\u77ed\u671f\u5de5\u4f5c\u8bb0\u5fc6\u3002\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u539f\u7406\uff0c\u5c06\u8fd9\u4e9b\u5c42\u6b21\u6620\u5c04\u5230\u6280\u672f\u7ec4\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u534f\u8c03\u3001\u4f18\u5148\u7ea7\u548c\u51b2\u7a81\u89e3\u51b3\u7b56\u7565", "result": "\u5728Heero\u5bf9\u8bdd\u667a\u80fd\u4f53\u4e2d\u5b9e\u73b0MMAG\uff0c\u52a0\u5bc6\u7684\u957f\u671f\u7528\u6237\u6863\u6848\u548c\u5bf9\u8bdd\u5386\u53f2\u5df2\u80fd\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u7559\u5b58\u7387", "conclusion": "MMAG\u4e3a\u6784\u5efa\u8bb0\u5fc6\u4e30\u5bcc\u7684\u8bed\u8a00\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u4f7f\u5176\u66f4\u52a0\u8fde\u8d2f\u3001\u4e3b\u52a8\u5e76\u7b26\u5408\u4eba\u7c7b\u9700\u6c42\u3002\u540c\u65f6\u8ba8\u8bba\u4e86\u5b58\u50a8\u3001\u68c0\u7d22\u3001\u9690\u79c1\u548c\u5ef6\u8fdf\u7b49\u5b9e\u65bd\u95ee\u9898\u53ca\u5f00\u653e\u6311\u6218"}}
{"id": "2512.00757", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00757", "abs": "https://arxiv.org/abs/2512.00757", "authors": ["Zongjian Han", "Yiran Liang", "Ruiwen Wang", "Yiwei Luo", "Yilin Huang", "Xiaotong Song", "Dongqing Wei"], "title": "Preventing Model Collapse via Contraction-Conditioned Neural Filters", "comment": null, "summary": "This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \\cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\\limsup_{t\\to\\infty}\\mathbb{P}(\\|\\mathbf{e}_t\\|>\u03b4)=0$ for any $\u03b4>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6536\u7f29\u7b97\u5b50\u7684\u795e\u7ecf\u7f51\u7edc\u6ee4\u6ce2\u5668\u65b9\u6cd5\uff0c\u89e3\u51b3\u751f\u6210\u6a21\u578b\u9012\u5f52\u8bad\u7ec3\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u65e0\u9700\u589e\u52a0\u6837\u672c\u91cf\u5373\u53ef\u5b9e\u73b0\u6982\u7387\u6536\u655b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Xu\u7b49\u4eba\u9700\u8981\u8d85\u7ebf\u6027\u6837\u672c\u589e\u957f(O(t^{1+s}))\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u5bf9\u589e\u52a0\u6837\u672c\u91cf\u7684\u4f9d\u8d56\uff0c\u5728\u65e0\u504f\u4f30\u8ba1\u6846\u67b6\u4e0b\u89e3\u51b3\u6a21\u578b\u5d29\u6e83\u95ee\u9898", "method": "\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u6ee4\u6ce2\u5668\u5b66\u4e60\u6ee1\u8db3\u6536\u7f29\u6761\u4ef6\uff0c\u5f00\u53d1\u4e13\u95e8\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u6ee4\u6ce2\u5668\u80fd\u591f\u4e3b\u52a8\u5b66\u4e60\u6ee1\u8db3\u6307\u6570\u65cf\u5206\u5e03\u4e2d\u5047\u8bbe2.3\u7684\u6536\u7f29\u6761\u4ef6", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5f53\u5b66\u4e60\u5230\u7684\u6536\u7f29\u6761\u4ef6\u6ee1\u8db3\u65f6\uff0c\u5373\u4f7f\u4f7f\u7528\u6052\u5b9a\u6837\u672c\u91cf\uff0c\u4f30\u8ba1\u8bef\u5dee\u4e5f\u80fd\u6982\u7387\u6536\u655b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u795e\u7ecf\u7f51\u7edc\u6ee4\u6ce2\u5668\u80fd\u6709\u6548\u5b66\u4e60\u6536\u7f29\u6761\u4ef6\u5e76\u5728\u56fa\u5b9a\u6837\u672c\u91cf\u8bbe\u7f6e\u4e0b\u9632\u6b62\u6a21\u578b\u5d29\u6e83", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u5bf9\u589e\u52a0\u6837\u672c\u91cf\u7684\u4f9d\u8d56\uff0c\u5728\u6052\u5b9a\u6837\u672c\u91cf\u4e0b\u5b9e\u73b0\u4e86\u6982\u7387\u6536\u655b\uff0c\u6709\u6548\u9632\u6b62\u4e86\u6a21\u578b\u5d29\u6e83"}}
{"id": "2512.01728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01728", "abs": "https://arxiv.org/abs/2512.01728", "authors": ["Zhengjia Wang", "Danding Wang", "Qiang Sheng", "Jiaying Wu", "Juan Cao"], "title": "Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference", "comment": "AAAI 2026", "summary": "This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.", "AI": {"tldr": "OmiGraph\uff1a\u9996\u4e2a\u8003\u8651\u4fe1\u606f\u9057\u6f0f\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9057\u6f0f\u611f\u77e5\u56fe\u3001\u5efa\u6a21\u9057\u6f0f\u5173\u7cfb\u3001\u6d88\u606f\u4f20\u9012\u805a\u5408\u6765\u68c0\u6d4b\u57fa\u4e8e\u4fe1\u606f\u9057\u6f0f\u7684\u6b3a\u9a97\u6027\u5185\u5bb9\u3002", "motivation": "\u865a\u5047\u4fe1\u606f\u4e0d\u4ec5\u901a\u8fc7\u660e\u786e\u634f\u9020\u5185\u5bb9\u6b3a\u9a97\u8bfb\u8005\uff0c\u8fd8\u901a\u8fc7\u9690\u542b\u5730\u9057\u6f0f\u5173\u952e\u4fe1\u606f\u6765\u8bef\u5bfc\u5224\u65ad\u3002\u867d\u7136\u524d\u8005\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u57fa\u4e8e\u9057\u6f0f\u7684\u6b3a\u9a97\u65b9\u5f0f\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u89c6\uff0c\u5c3d\u7ba1\u5b83\u80fd\u5728\u770b\u4f3c\u5b8c\u6574\u7684\u4fe1\u606f\u4e2d\u5fae\u5999\u5730\u5f15\u5bfc\u8bfb\u8005\u5f97\u51fa\u9519\u8bef\u7ed3\u8bba\u3002", "method": "1. \u6784\u5efa\u9057\u6f0f\u611f\u77e5\u56fe\uff1a\u5229\u7528\u6355\u6349\u540c\u4e00\u4e8b\u4ef6\u4e92\u8865\u89c6\u89d2\u7684\u4e0a\u4e0b\u6587\u73af\u5883\u4e3a\u76ee\u6807\u65b0\u95fb\u6784\u5efa\u56fe\uff0c\u63ed\u793a\u53ef\u80fd\u88ab\u9057\u6f0f\u7684\u5185\u5bb9\uff1b2. \u9057\u6f0f\u5173\u7cfb\u5efa\u6a21\uff1a\u8bc6\u522b\u5185\u90e8\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\u548c\u52a8\u6001\u9057\u6f0f\u610f\u56fe\uff0c\u5f62\u6210\u5168\u9762\u7684\u9057\u6f0f\u5173\u7cfb\u8868\u793a\uff1b3. \u9057\u6f0f\u611f\u77e5\u6d88\u606f\u4f20\u9012\u548c\u805a\u5408\uff1a\u6574\u5408\u9057\u6f0f\u5185\u5bb9\u548c\u5173\u7cfb\uff0c\u5efa\u7acb\u5168\u9762\u7684\u6b3a\u9a97\u611f\u77e5\u6a21\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmiGraph\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747F1\u5206\u6570\u63d0\u9ad85.4%\uff0c\u51c6\u786e\u7387\u63d0\u9ad85.3%\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u4fe1\u606f\u9057\u6f0f\u7684\u89c6\u89d2\uff0cOmiGraph\u6846\u67b6\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u5904\u7406\u57fa\u4e8e\u9057\u6f0f\u7684\u6b3a\u9a97\u5bf9\u4e8e\u5168\u9762\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.01992", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01992", "abs": "https://arxiv.org/abs/2512.01992", "authors": ["Sai Kolasani", "Maxim Saplin", "Nicholas Crispino", "Kyle Montgomery", "Jared Quincy Davis", "Matei Zaharia", "Chi Wang", "Chenguang Wang"], "title": "LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess", "comment": null, "summary": "We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.", "AI": {"tldr": "LLM CHESS\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8c61\u68cb\u9886\u57df\u7684\u6269\u5c55\u667a\u80fd\u4f53\u4ea4\u4e92\u6765\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u6cdb\u5316\u80fd\u529b\uff0c\u5bf950\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u6392\u540d\uff0c\u53d1\u73b0\u5373\u4f7f\u9876\u7ea7\u63a8\u7406\u6a21\u578b\u4e5f\u96be\u4ee5\u5b8c\u6210\u6e38\u620f\u6216\u53d6\u5f97\u7a33\u5b9a\u80dc\u5229\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5bb9\u6613\u8fc7\u62df\u5408\u548c\u8bb0\u5fc6\u5316\uff0c\u4e14\u5bb9\u6613\u9971\u548c\u3002\u9700\u8981\u4e00\u79cd\u968f\u673a\u3001\u52a8\u6001\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u51cf\u5c11\u8fd9\u4e9b\u95ee\u9898\uff0c\u540c\u65f6\u8bc4\u4f30LLMs\u5728\u590d\u6742\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1LLM CHESS\u6846\u67b6\uff0c\u8ba9LLMs\u4f5c\u4e3a\u8c61\u68cb\u667a\u80fd\u4f53\u4e0e\u968f\u673a\u5bf9\u624b\u5bf9\u5f08\u3002\u4f7f\u7528\u591a\u79cd\u884c\u4e3a\u6307\u6807\u8bc4\u4f30\uff1a\u80dc\u7387\u3001\u8d1f\u7387\u3001\u8d70\u5b50\u8d28\u91cf\u3001\u8d70\u5b50\u5408\u6cd5\u6027\u3001\u5e7b\u89c9\u52a8\u4f5c\u548c\u6e38\u620f\u65f6\u957f\u3002\u5bf9\u9876\u7ea7\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u4e0e\u53ef\u53d8\u6280\u80fd\u914d\u7f6e\u7684\u8c61\u68cb\u5f15\u64ce\u5bf9\u5f08\u6765\u4f30\u7b97Elo\u8bc4\u5206\u3002", "result": "\u5bf950\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u8fdb\u884c\u6392\u540d\u3002\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u96be\u4ee5\u5b8c\u6210\u6e38\u620f\u6216\u53d6\u5f97\u7a33\u5b9a\u80dc\u5229\u3002\u5b9e\u9a8c\u663e\u793a\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002LLM CHESS\u7684\u968f\u673a\u52a8\u6001\u7279\u6027\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u548c\u8bb0\u5fc6\u5316\uff0c\u5373\u4f7f\u5bf9\u9876\u7ea7\u63a8\u7406\u6a21\u578b\u4e5f\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "LLM CHESS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u6d4b\u8bd5LLMs\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u51cf\u5c11\u4e86\u57fa\u51c6\u9971\u548c\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002\u4f5c\u8005\u53d1\u5e03\u4e86\u5b9e\u9a8c\u6846\u67b6\u3001\u516c\u5171\u6392\u884c\u699c\u548c\u76f8\u5173\u6e38\u620f\u6570\u636e\u96c6\u3002"}}
{"id": "2512.01896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01896", "abs": "https://arxiv.org/abs/2512.01896", "authors": ["Jinzheng Yu", "Yang Xu", "Haozhen Li", "Junqi Li", "Yifan Feng", "Ligu Zhu", "Hao Shen", "Lei Shi"], "title": "OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation", "comment": "27 pages, accepted by CMC-Computers, Materials & Continua, 2025", "summary": "Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9a\u4e49\u4e86\u5728\u7ebf\u8206\u60c5\u62a5\u544a\u81ea\u52a8\u751f\u6210\u4efb\u52a1(OPOR-GEN)\uff0c\u6784\u5efa\u4e86\u5305\u542b463\u4e2a\u5371\u673a\u4e8b\u4ef6\u7684\u57fa\u51c6\u6570\u636e\u96c6OPOR-BENCH\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u6846\u67b6OPOR-EVAL\u6765\u6a21\u62df\u4e13\u5bb6\u8bc4\u4f30\u3002", "motivation": "\u5728\u7ebf\u8206\u60c5\u62a5\u544a\u5bf9\u653f\u5e9c\u548c\u4f01\u4e1a\u7684\u5371\u673a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u81ea\u52a8\u62a5\u544a\u751f\u6210\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u7279\u522b\u662f\u6b63\u5f0f\u7684\u4efb\u52a1\u5b9a\u4e49\u548c\u76f8\u5e94\u7684\u57fa\u51c6\u3002", "method": "1. \u5b9a\u4e49OPOR-GEN\u4efb\u52a1\uff1b2. \u6784\u5efa\u4e8b\u4ef6\u4e2d\u5fc3\u7684OPOR-BENCH\u6570\u636e\u96c6\uff0c\u5305\u542b463\u4e2a\u5371\u673a\u4e8b\u4ef6\u53ca\u76f8\u5173\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u548c\u53c2\u8003\u6458\u8981\uff1b3. \u63d0\u51faOPOR-EVAL\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOPOR-EVAL\u6846\u67b6\u4e0e\u4eba\u7c7b\u5224\u65ad\u5177\u6709\u9ad8\u5ea6\u76f8\u5173\u6027\uff0c\u4e3a\u524d\u6cbf\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u4efb\u52a1\u5b9a\u4e49\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5728\u7ebf\u8206\u60c5\u62a5\u544a\u81ea\u52a8\u751f\u6210\u8fd9\u4e00\u5173\u952e\u9886\u57df\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2512.02010", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02010", "abs": "https://arxiv.org/abs/2512.02010", "authors": ["Jack Cook", "Junxian Guo", "Guangxuan Xiao", "Yujun Lin", "Song Han"], "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling", "comment": "10 pages, 5 figures", "summary": "As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa4/6\u65b9\u6cd5\u6539\u8fdbNVFP4\u91cf\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u6bcf\u4e2a\u6570\u503c\u5757\u7684\u4e24\u4e2a\u7f29\u653e\u56e0\u5b50\uff0c\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u9632\u6b62\u8bad\u7ec3\u53d1\u6563\uff0c\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "NVFP4\u7b49\u4f4e\u7cbe\u5ea6\u6570\u503c\u683c\u5f0f\u56e0\u901f\u5ea6\u548c\u5185\u5b58\u4f18\u52bf\u800c\u6d41\u884c\uff0c\u4f46\u8981\u6c42\u6240\u6709\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u6570\u90fd\u91cf\u5316\u4e3aNVFP4\uff0c\u5e38\u5bfc\u81f4\u8bad\u7ec3\u53d1\u6563\u548c\u63a8\u7406\u6027\u80fd\u4e0b\u964d\u3002\u6d6e\u70b9\u683c\u5f0f\u5982FP4\u7684\u6700\u5927\u91cf\u5316\u8bef\u5dee\u51fa\u73b0\u5728\u6bcf\u4e2a\u5757\u7684\u63a5\u8fd1\u6700\u5927\u503c\u5904\uff0c\u8fd9\u662f\u4e0b\u6e38\u6027\u80fd\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51faFour Over Six (4/6)\u65b9\u6cd5\uff0c\u4fee\u6539NVFP4\u91cf\u5316\u7b97\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u6570\u503c\u5757\u8bc4\u4f30\u4e24\u4e2a\u6f5c\u5728\u7684\u7f29\u653e\u56e0\u5b50\u3002\u901a\u8fc7\u7f29\u653e\u5230\u66f4\u5c0f\u7684FP4\u503c\u4f7f\u53ef\u8868\u793a\u503c\u7684\u5206\u5e03\u66f4\u5747\u5300\uff0c\u4ece\u800c\u66f4\u597d\u5730\u8868\u793a\u63a5\u8fd1\u6700\u5927\u503c\u7684\u6570\u636e\u3002", "result": "4/6\u53ef\u5728NVIDIA Blackwell GPU\u4e0a\u9ad8\u6548\u5b9e\u73b0\u3002\u5728transformer\u548c\u6df7\u5408\u6a21\u578b\u67b6\u6784\u7684\u9884\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\uff0c4/6\u9632\u6b62\u4e86\u591a\u79cd\u60c5\u51b5\u4e0b\u7684\u8bad\u7ec3\u53d1\u6563\uff0c\u4f7f\u8bad\u7ec3\u635f\u5931\u663e\u8457\u63a5\u8fd1BF16\u57fa\u51c6\u30024/6\u4e5f\u80fd\u8f7b\u677e\u96c6\u6210\u5230\u591a\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4e2d\uff0c\u666e\u904d\u63d0\u5347\u4e0b\u6e38\u7cbe\u5ea6\u3002", "conclusion": "4/6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86NVFP4\u8bad\u7ec3\u4e2d\u7684\u53d1\u6563\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u4f7f\u7528NVFP4\u8bad\u7ec3\u548c\u90e8\u7f72\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.00949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00949", "abs": "https://arxiv.org/abs/2512.00949", "authors": ["Yansong Liu", "Ronnie Stafford", "Pramit Khetrapal", "Huriye Kocadag", "Gra\u00e7a Carvalho", "Patricia de Winter", "Maryam Imran", "Amelia Snook", "Adamos Hadjivasiliou", "D. Vijay Anand", "Weining Lin", "John Kelly", "Yukun Zhou", "Ivana Drobnjak"], "title": "Multi-Modal AI for Remote Patient Monitoring in Cancer Care", "comment": null, "summary": "For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop)", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u764c\u75c7\u60a3\u8005\u8fdc\u7a0b\u76d1\u6d4b\u7684\u591a\u6a21\u6001AI\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u65e5\u5e38\u8c03\u67e5\u7b49\u591a\u6e90\u6570\u636e\uff0c\u9884\u6d4b\u672a\u6765\u4e0d\u826f\u4e8b\u4ef6\u98ce\u9669\uff0c\u51c6\u786e\u7387\u8fbe83.9%", "motivation": "\u764c\u75c7\u60a3\u8005\u5728\u63a5\u53d7\u7cfb\u7edf\u6cbb\u7597\u671f\u95f4\uff0c\u95e8\u8bca\u5c31\u8bca\u95f4\u9694\u5b58\u5728\u76d1\u6d4b\u7a7a\u767d\uff0c\u5b58\u5728\u672a\u88ab\u76d1\u6d4b\u7684\u526f\u4f5c\u7528\u98ce\u9669\uff0c\u9700\u8981\u8fdc\u7a0b\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u6765\u586b\u8865\u8fd9\u4e00\u62a4\u7406\u7f3a\u53e3", "method": "\u5f00\u53d1\u591a\u6a21\u6001AI\u6846\u67b6\uff0c\u6574\u5408HALO-X\u5e73\u53f0\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u4eba\u53e3\u7edf\u8ba1\u3001\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u3001\u65e5\u5e38\u8c03\u67e5\u3001\u4e34\u5e8a\u4e8b\u4ef6\uff09\uff0c\u5904\u7406\u771f\u5b9e\u4e16\u754c\u8fdc\u7a0b\u76d1\u6d4b\u6570\u636e\u7684\u5f02\u6b65\u6027\u548c\u4e0d\u5b8c\u6574\u6027\uff0c\u9884\u6d4b\u672a\u6765\u4e0d\u826f\u4e8b\u4ef6\u7684\u8fde\u7eed\u98ce\u9669", "result": "\u572884\u540d\u60a3\u8005\u30016,080\u60a3\u8005\u65e5\u3001210\u4e07\u6570\u636e\u70b9\u7684\u89c2\u5bdf\u6027\u8bd5\u9a8c\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe83.9%\uff08AUROC=0.70\uff09\uff0c\u8bc6\u522b\u51fa\u65e2\u5f80\u6cbb\u7597\u3001\u5065\u5eb7\u68c0\u67e5\u3001\u6bcf\u65e5\u6700\u5927\u5fc3\u7387\u4e3a\u5173\u952e\u9884\u6d4b\u7279\u5f81\uff0c\u6848\u4f8b\u7814\u7a76\u663e\u793a\u6a21\u578b\u80fd\u5728\u4e8b\u4ef6\u53d1\u751f\u524d\u63d0\u4f9b\u98ce\u9669\u5347\u7ea7\u9884\u8b66", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u591a\u6a21\u6001AI\u8fdc\u7a0b\u76d1\u6d4b\u5728\u764c\u75c7\u62a4\u7406\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u66f4\u4e3b\u52a8\u7684\u60a3\u8005\u652f\u6301\u63d0\u4f9b\u4e86\u8def\u5f84"}}
{"id": "2512.02017", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02017", "abs": "https://arxiv.org/abs/2512.02017", "authors": ["Shaowei Liu", "David Yifan Yao", "Saurabh Gupta", "Shenlong Wang"], "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion", "comment": "Accepted to NeurIPS 2025. Project page: https://stevenlsw.github.io/visualsync/", "summary": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.", "AI": {"tldr": "VisualSync\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u89c6\u89d2\u52a8\u6001\u7684\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u6beb\u79d2\u7ea7\u7cbe\u5ea6\u5bf9\u9f50\u672a\u6807\u5b9a\u3001\u672a\u540c\u6b65\u7684\u89c6\u9891\uff0c\u901a\u8fc7\u5229\u75283D\u91cd\u5efa\u3001\u7279\u5f81\u5339\u914d\u548c\u5bc6\u96c6\u8ddf\u8e2a\u6765\u6700\u5c0f\u5316\u6781\u7ebf\u8bef\u5dee\u3002", "motivation": "\u968f\u7740\u6d88\u8d39\u7ea7\u76f8\u673a\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u53ef\u4ee5\u8f7b\u677e\u8bb0\u5f55\u5404\u79cd\u91cd\u8981\u65f6\u523b\uff0c\u4f46\u8de8\u76f8\u673a\u89c6\u9891\u6d41\u7684\u540c\u6b65\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u53d7\u63a7\u73af\u5883\u3001\u7279\u5b9a\u76ee\u6807\u3001\u624b\u52a8\u6821\u6b63\u6216\u6602\u8d35\u786c\u4ef6\u3002", "method": "VisualSync\u5229\u7528\u73b0\u6210\u76843D\u91cd\u5efa\u3001\u7279\u5f81\u5339\u914d\u548c\u5bc6\u96c6\u8ddf\u8e2a\u6280\u672f\u63d0\u53d6\u8f68\u8ff9\u7247\u6bb5\u3001\u76f8\u5bf9\u59ff\u6001\u548c\u8de8\u89c6\u89d2\u5bf9\u5e94\u5173\u7cfb\uff0c\u7136\u540e\u901a\u8fc7\u8054\u5408\u6700\u5c0f\u5316\u6781\u7ebf\u8bef\u5dee\u6765\u4f30\u8ba1\u6bcf\u4e2a\u76f8\u673a\u7684\u65f6\u95f4\u504f\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVisualSync\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e2d\u4f4d\u6570\u540c\u6b65\u8bef\u5dee\u4f4e\u4e8e50\u6beb\u79d2\u7684\u7cbe\u5ea6\u3002", "conclusion": "VisualSync\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89c6\u9891\u540c\u6b65\u6846\u67b6\uff0c\u80fd\u591f\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u65f6\u95f4\u5bf9\u9f50\uff0c\u4e3a\u591a\u89c6\u89d2\u89c6\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00771", "abs": "https://arxiv.org/abs/2512.00771", "authors": ["Xiaoshan Wu", "Yifei Yu", "Xiaoyang Lyu", "Yihua Huang", "Bo Wang", "Baoheng Zhang", "Zhongrui Wang", "Xiaojuan Qi"], "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes", "comment": "Accepted at NeurIPS 2025 (spotlight)", "summary": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.", "AI": {"tldr": "EAG3R\uff1a\u7ed3\u5408\u5f02\u6b65\u4e8b\u4ef6\u6d41\u589e\u5f3a\u70b9\u56fe\u91cd\u5efa\u7684\u9c81\u68d23D\u51e0\u4f55\u4f30\u8ba1\u6846\u67b6\uff0c\u5728\u52a8\u6001\u4f4e\u5149\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8eRGB-only\u65b9\u6cd5", "motivation": "\u73b0\u6709RGB-only\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u7269\u4f53\u548c\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u76f8\u673a\u56fa\u6709\u5c40\u9650\u6027\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027", "method": "\u57fa\u4e8eMonST3R\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f15\u5165Retinex\u542f\u53d1\u7684\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u3001\u8f7b\u91cf\u7ea7\u4e8b\u4ef6\u9002\u914d\u5668\u4e0eSNR\u611f\u77e5\u878d\u5408\u673a\u5236\uff0c\u4ee5\u53ca\u4e8b\u4ef6\u9a71\u52a8\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u635f\u5931", "result": "\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u76f8\u673a\u59ff\u6001\u8ddf\u8e2a\u548c\u52a8\u6001\u91cd\u5efa\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684RGB-only\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "EAG3R\u901a\u8fc7\u878d\u5408RGB\u548c\u4e8b\u4ef6\u6570\u636e\uff0c\u65e0\u9700\u591c\u95f4\u6570\u636e\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728\u6311\u6218\u6027\u52a8\u6001\u4f4e\u5149\u573a\u666f\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u51e0\u4f55\u4f30\u8ba1"}}
{"id": "2512.02018", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.02018", "abs": "https://arxiv.org/abs/2512.02018", "authors": ["Anbang Liu", "Guanzhong Hu", "Jiayi Wang", "Ping Guo", "Han Liu"], "title": "Data-Centric Visual Development for Self-Driving Labs", "comment": "11 pages, 4 figures", "summary": "Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u771f\u5b9e\u4e0e\u865a\u62df\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u89e3\u51b3\u81ea\u9a71\u5b9e\u9a8c\u5ba4\u4e2d\u6c14\u6ce1\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u7279\u522b\u662f\u8d1f\u6837\u672c\u4e0d\u8db3\uff0c\u5b9e\u73b099.6%\u51c6\u786e\u7387", "motivation": "\u81ea\u9a71\u5b9e\u9a8c\u5ba4\u9700\u8981\u9ad8\u7cbe\u5ea6\u6a21\u578b\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u7279\u522b\u662f\u8d1f\u6837\u672c\u96be\u4ee5\u83b7\u53d6\u3002\u79fb\u6db2\u662fSDL\u4e2d\u6700\u5173\u952e\u4e14\u7cbe\u5ea6\u654f\u611f\u7684\u64cd\u4f5c\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u6c14\u6ce1\u68c0\u6d4b", "method": "\u6784\u5efa\u6df7\u5408\u7ba1\u9053\u878d\u5408\u771f\u5b9e\u548c\u865a\u62df\u6570\u636e\u751f\u6210\u3002\u771f\u5b9e\u8f68\u9053\u91c7\u7528\u4eba\u673a\u534f\u540c\u65b9\u6848\uff0c\u7ed3\u5408\u81ea\u52a8\u91c7\u96c6\u548c\u9009\u62e9\u6027\u4eba\u5de5\u9a8c\u8bc1\uff1b\u865a\u62df\u8f68\u9053\u4f7f\u7528\u53c2\u8003\u6761\u4ef6\u63d0\u793a\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u6765\u589e\u5f3a\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u7b5b\u9009\u9a8c\u8bc1", "result": "\u5728\u4fdd\u7559\u7684\u771f\u5b9e\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4ec5\u7528\u81ea\u52a8\u91c7\u96c6\u7684\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u7684\u6a21\u578b\u8fbe\u523099.6%\u51c6\u786e\u7387\uff1b\u6df7\u5408\u771f\u5b9e\u548c\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7ef4\u630199.4%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u6536\u96c6\u548c\u5ba1\u67e5\u5de5\u4f5c\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSDL\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89c6\u89c9\u53cd\u9988\u6570\u636e\u4f9b\u5e94\u7b56\u7565\uff0c\u4e3a\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\u548c\u66f4\u5e7f\u6cdb\u7684\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.01457", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01457", "abs": "https://arxiv.org/abs/2512.01457", "authors": ["Rohin Manvi", "Joey Hong", "Tim Seyde", "Maxime Labonne", "Mathias Lechner", "Sergey Levine"], "title": "ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation", "comment": "Code coming soon", "summary": "Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.", "AI": {"tldr": "ZIP-RC\uff1a\u4e00\u79cd\u96f6\u5f00\u9500\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528LLM\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u4fdd\u7559logits\uff0c\u5b9e\u65f6\u9884\u6d4b\u5956\u52b1\u548c\u6210\u672c\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u81ea\u7701\u5f0f\u63a8\u7406\u51b3\u7b56\u3002", "motivation": "\u5f53\u524dLLM\u7f3a\u4e4f\u4eba\u7c7b\u7684\u81ea\u7701\u80fd\u529b\uff08\u5982\u9884\u6d4b\u81ea\u8eab\u6210\u529f\u6982\u7387\u548c\u6240\u9700\u8ba1\u7b97\u91cf\uff09\uff0c\u5bfc\u81f4\u65e0\u6cd5\u667a\u80fd\u5730\u8fdb\u884c\u5143\u8ba4\u77e5\u51b3\u7b56\u3002\u73b0\u6709\u65b9\u6cd5\u5982Best-of-N\u56fa\u5b9a\u91c7\u6837\u9884\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u5b66\u4e60\u9a8c\u8bc1\u5668\u6216\u5956\u52b1\u6a21\u578b\u867d\u80fd\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u4f1a\u589e\u52a0\u989d\u5916\u6210\u672c\u548c\u67b6\u6784\u590d\u6742\u5ea6\u3002", "method": "ZIP-RC\u5728\u6bcf\u6b21token\u751f\u6210\u65f6\uff0c\u91cd\u7528\u540c\u4e00\u524d\u5411\u4f20\u64ad\u4e2d\u4fdd\u7559\u6216\u672a\u4f7f\u7528\u7684logits\uff0c\u8f93\u51fa\u6700\u7ec8\u5956\u52b1\u548c\u5269\u4f59\u957f\u5ea6\u7684\u8054\u5408\u5206\u5e03\u3002\u57fa\u4e8e\u6b64\u5206\u5e03\u8ba1\u7b97\u91c7\u6837\u6548\u7528\u51fd\u6570\uff08\u671f\u671b\u6700\u5927\u5956\u52b1\u3001\u603b\u8ba1\u7b97\u91cf\u548c\u5ef6\u8fdf\u7684\u7ebf\u6027\u7ec4\u5408\uff09\uff0c\u901a\u8fc7\u6700\u5927\u5316\u8be5\u6548\u7528\u7684\u5143\u52a8\u4f5c\u51b3\u5b9a\u7ee7\u7eed\u751f\u6210\u54ea\u4e9btoken\u524d\u7f00\u6216\u542f\u52a8\u65b0\u91c7\u6837\u3002", "result": "\u5728\u6df7\u5408\u96be\u5ea6\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cZIP-RC\u5728\u76f8\u540c\u6216\u66f4\u4f4e\u5e73\u5747\u6210\u672c\u4e0b\uff0c\u6bd4\u591a\u6570\u6295\u7968\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12%\uff0c\u5e76\u5728\u8d28\u91cf\u3001\u8ba1\u7b97\u91cf\u548c\u5ef6\u8fdf\u4e4b\u95f4\u5f62\u6210\u5e73\u6ed1\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "ZIP-RC\u901a\u8fc7\u63d0\u4f9b\u5b9e\u65f6\u5956\u52b1-\u6210\u672c\u81ea\u7701\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86LLM\u7f3a\u4e4f\u81ea\u7701\u80fd\u529b\u7684\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u3001\u67b6\u6784\u53d8\u66f4\u6216\u63a8\u7406\u5f00\u9500\u3002"}}
{"id": "2512.01054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01054", "abs": "https://arxiv.org/abs/2512.01054", "authors": ["MohammadParsa Dini", "Human Jafari", "Sajjad Amini", "MohammadMahdi Mojahedian"], "title": "Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs", "comment": null, "summary": "Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.\n  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.\n  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.\n  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.", "AI": {"tldr": "\u63d0\u51faAdaptive-lambda SISS\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u63a8\u65ad\u6df7\u5408\u6743\u91cdlambda\u6765\u6539\u8fdb\u673a\u5668\u9057\u5fd8\uff0c\u76f8\u6bd4\u56fa\u5b9alambda\u65b9\u6cd5\u5728\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u8d28\u91cf\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u6df7\u5408\u6743\u91cdlambda\u7684\u9057\u5fd8\u65b9\u6cd5\uff08\u5982Static-lambda SISS\uff09\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u4e0d\u540c\u6837\u672c\u548c\u8bad\u7ec3\u9636\u6bb5\u6240\u9700\u7684\u9057\u5fd8\u5f3a\u5ea6\u4e0d\u540c\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u81ea\u9002\u5e94\u673a\u5236\u3002", "method": "\u5c06lambda\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a8\u7406\u7f51\u7edc\u52a8\u6001\u63a8\u65ad\uff0c\u8be5\u7f51\u7edc\u57fa\u4e8e\u77ac\u65f6SISS\u635f\u5931\u9879\uff08\u4fdd\u7559/\u9057\u5fd8\u635f\u5931\u53ca\u5176\u68af\u5ea6\uff09\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u53c2\u6570\u5316\u81ea\u9002\u5e94\u540e\u9a8c\u5206\u5e03\uff0c\u901a\u8fc7\u53d8\u5206\u76ee\u6807\u8054\u5408\u4f18\u5316\u6269\u6563\u6a21\u578b\u548clambda\u63a8\u7406\u673a\u5236\u3002", "result": "\u5728\u589e\u5f3aMNIST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaptive-lambda SISS\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u9759\u6001lambda SISS\uff0c\u5728\u66f4\u597d\u4fdd\u7559\u4fdd\u7559\u96c6\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9057\u5fd8\u7c7b\u522b\u66f4\u5f3a\u7684\u79fb\u9664\u6548\u679c\u3002", "conclusion": "\u81ea\u9002\u5e94lambda\u673a\u5236\u4e3a\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u57fa\u4e8e\u5206\u6570\u7684\u9057\u5fd8\u548c\u591a\u7c7b\u522b\u573a\u666f\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6df7\u5408\u76ee\u6807\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.01062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01062", "abs": "https://arxiv.org/abs/2512.01062", "authors": ["Seokhyun Chin", "Junghwan Park", "Woojin Cho"], "title": "PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting", "comment": "NeurIPS 2025 Machine Learning and Physical Sciences Workshop", "summary": "Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.", "AI": {"tldr": "\u63d0\u51faPIANO\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4e0e\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\uff0c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u7269\u7406\u4e00\u81f4\u6027", "motivation": "\u5f53\u524d\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9650\u5236\u591a\uff0c\u8bb8\u591a\u56fd\u5bb6\u96be\u4ee5\u83b7\u5f97\u3002\u9700\u8981\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u7269\u7406\u4e00\u81f4\u4e14\u53ef\u8bbf\u95ee\u7684\u9884\u6d4b\u65b9\u6cd5", "method": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u53cc\u795e\u7ecf\u7b97\u5b50(PIANO)\u7ed3\u6784\uff0c\u5728\u8bad\u7ec3\u4e2d\u901a\u8fc7PINN\u635f\u5931\u5f3a\u5236\u5b9e\u65bd\u5e73\u6d41-\u6269\u6563\u57fa\u672c\u65b9\u7a0b\u9884\u6d4b\u536b\u661f\u56fe\u50cf\uff0c\u7136\u540e\u7528\u751f\u6210\u6a21\u578b\u5c06\u536b\u661f\u56fe\u50cf\u8f6c\u6362\u4e3a\u96f7\u8fbe\u56fe\u50cf\u8fdb\u884c\u964d\u6c34\u9884\u62a5", "result": "\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u4e2d\u5ea6\u964d\u6c34(4mm/h)\u548c\u77ed\u671f\u5f3a\u964d\u6c34(8mm/h)\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u9884\u6d4b\u5b63\u8282\u6027\u53d8\u5316\u5c0f\uff0c\u8868\u660e\u6cdb\u5316\u80fd\u529b\u5f3a", "conclusion": "PIANO\u6a21\u578b\u5c55\u793a\u4e86\u7269\u7406\u4fe1\u606f\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u7684\u6f5c\u529b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u826f\u597d\u57fa\u51c6"}}
{"id": "2512.00912", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00912", "abs": "https://arxiv.org/abs/2512.00912", "authors": ["Abdelghafour Halimi", "Ali Alibrahim", "Didier Barradas-Bautista", "Ronell Sicat", "Abdulkader M. Afifi"], "title": "ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices", "comment": null, "summary": "This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u5229\u75283D\u626b\u63cf\u884d\u751f\u76842D\u5faeCT\u5207\u7247\u81ea\u52a8\u5206\u7c7b12\u79cd\u6709\u5b54\u866b\u7269\u79cd\uff0c\u6700\u7ec8\u96c6\u6210\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe95.64%\uff0c\u5e76\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u7528\u4e8e\u5b9e\u65f6\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u6709\u5b54\u866b\u5206\u7c7b\u4f9d\u8d56\u4e13\u5bb6\u4eba\u5de5\u8bc6\u522b\uff0c\u6548\u7387\u4f4e\u4e14\u4e3b\u89c2\u6027\u5f3a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u81ea\u52a8\u5316\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5fae\u4f53\u53e4\u751f\u7269\u5b66\u7814\u7a76\u63d0\u4f9bAI\u8f85\u52a9\u5de5\u5177\u3002", "method": "1. \u6784\u5efa\u5305\u542b97\u4e2a\u5faeCT\u626b\u63cf\u6807\u672c\uff0827\u79cd\u7269\u79cd\uff09\u7684\u6570\u636e\u96c6\uff0c\u9009\u53d612\u79cd\u4ee3\u8868\u6027\u7269\u79cd\uff1b2. \u91c7\u7528\u6807\u672c\u7ea7\u6570\u636e\u5206\u5272\u9632\u6b62\u6570\u636e\u6cc4\u9732\uff1b3. \u8bc4\u4f307\u79cd\u5148\u8fdb\u76842D CNN\u67b6\u6784\uff1b4. \u7ed3\u5408ConvNeXt-Large\u548cEfficientNetV2-Small\u6784\u5efa\u96c6\u6210\u6a21\u578b\uff1b5. \u5f00\u53d1\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u652f\u6301\u5b9e\u65f6\u5206\u7c7b\u548c3D\u5207\u7247\u5339\u914d\u3002", "result": "\u96c6\u6210\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523095.64%\u7684\u51c6\u786e\u7387\uff0ctop-3\u51c6\u786e\u738799.6%\uff0c\u6240\u6709\u7269\u79cd\u7684ROC\u66f2\u7ebf\u4e0b\u9762\u79ef(AUC)\u4e3a0.998\u3002\u521b\u5efa\u4e86\u5305\u542b109,617\u4e2a\u9ad8\u8d28\u91cf2D\u5207\u7247\u7684\u6570\u636e\u96c6\uff0844,103\u8bad\u7ec3\uff0c14,046\u9a8c\u8bc1\uff0c51,468\u6d4b\u8bd5\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aAI\u8f85\u52a9\u5fae\u4f53\u53e4\u751f\u7269\u5b66\u8bc6\u522b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u5b8c\u5168\u53ef\u590d\u73b0\u7684\u6709\u5b54\u866b\u5206\u7c7b\u6846\u67b6\uff0c\u5f25\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5e94\u7528\u5730\u7403\u79d1\u5b66\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5f00\u53d1\u7684\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u4fbf\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.00999", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00999", "abs": "https://arxiv.org/abs/2512.00999", "authors": ["Mohsin Rasheed", "Abdullah Al-Mamun"], "title": "Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints", "comment": null, "summary": "Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u4e0e\u533a\u5757\u94fe\u6eaf\u6e90\u6846\u67b6\uff0c\u63d0\u5347AI\u8f85\u52a9\u8bca\u65ad\u53ef\u9760\u6027\u548c\u53ef\u8ffd\u6eaf\u6027", "motivation": "\u73b0\u5b9e\u533b\u7597\u5f71\u50cf\u5e38\u53d7\u566a\u58f0\u3001\u635f\u574f\u548c\u7be1\u6539\u5f71\u54cd\uff0c\u4f20\u7edf\u91cd\u5efa\u65b9\u6cd5\u6ce8\u91cd\u50cf\u7d20\u6062\u590d\u4f46\u53ef\u80fd\u635f\u5bb3\u89e3\u5256\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5f71\u54cd\u4e34\u5e8a\u8bca\u65ad\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u5c42\u6f5c\u5728\u5d4c\u5165\u4e0e\u6df7\u5408U-Net\u67b6\u6784\u4fdd\u6301\u4e34\u5e8a\u76f8\u5173\u7ed3\u6784\uff1b\u96c6\u6210\u8f7b\u91cf\u7ea7\u533a\u5757\u94fe\u6eaf\u6e90\u5c42\uff0c\u91c7\u7528\u65e0\u6807\u5ea6\u56fe\u8bbe\u8ba1\u8bb0\u5f55\u91cd\u5efa\u8fc7\u7a0b", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u635f\u574f\u7c7b\u578b\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u91cd\u5efa\u7cbe\u5ea6\u548c\u6eaf\u6e90\u5b8c\u6574\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u91cd\u5efa\u4e0e\u5b89\u5168\u53ef\u8ffd\u6eaf\u6027\u7ed3\u5408\uff0c\u63a8\u8fdb\u533b\u7597\u5f71\u50cf\u53ef\u9760AI\u53d1\u5c55\uff0c\u589e\u5f3a\u8bca\u65ad\u4fe1\u5fc3\u548c\u533b\u7597\u73af\u5883\u5408\u89c4\u6027"}}
{"id": "2512.01116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.01116", "abs": "https://arxiv.org/abs/2512.01116", "authors": ["Yilan Zhang", "Li Nanbo", "Changchun Yang", "J\u00fcrgen Schmidhuber", "Xin Gao"], "title": "Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis", "comment": "37 pages, 14 Figures", "summary": "The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.", "AI": {"tldr": "SlotSPE\uff1a\u57fa\u4e8e\u69fd\u4f4d\u7684\u7ed3\u6784\u9884\u540e\u4e8b\u4ef6\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u591a\u6a21\u6001\u8f93\u5165\u4e3a\u4e92\u65a5\u69fd\u4f4d\u8868\u793a\uff0c\u6709\u6548\u6355\u6349\u7a00\u758f\u4f46\u5173\u952e\u7684\u9884\u540e\u4e8b\u4ef6\uff0c\u63d0\u5347\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5efa\u6a21\u7ec4\u7ec7\u5b66\u56fe\u50cf\u548c\u57fa\u56e0\u8c31\u4e4b\u95f4\u7684\u590d\u6742\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u4ea4\u4e92\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u51b3\u5b9a\u60a3\u8005\u9884\u540e\u7684\u7a00\u758f\u3001\u60a3\u8005\u7279\u5f02\u6027\u3001\u672a\u6807\u6ce8\u7684\u7ed3\u6784\u6027\u9884\u540e\u4e8b\u4ef6\u3002", "method": "\u63d0\u51faSlotSPE\u6846\u67b6\uff0c\u53d7\u56e0\u5b50\u7f16\u7801\u539f\u7406\u542f\u53d1\uff0c\u4f7f\u7528\u69fd\u6ce8\u610f\u529b\u5c06\u6bcf\u4e2a\u60a3\u8005\u7684\u591a\u6a21\u6001\u8f93\u5165\u538b\u7f29\u4e3a\u7d27\u51d1\u3001\u6a21\u6001\u7279\u5f02\u6027\u3001\u4e92\u65a5\u7684\u69fd\u4f4d\u96c6\u5408\uff0c\u5c06\u8fd9\u4e9b\u69fd\u4f4d\u8868\u793a\u4f5c\u4e3a\u9884\u540e\u4e8b\u4ef6\u7684\u7f16\u7801\u3002", "result": "\u572810\u4e2a\u764c\u75c7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSlotSPE\u57288\u4e2a\u961f\u5217\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6574\u4f53\u63d0\u53472.9%\uff0c\u5728\u57fa\u56e0\u7ec4\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u4e8b\u4ef6\u5206\u89e3\u663e\u8457\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SlotSPE\u901a\u8fc7\u69fd\u4f4d\u8868\u793a\u6709\u6548\u5efa\u6a21\u590d\u6742\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u6355\u6349\u5173\u952e\u9884\u540e\u4e8b\u4ef6\uff0c\u4e3a\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u63d0\u4f9b\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.01672", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01672", "abs": "https://arxiv.org/abs/2512.01672", "authors": ["Zhongyuan Wu", "Jingyuan Wang", "Zexuan Cheng", "Yilong Zhou", "Weizhi Wang", "Juhua Pu", "Chao Li", "Changqing Ma"], "title": "ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models", "comment": null, "summary": "Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.", "AI": {"tldr": "\u63d0\u51faICAD-LLM\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5b9e\u73b0\u8de8\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u573a\u666f", "motivation": "\u73b0\u4ee3\u7cfb\u7edf\u5728\u5feb\u901f\u6f14\u53d8\u73af\u5883\u4e2d\u4ea7\u751f\u591a\u79cd\u4e92\u8fde\u6570\u636e\u6a21\u6001\uff08\u65f6\u95f4\u5e8f\u5217\u3001\u7cfb\u7edf\u65e5\u5fd7\u3001\u8868\u683c\u8bb0\u5f55\uff09\uff0c\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u6a21\u6001\uff0c\u7f3a\u4e4f\u5904\u7406\u5f02\u6784\u6570\u636e\u683c\u5f0f\u548c\u8de8\u57df\u6cdb\u5316\u7684\u80fd\u529b", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u5f02\u5e38\u68c0\u6d4b\uff08ICAD\uff09\u65b0\u8303\u5f0f\uff0c\u57fa\u4e8e\u5f02\u5e38\u6837\u672c\u4e0e\u6b63\u5e38\u53c2\u8003\u96c6\u7684\u5dee\u5f02\u5ea6\u5b9a\u4e49\u5f02\u5e38\uff1b\u5f00\u53d1ICAD-LLM\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5904\u7406\u5f02\u6784\u6570\u636e", "result": "ICAD-LLM\u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u4e0e\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5bf9\u672a\u89c1\u4efb\u52a1\u8868\u73b0\u51fa\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u5e76\u652f\u6301\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883", "conclusion": "ICAD-LLM\u662f\u9996\u4e2a\u80fd\u591f\u8de8\u9886\u57df\u548c\u6a21\u6001\u5904\u7406\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u7684\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u5f02\u6784\u6570\u636e\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6"}}
{"id": "2512.01333", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01333", "abs": "https://arxiv.org/abs/2512.01333", "authors": ["A S M Ahsanul Sarkar Akib", "Raduana Khawla", "Abdul Hasib"], "title": "Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI", "comment": null, "summary": "Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u96c6\u6210\u5efa\u6a21\u548c\u53ef\u89e3\u91caAI\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5352\u4e2d\u98ce\u9669\u9884\u6d4b\uff0c\u5728\u5352\u4e2d\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.09%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5352\u4e2d\u662f\u5168\u7403\u4e3b\u8981\u7684\u6b7b\u4ea1\u548c\u6c38\u4e45\u6027\u635f\u4f24\u539f\u56e0\uff0c\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u5bf9\u4e8e\u53ca\u65f6\u5e72\u9884\u548c\u6709\u6548\u9884\u9632\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5f00\u53d1\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u6a21\u578b\u6765\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5efa\u6a21\u548c\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u5305\u62ec\u7279\u5f81\u5de5\u7a0b\u548c\u6570\u636e\u9884\u5904\u7406\uff08\u4f7f\u7528\u968f\u673a\u8fc7\u91c7\u6837\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff09\uff0c\u5bf910\u79cd\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u6700\u7ec8\u6784\u5efa\u4f18\u5316\u7684\u96c6\u6210\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797+ExtraTrees+XGBoost\uff09\uff0c\u5e76\u4f7f\u7528LIME\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u4f18\u5316\u540e\u7684\u96c6\u6210\u6a21\u578b\u5728\u5352\u4e2d\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u523099.09%\u7684\u51c6\u786e\u7387\u3002\u901a\u8fc7LIME\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8bc6\u522b\u51fa\u4e09\u4e2a\u5173\u952e\u4e34\u5e8a\u53d8\u91cf\uff1a\u5e74\u9f84\u3001\u9ad8\u8840\u538b\u548c\u8840\u7cd6\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u96c6\u6210\u5b66\u4e60\u4e0e\u53ef\u89e3\u91caAI\u7684\u7ed3\u5408\u80fd\u591f\u63d0\u4f9b\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5352\u4e2d\u98ce\u9669\u8bc4\u4f30\uff0c\u901a\u8fc7\u65e9\u671f\u9884\u6d4b\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u9884\u9632\u548c\u4e2a\u6027\u5316\u4e34\u5e8a\u51b3\u7b56\uff0c\u6709\u6f5c\u529b\u6539\u53d8\u5352\u4e2d\u9884\u6d4b\u548c\u5fc3\u8840\u7ba1\u98ce\u9669\u7ba1\u7406\u3002"}}
{"id": "2512.01478", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.01478", "abs": "https://arxiv.org/abs/2512.01478", "authors": ["Omer Sela", "Michael Chertok", "Lior Wolf"], "title": "CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball", "comment": null, "summary": "This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.", "AI": {"tldr": "CourtMotion\uff1a\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u548c\u9884\u6d4b\u804c\u4e1a\u7bee\u7403\u6bd4\u8d5b\u4e2d\u4e8b\u4ef6\u4e0e\u6218\u672f\u53d1\u5c55\u7684\u65f6\u7a7a\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u9aa8\u9abc\u8ffd\u8e2a\u6570\u636e\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u548c\u7bee\u7403\u5206\u6790\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7403\u5458\u4f4d\u7f6e\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u8eab\u4f53\u671d\u5411\u3001\u9632\u5b88\u59ff\u6001\u6216\u6295\u7bee\u51c6\u5907\u52a8\u4f5c\u7b49\u5173\u952e\u6307\u6807\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e8e\u7406\u89e3\u7bee\u7403\u6bd4\u8d5b\u4e2d\u7684\u8bed\u4e49\u610f\u4e49\u548c\u9884\u6d4b\u4e8b\u4ef6\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u9aa8\u9abc\u8ffd\u8e2a\u6570\u636e\u6355\u6349\u7ec6\u5fae\u52a8\u4f5c\u6a21\u5f0f\uff0c\u7136\u540e\u4f7f\u7528\u5e26\u6709\u4e13\u95e8\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\u67b6\u6784\u5efa\u6a21\u7403\u5458\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u4e8b\u4ef6\u6295\u5f71\u5934\u5c06\u7403\u5458\u52a8\u4f5c\u4e0e\u4f20\u7403\u3001\u6295\u7bee\u3001\u62a2\u65ad\u7b49\u7bee\u7403\u4e8b\u4ef6\u660e\u786e\u8fde\u63a5\u3002", "result": "\u5728NBA\u8ffd\u8e2a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u4f4d\u7f6e\u7684\u6a21\u578b\uff0c\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\u51cf\u5c1135%\uff1b\u5728\u5173\u952e\u7bee\u7403\u5206\u6790\u4efb\u52a1\uff08\u6321\u62c6\u68c0\u6d4b\u3001\u6295\u7bee\u8005\u8bc6\u522b\u3001\u52a9\u653b\u9884\u6d4b\u3001\u6295\u7bee\u4f4d\u7f6e\u5206\u7c7b\u3001\u6295\u7bee\u7c7b\u578b\u8bc6\u522b\uff09\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CourtMotion\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u52a8\u4f5c\u6a21\u5f0f\u4e0e\u6218\u672f\u8bed\u4e49\u7406\u89e3\uff0c\u4e3a\u7bee\u7403\u4e8b\u4ef6\u5206\u6790\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u57fa\u7840\uff0c\u5176\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4f5c\u4e3a\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u6548\u5de5\u5177\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.01906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01906", "abs": "https://arxiv.org/abs/2512.01906", "authors": ["Sanja Karilanova", "Subhrakanti Dey", "Ay\u00e7a \u00d6z\u00e7elikkale"], "title": "Delays in Spiking Neural Networks: A State Space Model Approach", "comment": null, "summary": "Spiking neural networks (SNNs) are biologically inspired, event-driven models that are suitable for processing temporal data and offer energy-efficient computation when implemented on neuromorphic hardware. In SNNs, richer neuronal dynamic allows capturing more complex temporal dependencies, with delays playing a crucial role by allowing past inputs to directly influence present spiking behavior. We propose a general framework for incorporating delays into SNNs through additional state variables. The proposed mechanism enables each neuron to access a finite temporal input history. The framework is agnostic to neuron models and hence can be seamlessly integrated into standard spiking neuron models such as LIF and adLIF. We analyze how the duration of the delays and the learnable parameters associated with them affect the performance. We investigate the trade-offs in the network architecture due to additional state variables introduced by the delay mechanism. Experiments on the Spiking Heidelberg Digits (SHD) dataset show that the proposed mechanism matches the performance of existing delay-based SNNs while remaining computationally efficient. Moreover, the results illustrate that the incorporation of delays may substantially improve performance in smaller networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5ef6\u8fdf\u673a\u5236\u878d\u5165\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u989d\u5916\u72b6\u6001\u53d8\u91cf\u4f7f\u795e\u7ecf\u5143\u80fd\u591f\u8bbf\u95ee\u6709\u9650\u65f6\u95f4\u8f93\u5165\u5386\u53f2\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u65f6\u5e8f\u6570\u636e\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u4f5c\u4e3a\u751f\u7269\u542f\u53d1\u7684\u80fd\u91cf\u9ad8\u6548\u8ba1\u7b97\u6a21\u578b\uff0c\u5728\u5904\u7406\u65f6\u5e8f\u6570\u636e\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002\u5ef6\u8fdf\u673a\u5236\u5141\u8bb8\u8fc7\u53bb\u7684\u8f93\u5165\u76f4\u63a5\u5f71\u54cd\u5f53\u524d\u7684\u8109\u51b2\u884c\u4e3a\uff0c\u80fd\u591f\u6355\u6349\u66f4\u590d\u6742\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5ef6\u8fdf\u96c6\u6210\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u795e\u7ecf\u5143\u5f15\u5165\u989d\u5916\u7684\u72b6\u6001\u53d8\u91cf\u6765\u6574\u5408\u5ef6\u8fdf\u673a\u5236\u3002\u8be5\u6846\u67b6\u4e0e\u795e\u7ecf\u5143\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff08\u5982LIF\u548cadLIF\uff09\u4e2d\u3002\u5ef6\u8fdf\u6301\u7eed\u65f6\u95f4\u548c\u76f8\u5173\u53ef\u5b66\u4e60\u53c2\u6570\u53ef\u8c03\u8282\uff0c\u6846\u67b6\u5206\u6790\u4e86\u5ef6\u8fdf\u673a\u5236\u5f15\u5165\u7684\u989d\u5916\u72b6\u6001\u53d8\u91cf\u5bf9\u7f51\u7edc\u67b6\u6784\u7684\u6743\u8861\u3002", "result": "\u5728Spiking Heidelberg Digits (SHD)\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u673a\u5236\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u57fa\u4e8e\u5ef6\u8fdf\u7684SNNs\u76f8\u5f53\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7ed3\u679c\u663e\u793a\u5728\u8f83\u5c0f\u7684\u7f51\u7edc\u4e2d\uff0c\u5ef6\u8fdf\u7684\u6574\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ef6\u8fdf\u96c6\u6210\u6846\u67b6\u4e3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6355\u6349\u590d\u6742\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5c0f\u578b\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u751f\u7269\u5408\u7406\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u65f6\u5e8f\u6570\u636e\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2512.01755", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.01755", "abs": "https://arxiv.org/abs/2512.01755", "authors": ["Yucheng Liao", "Jiajun Liang", "Kaiqian Cui", "Baoquan Zhao", "Haoran Xie", "Wei Liu", "Qing Li", "Xudong Mao"], "title": "FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing", "comment": null, "summary": "Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.", "AI": {"tldr": "FreqEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u9891\u7279\u5f81\u6ce8\u5165\u3001\u81ea\u9002\u5e94\u6ce8\u5165\u7b56\u7565\u548c\u8def\u5f84\u8865\u507f\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u591a\u8f6e\u7f16\u8f91\u4e2d\u7684\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8610+\u8f6e\u6b21\u7684\u7a33\u5b9a\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5355\u6b21\u7f16\u8f91\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u8f6e\u7f16\u8f91\u4e2d\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u9ad8\u9891\u4fe1\u606f\u7684\u6e10\u8fdb\u6027\u635f\u5931\u662f\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "FreqEdit\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a1\uff09\u4ece\u53c2\u8003\u901f\u5ea6\u573a\u6ce8\u5165\u9ad8\u9891\u7279\u5f81\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff1b2\uff09\u81ea\u9002\u5e94\u6ce8\u5165\u7b56\u7565\uff0c\u901a\u8fc7\u7a7a\u95f4\u8c03\u5236\u6ce8\u5165\u5f3a\u5ea6\u5b9e\u73b0\u7cbe\u786e\u7684\u533a\u57df\u63a7\u5236\uff1b3\uff09\u8def\u5f84\u8865\u507f\u673a\u5236\uff0c\u5b9a\u671f\u91cd\u65b0\u6821\u51c6\u7f16\u8f91\u8f68\u8ff9\u4ee5\u9632\u6b62\u8fc7\u5ea6\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreqEdit\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u5747\u4f18\u4e8e7\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b010+\u8f6e\u8fde\u7eed\u8fed\u4ee3\u7684\u7a33\u5b9a\u7f16\u8f91\u3002", "conclusion": "FreqEdit\u901a\u8fc7\u89e3\u51b3\u591a\u8f6e\u7f16\u8f91\u4e2d\u7684\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u7f16\u8f91\u7684\u6027\u80fd\u3002"}}
{"id": "2512.01816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01816", "abs": "https://arxiv.org/abs/2512.01816", "authors": ["Juanxi Tian", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "comment": "35 pages, 12 figures, 10 tables", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Envision\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u56fe\u50cf\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u56e0\u679c\u4e8b\u4ef6\u8fdb\u5c55\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u5355\u56fe\u50cf\u751f\u6210\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u6a21\u5f0f\u5339\u914d\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u65f6\u95f4\u8fc7\u7a0b\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u4e16\u754c\u77e5\u8bc6\u7684\u5185\u5316\u3002", "method": "\u63d0\u51faEnvision\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1000\u4e2a\u56db\u9636\u6bb5\u63d0\u793a\uff0c\u6db5\u76d6\u516d\u4e2a\u79d1\u5b66\u548c\u4eba\u6587\u9886\u57df\uff0c\u5e76\u5f15\u5165Envision-Score\u7efc\u5408\u6307\u6807\u8bc4\u4f30\u591a\u7ef4\u5ea6\u4e00\u81f4\u6027\u3001\u7269\u7406\u6027\u548c\u7f8e\u5b66\u3002", "result": "\u8bc4\u4f3015\u4e2a\u6a21\u578b\u53d1\u73b0\uff1a\u4e13\u7528T2I\u6a21\u578b\u7f8e\u5b66\u6e32\u67d3\u80fd\u529b\u5f3a\u4f46\u7f3a\u4e4f\u4e16\u754c\u77e5\u8bc6\uff1b\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u4e8b\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u65f6\u7a7a\u4e00\u81f4\u6027\u4ecd\u662f\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u4e13\u6ce8\u4e8e\u56e0\u679c\u5b64\u7acb\u7684\u5355\u56fe\u50cf\u4f1a\u963b\u788d\u591a\u5e27\u63a8\u7406\u548c\u751f\u6210\uff0c\u4fc3\u8fdb\u9759\u6001\u6a21\u5f0f\u5339\u914d\u800c\u975e\u52a8\u6001\u4e16\u754c\u5efa\u6a21\uff0c\u6700\u7ec8\u9650\u5236\u4e16\u754c\u77e5\u8bc6\u7684\u5185\u5316\u548c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2512.01843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.01843", "abs": "https://arxiv.org/abs/2512.01843", "authors": ["Zeqing Wang", "Keze Wang", "Lei Zhang"], "title": "PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models", "comment": "17 pages, 8 figures", "summary": "Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \\textbf{PID} (\\textbf{P}hysical \\textbf{I}mplausibility \\textbf{D}etection) dataset, which consists of a \\textit{test split} of 500 manually annotated videos and a \\textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \\textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \\href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PhyDetEx\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7269\u7406\u4e0d\u53ef\u884c\u6027\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6765\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u7269\u7406\u89c4\u5f8b\u7684\u9075\u5faa\u7a0b\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u8d28\u91cf\u548c\u957f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u80fd\u7406\u89e3\u7269\u7406\u89c4\u5f8b\u5e76\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u89c6\u9891\u4ecd\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u8bc6\u522b\u751f\u6210\u89c6\u9891\u4e2d\u7684\u7269\u7406\u4e0d\u53ef\u80fd\u5185\u5bb9\u3002", "method": "\u6784\u5efa\u4e86PID\u6570\u636e\u96c6\uff08\u5305\u542b500\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u6d4b\u8bd5\u89c6\u9891\u548c2,588\u4e2a\u914d\u5bf9\u8bad\u7ec3\u89c6\u9891\uff09\uff0c\u901a\u8fc7\u6539\u5199\u771f\u5b9e\u89c6\u9891\u7684\u6807\u9898\u8bf1\u5bfcT2V\u6a21\u578b\u751f\u6210\u7269\u7406\u4e0d\u5408\u7406\u5185\u5bb9\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u8bad\u7ec3VLM\uff0c\u4f7f\u5176\u80fd\u591f\u68c0\u6d4b\u7269\u7406\u4e0d\u5408\u7406\u4e8b\u4ef6\u5e76\u751f\u6210\u8fdd\u53cd\u7269\u7406\u539f\u7406\u7684\u6587\u672c\u89e3\u91ca\u3002", "result": "\u5f00\u53d1\u4e86PhyDetEx\u7269\u7406\u5408\u7406\u6027\u68c0\u6d4b\u5668\u548c\u89e3\u91ca\u5668\uff0c\u5bf9\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684T2V\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u6700\u8fd1\u7684T2V\u6a21\u578b\u5728\u751f\u6210\u7269\u7406\u5408\u7406\u5185\u5bb9\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7406\u89e3\u548c\u9075\u5faa\u7269\u7406\u89c4\u5f8b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u63d0\u51fa\u7684PhyDetEx\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347T2V\u6a21\u578b\u7684\u7269\u7406\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.01853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.01853", "abs": "https://arxiv.org/abs/2512.01853", "authors": ["Tsz-To Wong", "Ching-Chun Huang", "Hong-Han Shuai"], "title": "COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis", "comment": "Accepted by AAAI 2026 Workshop LaMAS", "summary": "Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct \"cognitive tool\" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u91cd\u6784\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4f5c\u4e3a\u4f53\u80b2\u89c6\u9891\u7406\u89e3\u7684\u57fa\u7840\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u4f5c\u4e3a\"\u8ba4\u77e5\u5de5\u5177\"\uff0c\u5b9e\u73b0\u4ece\u5fae\u89c2\u52a8\u4f5c\u5230\u5b8f\u89c2\u7b56\u7565\u7684\u8de8\u4efb\u52a1\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6a21\u578b\u5728\u5904\u7406\u4f53\u80b2\u89c6\u9891\u7684\u65f6\u5e8f\u5c42\u6b21\u7ed3\u6784\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u6cdb\u5316\u6027\u3001\u65b0\u4efb\u52a1\u5f00\u53d1\u6210\u672c\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u7ef4\u5ea6\u548c\u4efb\u52a1\u7684\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u53ef\u91cd\u6784\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u95e8\u8d1f\u8d23\u5206\u6790\u7684\u4e0d\u540c\u65b9\u9762\u3002\u901a\u8fc7\u667a\u80fd\u4f53\u7684\u8fed\u4ee3\u8c03\u7528\u548c\u7075\u6d3b\u7ec4\u5408\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u7ba1\u9053\uff0c\u652f\u6301\u4ece\u77ed\u671f\u5206\u6790\u63a8\u7406\uff08\u5982\u56de\u5408\u95ee\u7b54\uff09\u5230\u957f\u671f\u751f\u6210\u6458\u8981\uff08\u5982\u6bd4\u8d5b\u603b\u7ed3\uff09\u7684\u5404\u79cd\u4efb\u52a1\u3002", "result": "\u5728\u7fbd\u6bdb\u7403\u5206\u6790\u7684\u4e24\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u9002\u5e94\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6865\u63a5\u7ec6\u7c92\u5ea6\u4e8b\u4ef6\u68c0\u6d4b\u548c\u5168\u5c40\u8bed\u4e49\u7ec4\u7ec7\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u8de8\u4efb\u52a1\u4f53\u80b2\u89c6\u9891\u667a\u80fd\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u5411\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u7cfb\u7edf\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u9c81\u68d2\u7684\u8de8\u4efb\u52a1\u4f53\u80b2\u89c6\u9891\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9879\u76ee\u4e3b\u9875\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.01885", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.01885", "abs": "https://arxiv.org/abs/2512.01885", "authors": ["Florian B\u00fcrger", "Martim Dias Gomes", "Nica Gutu", "Adri\u00e1n E. Granada", "No\u00e9mie Moreau", "Katarzyna Bozek"], "title": "TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals", "comment": "13 pages, 7 figures, 2 tables. This work has been submitted to IEEE Transactions on Medical Imaging", "summary": "Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.", "AI": {"tldr": "TransientTrack\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ec6\u80de\u8ffd\u8e2a\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u5177\u6709\u77ac\u6001\u8367\u5149\u4fe1\u53f7\u7684\u591a\u901a\u9053\u663e\u5fae\u955c\u89c6\u9891\u6570\u636e\uff0c\u80fd\u591f\u68c0\u6d4b\u7ec6\u80de\u5206\u88c2\u548c\u6b7b\u4ea1\u4e8b\u4ef6\uff0c\u6784\u5efa\u5b8c\u6574\u7684\u7ec6\u80de\u8f68\u8ff9\u548c\u8c31\u7cfb\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7ec6\u80de\u8ffd\u8e2a\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u6052\u5b9a\u4fe1\u53f7\u7684\u89c6\u9891\uff0c\u65e0\u6cd5\u68c0\u6d4b\u7ec6\u80de\u6b7b\u4ea1\u7b49\u5173\u952e\u4e8b\u4ef6\uff0c\u4e14\u4e0d\u9002\u7528\u4e8e\u968f\u65f6\u95f4\u6ce2\u52a8\u7684\u77ac\u6001\u8367\u5149\u4fe1\u53f7\uff08\u5982\u7ec6\u80de\u663c\u591c\u8282\u5f8b\uff09\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u901a\u9053\u77ac\u6001\u4fe1\u53f7\u5e76\u8bc6\u522b\u5173\u952e\u7ec6\u80de\u4e8b\u4ef6\u7684\u8ffd\u8e2a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u7ec6\u80de\u68c0\u6d4b\u5d4c\u5165\u4e0a\u8fdb\u884c\u5339\u914d\uff0c\u65e0\u9700\u91cf\u5316\u8ffd\u8e2a\u7279\u5f02\u6027\u7279\u5f81\u3002\u6574\u5408Transformer\u7f51\u7edc\u3001\u4f7f\u7528\u6240\u6709\u68c0\u6d4b\u6846\u7684\u591a\u9636\u6bb5\u5339\u914d\uff0c\u4ee5\u53ca\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u63d2\u8865\u7f3a\u5931\u7684\u8f68\u8ff9\u7247\u6bb5\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u6709\u6548\u8ffd\u8e2a\u7ec6\u80de\u5e76\u6355\u6349\u7ec6\u80de\u5206\u88c2\u548c\u6b7b\u4ea1\u4e8b\u4ef6\u3002\u5728\u5316\u7597\u836f\u7269\u7597\u6548\u7684\u5355\u7ec6\u80de\u5206\u6790\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "TransientTrack\u4e3a\u764c\u75c7\u7ec6\u80de\u52a8\u529b\u5b66\u5b9a\u91cf\u7814\u7a76\u63d0\u4f9b\u4e86\u5148\u8fdb\u5de5\u5177\uff0c\u80fd\u591f\u8be6\u7ec6\u8868\u5f81\u6cbb\u7597\u53cd\u5e94\u548c\u8010\u836f\u673a\u5236\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
