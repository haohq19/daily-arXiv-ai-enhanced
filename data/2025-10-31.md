<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 提出了EgoExo-Con基准，用于评估视频LLM在不同视角下的时间理解一致性，并开发了View-GRPO强化学习框架来提升跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 研究视频LLM在不同视角下是否能够保持时间理解的一致性，现有模型在这方面存在明显不足。

Method: 引入EgoExo-Con基准，包含同步的自我中心和他者视角视频对，并提出View-GRPO强化学习框架来增强跨视角一致性。

Result: 现有视频LLM在跨视角一致性方面表现较差，View-GRPO方法相比传统SFT和GRPO能显著提升一致性表现。

Conclusion: 跨视角时间理解一致性是视频LLM的重要挑战，View-GRPO为解决这一问题提供了有效方案。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [2] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出Spiking Patches事件标记化方法，保留事件相机的异步和空间稀疏特性，在保持精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法将事件表示为帧或体素，虽然精度高但破坏了事件相机的异步和空间稀疏特性，需要一种能保留这些独特属性的表示方法。

Method: 设计Spiking Patches标记器，将异步稀疏事件流转换为标记表示，使用GNN、PCN和Transformer在姿态识别和物体检测任务上评估。

Result: 标记化方法推理速度比体素快3.4倍，比帧快10.4倍，精度相当甚至更好（姿态识别提升3.8，物体检测提升1.4）。

Conclusion: 事件标记化是事件视觉的新方向，为保留事件相机特性迈出了重要一步。

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [3] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 提出了LoCoT2V-Bench基准，专门用于评估复杂输入条件下的长视频生成，包含多维度评估框架和新指标，发现现有方法在事件间一致性、细粒度对齐和高级主题遵循方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成评估主要依赖简化提示和低级指标，忽视了与提示的细粒度对齐以及叙事连贯性、主题表达等抽象维度，特别是在处理复杂提示和长视频生成方面存在挑战。

Method: 基于真实视频构建包含场景转换和事件动态的现实复杂提示集，建立多维度评估框架，包括事件级对齐、细粒度时间一致性、内容清晰度和关注叙事流程、情感响应等抽象属性的人类期望实现度(HERD)等新指标。

Result: 对9个代表性长视频生成模型的综合评估显示，现有方法在基本视觉和时间方面表现良好，但在事件间一致性、细粒度对齐和高级主题遵循等方面存在困难。

Conclusion: LoCoT2V-Bench为长格式复杂文本到视频生成提供了全面可靠的评估平台，并指明了未来方法改进的关键方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出PCN-TA方法，通过利用时间相关性减少计算需求，在保持学习性能的同时显著降低预测编码网络的计算开销


<details>
  <summary>Details</summary>
Motivation: 边缘机器人系统需要高效的在线学习算法，传统反向传播不符合生物合理性原则，预测编码框架虽具生物合理性但计算开销大

Method: PCN-TA方法通过跨时间帧保留潜在状态，利用时间相关性来减少计算需求

Result: 在COIL-20数据集上，PCN-TA比反向传播减少10%权重更新，比基线PC网络减少50%推理步骤

Conclusion: PCN-TA为资源受限的机器人系统提供了高效的在线学习方案，适合未来神经形态硬件实现

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [5] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出了一种用于离散时间生存分析的双重专家混合框架，结合特征编码器MoE和风险MoE，在乳腺癌数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 生存分析需要同时建模患者异质性和时间动态，现有方法难以灵活整合这两个方面。

Method: 使用双重MoE框架：特征编码器MoE用于亚组感知表示学习，风险MoE利用患者特征和时间嵌入捕捉时间动态。

Result: 在METABRIC和GBSG乳腺癌数据集上，时间依赖性C指数在测试集上提升高达0.04，在Consurv框架中进一步获得增益。

Conclusion: 双重MoE框架能够有效建模患者异质性和时间动态，显著提升生存分析性能，并能灵活集成到现有深度学习生存分析流程中。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [6] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种超越传统参数优化的机器学习新范式，将模型视为可塑的几何实体，通过优化预定义拓扑流形上的度量张量场来动态塑造模型空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在固定几何空间中搜索最优参数，限制了模型的表达能力。本文旨在通过动态几何结构优化来增强模型表达能力，防止过拟合。

Method: 构建变分框架，损失函数平衡数据保真度和流形内在几何复杂度；使用离散微分几何方法，将连续流形离散化为三角网格，通过边长度参数化度量张量，利用自动微分工具进行高效优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用的深刻类比，为"数据驱动几何"概念提供了优雅物理解释；证明了即使固定拓扑，度量优化也比固定几何模型具有更强的表达能力。

Conclusion: 这项工作为构建能够自主演化几何和拓扑的完全动态"元学习器"奠定了坚实基础，在科学模型发现和鲁棒表示学习等领域具有广阔应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [7] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 使用机器学习技术分析健康索赔数据，识别导致再入院的关键因素，随机森林模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 减少可预防的医院再入院是国家优先事项，用于评估医疗质量和降低成本。

Method: 使用逻辑回归、随机森林和支持向量机分析高维健康索赔数据，采用主成分分析进行降维，基于AUC指标评估模型。

Result: 随机森林模型性能最高，其次是逻辑回归和支持向量机。

Conclusion: 这些模型可用于识别导致再入院的关键因素，帮助确定需要重点关注的患者，从而降低再入院率、减少成本并提高医疗质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [8] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 该论文通过修改经典强化学习方法，使其能够在稀疏、随机和非平稳环境中高效运行，特别是在水下污染云搜索等应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在随机和非平稳环境中表现有限，特别是在奖励稀疏的环境中（如自主水下车辆搜索污染云），需要改进以适应这些复杂条件。

Method: 系统研究多种修改方法，包括分层算法变更、多目标学习，以及集成位置记忆作为外部输出过滤器以防止状态重复访问，采用改进的蒙特卡洛方法。

Result: 改进的蒙特卡洛方法显著优于传统Q学习和两种穷举搜索模式，展示了在复杂环境中适应强化学习的潜力。

Conclusion: 强化学习方法可以有效适应随机、非平稳和奖励稀疏的环境，为复杂应用提供了可行的解决方案。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [9] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: 开发了基于Flow Matching的生成式AI模型FM-Cast，用于高效且准确的平流层环流时空演变的概率预报，在SSW事件预测中表现优异，计算效率远超传统数值天气预报系统。


<details>
  <summary>Details</summary>
Motivation: 平流层突然增温事件是次季节可预报性的关键来源和极端冬季天气的主要驱动因素，但传统数值天气预报系统在物理表示、初始化和计算需求方面存在限制，而数据驱动方法在复杂三维SSW动力学特别是概率预报方面的应用仍不足。

Method: 采用基于Flow Matching的生成式AI模型FM-Cast，开发了平流层环流时空演变的概率预报系统，并在18个主要SSW事件上进行评估。

Result: FM-Cast能够提前20天准确预报10个SSW事件的发生、强度和形态，集合准确率超过50%，性能达到或超过领先的NWP系统，在消费级GPU上仅需2分钟即可完成50个成员、30天的预报。

Conclusion: 建立了一个计算高效的平流层异常概率预报范式，展示了生成式AI在加深对大气-气候动力学物理理解方面的潜力，并通过理想化实验证明SSW可预报性与其基础物理驱动因素密切相关。

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [10] [Aeolus: A Multi-structural Flight Delay Dataset](https://arxiv.org/abs/2510.26616)
*Lin Xu,Xinyun Yuan,Yuxuan Liang,Suwan Yin,Yuankai Wu*

Main category: cs.LG

TL;DR: Aeolus是一个大规模多模态航班延误数据集，包含表格数据、航班链模块和航班网络图三种对齐模态，用于支持航班延误预测和表格数据基础模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有航班延误数据集通常局限于平面表格结构，无法捕捉延误传播的时空动态特性。Aeolus旨在解决这一局限性，为领域特定建模和通用结构化数据研究填补关键空白。

Method: 提供三种对齐模态：1）包含5000多万航班丰富特征的表格数据集；2）建模延误沿连续航班传播的航班链模块；3）编码共享资源的航班网络图。数据集采用时间分割、严格防泄漏设计。

Result: 构建了一个支持回归、分类、时序结构建模和图学习等多种任务的统一基准数据集，提供了基线实验和预处理工具。

Conclusion: Aeolus填补了航班延误预测和表格数据基础模型研究的关键空白，支持跨表格、序列和图模态的统一评估。

Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 提出利用自主AI代理实现FinOps自动化，解决云账单数据格式异构问题，通过模拟端到端行业流程验证代理性能


<details>
  <summary>Details</summary>
Motivation: FinOps从业者面临云账单数据格式异构、分类和指标不统一的挑战，难以快速合成可操作见解并做出时效性决策

Method: 构建FinOps代理系统，模拟从多源数据检索到数据整合分析再到优化建议生成的完整行业流程，使用多种语言模型评估

Result: 代理能够像实际FinOps从业者一样理解、规划和执行任务，在IT基础设施和成本优化用例中表现良好

Conclusion: 自主AI代理是解决FinOps数据异构挑战的有效方法，能够实现自动化成本优化决策

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [12] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance是一个框架，通过将法规文本表示为策略图，运行时上下文表示为上下文图，并对齐两者来提升网络规模合规性评估的效果。


<details>
  <summary>Details</summary>
Motivation: 网络规模的合规性面临实际挑战：每个请求都需要法规评估。法规文本具有交叉引用和规范性，而运行时上下文是非结构化的自然语言，需要将语义信息与法规的结构化规范元素对齐。

Method: 引入GraphCompliance框架，将法规文本表示为策略图（编码规范结构和交叉引用），运行时上下文表示为上下文图（将事件形式化为SAO三元组和实体关系三元组），并对齐两者。这种对齐将法官LLM的推理锚定在结构化信息中。

Result: 在300个GDPR衍生的真实场景实验中，GraphCompliance在五个评估任务上的micro-F1比仅使用LLM和RAG基线高出4.1-7.2个百分点，具有更少的欠预测和过预测，召回率更高，误报率更低。

Conclusion: 消融研究表明每个图组件都有贡献，结构化表示和法官LLM在规范推理中是互补的。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [13] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出将人格视为社会赋予实体的义务捆绑（权利与责任），而非形而上学属性，为AI代理融入社会提供实用框架。


<details>
  <summary>Details</summary>
Motivation: 应对AI代理涌现带来的新型人格多样性挑战，避免关于AI意识或理性的无解争论，解决具体治理问题。

Method: 提出人格义务捆绑解构方法，利用去中心化数字身份技术，分析人格作为问题（设计选择可能利用人类社交启发式）和作为解决方案（确保问责或防止冲突）的双重作用。

Result: 建立了无需解决AI意识争议即可创建实用工具（如可被制裁的AI合同主体）的框架，提供了更灵活的社会整合方式。

Conclusion: 通过拒绝寻求单一本质性人格定义的基础主义追求，本文为AI代理融入社会提供了更实用和灵活的思考方式。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 提出一种基于模糊逻辑和句法依赖解析的方法，对产品评论中的情感进行细粒度分类（非常弱、弱、中等、强、非常强），并基于情感方向和强度对实体进行排名。


<details>
  <summary>Details</summary>
Motivation: 传统的基于词典的情感分析方法没有考虑情感强度，无法区分非常强烈、强烈、中等、非常弱和弱等不同级别的积极或消极情感。

Method: 结合副词、形容词、名词和动词等观点词，使用模糊逻辑算法将观点词分类到不同类别，并通过句法依赖解析找到与目标方面词的关系，计算实体在特定方面的得分。

Result: 该方法能够对产品评论进行细粒度的情感强度分类，并基于情感方向和强度对实体进行排名。

Conclusion: 提出的方法能够更准确地分析评论中的情感强度和方向，为实体排名提供更精细的情感分析结果。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [15] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER方法通过自动生成教科书式课程和基于布鲁姆分类法的QA对，将通用LLM转化为领域专家，在保持通用能力的同时显著提升专业领域表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在经济学、心理学等专业领域表现不佳的问题，这些领域需要深入、原则性的理解。

Method: ACER方法：1) 自动生成教科书式课程大纲；2) 基于布鲁姆分类法创建QA对；3) 使用交错课程安排进行持续预训练，确保内容和认知维度的对齐学习。

Result: 在Llama 3.2模型上测试：微观经济学准确率提升5个百分点；所有目标领域平均提升3个百分点；非目标领域也提升0.7点；ARC和GPQA基准提升超过2个绝对点；通用推理任务性能保持稳定。

Conclusion: ACER提供了一种可扩展且有效的方法来缩小LLM在关键领域的性能差距，同时防止灾难性遗忘并促进跨领域知识迁移。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [16] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个高级数学推理基准测试，包含50道奥林匹克竞赛难度或更高的人类编写问题，旨在评估大型语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学竞赛基准测试对顶级LLMs的评估效果下降，存在性能饱和问题，需要更严格的挑战来推动模型推理能力的进步。

Method: 创建50道经过专家交叉验证、符合IMO难度标准的原创问题，仅要求最终答案而非证明，实现自动评分。

Result: 在26个LLMs上的实验结果显示，最佳模型准确率仅为52.4%，大多数模型低于40%，但显示出测试时计算增加的扩展趋势。

Conclusion: 当前LLMs在数学推理方面仍有很大改进空间，AMO-Bench的发布将促进语言模型推理能力的进一步研究。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration](https://arxiv.org/abs/2510.26536)
*Huajie Tan,Cheng Chi,Xiansheng Chen,Yuheng Ji,Zhongxia Zhao,Xiaoshuai Hao,Yaoxu Lyu,Mingyu Cao,Junkai Zhao,Huaihai Lyu,Enshen Zhou,Ning Chen,Yankai Fu,Cheng Peng,Wei Guo,Dong Liang,Zhuo Chen,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了RoboOS-NeXT框架，通过统一的时空-具身记忆(STEM)实现多机器人系统的终身适应、可扩展协调和鲁棒调度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的个体记忆，无法实现长期学习、扩展到异构团队或从故障中恢复，需要统一的内存表示。

Method: 引入STEM记忆，集成空间场景几何、时间事件历史和具身配置文件到共享表示中，采用大脑-小脑框架，高层大脑模型进行全局规划，低层控制器本地执行。

Result: 在餐厅、超市和家庭等复杂协调任务中，RoboOS-NeXT在异构具身机器人上表现出优越性能。

Conclusion: 该框架有效实现了终身、可扩展和鲁棒的多机器人协作。

Abstract: The proliferation of collaborative robots across diverse tasks and
embodiments presents a central challenge: achieving lifelong adaptability,
scalable coordination, and robust scheduling in multi-agent systems. Existing
approaches, from vision-language-action (VLA) models to hierarchical
frameworks, fall short due to their reliance on limited or dividual-agent
memory. This fundamentally constrains their ability to learn over long
horizons, scale to heterogeneous teams, or recover from failures, highlighting
the need for a unified memory representation. To address these limitations, we
introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,
and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel
Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene
geometry, temporal event history, and embodiment profiles into a shared
representation. This memory-centric design is integrated into a
brain-cerebellum framework, where a high-level brain model performs global
planning by retrieving and updating STEM, while low-level controllers execute
actions locally. This closed loop between cognition, memory, and execution
enables dynamic task allocation, fault-tolerant collaboration, and consistent
state synchronization. We conduct extensive experiments spanning complex
coordination tasks in restaurants, supermarkets, and households. Our results
demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous
embodiments, validating its effectiveness in enabling lifelong, scalable, and
robust multi-robot collaboration. Project website:
https://flagopen.github.io/RoboOS/

</details>


### [18] [Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments](https://arxiv.org/abs/2510.26646)
*Xiaoyi He,Danggui Chen,Zhenshuo Zhang,Zimeng Bai*

Main category: cs.RO

TL;DR: 提出分层路径规划与控制框架，结合高层DQN进行离散子目标选择与底层TD3控制器进行连续驱动，在动态和部分可观测环境中实现更好的路径规划性能。


<details>
  <summary>Details</summary>
Motivation: 解决单一算法在复杂环境中路径规划的局限性，通过分层结构结合离散决策和连续控制，提高规划成功率和样本效率。

Method: 使用高层Deep Q-Network选择行为和子目标，底层Twin Delayed Deep Deterministic Policy Gradient执行平滑速度命令，结合LiDAR安全门和实用奖励机制。

Result: 实验显示相比单一算法基准和基于规则的规划器，具有更高的成功率、更好的样本效率，对未见障碍物配置有更好的泛化能力，控制变化更平滑。

Conclusion: 分层框架有效结合离散和连续控制，在复杂环境中实现鲁棒的路径规划，优于单一算法方法。

Abstract: This paper presents a hierarchical path-planning and control framework that
combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with
a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller
for continuous actuation. The high-level module selects behaviors and
sub-goals; the low-level module executes smooth velocity commands. We design a
practical reward shaping scheme (direction, distance, obstacle avoidance,
action smoothness, collision penalty, time penalty, and progress), together
with a LiDAR-based safety gate that prevents unsafe motions. The system is
implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,
including success rate, collision rate, path efficiency, and re-planning
efficiency, in dynamic and partially observable environments. Experiments show
improved success rate and sample efficiency over single-algorithm baselines
(DQN or TD3 alone) and rule-based planners, with better generalization to
unseen obstacle configurations and reduced abrupt control changes. Code and
evaluation scripts are available at the project repository.

</details>
