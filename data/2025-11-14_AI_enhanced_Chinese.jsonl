{"id": "2511.09723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09723", "abs": "https://arxiv.org/abs/2511.09723", "authors": ["Balachandra Devarangadi Sunil", "Rakshith Venkatesh", "Shantanu Todmal"], "title": "Density Estimation and Crowd Counting", "comment": null, "summary": "This study enhances a crowd density estimation algorithm originally designed for image-based analysis by adapting it for video-based scenarios. The proposed method integrates a denoising probabilistic model that utilizes diffusion processes to generate high-quality crowd density maps. To improve accuracy, narrow Gaussian kernels are employed, and multiple density map outputs are generated. A regression branch is incorporated into the model for precise feature extraction, while a consolidation mechanism combines these maps based on similarity scores to produce a robust final result. An event-driven sampling technique, utilizing the Farneback optical flow algorithm, is introduced to selectively capture frames showing significant crowd movements, reducing computational load and storage by focusing on critical crowd dynamics. Through qualitative and quantitative evaluations, including overlay plots and Mean Absolute Error (MAE), the model demonstrates its ability to effectively capture crowd dynamics in both dense and sparse settings. The efficiency of the sampling method is further assessed, showcasing its capability to decrease frame counts while maintaining essential crowd events. By addressing the temporal challenges unique to video analysis, this work offers a scalable and efficient framework for real-time crowd monitoring in applications such as public safety, disaster response, and event management.", "AI": {"tldr": "\u5c06\u56fe\u50cf\u4eba\u7fa4\u5bc6\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u6269\u5c55\u5230\u89c6\u9891\u5206\u6790\uff0c\u5f15\u5165\u6269\u6563\u6982\u7387\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5bc6\u5ea6\u56fe\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u91c7\u6837\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u573a\u666f\u4e0b\u5747\u80fd\u6709\u6548\u6355\u6349\u4eba\u7fa4\u52a8\u6001\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5206\u6790\u4e2d\u7279\u6709\u7684\u65f6\u95f4\u6311\u6218\uff0c\u4e3a\u516c\u5171\u5b89\u5168\u3001\u707e\u96be\u54cd\u5e94\u548c\u4e8b\u4ef6\u7ba1\u7406\u7b49\u5b9e\u65f6\u4eba\u7fa4\u76d1\u63a7\u5e94\u7528\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "\u96c6\u6210\u53bb\u566a\u6982\u7387\u6a21\u578b\u4f7f\u7528\u6269\u6563\u8fc7\u7a0b\u751f\u6210\u5bc6\u5ea6\u56fe\uff1b\u91c7\u7528\u7a84\u9ad8\u65af\u6838\u751f\u6210\u591a\u4e2a\u5bc6\u5ea6\u56fe\u8f93\u51fa\uff1b\u52a0\u5165\u56de\u5f52\u5206\u652f\u8fdb\u884c\u7cbe\u786e\u7279\u5f81\u63d0\u53d6\uff1b\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u5206\u6570\u5408\u5e76\u5bc6\u5ea6\u56fe\uff1b\u5f15\u5165\u57fa\u4e8eFarneback\u5149\u6d41\u7b97\u6cd5\u7684\u4e8b\u4ef6\u9a71\u52a8\u91c7\u6837\u6280\u672f\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff08\u5305\u62ec\u53e0\u52a0\u56fe\u548c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff09\uff0c\u6a21\u578b\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u573a\u666f\u4e0b\u90fd\u80fd\u6709\u6548\u6355\u6349\u4eba\u7fa4\u52a8\u6001\uff1b\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u5e27\u6570\u540c\u65f6\u4fdd\u6301\u5173\u952e\u4eba\u7fa4\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u89e3\u51b3\u89c6\u9891\u5206\u6790\u7684\u65f6\u95f4\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u4eba\u7fa4\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09578", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.09578", "abs": "https://arxiv.org/abs/2511.09578", "authors": ["Hadi Keramati", "Morteza Sadeghi", "Rajeev K. Jaiman"], "title": "HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization", "comment": null, "summary": "This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f15\u5bfc\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b(DDPM)\u7684\u751f\u6210\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u4ee3\u7406\u68af\u5ea6\u751f\u6210\u6563\u70ed\u5668\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u8868\u9762\u6e29\u5ea6\u4f4e\u4e8e\u9608\u503c\u7684\u540c\u65f6\u6700\u5c0f\u5316\u538b\u964d\u3002", "motivation": "\u4f20\u7edf\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5(\u5982CMA-ES)\u5728\u6563\u70ed\u5668\u8bbe\u8ba1\u4f18\u5316\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u751f\u6210\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8fb9\u754c\u8868\u793a\u6cd5\u8868\u793a\u591a\u7fc5\u7247\u51e0\u4f55\u7ed3\u6784\uff0c\u91c7\u7528\u591a\u4fdd\u771f\u5ea6\u65b9\u6cd5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002\u8bad\u7ec3DDPM\u751f\u6210\u6563\u70ed\u5668\u8bbe\u8ba1\uff0c\u540c\u65f6\u8bad\u7ec3\u4e24\u4e2a\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u538b\u964d\u548c\u8868\u9762\u6e29\u5ea6\uff0c\u5229\u7528\u4ee3\u7406\u68af\u5ea6\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u538b\u964d\u6bd4\u4f20\u7edf\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u4f4e\u8fbe10%\uff0c\u4e14\u5728\u65b0\u7ea6\u675f\u6761\u4ef6\u4e0b\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u7535\u5b50\u51b7\u5374\u57fa\u7840\u751f\u6210\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u70ed\u7ba1\u7406\u8bbe\u8ba1\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.09958", "categories": ["cs.RO", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.09958", "abs": "https://arxiv.org/abs/2511.09958", "authors": ["Xiangyi Wei", "Haotian Zhang", "Xinyi Cao", "Siyu Xie", "Weifeng Ge", "Yang Li", "Changbo Wang"], "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation", "comment": null, "summary": "The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.", "AI": {"tldr": "Audio-VLA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u5229\u7528\u63a5\u89e6\u97f3\u9891\u611f\u77e5\u63a5\u89e6\u4e8b\u4ef6\u548c\u52a8\u6001\u8fc7\u7a0b\u53cd\u9988\uff0c\u514b\u670d\u4e86\u7eaf\u89c6\u89c9VLA\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4efb\u52a1\u5b8c\u6210\u7387(TCR)\u6307\u6807\u6765\u7cfb\u7edf\u8bc4\u4f30\u52a8\u6001\u64cd\u4f5c\u8fc7\u7a0b\u3002", "motivation": "\u7eaf\u89c6\u89c9VLA\u6a21\u578b\u5728\u611f\u77e5\u4ea4\u4e92\u548c\u64cd\u4f5c\u52a8\u6001\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u9700\u8981\u591a\u6a21\u6001\u611f\u77e5\u6765\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684DINOv2\u548cSigLIP\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\uff0cAudioCLIP\u4f5c\u4e3a\u97f3\u9891\u7f16\u7801\u5668\uff0cLlama2\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u548c\u591a\u6a21\u6001\u6295\u5f71\u5c42\u5b9e\u73b0\u8de8\u6a21\u6001\u7406\u89e3\uff0c\u5e76\u5728\u4eff\u771f\u73af\u5883\u4e2d\u6dfb\u52a0\u78b0\u649e\u97f3\u9891\u751f\u6210\u3002", "result": "\u5728LIBERO\u3001RLBench\u548c\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAudio-VLA\u4f18\u4e8e\u7eaf\u89c6\u89c9\u5bf9\u6bd4\u65b9\u6cd5\uff0cTCR\u6307\u6807\u80fd\u6709\u6548\u91cf\u5316\u52a8\u6001\u8fc7\u7a0b\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "Audio-VLA\u901a\u8fc7\u6574\u5408\u97f3\u9891\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0cTCR\u6307\u6807\u4e3a\u52a8\u6001\u64cd\u4f5c\u8fc7\u7a0b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2511.10635", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10635", "abs": "https://arxiv.org/abs/2511.10635", "authors": ["Pascal Strauch", "David M\u00fcller", "Sammy Christen", "Agon Serifi", "Ruben Grandia", "Espen Knoop", "Moritz B\u00e4cher"], "title": "Robot Crash Course: Learning Soft and Stylized Falling", "comment": null, "summary": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u65e0\u5173\u7684\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u53cc\u8db3\u673a\u5668\u4eba\u7684\u53d7\u63a7\u8f6f\u7740\u9646\uff0c\u5e73\u8861\u671f\u671b\u59ff\u6001\u8fbe\u6210\u4e0e\u51b2\u51fb\u6700\u5c0f\u5316\uff0c\u4fdd\u62a4\u5173\u952e\u90e8\u4ef6\u3002", "motivation": "\u53cc\u8db3\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u4ecd\u6709\u8dcc\u5012\u98ce\u9669\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9632\u6b62\u8dcc\u5012\uff0c\u672c\u6587\u4e13\u6ce8\u4e8e\u8dcc\u5012\u73b0\u8c61\u672c\u8eab\uff0c\u65e8\u5728\u51cf\u5c11\u7269\u7406\u635f\u4f24\u5e76\u8ba9\u7528\u6237\u63a7\u5236\u673a\u5668\u4eba\u6700\u7ec8\u59ff\u6001\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u65e0\u5173\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5e73\u8861\u671f\u671b\u59ff\u6001\u8fbe\u6210\u3001\u51b2\u51fb\u6700\u5c0f\u5316\u548c\u5173\u952e\u90e8\u4ef6\u4fdd\u62a4\uff1b\u5f15\u5165\u57fa\u4e8e\u4eff\u771f\u7684\u521d\u59cb\u59ff\u6001\u548c\u6700\u7ec8\u59ff\u6001\u91c7\u6837\u7b56\u7565\uff0c\u4f7f\u7b56\u7565\u5bf9\u5e7f\u6cdb\u521d\u59cb\u8dcc\u5012\u6761\u4ef6\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u53cc\u8db3\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u53d7\u63a7\u7684\u8f6f\u7740\u9646\u3002", "conclusion": "\u5373\u4f7f\u662f\u53cc\u8db3\u673a\u5668\u4eba\u4e5f\u80fd\u5b9e\u73b0\u53d7\u63a7\u7684\u8f6f\u7740\u9646\uff0c\u8fd9\u4e3a\u51cf\u5c11\u673a\u5668\u4eba\u8dcc\u5012\u65f6\u7684\u7269\u7406\u635f\u4f24\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.09754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09754", "abs": "https://arxiv.org/abs/2511.09754", "authors": ["Sarthak Khanna", "Armin Berger", "Muskaan Chopra", "Rafet Sifa"], "title": "History Rhymes: Macro-Contextual Retrieval for Robust Financial Forecasting", "comment": "Accepted in IEEE BigData 2025", "summary": "Financial markets are inherently non-stationary: structural breaks and macroeconomic regime shifts often cause forecasting models to fail when deployed out of distribution (OOD). Conventional multimodal approaches that simply fuse numerical indicators and textual sentiment rarely adapt to such shifts. We introduce macro-contextual retrieval, a retrieval-augmented forecasting framework that grounds each prediction in historically analogous macroeconomic regimes. The method jointly embeds macro indicators (e.g., CPI, unemployment, yield spread, GDP growth) and financial news sentiment in a shared similarity space, enabling causal retrieval of precedent periods during inference without retraining.\n  Trained on seventeen years of S&P 500 data (2007-2023) and evaluated OOD on AAPL (2024) and XOM (2024), the framework consistently narrows the CV to OOD performance gap. Macro-conditioned retrieval achieves the only positive out-of-sample trading outcomes (AAPL: PF=1.18, Sharpe=0.95; XOM: PF=1.16, Sharpe=0.61), while static numeric, text-only, and naive multimodal baselines collapse under regime shifts. Beyond metric gains, retrieved neighbors form interpretable evidence chains that correspond to recognizable macro contexts, such as inflationary or yield-curve inversion phases, supporting causal interpretability and transparency. By operationalizing the principle that \"financial history may not repeat, but it often rhymes,\" this work demonstrates that macro-aware retrieval yields robust, explainable forecasts under distributional change.\n  All datasets, models, and source code are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b8f\u89c2\u7ecf\u6d4e\u60c5\u5883\u68c0\u7d22\u7684\u91d1\u878d\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5386\u53f2\u76f8\u4f3c\u5b8f\u89c2\u7ecf\u6d4e\u9636\u6bb5\u6765\u589e\u5f3a\u9884\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u5177\u6709\u975e\u5e73\u7a33\u6027\uff0c\u7ed3\u6784\u6027\u65ad\u88c2\u548c\u5b8f\u89c2\u7ecf\u6d4e\u5236\u5ea6\u8f6c\u53d8\u4f1a\u5bfc\u81f4\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\u90e8\u7f72\u65f6\u5931\u6548\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u7b80\u5355\u878d\u5408\u6570\u503c\u6307\u6807\u548c\u6587\u672c\u60c5\u611f\uff0c\u96be\u4ee5\u9002\u5e94\u8fd9\u79cd\u5236\u5ea6\u8f6c\u53d8\u3002", "method": "\u5f15\u5165\u5b8f\u89c2\u60c5\u5883\u68c0\u7d22\u6846\u67b6\uff0c\u5728\u5171\u4eab\u76f8\u4f3c\u6027\u7a7a\u95f4\u4e2d\u8054\u5408\u5d4c\u5165\u5b8f\u89c2\u6307\u6807\uff08CPI\u3001\u5931\u4e1a\u7387\u3001\u6536\u76ca\u7387\u5229\u5dee\u3001GDP\u589e\u957f\u7b49\uff09\u548c\u91d1\u878d\u65b0\u95fb\u60c5\u611f\uff0c\u5b9e\u73b0\u63a8\u7406\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u56e0\u679c\u68c0\u7d22\u5386\u53f2\u76f8\u4f3c\u65f6\u671f\u3002", "result": "\u572817\u5e74S&P 500\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728AAPL\u548cXOM\u76842024\u5e74\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86CV\u5230OOD\u6027\u80fd\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u552f\u4e00\u6b63\u5411\u7684\u6837\u672c\u5916\u4ea4\u6613\u7ed3\u679c\uff08AAPL\uff1aPF=1.18\uff0c\u590f\u666e\u6bd4\u7387=0.95\uff1bXOM\uff1aPF=1.16\uff0c\u590f\u666e\u6bd4\u7387=0.61\uff09\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u65bd\"\u91d1\u878d\u5386\u53f2\u4e0d\u4f1a\u91cd\u6f14\u4f46\u4f1a\u62bc\u97f5\"\u7684\u539f\u5219\uff0c\u8bc1\u660e\u57fa\u4e8e\u5b8f\u89c2\u610f\u8bc6\u7684\u68c0\u7d22\u80fd\u591f\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u4ea7\u751f\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u5f62\u6210\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u94fe\uff0c\u5bf9\u5e94\u53ef\u8bc6\u522b\u7684\u5b8f\u89c2\u60c5\u5883\u3002"}}
{"id": "2511.09773", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.09773", "abs": "https://arxiv.org/abs/2511.09773", "authors": ["Mahdi Samaee", "Mehran Yazdi", "Daniel Massicotte"], "title": "NeuroLingua: A Language-Inspired Hierarchical Framework for Multimodal Sleep Stage Classification Using EEG and EOG", "comment": null, "summary": "Automated sleep stage classification from polysomnography remains limited by the lack of expressive temporal hierarchies, challenges in multimodal EEG and EOG fusion, and the limited interpretability of deep learning models. We propose NeuroLingua, a language-inspired framework that conceptualizes sleep as a structured physiological language. Each 30-second epoch is decomposed into overlapping 3-second subwindows (\"tokens\") using a CNN-based tokenizer, enabling hierarchical temporal modeling through dual-level Transformers: intra-segment encoding of local dependencies and inter-segment integration across seven consecutive epochs (3.5 minutes) for extended context. Modality-specific embeddings from EEG and EOG channels are fused via a Graph Convolutional Network, facilitating robust multimodal integration. NeuroLingua is evaluated on the Sleep-EDF Expanded and ISRUC-Sleep datasets, achieving state-of-the-art results on Sleep-EDF (85.3% accuracy, 0.800 macro F1, and 0.796 Cohen's kappa) and competitive performance on ISRUC (81.9% accuracy, 0.802 macro F1, and 0.755 kappa), matching or exceeding published baselines in overall and per-class metrics. The architecture's attention mechanisms enhance the detection of clinically relevant sleep microevents, providing a principled foundation for future interpretability, explainability, and causal inference in sleep research. By framing sleep as a compositional language, NeuroLingua unifies hierarchical sequence modeling and multimodal fusion, advancing automated sleep staging toward more transparent and clinically meaningful applications.", "AI": {"tldr": "NeuroLingua\u662f\u4e00\u4e2a\u53d7\u8bed\u8a00\u542f\u53d1\u7684\u7761\u7720\u5206\u671f\u6846\u67b6\uff0c\u5c06\u7761\u7720\u89c6\u4e3a\u7ed3\u6784\u5316\u751f\u7406\u8bed\u8a00\uff0c\u901a\u8fc7\u53cc\u7ea7Transformer\u548cGCN\u591a\u6a21\u6001\u878d\u5408\uff0c\u5728Sleep-EDF\u548cISRUC\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u7761\u7720\u5206\u671f\u4e2d\u7f3a\u4e4f\u8868\u8fbe\u6027\u65f6\u95f4\u5c42\u6b21\u7ed3\u6784\u3001\u591a\u6a21\u6001EEG\u548cEOG\u878d\u5408\u6311\u6218\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5c0630\u79d2epoch\u5206\u89e3\u4e3a3\u79d2\u5b50\u7a97\u53e3\u4f5c\u4e3a\"token\"\uff0c\u4f7f\u7528CNN\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u53cc\u7ea7Transformer\u8fdb\u884c\u5c42\u6b21\u65f6\u95f4\u5efa\u6a21\uff08\u5c40\u90e8\u4f9d\u8d56\u548c\u8de87\u4e2aepoch\u7684\u6269\u5c55\u4e0a\u4e0b\u6587\uff09\uff0c\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u878d\u5408EEG\u548cEOG\u6a21\u6001\u5d4c\u5165\u3002", "result": "\u5728Sleep-EDF\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.3%\u51c6\u786e\u7387\u30010.800\u5b8f\u89c2F1\u548c0.796 Cohen's kappa\uff1b\u5728ISRUC\u6570\u636e\u96c6\u4e0a\u8fbe\u523081.9%\u51c6\u786e\u7387\u30010.802\u5b8f\u89c2F1\u548c0.755 kappa\uff0c\u5339\u914d\u6216\u8d85\u8fc7\u5df2\u53d1\u8868\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7761\u7720\u6846\u67b6\u5316\u4e3a\u7ec4\u5408\u8bed\u8a00\uff0cNeuroLingua\u7edf\u4e00\u4e86\u5c42\u6b21\u5e8f\u5217\u5efa\u6a21\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u4e3a\u7761\u7720\u7814\u7a76\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2511.10120", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10120", "abs": "https://arxiv.org/abs/2511.10120", "authors": ["Philipp Seeberger", "Steffen Freisinger", "Tobias Bocklet", "Korbinian Riedhammer"], "title": "Generalizing to Unseen Disaster Events: A Causal View", "comment": "Accepted to Findings of AACL 2025", "summary": "Due to the rapid growth of social media platforms, these tools have become essential for monitoring information during ongoing disaster events. However, extracting valuable insights requires real-time processing of vast amounts of data. A major challenge in existing systems is their exposure to event-related biases, which negatively affects their ability to generalize to emerging events. While recent advancements in debiasing and causal learning offer promising solutions, they remain underexplored in the disaster event domain. In this work, we approach bias mitigation through a causal lens and propose a method to reduce event- and domain-related biases, enhancing generalization to future events. Our approach outperforms multiple baselines by up to +1.9% F1 and significantly improves a PLM-based classifier across three disaster classification tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u51cf\u5c11\u4e8b\u4ef6\u548c\u9886\u57df\u76f8\u5173\u504f\u5dee\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u5bf9\u707e\u5bb3\u4e8b\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e09\u4e2a\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5df2\u6210\u4e3a\u707e\u5bb3\u4e8b\u4ef6\u4fe1\u606f\u76d1\u6d4b\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u4e8b\u4ef6\u76f8\u5173\u504f\u5dee\u95ee\u9898\uff0c\u5f71\u54cd\u5bf9\u65b0\u5174\u4e8b\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u63d0\u51fa\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e8b\u4ef6\u548c\u9886\u57df\u76f8\u5173\u504f\u5dee\u3002", "result": "\u65b9\u6cd5\u5728\u4e09\u4e2a\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u6bd4\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa\u6700\u591a+1.9% F1\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5206\u7c7b\u5668\u6027\u80fd\u3002", "conclusion": "\u56e0\u679c\u5b66\u4e60\u5728\u707e\u5bb3\u4e8b\u4ef6\u9886\u57df\u7684\u504f\u5dee\u7f13\u89e3\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5bf9\u65b0\u5174\u4e8b\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.10266", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10266", "abs": "https://arxiv.org/abs/2511.10266", "authors": ["Rajab Aghamov", "Christel Baier", "Joel Ouaknine", "Jakob Piribauer", "Mihir Vahanwala", "Isa Vialard"], "title": "Temporal Properties of Conditional Independence in Dynamic Bayesian Networks", "comment": null, "summary": "Dynamic Bayesian networks (DBNs) are compact graphical representations used to model probabilistic systems where interdependent random variables and their distributions evolve over time. In this paper, we study the verification of the evolution of conditional-independence (CI) propositions against temporal logic specifications. To this end, we consider two specification formalisms over CI propositions: linear temporal logic (LTL), and non-deterministic B\u00fcchi automata (NBAs). This problem has two variants. Stochastic CI properties take the given concrete probability distributions into account, while structural CI properties are viewed purely in terms of the graphical structure of the DBN. We show that deciding if a stochastic CI proposition eventually holds is at least as hard as the Skolem problem for linear recurrence sequences, a long-standing open problem in number theory. On the other hand, we show that verifying the evolution of structural CI propositions against LTL and NBA specifications is in PSPACE, and is NP- and coNP-hard. We also identify natural restrictions on the graphical structure of DBNs that make the verification of structural CI properties tractable.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u4e2d\u6761\u4ef6\u72ec\u7acb\u6027\u547d\u9898\u6f14\u5316\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u9488\u5bf9\u968f\u673a\u6027\u548c\u7ed3\u6784\u6027\u4e24\u79cdCI\u5c5e\u6027\uff0c\u5206\u522b\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u6027\u548c\u53ef\u5904\u7406\u6027\u3002", "motivation": "\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u662f\u5efa\u6a21\u6982\u7387\u7cfb\u7edf\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u5bf9\u5176\u6761\u4ef6\u72ec\u7acb\u6027\u547d\u9898\u968f\u65f6\u95f4\u7684\u6f14\u5316\u9a8c\u8bc1\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u6765\u786e\u4fdd\u7cfb\u7edf\u6ee1\u8db3\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u548c\u975e\u786e\u5b9a\u6027B\u00fcchi\u81ea\u52a8\u673a\u4f5c\u4e3a\u89c4\u8303\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5c06CI\u5c5e\u6027\u9a8c\u8bc1\u5206\u4e3a\u968f\u673a\u6027\u548c\u7ed3\u6784\u6027\u4e24\u79cd\u53d8\u4f53\uff0c\u5206\u522b\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u968f\u673a\u6027CI\u5c5e\u6027\u9a8c\u8bc1\u81f3\u5c11\u4e0e\u7ebf\u6027\u9012\u63a8\u5e8f\u5217\u7684Skolem\u95ee\u9898\u4e00\u6837\u56f0\u96be\uff1b\u7ed3\u6784\u6027CI\u5c5e\u6027\u9a8c\u8bc1\u5c5e\u4e8ePSPACE\uff0c\u4e14\u662fNP-\u548ccoNP-\u56f0\u96be\u7684\uff1b\u8bc6\u522b\u4e86\u4f7f\u7ed3\u6784\u6027CI\u5c5e\u6027\u9a8c\u8bc1\u53ef\u5904\u7406\u7684\u56fe\u7ed3\u6784\u9650\u5236\u3002", "conclusion": "\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u4e2dCI\u547d\u9898\u7684\u9a8c\u8bc1\u5177\u6709\u4e0d\u540c\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u7ed3\u6784\u6027\u5c5e\u6027\u5728\u7279\u5b9a\u56fe\u7ed3\u6784\u9650\u5236\u4e0b\u53ef\u9ad8\u6548\u9a8c\u8bc1\uff0c\u800c\u968f\u673a\u6027\u5c5e\u6027\u9a8c\u8bc1\u9762\u4e34\u91cd\u5927\u7406\u8bba\u6311\u6218\u3002"}}
{"id": "2511.09871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09871", "abs": "https://arxiv.org/abs/2511.09871", "authors": ["Hyung-Jun Moon", "Sung-Bae Cho"], "title": "Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning", "comment": "To appear in AAAI 2026 (The 40th AAAI Conference on Artificial Intelligence)", "summary": "Continual learning methods used to force neural networks to process sequential tasks in isolation, preventing them from leveraging useful inter-task relationships and causing them to repeatedly relearn similar features or overly differentiate them. To address this problem, we propose a fully differentiable, exemplar-free expandable method composed of two complementary memories: One learns common features that can be used across all tasks, and the other combines the shared features to learn discriminative characteristics unique to each sample. Both memories are differentiable so that the network can autonomously learn latent representations for each sample. For each task, the memory adjustment module adaptively prunes critical slots and minimally expands capacity to accommodate new concepts, and orthogonal regularization enforces geometric separation between preserved and newly learned memory components to prevent interference. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that the proposed method outperforms 14 state-of-the-art methods for class-incremental learning, achieving final accuracies of 55.13\\%, 37.24\\%, and 30.11\\%, respectively. Additional analysis confirms that, through effective integration and utilization of knowledge, the proposed method can increase average performance across sequential tasks, and it produces feature extraction results closest to the upper bound, thus establishing a new milestone in continual learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u3001\u65e0\u9700\u793a\u4f8b\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e92\u8865\u8bb0\u5fc6\u6a21\u5757\uff1a\u4e00\u4e2a\u5b66\u4e60\u8de8\u4efb\u52a1\u901a\u7528\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u7ec4\u5408\u5171\u4eab\u7279\u5f81\u5b66\u4e60\u6837\u672c\u7279\u6709\u7279\u5f81\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u4fee\u526a\u548c\u6700\u5c0f\u5316\u6269\u5c55\u6765\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u73b0\u6709\u65b9\u6cd5\u5f3a\u5236\u795e\u7ecf\u7f51\u7edc\u5b64\u7acb\u5904\u7406\u987a\u5e8f\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u907f\u514d\u91cd\u590d\u5b66\u4e60\u76f8\u4f3c\u7279\u5f81\u6216\u8fc7\u5ea6\u533a\u5206\u7279\u5f81\uff0c\u5145\u5206\u5229\u7528\u4efb\u52a1\u95f4\u6709\u7528\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u53ef\u5fae\u8bb0\u5fc6\u6a21\u5757\uff1a\u5171\u4eab\u7279\u5f81\u8bb0\u5fc6\u548c\u6837\u672c\u7279\u6709\u7279\u5f81\u8bb0\u5fc6\u3002\u901a\u8fc7\u8bb0\u5fc6\u8c03\u6574\u6a21\u5757\u81ea\u9002\u5e94\u4fee\u526a\u5173\u952e\u69fd\u4f4d\u3001\u6700\u5c0f\u5316\u6269\u5c55\u5bb9\u91cf\uff0c\u5e76\u4f7f\u7528\u6b63\u4ea4\u6b63\u5219\u5316\u4fdd\u6301\u51e0\u4f55\u5206\u79bb\u4ee5\u9632\u6b62\u5e72\u6270\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTiny-ImageNet\u4e0a\u4f18\u4e8e14\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5206\u522b\u8fbe\u523055.13%\u300137.24%\u548c30.11%\u7684\u6700\u7ec8\u51c6\u786e\u7387\u3002\u7279\u5f81\u63d0\u53d6\u7ed3\u679c\u6700\u63a5\u8fd1\u4e0a\u9650\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u6574\u5408\u548c\u5229\u7528\u77e5\u8bc6\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u987a\u5e8f\u4efb\u52a1\u7684\u5e73\u5747\u6027\u80fd\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u8bbe\u7acb\u4e86\u65b0\u7684\u91cc\u7a0b\u7891\u3002"}}
{"id": "2511.10281", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10281", "abs": "https://arxiv.org/abs/2511.10281", "authors": ["Jing He", "Han Zhang", "Yuanhui Xiao", "Wei Guo", "Shaowen Yao", "Renyang Liu"], "title": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection", "comment": "Accepted by AAAI 2026", "summary": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.", "AI": {"tldr": "FactGuard\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e8b\u4ef6\u4e2d\u5fc3\u5185\u5bb9\u6765\u63d0\u5347\u5047\u65b0\u95fb\u68c0\u6d4b\u6027\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u7528\u6027\u673a\u5236\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u89e3\u51b3\u98ce\u683c\u654f\u611f\u6027\u548cLLM\u53ef\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u653b\u51fb\u8005\u6a21\u4eff\u771f\u5b9e\u65b0\u95fb\u5199\u4f5c\u98ce\u683c\uff0c\u57fa\u4e8e\u5199\u4f5c\u98ce\u683c\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u6548\u679c\u9010\u6e10\u4e0b\u964d\uff0c\u800c\u73b0\u6709LLM\u65b9\u6cd5\u5b58\u5728\u529f\u80fd\u63a2\u7d22\u6d45\u3001\u53ef\u7528\u6027\u6a21\u7cca\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faFactGuard\u6846\u67b6\uff1a1\uff09\u5229\u7528LLM\u63d0\u53d6\u4e8b\u4ef6\u4e2d\u5fc3\u5185\u5bb9\u51cf\u5c11\u5199\u4f5c\u98ce\u683c\u5f71\u54cd\uff1b2\uff09\u5f15\u5165\u52a8\u6001\u53ef\u7528\u6027\u673a\u5236\u8bc6\u522b\u77db\u76fe\u6848\u4f8b\uff1b3\uff09\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5f97\u5230FactGuard-D\u7528\u4e8e\u51b7\u542f\u52a8\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u683c\u654f\u611f\u6027\u548cLLM\u53ef\u7528\u6027\u6311\u6218\u3002", "conclusion": "FactGuard\u6846\u67b6\u6210\u529f\u5229\u7528LLM\u63d0\u5347\u5047\u65b0\u95fb\u68c0\u6d4b\u6027\u80fd\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5185\u5bb9\u63d0\u53d6\u548c\u52a8\u6001\u53ef\u7528\u6027\u673a\u5236\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u786e\u4fdd\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.09917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09917", "abs": "https://arxiv.org/abs/2511.09917", "authors": ["Jiazhen Chen", "Xiuqin Liang", "Sichao Fu", "Zheng Ma", "Weihua Ou"], "title": "Towards Multiple Missing Values-resistant Unsupervised Graph Anomaly Detection", "comment": "Accepted by 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Unsupervised graph anomaly detection (GAD) has received increasing attention in recent years, which aims to identify data anomalous patterns utilizing only unlabeled node information from graph-structured data. However, prevailing unsupervised GAD methods typically presuppose complete node attributes and structure information, a condition hardly satisfied in real-world scenarios owing to privacy, collection errors or dynamic node arrivals. Existing standard imputation schemes risk \"repairing\" rare anomalous nodes so that they appear normal, thereby introducing imputation bias into the detection process. In addition, when both node attributes and edges are missing simultaneously, estimation errors in one view can contaminate the other, causing cross-view interference that further undermines the detection performance. To overcome these challenges, we propose M$^2$V-UGAD, a multiple missing values-resistant unsupervised GAD framework on incomplete graphs. Specifically, a dual-pathway encoder is first proposed to independently reconstruct missing node attributes and graph structure, thereby preventing errors in one view from propagating to the other. The two pathways are then fused and regularized in a joint latent space so that normals occupy a compact inner manifold while anomalies reside on an outer shell. Lastly, to mitigate imputation bias, we sample latent codes just outside the normal region and decode them into realistic node features and subgraphs, providing hard negative examples that sharpen the decision boundary. Experiments on seven public benchmarks demonstrate that M$^2$V-UGAD consistently outperforms existing unsupervised GAD methods across varying missing rates.", "AI": {"tldr": "\u63d0\u51faM\u00b2V-UGAD\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u8282\u70b9\u5c5e\u6027\u548c\u7ed3\u6784\u540c\u65f6\u7f3a\u5931\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u7f16\u7801\u5668\u9632\u6b62\u8de8\u89c6\u56fe\u5e72\u6270\uff0c\u5e76\u5229\u7528\u8d1f\u6837\u672c\u91c7\u6837\u7f13\u89e3\u63d2\u8865\u504f\u5dee\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u5e38\u5b58\u5728\u8282\u70b9\u5c5e\u6027\u548c\u7ed3\u6784\u4fe1\u606f\u540c\u65f6\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u63d2\u8865\u65b9\u6cd5\u4f1a\u4fee\u590d\u5f02\u5e38\u8282\u70b9\u4f7f\u5176\u770b\u8d77\u6765\u6b63\u5e38\uff0c\u5bfc\u81f4\u68c0\u6d4b\u504f\u5dee\uff0c\u4e14\u7f3a\u5931\u89c6\u56fe\u95f4\u7684\u9519\u8bef\u4f1a\u76f8\u4e92\u4f20\u64ad\u3002", "method": "\u4f7f\u7528\u53cc\u8def\u5f84\u7f16\u7801\u5668\u72ec\u7acb\u91cd\u6784\u7f3a\u5931\u7684\u8282\u70b9\u5c5e\u6027\u548c\u56fe\u7ed3\u6784\uff0c\u5728\u8054\u5408\u6f5c\u5728\u7a7a\u95f4\u4e2d\u878d\u5408\u548c\u6b63\u5219\u5316\uff0c\u4f7f\u6b63\u5e38\u8282\u70b9\u5360\u636e\u7d27\u51d1\u5185\u6d41\u5f62\uff0c\u5f02\u5e38\u8282\u70b9\u4f4d\u4e8e\u5916\u8868\u9762\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u6f5c\u5728\u7a7a\u95f4\u5916\u7684\u7f16\u7801\u751f\u6210\u786c\u8d1f\u6837\u672c\u6765\u9510\u5316\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM\u00b2V-UGAD\u5728\u4e0d\u540c\u7f3a\u5931\u7387\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "M\u00b2V-UGAD\u80fd\u6709\u6548\u5904\u7406\u56fe\u6570\u636e\u4e2d\u591a\u89c6\u56fe\u7f3a\u5931\u95ee\u9898\uff0c\u9632\u6b62\u8de8\u89c6\u56fe\u5e72\u6270\uff0c\u7f13\u89e3\u63d2\u8865\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.09947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09947", "abs": "https://arxiv.org/abs/2511.09947", "authors": ["Sha Zhao", "Mingyi Peng", "Haiteng Jiang", "Tao Li", "Shijian Li", "Gang Pan"], "title": "EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models", "comment": null, "summary": "Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.", "AI": {"tldr": "EEGAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u8111\u7535\u56fe\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u8c03\u5ea6\u591a\u79cd\u5de5\u5177\u81ea\u52a8\u5b8c\u6210\u591a\u4efb\u52a1EEG\u5206\u6790\uff0c\u5305\u62ec\u57fa\u672c\u4fe1\u606f\u611f\u77e5\u3001\u65f6\u7a7a\u63a2\u7d22\u3001\u4e8b\u4ef6\u68c0\u6d4b\u3001\u7528\u6237\u4ea4\u4e92\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u73b0\u6709EEG\u6a21\u578b\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u4efb\u52a1\u548c\u8fde\u7eed\u63a8\u7406\u7684\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8c03\u5ea6\u548c\u89c4\u5212\u591a\u79cd\u5de5\u5177\uff0c\u8bbe\u8ba1\u5305\u542bEEG\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u3001\u4e8b\u4ef6\u68c0\u6d4b\u7b49\u529f\u80fd\u7684\u5de5\u5177\u7bb1\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cEEGAgent\u652f\u6301\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684EEG\u5206\u6790\uff0c\u5177\u6709\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "EEGAgent\u6846\u67b6\u5c55\u793a\u4e86\u5229\u7528LLM\u5b9e\u73b0\u901a\u7528EEG\u5206\u6790\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8111\u7535\u56fe\u7684\u4e34\u5e8a\u8bca\u65ad\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.10459", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.10459", "abs": "https://arxiv.org/abs/2511.10459", "authors": ["Zihan Gao", "Yifei Xu", "Jacob Thebault-Spieker"], "title": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning", "comment": null, "summary": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.", "AI": {"tldr": "LocalBench\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u7f8e\u56fd\u53bf\u7ea7\u672c\u5730\u77e5\u8bc6\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b14,782\u4e2a\u9a8c\u8bc1\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6526\u4e2a\u53bf\u3002\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u672c\u5730\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0c\u6700\u4f73\u6a21\u578b\u5728\u53d9\u4e8b\u7c7b\u95ee\u9898\u51c6\u786e\u7387\u4ec556.8%\uff0c\u6570\u503c\u63a8\u7406\u4f4e\u4e8e15.5%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u8d85\u672c\u5730\u77e5\u8bc6\u7684\u590d\u6742\u6027\uff0c\u800c\u73b0\u5b9e\u5e94\u7528\u5982\u516c\u6c11\u5e73\u53f0\u548c\u793e\u533a\u65b0\u95fb\u9700\u8981AI\u7cfb\u7edf\u80fd\u591f\u63a8\u7406\u90bb\u91cc\u7279\u5b9a\u52a8\u6001\u3001\u6587\u5316\u53d9\u4e8b\u548c\u672c\u5730\u6cbb\u7406\u3002", "method": "\u57fa\u4e8e\u672c\u5730\u6027\u6982\u5ff5\u6846\u67b6\u6784\u5efaLocalBench\u57fa\u51c6\uff0c\u6574\u5408\u4eba\u53e3\u666e\u67e5\u6570\u636e\u3001\u672c\u5730subreddit\u8ba8\u8bba\u548c\u533a\u57df\u65b0\u95fb\uff0c\u6db5\u76d6\u7269\u7406\u3001\u8ba4\u77e5\u548c\u5173\u7cfb\u7ef4\u5ea6\u3002\u8bc4\u4f3013\u4e2a\u6700\u5148\u8fdbLLM\u5728\u95ed\u5377\u548c\u7f51\u7edc\u589e\u5f3a\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4e25\u91cd\u4e0d\u8db3\uff1a\u6700\u4f73\u6a21\u578b\u53d9\u4e8b\u95ee\u9898\u51c6\u786e\u738756.8%\uff0c\u6570\u503c\u63a8\u7406\u4f4e\u4e8e15.5%\u3002\u6a21\u578b\u89c4\u6a21\u548c\u7f51\u7edc\u589e\u5f3a\u4e0d\u4fdd\u8bc1\u6027\u80fd\u63d0\u5347\uff0c\u641c\u7d22\u4f7fGemini\u51c6\u786e\u7387\u63d0\u534713.6%\uff0c\u4f46\u964d\u4f4eGPT\u7cfb\u521711.4%\u3002", "conclusion": "\u8feb\u5207\u9700\u8981\u80fd\u591f\u652f\u6301\u516c\u5e73\u3001\u4f4d\u7f6e\u611f\u77e5AI\u7cfb\u7edf\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u53c2\u4e0e\u4e0d\u540c\u5730\u7406\u548c\u6587\u5316\u80cc\u666f\u4e0b\u672c\u5730\u793e\u533a\u7684\u591a\u6837\u5316\u3001\u7ec6\u7c92\u5ea6\u73b0\u5b9e\u3002"}}
{"id": "2511.10134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10134", "abs": "https://arxiv.org/abs/2511.10134", "authors": ["Mingda Jia", "Weiliang Meng", "Zenghuang Fu", "Yiheng Li", "Qi Zeng", "Yifan Zhang", "Ju Xin", "Rongtao Xu", "Jiguang Zhang", "Xiaopeng Zhang"], "title": "Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction", "comment": "Accepted to AAAI 2026", "summary": "Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.", "AI": {"tldr": "\u63d0\u51faCACMI\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5e27\u805a\u5408\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7279\u5f81\u589e\u5f3a\uff0c\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u5efa\u6a21\uff0c\u4f7f\u7528\u5e27\u7ea7\u6216\u788e\u7247\u5316\u89c6\u9891\u7279\u5f81\uff0c\u65e0\u6cd5\u6355\u6349\u4e8b\u4ef6\u5e8f\u5217\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e2d\u7684\u5168\u9762\u8bed\u4e49", "method": "CACMI\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8de8\u6a21\u6001\u5e27\u805a\u5408\u901a\u8fc7\u8de8\u6a21\u6001\u68c0\u7d22\u63d0\u53d6\u65f6\u95f4\u8fde\u8d2f\u7684\u4e8b\u4ef6\u5bf9\u9f50\u6587\u672c\u7279\u5f81\uff1b\u4e0a\u4e0b\u6587\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u5229\u7528\u67e5\u8be2\u5f15\u5bfc\u6ce8\u610f\u529b\u6574\u5408\u89c6\u89c9\u52a8\u6001\u4e0e\u4f2a\u4e8b\u4ef6\u8bed\u4e49", "result": "\u5728ActivityNet Captions\u548cYouCook2\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCACMI\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "CACMI\u901a\u8fc7\u663e\u5f0f\u7684\u65f6\u95f4-\u8bed\u4e49\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u7684\u6027\u80fd"}}
{"id": "2511.10593", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10593", "abs": "https://arxiv.org/abs/2511.10593", "authors": ["Rados\u0142aw Miernik", "Marek Szyku\u0142a", "Jakub Kowalski", "Jakub Cie\u015bluk", "\u0141ukasz Galas", "Wojciech Pawlik"], "title": "Regular Games -- an Automata-Based General Game Playing Language", "comment": "Full version of AAAI 2026 paper", "summary": "We propose a new General Game Playing (GGP) system called Regular Games (RG). The main goal of RG is to be both computationally efficient and convenient for game design. The system consists of several languages. The core component is a low-level language that defines the rules by a finite automaton. It is minimal with only a few mechanisms, which makes it easy for automatic processing (by agents, analysis, optimization, etc.). The language is universal for the class of all finite turn-based games with imperfect information. Higher-level languages are introduced for game design (by humans or Procedural Content Generation), which are eventually translated to a low-level language. RG generates faster forward models than the current state of the art, beating other GGP systems (Regular Boardgames, Ludii) in terms of efficiency. Additionally, RG's ecosystem includes an editor with LSP, automaton visualization, benchmarking tools, and a debugger of game description transformations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRegular Games (RG)\u7684\u65b0\u901a\u7528\u6e38\u620f\u535a\u5f08\u7cfb\u7edf\uff0c\u65e8\u5728\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u548c\u6e38\u620f\u8bbe\u8ba1\u4fbf\u5229\u6027\u7684\u5e73\u8861\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u6709\u9650\u81ea\u52a8\u673a\u5b9a\u4e49\u6e38\u620f\u89c4\u5219\uff0c\u5177\u6709\u901a\u7528\u6027\u3001\u9ad8\u6548\u6027\u548c\u5b8c\u6574\u7684\u5f00\u53d1\u5de5\u5177\u751f\u6001\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u6e38\u620f\u535a\u5f08\u7cfb\u7edf\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6e38\u620f\u8bbe\u8ba1\u4fbf\u5229\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cRG\u7cfb\u7edf\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\uff0c\u63d0\u4f9b\u65e2\u9ad8\u6548\u53c8\u6613\u4e8e\u4f7f\u7528\u7684\u6e38\u620f\u5f00\u53d1\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8bed\u8a00\u8bbe\u8ba1\uff1a\u6838\u5fc3\u662f\u4f4e\u7ea7\u7684\u6709\u9650\u81ea\u52a8\u673a\u8bed\u8a00\u5b9a\u4e49\u6e38\u620f\u89c4\u5219\uff0c\u4e0a\u5c42\u63d0\u4f9b\u9762\u5411\u6e38\u620f\u8bbe\u8ba1\u7684\u9ad8\u7ea7\u8bed\u8a00\u3002\u7cfb\u7edf\u652f\u6301\u6e38\u620f\u63cf\u8ff0\u7684\u81ea\u52a8\u8f6c\u6362\u548c\u4f18\u5316\u3002", "result": "RG\u7cfb\u7edf\u5728\u751f\u6210\u524d\u5411\u6a21\u578b\u65b9\u9762\u6bd4\u73b0\u6709\u7cfb\u7edf\uff08\u5982Regular Boardgames\u3001Ludii\uff09\u66f4\u5feb\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5f00\u53d1\u5de5\u5177\u751f\u6001\u3002", "conclusion": "RG\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6e38\u620f\u8bbe\u8ba1\u4fbf\u5229\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u901a\u7528\u6e38\u620f\u535a\u5f08\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2511.10552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10552", "abs": "https://arxiv.org/abs/2511.10552", "authors": ["Yongxin Shi", "Jiapeng Wang", "Zeyu Shan", "Dezhi Peng", "Zening Lin", "Lianwen Jin"], "title": "URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.", "AI": {"tldr": "URaG\u662f\u4e00\u4e2a\u7edf\u4e00\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u65e9\u671f\u5c42\u8fdb\u884c\u8bc1\u636e\u9009\u62e9\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u950044-56%\uff0c\u540c\u65f6\u63d0\u5347\u957f\u6587\u6863\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u957f\u6587\u6863\u7406\u89e3\u4e0a\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u65e0\u5173\u5185\u5bb9\u7684\u4fe1\u606f\u5e72\u6270\uff0c\u4ee5\u53caTransformer\u67b6\u6784\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u7ec6\u8282\u7cbe\u5ea6\uff0c\u8981\u4e48\u589e\u52a0\u7cfb\u7edf\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51faURaG\u6846\u67b6\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u68c0\u7d22\u6a21\u5757\uff0c\u5c06\u65e9\u671fTransformer\u5c42\u8f6c\u6362\u4e3a\u9ad8\u6548\u8bc1\u636e\u9009\u62e9\u5668\uff0c\u8bc6\u522b\u5e76\u4fdd\u7559\u76f8\u5173\u9875\u9762\uff0c\u8ba9\u6df1\u5c42\u4e13\u6ce8\u4e8e\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eURaG\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1144-56%\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "URaG\u8bc1\u660e\u4e86MLLMs\u56fa\u6709\u7684\u8bc1\u636e\u5b9a\u4f4d\u80fd\u529b\u53ef\u4ee5\u663e\u5f0f\u7528\u4e8e\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u68c0\u7d22\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u6587\u6863\u7406\u89e3\u3002"}}
{"id": "2511.10054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10054", "abs": "https://arxiv.org/abs/2511.10054", "authors": ["Yun Wang", "Lingyun Yang", "Senhao Yu", "Yixiao Wang", "Ruixing Li", "Zhixiang Wei", "James Yen", "Zhengwei Qi"], "title": "BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures scale language models by activating only a subset of specialized expert networks for each input token, thereby reducing the number of floating-point operations. However, the growing size of modern MoE models causes their full parameter sets to exceed GPU memory capacity; for example, Mixtral-8x7B has 45 billion parameters and requires 87 GB of memory even though only 14 billion parameters are used per token. Existing systems alleviate this limitation by offloading inactive experts to CPU memory, but transferring experts across the PCIe interconnect incurs significant latency (about 10 ms). Prefetching heuristics aim to hide this latency by predicting which experts are needed, but prefetch failures introduce significant stalls and amplify inference latency. In the event of a prefetch failure, prior work offers two primary solutions: either fetch the expert on demand, which incurs a long stall due to the PCIe bottleneck, or drop the expert from the computation, which significantly degrades model accuracy. The critical challenge, therefore, is to maintain both high inference speed and model accuracy when prefetching fails.", "AI": {"tldr": "MoE\u6a21\u578b\u901a\u8fc7\u4ec5\u6fc0\u6d3b\u90e8\u5206\u4e13\u5bb6\u7f51\u7edc\u6765\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u5927\u6a21\u578b\u53c2\u6570\u8d85\u51faGPU\u5185\u5b58\u5bb9\u91cf\uff0c\u9700\u8981\u5c06\u975e\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230CPU\u3002\u73b0\u6709\u9884\u53d6\u65b9\u6cd5\u5728\u9884\u6d4b\u5931\u8d25\u65f6\u4f1a\u4e25\u91cd\u5f71\u54cd\u63a8\u7406\u901f\u5ea6\u6216\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3MoE\u6a21\u578b\u5728\u9884\u53d6\u5931\u8d25\u65f6\u9762\u4e34\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u7cbe\u5ea6\u4e0b\u964d\u7684\u77db\u76fe\u95ee\u9898\uff0c\u9700\u8981\u5728\u4fdd\u6301\u9ad8\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "method": "\u8bba\u6587\u672a\u660e\u786e\u8bf4\u660e\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u4ece\u95ee\u9898\u63cf\u8ff0\u770b\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6280\u672f\u6765\u5904\u7406\u9884\u53d6\u5931\u8d25\u7684\u60c5\u51b5\uff0c\u907f\u514dPCIe\u4f20\u8f93\u5ef6\u8fdf\u6216\u4e13\u5bb6\u4e22\u5f03\u5e26\u6765\u7684\u95ee\u9898\u3002", "result": "\u5185\u5bb9\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff1a\u9884\u53d6\u5931\u8d25\u65f6\u8981\u4e48\u4ea7\u751f10ms\u7684PCIe\u4f20\u8f93\u5ef6\u8fdf\uff0c\u8981\u4e48\u4e22\u5f03\u4e13\u5bb6\u5bfc\u81f4\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "MoE\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\u662f\u5728\u9884\u53d6\u5931\u8d25\u65f6\u540c\u65f6\u4fdd\u6301\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u6a21\u578b\u7cbe\u5ea6\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u8fd9\u4e00\u74f6\u9888\u3002"}}
{"id": "2511.10094", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10094", "abs": "https://arxiv.org/abs/2511.10094", "authors": ["Yiming Tang", "Abhijeet Sinha", "Dianbo Liu"], "title": "How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders", "comment": "10 pages, 5 figures", "summary": "Although recent generative models are remarkably capable of producing instruction-following and realistic outputs, they remain prone to notable physical plausibility failures. Though critical in applications, these physical plausibility errors often escape detection by existing evaluation methods. Furthermore, no framework exists for automatically identifying and interpreting specific physical error patterns in natural language, preventing targeted model improvements. We introduce Matryoshka Transcoders, a novel framework for the automatic discovery and interpretation of physical plausibility features in generative models. Our approach extends the Matryoshka representation learning paradigm to transcoder architectures, enabling hierarchical sparse feature learning at multiple granularity levels. By training on intermediate representations from a physical plausibility classifier and leveraging large multimodal models for interpretation, our method identifies diverse physics-related failure modes without manual feature engineering, achieving superior feature relevance and feature accuracy compared to existing approaches. We utilize the discovered visual patterns to establish a benchmark for evaluating physical plausibility in generative models. Our analysis of eight state-of-the-art generative models provides valuable insights into how these models fail to follow physical constraints, paving the way for further model improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86Matryoshka Transcoders\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u548c\u89e3\u91ca\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7269\u7406\u5408\u7406\u6027\u9519\u8bef\uff0c\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u7279\u5f81\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6a21\u578b\u89e3\u91ca\uff0c\u8bc6\u522b\u7269\u7406\u76f8\u5173\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u4ea7\u751f\u903c\u771f\u8f93\u51fa\uff0c\u4f46\u4ecd\u5b58\u5728\u7269\u7406\u5408\u7406\u6027\u9519\u8bef\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u8fd9\u4e9b\u9519\u8bef\uff0c\u7f3a\u4e4f\u81ea\u52a8\u8bc6\u522b\u548c\u89e3\u91ca\u7269\u7406\u9519\u8bef\u6a21\u5f0f\u7684\u6846\u67b6\u3002", "method": "\u6269\u5c55Matryoshka\u8868\u793a\u5b66\u4e60\u8303\u5f0f\u5230\u8f6c\u7801\u5668\u67b6\u6784\uff0c\u5728\u7269\u7406\u5408\u7406\u6027\u5206\u7c7b\u5668\u7684\u4e2d\u95f4\u8868\u793a\u4e0a\u8bad\u7ec3\uff0c\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u89e3\u91ca\uff0c\u5b9e\u73b0\u591a\u7c92\u5ea6\u5c42\u6b21\u7a00\u758f\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7279\u5f81\u76f8\u5173\u6027\u548c\u51c6\u786e\u6027\uff0c\u65e0\u9700\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u5373\u53ef\u8bc6\u522b\u591a\u6837\u5316\u7684\u7269\u7406\u76f8\u5173\u5931\u8d25\u6a21\u5f0f\uff0c\u5efa\u7acb\u4e86\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u7269\u7406\u5408\u7406\u6027\u7684\u57fa\u51c6\u3002", "conclusion": "\u5bf9\u516b\u4e2a\u6700\u5148\u8fdb\u751f\u6210\u6a21\u578b\u7684\u5206\u6790\u63ed\u793a\u4e86\u5b83\u4eec\u5982\u4f55\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.10615", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10615", "abs": "https://arxiv.org/abs/2511.10615", "authors": ["Shruti Singh Baghel", "Yash Pratap Singh Rathore", "Sushovan Jena", "Anurag Pradhan", "Amit Shukla", "Arnav Bhavsar", "Pawan Goyal"], "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals", "comment": "8 pages", "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.", "AI": {"tldr": "\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21\u7684SmolVLM2\u6a21\u578b\uff08500M\u548c2.2B\u53c2\u6570\uff09\u5728\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u63cf\u8ff0\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u90e8\u7f72\u9700\u6c42\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f9d\u8d56\u8be6\u7ec6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63cf\u8ff0\u7684\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u3002", "method": "\u5728\u4e24\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff08AVCaps\u6237\u5916\u548cCharades\u5ba4\u5185\uff09\u4e0a\u8bc4\u4f30SmolVLM2\u53d8\u4f53\uff0c\u5f15\u5165\u4e24\u4e2a\u65b0\u9896\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u591a\u4e0a\u4e0b\u6587BLV\u6846\u67b6\u548c\u5bfc\u822a\u8f85\u52a9\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u56db\u79cd\u63d0\u793a\u8bbe\u8ba1\u7b56\u7565\uff0c\u5e76\u5728\u667a\u80fd\u624b\u673a\u4e0a\u90e8\u7f72FP32\u548cINT8\u7cbe\u5ea6\u53d8\u4f53\u3002", "result": "\u8bc4\u4f30\u4e86\u6a21\u578b\u89c4\u6a21\u5bf9\u53ef\u8bbf\u95ee\u6027\u63cf\u8ff0\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u6027\u80fd\u7ea6\u675f\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u79fb\u52a8\u90e8\u7f72\u6d4b\u8bd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u9002\u5408\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u5b9e\u9645\u9700\u6c42\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.10334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10334", "abs": "https://arxiv.org/abs/2511.10334", "authors": ["Wenti Yin", "Huaxin Zhang", "Xiang Wang", "Yuqing Lu", "Yicheng Zhang", "Bingquan Gong", "Jialong Zuo", "Li Yu", "Changxin Gao", "Nong Sang"], "title": "Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment", "comment": "Accepted to AAAI 2026. Code is available at https://github.com/lessiYin/DSANet", "summary": "Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faDSANet\u7f51\u7edc\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u5206\u79bb\u5f02\u5e38\u4e0e\u6b63\u5e38\u7279\u5f81\uff0c\u63d0\u5347\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u503e\u5411\u4e8e\u68c0\u6d4b\u6700\u663e\u8457\u54cd\u5e94\u7247\u6bb5\uff0c\u5ffd\u7565\u4e86\u6316\u6398\u4e0e\u5f02\u5e38\u5206\u79bb\u7684\u591a\u6837\u6b63\u5e38\u6a21\u5f0f\uff0c\u4e14\u6613\u56e0\u76f8\u4f3c\u5916\u89c2\u5bfc\u81f4\u7c7b\u522b\u6df7\u6dc6\uff0c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7ed3\u679c\u4e0d\u7406\u60f3", "method": "1. \u7c97\u7c92\u5ea6\uff1a\u81ea\u5f15\u5bfc\u6b63\u5e38\u6027\u5efa\u6a21\u5206\u652f\uff0c\u5728\u5b66\u4e60\u7684\u6b63\u5e38\u539f\u578b\u6307\u5bfc\u4e0b\u91cd\u6784\u8f93\u5165\u89c6\u9891\u7279\u5f81\uff1b2. \u7ec6\u7c92\u5ea6\uff1a\u89e3\u8026\u5bf9\u6bd4\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u5c06\u89c6\u9891\u5206\u89e3\u4e3a\u4e8b\u4ef6\u4e2d\u5fc3\u548c\u80cc\u666f\u4e2d\u5fc3\u7ec4\u4ef6\uff0c\u5e94\u7528\u89c6\u89c9\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60", "result": "\u5728XD-Violence\u548cUCF-Crime\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDSANet\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "DSANet\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u5f02\u5e38\u548c\u6b63\u5e38\u7279\u5f81\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u53ef\u533a\u5206\u6027"}}
{"id": "2511.10382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10382", "abs": "https://arxiv.org/abs/2511.10382", "authors": ["Zhen Chen", "Yi Zhang", "Xiangyu Yin", "Chengxuan Qin", "Xingyu Zhao", "Xiaowei Huang", "Wenjie Ruan"], "title": "Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation", "comment": null, "summary": "Personalized AI applications such as DreamBooth enable the generation of customized content from user images, but also raise significant privacy concerns, particularly the risk of facial identity leakage. Recent defense mechanisms like Anti-DreamBooth attempt to mitigate this risk by injecting adversarial perturbations into user photos to prevent successful personalization. However, we identify two critical yet overlooked limitations of these methods. First, the adversarial examples often exhibit perceptible artifacts such as conspicuous patterns or stripes, making them easily detectable as manipulated content. Second, the perturbations are highly fragile, as even a simple, non-learned filter can effectively remove them, thereby restoring the model's ability to memorize and reproduce user identity. To investigate this vulnerability, we propose a novel evaluation framework, AntiDB_Purify, to systematically evaluate existing defenses under realistic purification threats, including both traditional image filters and adversarial purification. Results reveal that none of the current methods maintains their protective effectiveness under such threats. These findings highlight that current defenses offer a false sense of security and underscore the urgent need for more imperceptible and robust protections to safeguard user identity in personalized generation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u6709\u5bf9\u6297\u6027\u9632\u5fa1\u65b9\u6cd5\uff08\u5982Anti-DreamBooth\uff09\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u53ef\u611f\u77e5\u7684\u4f2a\u5f71\u4e14\u6613\u88ab\u7b80\u5355\u6ee4\u6ce2\u5668\u53bb\u9664\uff0c\u63d0\u51fa\u4e86AntiDB_Purify\u8bc4\u4f30\u6846\u67b6\u6765\u7cfb\u7edf\u8bc4\u4f30\u9632\u5fa1\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e2a\u6027\u5316AI\u5e94\u7528\u5982DreamBooth\u867d\u7136\u80fd\u751f\u6210\u5b9a\u5236\u5185\u5bb9\uff0c\u4f46\u4e5f\u5e26\u6765\u9762\u90e8\u8eab\u4efd\u6cc4\u9732\u7684\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u53ef\u611f\u77e5\u4f2a\u5f71\u548c\u6613\u88ab\u53bb\u9664\u7684\u8106\u5f31\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4fdd\u62a4\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86AntiDB_Purify\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u4f20\u7edf\u56fe\u50cf\u6ee4\u6ce2\u5668\u548c\u5bf9\u6297\u6027\u51c0\u5316\u5a01\u80c1\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6240\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u51c0\u5316\u5a01\u80c1\u4e0b\u90fd\u65e0\u6cd5\u4fdd\u6301\u4fdd\u62a4\u6548\u679c\uff0c\u8868\u660e\u73b0\u6709\u9632\u5fa1\u4ec5\u63d0\u4f9b\u865a\u5047\u7684\u5b89\u5168\u611f\u3002", "conclusion": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u66f4\u4e0d\u53ef\u611f\u77e5\u548c\u9c81\u68d2\u7684\u4fdd\u62a4\u673a\u5236\u6765\u4fdd\u969c\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u7528\u6237\u8eab\u4efd\u5b89\u5168\u3002"}}
{"id": "2511.10431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10431", "abs": "https://arxiv.org/abs/2511.10431", "authors": ["Daniele Perlo", "Vladimir Despotovic", "Selma Boudissa", "Sang-Yoon Kim", "Petr Nazarov", "Yanrong Zhang", "Max Wintermark", "Olivier Keunen"], "title": "RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation", "comment": null, "summary": "We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u5b9e\u9a8c\u5ba4\u556e\u9f7f\u52a8\u7269\u60ca\u53a5\u4e8b\u4ef6\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b10,101\u4e2a\u9634\u6027\u6837\u672c\u548c2,952\u4e2a\u9633\u6027\u6837\u672c\uff0c\u4f7f\u7528TimeSformer\u6a21\u578b\u5728\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8fbe\u523097%\u7684F1\u5206\u6570\u3002", "motivation": "\u4e3a\u4e34\u5e8a\u524d\u766b\u75eb\u7814\u7a76\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u3001\u57fa\u4e8e\u89c6\u9891\u7684\u76d1\u6d4b\u65b9\u6cd5\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u96c6\u6765\u652f\u6301\u81ea\u52a8\u60ca\u53a5\u4e8b\u4ef6\u68c0\u6d4b\u3002", "method": "\u6536\u96c6\u4e8619\u53ea\u556e\u9f7f\u52a8\u7269\u7684\u77ed\u65f6\uff0810\u79d2\uff09\u4fef\u89c6\u548c\u4fa7\u89c6\u89c6\u9891\u7247\u6bb5\uff0c\u91c7\u7528\u4e25\u683c\u7684\u53d7\u8bd5\u8005\u5212\u5206\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u89c6\u9891\u5206\u7c7b\u5668TimeSformer\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "TimeSformer\u67b6\u6784\u80fd\u591f\u533a\u5206\u60ca\u53a5\u548c\u6b63\u5e38\u6d3b\u52a8\uff0c\u5e73\u5747F1\u5206\u6570\u8fbe\u523097%\uff0c\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u524d\u766b\u75eb\u7814\u7a76\u4e2d\u7684\u975e\u4fb5\u5165\u6027\u89c6\u9891\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u7814\u7a76\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u89c6\u9891\u7684\u60ca\u53a5\u68c0\u6d4b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.10504", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10504", "abs": "https://arxiv.org/abs/2511.10504", "authors": ["Daryl Noupa Yongueng", "Hamidou Tembine"], "title": "Holonorm", "comment": "17 pages, 11 figures, 10 tables, 2 datasets. A stable geometric alternative to LayerNorm and Tanh normalization in deep neural networks", "summary": "Normalization is a key point in transformer training . In Dynamic Tanh (DyT), the author demonstrated that Tanh can be used as an alternative layer normalization (LN) and confirmed the effectiveness of the idea. But Tanh itself faces orthogonality, linearity and distortion problems. Due to that, his proposition cannot be reliable. So we propose a Holonorm (hn) which has residual connections and nonlinearity. Holonorm is suitable for replacing Tanh in the context of normalization. Although the HoloNorm expression could be similar to the softsign function in dimension one, softsign is a componentwise function which is not good for tensors and vectors of great dimension. Holonorm preserves the orthogonality, the direction, the invertibility of the signal. Holonorm is also a suitable metric, maps all vectors into the open unit ball. This prevents exploding activations and improves stability in deep Transformer models. In this work, we have meticulously examined the normalization in transformers and say that Holonorm, a generalized form of softsign function suited as a normalization function first.Second, defined between 0 and 1 hn serves as a percentage, and $1 - \\text{Holonorm}$ is its complement, making it better understandable in evaluating a model.", "AI": {"tldr": "\u63d0\u51faHolonorm\u4f5c\u4e3aTransformer\u4e2d\u66ff\u4ee3Tanh\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3Tanh\u5b58\u5728\u7684\u6b63\u4ea4\u6027\u3001\u7ebf\u6027\u548c\u5931\u771f\u95ee\u9898\uff0c\u5177\u6709\u6b8b\u5dee\u8fde\u63a5\u548c\u975e\u7ebf\u6027\u7279\u6027\uff0c\u80fd\u9632\u6b62\u6fc0\u6d3b\u503c\u7206\u70b8\u5e76\u63d0\u5347\u6df1\u5ea6Transformer\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "Tanh\u4f5c\u4e3a\u5c42\u5f52\u4e00\u5316\u7684\u66ff\u4ee3\u65b9\u6cd5\u5b58\u5728\u6b63\u4ea4\u6027\u3001\u7ebf\u6027\u548c\u5931\u771f\u95ee\u9898\uff0c\u5bfc\u81f4\u5176\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHolonorm\u65b9\u6cd5\uff0c\u5177\u6709\u6b8b\u5dee\u8fde\u63a5\u548c\u975e\u7ebf\u6027\u7279\u6027\uff0c\u4fdd\u6301\u4fe1\u53f7\u7684\u6b63\u4ea4\u6027\u3001\u65b9\u5411\u548c\u53ef\u9006\u6027\uff0c\u5c06\u5411\u91cf\u6620\u5c04\u5230\u5f00\u653e\u5355\u4f4d\u7403\u5185\uff0c\u9632\u6b62\u6fc0\u6d3b\u503c\u7206\u70b8\u3002", "result": "Holonorm\u4f5c\u4e3asoftsign\u51fd\u6570\u7684\u5e7f\u4e49\u5f62\u5f0f\uff0c\u9002\u5408\u4f5c\u4e3a\u5f52\u4e00\u5316\u51fd\u6570\uff0c\u57280\u52301\u8303\u56f4\u5185\u4f5c\u4e3a\u767e\u5206\u6bd4\u4f7f\u7528\uff0c\u4fbf\u4e8e\u6a21\u578b\u8bc4\u4f30\u3002", "conclusion": "Holonorm\u662f\u66ff\u4ee3Tanh\u7684\u6709\u6548\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u80fd\u89e3\u51b3Tanh\u7684\u95ee\u9898\u5e76\u63d0\u5347Transformer\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.10484", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10484", "abs": "https://arxiv.org/abs/2511.10484", "authors": ["Tejas Sudharshan Mathai", "Anisa V. Prasad", "Xinya Wang", "Praveen T. S. Balamuralikrishna", "Yan Zhuang", "Abhinav Suri", "Jianfei Liu", "Perry J. Pickhardt", "Ronald M. Summers"], "title": "Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes", "comment": "Submitted to IEEE ISBI 2026", "summary": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\\pm$ 8.32 compared to 3.19 $\\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\\pm$ 0.17 and lowest ASSD error of 1.94 $\\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\\% sensitivity, and 91.9\\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7CT\u6210\u50cf\u751f\u7269\u6807\u5fd7\u7269\u7b5b\u67e52\u578b\u7cd6\u5c3f\u75c5\uff0c\u53d1\u73b0\u80f0\u817a\u8868\u9762\u5206\u53f6\u5ea6(PSL)\u5728\u7cd6\u5c3f\u75c5\u60a3\u8005\u4e2d\u663e\u8457\u5347\u9ad8\uff0c\u591a\u53d8\u91cf\u6a21\u578b\u9884\u6d4bT2DM\u7684AUC\u8fbe\u52300.90\u3002", "motivation": "2\u578b\u7cd6\u5c3f\u75c5\u65e9\u671f\u68c0\u6d4b\u5f88\u91cd\u8981\uff0c\u4f46\u80f0\u817a\u8868\u9762\u5206\u53f6\u5ea6\u5728T2DM\u60a3\u8005\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728584\u540d\u60a3\u8005\u6570\u636e\u96c6\u4e2d\u5206\u5272\u80f0\u817a\uff0c\u81ea\u52a8\u68c0\u6d4bPSL\uff0c\u5e76\u5f00\u53d1\u591a\u53d8\u91cf\u6a21\u578b\u9884\u6d4bT2DM\u3002", "result": "\u7cd6\u5c3f\u75c5\u60a3\u8005PSL\u663e\u8457\u66f4\u9ad8(4.26\u00b18.32 vs 3.19\u00b13.62)\uff0cPancAP\u6a21\u578b\u5206\u5272\u6548\u679c\u6700\u597d(Dice=0.79)\uff0cT2DM\u9884\u6d4b\u6a21\u578bAUC=0.90\uff0c\u654f\u611f\u602766.7%\uff0c\u7279\u5f02\u602791.9%\u3002", "conclusion": "PSL\u53ef\u7528\u4e8eT2DM\u7b5b\u67e5\uff0c\u5e76\u53ef\u80fd\u5e2e\u52a9\u9884\u6d4bT2DM\u65e9\u671f\u53d1\u75c5\u3002"}}
{"id": "2511.10562", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.10562", "abs": "https://arxiv.org/abs/2511.10562", "authors": ["Emmanuel Asiedu Brempong", "Mohammed Alewi Hassen", "MohamedElfatih MohamedKhair", "Vusumuzi Dube", "Santiago Hincapie Potes", "Olivia Graham", "Amanie Brik", "Amy McGovern", "George Huffman", "Jason Hickey"], "title": "Oya: Deep Learning for Accurate Global Precipitation Estimation", "comment": null, "summary": "Accurate precipitation estimation is critical for hydrological applications, especially in the Global South where ground-based observation networks are sparse and forecasting skill is limited. Existing satellite-based precipitation products often rely on the longwave infrared channel alone or are calibrated with data that can introduce significant errors, particularly at sub-daily timescales. This study introduces Oya, a novel real-time precipitation retrieval algorithm utilizing the full spectrum of visible and infrared (VIS-IR) observations from geostationary (GEO) satellites. Oya employs a two-stage deep learning approach, combining two U-Net models: one for precipitation detection and another for quantitative precipitation estimation (QPE), to address the inherent data imbalance between rain and no-rain events. The models are trained using high-resolution GPM Combined Radar-Radiometer Algorithm (CORRA) v07 data as ground truth and pre-trained on IMERG-Final retrievals to enhance robustness and mitigate overfitting due to the limited temporal sampling of CORRA. By leveraging multiple GEO satellites, Oya achieves quasi-global coverage and demonstrates superior performance compared to existing competitive regional and global precipitation baselines, offering a promising pathway to improved precipitation monitoring and forecasting.", "AI": {"tldr": "Oya\u662f\u4e00\u79cd\u65b0\u578b\u5b9e\u65f6\u964d\u6c34\u53cd\u6f14\u7b97\u6cd5\uff0c\u5229\u7528\u5730\u7403\u9759\u6b62\u536b\u661f\u7684\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5168\u5149\u8c31\u89c2\u6d4b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u4e2aU-Net\u6a21\u578b\u5206\u522b\u8fdb\u884c\u964d\u6c34\u68c0\u6d4b\u548c\u5b9a\u91cf\u964d\u6c34\u4f30\u8ba1\uff0c\u5728\u51c6\u5168\u7403\u8303\u56f4\u5185\u4f18\u4e8e\u73b0\u6709\u964d\u6c34\u4ea7\u54c1\u3002", "motivation": "\u5168\u7403\u5357\u65b9\u5730\u533a\u5730\u9762\u89c2\u6d4b\u7f51\u7edc\u7a00\u758f\u4e14\u9884\u62a5\u80fd\u529b\u6709\u9650\uff0c\u73b0\u6709\u536b\u661f\u964d\u6c34\u4ea7\u54c1\u4ec5\u4f9d\u8d56\u957f\u6ce2\u7ea2\u5916\u901a\u9053\u6216\u4f7f\u7528\u53ef\u80fd\u5f15\u5165\u663e\u8457\u8bef\u5dee\u7684\u6821\u51c6\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u6b21\u65e5\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a\u4e00\u4e2aU-Net\u7528\u4e8e\u964d\u6c34\u68c0\u6d4b\uff0c\u53e6\u4e00\u4e2aU-Net\u7528\u4e8e\u5b9a\u91cf\u964d\u6c34\u4f30\u8ba1\uff0c\u4f7f\u7528GPM CORRA v07\u6570\u636e\u4f5c\u4e3a\u5730\u9762\u771f\u503c\uff0c\u5e76\u5728IMERG-Final\u4e0a\u9884\u8bad\u7ec3\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "Oya\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u5730\u7403\u9759\u6b62\u536b\u661f\u5b9e\u73b0\u51c6\u5168\u7403\u8986\u76d6\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u7ade\u4e89\u6027\u533a\u57df\u548c\u5168\u7403\u964d\u6c34\u57fa\u7ebf\u4ea7\u54c1\u3002", "conclusion": "Oya\u4e3a\u6539\u8fdb\u964d\u6c34\u76d1\u6d4b\u548c\u9884\u62a5\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u5730\u9762\u89c2\u6d4b\u7a00\u758f\u7684\u5730\u533a\u3002"}}
