{"id": "2509.18123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u571f\u58e4\u6e7f\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u8054\u5408\u68c0\u6d4b\u704c\u6e89\u6a21\u5f0f\u548c\u5f02\u5e38\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u6807\u6ce8\u6216\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u571f\u58e4\u6e7f\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u57fa\u4e8e\u9608\u503c\u7684\u89c4\u5219\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5b58\u5728\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528ChatGPT-4.1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u8bbe\u8ba1\u9886\u57df\u77e5\u8bc6\u63d0\u793a\u6a21\u677f\uff0c\u8bc6\u522b\u704c\u6e89\u4e8b\u4ef6\u3001\u4f30\u8ba1\u51c0\u704c\u6e89\u589e\u76ca\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u5f02\u5e38\uff0c\u5e76\u751f\u6210\u7ed3\u6784\u5316\u53ef\u89e3\u91ca\u62a5\u544a\u3002", "result": "\u5728\u7f8e\u56fd\u591a\u4e2a\u5546\u4e1a\u548c\u5b9e\u9a8c\u519c\u573a\u7684\u771f\u5b9e\u571f\u58e4\u6e7f\u5ea6\u4f20\u611f\u5668\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0cSPADE\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ec\u56de\u7387\u548cF1\u5206\u6570\u66f4\u9ad8\uff0c\u704c\u6e89\u4e8b\u4ef6\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e5f\u5f88\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86LLMs\u4f5c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u7684\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6574\u5408\u5b9a\u6027\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u63a8\u7406\uff0c\u4e3a\u51c6\u786e\u7684\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u548c\u6539\u8fdb\u7684\u704c\u6e89\u8c03\u5ea6\u63d0\u4f9b\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2509.18168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.", "AI": {"tldr": "HSGM\u662f\u4e00\u79cd\u5206\u5c42\u6bb5\u56fe\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u957f\u6587\u6863\u5206\u89e3\u4e3a\u6709\u610f\u4e49\u6bb5\u843d\u540e\u6784\u5efa\u5c40\u90e8\u8bed\u4e49\u56fe\u548c\u5168\u5c40\u56fe\u8bb0\u5fc6\uff0c\u663e\u8457\u964d\u4f4e\u957f\u6587\u6863\u8bed\u4e49\u89e3\u6790\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u957f\u6587\u6863\u8bed\u4e49\u89e3\u6790\u4e2d\u56e0\u6210\u5bf9\u7ec4\u5408\u548c\u5185\u5b58\u9700\u6c42\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u800c\u9762\u4e34\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u8d85\u957f\u6587\u672c\u8bed\u4e49\u5efa\u6a21\u3002", "method": "\u5c06\u8f93\u5165\u6587\u6863\u5206\u89e3\u4e3aM\u4e2a\u6bb5\u843d\uff0c\u4e3a\u6bcf\u4e2a\u6bb5\u843d\u6784\u5efa\u5c40\u90e8\u8bed\u4e49\u56fe\uff0c\u63d0\u53d6\u7d27\u51d1\u7684\u6458\u8981\u8282\u70b9\u5f62\u6210\u5168\u5c40\u56fe\u8bb0\u5fc6\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\u548c\u5206\u5c42\u67e5\u8be2\u5904\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHSGM\u5b9e\u73b0\u4e862-4\u500d\u63a8\u7406\u52a0\u901f\u3001\u8d85\u8fc760%\u7684\u5cf0\u503c\u5185\u5b58\u51cf\u5c11\uff0c\u5e76\u4fdd\u6301\u57fa\u7ebf\u51c6\u786e\u7387\u768495%\u4ee5\u4e0a\u3002", "conclusion": "HSGM\u4e3a\u8d85\u957f\u6587\u672c\u7684\u8bed\u4e49\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684NLP\u5e94\u7528\u3002"}}
{"id": "2509.18159", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPolypSeg-GradCAM\u6846\u67b6\uff0c\u7ed3\u5408U-Net\u548cGrad-CAM\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u606f\u8089\u5206\u5272\uff0c\u5728Kvasir-SEG\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff08IoU 0.9257\uff09\uff0c\u589e\u5f3aAI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u662f\u5168\u7403\u4e3b\u8981\u764c\u75c7\u6b7b\u56e0\uff0c\u80c3\u80a0\u9053\u606f\u8089\u662f\u5173\u952e\u524d\u5146\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u65b9\u6cd5\u51c6\u786e\u6027\u9ad8\u4f46\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u963b\u788d\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u96c6\u6210U-Net\u67b6\u6784\u548c\u68af\u5ea6\u52a0\u6743\u7c7b\u6fc0\u6d3b\u6620\u5c04(Grad-CAM)\uff0c\u57281000\u5f20\u6807\u6ce8\u5185\u955c\u56fe\u50cf\u7684Kvasir-SEG\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd5\u96c6\u5e73\u5747IoU\u8fbe0.9257\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6Dice\u7cfb\u6570\u5747\u9ad8\u4e8e0.96\uff0cGrad-CAM\u53ef\u89c6\u5316\u786e\u8ba4\u9884\u6d4b\u57fa\u4e8e\u4e34\u5e8a\u76f8\u5173\u533a\u57df\u3002", "conclusion": "PolypSeg-GradCAM\u5c06\u9ad8\u5206\u5272\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u7ed3\u5408\uff0c\u662f\u8fc8\u5411\u53ef\u9760AI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u548c\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u9884\u9632\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.18156", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRubin\u56e0\u679c\u6a21\u578b\u7684\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e8b\u4ef6\u89c6\u4e3a\u6cbb\u7597\u548c\u7ed3\u679c\uff0c\u5229\u7528\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u751f\u6210\u865a\u62df\u53cc\u80de\u80ce\u6765\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u6a21\u5f0f\u548c\u591a\u8df3\u5173\u7cfb\u63a8\u7406\uff0c\u5bb9\u6613\u56e0\u56e0\u679c\u5173\u7cfb\u7684\u975e\u6b63\u5f0f\u4f7f\u7528\u548c\u865a\u5047\u56fe\u5f62\u63a8\u7406\u800c\u5bfc\u81f4\u9519\u8bef\u8bc6\u522b\u3002\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u533a\u5206\u56e0\u679c\u5173\u7cfb\u548c\u76f8\u5173\u5173\u7cfb\u3002", "method": "\u91c7\u7528Rubin\u56e0\u679c\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u7b2c\u4e00\u4e2a\u4e8b\u4ef6\u89c6\u4e3a\u6cbb\u7597\uff0c\u7b2c\u4e8c\u4e2a\u4e8b\u4ef6\u89c6\u4e3a\u7ed3\u679c\u3002\u4f7f\u7528\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u4ece\u76f8\u5173\u5386\u53f2\u6570\u636e\u4e2d\u751f\u6210\u865a\u62df\u53cc\u80de\u80ce\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u5408\u6210\u548c\u53cd\u6f14\u6280\u672f\u6765\u4f30\u8ba1\u6cbb\u7597\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56e0\u679c\u5173\u7cfb\u57fa\u51c6\u6d4b\u8bd5COPES-hard\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc6\u522b\u6548\u679c\u4f18\u4e8e\u5305\u62ecGPT-4\u5728\u5185\u7684\u5148\u524d\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u7a33\u5065\u5730\u8bc6\u522b\u4e8b\u4ef6\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u57fa\u4e8eRubin\u56e0\u679c\u6a21\u578b\u7684\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u6982\u5ff5\u6027\u64cd\u4f5c\u548c\u865a\u62df\u5bf9\u7167\u7ec4\u7684\u6784\u5efa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18105", "abs": "https://arxiv.org/abs/2509.18105", "authors": ["Nachiket N. Naik", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand", "comment": null, "summary": "We study learning of continuous-time inventory dynamics under stochastic\ndemand and quantify when structure helps or hurts forecasting of the bullwhip\neffect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the\nentire right-hand side against a physics-informed Universal Differential\nEquation (UDE) that preserves conservation and order-up-to structure while\nlearning a small residual policy term. Classical supply chain models explain\nthe bullwhip through control/forecasting choices and information sharing, while\nrecent physics-informed and neural differential equation methods blend domain\nconstraints with learned components. It is unclear whether structural bias\nhelps or hinders forecasting under different demand regimes. We address this by\nusing a single-echelon testbed with three demand regimes - AR(1)\n(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done\non varying fractions of each trajectory, followed by evaluation of multi-step\nforecasts for inventory I, order rate O, and demand D. Across the structured\nregimes, UDE consistently generalizes better: with 90% of the training horizon,\ninventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96\nto 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the\nflexibility of NODE is better. These trends persist as train18 ing data\nshrinks, with NODE exhibiting phase drift in extrapolation while UDE remains\nstable but underreacts to rare spikes. Our results provide concrete guidance:\nenforce structure when noise is light-tailed or temporally correlated; relax\nstructure when extreme events dominate. Beyond inventory control, the results\noffer guidance for hybrid modeling in scientific and engineering systems:\nenforce known structure when conservation laws and modest noise dominate, and\nrelax structure to capture extremes in settings where rare events drive\ndynamics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5b8c\u5168\u5b66\u4e60\u7684\u795e\u7ecfODE\u4e0e\u7269\u7406\u4fe1\u606f\u901a\u7528\u5fae\u5206\u65b9\u7a0b\u5728\u8fde\u7eed\u65f6\u95f4\u5e93\u5b58\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u7ed3\u6784\u5316\u9700\u6c42\u673a\u5236\u4e0bUDE\u6cdb\u5316\u66f4\u597d\uff0c\u800c\u5728\u91cd\u5c3e\u5206\u5e03\u4e0bNODE\u66f4\u7075\u6d3b\u3002", "motivation": "\u7814\u7a76\u7ed3\u6784\u7ea6\u675f\u5728\u725b\u97ad\u6548\u5e94\u9884\u6d4b\u4e2d\u4f55\u65f6\u6709\u5e2e\u52a9\u6216\u6709\u5bb3\uff0c\u89e3\u51b3\u5728\u6df7\u5408\u5efa\u6a21\u4e2d\u662f\u5426\u5e94\u8be5\u5f3a\u5236\u6267\u884c\u5df2\u77e5\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u7ea7\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6bd4\u8f83\u5b8c\u5168\u5b66\u4e60\u7684NODE\u548c\u4fdd\u7559\u5b88\u6052\u7ed3\u6784\u7684UDE\uff0c\u5728\u4e09\u79cd\u9700\u6c42\u673a\u5236\uff08AR(1)\u3001\u9ad8\u65afi.i.d.\u3001\u91cd\u5c3e\u5bf9\u6570\u6b63\u6001\uff09\u4e0b\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u5728\u7ed3\u6784\u5316\u673a\u5236\u4e0b\uff0cUDE\u8868\u73b0\u66f4\u597d\uff08\u5e93\u5b58RMSE\u4ece4.92\u964d\u81f30.26\uff09\uff1b\u5728\u91cd\u5c3e\u673a\u5236\u4e0b\uff0cNODE\u66f4\u7075\u6d3b\u3002\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u65f6\uff0cNODE\u51fa\u73b0\u76f8\u4f4d\u6f02\u79fb\uff0cUDE\u4fdd\u6301\u7a33\u5b9a\u4f46\u5bf9\u7f55\u89c1\u5cf0\u503c\u53cd\u5e94\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u566a\u58f0\u8f7b\u5c3e\u6216\u65f6\u95f4\u76f8\u5173\u65f6\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u7ea6\u675f\uff1b\u5f53\u6781\u7aef\u4e8b\u4ef6\u4e3b\u5bfc\u65f6\u653e\u677e\u7ed3\u6784\u7ea6\u675f\u3002\u8fd9\u4e3a\u79d1\u5b66\u548c\u5de5\u7a0b\u7cfb\u7edf\u4e2d\u7684\u6df7\u5408\u5efa\u6a21\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2509.18218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76f8\u4f3c\u6027\u573a\u7406\u8bba\uff0c\u8fd9\u662f\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u5b9e\u4f53\u95f4\u76f8\u4f3c\u6027\u5173\u7cfb\u53ca\u5176\u6f14\u5316\u7684\u539f\u5219\u3002\u8be5\u7406\u8bba\u5b9a\u4e49\u4e86\u76f8\u4f3c\u6027\u573a\u3001\u7cfb\u7edf\u6f14\u5316\u3001\u6982\u5ff5\u7ea4\u7ef4\u548c\u751f\u6210\u7b97\u5b50\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f62\u5f0f\u5316\u5730\u5b9a\u4e49\u4e86\u667a\u80fd\u7684\u6982\u5ff5\u3002", "motivation": "\u4f5c\u8005\u8ba4\u4e3a\u6301\u4e45\u5316\u548c\u8f6c\u6362\u76f8\u4f3c\u6027\u5173\u7cfb\u662f\u4efb\u4f55\u53ef\u7406\u89e3\u52a8\u6001\u7cfb\u7edf\u7684\u7ed3\u6784\u57fa\u7840\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u76f8\u4f3c\u6027\u503c\u53ca\u5176\u6f14\u5316\u7684\u539f\u5219\u3002", "method": "\u5b9a\u4e49\u4e86\u76f8\u4f3c\u6027\u573aS: U\u00d7U\u2192[0,1]\uff0c\u6ee1\u8db3\u81ea\u53cd\u6027\uff1b\u7cfb\u7edf\u6f14\u5316\u5e8f\u5217Z_p=(X_p,S^(p))\uff1b\u6982\u5ff5K\u8bf1\u5bfc\u7684\u7ea4\u7ef4F_\u03b1(K)\uff1b\u751f\u6210\u7b97\u5b50G\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u5f62\u5f0f\u5316\u667a\u80fd\u7684\u5b9a\u4e49\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u4e2a\u5b9a\u7406\uff1a(i)\u4e0d\u5bf9\u79f0\u6027\u963b\u6b62\u76f8\u4e92\u5305\u542b\uff1b(ii)\u7a33\u5b9a\u6027\u9700\u8981\u951a\u5750\u6807\u6216\u6700\u7ec8\u9650\u5236\u5728f\u7684\u6c34\u5e73\u96c6\u4e2d\u3002\u8fd9\u4e9b\u7ed3\u679c\u786e\u4fdd\u76f8\u4f3c\u6027\u573a\u6f14\u5316\u65e2\u53d7\u7ea6\u675f\u53c8\u53ef\u89e3\u91ca\u3002", "conclusion": "\u76f8\u4f3c\u6027\u573a\u7406\u8bba\u4e3a\u8868\u5f81\u3001\u6bd4\u8f83\u548c\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u8bed\u8a00\uff0c\u53ef\u7528\u4e8e\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u5c06\u5176\u4f5c\u4e3a\u793e\u4f1a\u8ba4\u77e5\u7684\u5b9e\u9a8c\u63a2\u9488\u3002"}}
{"id": "2509.18182", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "AI\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u5f71\u50cf\u81ea\u52a8\u63a8\u65ad\u5c4b\u9876\u5c5e\u6027\uff0c\u4e3a\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\u63d0\u4f9b\u5efa\u7b51\u7ed3\u6784\u4fe1\u606f\u4ee5\u652f\u6301\u707e\u5bb3\u98ce\u9669\u8bc4\u4f30", "motivation": "\u89e3\u51b3\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7f3a\u4e4f\u8be6\u7ec6\u5efa\u7b51\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4e8e\u8bc4\u4f30\u98d3\u98ce\u3001\u6d2a\u6c34\u7b49\u707e\u5bb3\u98ce\u9669\u81f3\u5173\u91cd\u8981", "method": "\u6bd4\u8f83\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u6d45\u5c42\u5206\u7c7b\u5668\u4e0e\u5fae\u8c03\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5c4b\u9876\u5206\u7c7b\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u8bc4\u4f30\u52a0\u5165\u90bb\u8fd1\u5c9b\u5c7f\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd", "result": "\u6700\u4f73\u6a21\u578b\u5728\u5c4b\u9876\u5761\u5ea6\u548c\u5c4b\u9876\u6750\u6599\u5206\u7c7b\u4e0a\u5206\u522b\u8fbe\u52300.88\u548c0.83\u7684F1\u5206\u6570", "conclusion": "\u7ed3\u5408\u672c\u5730\u80fd\u529b\u5efa\u8bbe\uff0c\u8be5\u5de5\u4f5c\u4e3a\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\u63d0\u4f9b\u4e86\u5229\u7528AI\u548c\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u57ce\u5e02\u6cbb\u7406\u7684\u65b0\u80fd\u529b"}}
{"id": "2509.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5URNet\uff0c\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u7cbe\u70bc\u6a21\u5757\u548cKL\u6563\u5ea6\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u5728DSEC\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7684\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7684\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u7ec6\u8282\u6355\u6349\u548c\u9884\u6d4b\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86URNet\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7cbe\u70bc\u7f51\u7edc\uff0c\u5305\u542b\u5c40\u90e8-\u5168\u5c40\u7cbe\u70bc\u6a21\u5757\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ec6\u8282\u548c\u957f\u8ddd\u79bb\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u57fa\u4e8eKL\u6563\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u5728DSEC\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cURNet\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u90fd\u4e00\u81f4\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "URNet\u901a\u8fc7\u6709\u6548\u7684\u7cbe\u70bc\u673a\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "SubSpec\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4f4e\u6bd4\u7279\u91cf\u5316\u66ff\u4ee3\u5c42\u6765\u52a0\u901f\u53c2\u6570\u5378\u8f7d\uff0c\u5728\u5185\u5b58\u53d7\u9650\u7684GPU\u4e0a\u5b9e\u73b0\u65e0\u635f\u7684LLM\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u6d88\u8d39\u7ea7GPU\u4e0a\u90e8\u7f72\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u8d28\u91cf\uff08\u538b\u7f29\uff09\uff0c\u8981\u4e48\u63a8\u7406\u901f\u5ea6\u6162\uff08\u53c2\u6570\u5378\u8f7d\uff09\uff0c\u800c\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u8bad\u7ec3\u4e14\u52a0\u901f\u6548\u679c\u6709\u9650\u3002", "method": "\u6784\u5efa\u9ad8\u5ea6\u5bf9\u9f50\u7684\u8349\u7a3f\u6a21\u578b\uff1a\u4ece\u5378\u8f7d\u7684\u76ee\u6807LLM\u90e8\u5206\u751f\u6210\u4f4e\u6bd4\u7279\u91cf\u5316\u66ff\u4ee3\u5c42\uff0c\u5171\u4eab\u5269\u4f59\u7684GPU\u9a7b\u7559\u5c42\u548cKV-Cache\uff0c\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u5e76\u589e\u5f3a\u5bf9\u9f50\u6027\u3002", "result": "\u57288GB VRAM\u9650\u5236\u4e0b\uff0cQwen2.5 7B\u5728MT-Bench\u4e0a\u5b9e\u73b09.1\u500d\u52a0\u901f\uff1b\u572824GB VRAM\u9650\u5236\u4e0b\uff0cQwen2.5 32B\u5728\u6d41\u884c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5b9e\u73b012.5\u500d\u52a0\u901f\u3002", "conclusion": "SubSpec\u901a\u8fc7\u91cf\u5316\u66ff\u4ee3\u5c42\u548c\u5171\u4eab\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u548c\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "ComputerAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\u63a7\u5236\uff0c\u901a\u8fc7\u4e24\u7ea7\u9009\u9879\u8fc7\u7a0b\uff08\u7ba1\u7406\u5668\u548c\u5b50\u7b56\u7565\uff09\u5b9e\u73b0OS\u63a7\u5236\uff0c\u6bd4\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u684c\u9762\u5e94\u7528\u63a7\u5236\u4e2d\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u957f\u89c6\u91ce\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u6837\u672c\u6548\u7387\u5dee\u3001\u65e0\u6cd5\u5728\u8bbe\u5907\u4e0a\u90e8\u7f72\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e09\u7ea7\u6a21\u6001\u72b6\u6001\u7f16\u7801\u5668\uff08\u622a\u56fe\u3001\u4efb\u52a1ID\u3001\u6570\u503c\u72b6\u6001\uff09\uff0c\u96c6\u6210\u5143\u52a8\u4f5c\u548c\u65e9\u505c\u673a\u5236\uff0c\u4f7f\u7528\u7d27\u51d1\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u5c0f\u578b\u7b56\u7565\u7f51\u7edc\uff081500\u4e07\u53c2\u6570\uff09\u3002", "result": "\u5728135\u4e2a\u771f\u5b9e\u684c\u9762\u4efb\u52a1\u6d4b\u8bd5\u4e2d\uff0c\u7b80\u5355\u4efb\u52a1\uff08<8\u6b65\uff09\u6210\u529f\u738792.1%\uff0c\u56f0\u96be\u4efb\u52a1\uff08\u22658\u6b65\uff09\u6210\u529f\u738758.8%\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c114\u4e2a\u6570\u91cf\u7ea7\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e3a\u8ba1\u7b97\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u57fa\u4e8e\u5355\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u8f85\u52a9\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6821\u6b63\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u6765\u4fee\u6b63\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u9519\u8bef\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9519\u8bef\u7387\u3002", "motivation": "\u5927\u591a\u6570\u81ea\u52a8\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u4ee5\"\u5f00\u73af\"\u6a21\u5f0f\u8fd0\u884c\uff0c\u7f3a\u4e4f\u7528\u6237\u53cd\u9988\uff0c\u800c\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u7cfb\u7edf\u6267\u884c\u6d41\u5f0fASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\uff0c\u4f7f\u7528LLM\u751f\u6210\u7b80\u6d01\u6458\u8981\uff0c\u63a5\u53d7\u7b80\u77ed\u8bed\u97f3\u53cd\u9988\u5e76\u7acb\u5373\u6574\u5408\u3002\u5f00\u53d1\u4e86SWM\u6280\u672f\u68c0\u6d4b\u548c\u5206\u5272\u591a\u8bf4\u8bdd\u4eba\u6bb5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7528\u6237\u6821\u6b63\u7684\u5728\u7ebf\u8bf4\u8bdd\u4eba\u6ce8\u518c\u3002", "result": "\u5728AMI\u6d4b\u8bd5\u96c6\u4e0a\u7684LLM\u9a71\u52a8\u6a21\u62df\u663e\u793a\uff0c\u7cfb\u7edf\u663e\u8457\u964d\u4f4e\u4e86DER 9.92%\u548c\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef44.23%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6709\u6548\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u51c6\u786e\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6821\u6b63\u6548\u679c\u3002"}}
{"id": "2509.18383", "categories": ["cs.AI", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "G\u00f6del Test: Can Large Language Models Solve Easy Conjectures?", "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86G\u00f6del\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u89e3\u51b3\u6570\u5b66\u9886\u57df\u7684\u65b0\u7b80\u5355\u731c\u60f3\uff0c\u5e76\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u6d4b\u8bd5\u4e86GPT-5\u7684\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u7ea7\u6570\u5b66\u9886\u57df\u89e3\u51b3\u65b0\u7b80\u5355\u731c\u60f3\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5df2\u77e5\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u3002", "method": "\u9009\u62e9\u4e94\u4e2a\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u672a\u89e3\u51b3\u731c\u60f3\uff0c\u63d0\u4f9b\u76f8\u5173\u6e90\u8bba\u6587\u4f46\u4e0d\u544a\u77e5\u5177\u4f53\u731c\u60f3\uff0c\u8be6\u7ec6\u8bc4\u4f30GPT-5\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "GPT-5\u5728\u4e09\u4e2a\u8f83\u7b80\u5355\u95ee\u9898\u4e0a\u4ea7\u751f\u8fd1\u4e4e\u6b63\u786e\u7684\u89e3\uff0c\u751a\u81f3\u5728\u4e00\u4e2a\u95ee\u9898\u4e0a\u63a8\u7ffb\u4e86\u539f\u731c\u60f3\uff1b\u4f46\u5728\u9700\u8981\u8de8\u8bba\u6587\u7efc\u5408\u7684\u95ee\u9898\u4e0a\u5931\u8d25\uff0c\u5728\u65e0\u9a8c\u8bc1\u731c\u60f3\u7684\u96be\u9898\u4e0a\u7b97\u6cd5\u6b63\u786e\u4f46\u5206\u6790\u5931\u8d25\u3002", "conclusion": "GPT-5\u5728\u5e38\u89c4\u63a8\u7406\u4e0a\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u5076\u5c14\u5c55\u73b0\u539f\u521b\u6027\uff0c\u4f46\u5728\u8de8\u8bba\u6587\u7efc\u5408\u65b9\u9762\u4ecd\u6709\u660e\u663e\u5c40\u9650\uff0c\u53ef\u80fd\u662f\u901a\u8fc7G\u00f6del\u6d4b\u8bd5\u7684\u65e9\u671f\u6b65\u9aa4\u3002"}}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLMZ+\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u767d\u540d\u5355\u673a\u5236\u4fdd\u62a4\u667a\u80fdAI\u4ee3\u7406\u514d\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u786e\u4fdd\u53ea\u6709\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u5b89\u5168\u6d88\u606f\u80fd\u4e0eLLM\u4ea4\u4e92\u3002", "motivation": "\u667a\u80fdAI\u4ee3\u7406\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u5b89\u5168\u98ce\u9669\uff0c\u56e0\u4e3a\u5b83\u4eec\u62e5\u6709\u5bf9\u6570\u636e\u6e90\u548cAPI\u5de5\u5177\u7684\u7279\u6743\u8bbf\u95ee\uff0c\u4e14\u4f9d\u8d56AI\u7684\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u3002\u73b0\u6709\u9632\u5fa1\u673a\u5236\u4e3b\u8981\u4f9d\u8d56\u6076\u610f\u610f\u56fe\u68c0\u6d4b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u63d0\u793a\u767d\u540d\u5355\u65b9\u6cd5\uff0c\u4ec5\u5141\u8bb8\u4e0a\u4e0b\u6587\u9002\u5f53\u4e14\u5b89\u5168\u7684\u6d88\u606f\u4e0e\u667a\u80fdLLM\u4ee3\u7406\u4ea4\u4e92\u3002\u901a\u8fc7\u5229\u7528\u4e0a\u4e0b\u6587\u7279\u5f02\u6027\uff0c\u786e\u4fdd\u6240\u6709\u5916\u90e8\u7528\u6237\u4e0eLLM\u7684\u4ea4\u6362\u90fd\u7b26\u5408\u9884\u5b9a\u4e49\u7528\u4f8b\u548c\u64cd\u4f5c\u8fb9\u754c\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660eLLMZ+\u5bf9\u6700\u5e38\u89c1\u7684\u8d8a\u72f1\u63d0\u793a\u5177\u6709\u5f3a\u5927\u97e7\u6027\uff0c\u540c\u65f6\u4e0d\u5e72\u6270\u5408\u6cd5\u4e1a\u52a1\u901a\u4fe1\u3002\u5728\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\uff0c\u8bef\u62a5\u7387\u548c\u6f0f\u62a5\u7387\u5747\u53ef\u964d\u81f30\u3002", "conclusion": "LLMZ+\u65b9\u6cd5\u7b80\u5316\u4e86\u5b89\u5168\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u957f\u671f\u97e7\u6027\uff0c\u51cf\u5c11\u4e86\u7ef4\u6301LLM\u4fe1\u606f\u5b89\u5168\u6240\u9700\u7684\u8d44\u6e90\uff0c\u4e3a\u667a\u80fdAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u62a4\u3002"}}
{"id": "2509.18565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u6570\u5b66\u5e94\u7528\u9898\u3001\u751f\u6210\u65b9\u7a0b\u3001\u4f7f\u7528\u5916\u90e8\u7b26\u53f7\u6c42\u89e3\u5668\uff0c\u5e76\u7ed3\u5408\u4f30\u7b97\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4fee\u6b63\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u5e94\u7528\u9898\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u9700\u8981\u590d\u6742\u7684\u63a8\u7406\u548c\u6570\u5b66\u80fd\u529b\u3002\u73b0\u6709\u7684\u63d0\u793a\u65b9\u6cd5\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u9700\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u63d0\u793aLLM\u4ece\u95ee\u9898\u5206\u89e3\u4e2d\u521b\u5efa\u65b9\u7a0b\uff0c\u4f7f\u7528\u5916\u90e8\u7b26\u53f7\u65b9\u7a0b\u6c42\u89e3\u5668\u751f\u6210\u7b54\u6848\u3002\u7136\u540e\u8ba9LLM\u4f30\u7b97\u6b63\u786e\u7b54\u6848\u5e76\u4e0e\u751f\u6210\u7b54\u6848\u6bd4\u8f83\u9a8c\u8bc1\uff0c\u5982\u679c\u9a8c\u8bc1\u5931\u8d25\u5219\u8fdb\u884c\u8fed\u4ee3\u4fee\u6b63\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u503c\u548c\u4ee3\u6570\u6570\u5b66\u5e94\u7528\u9898\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5e73\u5747\u6bd4\u4e4b\u524d\u6700\u4f73\u7ed3\u679c\u63d0\u9ad8\u4e86\u8fd12%\u3002\u5728\u4e09\u89d2\u51fd\u6570\u5e94\u7528\u9898\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6ee1\u610f\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\u7684\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6SVAMPClean\u548cTrig300\u6765\u8fdb\u4e00\u6b65\u6d4b\u8bd5LLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.18636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u865a\u62df\u7ed3\u6784\uff08DVS\uff09\u7684\u65e0\u4eba\u673a\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u5904\u7406\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u7f16\u961f\u6062\u590d\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u65f6\u7f16\u961f\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6536\u655b\u5230\u671f\u671b\u914d\u7f6e\u3002", "method": "\u91c7\u7528Lloyd\u7b97\u6cd5\u8fdb\u884c\u5747\u5300\u5206\u533a\u548c\u5308\u7259\u5229\u7b97\u6cd5\u8fdb\u884c\u5206\u914d\uff08PAAS\uff09\uff0c\u7ed3\u5408\u57fa\u4e8e\u57fa\u5143\u7684\u8def\u5f84\u641c\u7d22\u548c\u975e\u7ebf\u6027\u8f68\u8ff9\u4f18\u5316\u6765\u89c4\u5212DVS\u7684\u65f6\u7a7a\u8f68\u8ff9\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8f6c\u6362\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u652f\u6301\u6700\u591a15%\u7684\u65e0\u4eba\u673a\u52a0\u5165\u6216\u79bb\u5f00\u7f16\u961f\uff0c\u540c\u65f6\u5feb\u901f\u6062\u590d\u671f\u671b\u7f16\u961f\u5f62\u72b6\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u7f16\u961f\u6062\u590d\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "conclusion": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u52a8\u6001\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18133", "abs": "https://arxiv.org/abs/2509.18133", "authors": ["Le Huang", "Jiazheng Kang", "Cheng Hou", "Zhe Zhao", "Zhenxiang Yan", "Chuan Shi", "Ting Bai"], "title": "Self-Evolving LLMs via Continual Instruction Tuning", "comment": null, "summary": "In real-world industrial settings, large language models (LLMs) must learn\ncontinually to keep pace with diverse and evolving tasks, requiring\nself-evolution to refine knowledge under dynamic data distributions. However,\nexisting continual learning (CL) approaches, such as replay and parameter\nisolation, often suffer from catastrophic forgetting: training on new tasks\ndegrades performance on earlier ones by overfitting to the new distribution and\nweakening generalization.We propose MoE-CL, a parameter-efficient adversarial\nmixture-of-experts framework for industrial-scale, self-evolving continual\ninstruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated\nLoRA expert per task to preserve task-specific knowledge via parameter\nindependence, mitigating forgetting; and (2) a shared LoRA expert to enable\ncross-task transfer. To prevent transferring task-irrelevant noise through the\nshared pathway, we integrate a task-aware discriminator within a GAN. The\ndiscriminator encourages the shared expert to pass only task-aligned\ninformation during sequential training. Through adversarial learning, the\nshared expert acquires generalized representations that mimic the\ndiscriminator, while dedicated experts retain task-specific details, balancing\nknowledge retention and cross-task generalization and thereby supporting\nself-evolution.Extensive experiments on the public MTL5 benchmark and an\nindustrial Tencent3 benchmark validate the effectiveness of MoE-CL for\ncontinual instruction tuning. In real-world A/B testing for content compliance\nreview on the Tencent Video platform, MoE-CL reduced manual review costs by\n15.3%. These results demonstrate that MoE-CL is practical for large-scale\nindustrial deployment where continual adaptation and stable transfer are\ncritical.", "AI": {"tldr": "MoE-CL\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\uff0c\u901a\u8fc7\u53cc\u4e13\u5bb6\u8bbe\u8ba1\u548cGAN\u9274\u522b\u5668\u6765\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u73af\u5883\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u65b0\u4efb\u52a1\u8bad\u7ec3\u65f6\u4f1a\u8fc7\u5ea6\u62df\u5408\u65b0\u5206\u5e03\u5e76\u524a\u5f31\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53ccLoRA\u4e13\u5bb6\u8bbe\u8ba1\uff1a\u4e13\u7528\u4e13\u5bb6\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u5171\u4eab\u4e13\u5bb6\u5b9e\u73b0\u8de8\u4efb\u52a1\u8fc1\u79fb\uff1b\u96c6\u6210\u4efb\u52a1\u611f\u77e5\u9274\u522b\u5668\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u786e\u4fdd\u5171\u4eab\u4e13\u5bb6\u53ea\u4f20\u9012\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5728MTL5\u57fa\u51c6\u548c\u817e\u8baf\u5de5\u4e1a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5728\u817e\u8baf\u89c6\u9891\u5e73\u53f0\u7684\u5185\u5bb9\u5408\u89c4\u5ba1\u67e5A/B\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u4e8615.3%\u7684\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u3002", "conclusion": "MoE-CL\u9002\u7528\u4e8e\u9700\u8981\u6301\u7eed\u9002\u5e94\u548c\u7a33\u5b9a\u8fc1\u79fb\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u90e8\u7f72\u573a\u666f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\uff08DataAgents\uff09\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u96c6\u6210LLM\u63a8\u7406\u4e0e\u4efb\u52a1\u5206\u89e3\u3001\u884c\u52a8\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u73b0\u4ece\u590d\u6742\u6570\u636e\u5230\u77e5\u8bc6\u7684\u81ea\u52a8\u5316\u8f6c\u6362\u3002", "motivation": "\u968f\u7740\u6570\u636e\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u3001\u8f6c\u6362\u548c\u5206\u6790\u5de5\u4f5c\u4ecd\u7136\u52b3\u52a8\u5bc6\u96c6\u3001\u91cd\u590d\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6570\u636e\u4e0eAI\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u901a\u5e38\u672a\u4ee5\u6700\u4f18\u65b9\u5f0f\u7ed3\u6784\u5316\u4ee5\u652f\u6301AI\u5229\u7528\u3002", "method": "DataAgents\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u3001\u8c03\u7528\u5f3a\u5927\u5de5\u5177\u548c\u9002\u5e94\u591a\u6837\u5316\u6570\u636e\u4efb\u52a1\uff0c\u80fd\u591f\u5904\u7406\u6570\u636e\u6536\u96c6\u3001\u96c6\u6210\u3001\u9884\u5904\u7406\u3001\u9009\u62e9\u3001\u8f6c\u6362\u3001\u91cd\u65b0\u52a0\u6743\u3001\u589e\u5f3a\u3001\u91cd\u7f16\u7a0b\u3001\u4fee\u590d\u548c\u68c0\u7d22\u7b49\u64cd\u4f5c\u3002", "result": "DataAgents\u80fd\u591f\u5c06\u590d\u6742\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u4e3a\u8fde\u8d2f\u4e14\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\uff0c\u4ee3\u8868\u4e86\u5411\u81ea\u4e3b\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "conclusion": "\u672c\u6587\u547c\u5401\u5728\u884c\u52a8\u5de5\u4f5c\u6d41\u4f18\u5316\u3001\u5efa\u7acb\u5f00\u653e\u6570\u636e\u96c6\u548c\u57fa\u51c6\u751f\u6001\u7cfb\u7edf\u3001\u4fdd\u62a4\u9690\u79c1\u3001\u5e73\u8861\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684DataAgent\u9632\u62a4\u63aa\u65bd\u65b9\u9762\u505a\u51fa\u534f\u540c\u52aa\u529b\u3002"}}
{"id": "2509.18775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e10-K\u6587\u4ef6\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u53d6\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u6355\u6349\u9690\u542b\u98ce\u9669\u8054\u7cfb\uff0c\u5e76\u751f\u6210\u53ef\u91cf\u5316\u7684\u98ce\u9669\u5173\u7cfb\u8bc4\u5206\u3002", "motivation": "\u4f20\u7edf\u4f9d\u8d56\u4e13\u5bb6\u5224\u65ad\u548c\u624b\u52a8\u5206\u6790\u8bc4\u4f30\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\u7684\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u752810-K\u6587\u4ef6\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u57fa\u4e8e\u65f6\u95f4\u987a\u5e8f\u548c\u8bcd\u6c47\u6a21\u5f0f\u6355\u6349\u9690\u542b\u98ce\u9669\u8054\u7cfb\uff0c\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u7684\u91d1\u878d\u7f16\u7801\u5668\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u548c\u6295\u8d44\u7b56\u7565\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18830", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "DexSkin\u662f\u4e00\u79cd\u67d4\u8f6f\u3001\u53ef\u9002\u5e94\u7684\u7535\u5bb9\u5f0f\u7535\u5b50\u76ae\u80a4\uff0c\u80fd\u591f\u5b9e\u73b0\u654f\u611f\u3001\u5c40\u90e8\u5316\u548c\u53ef\u6821\u51c6\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u53ef\u5b9a\u5236\u5230\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7684\u673a\u5668\u4eba\u624b\u6307\u4e0a\uff0c\u7528\u4e8e\u5b66\u4e60\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u4eba\u7c7b\u76ae\u80a4\u63d0\u4f9b\u4e30\u5bcc\u7684\u89e6\u89c9\u611f\u77e5\u6d41\uff0c\u80fd\u591f\u5728\u5927\u7684\u66f2\u9762\u533a\u57df\u5b9a\u4f4d\u6709\u610f\u548c\u65e0\u610f\u7684\u63a5\u89e6\u4e8b\u4ef6\u3002\u4e3a\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u590d\u5236\u8fd9\u4e9b\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86DexSkin\u7535\u5b50\u76ae\u80a4\uff0c\u5c06\u5176\u5b89\u88c5\u5728\u5e73\u884c\u5939\u722a\u624b\u6307\u4e0a\uff0c\u8986\u76d6\u51e0\u4e4e\u6574\u4e2a\u624b\u6307\u8868\u9762\u3002\u5728\u4ece\u793a\u8303\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u8bc4\u4f30\u5176\u5728\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "result": "DexSkin\u5728\u9700\u8981\u6574\u4e2a\u624b\u6307\u8868\u9762\u4f20\u611f\u8986\u76d6\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982\u624b\u4e2d\u91cd\u65b0\u5b9a\u5411\u7269\u4f53\u548c\u5c06\u5f39\u6027\u5e26\u7f20\u7ed5\u5728\u76d2\u5b50\u4e0a\u3002\u5173\u952e\u7684\u662f\uff0cDexSkin\u53ef\u4ee5\u6821\u51c6\u4ee5\u5b9e\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u4f20\u611f\u5668\u5b9e\u4f8b\u95f4\u7684\u8f6c\u79fb\u3002", "conclusion": "DexSkin\u5c55\u793a\u4e86\u5176\u5728\u5b66\u4e60\u771f\u5b9e\u4e16\u754c\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\u5728\u7269\u6d41\u516c\u53f8\u51fa\u5e93\u4ed3\u5e93\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u5728\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4e0a\u66f4\u4f18\u3002", "motivation": "\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u65e8\u5728\u9884\u6d4b\u6b63\u5728\u8fdb\u884c\u7684\u6d41\u7a0b\u6267\u884c\u7684\u672a\u6765\uff0c\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u662f\u5e38\u89c1\u76ee\u6807\u3002\u672c\u6587\u65e8\u5728\u5728\u771f\u5b9e\u7269\u6d41\u516c\u53f8\u51fa\u5e93\u6d41\u7a0b\u4e2d\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u5728\u7269\u6d41\u516c\u53f8\u63d0\u4f9b\u7684\u5305\u542b169,523\u6761\u8f68\u8ff9\u7684\u65b0\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u83b7\u5f97\u7ade\u4e89\u6027\u51c6\u786e\u7387\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u5728\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2509.18566", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5355\u76ee\u89c6\u9891\u52a8\u6001\u4eba\u4f53\u4e0e\u9759\u6001\u573a\u666f\u8054\u5408\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u635f\u5931\u51fd\u6570\u89e3\u51b3\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u4eba\u4f53\u548c\u9759\u6001\u573a\u666f\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u8fd0\u52a8\u65f6RGB\u5e27\u4f1a\u51fa\u73b0\u8fd0\u52a8\u6a21\u7cca\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4f18\u52bf\uff0c\u66f4\u9002\u5408\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u76843D\u9ad8\u65af\u96c6\u5408\uff0c\u5176\u4e2d\u5305\u542b\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u5c5e\u6027\uff1b\u53ea\u6709\u88ab\u5206\u7c7b\u4e3a\u4eba\u4f53\u7684\u9ad8\u65af\u4f1a\u8fdb\u884c\u5f62\u53d8\u52a8\u753b\uff0c\u573a\u666f\u9ad8\u65af\u4fdd\u6301\u9759\u6001\u3002\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u635f\u5931\u51fd\u6570\uff0c\u5339\u914d\u8fde\u7eed\u6e32\u67d3\u4e4b\u95f4\u7684\u6a21\u62df\u4eae\u5ea6\u53d8\u5316\u4e0e\u4e8b\u4ef6\u6d41\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6ZJU-MoCap-Blur\u548cMMHPSD-Blur\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4eba\u4f53-\u573a\u666f\u91cd\u5efa\u6548\u679c\uff0c\u5728PSNR/SSIM\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0cLPIPS\u6307\u6807\u964d\u4f4e\uff0c\u7279\u522b\u662f\u5bf9\u9ad8\u901f\u8fd0\u52a8\u4e3b\u4f53\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4eba\u4f53\u63a9\u7801\uff0c\u7b80\u5316\u4e86\u5355\u72ec\u9ad8\u65af\u96c6\u5408\u7684\u7ba1\u7406\uff0c\u5728\u5feb\u901f\u8fd0\u52a8\u573a\u666f\u4e0b\u80fd\u591f\u6709\u6548\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2509.18571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "AI": {"tldr": "Live-E2T\u662f\u4e00\u4e2a\u5b9e\u65f6\u5a01\u80c1\u76d1\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u4e49\u5143\u7ec4\u3001\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u673a\u5236\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u6027\u80fd\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u7684\u8981\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u65b0\u6846\u67b6\u3002", "method": "1. \u5c06\u89c6\u9891\u5e27\u89e3\u6784\u4e3a\u7ed3\u6784\u5316\u7684\u4eba-\u7269-\u4ea4\u4e92-\u5730\u70b9\u8bed\u4e49\u5143\u7ec4\uff1b2. \u63d0\u51fa\u9ad8\u6548\u7684\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u548c\u66f4\u65b0\u673a\u5236\uff1b3. \u4f7f\u7528\u601d\u7ef4\u94fe\u7b56\u7565\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u900f\u660e\u63a8\u7406\u3002", "result": "\u5728XD-Violence\u548cUCF-Crime\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLive-E2T\u5728\u5a01\u80c1\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u5b9e\u65f6\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "Live-E2T\u6210\u529f\u89e3\u51b3\u4e86\u5b9e\u65f6\u5a01\u80c1\u76d1\u63a7\u4e2d\u5b9e\u65f6\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18591", "abs": "https://arxiv.org/abs/2509.18591", "authors": ["Pengchao Deng", "Shengqi Chen"], "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network", "comment": null, "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eXMem\u6a21\u578b\u7684\u5b9e\u65f6MRI\u5f15\u5bfc\u653e\u7597\u80bf\u7624\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8eTrackRAD2025\u6311\u6218\u8d5b\uff0c\u80fd\u591f\u5728\u957f\u5e8f\u5217cine-MRI\u4e2d\u5b9e\u65f6\u8ddf\u8e2a\u80bf\u7624\u8fd0\u52a8\u3002", "motivation": "\u63d0\u9ad8MRI\u5f15\u5bfc\u653e\u7597\u4e2d\u80bf\u7624\u8ddf\u8e2a\u7684\u7cbe\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u5347\u764c\u75c7\u6cbb\u7597\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528XMem\u6a21\u578b\u7684\u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff0c\u5728\u957f\u5e8f\u5217cine-MRI\u4e2d\u5206\u5272\u80bf\u7624\uff0c\u901a\u8fc7\u9ad8\u6548\u96c6\u6210\u5185\u5b58\u673a\u5236\u5b9e\u73b0\u5b9e\u65f6\u80bf\u7624\u8fd0\u52a8\u8ddf\u8e2a\u3002", "result": "\u7531\u4e8e\u5b9e\u9a8c\u8bb0\u5f55\u4e22\u5931\uff0c\u65e0\u6cd5\u62a5\u544a\u7cbe\u786e\u7684\u5b9a\u91cf\u7ed3\u679c\uff0c\u4f46\u521d\u6b65\u5f00\u53d1\u5370\u8c61\u663e\u793aXMem\u6846\u67b6\u8868\u73b0\u51fa\u5408\u7406\u7684\u5206\u5272\u6027\u80fd\u5e76\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u65f6\u8981\u6c42\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u6539\u5584MRI\u5f15\u5bfc\u653e\u7597\u4e2d\u7684\u80bf\u7624\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u4e3a\u764c\u75c7\u6cbb\u7597\u7684\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2509.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "PDLS\u662f\u4e00\u79cd\u57fa\u4e8eRectified Flow\u6a21\u578b\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6f5c\u5728\u7a7a\u95f4\u5f15\u5bfc\u89e3\u51b3\u6269\u6563\u6a21\u578b\u56fe\u50cf\u53cd\u6f14\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5355\u6f5c\u5728\u5411\u91cf\u7684\u56fe\u50cf\u53cd\u6f14\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u5982\u7ec6\u8282\u6a21\u7cca\u6216\u5c5e\u6027\u9519\u8bef\u3002", "method": "PDLS\u5c06\u53cd\u6f14\u8fc7\u7a0b\u5206\u89e3\u4e3a\u7ed3\u6784\u8def\u5f84\u548c\u8bed\u4e49\u8def\u5f84\uff0c\u901a\u8fc7\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668(LQR)\u5f62\u6210\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u95ed\u5f0f\u89e3\uff0c\u5728\u751f\u6210\u8f68\u8ff9\u7684\u6bcf\u4e00\u6b65\u52a8\u6001\u5f15\u5bfc\uff0c\u907f\u514d\u8bed\u4e49\u6f02\u79fb\u3002", "result": "\u5728FFHQ-1K\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\u7684\u591a\u79cd\u53cd\u6f14\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0cPDLS\u76f8\u6bd4\u5355\u6f5c\u5728\u57fa\u7ebf\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5fe0\u5b9e\u4e8e\u539f\u56fe\u4e14\u8bed\u4e49\u4fe1\u606f\u66f4\u51c6\u786e\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "PDLS\u6846\u67b6\u901a\u8fc7\u53cc\u6f5c\u5728\u7a7a\u95f4\u5f15\u5bfc\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u56fe\u50cf\u53cd\u6f14\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u7684\u9010\u56fe\u50cf\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002"}}
{"id": "2509.19246", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan O\u011fuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u6301\u4e45\u7f51\u7edc\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u4e3b\u52a8-\u88ab\u52a8\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u57fa\u4e8e\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u591a\u8def\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u3002", "motivation": "\u95f4\u6b47\u6027\u6545\u969c\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6c38\u4e45\u6027\u6545\u969c\u3002\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7cfb\u7edf\uff08SoNS\uff09\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u9996\u6b21\u81ea\u7ec4\u7ec7\u6301\u4e45\u7f51\u7edc\u7ed3\u6784\uff0c\u4e3a\u68c0\u6d4b\u95f4\u6b47\u6027\u6545\u969c\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u91c7\u7528\u4e3b\u52a8-\u88ab\u52a8\u7b56\u7565\uff1a\u4e3b\u52a8\u65b9\u9762\uff0c\u673a\u5668\u4eba\u5728\u6545\u969c\u53d1\u751f\u524d\u81ea\u7ec4\u7ec7\u52a8\u6001\u5907\u4efd\u8def\u5f84\uff1b\u88ab\u52a8\u65b9\u9762\uff0c\u673a\u5668\u4eba\u4f7f\u7528\u4e00\u6b21\u6027\u4f3c\u7136\u6bd4\u68c0\u9a8c\u6bd4\u8f83\u591a\u8def\u7f51\u7edc\u4e2d\u4e0d\u540c\u8def\u5f84\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u3002\u68c0\u6d4b\u5230\u6545\u969c\u540e\uff0c\u901a\u4fe1\u4ee5\u81ea\u7ec4\u7ec7\u65b9\u5f0f\u4e34\u65f6\u91cd\u8def\u7531\u3002", "result": "\u5728\u5f62\u6210\u63a7\u5236\u4e2d\u6545\u969c\u4f4d\u7f6e\u6570\u636e\u7684\u4ee3\u8868\u6027\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u95f4\u6b47\u6027\u6545\u969c\u4e0d\u4f1a\u7834\u574f\u5411\u671f\u671b\u5f62\u6210\u7684\u6536\u655b\uff0c\u5177\u6709\u9ad8\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u7684\u95f4\u6b47\u6027\u6545\u969c\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u5206\u5e03\u5f0f\u5171\u8bc6\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6545\u969c\u68c0\u6d4b\u548c\u7f13\u89e3\u3002"}}
{"id": "2509.19224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u836f\u7269\u548c\u836f\u7269\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u66f4\u6709\u6548\uff0c\u800c\u901a\u7528\u9886\u57df\u9884\u8bad\u7ec3\u7684Bert Base\u5728\u836f\u7269\u76f8\u5173\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u5df2\u6210\u4e3a\u4e34\u5e8a\u7b14\u8bb0\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\u5728EHR\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u54c8\u4f5b\u533b\u5b66\u96622022\u5e74n2c2\u6311\u6218\u8d5bTrack 1\u7684\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86Bert Base\u3001BioBert\u3001\u4e24\u79cdBio+Clinical Bert\u53d8\u4f53\u3001RoBerta\u548cClinical Longformer\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728CMED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6267\u884c\u836f\u7269\u63d0\u53d6\u3001\u533b\u7597\u4e8b\u4ef6\u68c0\u6d4b\u548c\u591a\u7ef4\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86EHR\u5904\u7406\u65b9\u6cd5\u548c\u57fa\u4e8e\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u836f\u7269\u548c\u836f\u7269\u4e8b\u4ef6\u65b9\u9762\u66f4\u6709\u6548\uff0c\u4f46Bert Base\uff08\u5728\u901a\u7528\u9886\u57df\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff09\u5728\u5206\u7c7b\u4e0e\u836f\u7269\u76f8\u5173\u4e8b\u4ef6\u7684\u4e0a\u4e0b\u6587\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u836f\u7269\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u800c\u901a\u7528\u9886\u57df\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u590d\u6742\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\u4e2d\u53ef\u80fd\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u4e3a\u533b\u7597NLP\u4efb\u52a1\u4e2d\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.18386", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18386", "abs": "https://arxiv.org/abs/2509.18386", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "title": "Graph Enhanced Trajectory Anomaly Detection", "comment": null, "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection.", "AI": {"tldr": "\u63d0\u51faGETAD\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cTransformer\u89e3\u7801\u5668\u7ed3\u5408\u9053\u8def\u7f51\u7edc\u62d3\u6251\u3001\u8bed\u4e49\u548c\u5386\u53f2\u6a21\u5f0f\u8fdb\u884c\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\uff0c\u5f15\u5165CW NLL\u8bc4\u5206\u51fd\u6570\u63d0\u9ad8\u68c0\u6d4b\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u8003\u8651\u8f68\u8ff9\u7684\u91c7\u6837\u4f4d\u7f6e\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5e95\u5c42\u79fb\u52a8\u7f51\u7edc\uff08\u5982\u9053\u8def\u7f51\u7edc\uff09\u7684\u7ea6\u675f\u548c\u8fde\u901a\u6027\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u9053\u8def\u7ea6\u675f\u73af\u5883\u4e2d\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73", "method": "\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b66\u4e60\u9053\u8def\u611f\u77e5\u5d4c\u5165\uff0c\u7ed3\u5408\u56fe\u57fa\u4f4d\u7f6e\u7f16\u7801\uff1b\u91c7\u7528Transformer\u89e3\u7801\u5668\u5efa\u6a21\u5e8f\u5217\u79fb\u52a8\uff1b\u8bbe\u8ba1\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u7ed3\u5408\u81ea\u56de\u5f52\u9884\u6d4b\u548c\u76d1\u7763\u94fe\u63a5\u9884\u6d4b\uff1b\u5f15\u5165CW NLL\u5f02\u5e38\u8bc4\u5206\u51fd\u6570", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGETAD\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u9053\u8def\u7ea6\u675f\u73af\u5883\u4e2d\u68c0\u6d4b\u7ec6\u5fae\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "conclusion": "\u5c06\u56fe\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u878d\u5165\u8f68\u8ff9\u5efa\u6a21\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u9053\u8def\u7f51\u7edc\u4fe1\u606f\u7684\u91cd\u8981\u6027"}}
{"id": "2509.18898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18898", "abs": "https://arxiv.org/abs/2509.18898", "authors": ["Pengteng Li", "Yunfan Lu", "Pinhao Song", "Weiyu Guo", "Huizai Yao", "F. Richard Yu", "Hui Xiong"], "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring", "comment": null, "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65e0\u9700Structure-from-Motion\u7684\u8fd0\u52a8\u53bb\u6a21\u7cca3D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5DeblurSplat\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u89e3\u51b3\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56SfM\u8ba1\u7b97\u76f8\u673a\u4f4d\u59ff\uff0c\u4f46\u8fd0\u52a8\u6a21\u7cca\u4f1a\u5bfc\u81f4\u4f4d\u59ff\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u8fdb\u800c\u5f71\u54cd\u70b9\u4e91\u8d28\u91cf\u3002\u4e8b\u4ef6\u76f8\u673a\u5bf9\u52a8\u6001\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\uff0c\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u53bb\u6a21\u7cca\u76d1\u7763\u4fe1\u53f7", "method": "1) \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u7a20\u5bc6\u7acb\u4f53\u6a21\u5757(DUSt3R)\u76f4\u63a5\u4ece\u6a21\u7cca\u56fe\u50cf\u83b7\u53d6\u51c6\u786e\u521d\u59cb\u70b9\u4e91\uff0c\u907f\u514d\u76f8\u673a\u4f4d\u59ff\u7d2f\u79ef\u8bef\u5dee\uff1b2) \u5f15\u5165\u4e8b\u4ef6\u6d41\u5230\u53bb\u6a21\u7cca\u6d41\u7a0b\uff0c\u901a\u8fc7\u89e3\u7801\u4e8b\u4ef6\u6d41\u548c\u6a21\u7cca\u56fe\u50cf\u7684\u6f5c\u5728\u6e05\u6670\u56fe\u50cf\uff0c\u4e3a\u573a\u666f\u91cd\u5efa\u4f18\u5316\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDeblurSplat\u4e0d\u4ec5\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u65b0\u89c6\u89d2\uff0c\u800c\u4e14\u5728\u53bb\u6a21\u7cca3D-GS\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6e32\u67d3\u6548\u7387\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u65e0\u9700SfM\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u7684\u4f18\u52bf\uff0c\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f"}}
{"id": "2509.18842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18842", "abs": "https://arxiv.org/abs/2509.18842", "authors": ["Nikolas Chatzis", "Ioannis Kordonis", "Manos Theodosis", "Petros Maragos"], "title": "Shared-Weights Extender and Gradient Voting for Neural Network Expansion", "comment": "5 pages, 3 figures", "summary": "Expanding neural networks during training is a promising way to augment\ncapacity without retraining larger models from scratch. However, newly added\nneurons often fail to adjust to a trained network and become inactive,\nproviding no contribution to capacity growth. We propose the Shared-Weights\nExtender (SWE), a novel method explicitly designed to prevent inactivity of new\nneurons by coupling them with existing ones for smooth integration. In\nparallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based\nmethod for allocating neurons across layers during deep network expansion. Our\nextensive benchmarking on four datasets shows that our method can effectively\nsuppress neuron inactivity and achieve better performance compared to other\nexpanding methods and baselines.", "AI": {"tldr": "\u63d0\u51faShared-Weights Extender\uff08SWE\uff09\u548cSteepest Voting Distributor\uff08SVoD\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\uff0c\u9632\u6b62\u65b0\u589e\u795e\u7ecf\u5143\u5931\u6548\uff0c\u63d0\u9ad8\u6269\u5c55\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u65b9\u6cd5\u4e2d\uff0c\u65b0\u589e\u795e\u7ecf\u5143\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u878d\u5165\u5df2\u8bad\u7ec3\u7f51\u7edc\u800c\u53d8\u5f97\u4e0d\u6d3b\u8dc3\uff0c\u65e0\u6cd5\u771f\u6b63\u63d0\u5347\u7f51\u7edc\u5bb9\u91cf\u3002", "method": "SWE\u901a\u8fc7\u5c06\u65b0\u589e\u795e\u7ecf\u5143\u4e0e\u73b0\u6709\u795e\u7ecf\u5143\u8026\u5408\u5b9e\u73b0\u5e73\u6ed1\u96c6\u6210\uff1bSVoD\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u795e\u7ecf\u5143\u5206\u914d\u65b9\u6cd5\u5728\u6df1\u5c42\u7f51\u7edc\u6269\u5c55\u4e2d\u5206\u914d\u795e\u7ecf\u5143\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u795e\u7ecf\u5143\u4e0d\u6d3b\u8dc3\u73b0\u8c61\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6269\u5c55\u65b9\u6cd5\u548c\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7f51\u7edc\u52a8\u6001\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bb9\u91cf\u589e\u957f\u3002"}}
{"id": "2509.18949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18949", "abs": "https://arxiv.org/abs/2509.18949", "authors": ["Niccol\u00f2 Rocchi", "Fabio Stella", "Cassio de Campos"], "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach", "comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure", "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u7f6e\u4fe1\u7f51\u7edc\uff08CN\uff09\u4f5c\u4e3a\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u7cca\u5316\u800c\u975e\u6dfb\u52a0\u566a\u58f0\u7684\u65b9\u5f0f\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u516c\u5f00\u53d1\u5e03\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u6a21\u578b\u9700\u8981\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u3002\u73b0\u6709\u7684\u566a\u58f0\u6dfb\u52a0\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5c06\u8d1d\u53f6\u65af\u7f51\u7edc\u8f6c\u6362\u4e3a\u7f6e\u4fe1\u7f51\u7edc\uff0c\u901a\u8fc7\u6a21\u7cca\u5316\u6a21\u578b\u53c2\u6570\u6765\u51cf\u5c11\u8ffd\u8e2a\u653b\u51fb\u7684\u6210\u529f\u6982\u7387\uff0c\u540c\u65f6\u8bc6\u522b\u9700\u8981\u9690\u85cf\u7684\u5173\u952e\u5b66\u4e60\u4fe1\u606f\u4ee5\u9632\u6b62\u653b\u51fb\u8005\u6062\u590d\u539f\u59cbBN\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u8c03\u6574CN\u8d85\u53c2\u6570\u53ef\u4ee5\u8c03\u8282\u9690\u79c1\u589e\u76ca\uff0cCN\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u63a8\u7406\u3002", "conclusion": "\u7f6e\u4fe1\u7f51\u7edc\u4e3a\u5f00\u53d1\u9690\u79c1\u611f\u77e5\u7684\u6982\u7387\u56fe\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.19156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19156", "abs": "https://arxiv.org/abs/2509.19156", "authors": ["Maurf Hassan", "Steven Davy", "Muhammad Zawish", "Owais Bin Zuber", "Nouman Ashraf"], "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit", "comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore", "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.", "AI": {"tldr": "NeuCODEX\u662f\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u534f\u540c\u63a8\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u548c\u8fb9\u7f18\u80fd\u8017\uff0c\u540c\u65f6\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9ad8\u6548SNN\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u63a8\u7406\u65f6\u56e0\u56fa\u5b9a\u9ad8\u65f6\u95f4\u6b65\u5f00\u9500\u5bfc\u81f4\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u95ee\u9898\uff0c\u4ee5\u53ca\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u7cfb\u7edf\u4e2d\u9ad8\u5ef6\u8fdf\u548c\u7279\u5f81\u4f20\u8f93\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5b66\u4e60\u578b\u8109\u51b2\u9a71\u52a8\u538b\u7f29\u6a21\u5757\u51cf\u5c11\u6570\u636e\u4f20\u8f93\uff0c\u91c7\u7528\u52a8\u6001\u63d0\u524d\u9000\u51fa\u673a\u5236\u6839\u636e\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u7ec8\u6b62\u63a8\u7406\uff0c\u5728\u771f\u5b9e\u8fb9\u7f18\u5230\u4e91\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u57fa\u4e8eResNet-18\u548cVGG-16\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u539f\u578b\u9a8c\u8bc1\u3002", "result": "\u6570\u636e\u4f20\u8f93\u51cf\u5c11\u9ad8\u8fbe2048\u500d\uff0c\u8fb9\u7f18\u80fd\u8017\u964d\u4f4e\u8d85\u8fc790%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u76f8\u6bd4\u4ec5\u8fb9\u7f18\u63a8\u7406\u964d\u4f4e3\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u5c0f\u4e8e2%\u3002", "conclusion": "NeuCODEX\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u7528\u3001\u9ad8\u6027\u80fd\u7684SNN\u90e8\u7f72\u3002"}}
{"id": "2509.19112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19112", "abs": "https://arxiv.org/abs/2509.19112", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation", "comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic\n  Inference and Generative Modeling", "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.", "AI": {"tldr": "CARGO\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u6807\u7b7e\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u7a00\u758f\u9ad8\u7ef4\u4e8b\u4ef6\u5e8f\u5217\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u56e0\u679cTransformer\u548c\u81ea\u9002\u5e94\u9891\u7387\u878d\u5408\u6765\u63a8\u65ad\u56e0\u679c\u56fe\u3002", "motivation": "\u7406\u89e3\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u5bf9\u4e8e\u533b\u7597\u4fdd\u5065\u3001\u8f66\u8f86\u8bca\u65ad\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u56e0\u679cTransformer\u4f5c\u4e3a\u9886\u57df\u7279\u5b9a\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u884c\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u56e0\u679c\u56fe\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u878d\u5408\u805a\u5408\u6765\u91cd\u5efa\u6807\u7b7e\u7684\u5168\u5c40\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\u3002", "result": "\u5728\u5305\u542b29,100\u4e2a\u552f\u4e00\u4e8b\u4ef6\u7c7b\u578b\u548c474\u4e2a\u4e0d\u5e73\u8861\u6807\u7b7e\u7684\u771f\u5b9e\u4e16\u754c\u6c7d\u8f66\u6545\u969c\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cCARGO\u5c55\u73b0\u51fa\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CARGO\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u80fd\u591f\u5728\u89c4\u6a21\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u6982\u7387\u63a8\u7406\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5168\u6570\u636e\u96c6\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u4e0d\u53ef\u884c\u6210\u672c\u3002"}}
{"id": "2509.19230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19230", "abs": "https://arxiv.org/abs/2509.19230", "authors": ["Tianshuo Zhang", "Li Gao", "Siran Peng", "Xiangyu Zhu", "Zhen Lei"], "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces", "comment": "Accepted by NeurIPS 2025", "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u53d1\u6027\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u6765\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u4f2a\u9020\u6280\u672f\uff0c\u4f7f\u7528LoRA\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u6765\u5b66\u4e60\u771f\u5b9e\u9762\u90e8\u548c\u4e0d\u540c\u4f2a\u9020\u7c7b\u578b\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u9762\u90e8\u751f\u6210\u548c\u64cd\u7eb5\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u8ddf\u4e0a\u4e0d\u65ad\u6f14\u53d8\u7684\u4f2a\u9020\u6280\u672f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u9886\u57df\u3001\u907f\u514d\u9057\u5fd8\u5df2\u5b66\u77e5\u8bc6\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f00\u53d1\u6027\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u4f7f\u7528LoRA\u6a21\u578b\u4f5c\u4e3a\u4e2a\u4f53\u4e13\u5bb6\u3002\u4e13\u5bb6\u5206\u4e3a\u4e24\u7ec4\uff1aReal-LoRA\u5b66\u4e60\u771f\u5b9e\u9762\u90e8\u77e5\u8bc6\uff0c\u591a\u4e2aFake-LoRA\u6355\u83b7\u4e0d\u540c\u4f2a\u9020\u7c7b\u578b\u7684\u589e\u91cf\u4fe1\u606f\u3002\u901a\u8fc7\u6b63\u4ea4\u5b66\u4e60\u65b9\u5411\u548c\u6b63\u4ea4\u68af\u5ea6\u6765\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u6570\u636e\u96c6\u548c\u64cd\u7eb5\u7c7b\u578b\u589e\u91cf\u534f\u8bae\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u4f2a\u9020\u7c7b\u578b\u800c\u4e0d\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u6784\u5efa\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u5f00\u53d1\u6027MoE\u67b6\u6784\u5b9e\u73b0\u4e86\u5bf9\u65b0\u4f2a\u9020\u7c7b\u578b\u7684\u5feb\u901f\u9002\u5e94\u548c\u5bf9\u5df2\u5b66\u77e5\u8bc6\u7684\u4fdd\u6301\u3002"}}
{"id": "2509.19135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19135", "abs": "https://arxiv.org/abs/2509.19135", "authors": ["Wenying Luo", "Zhiyuan Lin", "Wenhao Xu", "Minghao Liu", "Zhi Li"], "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding", "comment": null, "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence.", "AI": {"tldr": "GSTM-HMU\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u65f6\u7a7a\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4eba\u7c7b\u79fb\u52a8\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u590d\u6742\u6027\u6765\u63a8\u8fdb\u79fb\u52a8\u6027\u5206\u6790\u3002\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u65f6\u7a7a\u6982\u5ff5\u7f16\u7801\u5668\u3001\u8ba4\u77e5\u8f68\u8ff9\u8bb0\u5fc6\u3001\u751f\u6d3b\u65b9\u5f0f\u6982\u5ff5\u5e93\u548c\u4efb\u52a1\u5bfc\u5411\u751f\u6210\u5934\u3002\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e09\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\u8bb0\u5f55\u4e86\u77ed\u671f\u8bbf\u95ee\u6a21\u5f0f\u548c\u6301\u4e45\u751f\u6d3b\u65b9\u5f0f\u89c4\u5f8b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u79fb\u52a8\u6570\u636e\u4e2d\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u590d\u6742\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u79fb\u52a8\u610f\u56fe\u548c\u504f\u597d\u7684\u751f\u6210\u5f0f\u6846\u67b6\u3002", "method": "\u63d0\u51faGSTM-HMU\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09STCE\u7f16\u7801\u5668\u6574\u5408\u5730\u7406\u4f4d\u7f6e\u3001POI\u7c7b\u522b\u8bed\u4e49\u548c\u5468\u671f\u6027\u65f6\u95f4\u8282\u594f\uff1b2\uff09CTM\u8bb0\u5fc6\u6a21\u5757\u81ea\u9002\u5e94\u8fc7\u6ee4\u5386\u53f2\u8bbf\u95ee\uff0c\u5f3a\u8c03\u8fd1\u671f\u548c\u884c\u4e3a\u663e\u8457\u4e8b\u4ef6\uff1b3\uff09LCB\u6982\u5ff5\u5e93\u63d0\u4f9b\u7ed3\u6784\u5316\u4eba\u7c7b\u504f\u597d\u7ebf\u7d22\uff1b4\uff09\u4efb\u52a1\u5bfc\u5411\u751f\u6210\u5934\u5c06\u5b66\u4e60\u8868\u793a\u8f6c\u5316\u4e3a\u591a\u4efb\u52a1\u9884\u6d4b\u3002", "result": "\u5728Gowalla\u3001WeePlace\u3001Brightkite\u548cFourSquare\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cGSTM-HMU\u5728\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u9884\u6d4b\u3001\u8f68\u8ff9\u7528\u6237\u8bc6\u522b\u548c\u65f6\u95f4\u4f30\u8ba1\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u751f\u6210\u5f0f\u5efa\u6a21\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u4eba\u7c7b\u79fb\u52a8\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002GSTM-HMU\u6846\u67b6\u80fd\u591f\u6709\u6548\u4ece\u590d\u6742\u79fb\u52a8\u6570\u636e\u4e2d\u63d0\u53d6\u8bed\u4e49\u89c4\u5f8b\u6027\uff0c\u5c55\u793a\u4e86\u5728\u79fb\u52a8\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
