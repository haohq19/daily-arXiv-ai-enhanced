{"id": "2510.26969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26969", "abs": "https://arxiv.org/abs/2510.26969", "authors": ["Lívia Dutra", "Arthur Lorenzi", "Laís Berno", "Franciany Campos", "Karoline Biscardi", "Kenneth Brown", "Marcelo Viridiano", "Frederico Belcavello", "Ely Matos", "Olívia Guaranha", "Erik Santos", "Sofia Reinach", "Tiago Timponi Torrent"], "title": "Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence", "comment": null, "summary": "We introduce a methodology for the identification of notifiable events in the\ndomain of healthcare. The methodology harnesses semantic frames to define\nfine-grained patterns and search them in unstructured data, namely, open-text\nfields in e-medical records. We apply the methodology to the problem of\nunderreporting of gender-based violence (GBV) in e-medical records produced\nduring patients' visits to primary care units. A total of eight patterns are\ndefined and searched on a corpus of 21 million sentences in Brazilian\nPortuguese extracted from e-SUS APS. The results are manually evaluated by\nlinguists and the precision of each pattern measured. Our findings reveal that\nthe methodology effectively identifies reports of violence with a precision of\n0.726, confirming its robustness. Designed as a transparent, efficient,\nlow-carbon, and language-agnostic pipeline, the approach can be easily adapted\nto other health surveillance contexts, contributing to the broader, ethical,\nand explainable use of NLP in public health systems."}
{"id": "2510.26910", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26910", "abs": "https://arxiv.org/abs/2510.26910", "authors": ["Kshitij Nikhal", "Luke Ackerknecht", "Benjamin S. Riggan", "Phil Stahlfeld"], "title": "Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study", "comment": "Tackling Climate Change with Machine Learning: Workshop at NeurIPS\n  2025", "summary": "The decarbonization of transportation relies on the widespread adoption of\nelectric vehicles (EVs), which requires an accurate understanding of charging\nbehavior to ensure cost-effective, grid-resilient infrastructure. Existing work\nis constrained by small-scale datasets, simple proximity-based modeling of\ntemporal dependencies, and weak generalization to sites with limited\noperational history. To overcome these limitations, this work proposes a\nframework that integrates clustering with few-shot forecasting to uncover site\narchetypes using a novel large-scale dataset of charging demand. The results\ndemonstrate that archetype-specific expert models outperform global baselines\nin forecasting demand at unseen sites. By establishing forecast performance as\na basis for infrastructure segmentation, we generate actionable insights that\nenable operators to lower costs, optimize energy and pricing strategies, and\nsupport grid resilience critical to climate goals."}
{"id": "2510.27333", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27333", "abs": "https://arxiv.org/abs/2510.27333", "authors": ["Hao Cheng", "Yanbo Jiang", "Qingyuan Shi", "Qingwen Meng", "Keyu Chen", "Wenhao Yu", "Jianqiang Wang", "Sifa Zheng"], "title": "Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict", "comment": null, "summary": "Effective, reliable, and efficient evaluation of autonomous driving safety is\nessential to demonstrate its trustworthiness. Criticality metrics provide an\nobjective means of assessing safety. However, as existing metrics primarily\ntarget longitudinal conflicts, accurately quantifying the risks of lateral\nconflicts - prevalent in urban settings - remains challenging. This paper\nproposes the Modified-Emergency Index (MEI), a metric designed to quantify\nevasive effort in lateral conflicts. Compared to the original Emergency Index\n(EI), MEI refines the estimation of the time available for evasive maneuvers,\nenabling more precise risk quantification. We validate MEI on a public lateral\nconflict dataset based on Argoverse-2, from which we extract over 1,500\nhigh-quality AV conflict cases, including more than 500 critical events. MEI is\nthen compared with the well-established ACT and the widely used PET metrics.\nResults show that MEI consistently outperforms them in accurately quantifying\ncriticality and capturing risk evolution. Overall, these findings highlight MEI\nas a promising metric for evaluating urban conflicts and enhancing the safety\nassessment framework for autonomous driving. The open-source implementation is\navailable at https://github.com/AutoChengh/MEI."}
{"id": "2510.27087", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.27087", "abs": "https://arxiv.org/abs/2510.27087", "authors": ["Adel Khorramrouz", "Sharon Levy"], "title": "Characterizing Selective Refusal Bias in Large Language Models", "comment": "21 pages, 12 figures, 14 tables", "summary": "Safety guardrails in large language models(LLMs) are developed to prevent\nmalicious users from generating toxic content at a large scale. However, these\nmeasures can inadvertently introduce or reflect new biases, as LLMs may refuse\nto generate harmful content targeting some demographic groups and not others.\nWe explore this selective refusal bias in LLM guardrails through the lens of\nrefusal rates of targeted individual and intersectional demographic groups,\ntypes of LLM responses, and length of generated refusals. Our results show\nevidence of selective refusal bias across gender, sexual orientation,\nnationality, and religion attributes. This leads us to investigate additional\nsafety implications via an indirect attack, where we target previously refused\ngroups. Our findings emphasize the need for more equitable and robust\nperformance in safety guardrails across demographic groups."}
{"id": "2510.27343", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27343", "abs": "https://arxiv.org/abs/2510.27343", "authors": ["Ali Norouzifar", "Wil van der Aalst"], "title": "Discriminative Rule Learning for Outcome-Guided Process Model Discovery", "comment": "The paper will be published as part of the CoopIS 2025 conference\n  proceedings", "summary": "Event logs extracted from information systems offer a rich foundation for\nunderstanding and improving business processes. In many real-world\napplications, it is possible to distinguish between desirable and undesirable\nprocess executions, where desirable traces reflect efficient or compliant\nbehavior, and undesirable ones may involve inefficiencies, rule violations,\ndelays, or resource waste. This distinction presents an opportunity to guide\nprocess discovery in a more outcome-aware manner. Discovering a single process\nmodel without considering outcomes can yield representations poorly suited for\nconformance checking and performance analysis, as they fail to capture critical\nbehavioral differences. Moreover, prioritizing one behavior over the other may\nobscure structural distinctions vital for understanding process outcomes. By\nlearning interpretable discriminative rules over control-flow features, we\ngroup traces with similar desirability profiles and apply process discovery\nseparately within each group. This results in focused and interpretable models\nthat reveal the drivers of both desirable and undesirable executions. The\napproach is implemented as a publicly available tool and it is evaluated on\nmultiple real-life event logs, demonstrating its effectiveness in isolating and\nvisualizing critical process patterns."}
{"id": "2510.27015", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.27015", "abs": "https://arxiv.org/abs/2510.27015", "authors": ["Zachary Izzo", "Eshaan Nichani", "Jason D. Lee"], "title": "Quantitative Bounds for Length Generalization in Transformers", "comment": "Equal contribution, order determined by coin flip", "summary": "We study the problem of length generalization (LG) in transformers: the\nability of a model trained on shorter sequences to maintain performance when\nevaluated on much longer, previously unseen inputs. Prior work by Huang et al.\n(2025) established that transformers eventually achieve length generalization\nonce the training sequence length exceeds some finite threshold, but left open\nthe question of how large it must be. In this work, we provide the first\nquantitative bounds on the required training length for length generalization\nto occur. Motivated by previous empirical and theoretical work, we analyze LG\nin several distinct problem settings: $\\ell_\\infty$ error control vs. average\nerror control over an input distribution, infinite-precision softmax attention\nvs. finite-precision attention (which reduces to an argmax) in the transformer,\nand one- vs. two-layer transformers. In all scenarios, we prove that LG occurs\nwhen the internal behavior of the transformer on longer sequences can be\n\"simulated\" by its behavior on shorter sequences seen during training. Our\nbounds give qualitative estimates for the length of training data required for\na transformer to generalize, and we verify these insights empirically. These\nresults sharpen our theoretical understanding of the mechanisms underlying\nextrapolation in transformers, and formalize the intuition that richer training\ndata is required for generalization on more complex tasks."}
{"id": "2510.27630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27630", "abs": "https://arxiv.org/abs/2510.27630", "authors": ["Dayuan Fu", "Yunze Wu", "Xiaojie Cai", "Lyumanshan Ye", "Shijie Xia", "Zhen Huang", "Weiye Si", "Tianze Xu", "Jie Sun", "Keyu Li", "Mohan Jiang", "Junfei Wang", "Qishuo Hua", "Pengrui Lu", "Yang Xiao", "Pengfei Liu"], "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training", "comment": null, "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks."}
{"id": "2510.27145", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.27145", "abs": "https://arxiv.org/abs/2510.27145", "authors": ["Sein Kwon", "Seulgi Baek", "Hyunseo Yang", "Youngwan Jo", "Sanghyun Park"], "title": "Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores", "comment": "13 pages", "summary": "Database Management Systems (DBMSs) are fundamental for managing large-scale\nand heterogeneous data, and their performance is critically influenced by\nconfiguration parameters. Effective tuning of these parameters is essential for\nadapting to diverse workloads and maximizing throughput while minimizing\nlatency. Recent research has focused on automated configuration optimization\nusing machine learning; however, existing approaches still exhibit several key\nlimitations. Most tuning frameworks disregard the dependencies among\nparameters, assuming that each operates independently. This simplification\nprevents optimizers from leveraging relational effects across parameters,\nlimiting their capacity to capture performancesensitive interactions. Moreover,\nto reduce the complexity of the high-dimensional search space, prior work often\nselects only the top few parameters for optimization, overlooking others that\ncontribute meaningfully to performance. Bayesian Optimization (BO), the most\ncommon method for automatic tuning, is also constrained by its reliance on\nsurrogate models, which can lead to unstable predictions and inefficient\nexploration. To overcome these limitations, we propose RelTune, a novel\nframework that represents parameter dependencies as a Relational Graph and\nlearns GNN-based latent embeddings that encode performancerelevant semantics.\nRelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),\nwhich combines surrogate predictions with an Affinity Score measuring proximity\nto previously high-performing configurations. Experimental results on multiple\nDBMSs and workloads demonstrate that RelTune achieves faster convergence and\nhigher optimization efficiency than conventional BO-based methods, achieving\nstate-of-the-art performance across all evaluated scenarios."}
{"id": "2510.26969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26969", "abs": "https://arxiv.org/abs/2510.26969", "authors": ["Lívia Dutra", "Arthur Lorenzi", "Laís Berno", "Franciany Campos", "Karoline Biscardi", "Kenneth Brown", "Marcelo Viridiano", "Frederico Belcavello", "Ely Matos", "Olívia Guaranha", "Erik Santos", "Sofia Reinach", "Tiago Timponi Torrent"], "title": "Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence", "comment": null, "summary": "We introduce a methodology for the identification of notifiable events in the\ndomain of healthcare. The methodology harnesses semantic frames to define\nfine-grained patterns and search them in unstructured data, namely, open-text\nfields in e-medical records. We apply the methodology to the problem of\nunderreporting of gender-based violence (GBV) in e-medical records produced\nduring patients' visits to primary care units. A total of eight patterns are\ndefined and searched on a corpus of 21 million sentences in Brazilian\nPortuguese extracted from e-SUS APS. The results are manually evaluated by\nlinguists and the precision of each pattern measured. Our findings reveal that\nthe methodology effectively identifies reports of violence with a precision of\n0.726, confirming its robustness. Designed as a transparent, efficient,\nlow-carbon, and language-agnostic pipeline, the approach can be easily adapted\nto other health surveillance contexts, contributing to the broader, ethical,\nand explainable use of NLP in public health systems."}
{"id": "2510.27316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.27316", "abs": "https://arxiv.org/abs/2510.27316", "authors": ["Zijia An", "Boyu Diao", "Ruiqi Liu", "Libo Huang", "Chuanguang Yang", "Fei Wang", "Zhulin An", "Yongjun Xu"], "title": "Overcoming Prompts Pool Confusion via Parameterized Prompt for Incremental Object Detection", "comment": null, "summary": "Recent studies have demonstrated that incorporating trainable prompts into\npretrained models enables effective incremental learning. However, the\napplication of prompts in incremental object detection (IOD) remains\nunderexplored. Existing prompts pool based approaches assume disjoint class\nsets across incremental tasks, which are unsuitable for IOD as they overlook\nthe inherent co-occurrence phenomenon in detection images. In co-occurring\nscenarios, unlabeled objects from previous tasks may appear in current task\nimages, leading to confusion in prompts pool. In this paper, we hold that\nprompt structures should exhibit adaptive consolidation properties across\ntasks, with constrained updates to prevent catastrophic forgetting. Motivated\nby this, we introduce Parameterized Prompts for Incremental Object Detection\n(P$^2$IOD). Leveraging neural networks global evolution properties, P$^2$IOD\nemploys networks as the parameterized prompts to adaptively consolidate\nknowledge across tasks. To constrain prompts structure updates, P$^2$IOD\nfurther engages a parameterized prompts fusion strategy. Extensive experiments\non PASCAL VOC2007 and MS COCO datasets demonstrate that P$^2$IOD's\neffectiveness in IOD and achieves the state-of-the-art performance among\nexisting baselines."}
{"id": "2510.27432", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27432", "abs": "https://arxiv.org/abs/2510.27432", "authors": ["WonJun Moon", "MinSeok Jung", "Gilhan Park", "Tae-Young Kim", "Cheol-Ho Cho", "Woojin Jun", "Jae-Pil Heo"], "title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval", "comment": "Accpeted to NeurIPS 2025. Code is available at\n  https://github.com/admins97/MSC_PRVR", "summary": "Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the\ncontent matches a text query. Existing methods treat every annotated text-video\npair as a positive and all others as negatives, ignoring the rich semantic\nvariation both within a single video and across different videos. Consequently,\nembeddings of both queries and their corresponding video-clip segments for\ndistinct events within the same video collapse together, while embeddings of\nsemantically similar queries and segments from different videos are driven\napart. This limits retrieval performance when videos contain multiple, diverse\nevents. This paper addresses the aforementioned problems, termed as semantic\ncollapse, in both the text and video embedding spaces. We first introduce Text\nCorrelation Preservation Learning, which preserves the semantic relationships\nencoded by the foundation model across text queries. To address collapse in\nvideo embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive\nalignment method that disentangles hierarchical video representations across\ntemporal scales. Subsequently, we introduce order-preserving token merging and\nadaptive CBVA to enhance alignment by producing video segments that are\ninternally coherent yet mutually distinctive. Extensive experiments on PRVR\nbenchmarks demonstrate that our framework effectively prevents semantic\ncollapse and substantially improves retrieval accuracy."}
{"id": "2510.27443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27443", "abs": "https://arxiv.org/abs/2510.27443", "authors": ["Meenu Ravi", "Shailik Sarkar", "Yanshen Sun", "Vaishnavi Singh", "Chang-Tien Lu"], "title": "MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss", "comment": "Accepted for 2025 ACM SIGSPATIAL conference", "summary": "Understanding post-wildfire vegetation loss is critical for developing\neffective ecological recovery strategies and is often challenging due to the\nextended time and effort required to capture the evolving ecosystem features.\nRecent works in this area have not fully explored all the contributing factors,\ntheir modalities, and interactions with each other. Furthermore, most research\nin this domain is limited by a lack of interpretability in predictive modeling,\nmaking it less useful in real-world settings. In this work, we propose a novel\nend-to-end ML pipeline called MVeLMA (\\textbf{M}ultimodal \\textbf{Ve}getation\n\\textbf{L}oss \\textbf{M}odeling \\textbf{A}rchitecture) to predict county-wise\nvegetation loss from fire events. MVeLMA uses a multimodal feature integration\npipeline and a stacked ensemble-based architecture to capture different\nmodalities while also incorporating uncertainty estimation through\nprobabilistic modeling. Through comprehensive experiments, we show that our\nmodel outperforms several state-of-the-art (SOTA) and baseline models in\npredicting post-wildfire vegetation loss. Furthermore, we generate vegetation\nloss confidence maps to identify high-risk counties, thereby helping targeted\nrecovery efforts. The findings of this work have the potential to inform future\ndisaster relief planning, ecological policy development, and wildlife recovery\nmanagement."}
{"id": "2510.27484", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.27484", "abs": "https://arxiv.org/abs/2510.27484", "authors": ["Uzay Macar", "Paul C. Bogdan", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling", "comment": "Uzay Macar and Paul C. Bogdan contributed equally to this work, and\n  their listed order was determined by coinflip", "summary": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions."}
{"id": "2510.27484", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.27484", "abs": "https://arxiv.org/abs/2510.27484", "authors": ["Uzay Macar", "Paul C. Bogdan", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling", "comment": "Uzay Macar and Paul C. Bogdan contributed equally to this work, and\n  their listed order was determined by coinflip", "summary": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions."}
{"id": "2510.27504", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27504", "abs": "https://arxiv.org/abs/2510.27504", "authors": ["Junkang Liu", "Yuxuan Tian", "Fanhua Shang", "Yuanyuan Liu", "Hongying Liu", "Junchao Zhou", "Daorui Ding"], "title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm", "comment": "21 pages, 8 figures", "summary": "To prevent inference attacks in Federated Learning (FL) and reduce the\nleakage of sensitive information, Client-level Differentially Private Federated\nLearning (CL-DPFL) is widely used. However, current CL-DPFL methods usually\nresult in sharper loss landscapes, which leads to a decrease in model\ngeneralization after differential privacy protection. By using Sharpness Aware\nMinimization (SAM), the current popular federated learning methods are to find\na local flat minimum value to alleviate this problem. However, the local\nflatness may not reflect the global flatness in CL-DPFL. Therefore, to address\nthis issue and seek global flat minima of models, we propose a new CL-DPFL\nalgorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to\nthe local loss to find the global flat minimum. Moreover, by using our global\ngradient norm penalty, we not only find a flatter global minimum but also\nreduce the locally updated norm, which means that we further reduce the error\nof gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN\nmitigates the performance degradation caused by DP. Meanwhile, the proposed\nDP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves\nfast convergence. We also use R\\'enyi DP to provide strict privacy guarantees\nand provide sensitivity analysis for local updates. Finally, we conduct\neffectiveness tests on both ResNet and Transformer models, and achieve\nsignificant improvements in six visual and natural language processing tasks\ncompared to existing state-of-the-art algorithms. The code is available at\nhttps://github.com/junkangLiu0/DP-FedPGN"}
{"id": "2510.27659", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.27659", "abs": "https://arxiv.org/abs/2510.27659", "authors": ["Alireza Saleh Abadi", "Leen-Kiat Soh"], "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems", "comment": null, "summary": "In the rapidly evolving field of multi-agent reinforcement learning (MARL),\nunderstanding the dynamics of open systems is crucial. Openness in MARL refers\nto the dynam-ic nature of agent populations, tasks, and agent types with-in a\nsystem. Specifically, there are three types of openness as reported in (Eck et\nal. 2023) [2]: agent openness, where agents can enter or leave the system at\nany time; task openness, where new tasks emerge, and existing ones evolve or\ndisappear; and type openness, where the capabil-ities and behaviors of agents\nchange over time. This report provides a conceptual and empirical review,\nfocusing on the interplay between openness and the credit assignment problem\n(CAP). CAP involves determining the contribution of individual agents to the\noverall system performance, a task that becomes increasingly complex in open\nenviron-ments. Traditional credit assignment (CA) methods often assume static\nagent populations, fixed and pre-defined tasks, and stationary types, making\nthem inadequate for open systems. We first conduct a conceptual analysis,\nin-troducing new sub-categories of openness to detail how events like agent\nturnover or task cancellation break the assumptions of environmental\nstationarity and fixed team composition that underpin existing CAP methods. We\nthen present an empirical study using representative temporal and structural\nalgorithms in an open environment. The results demonstrate that openness\ndirectly causes credit misattribution, evidenced by unstable loss functions and\nsignificant performance degradation."}
{"id": "2510.27432", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27432", "abs": "https://arxiv.org/abs/2510.27432", "authors": ["WonJun Moon", "MinSeok Jung", "Gilhan Park", "Tae-Young Kim", "Cheol-Ho Cho", "Woojin Jun", "Jae-Pil Heo"], "title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval", "comment": "Accpeted to NeurIPS 2025. Code is available at\n  https://github.com/admins97/MSC_PRVR", "summary": "Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the\ncontent matches a text query. Existing methods treat every annotated text-video\npair as a positive and all others as negatives, ignoring the rich semantic\nvariation both within a single video and across different videos. Consequently,\nembeddings of both queries and their corresponding video-clip segments for\ndistinct events within the same video collapse together, while embeddings of\nsemantically similar queries and segments from different videos are driven\napart. This limits retrieval performance when videos contain multiple, diverse\nevents. This paper addresses the aforementioned problems, termed as semantic\ncollapse, in both the text and video embedding spaces. We first introduce Text\nCorrelation Preservation Learning, which preserves the semantic relationships\nencoded by the foundation model across text queries. To address collapse in\nvideo embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive\nalignment method that disentangles hierarchical video representations across\ntemporal scales. Subsequently, we introduce order-preserving token merging and\nadaptive CBVA to enhance alignment by producing video segments that are\ninternally coherent yet mutually distinctive. Extensive experiments on PRVR\nbenchmarks demonstrate that our framework effectively prevents semantic\ncollapse and substantially improves retrieval accuracy."}
{"id": "2510.27484", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.27484", "abs": "https://arxiv.org/abs/2510.27484", "authors": ["Uzay Macar", "Paul C. Bogdan", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling", "comment": "Uzay Macar and Paul C. Bogdan contributed equally to this work, and\n  their listed order was determined by coinflip", "summary": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions."}
{"id": "2510.27504", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27504", "abs": "https://arxiv.org/abs/2510.27504", "authors": ["Junkang Liu", "Yuxuan Tian", "Fanhua Shang", "Yuanyuan Liu", "Hongying Liu", "Junchao Zhou", "Daorui Ding"], "title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm", "comment": "21 pages, 8 figures", "summary": "To prevent inference attacks in Federated Learning (FL) and reduce the\nleakage of sensitive information, Client-level Differentially Private Federated\nLearning (CL-DPFL) is widely used. However, current CL-DPFL methods usually\nresult in sharper loss landscapes, which leads to a decrease in model\ngeneralization after differential privacy protection. By using Sharpness Aware\nMinimization (SAM), the current popular federated learning methods are to find\na local flat minimum value to alleviate this problem. However, the local\nflatness may not reflect the global flatness in CL-DPFL. Therefore, to address\nthis issue and seek global flat minima of models, we propose a new CL-DPFL\nalgorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to\nthe local loss to find the global flat minimum. Moreover, by using our global\ngradient norm penalty, we not only find a flatter global minimum but also\nreduce the locally updated norm, which means that we further reduce the error\nof gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN\nmitigates the performance degradation caused by DP. Meanwhile, the proposed\nDP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves\nfast convergence. We also use R\\'enyi DP to provide strict privacy guarantees\nand provide sensitivity analysis for local updates. Finally, we conduct\neffectiveness tests on both ResNet and Transformer models, and achieve\nsignificant improvements in six visual and natural language processing tasks\ncompared to existing state-of-the-art algorithms. The code is available at\nhttps://github.com/junkangLiu0/DP-FedPGN"}
{"id": "2510.27659", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.27659", "abs": "https://arxiv.org/abs/2510.27659", "authors": ["Alireza Saleh Abadi", "Leen-Kiat Soh"], "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems", "comment": null, "summary": "In the rapidly evolving field of multi-agent reinforcement learning (MARL),\nunderstanding the dynamics of open systems is crucial. Openness in MARL refers\nto the dynam-ic nature of agent populations, tasks, and agent types with-in a\nsystem. Specifically, there are three types of openness as reported in (Eck et\nal. 2023) [2]: agent openness, where agents can enter or leave the system at\nany time; task openness, where new tasks emerge, and existing ones evolve or\ndisappear; and type openness, where the capabil-ities and behaviors of agents\nchange over time. This report provides a conceptual and empirical review,\nfocusing on the interplay between openness and the credit assignment problem\n(CAP). CAP involves determining the contribution of individual agents to the\noverall system performance, a task that becomes increasingly complex in open\nenviron-ments. Traditional credit assignment (CA) methods often assume static\nagent populations, fixed and pre-defined tasks, and stationary types, making\nthem inadequate for open systems. We first conduct a conceptual analysis,\nin-troducing new sub-categories of openness to detail how events like agent\nturnover or task cancellation break the assumptions of environmental\nstationarity and fixed team composition that underpin existing CAP methods. We\nthen present an empirical study using representative temporal and structural\nalgorithms in an open environment. The results demonstrate that openness\ndirectly causes credit misattribution, evidenced by unstable loss functions and\nsignificant performance degradation."}
