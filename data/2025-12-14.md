<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 该论文提出了一种基于脉冲神经网络（SNN）的神经形态眼动追踪模型，通过将高性能事件驱动眼动追踪模型中的循环和注意力模块替换为轻量级LIF层，并利用深度可分离卷积降低模型复杂度，在保持精度的同时大幅提升了能效。


<details>
  <summary>Details</summary>
Motivation: 可穿戴系统的眼动追踪需要低延迟和毫瓦级功耗，但传统基于帧的管道存在运动模糊、计算成本高和时间分辨率有限的问题。神经形态传感器和脉冲神经网络提供了有前景的替代方案，但现有SNN方法要么过于专门化，要么性能不及现代ANN架构。

Method: 提出了神经形态版本的高性能事件驱动眼动追踪模型，将原有模型的循环和注意力模块替换为轻量级LIF（泄漏积分发放）层，并采用深度可分离卷积来降低模型复杂度，从而在保持精度的同时大幅减少计算需求。

Result: 模型获得了3.7-4.1像素的平均误差，接近专用神经形态系统Retina（3.24像素）的精度，同时相比最接近的ANN变体，模型大小减少了20倍，理论计算量减少了850倍。这些高效变体预计可在3.9-4.9毫瓦功耗下以3毫秒延迟在1千赫兹频率下运行。

Conclusion: 结果表明，高性能事件驱动眼动追踪架构可以重新设计为SNN，在保持适合实时可穿戴部署精度的同时，获得显著的效率提升，为AR/VR等新兴技术中的无缝交互提供了可行的解决方案。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [2] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: ISMVC提出了一种基于信息量的选择性插补方法，通过评估缺失位置的信息量来决定是否插补，结合变分自编码器和混合高斯先验学习聚类友好的潜在表示。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法在处理不完整数据时面临挑战：基于插补的方法可能引入噪声和偏差，而免插补方法在严重不完整情况下缺乏跨视图互补性。

Method: 基于信息量的选择性插补多视图聚类(ISMVC)：1) 基于视图内相似性和跨视图一致性评估每个缺失位置的信息量，仅在信息充足时选择性插补；2) 结合变分自编码器和混合高斯先验学习聚类友好的潜在表示；3) 在分布层面进行插补，稳定后验分布聚合并建模插补不确定性。

Result: 在多个基准数据集上，在更现实和具有挑战性的不平衡缺失场景下，ISMVC优于现有的基于插补和免插补方法。

Conclusion: ISMVC提供了一种轻量级、数据驱动且模型无关的选择性插补策略，可作为插件模块集成到现有不完整多视图聚类模型中，有效平衡插补风险和信息利用。

Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.

</details>


### [3] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出OmniZoo大规模动物运动数据集和通用自回归运动生成框架，能够为任意骨骼拓扑结构生成文本驱动的运动


<details>
  <summary>Details</summary>
Motivation: 当前运动生成方法大多依赖固定的骨骼模板，无法泛化到不同或扰动拓扑结构的骨骼。缺乏大规模异构动物运动数据和能够联合建模任意骨骼拓扑与文本条件的统一生成框架

Method: 1) 引入OmniZoo数据集，涵盖140个物种的32,979个序列，带有多模态标注；2) 提出通用自回归运动生成框架，包含拓扑感知骨骼嵌入模块，将任意骨骼的几何和结构属性编码到共享标记空间，与文本语义无缝融合

Result: 给定文本提示和目标骨骼，方法能够生成时间一致、物理合理且语义对齐的运动，并支持跨物种运动风格迁移

Conclusion: 解决了当前运动生成方法的核心限制，通过大规模数据集和统一框架实现了对任意骨骼拓扑的文本驱动运动生成

Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.

</details>


### [4] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: P2S是一个无需训练的长视频时刻检索框架，通过自适应跨度生成和查询分解技术，在零样本设置下超越了监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 长视频时刻检索面临计算不可行性挑战，现有方法存在搜索阶段候选爆炸和精炼阶段语义差异问题，需要高成本视觉语言模型验证，计算开销大。

Method: 提出P2S框架：1）自适应跨度生成器防止搜索阶段候选爆炸；2）查询分解技术在不依赖高成本VLM验证的情况下精炼候选。

Result: P2S是首个能在小时级长视频中进行时间定位的零样本框架，在MAD数据集上R5@0.1指标超越监督SOTA方法3.7%。

Conclusion: P2S通过创新的训练免费方法解决了长视频时刻检索中的搜索效率低下和精炼成本高昂问题，在零样本设置下实现了优于监督方法的性能。

Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: HB-SGE是一种结合重球动量和预测梯度外推的鲁棒一阶优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定，收敛性更好。


<details>
  <summary>Details</summary>
Motivation: NAG等加速梯度方法在病态条件或非凸问题上容易发散，因为动量积累过于激进。需要一种既能保持加速效果又能在复杂地形上保持稳定的优化方法。

Method: HB-SGE结合了重球动量和预测梯度外推技术。不同于传统动量方法积累历史梯度，它使用局部泰勒近似来估计未来梯度方向，提供自适应加速同时保持稳定性。

Result: 在病态二次问题（条件数κ=50）上，HB-SGE在119次迭代内收敛，而SGD和NAG都发散。在非凸Rosenbrock函数上，HB-SGE在2718次迭代内收敛，而经典动量方法在10步内就发散。HB-SGE在强凸函数上有收敛保证。

Conclusion: 虽然NAG在良好条件问题上更快，但HB-SGE提供了一个鲁棒的替代方案，在多样化地形上比SGD有加速效果，仅需O(d)内存开销，且与标准动量方法使用相同的超参数。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [6] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: CHyLL提出了一种在潜空间中学习混合系统连续神经表示的新方法，无需轨迹分割、事件函数或模式切换，通过将状态空间重构为分段光滑商流形来实现连续流学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法学习每个离散模式的动态，但面临模式切换和流不连续性的组合问题。混合系统具有连续和离散时间动态，学习其流具有挑战性。

Method: CHyLL通过重置映射在守卫表面粘合状态空间，将状态空间重构为分段光滑商流形，使流在空间上连续。基于微分拓扑的嵌入定理，同时学习高维空间中的无奇点神经嵌入和其中的连续流。

Result: CHyLL能够准确预测混合系统的流，具有优越的准确性，并能识别混合系统的拓扑不变量。成功应用于随机最优控制问题。

Conclusion: CHyLL提供了一种无需轨迹分割或模式切换的连续混合系统学习方法，通过将状态空间重构为商流形实现连续流表示，在预测精度和拓扑分析方面表现优异。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [7] [Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography](https://arxiv.org/abs/2512.10179)
*Abolfazl Shahrooei,Luke Arthur,Om Patel,Derek Kamper*

Main category: cs.LG

TL;DR: 比较SNN和TCN在从HD-sEMG解码指尖力方面的性能，TCN表现更优但SNN作为神经形态基准有潜力


<details>
  <summary>Details</summary>
Motivation: 高密度表面肌电信号为非侵入式神经接口提供了可能，但将神经活动映射到用户运动意图仍然具有挑战性。需要评估不同神经网络架构在解码指尖力方面的性能。

Method: 使用前臂电极阵列收集单参与者数据（10次试验），通过FastICA分解获取运动单元放电活动。采用脉冲神经网络（SNN）作为神经形态架构，与时域卷积网络（TCN）进行比较。模型在重叠窗口上使用端到端因果卷积进行训练。

Result: 在保留试验中，TCN达到4.44% MVC RMSE（Pearson r = 0.974），而SNN达到8.25% MVC（r = 0.922）。TCN更准确，但SNN作为现实的神经形态基准，通过适度的架构和超参数优化可以缩小差距。

Conclusion: 虽然TCN在解码指尖力方面表现更优，但SNN作为神经形态计算架构具有潜力，通过改进可以接近TCN的性能，为未来的神经形态接口开发提供基准。

Abstract: High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.

</details>


### [8] [A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field](https://arxiv.org/abs/2512.10287)
*Apurba Sarker,Reza T. Batley,Darshan Sarojini,Sourav Saha*

Main category: cs.LG

TL;DR: KHRONOS是一种基于变分原理、插值理论和张量分解的核神经网络代理模型，在计算资源受限条件下，通过融合稀疏高保真数据和低保真信息来预测气动场，相比传统密集神经网络需要少得多的可训练参数，训练和推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 代理模型能够替代昂贵的气动模拟，在设计优化中非常有用。本研究旨在开发一种在计算资源受限条件下仍能高效预测气动场的模型，通过融合不同保真度的数据来解决高保真数据稀缺的问题。

Method: 提出KHRONOS模型，基于变分原理、插值理论和张量分解构建核神经网络。使用AirfRANS数据集作为高保真基准，NeuralFoil生成低保真数据。与多层感知机(MLP)、图神经网络(GNN)和物理信息神经网络(PINN)进行对比，考虑不同高保真数据可用性(0%、10%、30%)和复杂几何参数化，预测翼型表面压力系数分布。

Result: 虽然所有模型最终都能达到相当的预测精度，但KHRONOS在资源受限条件下表现优异。在相同精度下，KHRONOS需要的可训练参数少几个数量级，训练和推理速度远快于传统密集神经网络。

Conclusion: KHRONOS及其类似架构在多保真度气动场预测中能够平衡精度和效率，特别适用于计算资源受限的场景，为气动设计和优化提供了高效的代理模型解决方案。

Abstract: Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.

</details>


### [9] [Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks](https://arxiv.org/abs/2512.10355)
*Hyunsung Kim,Sangwoo Seo,Hoyoung Choi,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: DEFCON是一个评估足球防守贡献的框架，使用图注意力网络量化球员防守价值，通过对比进攻前后的期望控球价值来分配防守积分。


<details>
  <summary>Details</summary>
Motivation: 现有足球防守评估方法主要关注可见的抢断、拦截等有球动作，但有效防守往往体现在阻止危险机会发生之前，导致防守球员的真实影响未被充分衡量。

Method: 使用图注意力网络(GAT)评估每个进攻选项的成功概率和期望价值，计算每个防守球员的责任分配，通过对比进攻前后的期望控球价值(EPV)来分配防守积分。

Result: 在2023-24赛季训练、2024-25赛季荷甲数据评估，DEFCON的球员积分与市场估值呈现强正相关，并能提供比赛时间线、空间区域分析和攻防对位总结等实用应用。

Conclusion: DEFCON填补了足球防守评估的空白，能够量化防守球员的无球贡献，为球员评估和战术分析提供了新的工具。

Abstract: Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.

</details>


### [10] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 提出DCFO方法，专门为LOF算法生成反事实解释，通过数据空间分区实现梯度优化，在50个数据集上验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离群点检测需要可解释性，特别是对于广泛使用的LOF算法。现有反事实解释方法未能针对LOF等经典离群点检测算法的独特挑战，导致缺乏有效的解释工具。

Method: DCFO方法将数据空间划分为LOF行为平滑的区域，从而实现高效的基于梯度的优化，为LOF生成反事实解释。

Result: 在50个OpenML数据集上的实验验证表明，DCFO在生成反事实的接近度和有效性方面一致优于基准竞争对手。

Conclusion: DCFO填补了LOF算法缺乏可解释性的空白，为离群点检测提供了专门设计的有效反事实解释方法。

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [11] [UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting](https://arxiv.org/abs/2508.01426)
*Hang Ni,Weijia Zhang,Hao Liu*

Main category: cs.LG

TL;DR: UniExtreme是一个通用的极端天气预测基础模型，通过自适应频率调制和事件先验增强模块，解决了现有模型在多样化极端事件预测上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测基础模型在极端天气事件预测方面存在局限，它们要么关注一般天气条件，要么专门针对特定类型的极端事件，忽略了现实世界中多样化极端事件的大气模式。

Method: 提出了UniExtreme模型，包含两个核心模块：1) 自适应频率调制(AFM)模块，通过可学习的Beta分布滤波器和多粒度频谱聚合捕获正常与极端天气之间的区域频谱差异；2) 事件先验增强(EPA)模块，通过双级记忆融合网络整合区域特定的极端事件先验，解决极端事件的层次多样性和复合极端模式。

Result: 大量实验表明，UniExtreme在极端天气和一般天气预测方面都优于最先进的基线模型，展示了在不同极端场景下的卓越适应性。

Conclusion: UniExtreme通过整合频谱差异分析和事件先验知识，成功解决了多样化极端天气事件的预测挑战，为通用极端天气预测基础模型的发展提供了有效解决方案。

Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个全面的数据偏见检测与缓解流程，包含四个组件，针对表示偏见和刻板印象两种偏见类型，通过LLM生成词表、表示偏见量化、刻板印象过滤和反事实数据增强等方法，在性别、宗教和年龄等敏感属性上验证效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据存在多方面偏见，包括有害语言和倾斜的人口分布。欧盟AI法案等法规要求识别和缓解针对受保护群体的偏见，但缺乏实践指导和操作化方法。

Method: 提出四组件流水线：1) 基于质量标准的LLM生成相关群体标签词表；2) 使用人口表示分数量化表示偏见；3) 社会语言学知情过滤检测和缓解刻板印象；4) 语法和上下文感知的反事实数据增强补偿表示偏见。

Result: 在数据去偏方面，成功减少了文本数据集中的表示偏见和显性刻板印象。但在模型偏见减少方面，使用去偏数据微调的LLM在偏见基准测试中并未一致表现出改进，暴露了当前评估方法的差距。

Conclusion: 虽然提出的方法能有效减少数据偏见，但数据去偏不一定能转化为模型偏见的减少，需要更有针对性的数据操作来解决显现的模型偏见，并改进评估方法。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [13] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持架构，通过加勒比口音优化的ASR、本地实体提取和生物声学痛苦检测，在自动语音识别失败时仍能结构化紧急呼叫输入，确保加勒比口音人群公平获得国家分诊协议服务。


<details>
  <summary>Details</summary>
Motivation: 现有紧急语音识别系统对非标准英语变体（如加勒比口音）存在系统性性能下降，导致加勒比人群服务存在关键缺口，需要开发能够处理口音变化的应急系统。

Method: 三层架构：1) 加勒比口音优化的自动语音识别；2) 通过大语言模型进行本地实体提取；3) 生物声学痛苦检测。系统结合转录置信度、结构化临床实体和声音压力指标三个互补信号，即使ASR失败也能为调度员提供结构化输入。

Result: 建立了口音弹性紧急AI框架，确保加勒比口音人群能够公平获得国家分诊协议（ESI常规操作和START大规模伤亡事件）。低ASR置信度可作为队列优先级信号，与声音压力指标结合能识别危机中的呼叫者。

Conclusion: TRIDENT为口音弹性紧急AI建立了框架，确保加勒比声音能够公平获得既定的国家分诊协议。关键洞察是低ASR置信度可作为有价值的队列优先级信号，特别是在与升高的声音痛苦标记结合时。加勒比紧急呼叫的实证验证是未来工作。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>
