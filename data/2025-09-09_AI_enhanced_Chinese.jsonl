{"id": "2509.05337", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05337", "abs": "https://arxiv.org/abs/2509.05337", "authors": ["Younggeol Cho", "Gokhan Solak", "Olivia Nocentini", "Marta Lorenzini", "Andrea Fortuna", "Arash Ajoudani"], "title": "Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory", "comment": "Presented at IEEE RO-MAN 2025", "summary": "Detecting and preventing falls in humans is a critical component of assistive\nrobotic systems. While significant progress has been made in detecting falls,\nthe prediction of falls before they happen, and analysis of the transient state\nbetween stability and an impending fall remain unexplored. In this paper, we\npropose a anticipatory fall detection method that utilizes a hybrid model\ncombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory\n(LSTM) networks that decoupled the motion prediction and gait classification\ntasks to anticipate falls with high accuracy. Our approach employs real-time\nskeletal features extracted from video sequences as input for the proposed\nmodel. The DGNN acts as a classifier, distinguishing between three gait states:\nstable, transient, and fall. The LSTM-based network then predicts human\nmovement in subsequent time steps, enabling early detection of falls. The\nproposed model was trained and validated using the OUMVLP-Pose and URFD\ndatasets, demonstrating superior performance in terms of prediction error and\nrecognition accuracy compared to models relying solely on DGNN and models from\nliterature. The results indicate that decoupling prediction and classification\nimproves performance compared to addressing the unified problem using only the\nDGNN. Furthermore, our method allows for the monitoring of the transient state,\noffering valuable insights that could enhance the functionality of advanced\nassistance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u548cLSTM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6027\u8dcc\u5012\u68c0\u6d4b\uff0c\u901a\u8fc7\u89e3\u8026\u8fd0\u52a8\u9884\u6d4b\u548c\u6b65\u6001\u5206\u7c7b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8dcc\u5012\u9884\u8b66\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8dcc\u5012\u68c0\u6d4b\uff0c\u4f46\u5728\u8dcc\u5012\u53d1\u751f\u524d\u7684\u9884\u6d4b\u4ee5\u53ca\u7a33\u5b9a\u72b6\u6001\u4e0e\u5373\u5c06\u8dcc\u5012\u4e4b\u95f4\u7684\u77ac\u6001\u72b6\u6001\u5206\u6790\u65b9\u9762\u4ecd\u672a\u88ab\u63a2\u7d22\uff0c\u8fd9\u5bf9\u4e8e\u8f85\u52a9\u673a\u5668\u4eba\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u4ece\u89c6\u9891\u5e8f\u5217\u4e2d\u63d0\u53d6\u7684\u5b9e\u65f6\u9aa8\u9abc\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\uff0c\u91c7\u7528DGNN\u4f5c\u4e3a\u5206\u7c7b\u5668\u533a\u5206\u7a33\u5b9a\u3001\u77ac\u6001\u548c\u8dcc\u5012\u4e09\u79cd\u6b65\u6001\u72b6\u6001\uff0cLSTM\u7f51\u7edc\u9884\u6d4b\u540e\u7eed\u65f6\u95f4\u6b65\u7684\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5b9e\u73b0\u65e9\u671f\u8dcc\u5012\u68c0\u6d4b\u3002", "result": "\u5728OUMVLP-Pose\u548cURFD\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528DGNN\u7684\u6a21\u578b\u548c\u6587\u732e\u4e2d\u7684\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u8bef\u5dee\u548c\u8bc6\u522b\u51c6\u786e\u7387\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u89e3\u8026\u9884\u6d4b\u548c\u5206\u7c7b\u76f8\u6bd4\u4f7f\u7528\u5355\u4e00DGNN\u89e3\u51b3\u7edf\u4e00\u95ee\u9898\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u8be5\u65b9\u6cd5\u5141\u8bb8\u76d1\u6d4b\u77ac\u6001\u72b6\u6001\uff0c\u4e3a\u9ad8\u7ea7\u8f85\u52a9\u7cfb\u7edf\u529f\u80fd\u589e\u5f3a\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3\u3002"}}
{"id": "2509.05341", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05341", "abs": "https://arxiv.org/abs/2509.05341", "authors": ["Abhijeet Manoj Pal", "Rajbabu Velmurugan"], "title": "Handling imbalance and few-sample size in ML based Onion disease classification", "comment": "6 pages, 8 figures", "summary": "Accurate classification of pests and diseases plays a vital role in precision\nagriculture, enabling efficient identification, targeted interventions, and\npreventing their further spread. However, current methods primarily focus on\nbinary classification, which limits their practical applications, especially in\nscenarios where accurately identifying the specific type of disease or pest is\nessential. We propose a robust deep learning based model for multi-class\nclassification of onion crop diseases and pests. We enhance a pre-trained\nConvolutional Neural Network (CNN) model by integrating attention based modules\nand employing comprehensive data augmentation pipeline to mitigate class\nimbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1\nscore on real-world field image dataset. This model gives better results than\nother approaches using the same datasets.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u7c7b\u522b\u6d0b\u8471\u75be\u866b\u5206\u7c7b\u6a21\u578b\uff0c\u7edf\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u6570\u636e\u589e\u5e3d\uff0c\u5728\u5b9e\u9645\u7530\u95f4\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.90%\u7684\u603b\u4f53\u51c6\u786e\u7387\u548c0.96 F1\u5206\u6570", "motivation": "\u5f53\u524d\u75be\u866b\u5206\u7c7b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4e8c\u5143\u5206\u7c7b\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u53d7\u9650\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u51c6\u786e\u8bc6\u522b\u5177\u4f53\u75be\u866b\u7c7b\u578b\u7684\u573a\u666f\u4e2d", "method": "\u589e\u5f3a\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u6a21\u578b\uff0c\u96c6\u6210\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4f7f\u7528\u7efc\u5408\u6570\u636e\u589e\u5e3d\u6d41\u7a0b\u6765\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898", "result": "\u5728\u5b9e\u9645\u7530\u95f4\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.90%\u7684\u603b\u4f53\u51c6\u786e\u7387\u548c0.96 F1\u5206\u6570\uff0c\u6548\u679c\u4f18\u4e8e\u5176\u4ed6\u4f7f\u7528\u540c\u6837\u6570\u636e\u96c6\u7684\u65b9\u6cd5", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u6d0b\u8471\u75be\u866b\u7684\u591a\u7c7b\u522b\u5206\u7c7b\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.05545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05545", "abs": "https://arxiv.org/abs/2509.05545", "authors": ["Yang Yu"], "title": "Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks", "comment": null, "summary": "Solving long-horizon goal-conditioned tasks remains a significant challenge\nin reinforcement learning (RL). Hierarchical reinforcement learning (HRL)\naddresses this by decomposing tasks into more manageable sub-tasks, but the\nautomatic discovery of the hierarchy and the joint training of multi-level\npolicies often suffer from instability and can lack theoretical guarantees. In\nthis paper, we introduce Reinforcement Learning with Anticipation (RLA), a\nprincipled and potentially scalable framework designed to address these\nlimitations. The RLA agent learns two synergistic models: a low-level,\ngoal-conditioned policy that learns to reach specified subgoals, and a\nhigh-level anticipation model that functions as a planner, proposing\nintermediate subgoals on the optimal path to a final goal. The key feature of\nRLA is the training of the anticipation model, which is guided by a principle\nof value geometric consistency, regularized to prevent degenerate solutions. We\npresent proofs that RLA approaches the globally optimal policy under various\nconditions, establishing a principled and convergent method for hierarchical\nplanning and execution in long-horizon goal-conditioned tasks.", "AI": {"tldr": "RLA\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u9884\u671f\uff09\u662f\u4e00\u4e2a\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u5c42\u76ee\u6807\u5bfc\u5411\u7b56\u7565\u548c\u9ad8\u5c42\u9884\u671f\u6a21\u578b\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u51b3\u957f\u65f6\u57df\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u6536\u655b\u6027\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u57df\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4e2d\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u539f\u5219\u6027\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002", "method": "\u5b66\u4e60\u4e24\u4e2a\u534f\u540c\u6a21\u578b\uff1a\u4f4e\u5c42\u76ee\u6807\u5bfc\u5411\u7b56\u7565\uff08\u5b66\u4e60\u5230\u8fbe\u6307\u5b9a\u5b50\u76ee\u6807\uff09\u548c\u9ad8\u5c42\u9884\u671f\u6a21\u578b\uff08\u4f5c\u4e3a\u89c4\u5212\u5668\uff0c\u63d0\u51fa\u5230\u8fbe\u6700\u7ec8\u76ee\u6807\u7684\u6700\u4f18\u8def\u5f84\u4e2d\u7684\u4e2d\u95f4\u5b50\u76ee\u6807\uff09\uff0c\u901a\u8fc7\u4ef7\u503c\u51e0\u4f55\u4e00\u81f4\u6027\u539f\u5219\u8bad\u7ec3\u9884\u671f\u6a21\u578b\u3002", "result": "RLA\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u80fd\u591f\u903c\u8fd1\u5168\u5c40\u6700\u4f18\u7b56\u7565\uff0c\u4e3a\u957f\u65f6\u57df\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4e2d\u7684\u5206\u5c42\u89c4\u5212\u548c\u6267\u884c\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u6536\u655b\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "RLA\u6846\u67b6\u901a\u8fc7\u9884\u671f\u6a21\u578b\u548c\u4ef7\u503c\u51e0\u4f55\u4e00\u81f4\u6027\u539f\u5219\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u7f3a\u5931\u95ee\u9898\uff0c\u4e3a\u957f\u65f6\u57df\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05923", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05923", "abs": "https://arxiv.org/abs/2509.05923", "authors": ["Shuolong Chen", "Xingxing Li", "Liu Yuan"], "title": "eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems", "comment": null, "summary": "The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the visual-inertial setup is commonly\nadopted due to complementary characteristics between sensors (e.g., scale\nperception and low drift). For optimal event-based visual-inertial fusion,\naccurate spatiotemporal (extrinsic and temporal) calibration is required. In\nthis work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator\nfor event-based visual-inertial systems, utilizing the widely used circle grid\nboard. Building upon the grid pattern recognition and tracking methods in\neKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and\nefficient initialization, where all parameters in the estimator would be\naccurately recovered. Subsequently, a continuous-time-based batch optimization\nis conducted to refine the initialized parameters toward better states. The\nresults of extensive real-world experiments show that eKalibr-Inertial can\nachieve accurate event-based visual-inertial spatiotemporal calibration. The\nimplementation of eKalibr-Inertial is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.", "AI": {"tldr": "eKalibr-Inertial\u662f\u4e00\u4e2a\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a-\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u7cfb\u7edf\u7684\u7cbe\u786e\u65f6\u7a7a\u6807\u5b9a\u5de5\u5177\uff0c\u4f7f\u7528\u5706\u5f62\u7f51\u683c\u677f\u8fdb\u884c\u6821\u51c6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u521d\u59cb\u5316\u548c\u8fde\u7eed\u65f6\u95f4\u6279\u91cf\u4f18\u5316\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u9700\u8981\u4e0e\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u878d\u5408\uff0c\u4f46\u9700\u8981\u51c6\u786e\u7684\u65f6\u7a7a\uff08\u5916\u53c2\u548c\u65f6\u95f4\uff09\u6821\u51c6\u624d\u80fd\u5b9e\u73b0\u6700\u4f18\u878d\u5408\u6548\u679c\u3002", "method": "\u57fa\u4e8eeKalibr\u548ceKalibr-Stereo\u7684\u7f51\u683c\u6a21\u5f0f\u8bc6\u522b\u548c\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u91c7\u7528\u4e25\u683c\u7684\u521d\u59cb\u5316\u8fc7\u7a0b\u6062\u590d\u6240\u6709\u53c2\u6570\uff0c\u7136\u540e\u8fdb\u884c\u8fde\u7eed\u65f6\u95f4\u6279\u91cf\u4f18\u5316\u6765\u7ec6\u5316\u53c2\u6570\u3002", "result": "\u5e7f\u6cdb\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0ceKalibr-Inertial\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7684\u4e8b\u4ef6\u76f8\u673a-\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u65f6\u7a7a\u6821\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u89c6\u89c9-\u60ef\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u5b9e\u73b0\u4ee5\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2509.05388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05388", "abs": "https://arxiv.org/abs/2509.05388", "authors": ["Juan Olalla-Pombo", "Alberto Bad\u00edas", "Miguel \u00c1ngel Sanz-G\u00f3mez", "Jos\u00e9 Mar\u00eda Ben\u00edtez", "Francisco Javier Mont\u00e1ns"], "title": "Augmented Structure Preserving Neural Networks for cell biomechanics", "comment": null, "summary": "Cell biomechanics involve a great number of complex phenomena that are\nfundamental to the evolution of life itself and other associated processes,\nranging from the very early stages of embryo-genesis to the maintenance of\ndamaged structures or the growth of tumors. Given the importance of such\nphenomena, increasing research has been dedicated to their understanding, but\nthe many interactions between them and their influence on the decisions of\ncells as a collective network or cluster remain unclear. We present a new\napproach that combines Structure Preserving Neural Networks, which study cell\nmovements as a purely mechanical system, with other Machine Learning tools\n(Artificial Neural Networks), which allow taking into consideration\nenvironmental factors that can be directly deduced from an experiment with\nComputer Vision techniques. This new model, tested on simulated and real cell\nmigration cases, predicts complete cell trajectories following a roll-out\npolicy with a high level of accuracy. This work also includes a mitosis event\nprediction model based on Neural Networks architectures which makes use of the\nsame observed features.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u7ec6\u80de\u8fc1\u79fb\u8f68\u8ff9\u548c\u6709\u4e1d\u5206\u88c2\u4e8b\u4ef6\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7ec6\u80de\u8fc1\u79fb\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u7ec6\u80de\u751f\u7269\u529b\u5b66\u6d89\u53ca\u4ece\u80da\u80ce\u53d1\u751f\u5230\u80bf\u7624\u751f\u957f\u7b49\u4f17\u591a\u91cd\u8981\u751f\u547d\u8fc7\u7a0b\uff0c\u4f46\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5bf9\u96c6\u4f53\u51b3\u7b56\u7684\u5f71\u54cd\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u65b0\u7684\u7814\u7a76\u65b9\u6cd5\u6765\u7406\u89e3\u8fd9\u4e9b\u590d\u6742\u73b0\u8c61\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\uff08\u7814\u7a76\u7ec6\u80de\u8fd0\u52a8\u4f5c\u4e3a\u7eaf\u673a\u68b0\u7cfb\u7edf\uff09\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08\u8003\u8651\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u4ece\u5b9e\u9a8c\u4e2d\u76f4\u63a5\u63a8\u5bfc\u7684\u73af\u5883\u56e0\u7d20\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6709\u4e1d\u5206\u88c2\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u65b0\u6a21\u578b\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7ec6\u80de\u8fc1\u79fb\u6848\u4f8b\u4e2d\u6d4b\u8bd5\uff0c\u901a\u8fc7roll-out\u7b56\u7565\u9884\u6d4b\u5b8c\u6574\u7ec6\u80de\u8f68\u8ff9\uff0c\u8fbe\u5230\u4e86\u9ad8\u7cbe\u5ea6\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u7ec6\u80de\u8fc1\u79fb\u8f68\u8ff9\u548c\u6709\u4e1d\u5206\u88c2\u4e8b\u4ef6\u7684\u7efc\u5408\u6a21\u578b\uff0c\u4e3a\u7406\u89e3\u7ec6\u80de\u96c6\u4f53\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2509.05668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05668", "abs": "https://arxiv.org/abs/2509.05668", "authors": ["Michael Hoffmann", "Jophin John", "Stefan Schweter", "Gokul Ramakrishnan", "Hoi-Fong Mak", "Alice Zhang", "Dmitry Gaynullin", "Nicolay J. Hammer"], "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "comment": "Michael Hoffmann and Jophin John contributed equally to this work", "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.", "AI": {"tldr": "Llama-GENBA-10B\u662f\u4e00\u4e2a10B\u53c2\u6570\u7684\u4e09\u8bed\u57fa\u7840\u6a21\u578b\uff0c\u57fa\u4e8eLlama 3.1-8B\u6784\u5efa\uff0c\u5728164B tokens\uff08\u82f1\u8bed82B\u3001\u5fb7\u8bed82B\u3001\u5df4\u4f10\u5229\u4e9a\u8bed80M\uff09\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u82f1\u8bed\u4e2d\u5fc3\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u5fb7\u8bedNLP\u793e\u533a\u5e76\u63a8\u5e7f\u4f4e\u8d44\u6e90\u5df4\u4f10\u5229\u4e9a\u8bed\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u82f1\u8bed\u4e2d\u5fc3\u504f\u89c1\u95ee\u9898\uff0c\u4e3a\u5fb7\u8bedNLP\u793e\u533a\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u5df4\u4f10\u5229\u4e9a\u8bed\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eLlama 3.1-8B\u6269\u5c55\u523010B\u53c2\u6570\uff0c\u5728\u5e73\u8861\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u521b\u5efa\u7edf\u4e00\u7684\u5206\u8bcd\u5668\uff0c\u4f18\u5316\u67b6\u6784\u548c\u8bed\u8a00\u6bd4\u4f8b\u8d85\u53c2\u6570\uff0c\u5efa\u7acb\u9996\u4e2a\u6807\u51c6\u5316\u4e09\u8bed\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u6a21\u578b\u5728\u4e09\u8bed\u6027\u80fd\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5fae\u8c03\u7248\u672c\u5728\u5df4\u4f10\u5229\u4e9a\u8bed\u4e0a\u8d85\u8d8aApertus-8B-2509\u548cgemma-2-9b\uff0c\u6210\u4e3a\u8be5\u8bed\u8a00\u7c7b\u522b\u6700\u4f73\u6a21\u578b\uff0c\u540c\u65f6\u5728\u82f1\u8bed\u4e0a\u4f18\u4e8eEuroLLM\uff0c\u5fb7\u8bed\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5305\u5bb9\u6027\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u6574\u5408\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5e76\u5728Cerebras CS-2\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u3002"}}
{"id": "2509.05766", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.05766", "abs": "https://arxiv.org/abs/2509.05766", "authors": ["Jiaju Miao", "Wei Zhu"], "title": "Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders", "comment": null, "summary": "Anomaly detection underpins critical applications from network security and\nintrusion detection to fraud prevention, where recognizing aberrant patterns\nrapidly is indispensable. Progress in this area is routinely impeded by two\nobstacles: extreme class imbalance and the curse of dimensionality. To combat\nthe former, we previously introduced Precision-Recall Curve (PRC)\nclassification trees and their ensemble extension, the PRC Random Forest\n(PRC-RF). Building on that foundation, we now propose a hybrid framework that\nintegrates PRC-RF with autoencoders, unsupervised machine learning methods that\nlearn compact latent representations, to confront both challenges\nsimultaneously. Extensive experiments across diverse benchmark datasets\ndemonstrate that the resulting Autoencoder-PRC-RF model achieves superior\naccuracy, scalability, and interpretability relative to prior methods,\naffirming its potential for high-stakes anomaly-detection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408PRC\u968f\u673a\u68ee\u6797\u548c\u81ea\u7f16\u7801\u5668\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "motivation": "\u5f02\u5e38\u68c0\u6d4b\u5728\u7f51\u7edc\u5b89\u5168\u3001\u5165\u4fb5\u68c0\u6d4b\u548c\u6b3a\u8bc8\u9884\u9632\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7ef4\u5ea6\u8bc5\u5492\u4e24\u5927\u969c\u788d", "method": "\u5c06\u4e4b\u524d\u63d0\u51fa\u7684PRC\u968f\u673a\u68ee\u6797(PRC-RF)\u4e0e\u81ea\u7f16\u7801\u5668\u7ed3\u5408\uff0c\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\u6765\u5904\u7406\u9ad8\u7ef4\u95ee\u9898\uff0cPRC-RF\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAutoencoder-PRC-RF\u6a21\u578b\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u66f4\u4f18\u8d8a", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u5728\u9ad8\u98ce\u9669\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u540c\u65f6\u6709\u6548\u5e94\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u6311\u6218"}}
{"id": "2509.05778", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2"], "pdf": "https://arxiv.org/pdf/2509.05778", "abs": "https://arxiv.org/abs/2509.05778", "authors": ["Arantxa Urrea-Casta\u00f1o", "Nicol\u00e1s Segura-Kunsagi", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection", "comment": "20 pages and appendix", "summary": "Out-of-distribution (OOD) detection plays a key role in enhancing the\nrobustness of artificial intelligence systems by identifying inputs that differ\nsignificantly from the training distribution, thereby preventing unreliable\npredictions and enabling appropriate fallback mechanisms. Developing reliable\nOOD detection methods is a significant challenge, and rigorous evaluation of\nthese techniques is essential for ensuring their effectiveness, as it allows\nresearchers to assess their performance under diverse conditions and to\nidentify potential limitations or failure modes. Cross-validation (CV) has\nproven to be a highly effective tool for providing a reasonable estimate of the\nperformance of a learning algorithm. Although OOD scenarios exhibit particular\ncharacteristics, an appropriate adaptation of CV can lead to a suitable\nevaluation framework for this setting. This work proposes a dual CV framework\nfor robust evaluation of OOD detection models, aimed at improving the\nreliability of their assessment. The proposed evaluation framework aims to\neffectively integrate in-distribution (ID) and OOD data while accounting for\ntheir differing characteristics. To achieve this, ID data are partitioned using\na conventional approach, whereas OOD data are divided by grouping samples based\non their classes. Furthermore, we analyze the context of data with class\nhierarchy to propose a data splitting that considers the entire class hierarchy\nto obtain fair ID-OOD partitions to apply the proposed evaluation framework.\nThis framework is called Dual Cross-Validation for Robust Out-of-Distribution\nDetection (DCV-ROOD). To test the validity of the evaluation framework, we\nselected a set of state-of-the-art OOD detection methods, both with and without\noutlier exposure. The results show that the method achieves very fast\nconvergence to the true performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u4ea4\u53c9\u9a8c\u8bc1\u6846\u67b6DCV-ROOD\uff0c\u7528\u4e8e\u66f4\u7a33\u5065\u5730\u8bc4\u4f30\u5206\u5e03\u5916\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5206\u522b\u5904\u7406\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u7684\u7279\u6027\uff0c\u5e76\u8003\u8651\u7c7b\u522b\u5c42\u6b21\u7ed3\u6784\u6765\u83b7\u5f97\u516c\u5e73\u7684\u6570\u636e\u5206\u5272\u3002", "motivation": "\u5206\u5e03\u5916(OOD)\u68c0\u6d4b\u5bf9\u4e8e\u63d0\u9ad8AI\u7cfb\u7edf\u7a33\u5065\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5904\u7406ID\u548cOOD\u6570\u636e\u7279\u6027\u5dee\u5f02\u65f6\u5b58\u5728\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u51c6\u786e\u4f30\u8ba1OOD\u68c0\u6d4b\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u4ea4\u53c9\u9a8c\u8bc1(DCV-ROOD)\u6846\u67b6\uff1a1)\u5206\u5e03\u5185(ID)\u6570\u636e\u91c7\u7528\u6807\u51c6\u5206\u5272\u65b9\u5f0f\uff1b2)\u5206\u5e03\u5916(OOD)\u6570\u636e\u6309\u7c7b\u522b\u5206\u7ec4\u5206\u5272\uff1b3)\u5728\u7c7b\u522b\u5c42\u6b21\u6570\u636e\u4e2d\uff0c\u8003\u8651\u6574\u4e2a\u7c7b\u522b\u5c42\u6b21\u7ed3\u6784\u6765\u83b7\u5f97\u516c\u5e73\u7684ID-OOD\u5206\u5272\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u4e2a\u72ec\u7acbOOD\u68c0\u6d4b\u65b9\u6cd5\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u5feb\u901f\u6536\u655b\u5230\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\u4f30\u8ba1\u503c\u3002", "conclusion": "DCV-ROOD\u6846\u67b6\u4e3aOOD\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u3001\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u6d4b\u5de5\u5177\u3002"}}
{"id": "2509.06269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06269", "abs": "https://arxiv.org/abs/2509.06269", "authors": ["Vishal Raman", "Vijai Aravindh R", "Abhijith Ragav"], "title": "REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents", "comment": "8 pages, 2 figures, Accepted at the OARS Workshop, KDD 2025, Paper\n  link: https://oars-workshop.github.io/papers/Raman2025.pdf", "summary": "Personalized AI assistants often struggle to incorporate complex personal\ndata and causal knowledge, leading to generic advice that lacks explanatory\npower. We propose REMI, a Causal Schema Memory architecture for a multimodal\nlifestyle agent that integrates a personal causal knowledge graph, a causal\nreasoning engine, and a schema based planning module. The idea is to deliver\nexplainable, personalized recommendations in domains like fashion, personal\nwellness, and lifestyle planning. Our architecture uses a personal causal graph\nof the user's life events and habits, performs goal directed causal traversals\nenriched with external knowledge and hypothetical reasoning, and retrieves\nadaptable plan schemas to generate tailored action plans. A Large Language\nModel orchestrates these components, producing answers with transparent causal\nexplanations. We outline the CSM system design and introduce new evaluation\nmetrics for personalization and explainability, including Personalization\nSalience Score and Causal Reasoning Accuracy, to rigorously assess its\nperformance. Results indicate that CSM based agents can provide more context\naware, user aligned recommendations compared to baseline LLM agents. This work\ndemonstrates a novel approach to memory augmented, causal reasoning in\npersonalized agents, advancing the development of transparent and trustworthy\nAI lifestyle assistants.", "AI": {"tldr": "REMI\u662f\u4e00\u4e2a\u57fa\u4e8e\u56e0\u679c\u6a21\u5f0f\u8bb0\u5fc6\u7684\u591a\u6a21\u6001\u751f\u6d3b\u65b9\u5f0f\u52a9\u624b\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u4e2a\u4eba\u56e0\u679c\u77e5\u8bc6\u56fe\u8c31\u3001\u56e0\u679c\u63a8\u7406\u5f15\u64ce\u548c\u57fa\u4e8e\u6a21\u5f0f\u7684\u89c4\u5212\u6a21\u5757\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u63a8\u8350", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316AI\u52a9\u624b\u96be\u4ee5\u6574\u5408\u590d\u6742\u7684\u4e2a\u4eba\u6570\u636e\u548c\u56e0\u679c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5efa\u8bae\u8fc7\u4e8e\u901a\u7528\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027", "method": "\u4f7f\u7528\u4e2a\u4eba\u56e0\u679c\u56fe\u8c31\u8bb0\u5f55\u7528\u6237\u751f\u6d3b\u4e8b\u4ef6\u548c\u4e60\u60ef\uff0c\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u7684\u56e0\u679c\u904d\u5386\uff08\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u548c\u5047\u8bbe\u63a8\u7406\uff09\uff0c\u68c0\u7d22\u53ef\u9002\u5e94\u7684\u8ba1\u5212\u6a21\u5f0f\u6765\u751f\u6210\u5b9a\u5236\u5316\u884c\u52a8\u65b9\u6848\uff0c\u7531\u5927\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u5404\u7ec4\u4ef6", "result": "\u57fa\u4e8eCSM\u7684\u667a\u80fd\u4f53\u76f8\u6bd4\u57fa\u7ebfLLM\u667a\u80fd\u4f53\u80fd\u591f\u63d0\u4f9b\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u3001\u66f4\u8d34\u5408\u7528\u6237\u7684\u63a8\u8350", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u4e2a\u6027\u5316\u667a\u80fd\u4f53\u4e2d\u5b9e\u73b0\u8bb0\u5fc6\u589e\u5f3a\u548c\u56e0\u679c\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u900f\u660e\u53ef\u4fe1AI\u751f\u6d3b\u65b9\u5f0f\u52a9\u624b\u7684\u53d1\u5c55"}}
{"id": "2509.05801", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05801", "abs": "https://arxiv.org/abs/2509.05801", "authors": ["Debdeep Sanyal", "Aaryan Nagpal", "Dhruv Kumar", "Murari Mandal", "Saurabh Deshpande"], "title": "time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models", "comment": null, "summary": "While transformer-based foundation models excel at forecasting routine\npatterns, two questions remain: do they internalize semantic concepts such as\nmarket regimes, or merely fit curves? And can their internal representations be\nleveraged to simulate rare, high-stakes events such as market crashes? To\ninvestigate this, we introduce activation transplantation, a causal\nintervention that manipulates hidden states by imposing the statistical moments\nof one event (e.g., a historical crash) onto another (e.g., a calm period)\nduring the forward pass. This procedure deterministically steers forecasts:\ninjecting crash semantics induces downturn predictions, while injecting calm\nsemantics suppresses crashes and restores stability. Beyond binary control, we\nfind that models encode a graded notion of event severity, with the latent\nvector norm directly correlating with the magnitude of systemic shocks.\nValidated across two architecturally distinct TSFMs, Toto (decoder only) and\nChronos (encoder-decoder), our results demonstrate that steerable, semantically\ngrounded representations are a robust property of large time series\ntransformers. Our findings provide evidence for a latent concept space that\ngoverns model predictions, shifting interpretability from post-hoc attribution\nto direct causal intervention, and enabling semantic \"what-if\" analysis for\nstrategic stress-testing.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u65f6\u95f4\u5e8f\u5217Transformer\u6a21\u578b\u5185\u90e8\u7f16\u7801\u4e86\u8bed\u4e49\u6982\u5ff5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u79fb\u690d\u6280\u672f\u53ef\u4ee5\u56e0\u679c\u5e72\u9884\u6a21\u578b\u9884\u6d4b\uff0c\u5b9e\u73b0\u5e02\u573a\u72b6\u6001\u7684\u8bed\u4e49\u63a7\u5236", "motivation": "\u63a2\u7a76\u57fa\u4e8eTransformer\u7684\u57fa\u7840\u6a21\u578b\u662f\u5426\u771f\u6b63\u5185\u5316\u4e86\u8bed\u4e49\u6982\u5ff5\uff08\u5982\u5e02\u573a\u72b6\u6001\uff09\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u66f2\u7ebf\u62df\u5408\uff0c\u4ee5\u53ca\u80fd\u5426\u5229\u7528\u5176\u5185\u90e8\u8868\u793a\u6765\u6a21\u62df\u7f55\u89c1\u7684\u9ad8\u98ce\u9669\u4e8b\u4ef6\uff08\u5982\u5e02\u573a\u5d29\u76d8\uff09", "method": "\u5f15\u5165\u6fc0\u6d3b\u79fb\u690d\u6280\u672f\uff0c\u901a\u8fc7\u5728\u6b63\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u5c06\u4e00\u4e2a\u4e8b\u4ef6\uff08\u5982\u5386\u53f2\u5d29\u76d8\uff09\u7684\u7edf\u8ba1\u77e9\u5f3a\u52a0\u5230\u53e6\u4e00\u4e2a\u4e8b\u4ef6\uff08\u5982\u5e73\u9759\u671f\uff09\u4e0a\u6765\u64cd\u7eb5\u9690\u85cf\u72b6\u6001", "result": "\u6ce8\u5165\u5d29\u76d8\u8bed\u4e49\u4f1a\u8bf1\u5bfc\u4e0b\u8dcc\u9884\u6d4b\uff0c\u6ce8\u5165\u5e73\u9759\u8bed\u4e49\u5219\u6291\u5236\u5d29\u76d8\u5e76\u6062\u590d\u7a33\u5b9a\u6027\uff1b\u6a21\u578b\u7f16\u7801\u4e86\u4e8b\u4ef6\u4e25\u91cd\u7a0b\u5ea6\u7684\u68af\u5ea6\u6982\u5ff5\uff0c\u6f5c\u5728\u5411\u91cf\u8303\u6570\u4e0e\u7cfb\u7edf\u6027\u51b2\u51fb\u5e45\u5ea6\u76f4\u63a5\u76f8\u5173", "conclusion": "\u5927\u578b\u65f6\u95f4\u5e8f\u5217Transformer\u5177\u6709\u53ef\u64cd\u63a7\u7684\u3001\u57fa\u4e8e\u8bed\u4e49\u7684\u8868\u5f81\u80fd\u529b\uff0c\u5b58\u5728\u4e00\u4e2a\u63a7\u5236\u6a21\u578b\u9884\u6d4b\u7684\u6f5c\u5728\u6982\u5ff5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u4ece\u540e\u9a8c\u5f52\u56e0\u5230\u76f4\u63a5\u56e0\u679c\u5e72\u9884\u7684\u8f6c\u53d8"}}
{"id": "2509.05554", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05554", "abs": "https://arxiv.org/abs/2509.05554", "authors": ["Yihong Leng", "Siming Zheng", "Jinwei Chen", "Bo Li", "Jiaojiao Li", "Peng-Tao Jiang"], "title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation", "comment": null, "summary": "Event cameras provide sparse yet temporally high-temporal-resolution motion\ninformation, demonstrating great potential for motion deblurring. Existing\nmethods focus on cross-modal interaction, overlooking the inherent\nincompleteness of event streams, which arises from the trade-off between\nsensitivity and noise introduced by the thresholding mechanism of Dynamic\nVision Sensors (DVS). Such degradation compromises the integrity of motion\npriors and limits the effectiveness of event-guided deblurring. To tackle these\nchallenges, we propose a Robust Event-guided Deblurring (RED) network with\nmodality-specific disentangled representation. First, we introduce a\nRobustness-Oriented Perturbation Strategy (RPS) that applies random masking to\nevents, which exposes RED to incomplete patterns and then foster robustness\nagainst various unknown scenario conditions.Next, a disentangled OmniAttention\nis presented to explicitly model intra-motion, inter-motion, and cross-modality\ncorrelations from two inherently distinct but complementary sources: blurry\nimages and partially disrupted events. Building on these reliable features, two\ninteractive modules are designed to enhance motion-sensitive areas in blurry\nimages and inject semantic context into incomplete event representations.\nExtensive experiments on synthetic and real-world datasets demonstrate RED\nconsistently achieves state-of-the-art performance in both accuracy and\nrobustness.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8fd0\u52a8\u53bb\u6a21\u7cca\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e8b\u4ef6\u6d41\u7684\u672c\u8d28\u4e0d\u5b8c\u6574\u6027\u3002\u672c\u6587\u63d0\u51faRED\u7f51\u7edc\uff0c\u901a\u8fc7\u968f\u673a\u6295\u5f71\u7b56\u7565\u548c\u89e3\u8026\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u72ec\u521b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u5bfc\u5411\u53bb\u6a21\u7cca\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e8b\u4ef6\u6d41\u7684\u5185\u5728\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u8fd9\u79cd\u9000\u5316\u5f71\u54cd\u8fd0\u52a8\u5148\u9a8c\u77e5\u8bc6\u7684\u5b8c\u6574\u6027\u548c\u4e8b\u4ef6\u5bfc\u5411\u53bb\u6a21\u7cca\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faRED\u7f51\u7edc\uff1a1)\u7a33\u5065\u6027\u5bfc\u5411\u7684\u968f\u673a\u6295\u5f71\u7b56\u7565(RPS)\uff0c\u901a\u8fc7\u968f\u673a\u9690\u85cf\u4e8b\u4ef6\u6765\u57f9\u517b\u7f51\u7edc\u5bf9\u4e0d\u5b8c\u6574\u6a21\u5f0f\u7684\u7a33\u5065\u6027\uff1b2)\u89e3\u8026\u7684OmniAttention\u673a\u5236\uff0c\u663e\u5f0f\u5efa\u6a21\u5185\u90e8\u8fd0\u52a8\u3001\u8fd0\u52a8\u95f4\u548c\u8de8\u6a21\u6001\u76f8\u5173\u6027\uff1b3)\u4e24\u4e2a\u4ea4\u4e92\u6a21\u5757\u589e\u5f3a\u6a21\u7cca\u56fe\u50cf\u4e2d\u7684\u8fd0\u52a8\u654f\u611f\u533a\u57df\u5e76\u5411\u4e8b\u4ef6\u8868\u5f81\u6ce8\u5165\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cRED\u7f51\u7edc\u5728\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u72ec\u521b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u91cd\u89c6\u4e8b\u4ef6\u6d41\u7684\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u7a33\u5065\u6027\u7b56\u7565\uff0cRED\u7f51\u7edc\u6709\u6548\u63d0\u5347\u4e86\u4e8b\u4ef6\u5bfc\u5411\u53bb\u6a21\u7cca\u7684\u6548\u679c\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.05604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05604", "abs": "https://arxiv.org/abs/2509.05604", "authors": ["Jungin Park", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization", "comment": "Accepted to IJCV, 29 pages, 14 figures, 11 tables", "summary": "Video summarization aims to select keyframes that are visually diverse and\ncan represent the whole story of a given video. Previous approaches have\nfocused on global interlinkability between frames in a video by temporal\nmodeling. However, fine-grained visual entities, such as objects, are also\nhighly related to the main content of the video. Moreover, language-guided\nvideo summarization, which has recently been studied, requires a comprehensive\nlinguistic understanding of complex real-world videos. To consider how all the\nobjects are semantically related to each other, this paper regards video\nsummarization as a language-guided spatiotemporal graph modeling problem. We\npresent recursive spatiotemporal graph networks, called VideoGraph, which\nformulate the objects and frames as nodes of the spatial and temporal graphs,\nrespectively. The nodes in each graph are connected and aggregated with graph\nedges, representing the semantic relationships between the nodes. To prevent\nthe edges from being configured with visual similarity, we incorporate language\nqueries derived from the video into the graph node representations, enabling\nthem to contain semantic knowledge. In addition, we adopt a recursive strategy\nto refine initial graphs and correctly classify each frame node as a keyframe.\nIn our experiments, VideoGraph achieves state-of-the-art performance on several\nbenchmarks for generic and query-focused video summarization in both supervised\nand unsupervised manners. The code is available at\nhttps://github.com/park-jungin/videograph.", "AI": {"tldr": "VideoGraph\uff1a\u57fa\u4e8e\u8bed\u8a00\u5f15\u5bfc\u7684\u65f6\u7a7a\u56fe\u5efa\u6a21\u7684\u89c6\u9891\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5bf9\u8c61\u548c\u5e27\u5efa\u6a21\u4e3a\u56fe\u8282\u70b9\uff0c\u5229\u7528\u8bed\u4e49\u5173\u7cfb\u8fdb\u884c\u5173\u952e\u5e27\u9009\u62e9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u4f20\u7edf\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e27\u95f4\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u4f46\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9e\u4f53\uff08\u5982\u5bf9\u8c61\uff09\u4e0e\u89c6\u9891\u4e3b\u8981\u5185\u5bb9\u7684\u5173\u7cfb\u3002\u8bed\u8a00\u5f15\u5bfc\u7684\u89c6\u9891\u6458\u8981\u9700\u8981\u5168\u9762\u7406\u89e3\u590d\u6742\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7684\u8bed\u4e49\u5185\u5bb9", "method": "\u63d0\u51fa\u9012\u5f52\u65f6\u7a7a\u56fe\u7f51\u7edcVideoGraph\uff0c\u5c06\u5bf9\u8c61\u548c\u5e27\u5206\u522b\u5efa\u6a21\u4e3a\u7a7a\u95f4\u56fe\u548c\u65f6\u95f4\u56fe\u7684\u8282\u70b9\uff0c\u901a\u8fc7\u56fe\u8fb9\u8868\u793a\u8282\u70b9\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002\u5f15\u5165\u8bed\u8a00\u67e5\u8be2\u6765\u589e\u5f3a\u8282\u70b9\u8868\u793a\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u907f\u514d\u4ec5\u57fa\u4e8e\u89c6\u89c9\u76f8\u4f3c\u6027\u914d\u7f6e\u8fb9\u3002\u91c7\u7528\u9012\u5f52\u7b56\u7565\u4f18\u5316\u521d\u59cb\u56fe\u5e76\u6b63\u786e\u5206\u7c7b\u5173\u952e\u5e27", "result": "\u5728\u591a\u4e2a\u901a\u7528\u548c\u67e5\u8be2\u805a\u7126\u89c6\u9891\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u76d1\u7763\u548c\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "VideoGraph\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u7684\u65f6\u7a7a\u56fe\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6458\u8981\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8003\u8651\u5bf9\u8c61\u8bed\u4e49\u5173\u7cfb\u548c\u8bed\u8a00\u5f15\u5bfc\u7684\u91cd\u8981\u6027"}}
{"id": "2509.05839", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05839", "abs": "https://arxiv.org/abs/2509.05839", "authors": ["Daksh Mittal", "Shunri Zheng", "Jing Dong", "Hongseok Namkoong"], "title": "Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics", "comment": null, "summary": "While queueing network models are powerful tools for analyzing service\nsystems, they traditionally require substantial human effort and domain\nexpertise to construct. To make this modeling approach more scalable and\naccessible, we propose a data-driven framework for queueing network modeling\nand simulation based on autoregressive sequence models trained on event-stream\ndata. Instead of explicitly specifying arrival processes, service mechanisms,\nor routing logic, our approach learns the conditional distributions of event\ntypes and event times, recasting the modeling task as a problem of sequence\ndistribution learning. We show that Transformer-style architectures can\neffectively parameterize these distributions, enabling automated construction\nof high-fidelity simulators. As a proof of concept, we validate our framework\non event tables generated from diverse queueing networks, showcasing its\nutility in simulation, uncertainty quantification, and counterfactual\nevaluation. Leveraging advances in artificial intelligence and the growing\navailability of data, our framework takes a step toward more automated,\ndata-driven modeling pipelines to support broader adoption of queueing network\nmodels across service domains.", "AI": {"tldr": "\u57fa\u4e8e\u81ea\u56de\u5e8f\u5217\u6a21\u578b\u7684\u961f\u5217\u7f51\u7edc\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7Transformer\u5b66\u4e60\u4e8b\u4ef6\u6d41\u6570\u636e\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u961f\u5217\u7f51\u7edc\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6784\u5efa\u548c\u9ad8\u4fdd\u771f\u6a21\u62df\u3002", "motivation": "\u4f20\u7edf\u961f\u5217\u7f51\u7edc\u6a21\u578b\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52aa\u529b\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e3a\u4e86\u4f7f\u6b64\u5efa\u6a21\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u6613\u8bbf\u95ee\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u56de\u5e8f\u5217\u6a21\u578b\u8bad\u7ec3\u4e8e\u4e8b\u4ef6\u6d41\u6570\u636e\uff0c\u5b66\u4e60\u4e8b\u4ef6\u7c7b\u578b\u548c\u4e8b\u4ef6\u65f6\u95f4\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u91cd\u6784\u5efa\u6a21\u4efb\u52a1\u4e3a\u5e8f\u5217\u5206\u5e03\u5b66\u4e60\u95ee\u9898\uff0c\u91c7\u7528Transformer\u7ed3\u6784\u8fdb\u884c\u53c2\u6570\u5316\u3002", "result": "\u5728\u591a\u6837\u5316\u961f\u5217\u7f51\u7edc\u4e8b\u4ef6\u8868\u4e0a\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6a21\u62df\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u65b9\u9762\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8fdb\u6b65\u548c\u6570\u636e\u53ef\u7528\u6027\u7684\u63d0\u5347\uff0c\u8be5\u6846\u67b6\u5411\u66f4\u81ea\u52a8\u5316\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u6d41\u7a0b\u8fdb\u884c\u4e86\u63d2\u8bd1\uff0c\u652f\u6301\u961f\u5217\u7f51\u7edc\u6a21\u578b\u5728\u670d\u52a1\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2509.06355", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06355", "abs": "https://arxiv.org/abs/2509.06355", "authors": ["Yunzhe Wang", "Volkan Ustun", "Chris McGroarty"], "title": "A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research", "comment": "Accepted at the Winter Simulation Conference 2025, December, Seattle\n  USA", "summary": "Modern simulation environments for complex multi-agent interactions must\nbalance high-fidelity detail with computational efficiency. We present DECOY, a\nnovel multi-agent simulator that abstracts strategic, long-horizon planning in\n3D terrains into high-level discretized simulation while preserving low-level\nenvironmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a\ntestbed, our framework accurately simulates gameplay using only movement\ndecisions as tactical positioning -- without explicitly modeling low-level\nmechanics such as aiming and shooting. Central to our approach is a waypoint\nsystem that simplifies and discretizes continuous states and actions, paired\nwith neural predictive and generative models trained on real CS:GO tournament\ndata to reconstruct event outcomes. Extensive evaluations show that replays\ngenerated from human data in DECOY closely match those observed in the original\ngame. Our publicly available simulation environment provides a valuable tool\nfor advancing research in strategic multi-agent planning and behavior\ngeneration.", "AI": {"tldr": "DECOY\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5c063D\u5730\u5f62\u4e2d\u7684\u6218\u7565\u957f\u671f\u89c4\u5212\u62bd\u8c61\u4e3a\u9ad8\u7ea7\u79bb\u6563\u5316\u6a21\u62df\uff0c\u540c\u65f6\u4fdd\u6301\u5e95\u5c42\u73af\u5883\u4fdd\u771f\u5ea6\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7ec6\u8282\u771f\u5b9e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u73b0\u4ee3\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u6a21\u62df\u73af\u5883\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7ec6\u8282\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f20\u7edf\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u8def\u5f84\u70b9\u7cfb\u7edf\u7b80\u5316\u548c\u79bb\u6563\u5316\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\uff0c\u914d\u5408\u57fa\u4e8e\u771f\u5b9eCS:GO\u6bd4\u8d5b\u6570\u636e\u8bad\u7ec3\u7684\u795e\u7ecf\u9884\u6d4b\u548c\u751f\u6210\u6a21\u578b\u6765\u91cd\u5efa\u4e8b\u4ef6\u7ed3\u679c\u3002\u4ec5\u4f7f\u7528\u79fb\u52a8\u51b3\u7b56\u4f5c\u4e3a\u6218\u672f\u5b9a\u4f4d\uff0c\u65e0\u9700\u663e\u5f0f\u5efa\u6a21\u7784\u51c6\u548c\u5c04\u51fb\u7b49\u4f4e\u7ea7\u673a\u5236\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u4ece\u4eba\u7c7b\u6570\u636e\u751f\u6210\u7684DECOY\u56de\u653e\u4e0e\u539f\u59cb\u6e38\u620f\u4e2d\u89c2\u5bdf\u5230\u7684\u56de\u653e\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "DECOY\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u63a8\u8fdb\u6218\u7565\u591a\u667a\u80fd\u4f53\u89c4\u5212\u548c\u884c\u4e3a\u751f\u6210\u7684\u7814\u7a76\uff0c\u5176\u516c\u5f00\u53ef\u7528\u7684\u6a21\u62df\u73af\u5883\u4e3a\u76f8\u5173\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2509.06481", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06481", "abs": "https://arxiv.org/abs/2509.06481", "authors": ["Vinita Sao", "Tu Dac Ho", "Sujoy Bhore", "P. B. Sujit"], "title": "Event Driven CBBA with Reduced Communication", "comment": null, "summary": "In various scenarios such as multi-drone surveillance and search-and-rescue\noperations, deploying multiple robots is essential to accomplish multiple tasks\nat once. Due to the limited communication range of these vehicles, a\ndecentralised task allocation algorithm is crucial for effective task\ndistribution among robots. The consensus-based bundle algorithm (CBBA) has been\npromising for multi-robot operation, offering theoretical guarantees. However,\nCBBA demands continuous communication, leading to potential congestion and\npacket loss that can hinder performance. In this study, we introduce an\nevent-driven communication mechanism designed to address these communication\nchallenges while maintaining the convergence and performance bounds of CBBA. We\ndemonstrate theoretically that the solution quality matches that of CBBA and\nvalidate the approach with Monte-Carlo simulations across varying targets,\nagents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can\nreduce message transmissions by up to 52%.", "AI": {"tldr": "\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u673a\u5236\u6539\u8fdbCBBA\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4fe1\u606f\u4f20\u8f93\u91cf", "motivation": "\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\uff0c\u4f20\u7edfCBBA\u7b97\u6cd5\u9700\u8981\u8fde\u7eed\u901a\u4fe1\uff0c\u5bb9\u6613\u9020\u6210\u7f51\u7edc\u585e\u585e\u548c\u6570\u636e\u5305\u4e22\u5931\uff0c\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u673a\u5236\uff0c\u53ea\u5728\u9700\u8981\u65f6\u5019\u624d\u8fdb\u884c\u901a\u4fe1\uff0c\u4fdd\u6301\u4e86CBBA\u7684\u6536\u655b\u6027\u548c\u6027\u80fd\u4e0a\u754c", "result": "\u7406\u8bba\u8bc1\u660e\u89e3\u7684\u8d28\u91cf\u4e0eCBBA\u76f8\u540c\uff0c\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u53ef\u5c06\u6d88\u606f\u4f20\u8f93\u91cf\u51cf\u5c11\u8fbe52%", "conclusion": "ED-CBBA\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4efb\u52a1\u5206\u914d\u4e2d\u7684\u901a\u4fe1\u6311\u6218\uff0c\u5728\u4fdd\u6301\u7b97\u6cd5\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387"}}
{"id": "2509.06597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06597", "abs": "https://arxiv.org/abs/2509.06597", "authors": ["Frederik Plahl", "Georgios Katranis", "Ilshat Mamaev", "Andrey Morozov"], "title": "LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods", "comment": "Preprint of final paper that will appear in the Proceedings of the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\n  2025)", "summary": "We present LiHRA, a novel dataset designed to facilitate the development of\nautomated, learning-based, or classical risk monitoring (RM) methods for\nHuman-Robot Interaction (HRI) scenarios. The growing prevalence of\ncollaborative robots in industrial environments has increased the need for\nreliable safety systems. However, the lack of high-quality datasets that\ncapture realistic human-robot interactions, including potentially dangerous\nevents, slows development. LiHRA addresses this challenge by providing a\ncomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body\nkeypoints, and robot joint states, capturing the complete spatial and dynamic\ncontext of human-robot collaboration. This combination of modalities allows for\nprecise tracking of human movement, robot actions, and environmental\nconditions, enabling accurate RM during collaborative tasks. The LiHRA dataset\ncovers six representative HRI scenarios involving collaborative and coexistent\ntasks, object handovers, and surface polishing, with safe and hazardous\nversions of each scenario. In total, the data set includes 4,431 labeled point\nclouds recorded at 10 Hz, providing a rich resource for training and\nbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrate\nLiHRA's utility, we introduce an RM method that quantifies the risk level in\neach scenario over time. This method leverages contextual information,\nincluding robot states and the dynamic model of the robot. With its combination\nof high-resolution LiDAR data, precise human tracking, robot state data, and\nrealistic collision events, LiHRA offers an essential foundation for future\nresearch into real-time RM and adaptive safety strategies in human-robot\nworkspaces.", "AI": {"tldr": "LiHRA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u98ce\u9669\u76d1\u63a7\u7814\u7a76\uff0c\u5305\u542b3D LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u5173\u952e\u70b9\u548c\u673a\u5668\u4eba\u5173\u8282\u72b6\u6001\u6570\u636e\uff0c\u8986\u76d66\u79cdHRI\u573a\u666f\u76844431\u4e2a\u6807\u8bb0\u70b9\u4e91\u3002", "motivation": "\u5de5\u4e1a\u73af\u5883\u4e2d\u534f\u4f5c\u673a\u5668\u4eba\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u4eba\u673a\u4ea4\u4e92\u6570\u636e\u96c6\uff08\u5305\u62ec\u5371\u9669\u4e8b\u4ef6\uff09\u963b\u788d\u4e86\u53ef\u9760\u5b89\u5168\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u4f9b\u5305\u542b3D LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u5173\u952e\u70b9\u548c\u673a\u5668\u4eba\u5173\u8282\u72b6\u6001\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6355\u6349\u4eba\u673a\u534f\u4f5c\u7684\u5b8c\u6574\u7a7a\u95f4\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\uff0c\u8986\u76d66\u79cd\u4ee3\u8868\u6027HRI\u573a\u666f\u7684\u5b89\u5168\u548c\u5371\u9669\u7248\u672c\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b4431\u4e2a\u4ee510Hz\u8bb0\u5f55\u7684\u6807\u8bb0\u70b9\u4e91\uff0c\u4e3a\u4f20\u7edf\u548cAI\u9a71\u52a8\u7684\u98ce\u9669\u76d1\u63a7\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\u3002", "conclusion": "LiHRA\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387LiDAR\u6570\u636e\u3001\u7cbe\u786e\u4eba\u4f53\u8ffd\u8e2a\u3001\u673a\u5668\u4eba\u72b6\u6001\u6570\u636e\u548c\u771f\u5b9e\u78b0\u649e\u4e8b\u4ef6\uff0c\u4e3a\u5b9e\u65f6\u98ce\u9669\u76d1\u63a7\u548c\u81ea\u9002\u5e94\u5b89\u5168\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2509.06025", "categories": ["cs.LG", "cs.AI", "68T07, 62M20", "I.2.6; H.2.8; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.06025", "abs": "https://arxiv.org/abs/2509.06025", "authors": ["Vignesh Ethiraj", "Subhash Talluri"], "title": "Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior", "comment": null, "summary": "A central goal of artificial intelligence is to build systems that can\nunderstand and predict complex, evolving sequences of events. However, current\nfoundation models, designed for natural language, fail to grasp the holistic\nnature of structured interactions found in domains like telecommunications,\ne-commerce and finance. By serializing events into text, they disassemble them\ninto semantically fragmented parts, losing critical context. In this work, we\nintroduce the Unified Interaction Foundation Model (UIFM), a foundation model\nengineered for genuine behavioral understanding. At its core is the principle\nof composite tokenization, where each multi-attribute event is treated as a\nsingle, semantically coherent unit. This allows UIFM to learn the underlying\n\"grammar\" of user behavior, perceiving entire interactions rather than a\ndisconnected stream of data points. We demonstrate that this architecture is\nnot just more accurate, but represents a fundamental step towards creating more\nadaptable and intelligent predictive systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7edf\u4e00\u4ea4\u4e92\u57fa\u7840\u6a21\u578b(UIFM)\uff0c\u91c7\u7528\u590d\u5408\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u591a\u5c5e\u6027\u4e8b\u4ef6\u4f5c\u4e3a\u8bed\u4e49\u8fde\u8d2f\u5355\u5143\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u5728\u5e8f\u5217\u5316\u4e8b\u4ef6\u65f6\u4e22\u5931\u5173\u952e\u4e0a\u4e0b\u6587\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u7406\u89e3\u7535\u4fe1\u3001\u7535\u5546\u548c\u91d1\u878d\u7b49\u9886\u57df\u4e2d\u7ed3\u6784\u5316\u4ea4\u4e92\u7684\u6574\u4f53\u6027\u8d28\uff0c\u901a\u8fc7\u5c06\u4e8b\u4ef6\u5e8f\u5217\u5316\u4e3a\u6587\u672c\u4f1a\u7834\u574f\u8bed\u4e49\u5b8c\u6574\u6027", "method": "\u91c7\u7528\u590d\u5408\u6807\u8bb0\u5316\u539f\u5219\uff0c\u5c06\u6bcf\u4e2a\u591a\u5c5e\u6027\u4e8b\u4ef6\u89c6\u4e3a\u5355\u4e2a\u8bed\u4e49\u8fde\u8d2f\u5355\u5143\uff0c\u5b66\u4e60\u7528\u6237\u884c\u4e3a\u7684\u5e95\u5c42\"\u8bed\u6cd5\"", "result": "\u8be5\u67b6\u6784\u4e0d\u4ec5\u66f4\u51c6\u786e\uff0c\u800c\u4e14\u4ee3\u8868\u4e86\u521b\u5efa\u66f4\u9002\u5e94\u548c\u667a\u80fd\u9884\u6d4b\u7cfb\u7edf\u7684\u6839\u672c\u6027\u8fdb\u6b65", "conclusion": "UIFM\u6a21\u578b\u80fd\u591f\u611f\u77e5\u5b8c\u6574\u7684\u4ea4\u4e92\u800c\u975e\u65ad\u5f00\u7684\u6570\u636e\u6d41\uff0c\u662f\u5b9e\u73b0\u771f\u6b63\u884c\u4e3a\u7406\u89e3\u7684\u91cd\u8981\u6b65\u9aa4"}}
{"id": "2509.06277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06277", "abs": "https://arxiv.org/abs/2509.06277", "authors": ["Jinju Kim", "Taehan Kim", "Abdul Waheed", "Rita Singh"], "title": "No Encore: Unlearning as Opt-Out in Music Generation", "comment": "Work in progress. 7 pages", "summary": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\u4e2d\u5e94\u7528\u673a\u5668\u5fd8\u5370\u6280\u672f\u6765\u907f\u514d\u4f7f\u7528\u7248\u6743\u521b\u4f5c\u5185\u5bb9\u7684\u521d\u6b65\u7814\u7a76\u6210\u679c\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u5728\u521b\u9020\u6027\u884c\u4e1a\u4e2d\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5b58\u5728\u5229\u7528\u7248\u6743\u521b\u4f5c\u7684\u98ce\u9669\uff0c\u5f15\u53d1\u4e86\u9053\u5fb7\u548c\u6cd5\u5f8b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5c06\u73b0\u6709\u7684\u673a\u5668\u5fd8\u5370\u6280\u672f\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u97f3\u4e50\u57fa\u7ebf\u6a21\u578b\uff0c\u5206\u6790\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5fd8\u5370\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63d0\u4f9b\u4e86\u5728\u97f3\u4e50\u751f\u6210\u4e2d\u5e94\u7528\u5fd8\u5370\u6280\u672f\u7684\u6311\u6218\u548c\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u97f3\u4e50\u751f\u6210\u6a21\u578b\u7684\u673a\u5668\u5fd8\u5370\u5e94\u7528\u63d0\u4f9b\u4e86\u521d\u6b65\u5206\u6790\u548c\u57fa\u7840\u6027\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u7248\u6743\u4f7f\u7528\u7684\u9053\u5fb7\u98ce\u9669\u3002"}}
{"id": "2509.05746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05746", "abs": "https://arxiv.org/abs/2509.05746", "authors": ["Tianhao Guo", "Bingjie Lu", "Feng Wang", "Zhengyang Lu"], "title": "Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation", "comment": null, "summary": "Single image super-resolution traditionally assumes spatially-invariant\ndegradation models, yet real-world imaging systems exhibit complex\ndistance-dependent effects including atmospheric scattering, depth-of-field\nvariations, and perspective distortions. This fundamental limitation\nnecessitates spatially-adaptive reconstruction strategies that explicitly\nincorporate geometric scene understanding for optimal performance. We propose a\nrigorous variational framework that characterizes super-resolution as a\nspatially-varying inverse problem, formulating the degradation operator as a\npseudodifferential operator with distance-dependent spectral characteristics\nthat enable theoretical analysis of reconstruction limits across depth ranges.\nOur neural architecture implements discrete gradient flow dynamics through\ncascaded residual blocks with depth-conditional convolution kernels, ensuring\nconvergence to stationary points of the theoretical energy functional while\nincorporating learned distance-adaptive regularization terms that dynamically\nadjust smoothness constraints based on local geometric structure. Spectral\nconstraints derived from atmospheric scattering theory prevent bandwidth\nviolations and noise amplification in far-field regions, while adaptive kernel\ngeneration networks learn continuous mappings from depth to reconstruction\nfilters. Comprehensive evaluation across five benchmark datasets demonstrates\nstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM\nat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by\n0.44dB and 0.36dB respectively. This work establishes the first\ntheoretically-grounded distance-adaptive super-resolution framework and\ndemonstrates significant improvements on depth-variant scenarios while\nmaintaining competitive performance across traditional benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c34\u52a8\u529b\u5b66\u6846\u67b6\u7684\u8ddd\u79bb\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u6761\u4ef6\u5377\u79ef\u5185\u6838\u548c\u5b66\u4e60\u7684\u8ddd\u79bb\u9002\u5e94\u6027\u6b63\u5219\u5316\u9879\u6765\u5904\u7406\u5b9e\u9645\u5f71\u50cf\u4e2d\u7684\u7a7a\u95f4\u53d8\u5316\u964d\u7ea7\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u72ec\u521b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5355\u5f71\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5047\u8bbe\u7a7a\u95f4\u4e0d\u53d8\u7684\u964d\u7ea7\u6a21\u578b\uff0c\u4f46\u771f\u5b9e\u5f71\u50cf\u7cfb\u7edf\u5b58\u5728\u590d\u6742\u7684\u8ddd\u79bb\u4f9d\u8d56\u6548\u5e94\uff0c\u5982\u5927\u6c14\u6563\u5c04\u3001\u666f\u6df1\u53d8\u5316\u548c\u900f\u89c6\u626d\u66f2\u7b49\uff0c\u9700\u8981\u7a7a\u95f4\u9002\u5e94\u6027\u91cd\u5efa\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e25\u683c\u7684\u53d8\u5206\u6846\u67b6\uff0c\u5c06\u8d85\u5206\u8fa8\u7387\u5f62\u5f0f\u5316\u4e3a\u7a7a\u95f4\u53d8\u5316\u7684\u9006\u95ee\u9898\uff0c\u901a\u8fc7\u6df1\u5ea6\u6761\u4ef6\u5377\u79ef\u5185\u6838\u7684\u7ea7\u8054\u6b8a\u4f59\u5757\u5b9e\u73b0\u79bb\u6563\u68af\u5ea6\u6d41\u52a8\u529b\u5b66\uff0c\u5e76\u5305\u542b\u5b66\u4e60\u7684\u8ddd\u79bb\u9002\u5e94\u6027\u6b63\u5219\u5316\u9879\u3002", "result": "\u57285\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728KITTI\u6237\u5916\u573a\u666f\u4e2d\u57282\u500d\u548c4\u500d\u7f29\u653e\u4e0b\u5206\u522b\u83b7\u5f9736.89/0.9516\u548c30.54/0.8721\u7684PSNR/SSIM\u503c\uff0c\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd50.44dB\u548c0.36dB\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u575a\u5b9e\u7684\u8ddd\u79bb\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5728\u6df1\u5ea6\u53d8\u5316\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u5728\u4f20\u7edf\u6807\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002"}}
{"id": "2509.06741", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06741", "abs": "https://arxiv.org/abs/2509.06741", "authors": ["Christian Geckeler", "Niklas Neugebauer", "Manasi Muglikar", "Davide Scaramuzza", "Stefano Mintchev"], "title": "Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest\nenvironments for tasks such as environmental monitoring and search and rescue,\nwhich require safe navigation through dense foliage and precise data\ncollection. Traditional sensing approaches, including passive multispectral and\nRGB imaging, suffer from latency, poor depth resolution, and strong dependence\non ambient light - especially under forest canopies. In this work, we present a\nnovel event spectroscopy system that simultaneously enables high-resolution,\nlow-latency depth reconstruction and multispectral imaging using a single\nsensor. Depth is reconstructed using structured light, and by modulating the\nwavelength of the projected structured light, our system captures spectral\ninformation in controlled bands between 650 nm and 850 nm. We demonstrate up to\n$60\\%$ improvement in RMSE over commercial depth sensors and validate the\nspectral accuracy against a reference spectrometer and commercial multispectral\ncameras, demonstrating comparable performance. A portable version limited to\nRGB (3 wavelengths) is used to collect real-world depth and spectral data from\na Masoala Rainforest. We demonstrate the use of this prototype for color image\nreconstruction and material differentiation between leaves and branches using\nspectral and depth data. Our results show that adding depth (available at no\nextra effort with our setup) to material differentiation improves the accuracy\nby over $30\\%$ compared to color-only method. Our system, tested in both lab\nand real-world rainforest environments, shows strong performance in depth\nestimation, RGB reconstruction, and material differentiation - paving the way\nfor lightweight, integrated, and robust UAV perception and data collection in\ncomplex natural environments.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u578b\u4e8b\u4ef6\u5149\u8c31\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u4e2a\u4f20\u611f\u5668\u540c\u65f6\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u91cd\u5efa\u548c\u591a\u5149\u8c31\u6210\u50cf\uff0c\u5728\u68ee\u6797\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u80fd\u529b", "motivation": "\u4f20\u7edf\u611f\u77e5\u65b9\u6cd5\u5728\u68ee\u6797\u73af\u5883\u4e2d\u5b58\u5728\u5ef6\u8fdf\u3001\u6df1\u5ea6\u5206\u8fa8\u7387\u4f4e\u3001\u5149\u7167\u4f9d\u8d56\u6027\u5f3a\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65e0\u4eba\u673a\u5728\u5bc6\u6797\u4e2d\u5b89\u5168\u5bfc\u822a\u548c\u7cbe\u786e\u6570\u636e\u91c7\u96c6\u7684\u9700\u6c42", "method": "\u8bbe\u8ba1\u4e86\u4e8b\u4ef6\u5149\u8c31\u7cfb\u7edf\uff0c\u5229\u7528\u7ed3\u6784\u5149\u8fdb\u884c\u6df1\u5ea6\u91cd\u5efa\uff0c\u901a\u8fc7\u8c03\u5236\u6295\u5c04\u7ed3\u6784\u5149\u7684\u6ce2\u957f\u6765\u83b7\u53d6650-850nm\u8303\u56f4\u5185\u7684\u5149\u8c31\u4fe1\u606f\uff0c\u5f00\u53d1\u4e86\u4ec5\u652f\u6301RGB\u7684\u7b3c\u7248\u7cfb\u7edf", "result": "\u4e0e\u5546\u4e1a\u6df1\u5ea6\u4f20\u611f\u5668\u76f8\u6bd4RMSE\u63d0\u534760%\uff0c\u5149\u8c31\u51c6\u786e\u6027\u4e0e\u53c2\u8003\u5149\u8c31\u4eea\u548c\u5546\u4e1a\u591a\u5149\u8c31\u76f8\u673a\u76f8\u5f53\uff0c\u5728\u9a6c\u7d22\u62c9\u96e8\u6797\u83b7\u5f97\u771f\u5b9e\u6570\u636e\uff0c\u7ed3\u5408\u6df1\u5ea6\u548c\u5149\u8c31\u6570\u636e\u8bc6\u522b\u53f6\u5b50\u548c\u6811\u679d\u7684\u51c6\u786e\u6027\u6bd4\u4ec5\u4f7f\u7528\u989c\u8272\u7684\u65b9\u6cd5\u63d0\u534730%", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u96e8\u6797\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8270\u56fa\u7684\u6df1\u5ea6\u4f30\u8ba1\u3001RGB\u91cd\u5efa\u548c\u6750\u6599\u533a\u5206\u80fd\u529b\uff0c\u4e3a\u8f7b\u91cf\u5316\u3001\u96c6\u6210\u5316\u7684\u65e0\u4eba\u673a\u611f\u77e5\u548c\u6570\u636e\u91c7\u96c6\u5728\u590d\u6742\u81ea\u7136\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u4e86\u65b0\u8def\u5f84"}}
{"id": "2509.05913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05913", "abs": "https://arxiv.org/abs/2509.05913", "authors": ["Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Tamanna Shermin", "Md Rafiqul Islam", "Mukhtar Hussain", "Sami Azam"], "title": "A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features", "comment": "16 pages, 6 figures, 8 tables", "summary": "Musculoskeletal disorders pose significant risks to athletes, and assessing\nrisk early is important for prevention. However, most existing methods are\ndesigned for controlled settings and fail to reliably assess risk in complex\nenvironments due to their reliance on a single type of data. This research\nproposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel\nmultimodal deep learning framework designed to classify musculoskeletal risk\nusing visual and skeletal coordinate-based features. In addition, a custom\nmultimodal dataset is constructed by combining visual data and skeletal\ncoordinates for risk assessment. Each sample is labeled into eight risk\ncategories based on the Rapid Entire Body Assessment system. ViSK-GAT combines\na Residual Block with a Lightweight Transformer Block to learn spatial and\ntemporal dependencies jointly. It incorporates two novel modules: the\nFine-Grained Attention Module (FGAM), which enables precise inter-modal feature\nrefinement through cross-attention between visual and skeletal inputs, and the\nMultimodal Geometric Correspondence Module (MGCM), which enhances cross-modal\ncoherence by aligning image features with coordinate-based representations.\nViSK-GAT achieved strong performance with validation and test accuracies of\n93.55\\% and 93.89\\%, respectively; a precision of 93.86\\%; an F1 score of\n93.85\\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\\%. The\nregression results also indicated a low Root Mean Square Error of the predicted\nprobability distribution of 0.1205 and a corresponding Mean Absolute Error of\n0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT\nconsistently outperformed previous methods. The ViSK-GAT model advances\nartificial intelligence implementation and application, transforming\nmusculoskeletal risk classification and enabling impactful early interventions\nin sports.", "AI": {"tldr": "ViSK-GAT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u9aa8\u9abc\u5750\u6807\u7279\u5f81\uff0c\u7528\u4e8e\u808c\u8089\u9aa8\u9abc\u98ce\u9669\u5206\u7c7b\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8693%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u808c\u8089\u9aa8\u9abc\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u5927\u591a\u8bbe\u8ba1\u7528\u4e8e\u53d7\u63a7\u73af\u5883\uff0c\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u7c7b\u578b\uff0c\u65e0\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u53ef\u9760\u8bc4\u4f30\u98ce\u9669\u3002", "method": "\u63d0\u51faViSK-GAT\u6846\u67b6\uff0c\u7ed3\u5408\u6b8b\u5dee\u5757\u548c\u8f7b\u91cf\u7ea7Transformer\u5757\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u6ce8\u610f\u529b\u6a21\u5757(FGAM)\u548c\u591a\u6a21\u6001\u51e0\u4f55\u5bf9\u5e94\u6a21\u5757(MGCM)\uff0c\u4f7f\u7528\u89c6\u89c9\u548c\u9aa8\u9abc\u5750\u6807\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u9a8c\u8bc1\u51c6\u786e\u738793.55%\uff0c\u6d4b\u8bd5\u51c6\u786e\u738793.89%\uff0c\u7cbe\u786e\u5ea693.86%\uff0cF1\u5206\u657093.85%\uff0c\u5747\u4f18\u4e8e9\u79cd\u6d41\u884c\u7684\u8fc1\u79fb\u5b66\u4e60\u9aa8\u5e72\u7f51\u7edc\u3002", "conclusion": "ViSK-GAT\u6a21\u578b\u63a8\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u808c\u8089\u9aa8\u9abc\u98ce\u9669\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u65e9\u671f\u5e72\u9884\u3002"}}
{"id": "2509.05949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05949", "abs": "https://arxiv.org/abs/2509.05949", "authors": ["Qiqi Zhan", "Shiwei Li", "Qingjie Liu", "Yunhong Wang"], "title": "AttriPrompt: Dynamic Prompt Composition Learning for CLIP", "comment": null, "summary": "The evolution of prompt learning methodologies has driven exploration of\ndeeper prompt designs to enhance model performance. However, current deep text\nprompting approaches suffer from two critical limitations: Over-reliance on\nconstrastive learning objectives that prioritize high-level semantic alignment,\nneglecting fine-grained feature optimization; Static prompts across all input\ncategories, preventing content-aware adaptation. To address these limitations,\nwe propose AttriPrompt-a novel framework that enhances and refines textual\nsemantic representations by leveraging the intermediate-layer features of\nCLIP's vision encoder. We designed an Attribute Retrieval module that first\nclusters visual features from each layer. The aggregated visual features\nretrieve semantically similar prompts from a prompt pool, which are then\nconcatenated to the input of every layer in the text encoder. Leveraging\nhierarchical visual information embedded in prompted text features, we\nintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.\nFurthermore, we introduce a Self-Regularization mechanism by applying explicit\nregularization constraints between the prompted and non-prompted text features\nto prevent overfitting on limited training data. Extensive experiments across\nthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-art\nmethods, achieving up to 7.37\\% improvement in the base-to-novel setting. The\nobserved strength of our method in cross-domain knowledge transfer positions\nvision-language pre-trained models as more viable solutions for real-world\nimplementation.", "AI": {"tldr": "AttriPrompt\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6df1\u5ea6\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u7279\u5f81\u6765\u589e\u5f3a\u6587\u672c\u8bed\u4e49\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u6587\u672c\u63d0\u793a\u65b9\u6cd5\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u548c\u9759\u6001\u63d0\u793a\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u6587\u672c\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u8fc7\u5ea6\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u53ea\u5173\u6ce8\u9ad8\u5c42\u8bed\u4e49\u5bf9\u9f50\u800c\u5ffd\u7565\u7ec6\u7c92\u5ea6\u7279\u5f81\u4f18\u5316\uff1b2\uff09\u6240\u6709\u8f93\u5165\u7c7b\u522b\u4f7f\u7528\u9759\u6001\u63d0\u793a\uff0c\u7f3a\u4e4f\u5185\u5bb9\u611f\u77e5\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51faAttriPrompt\u6846\u67b6\uff0c\u5305\u542b\u5c5e\u6027\u68c0\u7d22\u6a21\u5757\uff08\u805a\u7c7b\u89c6\u89c9\u7279\u5f81\u5e76\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u7684\u63d0\u793a\uff09\u3001\u53cc\u6d41\u5bf9\u6bd4\u5b66\u4e60\uff08\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff09\u548c\u81ea\u6b63\u5219\u5316\u673a\u5236\uff08\u9632\u6b62\u8fc7\u62df\u5408\uff09\u3002\u901a\u8fc7\u5c06\u68c0\u7d22\u5230\u7684\u63d0\u793a\u8fde\u63a5\u5230\u6587\u672c\u7f16\u7801\u5668\u7684\u6bcf\u4e00\u5c42\u8f93\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAttriPrompt\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728base-to-novel\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.37%\u7684\u6539\u8fdb\uff0c\u5728\u8de8\u57df\u77e5\u8bc6\u8f6c\u79fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u6210\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u66f4\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5c42\u89c6\u89c9\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.06902", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06902", "abs": "https://arxiv.org/abs/2509.06902", "authors": ["Aivin V. Solatorio"], "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.", "AI": {"tldr": "Proof-Carrying Numbers (PCN) \u662f\u4e00\u79cd\u5c55\u793a\u5c42\u534f\u8bae\uff0c\u901a\u8fc7\u673a\u68b0\u9a8c\u8bc1\u6765\u786e\u4fdd\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6570\u5b57\u7684\u4fdd\u771f\u5ea6\uff0c\u5c06\u9a8c\u8bc1\u653e\u5728\u6e32\u67d3\u5668\u800c\u975e\u6a21\u578b\u4e2d\uff0c\u9632\u6b62\u6570\u5b57\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u968f\u673a\u7cfb\u7edf\u53ef\u80fd\u751f\u6210\u504f\u79bb\u53ef\u7528\u6570\u636e\u7684\u6570\u5b57\uff0c\u5373\u6570\u5b57\u5e7b\u89c9\u95ee\u9898\u3002\u73b0\u6709\u7684\u5b89\u5168\u63aa\u65bd\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5f15\u7528\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff09\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u4fdd\u771f\u5ea6\u3002", "method": "PCN\u534f\u8bae\u5c06\u6570\u5b57\u8de8\u5ea6\u4f5c\u4e3a\u4e0e\u7ed3\u6784\u5316\u58f0\u660e\u7ed1\u5b9a\u7684\u58f0\u660e\u7ed1\u5b9a\u4ee4\u724c\u53d1\u51fa\uff0c\u9a8c\u8bc1\u5668\u6839\u636e\u58f0\u660e\u7684\u7b56\u7565\uff08\u5982\u7cbe\u786e\u76f8\u7b49\u3001\u820d\u5165\u3001\u522b\u540d\u6216\u5e26\u9650\u5b9a\u7b26\u7684\u5bb9\u5dee\uff09\u68c0\u67e5\u6bcf\u4e2a\u4ee4\u724c\u3002\u9a8c\u8bc1\u5728\u6e32\u67d3\u5668\u4e2d\u8fdb\u884c\uff0c\u53ea\u6709\u7ecf\u8fc7\u58f0\u660e\u68c0\u67e5\u7684\u6570\u5b57\u624d\u88ab\u6807\u8bb0\u4e3a\u5df2\u9a8c\u8bc1\u3002", "result": "PCN\u88ab\u5f62\u5f0f\u5316\u5e76\u8bc1\u660e\u4e86\u58f0\u97f3\u6027\u3001\u8bda\u5b9e\u4ee4\u724c\u4e0b\u7684\u5b8c\u5907\u6027\u3001\u6545\u969c\u5173\u95ed\u884c\u4e3a\u548c\u7b56\u7565\u7ec6\u5316\u4e0b\u7684\u5355\u8c03\u6027\u3002PCN\u8f7b\u91cf\u4e14\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u5e94\u7528\u4e2d\uff0c\u5e76\u53ef\u6269\u5c55\u52a0\u5bc6\u627f\u8bfa\u3002", "conclusion": "\u901a\u8fc7\u5728\u663e\u793a\u524d\u5f3a\u5236\u8fdb\u884c\u9a8c\u8bc1\uff0cPCN\u4e3a\u6570\u5b57\u654f\u611f\u73af\u5883\u5efa\u7acb\u4e86\u4e00\u4e2a\u7b80\u5355\u5951\u7ea6\uff1a\u4fe1\u4efb\u53ea\u80fd\u901a\u8fc7\u8bc1\u660e\u83b7\u5f97\uff0c\u800c\u7f3a\u5c11\u6807\u8bb0\u5219\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u6709\u6548\u9632\u6b62\u6570\u5b57\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2509.06483", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06483", "abs": "https://arxiv.org/abs/2509.06483", "authors": ["Guanjie Cheng", "Boyi Li", "Peihan Wu", "Feiyi Chen", "Xinkui Zhao", "Mengying Zhu", "Shuiguang Deng"], "title": "DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT", "comment": null, "summary": "The wide spreading of Internet of Things (IoT) sensors generates vast\nspatio-temporal data streams, but ensuring data credibility is a critical yet\nunsolved challenge for applications like smart homes. While spatio-temporal\ngraph (STG) models are a leading paradigm for such data, they often fall short\nin dynamic, human-centric environments due to two fundamental limitations: (1)\ntheir reliance on static graph topologies, which fail to capture physical,\nevent-driven dynamics, and (2) their tendency to confuse spurious correlations\nwith true causality, undermining robustness in human-centric environments. To\naddress these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network\n(DyC-STG), a novel framework designed for real-time data credibility analysis\nin IoT. Our framework features two synergistic contributions: an event-driven\ndynamic graph module that adapts the graph topology in real-time to reflect\nphysical state changes, and a causal reasoning module to distill causally-aware\nrepresentations by strictly enforcing temporal precedence. To facilitate the\nresearch in this domain we release two new real-world datasets. Comprehensive\nexperiments show that DyC-STG establishes a new state-of-the-art, outperforming\nthe strongest baselines by 1.4 percentage points and achieving an F1-Score of\nup to 0.930.", "AI": {"tldr": "DyC-STG\u662f\u4e00\u4e2a\u7528\u4e8e\u7269\u8054\u7f51\u6570\u636e\u53ef\u4fe1\u5ea6\u5206\u6790\u7684\u52a8\u6001\u56e0\u679c\u65f6\u7a7a\u56fe\u7f51\u7edc\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u52a8\u6001\u56fe\u6a21\u5757\u548c\u56e0\u679c\u63a8\u7406\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSTG\u6a21\u578b\u5728\u9759\u6001\u62d3\u6251\u548c\u4f2a\u76f8\u5173\u6027\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027", "motivation": "\u7269\u8054\u7f51\u4f20\u611f\u5668\u4ea7\u751f\u5927\u91cf\u65f6\u7a7a\u6570\u636e\u6d41\uff0c\u4f46\u6570\u636e\u53ef\u4fe1\u5ea6\u662f\u672a\u89e3\u51b3\u7684\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u65f6\u7a7a\u56fe\u6a21\u578b\u5728\u52a8\u6001\u4eba\u672c\u73af\u5883\u4e2d\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u5c40\u9650\uff1a\u9759\u6001\u56fe\u62d3\u6251\u65e0\u6cd5\u6355\u6349\u7269\u7406\u4e8b\u4ef6\u9a71\u52a8\u7684\u52a8\u6001\u6027\uff0c\u4ee5\u53ca\u5bb9\u6613\u6df7\u6dc6\u4f2a\u76f8\u5173\u6027\u548c\u771f\u5b9e\u56e0\u679c\u5173\u7cfb", "method": "\u63d0\u51faDyC-STG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u8d21\u732e\uff1a\u4e8b\u4ef6\u9a71\u52a8\u7684\u52a8\u6001\u56fe\u6a21\u5757\u5b9e\u65f6\u8c03\u6574\u56fe\u62d3\u6251\u4ee5\u53cd\u6620\u7269\u7406\u72b6\u6001\u53d8\u5316\uff0c\u4ee5\u53ca\u56e0\u679c\u63a8\u7406\u6a21\u5757\u901a\u8fc7\u4e25\u683c\u5f3a\u5236\u6267\u884c\u65f6\u95f4\u4f18\u5148\u539f\u5219\u6765\u63d0\u53d6\u56e0\u679c\u611f\u77e5\u8868\u793a", "result": "\u7efc\u5408\u5b9e\u9a8c\u663e\u793aDyC-STG\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad8\u51fa1.4\u4e2a\u767e\u5206\u70b9\uff0cF1\u5206\u6570\u6700\u9ad8\u8fbe\u52300.930\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u4e24\u4e2a\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6", "conclusion": "DyC-STG\u901a\u8fc7\u52a8\u6001\u56fe\u62d3\u6251\u548c\u56e0\u679c\u63a8\u7406\u7684\u6709\u6548\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u8054\u7f51\u65f6\u7a7a\u6570\u636e\u53ef\u4fe1\u5ea6\u5206\u6790\u7684\u6027\u80fd\uff0c\u4e3a\u4eba\u672c\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u6570\u636e\u53ef\u4fe1\u5ea6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.06767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06767", "abs": "https://arxiv.org/abs/2509.06767", "authors": ["Zijie Ning", "Enmin Lin", "Sudarshan R. Iyengar", "Patrick Vandewalle"], "title": "Raw2Event: Converting Raw Frame Camera into Event Camera", "comment": "Submitted to IEEE Transactions on Robotics (Special Section on\n  Event-based Vision for Robotics), under review. This version is submitted for\n  peer review and may be updated upon acceptance", "summary": "Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md.", "AI": {"tldr": "Raw2Event\u662f\u4e00\u4e2a\u786c\u4ef6-\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u4f4e\u6210\u672c\u539f\u59cb\u5e27\u76f8\u673a\u5b9e\u65f6\u751f\u6210\u4e8b\u4ef6\u6d41\uff0c\u63d0\u4f9b\u6bd4RGB\u5e27\u8f6c\u6362\u5668\u66f4\u9ad8\u7684\u52a8\u6001\u8303\u56f4\u3001\u5206\u8fa8\u7387\u548c\u4fdd\u771f\u5ea6\u8f93\u51fa\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u867d\u7136\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u4f18\u52bf\uff0c\u4f46\u9ad8\u6210\u672c\u3001\u6709\u9650\u5206\u8fa8\u7387\u4ee5\u53ca\u7f3a\u4e4f\u81ea\u52a8\u5bf9\u7126\u7b49\u529f\u80fd\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u5f00\u53d1\u548c\u539f\u578b\u8bbe\u8ba1\u9636\u6bb5\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u8bbf\u95ee\u539f\u59cbBayer\u6570\u636e\u5e76\u7ed5\u8fc7\u4f20\u7edf\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\u5668(ISP)\uff0c\u57fa\u4e8eDVS-Voltmeter\u6a21\u578b\u6784\u5efa\u53ef\u914d\u7f6e\u7684\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u652f\u6301\u539f\u59cb\u6570\u636e\u3001RGB\u548c\u4e8b\u4ef6\u6d41\u540c\u6b65\u8bb0\u5f55\u7684\u6570\u636e\u91c7\u96c6\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eRaw2Event\u80fd\u591f\u751f\u6210\u4e0e\u771f\u5b9e\u4e8b\u4ef6\u76f8\u673a\u975e\u5e38\u76f8\u4f3c\u7684\u4e8b\u4ef6\u6d41\uff0c\u540c\u65f6\u53d7\u76ca\u4e8e\u66f4\u9ad8\u5206\u8fa8\u7387\u548c\u81ea\u52a8\u5bf9\u7126\u80fd\u529b\uff0c\u7cfb\u7edf\u8fd8\u652f\u6301\u7528\u6237\u76f4\u89c2\u7684\u53c2\u6570\u8c03\u4f18\uff0c\u5e76\u53ef\u5728\u6811\u8393\u6d3e\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u4e8b\u4ef6\u89c6\u89c9\u7814\u7a76\u548c\u65e9\u671f\u7cfb\u7edf\u5f00\u53d1\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5728\u7ebf\u5f00\u6e90\u3002"}}
