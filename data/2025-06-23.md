<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Assessing the impact of Binarization for Writer Identification in Greek Papyrus](https://arxiv.org/abs/2506.15852)
*Dominic Akt,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: 本文研究了希腊纸莎草文献的作者识别任务，重点探讨了图像二值化预处理对识别性能的影响，比较了传统方法与深度学习方法的效果。


<details>
  <summary>Details</summary>
Motivation: 历史文献（如希腊纸莎草）背景复杂（不均匀、碎片化、褪色且有纤维结构），传统二值化方法效果不佳，需探索更优方法以提高作者识别性能。

Method: 比较传统二值化方法与深度学习模型，结合自定义数据增强技术和不同模型选择标准，并在DIBCO 2019数据集上系统评估性能。

Result: 数据增强对深度学习方法显著有效，且二值化质量与后续作者识别性能强相关。

Conclusion: 二值化质量对作者识别至关重要，深度学习方法结合数据增强可显著提升性能。

Abstract: This paper tackles the task of writer identification for Greek papyri. A
common preprocessing step in writer identification pipelines is image
binarization, which prevents the model from learning background features. This
is challenging in historical documents, in our case Greek papyri, as background
is often non-uniform, fragmented, and discolored with visible fiber structures.
We compare traditional binarization methods to state-of-the-art Deep Learning
(DL) models, evaluating the impact of binarization quality on subsequent writer
identification performance. DL models are trained with and without a custom
data augmentation technique, as well as different model selection criteria are
applied. The performance of these binarization methods, is then systematically
evaluated on the DIBCO 2019 dataset. The impact of binarization on writer
identification is subsequently evaluated using a state-of-the-art approach for
writer identification. The results of this analysis highlight the influence of
data augmentation for DL methods. Furthermore, findings indicate a strong
correlation between binarization effectiveness on papyri documents of DIBCO
2019 and downstream writer identification performance.

</details>


### [2] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Main category: cs.CV

TL;DR: VideoSync是一个独立于特定特征提取方法的视频同步框架，适用于多种场景，并通过新数据集和严格评估框架证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视频同步方法依赖音频或特定视觉事件，适用性受限，且缺乏通用和可复现的基准。

Method: 提出VideoSync框架，不依赖特定特征提取方法，并在新数据集上评估，纠正了现有方法的偏见。

Result: VideoSync在公平实验条件下优于现有方法（如SeSyn-Net），并发现基于CNN的模型最有效。

Conclusion: VideoSync提升了视频同步的通用性和鲁棒性，适用于更广泛的现实应用。

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [3] [PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning](https://arxiv.org/abs/2506.16082)
*Yizhe Li,Sanping Zhou,Zheng Qin,Le Wang*

Main category: cs.CV

TL;DR: PR-DETR是一种新颖的密集视频字幕框架，通过显式位置和关系先验改进检测变换器，提升事件定位和字幕生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换器的方法隐式学习事件位置和语义，需要大量训练数据且性能受限。

Method: PR-DETR引入位置锚定查询和事件关系编码器，分别提供位置先验和关系先验。

Result: 在ActivityNet Captions和YouCook2数据集上表现优异。

Conclusion: 显式先验注入显著提升了密集视频字幕任务的性能。

Abstract: Dense video captioning is a challenging task that aims to localize and
caption multiple events in an untrimmed video. Recent studies mainly follow the
transformer-based architecture to jointly perform the two sub-tasks, i.e.,
event localization and caption generation, in an end-to-end manner. Based on
the general philosophy of detection transformer, these methods implicitly learn
the event locations and event semantics, which requires a large amount of
training data and limits the model's performance in practice. In this paper, we
propose a novel dense video captioning framework, named PR-DETR, which injects
the explicit position and relation prior into the detection transformer to
improve the localization accuracy and caption quality, simultaneously. On the
one hand, we first generate a set of position-anchored queries to provide the
scene-specific position and semantic information about potential events as
position prior, which serves as the initial event search regions to eliminate
the implausible event proposals. On the other hand, we further design an event
relation encoder to explicitly calculate the relationship between event
boundaries as relation prior to guide the event interaction to improve the
semantic coherence of the captions. Extensive ablation studies are conducted to
verify the effectiveness of the position and relation prior. Experimental
results also show the competitive performance of our method on ActivityNet
Captions and YouCook2 datasets.

</details>


### [4] [AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios](https://arxiv.org/abs/2506.16371)
*Yunhao Hou,Bochao Zou,Min Zhang,Ran Chen,Shangdong Yang,Yanmei Zhang,Junbao Zhuo,Siheng Chen,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 论文介绍了首个大规模真实世界数据集AGC-Drive，用于空中-地面协同3D感知，填补了现有数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注车辆间或车辆与基础设施的协同感知，忽视了无人机提供的空中视角。缺乏高质量数据集是主要原因。

Method: 通过两辆装备多摄像头和LiDAR的车辆及一架无人机收集数据，涵盖多种驾驶场景和动态交互事件。

Result: 数据集包含120K LiDAR帧和440K图像，覆盖14种场景，提供13类物体的3D标注，并发布开源工具包。

Conclusion: AGC-Drive填补了空中-地面协同感知数据集的空白，为相关研究提供了重要资源。

Abstract: By sharing information across multiple agents, collaborative perception helps
autonomous vehicles mitigate occlusions and improve overall perception
accuracy. While most previous work focus on vehicle-to-vehicle and
vehicle-to-infrastructure collaboration, with limited attention to aerial
perspectives provided by UAVs, which uniquely offer dynamic, top-down views to
alleviate occlusions and monitor large-scale interactive environments. A major
reason for this is the lack of high-quality datasets for aerial-ground
collaborative scenarios. To bridge this gap, we present AGC-Drive, the first
large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The
data collection platform consists of two vehicles, each equipped with five
cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and
a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.
Consisting of approximately 120K LiDAR frames and 440K images, the dataset
covers 14 diverse real-world driving scenarios, including urban roundabouts,
highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic
interaction events, including vehicle cut-ins, cut-outs, and frequent lane
changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and
fully annotated 3D bounding boxes covering 13 object categories. We provide
benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative
perception and vehicle-to-UAV collaborative perception. Additionally, we
release an open-source toolkit, including spatiotemporal alignment verification
tools, multi-agent visualization systems, and collaborative annotation
utilities. The dataset and code are available at
https://github.com/PercepX/AGC-Drive.

</details>


### [5] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: Part$^{2}$GS是一种用于建模多部分物体高保真几何和物理一致运动的新型框架，通过部分感知的3D高斯表示和物理约束实现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的关节物体建模结构和运动仍具挑战性，现有方法难以实现高保真几何和物理一致性。

Method: 采用部分感知的3D高斯表示，结合物理约束（如接触强制、速度一致性和矢量场对齐）和排斥点场防止碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部分的Chamfer Distance上比现有方法提升高达10倍。

Conclusion: Part$^{2}$GS通过结构化解耦变换和物理约束，显著提升了关节物体建模的几何保真度和运动一致性。

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [6] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MICS的新方法，用于生成高质量的医学推理路径数据，并构建了多任务医学推理数据集MMRP和医学MLLM模型Chiron-o1，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的多模态大语言模型（MLLMs）在推理能力上仍处于早期阶段，缺乏全面的框架来搜索和评估有效的推理路径。

Method: 提出MICS方法，通过导师模型初始化推理路径，实习模型继续推理，并根据MICS-Score选择最优路径，构建MMRP数据集和Chiron-o1模型。

Result: Chiron-o1在多个医学视觉问答和推理基准测试中达到最先进性能。

Conclusion: MICS方法有效提升了医学MLLMs的推理能力，Chiron-o1展示了强大的视觉问答和泛化推理能力。

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [7] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本的轻量级方法，利用大型语言模型（LLMs）替代传统的视觉语言模型（VLMs）来识别足球比赛中的关键动作，如进球、黄牌和换人。


<details>
  <summary>Details</summary>
Motivation: 传统视频分析方法计算复杂且昂贵，而专家评论提供了丰富的细粒度描述和上下文信息，足以可靠地识别关键动作。

Method: 使用SoccerNet Echoes数据集中的时间戳评论，通过三个专门评估结果、兴奋度和战术的LLM作为评委，滑动窗口分析评论以识别动作并生成准确时间戳。

Result: 实验表明，这种基于语言的方法在检测关键比赛事件上表现有效，为动作识别提供了一种轻量级且无需训练的替代方案。

Conclusion: 语言中心的方法为足球动作识别提供了一种高效、轻量级的解决方案，减少了计算负担。

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Main category: cs.LG

TL;DR: 论文提出了多粒度直接偏好优化（MDPO）方法，通过三个粒度优化LLMs的数学推理能力，解决了DPO在长链推理中的不足，并在实验中表现优于DPO及其变体。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法在长链数学推理中效果有限，无法有效捕捉偏好数据中的差异，且训练目标与生成指标不一致。

Method: 提出MDPO方法，在Solution2Solution、Inference2Inference和Step2Step三个粒度上优化LLMs的数学推理能力，并统一训练目标。

Result: 在Qwen2和Llama3模型上，GSM8K数据集提升1.7%和0.9%，MATH数据集提升2.3%和1.2%，优于DPO及其变体。

Conclusion: MDPO方法有效提升了LLMs的数学推理能力，并提供了无需人工标注的训练数据构建流程。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [9] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: Bohdi提出了一种基于合成数据的异构大语言模型融合框架，通过分层树结构和动态调整机制解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异构大语言模型融合方法依赖有限领域的真实数据且固定数据分配比例，导致知识获取不全面和能力不平衡。

Method: Bohdi通过分层树结构组织知识域，利用多模型协作生成合成数据，并通过DynaBranches机制动态调整数据采样比例。

Result: 实验表明Bohdi在多个基准测试中显著优于现有基线，数据效率更高且能力更平衡。

Conclusion: Bohdi通过合成数据和动态调整机制有效解决了异构大语言模型融合中的关键问题。

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [10] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Main category: cs.LG

TL;DR: DeepJ是一种新型图卷积变换器模型，用于捕捉电子健康记录（EHR）数据中的跨时间医疗事件交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法在建模跨时间医疗事件交互时存在不足，无法有效捕捉纵向事件间的依赖关系。

Method: 提出DeepJ模型，结合图卷积和变换器技术，采用可微分图池化方法，建模医疗事件的跨时间交互。

Result: DeepJ在预测患者风险方面显著优于五种基线模型，并提高了可解释性。

Conclusion: DeepJ为患者风险分层提供了更有效的工具，具有实际应用潜力。

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [11] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Main category: cs.LG

TL;DR: CRIA是一个自适应框架，通过跨视图信息整合和多视图特征融合，解决了EEG数据预训练中的特征提取和信息整合难题。


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法仅依赖单一视图的上下文语义，无法捕捉多视图间的复杂协同作用，限制了EEG表示学习的表达能力和泛化性。

Method: CRIA采用变长和变通道编码实现EEG数据的统一表示，利用跨注意力机制融合时间、频谱和空间特征，并结合信息瓶颈原则的注意力矩阵掩码策略。

Result: 在Temple University EEG corpus和CHB-MIT数据集上，CRIA在相同预训练条件下优于现有方法，多类事件分类和异常检测的平衡准确率分别为57.02%和80.03%。

Conclusion: CRIA通过多视图特征融合和自适应编码，显著提升了EEG表示学习的泛化能力和性能。

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [12] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Main category: cs.LG

TL;DR: 研究了在线博彩问题，庄家动态调整赔率以最大化利润并减少潜在损失，证明了最优损失是多项式最大根，并开发了高效算法。


<details>
  <summary>Details</summary>
Motivation: 探讨庄家如何在动态调整赔率时平衡公平性和财务风险，同时应对赌徒的最优策略。

Method: 通过多项式根和贝尔曼-帕累托前沿的显式表征，开发高效算法计算最优博彩策略。

Result: 庄家可实现任意公平性且避免财务风险，最优损失与埃尔米特多项式相关，算法在最优赌徒下达到最优损失。

Conclusion: 研究揭示了博彩策略与数学结构的深层联系，为动态博弈提供了理论框架和实用算法。

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [13] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Main category: cs.LG

TL;DR: 论文提出了一种称为“异常签名”的方法，旨在通过突出显示哪些特征影响了决策，提高异常检测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 机器学习常被视为黑箱，难以理解其输出（如决策或评分），尤其是在自动异常检测中，天文学家通常需要独立分析数据以理解异常标记的原因。

Method: 引入了“异常签名”的概念，通过突出显示对决策有贡献的特征来帮助解释异常。

Result: 该方法有助于提高异常检测的可解释性，使研究者更容易理解异常标记的原因。

Conclusion: “异常签名”是一种有效的工具，能够增强机器学习在异常检测中的透明度和可解释性。

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [14] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的三步建筑能源管理系统（BEMS），结合聚类、预测和约束策略学习，以解决可扩展性、适应性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 全球能源需求增长和可再生能源集成复杂性增加，使得建筑成为可持续能源管理的核心。

Method: 1. 聚类非可转移负荷模式以识别常见消费模式；2. 集成LSTM预测模块以预测未来状态；3. 使用领域知识约束策略学习确保安全操作。

Result: 在真实数据上评估，降低了某些建筑类型的运营成本达15%，保持稳定的环境性能，并能快速分类和优化新建筑。

Conclusion: 该框架实现了可扩展、稳健且经济高效的建筑能源管理。

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [15] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Main category: cs.LG

TL;DR: 提出了一种基于事件相机的创新碰撞避免系统，用于空间碎片监测。


<details>
  <summary>Details</summary>
Motivation: 空间碎片对航天器构成严重威胁，需要有效的监测和避免策略。

Method: 采用Stack-CNN算法分析事件相机的实时数据，检测微弱移动目标。

Result: 地面测试显示算法能提高信噪比，适用于空间成像和SSA/STM操作。

Conclusion: 该系统为空间交通管理和态势感知提供了有前景的解决方案。

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [16] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Main category: cs.LG

TL;DR: 论文提出了一种基于向量量化（VQ）的新方法，通过引入紧凑的语义ID（SID）替代传统嵌入集合，解决了大规模实时推荐系统中的存储和推理成本问题。


<details>
  <summary>Details</summary>
Motivation: 工业广告推荐系统通常需要处理用户历史或事件序列长度在O(10^3)到O(10^4)范围内的数据，传统嵌入方法在实时预测模型中难以高效实现。

Method: 方法包括：1）多任务VQ-VAE框架（VQ融合），融合多内容嵌入和分类预测为单一语义ID；2）参数自由的SID到嵌入转换技术（SIDE）；3）新型量化方法离散PCA（DPCA）。

Result: 在大规模工业广告推荐系统中，该方法实现了2.4倍的归一化熵增益提升和3倍的数据占用减少。

Conclusion: 通过紧凑语义ID和新型量化技术，论文显著提升了推荐系统的效率和性能。

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [17] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: 论文提出了FedFitTech基线，用于解决可穿戴设备在联邦学习中的挑战，如数据不平衡和隐私问题，并通过案例展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备在健身技术中的广泛应用面临隐私、监管和通信效率问题，传统集中式学习方法难以解决。

Method: 采用联邦学习（FL）进行分散式模型训练，提出FedFitTech基线，并结合客户端早期停止策略。

Result: 系统减少了13%的冗余通信，识别性能仅下降1%，平衡了通用性和个性化。

Conclusion: FedFitTech为健身技术研究提供了新基础，并开源供社区使用。

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [18] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Main category: cs.LG

TL;DR: 提出了一种新的软生存树模型（SST），通过非线性优化训练，结合灵活性和可解释性，在15个数据集上表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 决策树在生存分析中因其可解释性和建模复杂关系的能力而受欢迎，但传统方法多为启发式，本研究旨在通过全局优化提升性能。

Method: 提出软生存树模型（SST），采用软分割规则，通过非线性优化训练，支持多种生存函数（参数、半参数、非参数）。

Result: 在15个数据集上，SST在判别和校准指标上优于三种基准生存树模型。

Conclusion: SST结合了灵活性和可解释性，并可扩展至群体公平性，是一种有效的生存分析方法。

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [19] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Main category: cs.LG

TL;DR: 本文提出了一种无监督学习框架，用于学习事件触发时间序列之间的相似性，结合了分层多分辨率序列自编码器和高斯混合模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于事件触发时间序列的复杂动态性，现有相似性度量方法在安全相关任务中效果不佳，因此需要开发一种新的框架。

Method: 采用分层多分辨率序列自编码器和高斯混合模型（GMM）学习时间序列的低维表示。

Result: 通过实验验证，该方法在定性和定量上均显著优于现有方法。

Conclusion: 该框架为建模和学习事件触发时间序列相似性提供了系统化方法，具有实际应用潜力。

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: 论文提出了一种名为“安全提醒”的软提示调优方法，通过优化可学习的提示标记来增强视觉语言模型（VLM）的安全性，防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）在代码生成和聊天机器人等实际应用中的能力增强，其安全性问题日益突出。VLM因其多模态特性面临独特的漏洞，攻击者可能通过修改视觉或文本输入绕过安全防护，触发有害内容生成。

Method: 通过系统分析VLM在攻击下的行为，发现了一种称为“延迟安全意识”的现象。基于此，提出了一种名为“安全提醒”的软提示调优方法，通过优化可学习的提示标记，在文本生成过程中定期注入以增强安全意识。

Result: 在三个安全基准和一个对抗攻击测试中，该方法显著降低了攻击成功率，同时保持了模型在良性任务上的性能。

Conclusion: “安全提醒”方法为实际应用中部署更安全的VLM提供了一种实用解决方案。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [21] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: 论文提出了一种基于序列决策框架的自动化通信编排方法，替代传统的手动营销工作，通过个性化优化提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统通信编排依赖人工，效率低且难以实现个性化。研究旨在通过自动化方法优化内容、时机、频率和文案，提升用户参与度。

Method: 采用序列决策框架，结合差分设计（Difference-in-Differences）估计个体处理效应，并使用Thompson采样平衡探索与利用。

Result: 在多服务应用中，该方法显著提升了多种目标事件的参与度，并已部署至1.5亿用户。

Conclusion: 自动化通信编排方法有效替代传统手动工作，显著提升用户参与度，具有广泛的应用潜力。

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [22] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 本文探讨了低维、基于规则的模型是否能有效捕捉足球战术，通过定义可解释的状态变量并结合专家知识，训练XGBoost模型预测传球成功率。


<details>
  <summary>Details</summary>
Motivation: 现有模型计算成本高或缺乏可解释性，且未充分考虑所有球员状态，因此研究低维、基于规则的模型是否可行。

Method: 定义球持有者和潜在接球者的状态变量，结合专家知识，使用StatsBomb和SkillCorner数据训练XGBoost模型预测传球成功率。

Result: 球员与球的距离及其空间得分是影响传球成功的关键因素。

Conclusion: 低维模型通过直观变量支持战术分析，为足球决策提供实用工具。

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [23] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Main category: cs.AI

TL;DR: BFO 2020无法支持通用依赖持续体的功能、倾向和角色（如软件或数据集）。本文讨论了这一限制，并提出两种解决方案：定义类和修改BFO以支持这些实体的表示。


<details>
  <summary>Details</summary>
Motivation: BFO 2020的局限性导致无法充分表示通用依赖持续体（如计算机模型或数据集）的功能和角色，影响了模型的执行和描述。

Method: 讨论了BFO 2020的限制，并提出两种方法：使用定义类和修改BFO框架。

Result: 提出了两种可能的解决方案，以支持通用依赖持续体的功能、倾向和角色的表示。

Conclusion: 通过定义类或修改BFO，可以解决BFO 2020在表示通用依赖持续体功能与角色方面的不足。

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: 论文提出了一种评估管道Tibbe-AG，用于验证基于伊斯兰医学文本的AI系统，通过检索增强和自我评估提升回答质量。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰医学文本如《医典》和《先知医学》蕴含丰富的预防和整体疗法，但未被现代AI系统充分利用，且现有基准测试未能验证其文化背景下的医学指导。

Method: 提出Tibbe-AG评估管道，结合30个精选问题、人类验证疗法，测试三种LLM（LLaMA-3、Mistral-7B、Qwen2-7B）在直接生成、检索增强生成和科学自评过滤下的表现。

Result: 检索提升事实准确性13%，自评提示通过机制洞察和安全性考虑再提升10%。

Conclusion: 结合古典伊斯兰文本、检索和自我评估，可实现可靠且文化敏感的医学问答。

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [25] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）是否能表示现实世界内容，并提出了基于结构对应性的解释框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLMs是否能够表示非语言现实的问题，尤其是当它们的输入、输出和训练数据仅包含文本时。

Method: 采用结构对应性理论分析LLMs的表示能力，并探讨其在任务表现中的作用。

Result: 研究发现，单纯的结构对应性不足以支持LLMs表示现实内容，但如果这些对应性在任务表现中发挥关键作用，则可能实现。

Conclusion: 结论指出，LLMs的文本局限性可能阻碍其参与必要的任务，但通过适当利用结构对应性，仍可能实现现实内容的表示。

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [26] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 论文提出StoryWriter框架，通过多智能体协作解决长故事生成的连贯性和复杂性挑战，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 长故事生成对现有大语言模型仍是挑战，主要因连贯性和叙事复杂性不足。

Method: 采用三模块多智能体框架：提纲、规划和写作智能体，分别负责生成事件提纲、详细规划和动态写作。

Result: StoryWriter在故事质量和长度上显著优于基线，并生成了包含6000篇高质量长故事的数据集。

Conclusion: StoryWriter框架在长故事生成中表现出色，训练模型进一步验证其有效性。

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [27] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出了一种基于因子化和中心化的持续学习方法，防止多语言语音识别模型在持续学习中的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络语音识别模型需要在不重新训练整个系统的情况下持续吸收新数据，尤其是在无法访问原始训练数据的情况下。持续学习可能导致灾难性遗忘。

Method: 受人类大脑通过醒睡周期学习和巩固知识的启发，提出了一种包含因子化和中心化两个阶段的持续学习方法。

Result: 实验表明，中心化阶段通过积累多个低秩适配器的知识，有效防止了灾难性遗忘。

Conclusion: 该方法在多语言代码切换数据集上表现良好，为持续学习提供了一种有效解决方案。

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [28] [Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene](https://arxiv.org/abs/2506.15890)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy,David Merrick*

Main category: cs.RO

TL;DR: 论文总结了在飓风Debby和Helene中使用小型无人机系统（sUAS）进行机器学习（ML）损害评估时遇到的四个主要挑战，并提出了三个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决实际部署中sUAS和ML系统在灾害损害评估中的操作性问题，填补了现有文献中实际应用案例的空白。

Method: 通过分析飓风Debby和Helene中收集的sUAS影像数据，以及宾夕法尼亚州飓风Debby后的有人机影像数据，评估ML模型的性能。

Result: 识别出四个主要挑战：输入影像空间分辨率变化、影像与地理空间数据不对齐、无线连接问题和数据产品格式问题。

Conclusion: 提出了三个研究方向，以改进ML模型的实际操作能力，增强sUAS在灾害响应中的有效性。

Abstract: This paper details four principal challenges encountered with machine
learning (ML) damage assessment using small uncrewed aerial systems (sUAS) at
Hurricanes Debby and Helene that prevented, degraded, or delayed the delivery
of data products during operations and suggests three research directions for
future real-world deployments. The presence of these challenges is not
surprising given that a review of the literature considering both datasets and
proposed ML models suggests this is the first sUAS-based ML system for disaster
damage assessment actually deployed as a part of real-world operations. The
sUAS-based ML system was applied by the State of Florida to Hurricanes Helene
(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne
sUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a
Wintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial
imagery of inland flood damage resulting from post-tropical remnants of
Hurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing
further insights into the advantages and limitations of sUAS for disaster
response. The four challenges (variationin spatial resolution of input imagery,
spatial misalignment between imagery and geospatial data, wireless
connectivity, and data product format) lead to three recommendations that
specify research needed to improve ML model capabilities to accommodate the
wide variation of potential spatial resolutions used in practice, handle
spatial misalignment, and minimize the dependency on wireless connectivity.
These recommendations are expected to improve the effective operational use of
sUAS and sUAS-based ML damage assessment systems for disaster response.

</details>


### [29] [Full-Pose Tracking via Robust Control for Over-Actuated Multirotors](https://arxiv.org/abs/2506.16427)
*Mohamad Hachem,Clément Roos,Thierry Miquel,Murat Bronz*

Main category: cs.RO

TL;DR: 本文提出了一种针对过驱动多旋翼的鲁棒级联控制架构，结合INDI和H_inf控制，通过几何引导控制分配实现精确姿态和位置跟踪。


<details>
  <summary>Details</summary>
Motivation: 扩展INDI和H_inf控制方法至过驱动多旋翼，解决姿态和位置跟踪中的鲁棒性问题。

Method: 采用加权最小二乘几何引导控制分配方法，形式化为二次优化问题，实现全姿态跟踪。

Result: 数值模拟验证了方法在过驱动六旋翼上的有效性，适应性强且适用于实际应用。

Conclusion: 该方法解决了不可行姿态参考和干扰鲁棒性问题，具有实际应用潜力。

Abstract: This paper presents a robust cascaded control architecture for over-actuated
multirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)
control combined with structured H_inf control, initially proposed for
under-actuated multirotors, to a broader range of multirotor configurations. To
achieve precise and robust attitude and position tracking, we employ a weighted
least-squares geometric guidance control allocation method, formulated as a
quadratic optimization problem, enabling full-pose tracking. The proposed
approach effectively addresses key challenges, such as preventing infeasible
pose references and enhancing robustness against disturbances, as well as
considering multirotor's actual physical limitations. Numerical simulations
with an over-actuated hexacopter validate the method's effectiveness,
demonstrating its adaptability to diverse mission scenarios and its potential
for real-world aerial applications.

</details>


### [30] [DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy](https://arxiv.org/abs/2506.16720)
*Weitao Zhou,Bo Zhang,Zhong Cao,Xiang Li,Qian Cheng,Chunyang Liu,Yaqin Zhang,Diange Yang*

Main category: cs.RO

TL;DR: 论文提出了一种基于脱管原因增强的强化学习方法（DRARL），通过识别脱管原因优化驾驶策略，避免无效数据干扰，提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在开放道路上的增多，脱管案例日益普遍，但脱管数据稀缺且部分数据无效，限制了策略改进效果。

Method: 提出DRARL方法，利用OOD状态估计模型识别脱管原因，区分无效脱管案例，并在原因增强的虚拟环境中更新策略。

Result: 实验结果表明，该方法能准确识别策略相关脱管原因，提升策略性能，同时避免策略过于保守。

Conclusion: DRARL为利用脱管案例优化驾驶策略提供了一种高效方法。

Abstract: With the increasing presence of automated vehicles on open roads under driver
supervision, disengagement cases are becoming more prevalent. While some
data-driven planning systems attempt to directly utilize these disengagement
cases for policy improvement, the inherent scarcity of disengagement data
(often occurring as a single instances) restricts training effectiveness.
Furthermore, some disengagement data should be excluded since the disengagement
may not always come from the failure of driving policies, e.g. the driver may
casually intervene for a while. To this end, this work proposes
disengagement-reason-augmented reinforcement learning (DRARL), which enhances
driving policy improvement process according to the reason of disengagement
cases. Specifically, the reason of disengagement is identified by a
out-of-distribution (OOD) state estimation model. When the reason doesn't
exist, the case will be identified as a casual disengagement case, which
doesn't require additional policy adjustment. Otherwise, the policy can be
updated under a reason-augmented imagination environment, improving the policy
performance of disengagement cases with similar reasons. The method is
evaluated using real-world disengagement cases collected by autonomous driving
robotaxi. Experimental results demonstrate that the method accurately
identifies policy-related disengagement reasons, allowing the agent to handle
both original and semantically similar cases through reason-augmented training.
Furthermore, the approach prevents the agent from becoming overly conservative
after policy adjustments. Overall, this work provides an efficient way to
improve driving policy performance with disengagement cases.

</details>
