<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 16]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: SP-VLM是一种基于单像素传感和视觉语言模型的新型隐私保护监控框架，能在保护个人身份的同时检测异常行为


<details>
  <summary>Details</summary>
Motivation: 在厕所、更衣室等隐私敏感环境中，传统监控因隐私法规和伦理问题受限，但欺凌、骚扰等不良社会互动又需要及时干预，因此需要一种既能保护隐私又能确保安全的监控方案

Method: 提出单像素视觉语言模型(SP-VLM)框架，通过低维单像素模态捕捉人体动态，结合视觉语言集成推断复杂行为模式，实现隐私保护设计

Result: 单像素传感能有效抑制身份可恢复性，使先进人脸识别系统在低于临界采样率时失效；同时SP-VLM能从严重降级的单像素观测中提取有意义的语义信息，实现异常检测、人数统计和活动理解

Conclusion: SP-VLM在特定采样率范围内既能提取行为智能又能强保护个人身份，为隐私敏感空间的安全监控提供了一条人权对齐的路径，支持及时干预而不侵犯隐私

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [2] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 该研究对医学影像AI性能评估中的置信区间方法进行了大规模实证分析，揭示了影响置信区间可靠性和精确度的关键因素，为制定性能不确定性报告指南提供了依据。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI需要可靠的性能不确定性量化来支持临床转化，但社区对多种置信区间方法及其在不同场景下的行为了解不足，需要填补这一知识空白。

Method: 对24个分割和分类任务进行大规模实证分析，每个任务组使用19个训练模型，涵盖常用性能指标、多种聚合策略和广泛采用的置信区间方法，评估各方法的覆盖率和宽度。

Result: 发现五个主要结果：1)可靠置信区间所需样本量从几十到几千不等；2)性能指标选择强烈影响置信区间行为；3)聚合策略显著影响可靠性；4)机器学习问题类型调节这些效应；5)不同置信区间方法在不同用例中可靠性不同。

Conclusion: 该研究结果为制定医学影像AI性能不确定性报告的未来指南提供了关键组成部分，强调了根据具体研究参数选择适当置信区间方法的重要性。

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [3] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: 提出基于语义V2X的实时碰撞预测框架，利用V-JEPA生成时空语义嵌入替代原始视频传输，大幅降低通信开销的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要实时碰撞预测以确保道路安全，但传统方法传输原始视频或高维传感器数据在车载通信带宽和延迟约束下不实用。

Method: 构建语义V2X框架：路侧单元摄像头使用V-JEPA生成未来帧的时空语义嵌入，通过V2X链路传输给车辆，车辆端使用轻量级注意力探针和分类器解码预测碰撞。

Result: 实验结果显示，该框架在碰撞预测方面实现了10%的F1分数提升，同时传输需求相比原始视频降低了四个数量级。

Conclusion: 语义V2X通信有潜力在智能交通系统中实现协作式实时碰撞预测，在保持预测准确性的同时显著减少通信开销。

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [4] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: FineVAU是一个新的视频异常理解基准，提出了FVScore评估指标和FineW3数据集，专注于细粒度、领域特定的异常视频理解，解决了现有评估方法在捕捉LVLM响应丰富性和视觉基础性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常理解(VAU)任务的评估存在挑战：现有基准使用n-gram指标(如BLEU、ROUGE-L)无法捕捉LVLM响应的丰富性和视觉基础性，而基于LLM的评估则过于关注语言质量而非事实相关性，导致与人类感知不一致的主观判断。

Method: 1) 将VAU定义为三方面问题：事件(What)、参与实体(Who)和位置(Where)；2) 提出FVScore评估指标，评估LVLM答案中关键视觉元素的存在，提供可解释的细粒度反馈；3) 创建FineW3数据集，通过结构化全自动流程增强现有人工标注，添加高质量细粒度视觉信息。

Result: 人类评估显示，FVScore指标在异常感知方面与人类判断具有更好的对齐性。在FineVAU上的详细实验揭示了LVLM在需要空间和细粒度时间理解的异常事件感知方面存在关键局限性，尽管在粗粒度、静态信息和具有强烈视觉线索的事件上表现良好。

Conclusion: FineVAU基准通过细粒度评估指标和数据集，推动了视频异常理解任务的发展，揭示了当前LVLM在空间和时序理解方面的不足，为未来研究提供了重要方向。

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [5] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 提出一种无需训练的推理时颜色控制方法，通过区域约束和复合损失函数在扩散模型中实现精确的颜色保持


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散系统在精确颜色控制方面存在持续失败，特别是在设计导向的工作流程中，输出必须满足用户指定的颜色目标。现有的均值约束方法虽然能满足平均颜色要求，但会产生感知上显著的局部失败。

Method: 结合三种技术：1) 基于感兴趣区域的修复实现空间选择性；2) 背景潜在重新施加防止ROI外颜色漂移；3) 使用CIE Lab和线性RGB定义的复合损失进行梯度引导的潜在微调。损失函数不仅控制ROI的平均颜色，还通过CVaR风格和软最大值惩罚控制像素级误差分布的尾部。

Result: 该方法提供了一种实用的、无需训练的机制，用于实现目标颜色遵循，可以集成到标准的Stable Diffusion修复流程中。相比仅使用均值约束的基线方法，能够避免感知上显著的局部颜色失败。

Conclusion: 提出的推理时区域约束颜色保持方法能够有效解决扩散模型中的精确颜色控制问题，通过分布感知的目标函数和稳定的引导调度，实现了更好的颜色遵循效果。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [6] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: STARS是一个针对不完整多模态输入的遥感语义分割框架，通过非对称对齐机制和像素级语义采样对齐来解决模态缺失问题


<details>
  <summary>Details</summary>
Motivation: 多模态遥感技术通过整合光学图像、SAR和DSM等异构数据增强地表语义理解，但在实际应用中模态数据缺失是常见且严重的问题，导致传统多模态融合模型性能下降。现有方法存在特征崩溃和恢复特征过于泛化的局限性。

Method: 提出STARS框架，包含两个关键设计：1) 带双向翻译和停止梯度的非对称对齐机制，防止特征崩溃并降低对超参数的敏感性；2) 像素级语义采样对齐策略，结合类别平衡像素采样和跨模态语义对齐损失，缓解类别不平衡导致的对齐失败。

Result: 未在摘要中明确说明具体实验结果，但暗示该方法能够有效解决模态缺失问题，提高少数类别的识别能力。

Conclusion: STARS框架通过创新的对齐机制和采样策略，为不完整多模态输入的遥感语义分割提供了鲁棒的解决方案，克服了现有方法的局限性。

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [7] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 提出基于扩散模型的高质量鲁棒水印框架，通过空文本优化和迭代去噪过程，在保持图像质量的同时增强水印对各种图像损坏的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水印方法虽然能隐藏水印且对图像质量影响小，但在传输过程中遇到图像损坏时缺乏鲁棒性，限制了实际应用价值。

Method: 1) 通过空文本优化将干净图像转换为反转噪声；2) 在潜在空间优化反转噪声；3) 通过扩散模型的迭代去噪过程生成高质量水印图像；4) 引入自注意力约束和伪掩码策略防止图像原始语义失真。

Result: 在COCO数据集上的12种不同图像变换中，平均比稳定签名方法性能提升10%，在各种图像损坏下表现出优越性能。

Conclusion: 提出的基于扩散模型的水印框架在保持图像质量的同时显著提高了水印的鲁棒性，为实际应用提供了有效解决方案。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [8] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: 本文提出了一种新的时空多视图表示学习框架用于事件相机动作识别，通过平移不变的密集转换、双分支动态融合架构和生物启发的时序扭曲增强，在三个数据集上显著提升了准确率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的事件相机动作识别方法存在两个主要问题：1) 基于空间分箱的表示方法具有平移不变性限制；2) 早期简单拼接的融合架构无法有效建模不同视图间的互补性。这些问题限制了事件相机在动作识别中的性能。

Method: 提出了三个关键技术：1) 通过平移不变的密集转换将稀疏事件转换为时空多视图表示；2) 设计双分支动态融合架构，建模不同视图运动特征之间的样本级互补性；3) 引入生物启发的时序扭曲增强，模拟真实世界人类动作的速度变化。

Result: 在HARDVS、DailyDVS-200和THU-EACT-50-CHL三个数据集上，相比现有SMVRL方法分别获得了+7.0%、+10.7%和+10.2%的Top-1准确率提升，同时参数减少了30.1%，计算量降低了35.7%。

Conclusion: 本文提出的框架通过改进的时空多视图表示、动态融合架构和时序增强，建立了一个新颖且强大的事件相机动作识别范式，在准确率、参数效率和计算效率方面都取得了显著改进。

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [9] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: FlowMorph是一个物理一致的自监督框架，从微流体视频中学习红细胞的无标签力学代理k，结合层流物理和可微分建模，用于细胞力学特性分析。


<details>
  <summary>Details</summary>
Motivation: 红细胞力学特性是血液和系统性疾病的重要生物标志物，但现有方法依赖监督分割或手工特征，缺乏对层流斯托克斯流物理的编码。

Method: 使用低维参数轮廓建模细胞，通过可微分"流动中的胶囊"模型结合层流平流和曲率正则化弹性松弛，优化包含轮廓重叠、细胞内流一致性、面积守恒、壁约束和时间平滑度的损失函数。

Result: 在四个公共数据集上，平均轮廓IoU达到0.905，显著改善面积守恒和壁约束违规；力学代理k能以0.863的AUC区分坦克履带和翻转动态；仅用200个RT-DC事件校准，就能以0.118 MPa的平均绝对误差预测表观杨氏模量。

Conclusion: FlowMorph提供了一个物理一致的自监督框架，能够从微流体视频中学习红细胞的无标签力学代理，在多种实验条件下表现稳健，为细胞力学特性分析提供了新方法。

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [10] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: FoGA是一个轻量级视频异常检测模型，通过前向一致性学习和门控上下文聚合实现高效检测，仅需约200万参数，在边缘设备上可达155FPS。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法大多依赖大规模模型追求极致精度，但难以部署在资源有限的边缘设备上。同时，主流预测方法仅使用单帧未来预测误差，忽略了更长期时间前向信息的丰富约束。

Method: 提出基于Unet的方法，对连续帧进行特征提取，生成即时和前向预测；在跳跃连接中引入门控上下文聚合模块，动态融合编码器和解码器特征；使用前向一致性损失联合优化，采用混合异常测量策略整合即时和前向帧的误差。

Result: 实验表明该方法显著优于现有最先进方法，运行速度可达155FPS，在性能和效率指标之间取得了优秀平衡。

Conclusion: FoGA是一个轻量级视频异常检测模型，通过前向一致性学习和门控上下文聚合，在保持高性能的同时实现了高效率，适合部署在资源有限的边缘设备上。

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [11] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: EGAgent：基于实体场景图的智能体框架，用于长时程穿戴设备视频理解，通过结构化搜索和跨模态推理实现连续多天视频的复杂问答。


<details>
  <summary>Details</summary>
Motivation: 全天候可穿戴设备（如智能眼镜）需要理解连续、纵向的自我中心视频流，而现有方法受限于上下文窗口长度，无法对长达数天或数周的视频进行组合式多跳推理。

Method: 提出EGAgent框架，基于实体场景图（表示人物、地点、物体及其随时间的关系），为规划智能体提供结构化搜索和推理工具，以及混合视觉和音频搜索能力。

Result: 在EgoLifeQA数据集上达到57.5%的SOTA性能，在Video-MME（Long）数据集上达到74.1%的竞争性性能，用于复杂纵向视频理解任务。

Conclusion: EGAgent通过实体场景图和智能体框架有效解决了长时程视频理解中的上下文限制和组合推理问题，为全天候个人AI助手提供了关键技术。

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [12] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: 提出SwipeGen自动合成人类滑动交互的管道，构建首个GUI代理滑动执行能力基准，并开发GUISwiper代理显著提升滑动执行准确率


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在处理滑动交互时采用过于简化的策略，无法准确模拟人类行为，导致任务完成率受限，滑动执行能力成为新的瓶颈

Method: 将人类滑动手势分解为多个可量化维度，提出SwipeGen自动管道通过GUI探索合成人类滑动交互，基于此构建基准并开发GUISwiper代理

Result: GUISwiper达到69.07%的滑动执行准确率，相比现有VLM基线提升214%

Conclusion: 通过量化人类滑动手势并自动合成训练数据，能显著提升GUI代理的交互执行能力，为GUI自动化任务提供了新的解决方案

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [13] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: 提出一种基于网格形导频信号的数字水印方法，通过分析图像几何变换后网格的变形来估计变换矩阵，实现对裁剪、缩放、旋转等操作的鲁棒同步。


<details>
  <summary>Details</summary>
Motivation: 现有数字水印方法对裁剪攻击的鲁棒性不足，而裁剪会改变图像原点，使水印同步变得困难。需要一种能准确检测几何变换并实现同步的方法。

Method: 在图像中嵌入网格形导频信号，水平和垂直线采用不同编码。当图像经历几何变换时，网格也会相应变形。通过Radon变换分析变形网格的角度和间隔，估计变换矩阵。不同编码的网格线还能确定网格方向，减少模糊性。

Result: 在各项异性缩放、旋转、剪切和裁剪等攻击下进行仿真测试。结果表明，该方法能准确估计变换矩阵，在单一和复合攻击下均保持低误差。

Conclusion: 提出的基于网格导频信号的水印方法能有效估计几何变换，实现对裁剪等操作的鲁棒同步，为解决水印同步问题提供了有效方案。

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [14] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: Fair-Eye Net是一个公平的多模态AI系统，用于青光眼筛查、随访和风险预警，通过整合多种临床数据并优化公平性约束，显著减少了种族诊断差异。


<details>
  <summary>Details</summary>
Motivation: 青光眼是全球不可逆失明的主要原因，当前筛查和进展评估依赖单一测试或松散关联的检查，存在主观性和碎片化护理问题。高质量成像工具和专家资源的有限获取进一步影响了现实世界中的一致性和公平性。

Method: 开发了Fair-Eye Net系统，整合眼底照片、OCT结构指标、VF功能指数和人口统计学因素，采用双流异构融合架构，结合不确定性感知的分层门控策略进行选择性预测和安全转诊，通过公平性约束减少弱势亚组的漏诊。

Result: 系统达到AUC 0.912（特异性96.7%），将种族假阴性差异减少了73.4%（从12.31%降至3.28%），保持稳定的跨域性能，实现3-12个月的早期风险预警（敏感性92%，特异性88%）。

Conclusion: Fair-Eye Net将公平性作为主要目标进行优化，通过多任务学习确保临床可靠性，为临床转化和大规模部署提供了可复现的路径，有助于推进全球眼健康公平。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [15] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出自精炼视频采样方法，利用预训练视频生成器作为自身的精炼器，通过迭代内循环优化提升物理真实感，无需外部验证器或额外训练。


<details>
  <summary>Details</summary>
Motivation: 现代视频生成器在处理复杂物理动力学时仍存在困难，缺乏物理真实感。现有方法使用外部验证器或增强数据训练，计算成本高且难以捕捉细粒度运动。

Method: 将预训练视频生成器解释为去噪自编码器，在推理时进行迭代内循环精炼，无需外部验证器或额外训练。引入基于自一致性的不确定性感知精炼策略，选择性精炼区域以防止过度精炼导致的伪影。

Result: 在先进视频生成器上的实验显示，运动连贯性和物理对齐显著改善，相比默认采样器和基于引导的采样器获得超过70%的人类偏好。

Conclusion: 自精炼视频采样是一种简单有效的方法，能够显著提升视频生成的物理真实感，无需额外训练或外部验证器，为视频生成提供了新的优化思路。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [16] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: 提出AGSP-DSA框架，通过双图构建、谱图滤波和多尺度GCN实现跨模态数据融合，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构多模态数据（文本、音频、图像）的鲁棒融合问题，传统方法难以有效捕捉模态内和模态间的复杂关系，特别是在缺失模态场景下。

Method: 1. 双图构建学习模态内和模态间关系；2. 谱图滤波增强信息信号；3. 多尺度GCN进行节点嵌入；4. 语义感知注意力机制动态调整各模态贡献。

Result: 在CMU-MOSEI上达到95.3%准确率、0.936 F1分数、0.924 mAP，比MM-GNN提升2.6%；在AVE上达到93.4%准确率和0.911 F1分数；在MM-IMDB上达到91.8%准确率和0.886 F1分数。

Conclusion: AGSP-DSA框架在多模态学习中表现出色，在情感分析、事件识别和多媒体分类任务中验证了其有效性，特别是在缺失模态场景下具有良好的泛化性和鲁棒性。

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: 提出贝叶斯鲁棒框架，结合宏观条件生成对抗网络和鲁棒策略学习，解决算法交易中模型对市场机制变化适应不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有算法交易模型在样本内表现良好，但在面对真实市场机制变化时性能下降。主要问题包括：现有策略对高级市场波动的不确定性鲁棒性不足，以及缺乏真实多样的模拟训练环境导致策略过拟合。

Method: 1) 数据侧：提出宏观条件GAN生成器，利用宏观经济指标作为主要控制变量，合成具有真实时间、跨工具和宏观相关性的数据；2) 策略侧：将交易过程建模为两人零和贝叶斯马尔可夫博弈，对抗代理通过扰动宏观条件生成器中的宏观经济指标来模拟机制变化，交易代理通过分位数信念网络维护和更新对隐藏市场状态的信念，使用贝叶斯神经虚拟自博弈寻求鲁棒完美贝叶斯均衡。

Result: 在9种金融工具上的广泛实验表明，该框架优于9种最先进的基线方法。在COVID等极端事件中，该方法显示出改进的盈利能力和风险管理能力。

Conclusion: 该框架为不确定和变化的市场动态下的交易提供了可靠解决方案，通过系统整合宏观条件生成模型和鲁棒策略学习，有效解决了算法交易中的模型退化问题。

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [18] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出ThinkTank-ME框架，通过模拟智库多专家协作分析来改进中东事件预测，构建POLECAT-FOR-ME基准验证多专家协作优于单模型方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM事件预测方法采用单模型架构，只能生成单一明确轨迹，无法捕捉复杂区域背景下多样化的地缘政治细微差别。中东事件预测需要考虑国际关系、区域历史动态和文化背景等多方面因素。

Method: 提出ThinkTank-ME框架，模拟现实世界战略决策中的智库协作专家分析。构建POLECAT-FOR-ME中东事件预测基准，促进专家专业化和严格评估。

Result: 实验结果表明，多专家协作在处理复杂时间性地缘政治预测任务方面具有优越性。

Conclusion: 多专家协作框架能更好地处理复杂区域背景下的事件预测任务，代码已开源。

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [19] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: 本文研究联邦学习在心脏病预测中的应用，通过模拟四家异构医院的数据分布，验证FedProx算法在非IID医疗数据下的有效性，在保护隐私的同时获得比集中式和孤立训练更好的准确率。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私法规（如HIPAA和GDPR）无法直接共享，但联邦学习能实现协作训练而不集中原始数据。然而临床数据天然具有非IID特性（人口差异、疾病流行度、机构实践差异），需要有效处理异构性的算法。

Method: 使用UCI心脏病数据集（Cleveland Clinic，303名患者），通过人口统计分层模拟四家异构医院客户端的非IID数据分区。采用联邦近端优化（FedProx）算法，设置近端参数mu=0.05，进行50次独立运行的广泛消融研究。

Result: FedProx（mu=0.05）达到85.00%准确率，优于集中式学习（83.33%）和孤立本地模型（平均78.45%）。实验证明近端正则化能有效抑制异构环境中的客户端漂移，且不泄露患者隐私。

Conclusion: 这项概念验证研究为现实世界联邦医疗系统提供了算法见解和实际部署指南，结果可直接转移给医院IT管理员，用于实施隐私保护的协作学习。

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [20] [Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach](https://arxiv.org/abs/2601.17303)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出一种去中心化多智能体群架构，用于工业物联网安全监控，实现亚毫秒级响应和97.3%恶意活动检测准确率


<details>
  <summary>Details</summary>
Motivation: 工业物联网设备数量激增，传统集中式安全监控架构存在延迟问题，攻击者可利用这些延迟破坏整个制造生态系统

Method: 设计去中心化多智能体群架构，在每个边缘网关部署自主AI代理，通过轻量级P2P协议协同检测异常行为，采用基于共识的威胁验证流程进行威胁等级投票

Result: 在模拟2000个IIoT设备的创新工厂测试中，DMAS实现0.85ms平均响应时间，高负载下97.3%恶意活动检测准确率，零日攻击检测准确率87%，网络带宽使用减少89%

Conclusion: DMAS架构能有效解决集中式安全监控的延迟问题，显著提升工业物联网安全防护能力，防止实时级联故障，降低网络带宽需求

Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.

</details>


### [21] [Automatic Stability and Recovery for Neural Network Training](https://arxiv.org/abs/2601.17483)
*Barak Or*

Main category: cs.LG

TL;DR: 提出一个运行时稳定性框架，通过隔离创新信号实现自动检测和恢复，不修改底层优化器，提供理论安全保证


<details>
  <summary>Details</summary>
Motivation: 现代神经网络训练越来越脆弱，罕见但严重的破坏性更新常导致不可逆发散或性能退化。现有优化方法主要依赖优化器内的预防机制，一旦发生不稳定性，检测和恢复能力有限。

Method: 引入监督运行时稳定性框架，将优化视为受控随机过程。通过隔离来自二级测量（如验证探针）的创新信号，实现自动检测和恢复破坏性更新，不修改底层优化器。

Result: 提供理论运行时安全保证，形式化有界退化和恢复。实现开销最小，兼容内存受限的训练设置。

Conclusion: 提出的框架能有效解决神经网络训练中的稳定性问题，提供自动检测和恢复机制，具有理论保证和实际可行性。

Abstract: Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.

</details>


### [22] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: 提出PRECISE框架，结合少量人工标注与LLM判断，显著降低检索系统评估的标注需求，同时校正LLM偏见


<details>
  <summary>Details</summary>
Motivation: 传统检索、排序和RAG系统评估需要大量人工相关性标注，现有LLM自动评估存在偏见问题，需要更高效可靠的评估方法

Method: 扩展预测驱动推理(PPI)框架，结合少量人工标注查询(100个)和大量未标注样本(10,000个)，重新定义度量集成空间，将计算复杂度从O(2^|C|)降至O(2^K)

Result: 在多个检索数据集上验证，PRECISE能减少Precision@K度量的估计方差，在低资源设置下有效校正LLM偏见

Conclusion: PRECISE框架显著降低评估标注需求，提供可靠度量估计，为LLM增强的查询重写等应用提供实用评估方案

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实世界电子健康记录开发生成式模拟器，能够基于患者历史生成高保真的未来临床轨迹，为个性化治疗和虚拟临床试验提供工具。


<details>
  <summary>Details</summary>
Motivation: 模拟在临床医学中具有变革潜力，可用于个性化治疗规划和虚拟临床试验。然而，由于复杂的生物和社会文化影响，模拟患者轨迹具有挑战性。本研究旨在利用真实世界临床记录来经验性地建模患者时间线。

Method: 开发了一个生成式模拟器模型，以患者历史为输入，合成细粒度、真实的未来轨迹。该模型在超过2亿条临床记录上进行了预训练。

Result: 模型生成了高保真的未来时间线，与真实患者未来数据中的事件发生率、实验室检测结果和时间动态密切匹配。准确估计了未来事件概率，观察与预期比率在不同结果和时间范围内始终接近1.0。

Conclusion: 研究揭示了电子健康记录中真实世界数据的未开发价值，并引入了一个可扩展的框架，用于临床护理的计算机模拟建模。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [24] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice框架通过自构建和持续改进机制为对话AI系统创建自适应防护栏，相比静态规则方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有对话AI防护栏使用静态规则，无法适应新威胁和部署环境变化，需要能够自我构建和持续改进的防护框架

Method: Lattice框架包含两个阶段：构建阶段通过迭代模拟和优化从标注示例创建初始防护栏；持续改进阶段通过风险评估、对抗测试和整合自主适应已部署防护栏

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1分数，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点；持续改进阶段通过闭环优化在跨域数据上实现7个百分点F1提升

Conclusion: Lattice框架证明有效的防护栏可以通过迭代优化自我构建，为对话AI系统提供自适应、持续改进的安全保障机制

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [25] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: 提出一个结合深度学习（CNN、LSTM）与传统机器学习（KNN、XGB）的混合集成框架，用于心血管疾病预测，在两个Kaggle数据集上分别达到82.30%和97.10%的准确率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死亡原因，需要智能数据驱动的诊断工具。传统预测模型在处理异构数据集和复杂生理模式时泛化能力有限。

Method: 提出混合集成框架，整合CNN和LSTM深度学习架构与KNN和XGB传统机器学习算法，采用集成投票机制，结合深度网络的表征能力和传统模型的可解释性与效率。

Result: 在两个公开Kaggle数据集上，模型在数据集I达到82.30%准确率，数据集II达到97.10%准确率，在精确率、召回率和F1分数上均有稳定提升。

Conclusion: 混合AI框架在心血管疾病预测中表现出鲁棒性和临床潜力，支持联合国可持续发展目标3（良好健康与福祉），通过创新数据驱动医疗解决方案促进早期诊断、预防和管理非传染性疾病。

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [26] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究LLM智能体在未知测试领域中的泛化能力，发现状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，提出通过添加干扰性特征增强状态信息丰富度来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在有限环境中进行后训练，但需要在更广泛的未知领域部署。本研究旨在探索当最终测试领域未知时，哪些环境属性和建模选择对跨域性能影响最大。

Method: 首先识别影响跨域泛化的两个关键环境轴：状态信息丰富度和规划复杂度。提出随机化技术，通过添加少量与目标无关的干扰性特征来增强状态信息丰富度。同时分析建模选择，包括SFT预热/中期训练和逐步思考机制的影响。

Result: 研究发现：1）状态信息丰富度和规划复杂度与跨域泛化强相关，而领域真实性和文本相似度不是主要因素；2）仅增加状态信息丰富度就能有效提升跨域鲁棒性；3）SFT预热/中期训练有助于防止灾难性遗忘，但会损害未包含在训练数据中的领域的泛化能力；4）逐步思考机制在保持泛化能力方面起关键作用。

Conclusion: 状态信息丰富度和规划复杂度是影响LLM智能体跨域泛化的关键因素。通过简单的随机化技术增强状态信息丰富度可以提升泛化能力，同时需要谨慎使用SFT训练并启用逐步思考机制来平衡性能与泛化。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [27] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的可靠性层，将灾害预警从警报传递转变为行动执行，通过整合多源数据和LLM提供个性化建议，提高保护行动执行率并减少响应延迟。


<details>
  <summary>Details</summary>
Motivation: 传统预警系统虽然快速传播警报，但往往无法触发及时的保护行动，导致可预防的损失和不公平现象。需要将灾害沟通从"警报传递"转变为"行动执行"。

Method: 整合气象、水文、脆弱性和社会数据形成综合风险指数，使用带有护栏的大型语言模型(LLMs)为公民、志愿者和市政界面提供个性化建议。

Result: 通过模拟、用户研究和市政试点评估显示：提高了保护行动执行率、减少了响应延迟、增加了可用性和信任度。

Conclusion: Climate RADAR通过结合预测分析、行为科学和负责任AI，推进以人为本、透明和公平的预警系统，为合规就绪的灾害韧性基础设施提供实用路径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: 提出动态参数隔离策略，通过识别任务核心参数区域、合并重叠任务、分阶段训练并冻结先前任务核心参数，解决SFT中的跷跷板效应


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)中，异构任务间的冲突目标会导致"跷跷板效应"：优化一个任务会损害其他任务性能，特别是当模型参数被无差别更新时。参数异质性可能是跨任务干扰的根本原因。

Method: 1. 在不同SFT任务上独立微调LLMs，识别每个任务的核心参数区域（更新幅度最大的参数子集）；2. 合并核心参数区域高度重叠的任务进行联合训练；3. 将不相交的任务组织到不同阶段；4. 在多阶段SFT中，冻结先前任务获得的核心参数，防止被后续任务覆盖。

Result: 在多个公共数据集上的密集实验表明，动态参数隔离策略持续减少了数据冲突，相比多阶段和多任务调优基线，实现了持续的性能提升。

Conclusion: 通过参数异质性假设驱动的动态参数隔离方法，有效解决了SFT中的跷跷板效应，通过识别、合并和隔离任务特定参数区域，实现了更好的多任务适应性能。

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [29] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu,Elliot Murphy*

Main category: cs.CL

TL;DR: 论文主张将振荡特征纳入双语产出错误研究，提出ROSE模型能解释双语产出中的句法迁移，并以跨语言影响为例说明振荡失败模式驱动功能抑制/竞争理论。


<details>
  <summary>Details</summary>
Motivation: 传统双语研究主要关注事件相关电位等时间特征，需要引入振荡特征来为双语理论提供新的实现层面约束，探索更复杂的语言功能障碍生物标志物。

Method: 采用ROSE神经语言模型作为神经计算框架，以跨语言影响为案例研究，分析双语产出中的振荡失败模式如何驱动功能抑制和竞争机制。

Result: ROSE模型能够捕捉双语产出中句法迁移的形式特性和形态句法序列失败模式的范围，为跨语言影响提供了基于振荡失败机制的神经计算解释。

Conclusion: 将振荡特征纳入双语研究不仅提供了ROSE模型所倡导的连接假设，还允许探索比传统神经特征更复杂的时空生物标志物，为双语理论提供新的实现层面约束。

Abstract: We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [30] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: 研究评估了17个LLM智能体在Hanabi纸牌游戏中的表现，通过上下文工程提升协作推理能力，并发布首个公开数据集用于微调，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在不完全信息下的协作推理对多智能体系统具有挑战性。Hanabi游戏需要心智理论和战略沟通，是研究这一问题的理想测试平台。

Method: 在2-5人游戏中测试17个LLM智能体，采用三种上下文设置：Watson（仅显式信息）、Sherlock（贝叶斯推理）、Mycroft（多轮状态跟踪）。发布HanabiLogs和HanabiRewards数据集，用于监督学习和RL微调。

Result: 最强推理模型在Sherlock设置下平均得分超过15分，但仍低于人类专家（20+分）。使用数据集微调的4B模型性能提升21%（监督）和156%（RL），接近o4-mini水平，超越GPT-4.1 52%。RL微调模型还能泛化到其他推理任务。

Conclusion: 上下文工程和专用数据集能显著提升LLM在不完全信息协作推理中的表现，但仍有改进空间。RL微调不仅提升Hanabi性能，还能泛化到其他推理任务。

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [31] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: Temp-R1：首个基于强化学习的端到端自主TKGQA智能体，通过扩展动作空间和逆向课程学习，在复杂时序知识图谱问答任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TKGQA方法依赖固定工作流程和昂贵的闭源API，缺乏灵活性和可扩展性，无法有效处理动态事实的多跳依赖和复杂时序约束。

Method: 提出Temp-R1：1）扩展动作空间，包含专用内部动作和外部动作以解决单动作推理的认知过载问题；2）引入逆向课程学习，先训练困难问题再迁移到简单问题，防止捷径学习。

Result: 在MultiTQ和TimelineKGQA基准测试中取得SOTA性能，8B参数模型在复杂问题上比强基线提升19.8%，建立了自主时序推理智能体的新范式。

Conclusion: Temp-R1通过强化学习框架、扩展动作空间和逆向课程学习，成功解决了TKGQA中的复杂推理挑战，为自主时序推理智能体开辟了新方向。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [32] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde,Nicolaas Jedema,Ravi Krishnan,Phong Le*

Main category: cs.CL

TL;DR: TaxMorph使用LLM重构分类学层次结构，通过重命名、合并、拆分和重排序操作，使分类学更符合语言模型的语义理解，在三个HTC基准测试中性能提升高达+2.9pp F1。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的分类学存在模糊性，如相似父节点下的相同叶节点名称，这阻碍了语言模型学习清晰的决策边界。现有方法未能充分利用LLM的语义理解能力来优化整个分类学层次结构。

Method: 提出TaxMorph框架，使用大语言模型（LLMs）通过重命名、合并、拆分和重排序等操作来重构整个分类学层次结构，使分类学更好地匹配语言模型编码的语义。

Result: 在三个层次文本分类基准测试中，LLM优化的分类学在各种设置下始终优于人工策划的分类学，性能提升高达+2.9pp F1。研究发现LLM优化的分类学虽然更难分离，但更符合模型的混淆模式。

Conclusion: LLM引导的分类学优化创建了更符合模型学习方式的分类学结构，提高了层次文本分类性能。这表明分类学设计应考虑模型的归纳偏置，而不仅仅是人类可解释性。

Abstract: Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [33] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 研究显示大型语言模型在预训练后存储生物医学事实的强度不均，潜在知识（存在于权重但无法通过确定性解码可靠访问）能预测微调期间事实学习的速度和未见本体事实的有限泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预训练后存储生物医学事实的强度不均：有些事实存在于权重中但无法通过确定性解码可靠访问（潜在知识），而其他事实则很少被表示。研究者希望了解这种潜在知识如何影响微调过程中的事实获取、泛化和退化。

Method: 对Llama 3.1 8B Instruct进行微调，学习人类表型本体（800对）和基因本体（400个训练对）的术语标识符映射，保留400个GO对用于测试泛化。将学习视为20个epoch的时间到事件过程，使用随机解码检测基线潜在知识，使用Cox比例风险模型识别获取、泛化和退化的预测因子。

Result: HPO的基线确定性召回率为2.8%，微调后升至71.9%。潜在知识是事实获取速度的最强预测因子（HR 2.6），与更早、更高的峰值学习率和更快收敛相关。泛化到保留的GO事实不常见（5.8%），但当潜在知识存在时更可能发生。先前正确的GO映射在未见术语中比在已训练术语中更常退化，表明训练期间的强化具有保护作用。

Conclusion: 潜在知识能预测微调期间事实学习的速度和未见本体事实的有限泛化能力，而对退化的抵抗取决于事实是否被强化。这揭示了LLM中事实表示的不均匀性如何影响下游学习动态。

Abstract: Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [34] [Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models](https://arxiv.org/abs/2601.17556)
*Ulices Santa Cruz,Mahmoud Elfar,Yasser Shoukry*

Main category: cs.RO

TL;DR: 提出一个可认证的神经网络框架，用于自动驾驶系统的视觉位姿估计，结合物理驱动建模与学习估计，在杂乱环境中提供误差保证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在视觉任务中缺乏可证明的正确性保证，这对安全关键应用至关重要。需要为基于感知的位姿估计设计可认证的神经网络。

Method: 提出几何生成模型(GGM)，其参数来自目标物体在相机中的成像过程。使用GGM训练具有认证保证的神经网络位姿估计器，并扩展到杂乱环境中的目标检测，构建多阶段感知管道。

Result: 在合成和真实图像上评估框架，包括事件相机捕获的交通标志图像。训练后的编码器能有效估计位姿，符合框架提供的认证边界。

Conclusion: 该框架成功将物理驱动建模与学习估计结合，为自动驾驶系统的视觉位姿估计提供了可认证的神经网络解决方案，在杂乱环境中保持认证保证。

Abstract: We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.

</details>


### [35] [NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi](https://arxiv.org/abs/2601.17991)
*Roman Akinshin,Elizaveta Lopatina,Kirill Bogatikov,Nikolai Kiz,Anna V. Makarova,Mikhail Lebedev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Valerii Kangler*

Main category: cs.RO

TL;DR: 提出结合表面肌电信号与视觉引导的神经形态控制架构，用于上肢假肢，在低功耗下实现高精度手势识别


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制存在功耗高、缺乏情境感知、安全性不足等问题，需要开发更智能、节能的解决方案

Method: 使用表面肌电信号分类的尖峰神经网络部署在AltAi神经形态处理器上，结合眼动追踪和场景摄像头识别用户注视物体

Result: 系统在6种手势识别上达到先进水平，结合视觉情境限制后准确率提升至约95%，功耗低于1瓦

Conclusion: 提出的神经形态情境感知控制器能提供节能可靠的假肢控制，有望改善上肢截肢者日常活动的安全性和可用性

Abstract: This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.

</details>


### [36] [SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation](https://arxiv.org/abs/2601.18442)
*Hongyi Zhao,Shuo Wang,Qijie He,Ziyuan Pu*

Main category: cs.RO

TL;DR: SG-CADVLM框架利用上下文感知解码和多模态输入处理，从事故报告和道路网络图中生成安全关键场景，解决了现有方法缺乏多样性和物理真实性的问题，显著提高了高风险场景生成率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全验证需要测试安全关键场景，但这些事件在真实驾驶中罕见且测试成本高。事故报告提供了真实的安全事件规格，但现有方法存在局限性：数据驱动方法缺乏多样性，对抗方法缺乏物理真实性，LLM/VLM方法存在上下文抑制问题导致偏离事故特征。

Method: 提出SG-CADVLM框架，集成上下文感知解码与多模态输入处理，从事故报告和道路网络图生成安全关键场景。该框架缓解了VLM幻觉问题，同时支持道路几何和车辆轨迹的同步生成。

Result: 实验结果显示，SG-CADVLM生成关键风险场景的比例达到84.4%，而基线方法仅为12.5%，提升了469%。同时能够生成可执行的自动驾驶测试仿真场景。

Conclusion: SG-CADVLM框架有效解决了现有方法在安全关键场景生成中的局限性，通过上下文感知解码和多模态处理，显著提高了场景生成的真实性和可用性，为自动驾驶安全验证提供了有效工具。

Abstract: Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.

</details>
