<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: PromptGuard是一个模块化提示框架，通过VulnGuard Prompt技术使用对比学习防止LLM生成有害信息，特别保护LGBTQ+等脆弱群体，理论证明可减少25-30%的危害。


<details>
  <summary>Details</summary>
Motivation: 现有安全方法依赖事后过滤或通用对齐技术，无法在生成源头主动防止有害输出，特别是对脆弱群体造成风险。

Method: 提出PromptGuard框架，包含VulnGuard Prompt混合技术，整合GitHub数据驱动的对比学习、少样本示例、伦理思维链推理和自适应角色提示，采用多目标优化理论。

Result: 通过熵界和帕累托最优性理论证明可实现25-30%的危害减少，建立了使用GitHub数据集的理论验证框架。

Conclusion: PromptGuard提供了一个系统化的实时危害预防专家系统，为系统性实证研究奠定了数学基础。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [2] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event是首个基于事件相机数据的大规模语言驱动物体定位基准数据集，包含5,567个真实驾驶场景、13,458个标注物体和30,000+验证过的指代表达式，支持可解释的组合式物体定位。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级精度和抗运动模糊的优势，但在自然语言理解方面的多模态感知研究较少，存在研究空白。

Method: 构建基于真实驾驶场景的大规模数据集，每个指代表达式包含外观、状态、与观察者关系、与周围物体关系四个结构化属性，显式捕捉时空和关系线索。

Result: 创建了包含5,567个场景、13,458个标注物体和30,000+验证表达式的基准数据集，支持超越简单物体识别的上下文推理。

Conclusion: Talk2Event为推进多模态和时间感知感知提供了基础，在机器人、人机交互等领域具有应用前景。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [3] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: RRDataset是一个用于评估AI生成图像检测模型在真实世界复杂条件下的数据集，涵盖7个场景、网络传输鲁棒性和重数字化鲁棒性三个维度。基准测试显示当前检测方法在真实条件下存在局限，需要借鉴人类适应能力开发更鲁棒的算法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，高度逼真的图像合成对数字安全和媒体可信度提出了新挑战。现有AI生成图像检测方法在复杂真实世界条件下的性能评估存在研究空白。

Method: 提出Real-World Robustness Dataset (RRDataset)，从三个维度进行评估：1)场景泛化-7个主要场景的高质量图像；2)网络传输鲁棒性-社交媒体多次分享后的图像；3)重数字化鲁棒性-4种不同重数字化方法处理的图像。对17个检测器和10个视觉语言模型进行基准测试，并进行192人参与的大规模人类研究。

Result: 基准测试结果揭示了当前AI检测方法在真实世界条件下的局限性，强调了借鉴人类适应能力开发更鲁棒检测算法的重要性。

Conclusion: 需要开发能够更好应对真实世界复杂条件的AI生成图像检测算法，人类在少样本学习方面的能力为算法改进提供了重要参考。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [4] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 提出CoAtNeXt混合模型用于胃组织图像分类，在多个数据集上超越现有CNN和ViT模型，性能优异


<details>
  <summary>Details</summary>
Motivation: 胃病早期诊断至关重要，但传统组织病理学检查完全手动操作，工作量大且存在诊断差异，需要自动化、可靠的胃组织分析方法

Method: 基于CoAtNet架构，用增强的ConvNeXtV2块替换MBConv层，集成CBAM注意力机制改善局部特征提取，平衡计算效率和分类性能

Result: 在HMU-GC-HE-30K八分类数据集上达到96.47%准确率，在GasHisSDB二分类数据集上达到98.29%准确率，超越所有测试的CNN和ViT模型

Conclusion: CoAtNeXt是胃组织病理学图像分类的鲁棒架构，具有协助病理学家提高诊断准确性和减少工作负荷的潜力

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [5] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 提出了DATE方法，通过时间戳注入机制和时序感知相似性采样策略，增强多模态大语言模型的长视频时序理解能力，在7B和72B模型上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法采用均匀帧采样和隐式位置编码，难以处理长视频中的长距离依赖关系，导致关键信息丢失和时序理解能力下降

Method: DATE方法包含时间戳注入机制(TIM)和时序感知相似性采样策略(TASS)：1)在视频帧嵌入中插入文本时间戳标记构建连续时序参考系统；2)将视频采样重构为视觉-语言检索任务，采用两阶段算法确保语义相关性和时序覆盖

Result: 在小时级长视频基准测试中取得显著改进，7B模型在某些基准上甚至超越了许多72B模型，实现了最先进的绝对时间理解和关键事件定位性能

Conclusion: DATE方法通过显式的时间建模和语义引导的采样策略，有效解决了长视频理解中的时序推理挑战，为多模态大语言模型的长视频处理提供了有效解决方案

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [6] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 通过代码作为思维(CaT)方法，让视觉-语言模型动态选择最优的推理路径（代码表示或直接视觉分析）来提升图表理解能力


<details>
  <summary>Details</summary>
Motivation: 解决现有图表理解方法的局限性：外部工具依赖性强且容易出错，专业模型只采用单一的文本链式思维策略，中间步骤难以验证

Method: 提出代码作为思维(CaT)方法，将图表视觉信息转换为可验证的符号格式；引入可学习的视觉可编程性概念，让模型动态选择代码路径或直接视觉推理路径；使用双重奖励系统进行强化学习

Result: 在多样化的图表理解测试集上展现出强大且稳健的性能

Conclusion: 视觉-语言模型不仅可以学会如何推理，还可以学会动态选择最优的推理方式，为每个任务选择最合适的解决策略

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [7] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对密集自监督学习的显式语义集中方法，通过解耦空间对齐和对象感知过滤来解决补丁级表示中的过度分散问题。


<details>
  <summary>Details</summary>
Motivation: 现有的图像级自监督学习方法在密集任务中存在补丁过度分散问题，即来自同一实例/类别的补丁在表示空间中分散，影响下游密集任务的性能。

Method: 1) 通过蒸馏补丁对应关系打破严格空间对齐；2) 提出噪声容忍排序损失处理噪声和不平衡伪标签；3) 设计对象感知过滤器将输出空间映射到基于对象的空间。

Result: 在各种任务上的实证研究充分证明了该方法的有效性，代码已开源。

Conclusion: 该方法通过显式语义集中成功解决了密集自监督学习中的过度分散问题，为密集表示学习提供了有效解决方案。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 提出了一种针对表格数据的上下文条件异常检测框架，通过自动识别上下文特征和使用深度自编码器建模条件数据分布，在多个基准数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据包含异构上下文（如不同用户），全局稀有事件在特定上下文中可能是正常的，单一全局分布会忽略这些上下文差异，导致检测性能下降

Method: 自动识别上下文特征，使用简单的深度自编码器建模条件数据分布

Result: 在多个表格基准数据集上的广泛实验表明，该方法优于最先进的方法

Conclusion: 上下文在准确区分异常和正常实例中具有重要作用

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [9] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 深度学习天气预测模型存在"统计相似性陷阱"，无法准确预测极端对流事件。DART框架通过双解码器结构和物理驱动方法，显著提升了极端对流检测的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习天气预测模型的评估指标存在"统计相似性陷阱"，导致模型评分高但缺乏对稀罕高影响事件的预测能力。需要专门为极端天气预测设计的方案。

Method: 提出DART双解码器框架，包括：1）明确的背景/极端分解；2）物理驱动的过采样技术；3）任务特定损失函数。该框架将粗糕大气预报转换为高分辨率卫星亮度温度场。

Result: 四项关键发现：1）实证统计相似性陷阱；2）IVT矛盾（移除总水气运输反而提升270%检测效果）；3）DART达到CSI=0.273，偏差仅2.52（基线模型为6.72）；4）2023年8月吉大钢洪水实例验证。训练时间仅需10分钟。

Conclusion: DART框架系统性解决了混合转换-分割-降维任务的挑战，通过专门设计显著提升了极端天气预测的可靠性和准确性，为极端天气预防提供了信任AI解决方案。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [10] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过从LLM蒸馏领域知识和推理能力来提升QA模拟器性能，在保持高效推理的同时实现更好的预测效果


<details>
  <summary>Details</summary>
Motivation: 解决现有QA模拟器中LLM-free方法性能不佳、LLM-based方法推理速度慢和GPU内存消耗高的问题，需要在性能和效率之间找到平衡

Method: 提出LLM蒸馏模拟器(LDSim)，从大型语言模型蒸馏领域知识和推理能力来辅助预测，提升模拟性能

Result: 大量实验表明LDSim在模拟任务和知识追踪任务上都取得了强劲的结果

Conclusion: LDSim方法通过知识蒸馏有效平衡了QA模拟器的性能和效率，为教育推荐系统提供了高质量的模拟数据生成方案

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [11] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高和OpenML数据局限性的实验数据集，包含9,408个管道在300个数据集上的完整实验结果


<details>
  <summary>Details</summary>
Motivation: OpenML等在线存储库中的机器学习实验记录存在局限性：管道多样性不足、数据预处理步骤探索有限、样本不平衡，无法满足元学习需求

Method: 构建PIPES实验集合，设计代表所有技术组合的多样化管道，在300个数据集上执行9,408个管道的实验，记录详细的管道块信息、训练测试时间、预测结果、性能指标和错误信息

Result: 创建了包含全面实验结果的PIPES数据集，支持研究人员进行多样化和代表性管道的分析，为元学习社区提供扩展潜力

Conclusion: PIPES克服了OpenML的局限性，提供了更加多样和完整的实验数据集合，有助于推动算法选择问题和元学习领域的研究发展

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [12] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出了一种针对随机和混沌时空系统的深度学习仿真器，能够根据PDE参数值进行条件化建模，通过预训练和微调实现参数空间泛化，并提供计算加速和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统数值积分方法计算成本高，难以高效探索参数空间。需要开发能够处理不同参数值、域大小和分辨率的通用仿真器，同时提供不确定性量化来研究罕见事件。

Method: 采用预训练+微调策略，在单一参数域预训练后，使用多样化小数据集微调以实现参数泛化。引入局部注意力机制处理可变域大小和分辨率，支持从小域预训练扩展到更大域。

Result: 在混沌Kuramoto-Sivashinsky方程和随机强迫beta-plane湍流上验证，模型能够捕捉插值参数下的现象，相比传统数值积分提供显著计算加速，概率变体提供不确定性量化。

Conclusion: 该方法成功实现了参数条件化的时空系统仿真，提供高效参数空间探索能力，局部注意力机制支持灵活域处理，概率版本支持统计研究和罕见事件分析。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的截断交叉熵损失函数(TCE)，通过降低模型对自身生成数据的过高信心度来延缓模型崩溃现象，将模型保真度持续时间延长了2.3倍以上。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型生成的合成数据越来越多，反复在合成数据上训练会导致模型崩溃，而现有的缓解策略有限。识别到模型对自身生成数据的过高信心度是崩溃的关键驱动因素。

Method: 提出了一种信心感知损失函数，在训练过程中降低高信心度预测的权重。介绍了新的截断交叉熵损失函数(TCE)，并提供了一个模型无关的框架。

Result: TCE显著延迟了递归训练中的模型崩溃，将模型保真度持续时间延长了2.3倍以上。方法在多种模态上都有良好的通用性。

Conclusion: 损失函数的设计是一种简单但强大的工具，可以在合成数据日益增长的时代保持生成式模型的质量。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 基于NeuroTac感应器咈米皮肤咈米皮肤的神经元形态触觉感知系统，通过间刻卷积神经网络实现94.33%的滑动状态分类准确率，能够在滑动发生前360ms检测到初始滑动


<details>
  <summary>Details</summary>
Motivation: 解决在边缘计算平台上部署初始滑动检测系统时遇到的能源约束挑战，提高机器人操作的安全性

Method: 使用NeuroTac感应器配备突出的肉美皮肤，构建间刻卷积神经网络(SCNN)进行滑动状态分类，包括无滑动、初始滑动和显著滑动三个类别

Result: SCNN模型在感应器移动引起的滑动条件下达刱94.33%的分类准确率；在动态重力引起的滑动验证条件下，系统能够在显著滑动发生前至少360ms检测到初始滑动

Conclusion: 这种神经元形态系统具有稳定且响应快速的初始滑动检测能力，为机器人安全操作提供了有效的早期干预手段

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>
