{"id": "2511.08653", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.08653", "abs": "https://arxiv.org/abs/2511.08653", "authors": ["Kaleem Ullah Qasim", "Jiashu Zhang"], "title": "Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion", "comment": null, "summary": "Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku", "AI": {"tldr": "CGAR\u662f\u4e00\u79cd\u65b0\u7684\u9012\u5f52\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u6df1\u5ea6\u8bfe\u7a0b\u5b66\u4e60\u548c\u5206\u5c42\u76d1\u7763\u52a0\u6743\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u5728Sudoku-Extreme\u6570\u636e\u96c6\u4e0a\u5b9e\u73b01.71\u500d\u8bad\u7ec3\u52a0\u901f\u548c42%\u6210\u672c\u51cf\u5c11\u3002", "motivation": "\u9012\u5f52\u63a8\u7406\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff08\u7ea636 GPU\u5c0f\u65f6/\u6570\u636e\u96c6\uff09\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u548c\u7814\u7a76\u3002", "method": "\u63d0\u51faCGAR\u65b9\u6cd5\uff1a\u6e10\u8fdb\u6df1\u5ea6\u8bfe\u7a0b\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u9012\u5f52\u6df1\u5ea6\uff0c\u5206\u5c42\u76d1\u7763\u52a0\u6743\u5bf9\u76d1\u7763\u6b65\u9aa4\u5e94\u7528\u6307\u6570\u8870\u51cf\u6743\u91cd\uff0c\u4e0e\u68af\u5ea6\u5e45\u5ea6\u8870\u51cf\u5bf9\u9f50\u3002", "result": "\u5728Sudoku-Extreme\u6570\u636e\u96c6\u4e0a\uff0cCGAR\u5b9e\u73b01.71\u500d\u8bad\u7ec3\u52a0\u901f\uff0810.93\u52306.38\u5c0f\u65f6\uff09\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.63%\uff0886.65%\u523086.02%\uff09\uff0c\u63a8\u7406\u6548\u7387\u63d0\u5347\uff0c100%\u505c\u6b62\u51c6\u786e\u7387\u548c11%\u66f4\u5c11\u63a8\u7406\u6b65\u9aa4\u3002", "conclusion": "\u57fa\u4e8e\u67b6\u6784\u6df1\u5ea6\u7684\u539f\u5219\u6027\u8bfe\u7a0b\u5b66\u4e60\u80fd\u591f\u5728\u9002\u5ea6\u786c\u4ef6\u4e0a\u9ad8\u6548\u8bad\u7ec3\u9012\u5f52\u63a8\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\u3002"}}
{"id": "2511.08711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08711", "abs": "https://arxiv.org/abs/2511.08711", "authors": ["Abhipsa Basu", "Aviral Gupta", "Abhijnya Bhat", "R. Venkatesh Babu"], "title": "Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification", "comment": null, "summary": "Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5fae\u8c03\u6280\u672f\u751f\u6210\u5e73\u8861\u8bad\u7ec3\u6570\u636e\u4ee5\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u7fa4\u4f53\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7LoRA\u548cDreamBooth\u7b49\u65b9\u6cd5\u751f\u6210\u66f4\u51c6\u786e\u7684\u7fa4\u4f53\u8868\u793a\u56fe\u50cf\uff0c\u5e76\u91c7\u7528\u805a\u7c7b\u7b56\u7565\u5904\u7406\u7ec4\u5185\u53d8\u5f02\u3002", "motivation": "\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\u5e38\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u7fa4\u4f53\u5206\u5e03\u4e0d\u5747\u800c\u7ee7\u627f\u504f\u89c1\uff0c\u4f8b\u5982\u4eba\u8138\u6570\u636e\u96c6\u4e2d\u91d1\u53d1\u4e0e\u5973\u6027\u7684\u8fc7\u5ea6\u5173\u8054\u4f1a\u5f3a\u5316\u523b\u677f\u5370\u8c61\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528Stable Diffusion\u751f\u6210\u5e73\u8861\u6570\u636e\u4f46\u96be\u4ee5\u4fdd\u6301\u539f\u59cb\u6570\u636e\u5206\u5e03\u3002", "method": "\u63a2\u7d22\u591a\u79cd\u6269\u6563\u6a21\u578b\u5fae\u8c03\u6280\u672f\uff08LoRA\u548cDreamBooth\uff09\uff0c\u76f4\u63a5\u4ece\u6837\u672c\u5b66\u4e60\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u7fa4\u4f53\u8868\u793a\u56fe\u50cf\uff1b\u4e3a\u9632\u6b62\u5355\u4e2aDreamBooth\u6a21\u578b\u88ab\u7ec4\u5185\u53d8\u5f02\u6df9\u6ca1\uff0c\u5bf9\u6bcf\u4e2a\u7fa4\u4f53\u5185\u7684\u56fe\u50cf\u8fdb\u884c\u805a\u7c7b\u5e76\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u8bad\u7ec3\u4e00\u4e2aDreamBooth\u6a21\u578b\uff1b\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u751f\u6210\u7fa4\u4f53\u5e73\u8861\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u7814\u7a76\u7684\u5fae\u8c03\u65b9\u6cd5\u5e73\u5747\u4f18\u4e8e\u539f\u59cbStable Diffusion\uff0c\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u53bb\u504f\u6280\u672f\u5982Group-DRO\u6548\u679c\u76f8\u5f53\uff0c\u968f\u7740\u6570\u636e\u96c6\u504f\u89c1\u4e25\u91cd\u7a0b\u5ea6\u589e\u52a0\uff0c\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5fae\u8c03\u6280\u672f\u80fd\u6709\u6548\u751f\u6210\u5e73\u8861\u8bad\u7ec3\u6570\u636e\u4ee5\u7f13\u89e3\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u7fa4\u4f53\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u504f\u89c1\u4e25\u91cd\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2511.08748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08748", "abs": "https://arxiv.org/abs/2511.08748", "authors": ["Estefania Talavera", "Deblina Bhattacharjee", "Himangi Mittal", "Mengwei Ren", "Karen Sanchez", "Carla Muntean", "JungEun Kim", "Mona Jalal"], "title": "WiCV at CVPR 2025: The Women in Computer Vision Workshop", "comment": null, "summary": "The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year's workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.", "AI": {"tldr": "WiCV@CVPR 2025\u662f\u7b2c16\u5c4a\u8ba1\u7b97\u673a\u89c6\u89c9\u5973\u6027\u7814\u8ba8\u4f1a\uff0c\u65e8\u5728\u63d0\u5347\u5973\u6027\u53ca\u5c11\u6570\u7fa4\u4f53\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u53ef\u89c1\u5ea6\u3001\u5305\u5bb9\u6027\u548c\u4e13\u4e1a\u53d1\u5c55\u3002\u6d3b\u52a8\u5305\u62ec\u8bba\u6587\u5c55\u793a\u3001\u5bfc\u5e08\u8ba1\u5212\u548c\u7f51\u7edc\u4ea4\u6d41\uff0c\u5438\u5f15\u4e86100\u591a\u540d\u73b0\u573a\u53c2\u4e0e\u8005\u3002", "motivation": "\u8bb0\u5f55WiCV\u7814\u8ba8\u4f1a\u7684\u5f71\u54cd\u548c\u6f14\u53d8\uff0c\u4e3a\u672a\u6765\u7248\u672c\u548c\u5176\u4ed6\u4fc3\u8fdbAI\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u793e\u533a\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u7684\u5021\u8bae\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u8bba\u6587\u5c55\u793a\uff0814\u7bc7\u5168\u6587\u8bba\u6587\uff0c\u5176\u4e2d5\u7bc7\u53e3\u5934\u62a5\u544a\uff0c36\u7bc7\u6269\u5c55\u6458\u8981\u6d77\u62a5\uff09\u3001\u5bfc\u5e08\u8ba1\u5212\uff0880\u540d\u5b66\u5458\u4e0e37\u540d\u5bfc\u5e08\u5339\u914d\uff09\u548c\u7f51\u7edc\u6d3b\u52a8\u6765\u4fc3\u8fdb\u4e13\u4e1a\u53d1\u5c55\u3002", "result": "\u7814\u8ba8\u4f1a\u6210\u529f\u5438\u5f15\u4e86100\u591a\u540d\u73b0\u573a\u53c2\u4e0e\u8005\uff0c\u83b7\u5f97\u4e8610\u5bb6\u8d5e\u52a9\u5546\u652f\u6301\uff0c\u63d0\u4f9b\u4e86\u7ea644,000\u7f8e\u5143\u7684\u65c5\u884c\u8865\u52a9\u548c\u591a\u6837\u6027\u5956\u9879\u3002", "conclusion": "WiCV@CVPR 2025\u7ee7\u7eed\u5176\u4f7f\u547d\uff0c\u8d4b\u80fd\u65b0\u5174\u7814\u7a76\u4eba\u5458\uff0c\u5e76\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u653e\u5927\u591a\u5143\u58f0\u97f3\uff0c\u4e3a\u4fc3\u8fdb\u793e\u533a\u591a\u6837\u6027\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2511.08942", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08942", "abs": "https://arxiv.org/abs/2511.08942", "authors": ["Mobin Habibpour", "Fatemeh Afghah"], "title": "Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning", "comment": null, "summary": "While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u89c2\u5bdf\u8005\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u7b56\u7565\u5236\u5b9a\u8005\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u3001\u52a8\u6001\u52a8\u4f5c\u5386\u53f2\u96c6\u6210\u548c\u969c\u788d\u7269\u5730\u56fe\u89e3\u91ca\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5c06\u5176\u89d2\u8272\u4ece\u88ab\u52a8\u89c2\u5bdf\u8005\u8f6c\u53d8\u4e3a\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u52a8\u7b56\u7565\u5236\u5b9a\u8005\uff0c\u4ee5\u91ca\u653e\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5168\u90e8\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u6fc0\u53d1\u9010\u6b65\u903b\u8f91\u63a8\u7406\uff0c\u52a8\u6001\u96c6\u6210\u667a\u80fd\u4f53\u8fd1\u671f\u52a8\u4f5c\u5386\u53f2\u9632\u6b62\u9677\u5165\u5faa\u73af\uff0c\u4ee5\u53ca\u8ba9VLM\u80fd\u591f\u89e3\u91ca\u4fef\u89c6\u969c\u788d\u7269\u5730\u56fe\u548c\u7b2c\u4e00\u4eba\u79f0\u89c6\u56fe\u4ee5\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728HM3D\u3001Gibson\u548cMP3D\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u6781\u5176\u76f4\u63a5\u548c\u903b\u8f91\u7684\u8f68\u8ff9\uff0c\u5bfc\u822a\u6548\u7387\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u901a\u8fc7\u5c06VLM\u4f5c\u4e3a\u9ad8\u7ea7\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4e86\u66f4\u667a\u80fd\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002"}}
{"id": "2511.09030", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09030", "abs": "https://arxiv.org/abs/2511.09030", "authors": ["Elliot Meyerson", "Giuseppe Paolo", "Roberto Dailey", "Hormoz Shahrzad", "Olivier Francon", "Conor F. Hayes", "Xin Qiu", "Babak Hodjat", "Risto Miikkulainen"], "title": "Solving a Million-Step LLM Task with Zero Errors", "comment": "Main paper: 14 pages, 29 pages with references and appendix", "summary": "LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.", "AI": {"tldr": "MAKER\u7cfb\u7edf\u9996\u6b21\u5b9e\u73b0\u4e86\u8d85\u8fc7100\u4e07\u6b65LLM\u63a8\u7406\u7684\u96f6\u9519\u8bef\u4efb\u52a1\u6267\u884c\uff0c\u901a\u8fc7\u6781\u7aef\u4efb\u52a1\u5206\u89e3\u548c\u5fae\u4ee3\u7406\u6295\u7968\u673a\u5236\u89e3\u51b3\u4e86LLM\u5728\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u63a8\u7406\u3001\u6d1e\u5bdf\u548c\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728\u6267\u884c\u7c7b\u4f3c\u4eba\u7c7b\u7ec4\u7ec7\u548c\u793e\u4f1a\u89c4\u6a21\u7684\u6269\u5c55\u8fc7\u7a0b\u65f6\u5b58\u5728\u6301\u7eed\u9519\u8bef\u7387\u95ee\u9898\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5927\u89c4\u6a21\u4efb\u52a1\u94fe\u5f0f\u6267\u884c\u3002", "method": "\u91c7\u7528\u6781\u7aef\u4efb\u52a1\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u7531\u4e13\u6ce8\u5fae\u4ee3\u7406\u5904\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u591a\u4ee3\u7406\u6295\u7968\u65b9\u6848\u5728\u6bcf\u4e00\u6b65\u8fdb\u884c\u9519\u8bef\u6821\u6b63\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u8fc7100\u4e07\u6b65LLM\u63a8\u7406\u7684\u4efb\u52a1\u4e14\u96f6\u9519\u8bef\uff0c\u7406\u8bba\u4e0a\u53ef\u6269\u5c55\u5230\u66f4\u9ad8\u6c34\u5e73\u3002", "conclusion": "\u76f8\u6bd4\u6301\u7eed\u6539\u8fdb\u5f53\u524dLLMs\uff0c\u5927\u89c4\u6a21\u5206\u89e3\u4ee3\u7406\u8fc7\u7a0b\uff08MDAPs\uff09\u53ef\u80fd\u4e3a\u7ec4\u7ec7\u548c\u793e\u4f1a\u5c42\u9762\u7684\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09032", "categories": ["cs.AI", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09032", "abs": "https://arxiv.org/abs/2511.09032", "authors": ["Dingji Wang", "You Lu", "Bihuan Chen", "Shuo Hao", "Haowen Jiang", "Yifan Tian", "Xin Peng"], "title": "Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs", "comment": "The paper has been accepted by the 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.\n  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aArgus\u7684\u8fd0\u884c\u65f6\u97e7\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u6301\u7eed\u76d1\u63a7\u8f68\u8ff9\u548c\u5371\u9669\u7f13\u89e3\u673a\u5236\u6765\u9632\u6b62\u5b89\u5168\u8fdd\u89c4\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u516c\u5171\u9053\u8def\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u5404\u79cd\u9a7e\u9a76\u5371\u9669\uff0c\u9700\u8981\u5177\u5907\u6301\u7eed\u76d1\u63a7\u548c\u81ea\u9002\u5e94\u54cd\u5e94\u80fd\u529b\u6765\u7ef4\u6301\u7a33\u5065\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "method": "Argus\u6846\u67b6\u6301\u7eed\u76d1\u63a7ADS\u751f\u6210\u7684\u8f68\u8ff9\uff0c\u5f53\u68c0\u6d4b\u5230\u4e0d\u5b89\u5168\u72b6\u6001\u65f6\u901a\u8fc7\u5371\u9669\u7f13\u89e3\u5668\u65e0\u7f1d\u63a5\u7ba1\u63a7\u5236\u3002", "result": "\u4e0eTCP\u3001UniAD\u548cVAD\u4e09\u4e2a\u5148\u8fdbADS\u96c6\u6210\u6d4b\u8bd5\uff0c\u9a7e\u9a76\u8bc4\u5206\u5e73\u5747\u63d0\u5347150.30%\uff0c\u9632\u6b62\u4e8664.38%\u7684\u8fdd\u89c4\uff0c\u65f6\u95f4\u5f00\u9500\u5f88\u5c0f\u3002", "conclusion": "Argus\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u97e7\u6027\uff0c\u663e\u8457\u6539\u5584\u9a7e\u9a76\u6027\u80fd\u5e76\u9884\u9632\u5b89\u5168\u8fdd\u89c4\u3002"}}
{"id": "2511.08798", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08798", "abs": "https://arxiv.org/abs/2511.08798", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "title": "Structured Uncertainty guided Clarification for LLM Agents", "comment": null, "summary": "LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.", "AI": {"tldr": "\u63d0\u51faSAGE-Agent\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u5de5\u5177\u8c03\u7528\u53c2\u6570\uff0c\u4f7f\u7528POMDP\u548cEVPI\u4f18\u5316\u95ee\u9898\u9009\u62e9\uff0c\u5728\u6a21\u7cca\u4efb\u52a1\u4e2d\u63d0\u9ad8\u8986\u76d6\u73877-39%\uff0c\u540c\u65f6\u51cf\u5c11\u6f84\u6e05\u95ee\u98981.5-2.7\u500d\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u5de5\u5177\u8c03\u7528\u65f6\uff0c\u6a21\u7cca\u7684\u7528\u6237\u6307\u4ee4\u4f1a\u5bfc\u81f4\u9519\u8bef\u8c03\u7528\u548c\u4efb\u52a1\u5931\u8d25\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6f84\u6e05\u673a\u5236\u3002", "method": "\u5c06\u5de5\u5177\u53c2\u6570\u7684\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e3aPOMDP\uff0c\u4f7f\u7528EVPI\u76ee\u6807\u8fdb\u884c\u6700\u4f18\u95ee\u9898\u9009\u62e9\uff0c\u91c7\u7528\u57fa\u4e8e\u65b9\u9762\u7684\u6210\u672c\u5efa\u6a21\u9632\u6b62\u5197\u4f59\u3002", "result": "\u5728ClarifyBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u9ad8\u6a21\u7cca\u4efb\u52a1\u8986\u76d6\u73877-39%\uff0c\u51cf\u5c11\u6f84\u6e05\u95ee\u98981.5-2.7\u500d\u3002\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743GRPO\u8bad\u7ec3\uff0c\u5c06When2Call\u51c6\u786e\u7387\u4ece36.5%\u63d0\u5347\u81f365.2%\u3002", "conclusion": "\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u4e3a\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u540c\u65f6\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u4ea4\u4e92\u6548\u7387\u3002"}}
{"id": "2511.08864", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08864", "abs": "https://arxiv.org/abs/2511.08864", "authors": ["Woosuk Chung", "Seokwoo Hong", "Wonhyeok Lee", "Sangyoon Bae"], "title": "Transformer-Based Sleep Stage Classification Enhanced by Clinical Information", "comment": null, "summary": "Manual sleep staging from polysomnography (PSG) is labor-intensive and prone to inter-scorer variability. While recent deep learning models have advanced automated staging, most rely solely on raw PSG signals and neglect contextual cues used by human experts. We propose a two-stage architecture that combines a Transformer-based per-epoch encoder with a 1D CNN aggregator, and systematically investigates the effect of incorporating explicit context: subject-level clinical metadata (age, sex, BMI) and per-epoch expert event annotations (apneas, desaturations, arousals, periodic breathing). Using the Sleep Heart Health Study (SHHS) cohort (n=8,357), we demonstrate that contextual fusion substantially improves staging accuracy. Compared to a PSG-only baseline (macro-F1 0.7745, micro-F1 0.8774), our final model achieves macro-F1 0.8031 and micro-F1 0.9051, with event annotations contributing the largest gains. Notably, feature fusion outperforms multi-task alternatives that predict the same auxiliary labels. These results highlight that augmenting learned representations with clinically meaningful features enhances both performance and interpretability, without modifying the PSG montage or requiring additional sensors. Our findings support a practical and scalable path toward context-aware, expert-aligned sleep staging systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u7f16\u7801\u5668\u548cCNN\u805a\u5408\u5668\u7684\u4e24\u9636\u6bb5\u67b6\u6784\uff0c\u901a\u8fc7\u878d\u5408\u4e34\u5e8a\u5143\u6570\u636e\u548c\u4e13\u5bb6\u4e8b\u4ef6\u6807\u6ce8\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u7761\u7720\u5206\u671f\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u52a8\u7761\u7720\u5206\u671f\u52b3\u52a8\u5bc6\u96c6\u4e14\u5b58\u5728\u8bc4\u5206\u8005\u95f4\u5dee\u5f02\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5927\u591a\u53ea\u4f9d\u8d56\u539f\u59cbPSG\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e13\u5bb6\u4f7f\u7528\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1aTransformer-based\u6bcf\u65f6\u6bb5\u7f16\u7801\u5668+1D CNN\u805a\u5408\u5668\uff0c\u7cfb\u7edf\u6027\u5730\u878d\u5408\u53d7\u8bd5\u8005\u4e34\u5e8a\u5143\u6570\u636e\uff08\u5e74\u9f84\u3001\u6027\u522b\u3001BMI\uff09\u548c\u6bcf\u65f6\u6bb5\u4e13\u5bb6\u4e8b\u4ef6\u6807\u6ce8\uff08\u547c\u5438\u6682\u505c\u3001\u8840\u6c27\u4e0b\u964d\u3001\u89c9\u9192\u3001\u5468\u671f\u6027\u547c\u5438\uff09\u3002", "result": "\u5728SHHS\u961f\u5217\uff08n=8,357\uff09\u4e0a\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528PSG\u7684\u57fa\u7ebf\uff08macro-F1 0.7745\uff0cmicro-F1 0.8774\uff09\uff0c\u6700\u7ec8\u6a21\u578b\u8fbe\u5230macro-F1 0.8031\u548cmicro-F1 0.9051\uff0c\u4e8b\u4ef6\u6807\u6ce8\u8d21\u732e\u6700\u5927\u589e\u76ca\u3002\u7279\u5f81\u878d\u5408\u4f18\u4e8e\u591a\u4efb\u52a1\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5c06\u4e34\u5e8a\u6709\u610f\u4e49\u7279\u5f81\u4e0e\u5b66\u4e60\u8868\u793a\u7ed3\u5408\uff0c\u5728\u4e0d\u6539\u53d8PSG\u914d\u7f6e\u6216\u9700\u8981\u989d\u5916\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u65e2\u63d0\u5347\u4e86\u6027\u80fd\u53c8\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u4e13\u5bb6\u5bf9\u9f50\u7684\u7761\u7720\u5206\u671f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2511.09275", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09275", "abs": "https://arxiv.org/abs/2511.09275", "authors": ["Minlan Shao", "Zijian Zhang", "Yili Wang", "Yiwei Dai", "Xu Shen", "Xin Wang"], "title": "HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting", "comment": null, "summary": "Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.", "AI": {"tldr": "HyperD\u662f\u4e00\u4e2a\u7528\u4e8e\u4ea4\u901a\u9884\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4ea4\u901a\u6570\u636e\u5206\u89e3\u4e3a\u5468\u671f\u6027\u548c\u6b8b\u5dee\u5206\u91cf\u6765\u5e94\u5bf9\u590d\u6742\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u591a\u5c3a\u5ea6\u5468\u671f\u6a21\u5f0f\u3002", "motivation": "\u4ea4\u901a\u9884\u6d4b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u590d\u6742\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u591a\u5c3a\u5ea6\u5468\u671f\u6a21\u5f0f\u4e0e\u4e0d\u89c4\u5219\u6ce2\u52a8\u7684\u5171\u5b58\u3002\u9700\u8981\u6709\u6548\u5206\u79bb\u5468\u671f\u6027\u6a21\u5f0f\u548c\u975e\u5468\u671f\u6027\u6ce2\u52a8\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faHyperD\u6846\u67b6\uff0c\u5305\u542b\u6df7\u5408\u5468\u671f\u8868\u793a\u6a21\u5757\uff08\u5904\u7406\u5468\u671f\u6027\u5206\u91cf\uff09\u548c\u9891\u7387\u611f\u77e5\u6b8b\u5dee\u8868\u793a\u6a21\u5757\uff08\u5904\u7406\u975e\u5468\u671f\u6027\u6ce2\u52a8\uff09\uff0c\u5e76\u5f15\u5165\u53cc\u89c6\u56fe\u5bf9\u9f50\u635f\u5931\u6765\u5f3a\u5236\u8bed\u4e49\u5206\u79bb\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHyperD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u5e72\u6270\u4e0b\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HyperD\u901a\u8fc7\u89e3\u8026\u5468\u671f\u6027\u548c\u6b8b\u5dee\u5206\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u51c6\u786e\u53ef\u9760\u7684\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09003", "abs": "https://arxiv.org/abs/2511.09003", "authors": ["Zhouxing Tan", "Ruochong Xiong", "Yulong Wan", "Jinlong Ma", "Hanlin Xue", "Qichun Deng", "Haifeng Jing", "Zhengtong Zhang", "Depei Liu", "Shiyuan Luo", "Junfei Liu"], "title": "Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models", "comment": null, "summary": "Emotional support is a core capability in human-AI interaction, with applications including psychological counseling, role play, and companionship. However, existing evaluations of large language models (LLMs) often rely on short, static dialogues and fail to capture the dynamic and long-term nature of emotional support. To overcome this limitation, we shift from snapshot-based evaluation to trajectory-based assessment, adopting a user-centered perspective that evaluates models based on their ability to improve and stabilize user emotional states over time. Our framework constructs a large-scale benchmark consisting of 328 emotional contexts and 1,152 disturbance events, simulating realistic emotional shifts under evolving dialogue scenarios. To encourage psychologically grounded responses, we constrain model outputs using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and we apply causally-adjusted emotion estimation to obtain unbiased emotional state tracking. Based on this framework, we introduce three trajectory-level metrics: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). These metrics collectively capture user emotional dynamics over time and support comprehensive evaluation of long-term emotional support performance of LLMs. Extensive evaluations across a diverse set of LLMs reveal significant disparities in emotional support capabilities and provide actionable insights for model development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u7684\u60c5\u611f\u652f\u6301\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u9759\u6001\u5bf9\u8bdd\u8bc4\u4f30\u8f6c\u5411\u52a8\u6001\u957f\u671f\u8bc4\u4f30\uff0c\u901a\u8fc7\u6a21\u62df328\u4e2a\u60c5\u611f\u60c5\u5883\u548c1152\u4e2a\u6270\u52a8\u4e8b\u4ef6\u6765\u8bc4\u4f30LLMs\u5728\u957f\u671f\u60c5\u611f\u652f\u6301\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u60c5\u611f\u652f\u6301\u8bc4\u4f30\u591a\u4f9d\u8d56\u77ed\u65f6\u9759\u6001\u5bf9\u8bdd\uff0c\u65e0\u6cd5\u6355\u6349\u60c5\u611f\u652f\u6301\u7684\u52a8\u6001\u6027\u548c\u957f\u671f\u6027\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u8d34\u8fd1\u771f\u5b9e\u4ea4\u4e92\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7528\u6237\u4e2d\u5fc3\u89c6\u89d2\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u9a8c\u8bc1\u7684\u60c5\u7eea\u8c03\u8282\u7b56\u7565\u7ea6\u675f\u6a21\u578b\u8f93\u51fa\uff0c\u5c06\u7528\u6237\u60c5\u611f\u8f68\u8ff9\u5efa\u6a21\u4e3a\u4e00\u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u5e76\u5e94\u7528\u56e0\u679c\u8c03\u6574\u7684\u60c5\u611f\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u8f68\u8ff9\u7ea7\u6307\u6807\uff08BEL\u3001ETV\u3001ECP\uff09\u6765\u6355\u6349\u7528\u6237\u60c5\u611f\u52a8\u6001\uff0c\u5e76\u5728\u591a\u6837\u5316LLMs\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u60c5\u611f\u652f\u6301\u80fd\u529b\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLMs\u7684\u957f\u671f\u60c5\u611f\u652f\u6301\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4e3a\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.09222", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09222", "abs": "https://arxiv.org/abs/2511.09222", "authors": ["Jiarui Liu", "Kaustubh Dhole", "Yingheng Wang", "Haoyang Wen", "Sarah Zhang", "Haitao Mao", "Gaotang Li", "Neeraj Varshney", "Jingguo Liu", "Xiaoman Pan"], "title": "Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as a promising framework for aligning language models with complex reasoning objectives. However, most existing methods optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. This challenge is especially pronounced in honesty alignment, where models must not only solve answerable queries but also identify when conclusions cannot be drawn from the given premises. Deductive reasoning provides an ideal testbed because it isolates reasoning capability from reliance on external factual knowledge. To investigate honesty alignment, we curate two multi-step deductive reasoning datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that GRPO, with or without supervised fine tuning initialization, struggles on these tasks. Through extensive experiments across three models, we evaluate stabilization strategies and show that curriculum learning provides some benefit but requires carefully designed in distribution datasets with controllable difficulty. To address these limitations, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling reliable deductive reasoning in aligned language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAnchor\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u771f\u5b9e\u8f68\u8ff9\u6ce8\u5165rollouts\u6765\u9632\u6b62\u65e9\u671f\u8bad\u7ec3\u5d29\u6e83\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4f18\u5316\u6700\u7ec8\u4efb\u52a1\u7ed3\u679c\u65f6\uff0c\u5bb9\u6613\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u56e0\u8d1f\u5956\u52b1\u4e3b\u5bfc\u800c\u5d29\u6e83\uff0c\u7279\u522b\u662f\u5728\u8bda\u5b9e\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u9700\u8981\u65e2\u80fd\u56de\u7b54\u53ef\u89e3\u7b54\u67e5\u8be2\uff0c\u53c8\u80fd\u8bc6\u522b\u65e0\u6cd5\u4ece\u524d\u63d0\u5f97\u51fa\u7ed3\u8bba\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faAnchor\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u8f68\u8ff9\u6ce8\u5165rollouts\u4ee5\u9632\u6b62\u65e9\u671f\u8bad\u7ec3\u5d29\u6e83\uff1b\u4f7f\u7528\u4ece\u56fe\u7ed3\u6784\u6784\u5efa\u7684\u4e24\u4e2a\u591a\u6b65\u6f14\u7ece\u63a8\u7406\u6570\u636e\u96c6\uff08\u7ebf\u6027\u4ee3\u6570\u548c\u903b\u8f91\u63a8\u7406\uff09\uff0c\u5176\u4e2d\u4e00\u534a\u5b9e\u4f8b\u901a\u8fc7\u968f\u673a\u6270\u52a8\u8fb9\u6765\u521b\u5efa\u4e0d\u53ef\u56de\u7b54\u6848\u4f8b\u3002", "result": "Anchor\u65b9\u6cd5\u7a33\u5b9a\u4e86\u5b66\u4e60\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u63a8\u7406\u6027\u80fd\uff1b\u800cGRPO\u65b9\u6cd5\uff08\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u521d\u59cb\u5316\uff09\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u8bfe\u7a0b\u5b66\u4e60\u867d\u6709\u4e00\u5b9a\u5e2e\u52a9\u4f46\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5206\u5e03\u5185\u6570\u636e\u96c6\u3002", "conclusion": "\u8bad\u7ec3\u52a8\u6001\u5bf9\u4e8e\u4f7f\u5bf9\u9f50\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u53ef\u9760\u6f14\u7ece\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0cAnchor\u65b9\u6cd5\u901a\u8fc7\u9632\u6b62\u65e9\u671f\u8bad\u7ec3\u5d29\u6e83\u6709\u6548\u89e3\u51b3\u4e86\u8bda\u5b9e\u5bf9\u9f50\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2511.09101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09101", "abs": "https://arxiv.org/abs/2511.09101", "authors": ["Byunghyun Kim"], "title": "Ultra-Light Test-Time Adaptation for Vision--Language Models", "comment": "7 pages", "summary": "Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.", "AI": {"tldr": "UL-TTA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u53cd\u5411\u4f20\u64ad\u7684\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebfEM\u98ce\u683c\u8fc7\u7a0b\u8c03\u6574logit\u7ea7\u53c2\u6570\uff08\u7c7b\u522b\u539f\u578b\u3001\u5148\u9a8c\u6982\u7387\u548c\u6e29\u5ea6\uff09\uff0c\u5728\u9886\u57df\u504f\u79fb\u4e0b\u663e\u8457\u63d0\u5347CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u9700\u8981\u53cd\u5411\u4f20\u64ad\u3001\u534f\u65b9\u5dee\u4f30\u8ba1\u6216\u5927\u91cf\u5185\u5b58\uff0c\u4e0d\u9002\u7528\u4e8e\u6d41\u5f0f\u548c\u8fb9\u7f18\u573a\u666f\u3002\u5728\u9886\u57df\u504f\u79fb\u4e0b\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7279\u5f81\u6f02\u79fb\u3001\u7c7b\u522b\u5148\u9a8c\u4e0d\u5339\u914d\u548c\u4e25\u91cd\u6821\u51c6\u9519\u8bef\u7684\u95ee\u9898\u3002", "method": "\u51bb\u7ed3\u4e3b\u5e72\u7f51\u7edc\uff0c\u4ec5\u5728\u7ebf\u8c03\u6574logit\u7ea7\u53c2\u6570\uff1a\u4f7f\u7528\u9009\u62e9\u6027\u6837\u672c\u8fc7\u6ee4\u3001\u57fa\u4e8e\u6587\u672c\u548cDirichlet\u5148\u9a8c\u7684\u95ed\u5f0f\u8d1d\u53f6\u65af\u66f4\u65b0\u3001\u89e3\u8026\u7684\u6e29\u5ea6\u53c2\u6570\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u9632\u62a4\u673a\u5236\u9632\u6b62\u957f\u671f\u6d41\u4e2d\u7684\u6f02\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u8de8\u57df\u548cOOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUL-TTA\u76f8\u6bd4\u96f6\u6837\u672cCLIP\u5e73\u5747\u63d0\u53474.7\u4e2a\u767e\u5206\u70b9\u7684top-1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5c06ECE\u964d\u4f4e20-30%\uff0c\u5ef6\u8fdf\u5f00\u9500\u5c0f\u4e8e8%\u3002\u5728\u957f\u8fbe20\u4e07\u6837\u672c\u7684\u6d41\u5f0f\u6d4b\u8bd5\u4e2d\u65e0\u5d29\u6e83\u3002", "conclusion": "logit\u7ea7\u8d1d\u53f6\u65af\u9002\u5e94\u8db3\u4ee5\u5728\u9886\u57df\u504f\u79fb\u4e0b\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387-\u6821\u51c6\u6743\u8861\uff0c\u65e0\u9700\u66f4\u65b0\u4efb\u4f55\u4e3b\u5e72\u7f51\u7edc\u53c2\u6570\u3002"}}
{"id": "2511.09114", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09114", "abs": "https://arxiv.org/abs/2511.09114", "authors": ["Tim Dudman", "Martyn Bull"], "title": "Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks", "comment": "CAMLIS 2025. To be published in the Proceedings of Machine Learning Research (PMLR)", "summary": "Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TERLA\uff08\u62d3\u6251\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff09\uff0c\u4f7f\u7f51\u7edc\u9632\u5fa1\u4ee3\u7406\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u62d3\u6251\u548c\u5927\u5c0f\u7684\u7f51\u7edc\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u56fa\u5b9a\u5927\u5c0f\u52a8\u4f5c\u7a7a\u95f4\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7f51\u7edc\u9632\u5fa1\u4ee3\u7406\u5728\u62d3\u6251\u6216\u89c4\u6a21\u53d8\u5316\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u7f51\u7edc\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u7f51\u7edc\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u5c42\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u7f51\u7edc\u72b6\u6001\u6f5c\u5728\u5d4c\u5165\uff0c\u7ed3\u5408\u56fa\u5b9a\u5927\u5c0f\u3001\u8bed\u4e49\u660e\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u57fa\u4e8ePPO\u7b97\u6cd5\u6784\u5efaTERLA\u4ee3\u7406\u3002", "result": "TERLA\u4ee3\u7406\u5728\u4fdd\u6301\u6807\u51c6PPO\u4ee3\u7406\u9632\u5fa1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u52a8\u4f5c\u6548\u7387\uff0c\u80fd\u591f\u90e8\u7f72\u5230\u4e0d\u540c\u62d3\u6251\u548c\u5927\u5c0f\u7684\u7f51\u7edc\u6bb5\u4e0a\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TERLA\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u7f51\u7edc\u9632\u5fa1\u4ee3\u7406\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u9632\u5fa1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.09200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09200", "abs": "https://arxiv.org/abs/2511.09200", "authors": ["Philipp Dingfelder", "Christian Riess"], "title": "Sure! Here's a short and concise title for your paper: \"Contamination in Generated Text Detection Benchmarks\"", "comment": "published at CSCML 2025", "summary": "Large language models are increasingly used for many applications. To prevent illicit use, it is desirable to be able to detect AI-generated text. Training and evaluation of such detectors critically depend on suitable benchmark datasets. Several groups took on the tedious work of collecting, curating, and publishing large and diverse datasets for this task. However, it remains an open challenge to ensure high quality in all relevant aspects of such a dataset. For example, the DetectRL benchmark exhibits relatively simple patterns of AI-generation in 98.5% of the Claude-LLM data. These patterns may include introductory words such as \"Sure! Here is the academic article abstract:\", or instances where the LLM rejects the prompted task. In this work, we demonstrate that detectors trained on such data use such patterns as shortcuts, which facilitates spoofing attacks on the trained detectors. We consequently reprocessed the DetectRL dataset with several cleansing operations. Experiments show that such data cleansing makes direct attacks more difficult. The reprocessed dataset is publicly available.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86DetectRL\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684AI\u751f\u6210\u6587\u672c\u6a21\u5f0f\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u63d0\u9ad8\u4e86\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u91cd\u65b0\u5904\u7406\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684AI\u6587\u672c\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u5b58\u5728\u8d28\u91cf\u95ee\u9898\uff0c\u7279\u522b\u662fDetectRL\u6570\u636e\u96c6\u4e2d98.5%\u7684Claude-LLM\u6570\u636e\u5305\u542b\u7b80\u5355\u7684AI\u751f\u6210\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u53ef\u80fd\u88ab\u68c0\u6d4b\u5668\u4f5c\u4e3a\u6377\u5f84\u5229\u7528\uff0c\u5bfc\u81f4\u6613\u53d7\u6b3a\u9a97\u653b\u51fb\u3002", "method": "\u5bf9DetectRL\u6570\u636e\u96c6\u8fdb\u884c\u591a\u79cd\u6e05\u6d17\u64cd\u4f5c\uff0c\u79fb\u9664AI\u751f\u6210\u6587\u672c\u4e2d\u7684\u7279\u5b9a\u6a21\u5f0f\uff08\u5982\u5f00\u5934\u8bed\"Sure! Here is the academic article abstract:\"\u548c\u4efb\u52a1\u62d2\u7edd\u5b9e\u4f8b\uff09\uff0c\u91cd\u65b0\u5904\u7406\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6570\u636e\u6e05\u6d17\u4f7f\u5f97\u76f4\u63a5\u653b\u51fb\u68c0\u6d4b\u5668\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6570\u636e\u6e05\u6d17\u662f\u63d0\u9ad8AI\u6587\u672c\u68c0\u6d4b\u5668\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u91cd\u65b0\u5904\u7406\u7684\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u6709\u52a9\u4e8e\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u5668\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002"}}
{"id": "2511.09315", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09315", "abs": "https://arxiv.org/abs/2511.09315", "authors": ["Andi Chen"], "title": "A Tensor Residual Circuit Neural Network Factorized with Matrix Product Operation", "comment": "This is the supplementary material link: https://drive.google.com/file/d/141v7as_sMheOFxYkcqUPtG6F7KD1-50Y/view?usp=sharing", "summary": "It is challenging to reduce the complexity of neural networks while maintaining their generalization ability and robustness, especially for practical applications. Conventional solutions for this problem incorporate quantum-inspired neural networks with Kronecker products and hybrid tensor neural networks with MPO factorization and fully-connected layers. Nonetheless, the generalization power and robustness of the fully-connected layers are not as outstanding as circuit models in quantum computing. In this paper, we propose a novel tensor circuit neural network (TCNN) that takes advantage of the characteristics of tensor neural networks and residual circuit models to achieve generalization ability and robustness with low complexity. The proposed activation operation and parallelism of the circuit in complex number field improves its non-linearity and efficiency for feature learning. Moreover, since the feature information exists in the parameters in both the real and imaginary parts in TCNN, an information fusion layer is proposed for merging features stored in those parameters to enhance the generalization capability. Experimental results confirm that TCNN showcases more outstanding generalization and robustness with its average accuracies on various datasets 2\\%-3\\% higher than those of the state-of-the-art compared models. More significantly, while other models fail to learn features under noise parameter attacking, TCNN still showcases prominent learning capability owing to its ability to prevent gradient explosion. Furthermore, it is comparable to the compared models on the number of trainable parameters and the CPU running time. An ablation study also indicates the advantage of the activation operation, the parallelism architecture and the information fusion layer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f20\u91cf\u7535\u8def\u795e\u7ecf\u7f51\u7edc\uff08TCNN\uff09\uff0c\u7ed3\u5408\u5f20\u91cf\u795e\u7ecf\u7f51\u7edc\u548c\u6b8b\u5dee\u7535\u8def\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u590d\u6742\u5ea6\u7684\u540c\u65f6\u96be\u4ee5\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u91cf\u5b50\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u6df7\u5408\u5f20\u91cf\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5168\u8fde\u63a5\u5c42\u5728\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4e0d\u5982\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u7535\u8def\u6a21\u578b\u3002", "method": "\u63d0\u51faTCNN\u6a21\u578b\uff0c\u5229\u7528\u590d\u6570\u57df\u4e2d\u7684\u6fc0\u6d3b\u64cd\u4f5c\u548c\u7535\u8def\u5e76\u884c\u6027\u63d0\u9ad8\u975e\u7ebf\u6027\u7279\u5f81\u5b66\u4e60\u6548\u7387\uff0c\u5f15\u5165\u4fe1\u606f\u878d\u5408\u5c42\u5408\u5e76\u5b9e\u90e8\u548c\u865a\u90e8\u53c2\u6570\u4e2d\u7684\u7279\u5f81\u4fe1\u606f\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTCNN\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u6700\u5148\u8fdb\u7684\u5bf9\u6bd4\u6a21\u578b\u9ad8\u51fa2%-3%\uff0c\u5728\u566a\u58f0\u53c2\u6570\u653b\u51fb\u4e0b\u4ecd\u80fd\u4fdd\u6301\u663e\u8457\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4e14\u53c2\u6570\u91cf\u548cCPU\u8fd0\u884c\u65f6\u95f4\u4e0e\u5bf9\u6bd4\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "TCNN\u901a\u8fc7\u6fc0\u6d3b\u64cd\u4f5c\u3001\u5e76\u884c\u67b6\u6784\u548c\u4fe1\u606f\u878d\u5408\u5c42\u7684\u7ed3\u5408\uff0c\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.09478", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09478", "abs": "https://arxiv.org/abs/2511.09478", "authors": ["Renda Li", "Hailang Huang", "Fei Wei", "Feng Xiong", "Yong Wang", "Xiangxiang Chu"], "title": "AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.", "AI": {"tldr": "AdaCuRL\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u96be\u5ea6\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u8c03\u5ea6\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u9965\u997f\u548c\u7b56\u7565\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6df7\u5408\u96be\u5ea6\u6837\u672c\u4e0a\u8bad\u7ec3\u65f6\u4f1a\u51fa\u73b0\u68af\u5ea6\u9965\u997f\u548c\u7b56\u7565\u9000\u5316\u95ee\u9898\uff0c\u800c\u6784\u5efa\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u96be\u5ea6\u4e0d\u5339\u914d\u3001\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u548c\u707e\u96be\u6027\u9057\u5fd8\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faAdaCuRL\u6846\u67b6\uff0c\u6574\u5408\u7c97\u5230\u7ec6\u96be\u5ea6\u4f30\u8ba1\u4e0e\u81ea\u9002\u5e94\u8bfe\u7a0b\u8c03\u5ea6\uff0c\u52a8\u6001\u5bf9\u9f50\u6570\u636e\u96be\u5ea6\u4e0e\u6a21\u578b\u80fd\u529b\uff0c\u5305\u542b\u6570\u636e\u91cd\u8bbf\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u53c2\u8003\u548c\u7a00\u758fKL\u7b56\u7565\u9632\u6b62\u7b56\u7565\u9000\u5316\u3002", "result": "\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaCuRL\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e0a\u90fd\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AdaCuRL\u901a\u8fc7\u81ea\u9002\u5e94\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09475", "abs": "https://arxiv.org/abs/2511.09475", "authors": ["Anli Ji", "Pranjal Patil", "Chetraj Pandey", "Manolis K. Georgoulis", "Berkay Aydin"], "title": "Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach", "comment": "10 pages, 3 Figures. This is a pre-print of an accepted paper at ICDMW: SABID 2025", "summary": "Solar energetic particle (SEP) events, as one of the most prominent manifestations of solar activity, can generate severe hazardous radiation when accelerated by solar flares or shock waves formed aside from coronal mass ejections (CMEs). However, most existing data-driven methods used for SEP predictions are operated as black-box models, making it challenging for solar physicists to interpret the results and understand the underlying physical causes of such events rather than just obtain a prediction. To address this challenge, we propose a novel framework that integrates global explanations and ad-hoc feature mapping to enhance model transparency and provide deeper insights into the decision-making process. We validate our approach using a dataset of 341 SEP events, including 244 significant (>=10 MeV) proton events exceeding the Space Weather Prediction Center S1 threshold, spanning solar cycles 22, 23, and 24. Furthermore, we present an explainability-focused case study of major SEP events, demonstrating how our method improves explainability and facilitates a more physics-informed understanding of SEP event prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5168\u5c40\u89e3\u91ca\u548c\u7279\u8bbe\u7279\u5f81\u6620\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u592a\u9633\u9ad8\u80fd\u7c92\u5b50\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u7406\u89e3\u7269\u7406\u673a\u5236\u800c\u4e0d\u4ec5\u4ec5\u662f\u83b7\u5f97\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684SEP\u9884\u6d4b\u65b9\u6cd5\u591a\u4e3a\u9ed1\u76d2\u6a21\u578b\uff0c\u96be\u4ee5\u89e3\u91ca\u7ed3\u679c\u548c\u7406\u89e3\u7269\u7406\u539f\u56e0\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u4ee5\u652f\u6301\u592a\u9633\u7269\u7406\u7814\u7a76\u3002", "method": "\u96c6\u6210\u5168\u5c40\u89e3\u91ca\u548c\u7279\u8bbe\u7279\u5f81\u6620\u5c04\u7684\u6846\u67b6\uff0c\u4f7f\u7528341\u4e2aSEP\u4e8b\u4ef6\u6570\u636e\u96c6\uff08\u5305\u62ec244\u4e2a\u91cd\u8981\u8d28\u5b50\u4e8b\u4ef6\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u8fdb\u884c\u4e86\u53ef\u89e3\u91ca\u6027\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4fc3\u8fdb\u4e86\u57fa\u4e8e\u7269\u7406\u7684SEP\u4e8b\u4ef6\u9884\u6d4b\u7406\u89e3\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u589e\u5f3a\u4e86SEP\u9884\u6d4b\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u4e3a\u592a\u9633\u7269\u7406\u5b66\u5bb6\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7406\u89e3\u4e8b\u4ef6\u7269\u7406\u673a\u5236\u7684\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91caAI\u5728\u7a7a\u95f4\u5929\u6c14\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2511.09527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09527", "abs": "https://arxiv.org/abs/2511.09527", "authors": ["Tian Lan", "Rishad Shafik", "Alex Yakovlev"], "title": "Event-Driven Digital-Time-Domain Inference Architectures for Tsetlin Machines", "comment": null, "summary": "Machine learning fits model parameters to approximate input-output mappings, predicting unknown samples. However, these models often require extensive arithmetic computations during inference, increasing latency and power consumption. This paper proposes a digital-time-domain computing approach for Tsetlin machine (TM) inference process to address these challenges. This approach leverages a delay accumulation mechanism to mitigate the costly arithmetic sums of classes and employs a Winner-Takes-All scheme to replace conventional magnitude comparators. Specifically, a Hamming distance-driven time-domain scheme is implemented for multi-class TMs. Furthermore, differential delay paths, combined with a leading-ones-detector logarithmic delay compression digital-time-domain scheme, are utilised for the coalesced TMs, accommodating both binary-signed and exponential-scale delay accumulation issues. Compared to the functionally equivalent, post-implementation digital TM architecture baseline, the proposed architecture demonstrates orders-of-magnitude improvements in energy efficiency and throughput.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u65f6\u57df\u8ba1\u7b97\u7684Tsetlin\u673a\u5668\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ef6\u8fdf\u7d2f\u79ef\u673a\u5236\u548c\u80dc\u8005\u5168\u5f97\u65b9\u6848\u6765\u51cf\u5c11\u7b97\u672f\u8fd0\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u5e38\u9700\u8981\u5927\u91cf\u7b97\u672f\u8ba1\u7b97\uff0c\u8fd9\u4f1a\u589e\u52a0\u5ef6\u8fdf\u548c\u529f\u8017\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u67b6\u6784\u3002", "method": "\u91c7\u7528\u6570\u5b57\u65f6\u57df\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4f7f\u7528\u5ef6\u8fdf\u7d2f\u79ef\u673a\u5236\u66ff\u4ee3\u6602\u8d35\u7684\u7c7b\u522b\u7b97\u672f\u548c\uff0c\u7528\u80dc\u8005\u5168\u5f97\u65b9\u6848\u66ff\u6362\u4f20\u7edf\u7684\u5e45\u5ea6\u6bd4\u8f83\u5668\u3002\u5bf9\u4e8e\u591a\u7c7bTM\u5b9e\u73b0\u6c49\u660e\u8ddd\u79bb\u9a71\u52a8\u7684\u65f6\u57df\u65b9\u6848\uff0c\u5bf9\u4e8e\u878d\u5408TM\u4f7f\u7528\u5dee\u5206\u5ef6\u8fdf\u8def\u5f84\u548c\u9886\u5148\u4e00\u68c0\u6d4b\u5668\u5bf9\u6570\u5ef6\u8fdf\u538b\u7f29\u65b9\u6848\u3002", "result": "\u4e0e\u529f\u80fd\u7b49\u6548\u7684\u540e\u5b9e\u73b0\u6570\u5b57TM\u67b6\u6784\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5728\u80fd\u6548\u548c\u541e\u5410\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u63d0\u5347\u3002", "conclusion": "\u6570\u5b57\u65f6\u57df\u8ba1\u7b97\u65b9\u6cd5\u4e3aTsetlin\u673a\u5668\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u529f\u8017\u3002"}}
