<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming](https://arxiv.org/abs/2601.19433)
*Jisheng Chu,Wenrui Li,Rui Zhao,Wangmeng Zuo,Shifeng Chen,Xiaopeng Fan*

Main category: cs.CV

TL;DR: RoamScene3D：基于语义推理和几何约束的文本到3D场景生成框架，通过场景图引导自适应漫游轨迹和运动注入修复模型解决现有方法的空间盲区和静态2D先验限制


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D场景生成方法存在空间盲区，依赖预定义轨迹无法利用显著对象间内在关系，难以理解语义布局和推断遮挡内容；同时当前修复模型在2D图像空间操作，难以合理填充相机运动造成的空洞

Method: 1）使用视觉语言模型构建编码对象关系的场景图，引导相机感知显著对象边界并规划自适应漫游轨迹；2）提出运动注入修复模型，在合成全景数据集上微调，整合真实相机轨迹使其适应相机运动

Result: 通过语义推理和几何约束，RoamScene3D在生成一致且逼真的3D场景方面显著优于现有最先进方法

Conclusion: RoamScene3D成功弥合了语义引导与空间生成之间的差距，通过语义关系推理和自适应相机轨迹规划，能够生成一致且逼真的3D场景，解决了现有方法的局限性

Abstract: Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.

</details>


### [2] [Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation](https://arxiv.org/abs/2601.19488)
*Yizhao Han,Tianxing Shi,Zhao Wang,Zifan Xu,Zhiyuan Pu,Mingxiao Li,Qian Zhang,Wei Yin,Xiao-Xiao Long*

Main category: cs.CV

TL;DR: 提出ENkG采样策略，通过基于熵的自适应候选token数量来解决视频生成中静态top-k/top-p采样策略的不足，提升长序列生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频token与语言token存在本质差异：视频token语义密度低、时空冗余度高，导致静态top-k/top-p采样策略在视频生成中效果不佳。低不确定性区域（静态背景）会引入不必要随机性，高不确定性区域（前景物体）则容易陷入早期错误累积，最终严重降低长序列生成质量。

Method: 提出熵引导的k-Guard（ENkG）采样策略，根据每个token预测分布的熵值（量化token分散程度）自适应调整候选token数量。低熵区域使用较少候选以抑制冗余噪声并保持结构完整性，高熵区域使用更多候选以减轻错误累积。该方法模型无关、无需训练、计算开销极小。

Result: 实验表明，相比静态top-k/top-p策略，ENkG在感知质量和结构稳定性方面带来一致改进。

Conclusion: ENkG采样策略通过自适应候选token数量有效解决了视频生成中静态采样策略的局限性，为自回归视频解码器提供了更合适的采样方法。

Abstract: Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.

</details>


### [3] [A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder](https://arxiv.org/abs/2601.19526)
*Fouad Boutaleb,Emery Pierson,Mohamed Daoudi,Clémence Nineuil,Ali Amad,Fabien D'Hondt*

Main category: cs.CV

TL;DR: 提出非侵入式计算框架，将单目RGB视频转换为临床相关3D步态运动学，用于检测抑郁症的精神运动迟缓，准确率达83.3%


<details>
  <summary>Details</summary>
Motivation: 目前抑郁症诊断主要依赖主观评估，缺乏客观、可解释的生物标志物。精神运动迟缓是抑郁症核心症状，但临床评估主观性强。3D动作捕捉虽客观但设备昂贵不便临床常规使用，需要开发基于普通摄像头的客观评估方法。

Method: 1) 提出计算框架将单目RGB视频转换为3D步态运动学：使用重力视图坐标和新颖的轨迹校正算法，利用改良版Timed Up and Go协议闭环拓扑缓解单目深度误差；2) 从单摄像头提取297个明确步态生物力学标志物；3) 针对小临床数据集，引入基于稳定性的机器学习框架识别稳健运动特征，防止过拟合。

Result: 在CALYPSO数据集上验证：1) 检测精神运动迟缓准确率达83.3%；2) 解释总体抑郁严重程度64%的方差(R²=0.64)；3) 发现踝关节推进力减少和骨盆活动受限与抑郁运动表型强相关；4) 证明身体运动可作为认知状态的稳健代理指标。

Conclusion: 该方法为抑郁症客观监测提供了透明、可扩展的工具，证明物理运动能有效反映认知状态，可在标准临床环境中实现抑郁症的客观评估和监测。

Abstract: Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.
  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.
  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.

</details>


### [4] [KeepLoRA: Continual Learning with Residual Gradient Adaptation](https://arxiv.org/abs/2601.19659)
*Mao-Lin Luo,Zi-Hao Zhou,Yi-Lin Zhang,Yuanyu Wan,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: KeepLoRA：一种用于预训练视觉语言模型持续学习的简单有效方法，通过将LoRA参数更新限制在残差子空间来平衡预训练知识保留、已学任务知识保持和新知识获取能力


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型的持续学习需要平衡三个相互竞争的目标：保留预训练知识、保持已学任务序列的知识、以及维持获取新知识的可塑性。现有方法难以有效平衡这些目标。

Method: 首先分析模型参数空间中的知识保留机制，发现通用知识主要编码在主成分子空间，而任务特定知识编码在残差子空间。基于此，KeepLoRA通过将新任务的梯度投影到与预训练模型主成分子空间和先前任务特征主导方向正交的子空间，将LoRA参数更新限制在残差子空间，避免干扰先前学习的能力。

Result: 理论和实证分析证实KeepLoRA能有效平衡三个目标，并在持续学习任务中实现了最先进的性能。

Conclusion: KeepLoRA提供了一种简单而有效的持续学习方法，通过参数空间分析指导的梯度投影策略，在预训练视觉语言模型中实现了知识保留、任务保持和可塑性之间的良好平衡。

Abstract: Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.

</details>


### [5] [DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization](https://arxiv.org/abs/2601.19717)
*Yitong Yang,Xuexin Liu,Yinglin Wang,Jing Wang,Hao Dou,Changshuo Wang,Shuting He*

Main category: cs.CV

TL;DR: DiffStyle3D：一种基于扩散的3D高斯泼溅风格迁移新范式，通过潜在空间直接优化，引入注意力感知损失和几何引导的多视角一致性方法，提升风格化质量和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法存在局限性：基于VGG和CLIP的方法难以在模型内部建模多视角一致性，而基于扩散的方法虽然能捕捉一致性，但依赖去噪方向导致训练不稳定。需要一种既能保持多视角一致性又训练稳定的新方法。

Method: 提出DiffStyle3D框架：1）在潜在空间直接优化；2）引入注意力感知损失，在自注意力空间对齐风格特征，同时通过内容特征对齐保持原始内容；3）提出几何引导的多视角一致性方法，将几何信息融入自注意力实现跨视角对应建模；4）构建几何感知掩码，防止重叠区域冗余优化。

Result: 大量实验表明，DiffStyle3D在风格化质量和视觉真实感方面优于现有最先进方法，实现了更高的多视角一致性和更好的风格迁移效果。

Conclusion: DiffStyle3D通过潜在空间直接优化、注意力感知损失和几何引导的多视角一致性方法，有效解决了现有3D风格迁移方法的局限性，为3D高斯泼溅风格迁移提供了稳定高效的解决方案。

Abstract: 3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Attention-Enhanced Graph Filtering for False Data Injection Attack Detection and Localization](https://arxiv.org/abs/2601.18981)
*Ruslan Abdulin,Mohammad Rasoul Narimani*

Main category: cs.LG

TL;DR: 提出结合ARMA图卷积滤波器和Encoder-Only Transformer的联合FDIA检测与定位框架，有效利用电网状态和拓扑信息，在IEEE测试系统中实现高精度攻击检测和节点定位。


<details>
  <summary>Details</summary>
Motivation: 随着物联网测量设备在电力系统中的广泛应用，电网面临虚假数据注入攻击（FDIAs）的威胁日益严重。现有检测方法主要依赖图学习利用空间相关性和网络拓扑，但通常采用高维表示和浅层分类器，难以捕捉局部结构依赖和全局上下文关系。直接使用Transformer架构可能导致模型过深，无法有效建模局部电网动态。

Method: 提出联合FDIA检测与定位框架，整合自回归移动平均（ARMA）图卷积滤波器和Encoder-Only Transformer架构。ARMA图滤波器提供鲁棒的拓扑感知特征提取，适应突发频谱变化；Transformer编码器利用自注意力机制捕捉电网元素间的长程依赖关系，同时保留必要的局部上下文信息。

Result: 使用纽约独立系统运营商（NYISO）的真实负荷数据，在IEEE 14-和300-总线系统上进行评估。数值结果表明，所提模型有效利用了电网状态和拓扑信息，在检测FDIA事件和定位受损节点方面实现了高精度。

Conclusion: 提出的ARMA图卷积滤波器与Transformer编码器结合框架能够有效解决电力系统虚假数据注入攻击的检测与定位问题，既捕捉了局部结构依赖，又建模了全局上下文关系，为电网网络安全提供了有效的解决方案。

Abstract: The increasing deployment of Internet-of-Things (IoT)-enabled measurement devices in modern power systems has expanded the cyberattack surface of the grid. As a result, this critical infrastructure is increasingly exposed to cyberattacks, including false data injection attacks (FDIAs) that compromise measurement integrity and threaten reliable system operation. Existing FDIA detection methods primarily exploit spatial correlations and network topology using graph-based learning; however, these approaches often rely on high-dimensional representations and shallow classifiers, limiting their ability to capture local structural dependencies and global contextual relationships. Moreover, naively incorporating Transformer architectures can result in overly deep models that struggle to model localized grid dynamics. This paper proposes a joint FDIA detection and localization framework that integrates auto-regressive moving average (ARMA) graph convolutional filters with an Encoder-Only Transformer architecture. The ARMA-based graph filters provide robust, topology-aware feature extraction and adaptability to abrupt spectral changes, while the Transformer encoder leverages self-attention to capture long-range dependencies among grid elements without sacrificing essential local context. The proposed method is evaluated using real-world load data from the New York Independent System Operator (NYISO) applied to the IEEE 14- and 300-bus systems. Numerical results demonstrate that the proposed model effectively exploits both the state and topology of the power grid, achieving high accuracy in detecting FDIA events and localizing compromised nodes.

</details>


### [7] [EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting](https://arxiv.org/abs/2601.19022)
*Antanas Zilinskas,Robert N. Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: EVEREST：基于Transformer的罕见事件概率预测架构，集成注意力瓶颈、证据头、极值头和轻量级前兆头，在空间天气数据上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中的罕见事件预测面临类别不平衡、长程依赖和分布不确定性等挑战，需要能够提供校准预测和尾部风险估计的解决方案

Method: 提出EVEREST架构，包含四个组件：可学习注意力瓶颈（软聚合时序动态）、证据头（通过Normal-Inverse-Gamma分布估计不确定度）、极值头（使用广义帕累托分布建模尾部风险）、轻量级前兆头（早期事件检测）。通过复合损失函数联合优化，部署时仅使用单个分类头

Result: 在十年空间天气数据上，对C级耀斑在24/48/72小时预测范围内分别达到0.973/0.970/0.966的True Skill Statistic，模型紧凑（约0.81M参数），可在普通硬件上高效训练

Conclusion: EVEREST为罕见事件预测提供了紧凑、高效且可解释的解决方案，适用于工业监控、天气和卫星诊断等高风险领域。未来需扩展到流式和多模态预测

Abstract: Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.

</details>


### [8] [Foresight Learning for SEC Risk Prediction](https://arxiv.org/abs/2601.19189)
*Benjamin Turtel,Paul Wilczewski,Danny Franklin,Kris Skotheim*

Main category: cs.LG

TL;DR: 开发了一个自动化系统，将SEC风险披露转化为带时间标签的监督数据，并训练了一个紧凑的LLM来预测风险实现概率，性能优于GPT-5等前沿模型。


<details>
  <summary>Details</summary>
Motivation: SEC文件中的风险披露通常只描述潜在不利事件而不量化其可能性，这限制了概率分析的有用性。主要障碍是缺乏大规模、风险级别的监督数据来将披露的风险与实际结果联系起来。

Method: 1. 开发完全自动化的数据生成管道，将定性SEC风险披露转化为时间基础监督数据；2. 从风险因素部分生成公司特定、有时间限制的风险查询；3. 通过自动解析后续披露结果来标注查询；4. 使用该数据集训练紧凑的大型语言模型来估计风险在特定时间范围内实现的概率。

Result: 尽管模型规模适中，但在概率准确性和校准方面显著优于预训练和启发式基线，并且超越了包括GPT-5在内的前沿通用模型。模型可在单个GPU上部署。

Conclusion: 这项工作展示了"前瞻学习"能够仅使用原始、按时间顺序排列的领域内文本，无需专有数据、外部语料库或手动标注，即可实现领域特定专家模型的可扩展、全自动化训练。为从企业文档中学习校准的、与决策相关的信号提供了一条通用路径。

Abstract: Risk disclosures in SEC filings describe potential adverse events but rarely quantify their likelihood, limiting their usefulness for probabilistic analysis. A central obstacle is the absence of large-scale, risk-level supervision linking disclosed risks to realized outcomes.
  We introduce a fully automated data generation pipeline that converts qualitative SEC risk disclosures into temporally grounded supervision using only public data. For each filing, the pipeline generates firm-specific, time-bounded risk queries from the Risk Factors section and labels them by automatically resolving outcomes against subsequent disclosures.
  Using this dataset of risk queries and outcomes grounded in SEC filings, we train a compact large language model to estimate the probability that a disclosed risk will materialize within a specified horizon. Despite its modest size, the resulting model substantially improves over pretrained and heuristic baselines, and outperforms frontier general-purpose models, including GPT-5, on probabilistic accuracy and calibration.
  More broadly, this work demonstrates that Foresight Learning enables scalable and fully automated training of domain-specific expert models using only raw, chronological, in-domain text -- without proprietary data, external corpora, or manual annotation. The resulting models achieve frontier-level performance while remaining deployable on a single GPU. This result suggests a general pathway for learning calibrated, decision-relevant signals from naturally occurring enterprise documents.
  To support transparency and reproducibility, we open-source the evaluation dataset used in this study.
  Evaluation Data: https://huggingface.co/datasets/LightningRodLabs/sec_risk_questions_test_set
  Data Generation Platform: https://lightningrod.ai/
  SDK: https://github.com/lightning-rod-labs/lightningrod-python-sdk

</details>


### [9] [Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework](https://arxiv.org/abs/2601.19285)
*Xinyu Zhou,Jiawei Zhang,Stephen J. Wright*

Main category: cs.LG

TL;DR: 扩散模型存在记忆化问题，生成样本可能完全复制训练样本。本文提出理论框架解释此现象，并提出两种新方法（噪声无条件化和温度平滑）来增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优异，但面临记忆化挑战，即生成样本可能完全复制训练样本。需要理论解释此现象并提出解决方案来改善泛化能力。

Method: 1. 理论框架：证明经验得分函数是高斯分布得分函数的加权和，权重为尖锐的softmax函数，导致单个训练样本主导得分函数；2. 噪声无条件化：让每个训练样本自适应确定其得分函数权重，增加更多训练样本的影响；3. 温度平滑：引入温度参数控制softmax权重的平滑度，减少单个样本的主导作用。

Result: 理论分析解释了神经网络通过平滑近似加权和来改善泛化的机制。实验验证了理论分析，并证明提出的两种方法在多个数据集上能有效提高泛化能力，同时保持高生成质量。

Conclusion: 扩散模型的记忆化问题源于经验得分函数的尖锐加权结构。通过理论分析揭示了神经网络平滑近似的泛化机制，提出的噪声无条件化和温度平滑方法能有效缓解记忆化问题，提升模型泛化性能。

Abstract: Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.

</details>


### [10] [Process-Aware Procurement Lead Time Prediction for Shipyard Delay Mitigation](https://arxiv.org/abs/2601.19296)
*Yongjae Lee,Eunhee Park,Daesan Park,Dongho Kim,Jongho Choi,Hyerim Bae*

Main category: cs.LG

TL;DR: 该论文提出了一种结合事件日志和静态属性的新框架，用于预测造船业中管段采购的提前期，相比现有方法在MAE指标上提升了22.6%到50.4%。


<details>
  <summary>Details</summary>
Motivation: 在造船等按订单设计的行业中，准确预测采购提前期（PLT）至关重要，因为单个关键组件（如管段）的延迟会破坏整个项目时间表。现有研究仅使用管段的静态物理属性进行预测，忽略了采购本质上是一个涉及船厂内外连续事件的多方动态业务流程。

Method: 提出一个新颖框架，将事件日志（采购事件的记录）与静态属性相结合。提取每个事件的时间属性以反映过程的连续性和时间上下文。然后采用深度序列神经网络与多层感知机相结合的方法，整合静态和动态特征，使模型能够捕捉采购中的结构性和上下文信息。

Result: 使用韩国知名造船公司的真实管段采购数据进行对比实验，评估了生产、后处理和采购提前期预测三个任务。结果显示，在平均绝对误差（MAE）方面，相比现有最佳方法，预测性能提升了22.6%到50.4%。

Conclusion: 研究结果表明，考虑采购过程信息对于更准确地预测采购提前期具有重要价值。将事件日志与静态属性相结合的方法能够显著提升预测性能，为按订单设计行业的供应链管理提供了有效工具。

Abstract: Accurately predicting procurement lead time (PLT) remains a challenge in engineered-to-order industries such as shipbuilding and plant construction, where delays in a single key component can disrupt project timelines. In shipyards, pipe spools are critical components; installed deep within hull blocks soon after steel erection, any delay in their procurement can halt all downstream tasks. Recognizing their importance, existing studies predict PLT using the static physical attributes of pipe spools. However, procurement is inherently a dynamic, multi-stakeholder business process involving a continuous sequence of internal and external events at the shipyard, factors often overlooked in traditional approaches. To address this issue, this paper proposes a novel framework that combines event logs, dataset records of the procurement events, with static attributes to predict PLT. The temporal attributes of each event are extracted to reflect the continuity and temporal context of the process. Subsequently, a deep sequential neural network combined with a multi-layered perceptron is employed to integrate these static and dynamic features, enabling the model to capture both structural and contextual information in procurement. Comparative experiments are conducted using real-world pipe spool procurement data from a globally renowned South Korean shipbuilding corporation. Three tasks are evaluated, which are production, post-processing, and procurement lead time prediction. The results show a 22.6% to 50.4% improvement in prediction performance in terms of mean absolute error over the best-performing existing approaches across the three tasks. These findings indicate the value of considering procurement process information for more accurate PLT prediction.

</details>


### [11] [Post-LayerNorm Is Back: Stable, ExpressivE, and Deep](https://arxiv.org/abs/2601.19895)
*Chen Chen,Lai Wei*

Main category: cs.LG

TL;DR: Keel提出了一种使用Highway-style连接替代传统残差连接的Post-LN Transformer，解决了深度训练中的梯度消失问题，实现了超过1000层的稳定训练。


<details>
  <summary>Details</summary>
Motivation: 当前LLM扩展面临瓶颈：宽度扩展收益递减，上下文长度扩展无法提升基础表达能力，而深度扩展理论上具有更好的表达能力，但现有Transformer架构在极端深度下训练不稳定。Post-LN因不稳定被Pre-LN取代，但其失败模式源于ResNet-style残差路径导致的梯度消失。

Method: 提出Keel架构，在Post-LN Transformer中用Highway-style连接替代传统的残差连接，保持残差分支的梯度流动，防止信号从顶层到底层的消失。该方法无需特殊初始化或复杂优化技巧。

Result: Keel能够在超过1000层的极端深度下稳定训练，在困惑度和深度扩展特性上持续优于Pre-LN，展示了Post-LN与Highway-style连接结合的有效性。

Conclusion: Post-LN与Highway-style连接结合为构建深度可扩展的LLM提供了简单有效的基础，为未来无限深度架构开辟了可能性。

Abstract: Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.

</details>


### [12] [From Observations to Events: Event-Aware World Model for Reinforcement Learning](https://arxiv.org/abs/2601.19336)
*Zhao-Han Peng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.LG

TL;DR: EAWM提出事件感知世界模型框架，通过自动事件生成和分割学习事件感知表示，提升模型强化学习的泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的强化学习方法在结构相似场景间泛化能力不足，容易受到纹理、颜色等虚假变化的影响。受人类认知科学启发，人类将连续感官流分割为离散事件并依赖关键事件进行决策。

Method: 提出事件感知世界模型(EAWM)框架，包含自动事件生成器从原始观察中推导事件，引入通用事件分割器识别事件边界，通过事件预测塑造表示空间以捕捉有意义的时空转换。

Result: 在Atari 100K、Craftax 1M、DeepMind Control 500K和DMC-GB2 500K等基准测试中，EAWM将强MBRL基线的性能提升10%-45%，创造了新的最先进结果。

Conclusion: EAWM通过事件感知表示学习有效提升模型强化学习的泛化能力和样本效率，为不同世界模型架构提供了统一框架，展示了广泛适用性。

Abstract: While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.

</details>


### [13] [Time-to-Injury Forecasting in Elite Female Football: A DeepHit Survival Approach](https://arxiv.org/abs/2601.19479)
*Victoria Catterall,Cise Midoglu,Stephen Lynch*

Main category: cs.LG

TL;DR: 本研究使用DeepHit神经网络对足球运动员的受伤时间进行预测，通过生存分析模型提供个体化、随时间变化的风险估计，相比传统方法在预测性能和可解释性方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 足球运动中的伤病问题对运动员和团队具有重大影响，现有机器学习方法多依赖静态季前数据和二元结果，限制了实际应用价值。需要开发能够利用纵向监测数据、提供时间序列预测和可解释结果的新方法。

Method: 使用公开的SoccerMon数据集（包含精英女子足球运动员两个赛季的训练、比赛和健康记录），通过数据清洗、特征工程和三种插补策略进行预处理。建立基线模型（随机森林、XGBoost、逻辑回归）进行网格搜索优化，同时采用基于多层感知器的DeepHit神经网络模型，使用时间序列验证和留一球员交叉验证进行评估。

Result: DeepHit模型获得0.762的一致性指数，优于基线模型，能够提供个体化、随时间变化的风险估计。通过SHAP方法识别出与临床相关的预测因子，这些因子与已知风险因素一致，增强了模型的可解释性。

Conclusion: 本研究提供了一个新颖的概念验证：使用DeepHit进行生存建模在足球伤病预测方面显示出强大潜力，能够为不同竞技水平的伤病预防提供准确、可解释且可操作的见解。

Abstract: Injury occurrence in football poses significant challenges for athletes and teams, carrying personal, competitive, and financial consequences. While machine learning has been applied to injury prediction before, existing approaches often rely on static pre-season data and binary outcomes, limiting their real-world utility. This study investigates the feasibility of using a DeepHit neural network to forecast time-to-injury from longitudinal athlete monitoring data, while providing interpretable predictions. The analysis utilised the publicly available SoccerMon dataset, containing two seasons of training, match, and wellness records from elite female footballers. Data was pre-processed through cleaning, feature engineering, and the application of three imputation strategies. Baseline models (Random Forest, XGBoost, Logistic Regression) were optimised via grid search for benchmarking, while the DeepHit model, implemented with a multilayer perceptron backbone, was evaluated using chronological and leave-one-player-out (LOPO) validation. DeepHit achieved a concordance index of 0.762, outperforming baseline models and delivering individualised, time-varying risk estimates. Shapley Additive Explanations (SHAP) identified clinically relevant predictors consistent with established risk factors, enhancing interpretability. Overall, this study provides a novel proof of concept: survival modelling with DeepHit shows strong potential to advance injury forecasting in football, offering accurate, explainable, and actionable insights for injury prevention across competitive levels.

</details>


### [14] [To Grok Grokking: Provable Grokking in Ridge Regression](https://arxiv.org/abs/2601.19791)
*Mingyue Xu,Gal Vardi,Itay Safran*

Main category: cs.LG

TL;DR: 论文在线性回归中证明了grokking现象（过拟合后延迟泛化），给出了泛化延迟时间的定量界限，并表明通过超参数调整可以控制或消除grokking。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习中的grokking现象——模型在过拟合后很长时间才出现泛化能力。目前对这一现象的机制缺乏理论理解，特别是在简单线性模型中的定量分析。

Method: 使用带权重衰减的梯度下降训练过参数化线性回归模型，从理论上证明grokking的三个阶段，推导泛化延迟时间的定量界限，并通过实验验证理论结果在非线性神经网络中的适用性。

Result: 证明了在线性回归中确实存在grokking的三个阶段：早期过拟合、长期泛化差、最终泛化误差任意小。给出了"grokking时间"的定量界限，表明通过超参数调整可以放大或消除grokking现象。

Conclusion: grokking不是深度学习的固有缺陷，而是特定训练条件的结果，不需要改变模型架构或学习算法就能避免。理论结果在非线性网络中也得到了实证支持。

Abstract: We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persists long after overfitting has manifested; and (iii) the generalization error eventually becomes arbitrarily small. Moreover, we show, both theoretically and empirically, that grokking can be amplified or eliminated in a principled manner through proper hyperparameter tuning. To the best of our knowledge, these are the first rigorous quantitative bounds on the generalization delay (which we refer to as the "grokking time") in terms of training hyperparameters. Lastly, going beyond the linear setting, we empirically demonstrate that our quantitative bounds also capture the behavior of grokking on non-linear neural networks. Our results suggest that grokking is not an inherent failure mode of deep learning, but rather a consequence of specific training conditions, and thus does not require fundamental changes to the model architecture or learning algorithm to avoid.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge](https://arxiv.org/abs/2601.19155)
*Qiujun Li,Zijin Xiao,Xulin Wang,Zhidan Ma,Cheng Yang,Haifeng Li*

Main category: cs.AI

TL;DR: LocationAgent：一种分层定位代理，通过RER架构（推理器-执行器-记录器）进行分层推理，将地理证据验证卸载到外部工具，显著提升零样本图像地理定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像地理定位方法将位置知识和推理模式内化为静态记忆，容易在开放世界或需要动态知识的场景中出现事实幻觉和泛化瓶颈。需要一种能够进行分层推理并验证地理证据的方法。

Method: 提出LocationAgent分层定位代理：1）采用RER架构（推理器-执行器-记录器）实现分层推理，通过角色分离和上下文压缩防止多步推理漂移；2）构建线索探索工具套件进行证据验证；3）创建CCL-Bench中文城市定位基准数据集。

Result: LocationAgent在零样本设置下显著优于现有方法，性能提升至少30%。

Conclusion: 通过将分层推理逻辑保留在模型中，同时将地理证据验证卸载到外部工具，LocationAgent能够有效解决现有方法的幻觉和泛化问题，在图像地理定位任务中取得了显著改进。

Abstract: Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.

</details>


### [16] [Benchmarks Saturate When The Model Gets Smarter Than The Judge](https://arxiv.org/abs/2601.19532)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.AI

TL;DR: Omni-MATH-2是一个经过人工修订的数学基准数据集，包含4181个精确答案问题和247个非标准标记问题，通过审计确保可编译性、可解性和可验证性，显著减少了数据集引起的噪声，为模型性能评估提供了更精确的工具。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型基准测试存在数据集不准确和评估方法不可靠的问题，这些问题削弱了基准测试的有效性。作者旨在创建一个更干净、更可靠的数学基准数据集，以更精确地评估模型性能。

Method: 创建Omni-MATH-2数据集，包含4181个精确答案问题和247个非标准标记问题。对每个问题进行人工审计，确保LaTeX可编译性、可解性和可验证性，包括添加缺失图表或信息、标记需要证明/估计/图像的问题、去除冗余内容。使用该数据集评估评估者引起的噪声，比较GPT-5 mini和原始Omni-Judge的表现。

Result: 数据集修订显著减少了数据集引起的噪声。评估者比较显示，在评估者分歧中，Omni-Judge在96.4%的情况下是错误的，表明其无法区分模型能力差异。随着问题难度增加，需要更胜任的评估者来防止评估错误掩盖模型间的真实差异。两种评估者都无法识别标记问题子集的当前失败模式。

Conclusion: 数据集质量和评估者可靠性对于开发准确的模型性能基准都至关重要。仅靠改进数据集不足以确保可靠的评估，需要同时关注数据集质量和评估者能力，特别是在问题变得更加复杂时。

Abstract: Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.

</details>


### [17] [GAVEL: Towards rule-based safety through activation monitoring](https://arxiv.org/abs/2601.19768)
*Shir Rozenfeld,Rahul Pankajakshan,Itay Zloczower,Eyal Lenga,Gilad Gressel,Yisroel Mirsky*

Main category: cs.AI

TL;DR: 提出基于规则的激活安全新范式，将LLM激活建模为可组合的认知元素，通过谓词规则实时检测违规行为，实现高精度、可定制、可解释的安全监控。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的安全方法存在精度低、灵活性差、缺乏可解释性等问题，需要一种更精确、可定制且透明的方法来检测LLM的潜在有害行为。

Method: 提出规则式激活安全框架：1) 将激活建模为认知元素（CEs）——细粒度可解释因子；2) 在CEs上定义谓词规则；3) 实时检测规则违规；4) 提供自动化规则创建工具GAVEL。

Result: 组合式规则激活安全提高了检测精度，支持领域定制，为可扩展、可解释、可审计的AI治理奠定了基础。将开源GAVEL框架和自动化规则创建工具。

Conclusion: 基于规则的激活安全范式通过认知元素和谓词规则，实现了更精确、可定制、可解释的LLM安全监控，为AI治理提供了实用框架。

Abstract: Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [18] [MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2601.19290)
*Yimeng Wang,Jiaxing Zhao,Hongbin Xie,Hexing Ma,Yuzhen Lei,Shuangxue Liu,Xuan Song,Zichen Zhang,Haoran Zhang*

Main category: cs.CL

TL;DR: MetaGen是一个无需训练的多智能体框架，能够在推理时动态调整角色空间和协作拓扑结构，提高任务匹配度和适应性，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体系统通常采用固定的角色库和冻结的交互拓扑结构，这种刚性设计导致任务不匹配、无法及时适应推理过程中出现的新证据，并且增加了推理成本。

Method: MetaGen在推理时无需更新基础模型权重，通过生成和重写查询条件化的角色规范来维护可控的动态角色池，然后围绕最小骨干网络实例化约束执行图。在执行过程中，使用轻量级反馈信号迭代更新角色提示并调整结构决策。

Result: 在代码生成和多步推理基准测试中，MetaGen相比强大的多智能体基线方法，在准确性和成本权衡方面都有所改进。

Conclusion: MetaGen通过动态调整角色空间和协作拓扑结构，解决了现有多智能体系统的刚性设计问题，实现了更好的任务适应性和成本效益。

Abstract: Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.

</details>


### [19] [Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition](https://arxiv.org/abs/2601.19451)
*Isha Pandey,Ashish Mittal,Vartul Bahuguna,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 该论文提出SMEAR-MoE，一种稳定的混合专家投影器，用于解决多语言ASR中单投影器难以捕捉多样化声学-语义映射的问题，在四种印度语言上实现了显著的WER降低。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的ASR系统在单语场景中表现良好，但单个投影器难以捕捉多语言ASR所需的多样化声学到语义映射，限制了其在多语言环境中的性能。

Method: 提出SMEAR-MoE（稳定的混合专家投影器），确保所有专家都能获得密集梯度流，防止专家崩溃，同时支持跨语言共享。系统比较了单体、静态多投影器和动态MoE设计。

Result: 在四种印度语言（印地语、马拉地语、泰米尔语、泰卢固语）上，SMEAR-MoE相比单投影器基线实现了高达7.6%的相对WER降低，同时保持相当的运行时效率。专家路由分析显示语言相关的专家共享模式。

Conclusion: 稳定的多专家投影器是实现可扩展和鲁棒多语言ASR的关键，SMEAR-MoE通过防止专家崩溃和促进跨语言共享，在多语言ASR任务中表现出色。

Abstract: Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.

</details>


### [20] [Decompose-and-Formalise: Recursively Verifiable Natural Language Inference](https://arxiv.org/abs/2601.19605)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Main category: cs.CL

TL;DR: 提出分解与形式化框架，通过将前提-假设对分解为蕴含树，自底向上验证以隔离失败节点，并进行局部诊断引导的细化，显著提高解释验证率并减少迭代次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 当前将大语言模型与定理证明器结合的神经符号方法在自然语言推理中存在挑战：长而复杂的输入和多步推理放大了形式化错误，单个局部不匹配就可能导致证明无效。现有方法通常通过昂贵的全局重新生成来处理失败，难以从证明器诊断中定位责任范围或步骤。

Method: 提出分解与形式化框架：(1) 将前提-假设对分解为原子步骤的蕴含树；(2) 自底向上验证树以将失败隔离到特定节点；(3) 执行局部诊断引导的细化而非重新生成整个解释。此外，引入基于事件的逻辑形式中的θ-替换，以强制执行一致的论元角色绑定，提高形式化的忠实度。

Result: 在五个LLM骨干上进行的一系列推理任务中，该方法实现了最高的解释验证率，比最先进方法提高了26.2%、21.7%、21.6%和48.9%，同时减少了细化迭代次数和运行时间，并保持了强大的NLI准确性。

Conclusion: 分解与形式化框架通过结构化分解、局部化失败和诊断引导的细化，有效解决了自然语言推理中形式化错误和失败处理的问题，显著提高了验证效率并减少了计算成本。

Abstract: Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $θ$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.

</details>


### [21] [One Token Is Enough: Improving Diffusion Language Models with a Sink Token](https://arxiv.org/abs/2601.19657)
*Zihou Zhang,Zheyong Xie,Li Zhong,Haifeng Liu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文提出在扩散语言模型中引入一个额外的sink token来解决moving sink现象，通过修改注意力掩码使该token仅关注自身但对其他token全局可见，从而稳定注意力机制并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然具有并行文本生成的优点，但存在moving sink现象的不稳定性问题。sink token在Transformer值空间中具有低范数表示，moving sink现象是防止信息过度混合的保护机制，但其在扩散步骤中不可预测的位置会损害推理鲁棒性。

Method: 提出一个简单但有效的额外sink token，通过修改注意力掩码实现。具体来说，引入一个特殊token，该token仅能关注自身，但对所有其他token全局可见。这种方法稳定了注意力sink，改善了模型性能。

Result: 实验结果表明，引入单个额外token能稳定注意力sink，显著提升模型性能。进一步分析证实该token的有效性与其位置无关，且语义内容可忽略，验证了其作为稳健专用结构sink的作用。

Conclusion: 通过引入一个专门设计的额外sink token，成功解决了扩散语言模型中的moving sink不稳定性问题，该方法简单有效，为扩散语言模型的稳定推理提供了可靠解决方案。

Abstract: Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [22] [Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing](https://arxiv.org/abs/2601.19079)
*Naqash Afzal,Niklas Funk,Erik Helmut,Jan Peters,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 提出基于神经形态事件触觉传感器Evetac的连续盲文识别系统，通过时空分割和轻量级ResNet分类器处理稀疏事件流，实现高精度实时识别。


<details>
  <summary>Details</summary>
Motivation: 传统盲文阅读器采用离散逐字符扫描，限制阅读速度且破坏自然流程；基于视觉的方法计算量大、延迟高且在真实环境下性能下降。

Method: 使用开源神经形态事件触觉传感器Evetac，结合时空分割和轻量级ResNet分类器处理连续滑动中的动态接触事件，模拟人类手指扫描策略。

Result: 在标准深度下达到接近完美精度（≥98%），跨多种盲文板布局泛化能力强，快速扫描下保持良好性能，在实际盲文板上单词级准确率超过90%。

Conclusion: 神经形态触觉感知为机器人盲文阅读提供了可扩展、低延迟的解决方案，对辅助技术和机器人应用中的触觉感知具有更广泛意义。

Abstract: Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.

</details>
