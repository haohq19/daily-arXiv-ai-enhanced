{"id": "2509.19552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19552", "abs": "https://arxiv.org/abs/2509.19552", "authors": ["Manyi Yao", "Bingbing Zhuang", "Sparsh Garg", "Amit Roy-Chowdhury", "Christian Shelton", "Manmohan Chandraker", "Abhishek Aich"], "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning", "comment": null, "summary": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.", "AI": {"tldr": "iFinder\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u8bed\u4e49\u57fa\u7840\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u8f6c\u6362\u4e3a\u5206\u5c42\u53ef\u89e3\u91ca\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5c06\u611f\u77e5\u4e0e\u63a8\u7406\u89e3\u8026\uff0c\u4ece\u800c\u5728\u96f6\u6837\u672c\u9a7e\u9a76\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u5206\u6790\uff09\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u5f52\u7eb3\u504f\u7f6e\uff0c\u73b0\u6709\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u56e0\u679c\u63a8\u7406\u548c\u4e8b\u4ef6\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "iFinder\u91c7\u7528\u6a21\u5757\u5316\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6d41\u7a0b\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u5173\u952e\u7ebf\u7d22\uff08\u7269\u4f53\u59ff\u6001\u3001\u8f66\u9053\u4f4d\u7f6e\u3001\u7269\u4f53\u8f68\u8ff9\uff09\uff0c\u5e76\u5206\u5c42\u7ec4\u7ec7\u6210\u5e27\u7ea7\u548c\u89c6\u9891\u7ea7\u7ed3\u6784\uff0c\u7ed3\u5408\u4e09\u5757\u63d0\u793a\u7b56\u7565\u5b9e\u73b0\u9010\u6b65\u63a8\u7406\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ciFinder\u5728\u96f6\u6837\u672c\u9a7e\u9a76\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e8b\u6545\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe39%\u3002", "conclusion": "\u901a\u8fc7\u5c06LLMs\u4e0e\u9a7e\u9a76\u9886\u57df\u7279\u5b9a\u8868\u793a\u76f8\u7ed3\u5408\uff0ciFinder\u4e3a\u96f6\u6837\u672c\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u66ff\u4ee3\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6848\u3002"}}
{"id": "2509.19925", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19925", "abs": "https://arxiv.org/abs/2509.19925", "authors": ["Ajeet Kumar Singh", "Rajsabi Surya", "Anurag Tripathi", "Santanu Choudhury", "Sudhir Bisane"], "title": "CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain", "comment": null, "summary": "As enterprises increasingly integrate cloud-based large language models\n(LLMs) such as ChatGPT and Gemini into their legal document workflows,\nprotecting sensitive contractual information - including Personally\nIdentifiable Information (PII) and commercially sensitive clauses - has emerged\nas a critical challenge. In this work, we propose CON-QA, a hybrid\nprivacy-preserving framework designed specifically for secure question\nanswering over enterprise contracts, effectively combining local and\ncloud-hosted LLMs. The CON-QA framework operates through three stages: (i)\nsemantic query decomposition and query-aware document chunk retrieval using a\nlocally deployed LLM analysis, (ii) anonymization of detected sensitive\nentities via a structured one-to-many mapping scheme, ensuring semantic\ncoherence while preventing cross-session entity inference attacks, and (iii)\nanonymized response generation by a cloud-based LLM, with accurate\nreconstruction of the original answer locally using a session-consistent\nmany-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce\nCUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world\nCUAD contract documents, encompassing simple, complex, and summarization-style\nqueries. Empirical evaluations, complemented by detailed human assessments,\nconfirm that CON-QA effectively maintains both privacy and utility, preserves\nanswer quality, maintains fidelity to legal clause semantics, and significantly\nmitigates privacy risks, demonstrating its practical suitability for secure,\nenterprise-level contract documents.", "AI": {"tldr": "CON-QA\u662f\u4e00\u4e2a\u6df7\u5408\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u4f01\u4e1a\u5408\u540c\u7684\u5b89\u5168\u95ee\u7b54\uff0c\u7ed3\u5408\u672c\u5730\u548c\u4e91\u7aefLLM\u6765\u4fdd\u62a4\u654f\u611f\u4fe1\u606f", "motivation": "\u4f01\u4e1a\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u4e91\u7aefLLM\u96c6\u6210\u5230\u6cd5\u5f8b\u6587\u6863\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4fdd\u62a4\u654f\u611f\u5408\u540c\u4fe1\u606f\uff08\u5305\u62ec\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u548c\u5546\u4e1a\u654f\u611f\u6761\u6b3e\uff09\u5df2\u6210\u4e3a\u5173\u952e\u6311\u6218", "method": "CON-QA\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u8fd0\u4f5c\uff1a\u8bed\u4e49\u67e5\u8be2\u5206\u89e3\u548c\u67e5\u8be2\u611f\u77e5\u6587\u6863\u5757\u68c0\u7d22\u3001\u654f\u611f\u5b9e\u4f53\u533f\u540d\u5316\u3001\u533f\u540d\u5316\u54cd\u5e94\u751f\u6210\u548c\u539f\u59cb\u7b54\u6848\u91cd\u5efa", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u548c\u8be6\u7ec6\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9eCON-QA\u6709\u6548\u7ef4\u62a4\u9690\u79c1\u548c\u5b9e\u7528\u6027\uff0c\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\uff0c\u7ef4\u62a4\u6cd5\u5f8b\u6761\u6b3e\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u9690\u79c1\u98ce\u9669", "conclusion": "CON-QA\u5c55\u793a\u4e86\u5176\u5728\u5b89\u5168\u4f01\u4e1a\u7ea7\u5408\u540c\u6587\u6863\u4e2d\u7684\u5b9e\u9645\u9002\u7528\u6027"}}
{"id": "2509.19465", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.19465", "abs": "https://arxiv.org/abs/2509.19465", "authors": ["Kin G. Olivares", "Malcolm Wolff", "Tatiana Konstantinova", "Shankar Ramasubramanian", "Andrew Gordon Wilson", "Andres Potapczynski", "Willa Potosnak", "Mengfei Cao", "Boris Oreshkin", "Dmitry Efimov"], "title": "A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models", "comment": "Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems {NeurIPS 2025}. Recent Advances in Time Series Foundation Models Have\n  We Reached the 'BERT Moment'?", "summary": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework\nfor curating large-scale time series datasets to pre-train foundation\nforecasting models (FFMs). Although CFTL has shown promise, current\nbenchmarking practices fall short of accurately assessing its performance. This\nshortcoming stems from many factors: an over-reliance on small-scale evaluation\ndatasets; inadequate treatment of sample size when computing summary\nstatistics; reporting of suboptimal statistical models; and failing to account\nfor non-negligible risks of overlap between pre-training and test datasets. To\naddress these limitations, we introduce a unified reimplementation of\nwidely-adopted neural forecasting networks, adapting them for the CFTL setup;\nwe pre-train only on proprietary and synthetic data, being careful to prevent\ntest leakage; and we evaluate on 15 large, diverse public forecast competition\ndatasets. Our empirical analysis reveals that statistical models' accuracy is\nfrequently underreported. Notably, we confirm that statistical models and their\nensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and\nby more than 20% MASE, across datasets. However, we also find that synthetic\ndataset pre-training does improve the accuracy of a FFM by 7% percent.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8de8\u9891\u8fc1\u79fb\u5b66\u4e60\uff08CFTL\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u57fa\u51c6\u8bc4\u4f30\u95ee\u9898\uff0c\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u591a\u4e2a\u7f3a\u9677\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc1\u660e\u7edf\u8ba1\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7840\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u5f53\u524dCFTL\u57fa\u51c6\u8bc4\u4f30\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u5c0f\u89c4\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\u3001\u6837\u672c\u91cf\u5904\u7406\u4e0d\u5f53\u3001\u62a5\u544a\u6b21\u4f18\u7edf\u8ba1\u6a21\u578b\u3001\u672a\u80fd\u8003\u8651\u9884\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u91cd\u53e0\u7684\u98ce\u9669\u3002\u8fd9\u4e9b\u7f3a\u9677\u5bfc\u81f4\u5bf9CFTL\u6027\u80fd\u7684\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u91cd\u65b0\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u91c7\u7528\u7684\u795e\u7ecf\u9884\u6d4b\u7f51\u7edc\u5e76\u9002\u914dCFTL\u8bbe\u7f6e\uff1b\u4ec5\u5728\u4e13\u6709\u548c\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u907f\u514d\u6d4b\u8bd5\u6cc4\u6f0f\uff1b\u572815\u4e2a\u5927\u578b\u591a\u6837\u7684\u516c\u5171\u9884\u6d4b\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7edf\u8ba1\u6a21\u578b\u7684\u51c6\u786e\u6027\u7ecf\u5e38\u88ab\u4f4e\u4f30\u3002\u7edf\u8ba1\u6a21\u578b\u53ca\u5176\u96c6\u6210\u5728sCRPS\u4e0a\u6bd4\u73b0\u6709FFMs\u9ad8\u51fa8.2%\u4ee5\u4e0a\uff0c\u5728MASE\u4e0a\u9ad8\u51fa20%\u4ee5\u4e0a\u3002\u4f46\u5408\u6210\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u786e\u5b9e\u80fd\u5c06FFM\u7684\u51c6\u786e\u6027\u63d0\u9ad87%\u3002", "conclusion": "\u5f53\u524dCFTL\u57fa\u51c6\u8bc4\u4f30\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u7edf\u8ba1\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u795e\u7ecf\u57fa\u7840\u9884\u6d4b\u6a21\u578b\u3002\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u5bf9FFMs\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u6574\u4f53\u4e0a\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u4ecd\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2509.19719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19719", "abs": "https://arxiv.org/abs/2509.19719", "authors": ["Bo Yu", "Jianhua Yang", "Zetao Du", "Yan Huang", "Chenglong Li", "Liang Wang"], "title": "Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation", "comment": "Accepted by MICCAI 2025", "summary": "Automatically segmenting infected areas in radiological images is essential\nfor diagnosing pulmonary infectious diseases. Recent studies have demonstrated\nthat the accuracy of the medical image segmentation can be improved by\nincorporating clinical text reports as semantic guidance. However, the complex\nmorphological changes of lesions and the inherent semantic gap between\nvision-language modalities prevent existing methods from effectively enhancing\nthe representation of visual features and eliminating semantically irrelevant\ninformation, ultimately resulting in suboptimal segmentation performance. To\naddress these problems, we propose a Frequency-domain Multi-modal Interaction\nmodel (FMISeg) for language-guided medical image segmentation. FMISeg is a late\nfusion model that establishes interaction between linguistic features and\nfrequency-domain visual features in the decoder. Specifically, to enhance the\nvisual representation, our method introduces a Frequency-domain Feature\nBidirectional Interaction (FFBI) module to effectively fuse frequency-domain\nfeatures. Furthermore, a Language-guided Frequency-domain Feature Interaction\n(LFFI) module is incorporated within the decoder to suppress semantically\nirrelevant visual features under the guidance of linguistic information.\nExperiments on QaTa-COV19 and MosMedData+ demonstrated that our method\noutperforms the state-of-the-art methods qualitatively and quantitatively.", "AI": {"tldr": "\u63d0\u51faFMISeg\u6a21\u578b\uff0c\u901a\u8fc7\u9891\u57df\u591a\u6a21\u6001\u4ea4\u4e92\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5728\u80ba\u90e8\u611f\u67d3\u533a\u57df\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\uff0c\u65e0\u6cd5\u5145\u5206\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\u8868\u793a\u548c\u6d88\u9664\u8bed\u4e49\u65e0\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0d\u7406\u60f3", "method": "FMISeg\u662f\u4e00\u79cd\u540e\u878d\u5408\u6a21\u578b\uff0c\u5728\u89e3\u7801\u5668\u4e2d\u5efa\u7acb\u8bed\u8a00\u7279\u5f81\u4e0e\u9891\u57df\u89c6\u89c9\u7279\u5f81\u7684\u4ea4\u4e92\uff0c\u5305\u542b\u9891\u57df\u7279\u5f81\u53cc\u5411\u4ea4\u4e92\u6a21\u5757\u548c\u8bed\u8a00\u5f15\u5bfc\u9891\u57df\u7279\u5f81\u4ea4\u4e92\u6a21\u5757", "result": "\u5728QaTa-COV19\u548cMosMedData+\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5", "conclusion": "FMISeg\u901a\u8fc7\u9891\u57df\u591a\u6a21\u6001\u4ea4\u4e92\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u5f15\u5bfc\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd"}}
{"id": "2509.19360", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19360", "abs": "https://arxiv.org/abs/2509.19360", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Semantic Representation Attack against Aligned Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) increasingly employ alignment techniques to\nprevent harmful outputs. Despite these safeguards, attackers can circumvent\nthem by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure,\nhere is...'', suffering from limited convergence, unnatural prompts, and high\ncomputational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that\nfundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the\nsemantic representation space comprising diverse responses with equivalent\nharmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and\nprompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to\nefficiently generate semantically coherent and concise adversarial prompts by\nmaintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and\ndemonstrate that our method achieves unprecedented attack success rates\n(89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while\nmaintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our\nSemantic Representation Attack.\n  The code will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u8868\u793a\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u8868\u793a\u7a7a\u95f4\u800c\u975e\u7cbe\u786e\u6587\u672c\u6a21\u5f0f\u6765\u7ed5\u8fc7LLM\u7684\u5bf9\u9f50\u9632\u62a4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u653b\u51fb\u6548\u679c\u548c\u63d0\u793a\u81ea\u7136\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9\u5bf9\u9f50LLM\u7684\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7cbe\u786e\u7684\u80af\u5b9a\u54cd\u5e94\u6a21\u5f0f\uff0c\u5b58\u5728\u6536\u655b\u6709\u9650\u3001\u63d0\u793a\u4e0d\u81ea\u7136\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u8303\u5f0f\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u8bed\u4e49\u8868\u793a\u653b\u51fb\u8303\u5f0f\uff0c\u5229\u7528\u5305\u542b\u7b49\u6548\u6709\u5bb3\u542b\u4e49\u7684\u591a\u6837\u5316\u54cd\u5e94\u7684\u8bed\u4e49\u8868\u793a\u7a7a\u95f4\u3002\u5f00\u53d1\u4e86\u8bed\u4e49\u8868\u793a\u542f\u53d1\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u589e\u91cf\u6269\u5c55\u6765\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u4e14\u7b80\u6d01\u7684\u5bf9\u6297\u63d0\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u572818\u4e2aLLM\u4e0a\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u653b\u51fb\u6210\u529f\u7387\uff08\u5e73\u574789.41%\uff0c\u5176\u4e2d11\u4e2a\u6a21\u578b\u8fbe\u5230100%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9690\u853d\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8bed\u4e49\u8868\u793a\u653b\u51fb\u5728\u653b\u51fb\u6548\u679c\u3001\u63d0\u793a\u81ea\u7136\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u6574\u4f53\u4f18\u52bf\uff0c\u4e3a\u5bf9\u6297\u5bf9\u9f50LLM\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.19526", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19526", "abs": "https://arxiv.org/abs/2509.19526", "authors": ["Ali Baheri", "Lars Lindemann"], "title": "Metriplectic Conditional Flow Matching for Dissipative Dynamics", "comment": null, "summary": "Metriplectic conditional flow matching (MCFM) learns dissipative dynamics\nwithout violating first principles. Neural surrogates often inject energy and\ndestabilize long-horizon rollouts; MCFM instead builds the\nconservative-dissipative split into both the vector field and a structure\npreserving sampler. MCFM trains via conditional flow matching on short\ntransitions, avoiding long rollout adjoints. In inference, a Strang-prox scheme\nalternates a symplectic update with a proximal metric step, ensuring discrete\nenergy decay; an optional projection enforces strict decay when a trusted\nenergy is available. We provide continuous and discrete time guarantees linking\nthis parameterization and sampler to conservation, monotonic dissipation, and\nstable rollouts. On a controlled mechanical benchmark, MCFM yields phase\nportraits closer to ground truth and markedly fewer energy-increase and\npositive energy rate events than an equally expressive unconstrained neural\nflow, while matching terminal distributional fit.", "AI": {"tldr": "MCFM\u662f\u4e00\u79cd\u5b66\u4e60\u8017\u6563\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4fdd\u5b88-\u8017\u6563\u5206\u88c2\u6784\u5efa\u5230\u5411\u91cf\u573a\u548c\u7ed3\u6784\u4fdd\u6301\u91c7\u6837\u5668\u4e2d\uff0c\u786e\u4fdd\u4e0d\u8fdd\u53cd\u7b2c\u4e00\u539f\u7406\uff0c\u4ece\u800c\u5728\u957f\u671f\u63a8\u6f14\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u5728\u957f\u671f\u63a8\u6f14\u4e2d\u5e38\u5e38\u6ce8\u5165\u80fd\u91cf\u5bfc\u81f4\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u7269\u7406\u7b2c\u4e00\u539f\u7406\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u8017\u6563\u52a8\u529b\u5b66\u3002", "method": "MCFM\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u5728\u77ed\u65f6\u8f6c\u6362\u4e0a\u8bad\u7ec3\uff0c\u907f\u514d\u957f\u63a8\u6f14\u4f34\u968f\u8ba1\u7b97\u3002\u5728\u63a8\u7406\u65f6\u4f7f\u7528Strang-prox\u65b9\u6848\u4ea4\u66ff\u8fdb\u884c\u8f9b\u66f4\u65b0\u548c\u8fd1\u7aef\u5ea6\u91cf\u6b65\u9aa4\uff0c\u786e\u4fdd\u79bb\u6563\u80fd\u91cf\u8870\u51cf\u3002", "result": "\u5728\u53d7\u63a7\u673a\u68b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCFM\u4ea7\u751f\u7684\u76f8\u56fe\u66f4\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\uff0c\u80fd\u91cf\u589e\u52a0\u548c\u6b63\u80fd\u91cf\u7387\u4e8b\u4ef6\u663e\u8457\u5c11\u4e8e\u540c\u7b49\u8868\u8fbe\u80fd\u529b\u7684\u65e0\u7ea6\u675f\u795e\u7ecf\u6d41\uff0c\u540c\u65f6\u5339\u914d\u7ec8\u7aef\u5206\u5e03\u62df\u5408\u3002", "conclusion": "MCFM\u63d0\u4f9b\u4e86\u8fde\u7eed\u548c\u79bb\u6563\u65f6\u95f4\u4fdd\u8bc1\uff0c\u5c06\u53c2\u6570\u5316\u548c\u91c7\u6837\u5668\u4e0e\u5b88\u6052\u3001\u5355\u8c03\u8017\u6563\u548c\u7a33\u5b9a\u63a8\u6f14\u8054\u7cfb\u8d77\u6765\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u7ed3\u6784\u4fdd\u6301\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.19661", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19661", "abs": "https://arxiv.org/abs/2509.19661", "authors": ["Puning Zhao", "Zhikun Zhang", "Bo Sun", "Li Shen", "Liang Zhang", "Shaowei Wang", "Zhe Liu"], "title": "Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion", "comment": null, "summary": "Distribution estimation under local differential privacy (LDP) is a\nfundamental and challenging task. Significant progresses have been made on\ncategorical data. However, due to different evaluation metrics, these methods\ndo not work well when transferred to numerical data. In particular, we need to\nprevent the probability mass from being misplaced far away. In this paper, we\npropose a new approach that express the sample distribution using wavelet\nexpansions. The coefficients of wavelet series are estimated under LDP. Our\nmethod prioritizes the estimation of low-order coefficients, in order to ensure\naccurate estimation at macroscopic level. Therefore, the probability mass is\nprevented from being misplaced too far away from its ground truth. We establish\ntheoretical guarantees for our methods. Experiments show that our wavelet\nexpansion method significantly outperforms existing solutions under Wasserstein\nand KS distances.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u5c55\u5f00\u7684\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u5206\u5e03\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5148\u4f30\u8ba1\u4f4e\u9636\u5c0f\u6ce2\u7cfb\u6570\u6765\u9632\u6b62\u6982\u7387\u8d28\u91cf\u88ab\u9519\u8bef\u5206\u914d\u5230\u8fdc\u79bb\u771f\u5b9e\u503c\u7684\u4f4d\u7f6e\u3002", "motivation": "\u73b0\u6709\u7684\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u5206\u5e03\u4f30\u8ba1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u6570\u636e\uff0c\u5728\u6570\u503c\u6570\u636e\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u6709\u6548\u9632\u6b62\u6982\u7387\u8d28\u91cf\u88ab\u9519\u8bef\u5206\u914d\u5230\u8fdc\u79bb\u771f\u5b9e\u503c\u7684\u4f4d\u7f6e\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u5c55\u5f00\u8868\u793a\u6837\u672c\u5206\u5e03\uff0c\u5728\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u4f30\u8ba1\u5c0f\u6ce2\u7cfb\u6570\uff0c\u4f18\u5148\u4f30\u8ba1\u4f4e\u9636\u7cfb\u6570\u4ee5\u786e\u4fdd\u5b8f\u89c2\u5c42\u9762\u7684\u51c6\u786e\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u5c0f\u6ce2\u5c55\u5f00\u65b9\u6cd5\u5728Wasserstein\u8ddd\u79bb\u548cKS\u8ddd\u79bb\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5c0f\u6ce2\u5c55\u5f00\u65b9\u6cd5\u4e3a\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u6570\u503c\u6570\u636e\u5206\u5e03\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u6301\u5206\u5e03\u7684\u5b8f\u89c2\u7ed3\u6784\u7279\u5f81\u3002"}}
{"id": "2509.19640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19640", "abs": "https://arxiv.org/abs/2509.19640", "authors": ["Ryan Shea", "Zhou Yu"], "title": "AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification", "comment": "EMNLP Findings 2025", "summary": "Patents play a critical role in driving technological innovation by granting\ninventors exclusive rights to their inventions. However the process of drafting\na patent application is often expensive and time-consuming, making it a prime\ncandidate for automation. Despite recent advancements in language models,\nseveral challenges hinder the development of robust automated patent drafting\nsystems. First, the information within a patent application is highly\nconfidential, which often prevents the use of closed-source LLMs for automating\nthis task. Second, the process of drafting a patent application is difficult\nfor even the most advanced language models due to their long context, technical\nwriting style, and specialized domain knowledge. To address these challenges,\nwe introduce AutoSpec, a secure, agentic framework for Automatically drafting\npatent Specification. Our approach decomposes the drafting process into a\nsequence of manageable subtasks, each solvable by smaller, open-source language\nmodels enhanced with custom tools tailored for drafting patent specification.\nTo assess our system, we design a novel evaluation protocol in collaboration\nwith experienced patent attorneys. Our automatic and expert evaluations show\nthat AutoSpec outperforms existing baselines on a patent drafting task.", "AI": {"tldr": "AutoSpec\u662f\u4e00\u4e2a\u5b89\u5168\u3001\u667a\u80fd\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8d77\u8349\u4e13\u5229\u8bf4\u660e\u4e66\uff0c\u901a\u8fc7\u5c06\u8d77\u8349\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u548c\u5b9a\u5236\u5de5\u5177\u6765\u89e3\u51b3\u4e13\u5229\u8d77\u8349\u7684\u6311\u6218\u3002", "motivation": "\u4e13\u5229\u8d77\u8349\u8fc7\u7a0b\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u4f46\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4fe1\u606f\u4fdd\u5bc6\u6027\u3001\u957f\u6587\u672c\u5904\u7406\u548c\u6280\u672f\u5199\u4f5c\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5b89\u5168\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4e13\u5229\u8d77\u8349\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5e8f\u5217\u5316\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u7531\u589e\u5f3a\u5b9a\u5236\u5de5\u5177\u7684\u5f00\u6e90\u5c0f\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u548c\u4e13\u4e1a\u6027\u3002", "result": "\u901a\u8fc7\u4e0e\u4e13\u5229\u5f8b\u5e08\u5408\u4f5c\u8bbe\u8ba1\u7684\u65b0\u578b\u8bc4\u4f30\u534f\u8bae\uff0c\u81ea\u52a8\u548c\u4e13\u5bb6\u8bc4\u4f30\u663e\u793aAutoSpec\u5728\u4e13\u5229\u8d77\u8349\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AutoSpec\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4e13\u5229\u8d77\u8349\u81ea\u52a8\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b89\u5168\u9ad8\u6548\u7684\u4e13\u5229\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.19750", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19750", "abs": "https://arxiv.org/abs/2509.19750", "authors": ["Kainat"], "title": "Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods", "comment": "MS Thesis", "summary": "This research presents a novel method for noninvasive arterial blood pressure\nABP prediction using speech signals employing a BERT based regression model\nArterial blood pressure is a vital indicator of cardiovascular health and\naccurate monitoring is essential in preventing hypertension related\ncomplications Traditional cuff based methods often yield inconsistent results\ndue to factors like whitecoat and masked hypertension Our approach leverages\nthe acoustic characteristics of speech capturing voice features to establish\ncorrelations with blood pressure levels Utilizing advanced deep learning\ntechniques we analyze speech signals to extract relevant patterns enabling real\ntime monitoring without the discomfort of conventional methods In our study we\nemployed a dataset comprising recordings from 95 participants ensuring diverse\nrepresentation The BERT model was fine tuned on extracted features from speech\nleading to impressive performance metrics achieving a mean absolute error MAE\nof 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood\npressure DBP with R scores of 099 and 094 respectively These results indicate\nthe models robustness in accurately predicting blood pressure levels\nFurthermore the training and validation loss analysis demonstrates effective\nlearning and minimal overfitting Our findings suggest that integrating deep\nlearning with speech analysis presents a viable alternative for blood pressure\nmonitoring paving the way for improved applications in telemedicine and remote\nhealth monitoring By providing a user friendly and accurate method for blood\npressure assessment this research has significant implications for enhancing\npatient care and proactive management of cardiovascular health", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u56de\u5f52\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u97f3\u4fe1\u53f7\u65e0\u521b\u9884\u6d4b\u52a8\u8109\u8840\u538b\uff0c\u4e3a\u5fc3\u8840\u7ba1\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u8896\u5e26\u5f0f\u8840\u538b\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u767d\u5927\u8902\u9ad8\u8840\u538b\u548c\u9690\u533f\u6027\u9ad8\u8840\u538b\u7b49\u95ee\u9898\uff0c\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u66f4\u8212\u9002\u3001\u51c6\u786e\u7684\u5b9e\u65f6\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528BERT\u6a21\u578b\u5206\u6790\u8bed\u97f3\u4fe1\u53f7\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u5efa\u7acb\u8bed\u97f3\u7279\u5f81\u4e0e\u8840\u538b\u6c34\u5e73\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u53d6\u76f8\u5173\u6a21\u5f0f\u3002", "result": "\u572895\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff1a\u6536\u7f29\u538bMAE\u4e3a13.6 mmHg\uff08R\u00b2=0.99\uff09\uff0c\u8212\u5f20\u538bMAE\u4e3a12.4 mmHg\uff08R\u00b2=0.94\uff09\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\u5206\u6790\u663e\u793a\u6709\u6548\u5b66\u4e60\u4e14\u8fc7\u62df\u5408\u6700\u5c0f\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e0e\u8bed\u97f3\u5206\u6790\u7684\u7ed3\u5408\u4e3a\u8840\u538b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u8fdc\u7a0b\u533b\u7597\u548c\u5065\u5eb7\u76d1\u6d4b\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\uff0c\u80fd\u591f\u63d0\u5347\u60a3\u8005\u62a4\u7406\u548c\u5fc3\u8840\u7ba1\u5065\u5eb7\u7ba1\u7406\u3002"}}
{"id": "2509.19771", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19771", "abs": "https://arxiv.org/abs/2509.19771", "authors": ["Hyunwoo Kim", "Hyo Kyung Lee"], "title": "Frictional Q-Learning", "comment": null, "summary": "We draw an analogy between static friction in classical mechanics and\nextrapolation error in off-policy RL, and use it to formulate a constraint that\nprevents the policy from drifting toward unsupported actions. In this study, we\npresent Frictional Q-learning, a deep reinforcement learning algorithm for\ncontinuous control, which extends batch-constrained reinforcement learning. Our\nalgorithm constrains the agent's action space to encourage behavior similar to\nthat in the replay buffer, while maintaining a distance from the manifold of\nthe orthonormal action space. The constraint preserves the simplicity of\nbatch-constrained, and provides an intuitive physical interpretation of\nextrapolation error. Empirically, we further demonstrate that our algorithm is\nrobustly trained and achieves competitive performance across standard\ncontinuous control benchmarks.", "AI": {"tldr": "\u63d0\u51faFrictional Q-learning\u7b97\u6cd5\uff0c\u901a\u8fc7\u7c7b\u6bd4\u7ecf\u5178\u529b\u5b66\u4e2d\u7684\u9759\u6469\u64e6\u529b\u4e0e\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5916\u63a8\u8bef\u5dee\uff0c\u7ea6\u675f\u667a\u80fd\u4f53\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u9632\u6b62\u7b56\u7565\u6f02\u79fb\u5230\u4e0d\u53d7\u652f\u6301\u7684\u52a8\u4f5c\u533a\u57df\u3002", "motivation": "\u89e3\u51b3\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5916\u63a8\u8bef\u5dee\u95ee\u9898\uff0c\u9632\u6b62\u7b56\u7565\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u504f\u79bb\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u652f\u6301\u7684\u52a8\u4f5c\u5206\u5e03\u3002", "method": "\u6269\u5c55\u6279\u91cf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u7ea6\u675f\u667a\u80fd\u4f53\u52a8\u4f5c\u7a7a\u95f4\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u6b63\u4ea4\u52a8\u4f5c\u7a7a\u95f4\u6d41\u5f62\u7684\u8ddd\u79bb\uff0c\u9f13\u52b1\u884c\u4e3a\u4e0e\u56de\u653e\u7f13\u51b2\u533a\u76f8\u4f3c\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u9c81\u68d2\u7684\u8bad\u7ec3\u7279\u6027\uff0c\u5728\u6807\u51c6\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "Frictional Q-learning\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684\u7ea6\u675f\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5916\u63a8\u8bef\u5dee\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b97\u6cd5\u7684\u7b80\u6d01\u6027\u548c\u76f4\u89c2\u7684\u7269\u7406\u89e3\u91ca\u3002"}}
{"id": "2509.19774", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19774", "abs": "https://arxiv.org/abs/2509.19774", "authors": ["Xiaocheng Fang", "Jiarui Jin", "Haoyu Wang", "Che Liu", "Jieyi Cai", "Guangkun Nie", "Jun Li", "Hongyan Li", "Shenda Hong"], "title": "PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection", "comment": null, "summary": "In clinical practice, electrocardiography (ECG) remains the gold standard for\ncardiac monitoring, providing crucial insights for diagnosing a wide range of\ncardiovascular diseases (CVDs). However, its reliance on specialized equipment\nand trained personnel limits feasibility for continuous routine monitoring.\nPhotoplethysmography (PPG) offers accessible, continuous monitoring but lacks\ndefinitive electrophysiological information, preventing conclusive diagnosis.\nGenerative models present a promising approach to translate PPG into clinically\nvaluable ECG signals, yet current methods face substantial challenges,\nincluding the misalignment of physiological semantics in generative models and\nthe complexity of modeling in high-dimensional signals. To this end, we propose\nPPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent\nspace via the CardioAlign Encoder and employs latent rectified flow to generate\nECGs with high fidelity and interpretability. To the best of our knowledge,\nthis is the first study to experiment on MCMED, a newly released clinical-grade\ndataset comprising over 10 million paired PPG-ECG samples from more than\n118,000 emergency department visits with expert-labeled cardiovascular disease\nannotations. Results demonstrate the effectiveness of our method for PPG-to-ECG\ntranslation and cardiovascular disease detection. Moreover, cardiologist-led\nevaluations confirm that the synthesized ECGs achieve high fidelity and improve\ndiagnostic reliability, underscoring our method's potential for real-world\ncardiovascular screening.", "AI": {"tldr": "PPGFlowECG\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7CardioAlign\u7f16\u7801\u5668\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50PPG\u548cECG\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528\u6f5c\u5728\u6574\u6d41\u6d41\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684ECG\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86PPG\u5230ECG\u8f6c\u6362\u4e2d\u7684\u751f\u7406\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5fc3\u7535\u56fe\uff08ECG\uff09\u662f\u5fc3\u810f\u76d1\u6d4b\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u4f9d\u8d56\u4e13\u4e1a\u8bbe\u5907\u548c\u4eba\u5458\u9650\u5236\u4e86\u8fde\u7eed\u76d1\u6d4b\u7684\u53ef\u884c\u6027\u3002\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2\uff08PPG\uff09\u867d\u7136\u53ef\u8fde\u7eed\u76d1\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u7535\u751f\u7406\u4fe1\u606f\uff0c\u65e0\u6cd5\u8fdb\u884c\u786e\u5b9a\u6027\u8bca\u65ad\u3002\u751f\u6210\u6a21\u578b\u6709\u671b\u5c06PPG\u8f6c\u6362\u4e3a\u4e34\u5e8a\u6709\u4ef7\u503c\u7684ECG\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u751f\u7406\u8bed\u4e49\u4e0d\u5bf9\u9f50\u548c\u9ad8\u7ef4\u4fe1\u53f7\u5efa\u6a21\u590d\u6742\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPPGFlowECG\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528CardioAlign\u7f16\u7801\u5668\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50PPG\u548cECG\u4fe1\u53f7\uff1b2\uff09\u5e94\u7528\u6f5c\u5728\u6574\u6d41\u6d41\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684ECG\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u5728MCMED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd9\u662f\u5305\u542b\u8d85\u8fc71000\u4e07\u5bf9PPG-ECG\u6837\u672c\u7684\u65b0\u4e34\u5e8a\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728PPG\u5230ECG\u8f6c\u6362\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002\u5fc3\u810f\u75c5\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u5408\u6210\u7684ECG\u8fbe\u5230\u9ad8\u4fdd\u771f\u5ea6\u5e76\u63d0\u9ad8\u4e86\u8bca\u65ad\u53ef\u9760\u6027\u3002", "conclusion": "PPGFlowECG\u6846\u67b6\u5728\u5fc3\u8840\u7ba1\u7b5b\u67e5\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684ECG\u4fe1\u53f7\u7528\u4e8e\u4e34\u5e8a\u8bca\u65ad\u3002"}}
{"id": "2509.19877", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.19877", "abs": "https://arxiv.org/abs/2509.19877", "authors": ["Shi Yin", "Zujian Dai", "Xinyang Pan", "Lixin He"], "title": "Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials", "comment": null, "summary": "Deep learning methods for electronic-structure Hamiltonian prediction has\noffered significant computational efficiency advantages over traditional DFT\nmethods, yet the diversity of atomic types, structural patterns, and the\nhigh-dimensional complexity of Hamiltonians pose substantial challenges to the\ngeneralization performance. In this work, we contribute on both the methodology\nand dataset sides to advance universal deep learning paradigm for Hamiltonian\nprediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and\nexpressive correction method for efficient and generalizable materials\nelectronic-structure Hamiltonian prediction. First, we introduce the\nzeroth-step Hamiltonians, which can be efficiently constructed by the initial\ncharge density of DFT, as informative descriptors of neural regression model in\nthe input level and initial estimates of the target Hamiltonian in the output\nlevel, so that the regression model directly predicts the correction terms to\nthe target ground truths, thereby significantly simplifying the input-output\nmapping for learning. Second, we present a neural Transformer architecture with\nstrict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian\nprediction. Third, we propose a novel training objective to ensure the accuracy\nperformance of Hamiltonians in both real space and reciprocal space, preventing\nerror amplification and the occurrence of \"ghost states\" caused by the large\ncondition number of the overlap matrix. On the dataset side, we curate a\nhigh-quality broad-coverage large benchmark, namely Materials-HAM-SOC,\ncomprising 17,000 material structures spanning 68 elements from six rows of the\nperiodic table and explicitly incorporating SOC effects. Experimental results\non Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and\nefficiency in predicting Hamiltonians and band structures.", "AI": {"tldr": "NextHAM\u662f\u4e00\u79cd\u7528\u4e8e\u6750\u6599\u7535\u5b50\u7ed3\u6784\u54c8\u5bc6\u987f\u91cf\u9884\u6d4b\u7684\u795e\u7ecfE(3)\u5bf9\u79f0\u6027\u548c\u8868\u8fbe\u6027\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u96f6\u9636\u54c8\u5bc6\u987f\u91cf\u3001Transformer\u67b6\u6784\u548c\u65b0\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5728Materials-HAM-SOC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfDFT\u65b9\u6cd5\u8ba1\u7b97\u7535\u5b50\u7ed3\u6784\u54c8\u5bc6\u987f\u91cf\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u539f\u5b50\u7c7b\u578b\u591a\u6837\u6027\u548c\u54c8\u5bc6\u987f\u91cf\u9ad8\u7ef4\u590d\u6742\u6027\u65b9\u9762\u9762\u4e34\u6cdb\u5316\u6027\u80fd\u6311\u6218\u3002", "method": "1) \u5f15\u5165\u96f6\u9636\u54c8\u5bc6\u987f\u91cf\u4f5c\u4e3a\u795e\u7ecf\u56de\u5f52\u6a21\u578b\u7684\u8f93\u5165\u63cf\u8ff0\u7b26\u548c\u521d\u59cb\u4f30\u8ba1\uff1b2) \u8bbe\u8ba1\u5177\u6709\u4e25\u683cE(3)\u5bf9\u79f0\u6027\u7684\u795e\u7ecfTransformer\u67b6\u6784\uff1b3) \u63d0\u51fa\u65b0\u7684\u8bad\u7ec3\u76ee\u6807\u786e\u4fdd\u5b9e\u7a7a\u95f4\u548c\u5012\u7a7a\u95f4\u54c8\u5bc6\u987f\u91cf\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u5305\u542b17,000\u4e2a\u6750\u6599\u7ed3\u6784\u7684Materials-HAM-SOC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNextHAM\u5728\u54c8\u5bc6\u987f\u91cf\u548c\u80fd\u5e26\u7ed3\u6784\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "NextHAM\u901a\u8fc7\u65b9\u6cd5\u8bba\u521b\u65b0\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6784\u5efa\uff0c\u63a8\u8fdb\u4e86\u54c8\u5bc6\u987f\u91cf\u9884\u6d4b\u7684\u901a\u7528\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u7684\u6cdb\u5316\u6311\u6218\u3002"}}
{"id": "2509.20162", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20162", "abs": "https://arxiv.org/abs/2509.20162", "authors": ["Chaojun Nie", "Jun Zhou", "Guanxiang Wang", "Shisong Wud", "Zichen Wang"], "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation", "comment": null, "summary": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG.", "AI": {"tldr": "\u63d0\u51faRLAG\u65b9\u6cd5\u89e3\u51b3LLMs\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u7a00\u7f3a\u548c\u65f6\u95f4\u6ede\u540e\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u589e\u5f3a\u751f\u6210\u4e2d\u5d4c\u5165\u5173\u952e\u9886\u57df\u77e5\u8bc6", "motivation": "LLMs\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u4e13\u4e1a\u4fe1\u606f\u4ee3\u8868\u6027\u4e0d\u8db3\u4e14\u6570\u636e\u96c6\u9759\u6001\uff0c\u5bfc\u81f4\u77e5\u8bc6\u5dee\u8ddd\u3002\u73b0\u6709\u65b9\u6cd5\u5982CPT\u548cSFT\u5b58\u5728\u5c40\u9650\u6027", "method": "\u63d0\u51faRLAG\u65b9\u6cd5\uff0c\u5728\u91c7\u6837\u751f\u6210\u548c\u6a21\u578b\u4f18\u5316\u4e4b\u95f4\u8fed\u4ee3\u5faa\u73af\uff0c\u901a\u8fc7\u8ba1\u7b97\u5956\u52b1\u6765\u6709\u6548\u5d4c\u5165\u5173\u952e\u4e14\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u9886\u57df\u77e5\u8bc6\u3002\u9009\u62e9\u6700\u9ad8\u5bf9\u6570\u6982\u7387\u7684\u751f\u6210\u8f93\u51fa\u4f5c\u4e3a\u91c7\u6837\u7ed3\u679c\uff0c\u8ba1\u7b97\u4e09\u4e2a\u5b9a\u5236\u5956\u52b1\u6307\u6807\u6307\u5bfc\u4f18\u5316", "result": "\u5728\u533b\u5b66\u3001\u6cd5\u5f8b\u3001\u5929\u6587\u5b66\u548c\u65f6\u4e8b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "RLAG\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3LLMs\u5728\u9886\u57df\u5e94\u7528\u4e2d\u7684\u77e5\u8bc6\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u5408\u7406\u6027"}}
{"id": "2509.20168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20168", "abs": "https://arxiv.org/abs/2509.20168", "authors": ["Ghazal Kalhor", "Behnam Bahrak"], "title": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian", "comment": "Accepted and forthcoming at the Widening Natural Language Processing\n  Workshop (WiNLP 2025) at EMNLP 2025", "summary": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u677f\u7684\u63a2\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u6ce2\u65af\u8bed\uff0c\u5e76\u5f15\u5165\u4e86\u9886\u57df\u7279\u5b9a\u6027\u522b\u504f\u659c\u6307\u6570(DS-GSI)\u6765\u91cf\u5316\u504f\u89c1\u7a0b\u5ea6\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\u4ecd\u7f3a\u4e4f\u5145\u5206\u7814\u7a76\u3002\u4e3a\u786e\u4fdd\u591a\u8bed\u8a00LLM\u7684\u516c\u5e73\u6027\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u91cf\u5316\u8fd9\u4e9b\u8bed\u8a00\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u3002", "method": "\u91c7\u7528\u6a21\u677f\u5316\u63a2\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\uff0c\u8bc4\u4f30GPT-4o mini\u3001DeepSeek R1\u3001Gemini 2.0 Flash\u548cQwen QwQ 32B\u56db\u4e2a\u6a21\u578b\u5728\u56db\u4e2a\u8bed\u4e49\u9886\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u6ce2\u65af\u8bed\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u6ce2\u65af\u8bed\u4e2d\u7684\u6027\u522b\u504f\u89c1\u6bd4\u82f1\u8bed\u66f4\u4e25\u91cd\uff0c\u5176\u4e2d\u4f53\u80b2\u9886\u57df\u663e\u793a\u51fa\u6700\u4e25\u91cd\u7684\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5305\u5bb9\u6027NLP\u5b9e\u8df5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u8bc4\u4f30\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u504f\u89c1\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2509.20265", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20265", "abs": "https://arxiv.org/abs/2509.20265", "authors": ["\u00d6mer Veysel \u00c7a\u011fatan", "Bar\u0131\u015f Akg\u00fcn"], "title": "Failure Modes of Maximum Entropy RLHF", "comment": "26 pages, 9 figures", "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be\nderived as Maximum Entropy Reinforcement Learning with length-normalized\ntemperature, providing a theoretical foundation for this reference-free method.\nMotivated by SimPO's strong performance in offline preference optimization, we\ninvestigate whether Maximum Entropy RL can achieve similar results in online\nRLHF settings. Our experiments find that Maximum Entropy RL consistently\nexhibits overoptimization and unstable KL dynamics, even at very low learning\nrates. Unlike KL-constrained methods that maintain stable training, entropy\nregularization fails to prevent reward hacking and appears to correlate with\noveroptimization. Lastly, we discuss possible explanations for why SimPO\nsucceeds in offline settings while Maximum Entropy RL struggles in online\nscenarios. Our findings suggest that reference-free approaches may face\ndistinct challenges when applied to online or offline preference learning.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86Simple Preference Optimization (SimPO)\u53ef\u63a8\u5bfc\u4e3a\u5177\u6709\u957f\u5ea6\u5f52\u4e00\u5316\u6e29\u5ea6\u7684\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u8fd9\u79cd\u65e0\u53c2\u8003\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002\u7814\u7a76\u53d1\u73b0\u6700\u5927\u71b5RL\u5728\u5728\u7ebfRLHF\u8bbe\u7f6e\u4e2d\u4f1a\u51fa\u73b0\u8fc7\u4f18\u5316\u548c\u4e0d\u7a33\u5b9a\u7684KL\u52a8\u6001\uff0c\u800cSimPO\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u53d7SimPO\u5728\u79bb\u7ebf\u504f\u597d\u4f18\u5316\u4e2d\u5f3a\u5927\u8868\u73b0\u7684\u542f\u53d1\uff0c\u7814\u7a76\u6700\u5927\u71b5RL\u662f\u5426\u80fd\u5728\u5728\u7ebfRLHF\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u7c7b\u4f3c\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u6700\u5927\u71b5RL\u548cKL\u7ea6\u675f\u65b9\u6cd5\u5728\u5728\u7ebfRLHF\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8fc7\u4f18\u5316\u73b0\u8c61\u3002", "result": "\u6700\u5927\u71b5RL\u5728\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u6301\u7eed\u8868\u73b0\u51fa\u8fc7\u4f18\u5316\u548c\u4e0d\u7a33\u5b9a\u7684KL\u52a8\u6001\uff0c\u5373\u4f7f\u5b66\u4e60\u7387\u5f88\u4f4e\uff1b\u800cKL\u7ea6\u675f\u65b9\u6cd5\u80fd\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3\u3002\u71b5\u6b63\u5219\u5316\u65e0\u6cd5\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u4e14\u4e0e\u8fc7\u4f18\u5316\u76f8\u5173\u3002", "conclusion": "SimPO\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u6210\u529f\u800c\u6700\u5927\u71b5RL\u5728\u5728\u7ebf\u573a\u666f\u4e2d\u56f0\u96be\u7684\u539f\u56e0\u53ef\u80fd\u662f\u65e0\u53c2\u8003\u65b9\u6cd5\u5728\u5728\u7ebf\u548c\u79bb\u7ebf\u504f\u597d\u5b66\u4e60\u4e2d\u9762\u4e34\u4e0d\u540c\u7684\u6311\u6218\u3002"}}
{"id": "2509.20269", "categories": ["cs.LG", "cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.20269", "abs": "https://arxiv.org/abs/2509.20269", "authors": ["Matteo Cardoni", "Sam Leroux"], "title": "Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation", "comment": "20 pages, 4 figures", "summary": "As deep neural networks are increasingly deployed in dynamic, real-world\nenvironments, relying on a single static model is often insufficient. Changes\nin input data distributions caused by sensor drift or lighting variations\nnecessitate continual model adaptation. In this paper, we propose a hybrid\ntraining methodology that enables efficient on-device domain adaptation by\ncombining the strengths of Backpropagation and Predictive Coding. The method\nbegins with a deep neural network trained offline using Backpropagation to\nachieve high initial performance. Subsequently, Predictive Coding is employed\nfor online adaptation, allowing the model to recover accuracy lost due to\nshifts in the input data distribution. This approach leverages the robustness\nof Backpropagation for initial representation learning and the computational\nefficiency of Predictive Coding for continual learning, making it particularly\nwell-suited for resource-constrained edge devices or future neuromorphic\naccelerators. Experimental results on the MNIST and CIFAR-10 datasets\ndemonstrate that this hybrid strategy enables effective adaptation with a\nreduced computational overhead, offering a promising solution for maintaining\nmodel performance in dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cd\u5411\u4f20\u64ad\u548c\u9884\u6d4b\u7f16\u7801\u7684\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5728\u7ebf\u57df\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u8f93\u5165\u6570\u636e\u5206\u5e03\u53d8\u5316\uff08\u5982\u4f20\u611f\u5668\u6f02\u79fb\u3001\u5149\u7167\u53d8\u5316\uff09\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9700\u8981\u6301\u7eed\u6a21\u578b\u9002\u5e94\u3002", "method": "\u9996\u5148\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u79bb\u7ebf\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u83b7\u5f97\u9ad8\u521d\u59cb\u6027\u80fd\uff0c\u7136\u540e\u4f7f\u7528\u9884\u6d4b\u7f16\u7801\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\uff0c\u4ee5\u6062\u590d\u56e0\u8f93\u5165\u6570\u636e\u5206\u5e03\u53d8\u5316\u800c\u635f\u5931\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u7b56\u7565\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u6709\u6548\u7684\u9002\u5e94\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u53cd\u5411\u4f20\u64ad\u5728\u521d\u59cb\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u7f16\u7801\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
