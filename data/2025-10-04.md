<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程实现领域自适应的方法，仅在骨干网络前添加轻量级的视觉提示层，无需微调整个骨干网络，显著减少了可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个新的源-目标对微调整个骨干网络，导致训练参数和存储需求线性增长，且无法重用训练好的骨干网络参数。

Method: 在骨干网络前添加领域特定的视觉重编程层，生成视觉提示作为纹理偏置来调整输入图像的风格，通过优化域内和域间分布差异来训练这些层，骨干网络参数保持不变。

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，超越PDA方法+1.6%准确率且参数减少54%，相比全骨干微调方法参数减少98%以上。

Conclusion: VirDA提供了一种参数高效的领域自适应方法，通过视觉重编程实现领域适应，显著减少计算和存储需求，同时保持竞争性的性能。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [2] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两个创新方法，解决长视频生成中的长期依赖建模和错误累积问题，提升分钟级时间一致性和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕捉长期依赖关系，同时防止自回归解码中固有的错误累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理时的错误传播。

Result: 实现了分钟级时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同推进了自回归视频模型的实际可用性，为长视频生成提供了有效的解决方案。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [3] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: 提出了Patch-as-Decodable Token (PaDT)方法，使多模态大语言模型能直接生成文本和视觉输出，解决了现有方法依赖间接表示的限制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉任务中依赖间接表示（如将坐标生成为文本），限制了性能并阻碍了分割等密集预测任务。

Method: 引入视觉参考标记(VRTs)，从查询图像的视觉块嵌入中提取，与LLM输出文本标记交错；使用轻量级解码器将LLM输出转换为检测、分割和定位预测；动态扩展嵌入表以改善定位和区分相似对象。

Result: 在四个视觉感知和理解任务中始终达到最先进性能，甚至优于显著更大的MLLM模型。

Conclusion: PaDT提供了一种统一的范式，使MLLM能够直接生成多样化的视觉输出，在多个任务上表现出色。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [4] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 提出F2C方法，通过选择关键片段而非单帧来保持时间连贯性，并使用自适应分辨率策略在固定计算预算下处理长视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型存在"大海捞针"问题：原始视频帧产生的大量视觉标记耗尽模型上下文窗口。现有解决方案通过选择稀疏帧集来减少标记数量，但会丢弃基本的时间动态信息。

Method: 提出F2C方法：1）从孤立关键帧扩展到关键片段选择；2）采用自适应分辨率策略动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准测试上，F2C方法相比均匀采样分别提升了8.1%（Video-MME）、5.6%（LongVideoBench）和10.3%（MLVU）。

Conclusion: 保持时间连贯性在帧选择中至关重要，F2C为视频大语言模型扩展到实际视频理解应用提供了实用途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 该论文从变分互信息下界出发，推导出两种自监督表示学习训练范式SDMI和JMI，为现有方法的架构组件提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 自监督表示学习虽然取得了显著经验成功，但其基本原理仍不够清晰。现有工作往往将预测器网络、停止梯度操作等架构元素视为经验性补充，缺乏理论依据。

Method: 从变分互信息下界出发，推导出两种训练范式：自蒸馏互信息(SDMI)和联合互信息(JMI)。SDMI需要交替优化，必须使用停止梯度操作；JMI允许通过对称架构进行联合优化。

Result: 证明了预测器网络在SDMI中和统计正则化器在JMI中是互信息目标的可行替代方案。许多现有自监督学习方法都是这两种范式的具体实例或近似。

Conclusion: 该研究为不同自监督学习方法架构组件的选择提供了理论解释，超越了启发式便利性，统一了自监督表示学习的理论框架。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [6] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出了一个评估数据集对称性假设的方法，通过神经网络分类器测试量化数据集各向异性程度，发现多个基准点云数据集存在高度对齐，并证明分布对称性破缺会影响对称感知方法的性能。


<details>
  <summary>Details</summary>
Motivation: 对称感知方法（如数据增强和等变架构）假设变换后的数据点在测试分布中具有高概率或"重要性"，但这一假设缺乏有效的评估方法。

Method: 提出通过双样本神经网络分类器测试来量化数据集各向异性程度，区分原始数据集与其随机增强等价版本，并在合成和基准点云数据集上进行验证。

Result: 在多个基准点云数据集中发现了出乎意料的高度对齐，理论分析表明分布对称性破缺会阻止不变方法达到最优性能，即使底层标签确实是不变的。

Conclusion: 等变方法的有效性取决于具体数据集，理解等变性需要重新思考数据中的对称性偏差，对称感知方法的适用性需要更细致的评估。

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [7] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: 这篇综述系统性地分析了边缘人工智能(Edge AI)的发展历程、当前现状和未来方向，通过多维分类法涵盖部署位置、处理能力、应用领域和硬件类型，并探讨了核心技术、挑战和新兴机遇。


<details>
  <summary>Details</summary>
Motivation: 边缘AI通过在网络边缘设备中嵌入智能，实现近数据源的实时处理，具有改善隐私和降低延迟的优势，需要系统性地梳理该领域的发展脉络和技术现状。

Method: 采用PRISMA指南进行系统性综述，构建包含部署位置、处理能力(如TinyML和联邦学习)、应用领域和硬件类型的多维分类法，分析从早期内容分发网络到现代设备智能的演进过程。

Result: 识别了边缘AI的核心使能技术包括专用硬件加速器、优化软件和通信协议，评估了资源限制、安全、模型管理、功耗和连接性等关键挑战，并揭示了神经形态硬件、持续学习算法、边云协作和可信集成等新兴机遇。

Conclusion: 为研究人员和从业者提供了边缘AI的全面框架，强调了该技术在实现实时智能处理、保护隐私和降低延迟方面的重要价值，并指出了未来发展的关键方向。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [8] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 提出了一种名为Flock的知识图谱基础模型，通过引入概率性节点-关系等变性来克服传统确定性等变性的局限性，在零样本链接预测任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱基础模型中的确定性等变性限制了其表达能力，无法区分结构相似但语义不同的关系，这阻碍了模型在零样本链接预测中的性能。

Method: 提出概率性节点-关系等变性，通过原则性随机化在推理过程中打破对称性。Flock模型迭代采样随机游走，通过记录协议编码为序列，使用序列模型嵌入，并通过学习池化聚合节点和关系的表示。

Result: Flock完美解决了新的诊断数据集Petals（当前KGFMs失败的任务），并在54个不同领域的知识图谱上的实体和关系预测任务中取得了最先进的性能。

Conclusion: 概率性节点-关系等变性是提升知识图谱基础模型表达能力的关键，Flock模型证明了该方法在零样本链接预测任务中的有效性。

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [9] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: 该研究开发了一个预测框架，使用FDA兽医药物不良事件报告数据来分类动物用药安全结果（死亡vs恢复），结合数据预处理、机器学习模型和可解释AI方法，实现了高精度的预测。


<details>
  <summary>Details</summary>
Motivation: 保护动物福利和人类食品安全，通过预测药物不良事件来识别可能导致食品链中违规残留物的风险，支持早期检测高风险药物事件特征。

Method: 使用约128万份FDA兽医药物报告，进行数据预处理、标准化、缺失值填补和特征降维，整合药物理化性质，评估多种监督学习模型（随机森林、CatBoost、XGBoost等）和集成方法，采用欠采样和过采样处理类别不平衡问题。

Result: 集成方法和CatBoost表现最佳，达到0.95的精确度、召回率和F1分数，通过AUM伪标记方法改进了少数类检测，SHAP分析识别出与致命结果相关的生物合理性预测因子。

Conclusion: 结合严格的数据工程、先进机器学习和可解释AI，能够准确、可解释地预测兽医安全结果，支持FARAD任务，加强残留风险评估和监管决策。

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [10] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 提出了一种绕过提示防护的新攻击方法，利用提示防护与主LLM之间的资源不对称性，成功越狱多个生产模型，揭示了轻量级提示防护的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，确保AI安全和对齐至关重要。提示防护作为轻量级机制被广泛使用，但其安全性需要验证。

Method: 利用提示防护与主LLM之间的资源不对称性，编码一个轻量级防护无法解码但主模型可以解码的越狱提示。

Result: 该方法在Google Gemini、DeepSeek Chat、Grok和Mistral Le Chat等高度保护的聊天界面中持续成功越狱，同时保持响应质量。

Conclusion: 揭示了轻量级提示防护在现代LLM架构中的固有攻击面，强调需要将防御重点从阻止恶意输入转向防止恶意输出。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [11] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: 提出了MIRA方法，一种无需训练、基于推理时间的对齐方法，通过图像空间的KL代理正则化来防止奖励攻击，在保持提示一致性的同时提升奖励分数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像虽然符合文本提示，但往往无法满足用户特定的标量奖励标准（如美学评分）。现有的推理时间对齐方法存在奖励攻击问题，即模型生成高评分图像但严重偏离原始提示。

Method: MIRA方法引入图像空间的基于分数的KL代理，使用冻结的主干网络正则化采样轨迹，约束输出分布，使奖励增加而不发生分布外漂移。还提出了MIRA-DPO，将偏好优化映射到推理时间。

Result: 在SDv1.5和SDXL模型、多种奖励（美学、HPSv2、PickScore）和公共数据集上，MIRA相对于强基线实现了>60%的胜率，同时保持提示一致性。机制图显示奖励增益接近零漂移，而DNO随着计算增加而漂移。

Conclusion: MIRA提供了一种有效的推理时间对齐方法，能够防止奖励攻击，在提升奖励分数的同时保持图像与提示的一致性，且无需微调。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [12] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: 本文分析了RLHF中KL散度损失的不同实现方式，建立了统一框架证明'k1 in reward'(如PPO)是Reverse KL正则化的原则性损失，'k2 as loss'在on-policy条件下与'k1 in reward'梯度等价，而'k3 as loss'(如GRPO)只是有偏的一阶近似。


<details>
  <summary>Details</summary>
Motivation: RLHF方法使用KL散度损失来稳定训练和防止过拟合，但不同实现方式(如GRPO)可能基于数值估计原则而忽视了其作为优化损失的功能作用，需要建立统一框架来分析这些实现方式的理论基础。

Method: 建立统一框架连接两种实现风格：将数学项kn作为策略评分函数的分离系数('kn in reward')或作为直接损失函数('kn as loss')，证明后者可以通过前者的等效梯度系数来分析。

Result: 证明'k1 in reward'是Reverse KL正则化的原则性损失；在on-policy条件下，'k2 as loss'与'k1 in reward'梯度等价；'k3 as loss'只是有偏的一阶近似；常见off-policy实现因忽略重要性采样而有偏，提出了原则性修正。

Conclusion: 研究为选择和正确实现KL正则化提供了基于梯度的全面理论依据，为开发更稳健有效的RLHF系统铺平了道路。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [13] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fernández*

Main category: cs.LG

TL;DR: Poolformer是一个序列到序列模型，用循环层和池化操作替代自注意力机制，解决了长序列处理中的计算效率问题，在音频处理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长序列处理中存在二次方复杂度问题，限制了其实际应用。为了更高效地处理长序列，需要开发新的架构来替代自注意力机制。

Method: 使用循环层替代自注意力机制，并引入池化操作来减少序列长度。模型采用递归定义的SkipBlocks结构，包含残差块、下采样池化层、嵌套SkipBlock、上采样池化层和额外的残差块。

Result: 池化操作显著加速了训练过程，改善了感知指标（FID和IS），并防止了过拟合。在原始音频处理任务中，Poolformer超越了SaShiMi和Mamba等最先进模型。

Conclusion: Poolformer通过池化和循环层的结合，有效解决了长序列处理问题，未来可扩展到文本、视觉和多模态应用中。

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [14] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 使用强化学习对抗反应式干扰，通过Q学习和DQN自适应调整传输参数来维持高吞吐量


<details>
  <summary>Details</summary>
Motivation: 解决反应式干扰问题，干扰者采用动态策略选择信道和感知阈值来检测和干扰传输，需要学习避免干扰并优化吞吐量

Method: 使用强化学习（Q学习和深度Q网络）自适应调整传输功率、调制方式和信道选择，无需先验信道条件或干扰策略知识

Result: RL能够快速适应频谱动态变化，在信道和干扰策略随时间变化时维持高传输速率

Conclusion: 强化学习是应对动态干扰策略的有效方法，能够自适应优化传输参数以维持通信性能

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个评估多平台智能体环境中长期记忆和状态跟踪的基准测试，模拟真实企业工作流程，整合Slack、Linear、Git等平台的异步事件，测试记忆获取、选择和冲突解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有上下文和记忆基准测试主要关注对话场景，但企业动态环境中的记忆评估对于实际应用至关重要。

Method: 通过专家手动设计和基于智能体的可扩展合成方法构建数据集，创建生态有效的软件开发场景，引入正确性、效率和冗余度等指标。

Result: 实验显示最先进的LLM和记忆后端在利用长期记忆、处理跨平台依赖和解决矛盾方面面临挑战，表现最佳的GPT-5模型在MEMTRACK上仅达到60%的正确性得分。

Conclusion: 这项工作为记忆增强智能体的评估研究提供了可扩展框架，超越了现有对话设置的局限，为复杂组织环境中的多智能体、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [16] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个基于LLM的自主AI代理，可将原始日志转换为基于本体的知识图谱，通过RAG和迭代校正确保KG的有效性，并关联到MITRE ATT&CK战术，实现从低级日志到高级威胁情报的映射。


<details>
  <summary>Details</summary>
Motivation: 系统日志是宝贵的网络威胁情报来源，但由于缺乏结构、语义不一致和跨设备碎片化，其效用受限。需要将嘈杂、异构的数据转化为连贯且可互操作的表示形式。

Method: 集成轻量级日志本体与检索增强生成(RAG)和迭代校正步骤，确保生成的KG在语法和语义上有效；将KG聚合到会话中，并使用LLM预测MITRE ATT&CK战术。

Result: 在公共基准和真实世界蜜罐数据集上的评估显示，OntoLogX在多个KG后端上实现了稳健的KG生成，并能准确将对抗活动映射到ATT&CK战术。检索和校正提高了精确率和召回率。

Conclusion: 基于本体的表示对于可操作的CTI提取具有重要价值，代码导向模型在结构化日志分析中表现有效，检索和校正机制提升了KG生成的准确性。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [17] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于稀疏自编码器和聚类技术的方法，用于分析大语言模型的内部token表示，并在数学推理任务中指导生成过程。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在数学推理任务中的内部表示和生成行为，旨在平衡推理过程中的利用和探索，以提高推理质量。

Method: 首先训练稀疏自编码器生成稀疏向量表示，然后应用k-means聚类构建token簇图，其中顶点表示token簇，加权边捕捉序列token转移。基于此图定义奖励函数来量化对已建立推理轨迹的遵循程度。

Result: 研究发现平衡利用和探索对于数学推理任务的高准确性至关重要。稀疏自编码器可作为可扩展的奖励模型来指导生成，确保利用和探索之间的平衡权衡。

Conclusion: 该方法能够防止极端行为，最终促进大语言模型中更高质量的推理过程，通过平衡利用和探索来提升数学推理性能。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [18] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种对抗性逆强化学习方法，为语言模型推理学习密集的token级奖励模型，用于过程监督而非风格模仿。该奖励模型在训练时提供步骤级反馈优化推理策略，在推理时作为critic重排采样轨迹。


<details>
  <summary>Details</summary>
Motivation: 重新构建对抗性逆强化学习，直接从专家演示中学习密集的token级奖励模型，以过程监督替代通过监督微调模仿风格，旨在优先考虑正确性而非表面形式。

Method: 采用对抗性逆强化学习方法，学习密集的推理奖励模型。该模型在训练时提供步骤级反馈优化推理策略，在推理时作为critic在固定计算预算下重排采样轨迹。

Result: 在GSM8K数据集上使用Llama3和Qwen2.5骨干网络验证：密集推理奖励可作为学习信号引发推理；通过奖励引导的重排提高了预测性能（特别是基于Llama的策略）。奖励分数与最终答案有效性相关，并能定位轨迹中的错误。

Conclusion: 通过将训练信号、推理时选择和token级诊断统一到单一推理奖励中，这项工作表明可重用的过程级奖励具有增强语言模型中多步推理的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [19] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个表格异常检测基准，通过恢复文本语义来支持上下文感知的异常检测研究，包含20个带有文本元数据的表格数据集和多种检测算法。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文，而实际应用中异常定义与领域特定语境密切相关，这限制了模型充分利用领域知识进行检测的能力。

Method: 提供20个精心策划的表格数据集，包含结构化文本元数据，并实现经典、深度学习和基于LLM的异常检测算法，以及一个无需任务特定训练的零样本LLM框架。

Result: 实验结果显示语义上下文能提升检测性能，并通过支持领域感知推理增强可解释性。

Conclusion: ReTabAD为系统探索上下文感知异常检测建立了基准，证明了文本元数据在异常检测中的重要价值。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出基于置信度感知的路由系统，在生成前评估模型不确定性，根据可靠性估计重定向查询，显著减少幻觉并降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，生成看似合理但事实错误的内容。现有缓解策略依赖生成后修正，计算成本高且无法阻止不可靠内容的生成

Method: 结合三种互补信号：内部表示与参考嵌入的语义对齐、跨模型层的内部收敛分析、学习置信度估计。统一置信度分数决定四种路由路径：高置信度本地生成、中置信度检索增强生成、低置信度使用更大模型、极低置信度人工审核

Result: 在知识密集型QA基准测试中，幻觉检测显著改进（0.74 vs 0.42基线），计算成本比后处理方法降低40%。F1分数从0.61提升至0.82，假阳性率低至0.09

Conclusion: 从反应式修正到主动评估的范式转变，为LLM可靠性增强提供了计算高效的方法

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [21] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob是一个基于大语言模型的语义合成管道，用于动态移动性预测，通过多智能体框架从复杂在线文本中提取和推理时空相关文本，结合时空数据实现更准确的事件驱动预测。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型难以利用描述外部事件的文本信息来预测人类移动性，特别是在突发事件导致移动模式突变时。

Method: 采用多智能体框架，LLM智能体自动从复杂在线文本中提取时空相关信息，通过创新的渐进融合架构将细粒度上下文与时空数据结合。

Result: 在通过该管道构建的数据集上评估，SeMob相比时空模型在MAE和RMSE上分别最大降低了13.92%和11.12%，在事件发生位置和时间附近的时空区域表现尤为突出。

Conclusion: SeMob框架通过利用丰富的事件先验知识，显著提升了事件驱动的移动性预测准确性，特别是在事件相关时空区域。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [22] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 提出了一种名为渐进式自我反思（PSR）的推理时技术，使LLM能够动态自我监控和修正输出，显著降低有害内容生成风险，同时保持良性任务性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成连贯和上下文相关文本方面表现出色，但其部署存在生成有害或不适当内容的风险，需要有效的安全机制。

Method: 采用渐进式自我反思技术，在推理时让模型进行多轮自我评估和修正，并引入轻量级自我反思预测器来根据输入复杂度自适应确定最佳反思轮数。

Result: 在多个模型上显著降低攻击成功率：Llama-3.1-8B-Instruct从77.5%降至5.9%，Llama-3.1-8B基础版从89.7%降至5.6%，Qwen2.5-7B-Instruct从44.4%降至3.8%，且不影响良性任务性能。

Conclusion: 渐进式自我反思作为一种可扩展的测试时方法，通过根据输入风险特征动态分配计算资源，有效提升了LLM的安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [23] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA是一个提示框架，通过将因果事件图转换为自然语言语句注入LLM输入，提升事件问答性能，在TORQUESTRA基准上平均准确率提高5%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在一般语言任务上表现出色，但在需要因果或时序推理的事件类问题上表现不佳。

Method: 提出TAG-EQA框架，将结构化因果事件图转换为自然语言语句注入LLM输入，包含9种提示配置（三种策略×三种输入模态）。

Result: 在TORQUESTRA基准上，TAG-EQA比纯文本基线平均准确率提高5%，零样本设置下最高提升12%，图增强思维链提示有效时提升18%。

Conclusion: 因果图可以在不进行微调的情况下增强LLM的事件推理能力，为基于提示的问答提供了一种灵活的结构编码方式。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [24] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: 本研究采用过程导向方法，通过约束性决策分析LLM作为计算作者的创造力，发现LLM在创作中一致性地强调风格元素。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型创造力的评估主要关注输出质量而非创作过程，本研究旨在填补这一空白，从过程角度分析LLM的作者创造力。

Method: 使用受控提示分配作者角色，基于叙事学理论分析模型的创意偏好，通过约束性决策框架考察创作过程。

Result: 研究发现LLM在创作决策中一致性地优先考虑风格元素，而非角色、事件或场景等其他要素，不同模型展现出独特的创意偏好特征。

Conclusion: 该方法为分析AI作者创造力提供了新颖的系统性工具，揭示了LLM在创作过程中的独特偏好模式。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill是一个结合模仿学习和任务运动规划优势的统一学习框架，支持组合泛化和实时故障恢复，能够从无标注演示中学习谓词、操作符和技能，并在执行时通过符号规划器组合技能实现目标。


<details>
  <summary>Details</summary>
Motivation: 解决多步操作在动态环境中的挑战：模仿学习缺乏组合泛化能力，而任务运动规划存在规划延迟问题，无法实时恢复故障。

Method: 从无标注、无分割的演示中联合学习谓词、操作符和技能；执行时使用符号规划器组合和重排序学习到的技能，在运动和符号层面进行实时恢复。

Result: 在RoboCasa仿真中执行12个单步任务成功率85%，无需额外数据即可组合成最多需要6次技能重组的多步计划；在真实机器人上，仅用5分钟无标注数据就能通过目标规范执行多个任务。

Conclusion: SymSkill框架成功结合了IL和TAMP的优势，实现了组合泛化和实时故障恢复，在仿真和真实机器人上都表现出色。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>
