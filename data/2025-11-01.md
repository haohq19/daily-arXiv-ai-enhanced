<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 提出了EgoExo-Con基准来评估视频LLM在多视角下的一致性时序理解能力，发现现有模型存在跨视角一致性问题，并提出View-GRPO强化学习框架来改善这一缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究视频LLM在不同视角下是否能保持一致的时序理解能力，当前模型在跨视角一致性方面表现不佳。

Method: 引入EgoExo-Con基准评估时序验证和时序定位任务，提出View-GRPO强化学习框架来增强视角特定的时序推理和跨视角一致性理解。

Result: 现有视频LLM在跨视角一致性方面表现较差，View-GRPO方法相比朴素SFT和GRPO在提升跨视角一致性方面表现更优。

Conclusion: 跨视角一致性是视频LLM的重要挑战，View-GRPO框架能有效提升模型在多视角视频中的时序理解一致性。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [2] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出了专门为事件相机设计的Spiking Patches标记化方法，将异步稀疏事件流转换为标记表示，在保持事件相机特性的同时实现高效推理


<details>
  <summary>Details</summary>
Motivation: 现有方法将事件表示为帧或体素，虽然精度高但破坏了事件的异步性和空间稀疏性，需要一种能保持事件相机独特特性的表示方法

Method: 使用Spiking Patches标记器将异步稀疏事件流转换为标记表示，并在手势识别和物体检测任务上使用GNN、PCN和Transformer进行评估

Result: Spiking Patches标记比体素标记推理速度快3.4倍，比帧快10.4倍，同时精度相当甚至在某些情况下更高，手势识别提升3.8%，物体检测提升1.4%

Conclusion: 标记化为事件视觉提供了新方向，是朝着保持事件相机特性方法的重要一步

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [3] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 提出了LoCoT2V-Bench基准，专门用于评估复杂输入条件下的长视频生成，包含多维度评估框架和新指标，发现现有方法在事件间一致性、细粒度对齐和高级主题一致性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成评估基准主要依赖简化提示和低级指标，忽视了与提示的细粒度对齐以及叙事连贯性、主题表达等抽象维度，特别是在处理复杂提示的长视频生成方面存在评估挑战。

Method: 基于真实世界视频构建包含场景转换和事件动态等元素的复杂提示集，建立多维度评估框架，包括新提出的指标：事件级对齐、细粒度时间一致性、内容清晰度以及关注叙事流程、情感响应和角色发展等抽象属性的人类期望实现度(HERD)。

Result: 对9个代表性长视频生成模型进行全面评估，发现当前方法在基础视觉和时间方面表现良好，但在事件间一致性、细粒度对齐和高级主题一致性等方面存在困难。

Conclusion: LoCoT2V-Bench为长格式复杂文本到视频生成提供了全面可靠的评估平台，并指明了未来方法改进的关键方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出了PCN-TA（带时间摊销的预测编码网络），通过保留跨时间帧的潜在状态，利用时间相关性显著降低计算需求，同时保持学习性能。


<details>
  <summary>Details</summary>
Motivation: 边缘机器人系统需要高效的在线学习算法来持续适应变化的环境，传统反向传播不符合生物合理性原则，预测编码框架虽然生物合理但计算开销大。

Method: PCN-TA方法通过保留潜在状态跨时间帧，利用时间相关性来减少计算需求，保持预测编码的局部更新规则。

Result: 在COIL-20机器人感知数据集上，PCN-TA比反向传播减少10%权重更新，比基线PC网络减少50%推理步骤。

Conclusion: PCN-TA显著降低了计算开销，使边缘部署和实时适应支持更可行，其生物启发特性也适合未来神经形态硬件实现。

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [5] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出了一种用于离散时间生存分析的双混合专家框架，结合特征编码器MoE和风险MoE来建模患者异质性和时间动态，在乳腺癌数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 生存分析需要同时建模患者异质性和时间动态，现有方法难以灵活整合这两个方面。

Method: 使用双混合专家框架：特征编码器MoE用于亚组感知表示学习，风险MoE利用患者特征和时间嵌入捕捉时间动态。

Result: 在METABRIC和GBSG乳腺癌数据集上，时间依赖C指数在测试集上提升高达0.04，与Consurv框架结合时获得进一步增益。

Conclusion: 双MoE框架能有效提升生存分析性能，灵活集成现有深度学习生存分析流程。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [6] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种超越传统参数优化的机器学习新范式，将模型视为可塑的几何实体，通过优化预定义拓扑流形上的度量张量场来动态塑造模型空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在固定几何空间中搜索最优参数存在局限性，需要一种能够动态调整模型几何结构的新方法。

Method: 构建变分框架，损失函数平衡数据保真度和流形内在几何复杂度；使用离散微分几何方法，将连续流形离散化为三角网格，通过边长度参数化度量张量，利用自动微分工具进行高效优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用的深刻类比，为"数据驱动几何"概念提供了优雅物理解释；证明了即使固定拓扑，度量优化也比固定几何模型具有更强的表达能力。

Conclusion: 这项工作为构建能够自主演化几何和拓扑的完全动态"元学习器"奠定了坚实基础，在科学模型发现和鲁棒表示学习等领域具有广泛应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [7] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 使用机器学习方法预测医院再入院风险，随机森林模型表现最佳，可识别关键因素帮助降低再入院率和医疗成本。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是医疗支付方、提供者和政策制定者的国家优先事项，用于改善医疗质量和降低成本。

Method: 使用逻辑回归、随机森林和支持向量机分析健康索赔数据，采用主成分分析进行降维，基于AUC指标评估模型性能。

Result: 随机森林模型表现最佳，其次是逻辑回归和支持向量机，能够识别导致再入院的关键人口统计学和医疗因素。

Conclusion: 这些模型可用于识别再入院的关键因素，帮助确定需要重点关注的患者，从而降低再入院率、减少成本并提高医疗质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [8] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 本文通过修改经典强化学习方法，使其能够在稀疏、随机和非平稳环境中高效运行，特别是在自主水下车辆搜索污染云等应用中。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在随机、非平稳和奖励稀疏的环境中表现受限，特别是在自主水下车辆搜索污染云等应用中，行动经常导致零奖励。

Method: 系统研究多种修改方法，包括分层算法变更、多目标学习，以及集成位置记忆作为外部输出过滤器以防止状态重复访问。

Result: 改进的基于蒙特卡洛的方法显著优于传统Q学习和两种穷举搜索模式，展示了其在复杂环境中的适应潜力。

Conclusion: 强化学习方法可以有效适应随机、非平稳和奖励稀疏的环境，为复杂应用场景提供了可行的解决方案。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [9] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: 开发基于流匹配的生成AI模型FM-Cast，用于高效准确地对平流层环流进行概率预报，在18次主要SSW事件中表现出色，计算效率远超传统数值天气预报系统。


<details>
  <summary>Details</summary>
Motivation: 平流层突然增温（SSW）是次季节可预报性的关键来源和极端冬季天气的主要驱动因素，但传统数值天气预报系统在物理表示、初始化和计算需求方面存在限制，数据驱动方法在SSW的复杂三维动力学特别是概率预报方面研究不足。

Method: 采用基于流匹配的生成AI模型FM-Cast，构建高效的平流层环流时空演变的概率预报系统。

Result: 在1998-2024年的18次主要SSW事件中，FM-Cast能够提前20天准确预报10次事件的开始、强度和形态，集合准确率超过50%，性能达到或超过领先的NWP系统，在消费级GPU上仅需2分钟完成50成员、30天的预报。

Conclusion: 建立了一个计算高效的平流层异常概率预报范式，展示了生成AI在加深对大气-气候动力学物理理解方面的潜力，通过理想化实验证明SSW可预报性与其物理驱动因素密切相关。

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [10] [Aeolus: A Multi-structural Flight Delay Dataset](https://arxiv.org/abs/2510.26616)
*Lin Xu,Xinyun Yuan,Yuxuan Liang,Suwan Yin,Yuankai Wu*

Main category: cs.LG

TL;DR: Aeolus是一个大规模多模态航班延误数据集，包含表格数据、航班链模块和航班网络图，用于支持航班延误预测和表格数据基础模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有航班延误数据集通常局限于平面表格结构，无法捕捉延误传播的时空动态特性。Aeolus旨在解决这一局限性。

Method: 提供三种对齐模态：包含5000多万航班丰富特征的表格数据集、建模延误传播的航班链模块、编码共享资源的航班网络图。

Result: 数据集包含时间分割、全面特征和严格防泄漏设计，支持回归、分类、时序结构建模和图学习等多种任务。

Conclusion: Aeolus填补了领域特定建模和通用结构化数据研究的关键空白，提供了统一的基准测试平台。

Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 提出利用自主AI代理实现FinOps自动化，解决云账单数据格式异构、难以整合分析的问题。构建了一个用于IT基础设施和成本优化的FinOps代理系统，能够从多源获取数据、整合分析并生成优化建议。


<details>
  <summary>Details</summary>
Motivation: FinOps从业者面临云账单数据格式异构、分类标准和度量不统一的问题，这导致难以快速整合数据、生成可操作洞察并做出及时决策。

Method: 构建了一个模拟真实端到端行业流程的FinOps代理系统，包括从多源检索数据、整合分析数据到生成优化建议。使用多个开源和闭源语言模型评估代理性能。

Result: 评估结果显示，该代理能够像真实的FinOps从业者一样理解、规划和执行任务。

Conclusion: 自主目标驱动的AI代理可以有效实现FinOps自动化，解决云成本管理中的数据整合和分析挑战。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [12] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance框架通过将法规文本表示为策略图、运行时上下文表示为上下文图，并将两者对齐，来增强网络规模的合规性评估。该方法在GDPR相关场景中比纯LLM和RAG基线表现更好。


<details>
  <summary>Details</summary>
Motivation: 网络规模合规面临实际挑战：每个请求都需要进行法规评估。法规文本具有交叉引用和规范性特点，而运行时上下文则用非结构化自然语言表达，需要将非结构化文本中的语义信息与法规的结构化规范元素对齐。

Method: 引入GraphCompliance框架，将法规文本表示为策略图（编码规范结构和交叉引用），将运行时上下文表示为上下文图（将事件形式化为主体-动作-对象三元组和实体关系三元组），然后对齐这两个图。

Result: 在300个GDPR衍生真实场景的五个评估任务中，GraphCompliance比纯LLM和RAG基线的micro-F1分数高出4.1-7.2个百分点，具有更少的欠预测和过预测，召回率更高，误报率更低。

Conclusion: 消融研究表明每个图组件都有贡献，表明结构化表示和法官LLM在规范推理中是互补的。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [13] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出将人格视为社会赋予实体的义务束（权利与责任），而非形而上学属性，以务实应对AI主体性带来的新型人格爆炸。


<details>
  <summary>Details</summary>
Motivation: AI主体性的出现将引发新型人格的"寒武纪大爆发"，需要实用框架来应对这种多样化，避免陷入关于AI意识或理性的无解争论。

Method: 采用实用主义框架，将人格视为可解构的义务束，利用去中心化数字身份技术，探讨"人格作为问题"和"人格作为解决方案"两种视角。

Result: 提出了一种灵活的人格概念，允许为不同情境创建定制解决方案，如通过创建可被制裁的AI"个体"来促进AI合同签订。

Conclusion: 通过拒绝寻求单一本质的人格定义，本文提供了更实用灵活的方式来思考AI智能体融入社会的问题。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 提出基于模糊逻辑和句法依赖解析的情感分析方法，通过细粒度分类（非常弱、弱、中等、强、非常强）来评估实体评论的情感强度和方向，从而对实体进行排名。


<details>
  <summary>Details</summary>
Motivation: 传统基于词典的情感分析方法不考虑情感强度，无法区分不同程度的情感表达（如非常强烈负面、中等负面等）。

Method: 结合模糊逻辑算法将情感词分类到不同强度级别，使用句法依赖解析找到与目标方面相关的情感词，计算实体在特定方面的情感得分。

Result: 能够更精确地分析评论中针对特定产品方面的情感强度和方向，实现细粒度的实体排名。

Conclusion: 该方法能够有效解决传统情感分析忽略情感强度的问题，提供更准确和细粒度的实体评价。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [15] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER方法通过自动生成教科书式课程和基于布鲁姆分类法的问答对，将通用大语言模型转化为领域专家，在专业领域性能显著提升且不损失通用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用任务上表现出色，但在经济学、心理学等需要深度原理性理解的专业领域表现不佳，存在关键领域差距。

Method: ACER方法首先为学科生成目录结构，然后基于布鲁姆分类法创建问答对，构建系统性课程。使用交错课程安排进行持续预训练，在内容和认知维度上对齐学习过程。

Result: 在Llama 3.2模型上，ACER在专业MMLU子集上显著提升，微观经济学准确率提高5个百分点，所有目标领域平均提升3个百分点。非目标领域性能也提升0.7点，知识密集型基准测试ARC和GPQA提升超过2点。

Conclusion: ACER提供了一个可扩展且有效的方法来弥补LLMs在关键领域的能力差距，既能防止灾难性遗忘，又能促进跨领域知识迁移。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [16] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个高级数学推理基准，包含50道奥林匹克竞赛难度或更高的人类编写问题，旨在评估大型语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学竞赛基准对顶级LLM的评估效果下降，因为出现了性能饱和现象（如AIME24/25）。需要更严格的挑战来评估LLM的数学推理能力。

Method: 创建包含50道原创问题的基准，所有问题都经过专家交叉验证，确保达到国际数学奥林匹克竞赛难度标准，且仅需最终答案而非证明，便于自动评分。

Result: 在26个LLM上的实验结果显示，表现最好的模型在AMO-Bench上仅达到52.4%准确率，大多数LLM得分低于40%。但分析显示随着测试时计算量的增加存在有前景的扩展趋势。

Conclusion: 当前LLM在数学推理方面仍有显著改进空间，AMO-Bench的发布将促进语言模型推理能力的研究进展。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration](https://arxiv.org/abs/2510.26536)
*Huajie Tan,Cheng Chi,Xiansheng Chen,Yuheng Ji,Zhongxia Zhao,Xiaoshuai Hao,Yaoxu Lyu,Mingyu Cao,Junkai Zhao,Huaihai Lyu,Enshen Zhou,Ning Chen,Yankai Fu,Cheng Peng,Wei Guo,Dong Liang,Zhuo Chen,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了RoboOS-NeXT框架，通过统一的时空-本体记忆(STEM)实现多机器人系统的终身适应、可扩展协调和鲁棒调度


<details>
  <summary>Details</summary>
Motivation: 现有方法因依赖有限的单智能体记忆而无法实现长期学习、扩展到异构团队或从故障中恢复，需要统一的内存表示

Method: 引入Spatio-Temporal-Embodiment Memory (STEM)集成空间场景几何、时间事件历史和本体配置文件，采用大脑-小脑框架，高层大脑模型通过检索和更新STEM进行全局规划，低层控制器本地执行动作

Result: 在餐厅、超市和家庭等复杂协调任务中的广泛实验表明，RoboOS-NeXT在异构本体上实现了优越性能

Conclusion: RoboOS-NeXT通过其记忆中心设计有效实现了终身、可扩展和鲁棒的多机器人协作

Abstract: The proliferation of collaborative robots across diverse tasks and
embodiments presents a central challenge: achieving lifelong adaptability,
scalable coordination, and robust scheduling in multi-agent systems. Existing
approaches, from vision-language-action (VLA) models to hierarchical
frameworks, fall short due to their reliance on limited or dividual-agent
memory. This fundamentally constrains their ability to learn over long
horizons, scale to heterogeneous teams, or recover from failures, highlighting
the need for a unified memory representation. To address these limitations, we
introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,
and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel
Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene
geometry, temporal event history, and embodiment profiles into a shared
representation. This memory-centric design is integrated into a
brain-cerebellum framework, where a high-level brain model performs global
planning by retrieving and updating STEM, while low-level controllers execute
actions locally. This closed loop between cognition, memory, and execution
enables dynamic task allocation, fault-tolerant collaboration, and consistent
state synchronization. We conduct extensive experiments spanning complex
coordination tasks in restaurants, supermarkets, and households. Our results
demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous
embodiments, validating its effectiveness in enabling lifelong, scalable, and
robust multi-robot collaboration. Project website:
https://flagopen.github.io/RoboOS/

</details>


### [18] [Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments](https://arxiv.org/abs/2510.26646)
*Xiaoyi He,Danggui Chen,Zhenshuo Zhang,Zimeng Bai*

Main category: cs.RO

TL;DR: 提出了一种分层路径规划与控制框架，结合高层DQN进行离散子目标选择和底层TD3控制器进行连续动作执行，在动态和部分可观测环境中实现了更高的成功率和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决单一算法在复杂环境中路径规划的局限性，通过分层结构结合离散决策和连续控制，提高在动态和部分可观测环境中的导航性能。

Method: 使用高层Deep Q-Network进行子目标选择，底层Twin Delayed Deep Deterministic Policy Gradient进行连续速度控制，设计了包含方向、距离、避障、动作平滑度等要素的奖励函数，并集成了LiDAR安全门机制。

Result: 在ROS+Gazebo平台上验证，使用PathBench指标评估，相比单一算法基线和基于规则的规划器，在动态环境中表现出更高的成功率、更好的泛化能力和更平滑的控制变化。

Conclusion: 分层强化学习框架有效结合了离散决策和连续控制的优势，在复杂导航任务中实现了优越的性能和泛化能力。

Abstract: This paper presents a hierarchical path-planning and control framework that
combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with
a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller
for continuous actuation. The high-level module selects behaviors and
sub-goals; the low-level module executes smooth velocity commands. We design a
practical reward shaping scheme (direction, distance, obstacle avoidance,
action smoothness, collision penalty, time penalty, and progress), together
with a LiDAR-based safety gate that prevents unsafe motions. The system is
implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,
including success rate, collision rate, path efficiency, and re-planning
efficiency, in dynamic and partially observable environments. Experiments show
improved success rate and sample efficiency over single-algorithm baselines
(DQN or TD3 alone) and rule-based planners, with better generalization to
unseen obstacle configurations and reduced abrupt control changes. Code and
evaluation scripts are available at the project repository.

</details>
