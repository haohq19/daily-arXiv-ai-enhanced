{"id": "2508.01103", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.01103", "abs": "https://arxiv.org/abs/2508.01103", "authors": ["Haocheng Zhao", "Niklas Schl\u00fcter", "Lukas Brunke", "Angela P. Schoellig"], "title": "Improving Drone Racing Performance Through Iterative Learning MPC", "comment": "Accepted for oral presentation at IROS 2025", "summary": "Autonomous drone racing presents a challenging control problem, requiring\nreal-time decision-making and robust handling of nonlinear system dynamics.\nWhile iterative learning model predictive control~(LMPC) offers a promising\nframework for iterative performance improvement, its direct application to\ndrone racing faces challenges like real-time compatibility or the trade-off\nbetween time-optimal and safe traversal. In this paper, we enhance LMPC with\nthree key innovations:~(1) an adaptive cost function that dynamically weights\ntime-optimal tracking against centerline adherence,~(2)~a shifted local safe\nset to prevent excessive shortcutting and enable more robust iterative updates,\nand~(3) a Cartesian-based formulation that accommodates safety constraints\nwithout the singularities or integration errors associated with Frenet-frame\ntransformations. Results from extensive simulation and real-world experiments\ndemonstrate that our improved algorithm can optimize initial trajectories\ngenerated by a wide range of controllers with varying levels of tuning for a\nmaximum improvement in lap time by 60.85\\%. Even applied to the most\naggressively tuned state-of-the-art model-based controller, MPCC++, on a real\ndrone, a 6.05\\% improvement is still achieved. Overall, the proposed method\npushes the drone toward faster traversal and avoids collisions in simulation\nand real-world experiments, making it a practical solution to improve the peak\nperformance of drone racing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8fed\u4ee3\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08LMPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6210\u672c\u51fd\u6570\u3001\u5c40\u90e8\u5b89\u5168\u96c6\u548c\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4f18\u5316\u65e0\u4eba\u673a\u7ade\u901f\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u7ade\u901f\u9700\u8981\u5b9e\u65f6\u51b3\u7b56\u548c\u9c81\u68d2\u63a7\u5236\uff0c\u4f46\u73b0\u6709LMPC\u65b9\u6cd5\u5728\u65f6\u95f4\u6700\u4f18\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u5b9e\u65f6\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e09\u9879\u521b\u65b0\uff1a\u81ea\u9002\u5e94\u6210\u672c\u51fd\u6570\u52a8\u6001\u5e73\u8861\u65f6\u95f4\u6700\u4f18\u4e0e\u4e2d\u5fc3\u7ebf\u8ddf\u8e2a\uff0c\u5c40\u90e8\u5b89\u5168\u96c6\u9632\u6b62\u8fc7\u5ea6\u6377\u5f84\uff0c\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u907f\u514d\u5947\u5f02\u6027\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u663e\u793a\uff0c\u7b97\u6cd5\u53ef\u5c06\u521d\u59cb\u8f68\u8ff9\u4f18\u5316\u81f3\u6700\u9ad860.85%\u7684\u5708\u901f\u63d0\u5347\uff0c\u5373\u4f7f\u5bf9\u6700\u4f18\u63a7\u5236\u5668MPCC++\u4ecd\u67096.05%\u63d0\u5347\u3002", "conclusion": "\u6539\u8fdb\u7684LMPC\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u7ade\u901f\u6027\u80fd\uff0c\u517c\u987e\u901f\u5ea6\u4e0e\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.00913", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00913", "abs": "https://arxiv.org/abs/2508.00913", "authors": ["Mohammad Mohammadi", "Ziyi Wu", "Igor Gilitschenski"], "title": "TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras", "comment": "Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025", "summary": "Long-term temporal information is crucial for event-based perception tasks,\nas raw events only encode pixel brightness changes. Recent works show that when\ntrained from scratch, recurrent models achieve better results than feedforward\nmodels in these tasks. However, when leveraging self-supervised pre-trained\nweights, feedforward models can outperform their recurrent counterparts.\nCurrent self-supervised learning (SSL) methods for event-based pre-training\nlargely mimic RGB image-based approaches. They pre-train feedforward models on\nraw events within a short time interval, ignoring the temporal information of\nevents. In this work, we introduce TESPEC, a self-supervised pre-training\nframework tailored for learning spatio-temporal information. TESPEC is\nwell-suited for recurrent models, as it is the first framework to leverage long\nevent sequences during pre-training. TESPEC employs the masked image modeling\nparadigm with a new reconstruction target. We design a novel method to\naccumulate events into pseudo grayscale videos containing high-level semantic\ninformation about the underlying scene, which is robust to sensor noise and\nreduces motion blur. Reconstructing this target thus requires the model to\nreason about long-term history of events. Extensive experiments demonstrate our\nstate-of-the-art results in downstream tasks, including object detection,\nsemantic segmentation, and monocular depth estimation. Project webpage:\nhttps://mhdmohammadi.github.io/TESPEC_webpage.", "AI": {"tldr": "TESPEC\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u4e13\u4e3a\u5b66\u4e60\u4e8b\u4ef6\u6570\u636e\u7684\u65f6\u7a7a\u4fe1\u606f\u8bbe\u8ba1\uff0c\u7279\u522b\u9002\u5408\u5faa\u73af\u6a21\u578b\uff0c\u901a\u8fc7\u957f\u4e8b\u4ef6\u5e8f\u5217\u9884\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u4e8b\u4ef6\u6570\u636e\u7684\u957f\u671f\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u5faa\u73af\u6a21\u578b\u5728\u4ece\u5934\u8bad\u7ec3\u65f6\u8868\u73b0\u4f18\u4e8e\u524d\u9988\u6a21\u578b\uff0c\u4f46\u9884\u8bad\u7ec3\u65f6\u524d\u9988\u6a21\u578b\u66f4\u4f18\u3002TESPEC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TESPEC\u91c7\u7528\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u8303\u5f0f\uff0c\u8bbe\u8ba1\u65b0\u91cd\u5efa\u76ee\u6807\uff0c\u5c06\u4e8b\u4ef6\u7d2f\u79ef\u4e3a\u4f2a\u7070\u5ea6\u89c6\u9891\uff0c\u51cf\u5c11\u566a\u58f0\u548c\u8fd0\u52a8\u6a21\u7cca\uff0c\u8981\u6c42\u6a21\u578b\u63a8\u7406\u957f\u671f\u4e8b\u4ef6\u5386\u53f2\u3002", "result": "\u5728\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cTESPEC\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "TESPEC\u901a\u8fc7\u5229\u7528\u957f\u4e8b\u4ef6\u5e8f\u5217\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5faa\u73af\u6a21\u578b\u5728\u4e8b\u4ef6\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00945", "abs": "https://arxiv.org/abs/2508.00945", "authors": ["Yifan Wang", "Hongfeng Ai", "Quangao Liu", "Maowei Jiang", "Ruiyuan Kang", "Ruiqi Li", "Jiahua Dong", "Mengting Xiao", "Cheng Jiang", "Chenzhong Li"], "title": "Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment", "comment": "10 pages", "summary": "Vision Language Models (VLMs) face challenges in effectively coordinating\ndiverse attention mechanisms for cross-modal embedding learning, leading to\nmismatched attention and suboptimal performance. We propose Consistent\nCross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross\nAttention (LPWCA) to capture fine-grained regional-semantic correlations by\njointly weighting patch and layer-wise embedding, and Progressive Attention\nIntegration (PAI) that systematically coordinates LPWCA, layer-wise, and\npatch-wise attention mechanisms in sequence. This progressive design ensures\nconsistency from semantic to regional levels while preventing attention drift\nand maximizing individual attention benefits. Experimental results on ten\ndiverse vision-language benchmarks demonstrate that our CCRA-enhanced\nLLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all\nbaseline methods with only 3.55M additional parameters, while providing\nenhanced interpretability through more regionally focused and semantically\naligned attention patterns.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCCRA\u65b9\u6cd5\uff0c\u901a\u8fc7LPWCA\u548cPAI\u534f\u8c03\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8de8\u6a21\u6001\u5d4c\u5165\u5b66\u4e60\u4e2d\u5b58\u5728\u6ce8\u610f\u529b\u4e0d\u5339\u914d\u548c\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCCRA\uff0c\u5305\u62ecLPWCA\uff08\u5c42-\u5757\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u548cPAI\uff08\u6e10\u8fdb\u6ce8\u610f\u529b\u96c6\u6210\uff09\uff0c\u4ee5\u7ec6\u7c92\u5ea6\u534f\u8c03\u533a\u57df\u8bed\u4e49\u5173\u8054\u3002", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCRA\u589e\u5f3a\u7684LLaVA-v1.5-7B\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u4ec5\u589e\u52a03.55M\u53c2\u6570\u3002", "conclusion": "CCRA\u901a\u8fc7\u4e00\u81f4\u6027\u548c\u6e10\u8fdb\u6027\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6ce8\u610f\u529b\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2508.01198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff08Suffix Optimization, SOP\uff09\uff0c\u7528\u4e8e\u5728\u4e0d\u5fae\u8c03\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u9650\u5236LLM\u751f\u6210\u7279\u5b9a\u53d7\u9650\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u751f\u6210\u5185\u5bb9\u7684\u5e7f\u6cdb\u6027\uff0c\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u548c\u5feb\u901f\u53d8\u5316\u7684\u9650\u5236\u9700\u6c42\uff0c\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSuffix Optimization (SOP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63d0\u793a\u540e\u6dfb\u52a0\u4f18\u5316\u7684\u540e\u7f00\uff0c\u9632\u6b62LLM\u751f\u6210\u53d7\u9650\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "result": "SOP\u5728\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u9650\u5236\u7387\u63d0\u53476%-17%\uff0c\u5e76\u5728\u5b9e\u9645\u5e73\u53f0POE\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "SOP\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5185\u5bb9\u9650\u5236\u9700\u6c42\u3002"}}
{"id": "2508.00888", "categories": ["cs.LG", "stat.AP", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00888", "abs": "https://arxiv.org/abs/2508.00888", "authors": ["Amir Hossein Kalantari", "Eleonora Papadimitriou", "Amir Pooyan Afghari"], "title": "A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data", "comment": "32 pages", "summary": "Naturalistic driving studies offer a powerful means for observing and\nquantifying real-world driving behaviour. One of their prominent applications\nin traffic safety is the continuous monitoring and classification of risky\ndriving behaviour. However, many existing frameworks rely on fixed time windows\nand static thresholds for distinguishing between safe and risky behaviour -\nlimiting their ability to respond to the stochastic nature of real-world\ndriving. This study proposes a dynamic and individualised framework for\nidentifying risky driving behaviour using Belgian naturalistic driving data.\nThe approach leverages a rolling time window and bi-level optimisation to\ndynamically calibrate both risk thresholds and model hyperparameters, capturing\nsubtle behavioural shifts. Two safety indicators, speed-weighted headway and\nharsh driving events, were evaluated using three data-driven models: Random\nForest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong\ncapability in capturing subtle changes in driving behaviour, particularly\nexcelling in high-recall tasks, making it promising for early-stage risk\ndetection. XGBoost provided the most balanced and stable performance across\ndifferent thresholds and evaluation metrics. While random forest showed more\nvariability, it responded sensitively to dynamic threshold adjustments, which\nmay be advantageous during model adaptation or tuning. Speed-weighted headway\nemerged as a more stable and context-sensitive risk indicator than harsh\ndriving events, likely due to its robustness to label sparsity and contextual\nvariation. Overall, the findings support the value of adaptive, personalised\nrisk detection approaches for enhancing real-time safety feedback and tailoring\ndriver support in intelligent transport systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e2a\u6027\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u6bd4\u5229\u65f6\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u4e2d\u7684\u5371\u9669\u9a7e\u9a76\u884c\u4e3a\uff0c\u901a\u8fc7\u6eda\u52a8\u65f6\u95f4\u7a97\u53e3\u548c\u53cc\u5c42\u4f18\u5316\u52a8\u6001\u6821\u51c6\u98ce\u9669\u9608\u503c\u548c\u6a21\u578b\u8d85\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u65f6\u95f4\u7a97\u53e3\u548c\u9759\u6001\u9608\u503c\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u9a7e\u9a76\u7684\u968f\u673a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u548c\u4e2a\u6027\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u98ce\u9669\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6eda\u52a8\u65f6\u95f4\u7a97\u53e3\u548c\u53cc\u5c42\u4f18\u5316\u6280\u672f\uff0c\u52a8\u6001\u6821\u51c6\u98ce\u9669\u9608\u503c\u548c\u6a21\u578b\u8d85\u53c2\u6570\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u4e24\u79cd\u5b89\u5168\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9ad8\u53ec\u56de\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cXGBoost\u5728\u5e73\u8861\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u6700\u4f73\uff0c\u968f\u673a\u68ee\u6797\u5bf9\u52a8\u6001\u9608\u503c\u8c03\u6574\u654f\u611f\u3002\u901f\u5ea6\u52a0\u6743\u8f66\u8ddd\u6bd4\u6fc0\u70c8\u9a7e\u9a76\u4e8b\u4ef6\u66f4\u7a33\u5b9a\u4e14\u4e0a\u4e0b\u6587\u654f\u611f\u3002", "conclusion": "\u81ea\u9002\u5e94\u548c\u4e2a\u6027\u5316\u7684\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u5b9e\u65f6\u5b89\u5168\u53cd\u9988\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u9a7e\u9a76\u5458\u652f\u6301\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.01583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01583", "abs": "https://arxiv.org/abs/2508.01583", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Rongguang Ye", "Jingreng Lei", "Shuai Wang", "Qingfeng Lin", "Ming Tang", "Yik-Chung Wu"], "title": "Adverse Weather-Independent Framework Towards Autonomous Driving Perception through Temporal Correlation and Unfolded Regularization", "comment": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2409.14737", "summary": "Various adverse weather conditions such as fog and rain pose a significant\nchallenge to autonomous driving (AD) perception tasks like semantic\nsegmentation, object detection, etc. The common domain adaption strategy is to\nminimize the disparity between images captured in clear and adverse weather\nconditions. However, domain adaption faces two challenges: (I) it typically\nrelies on utilizing clear image as a reference, which is challenging to obtain\nin practice; (II) it generally targets single adverse weather condition and\nperforms poorly when confronting the mixture of multiple adverse weather\nconditions. To address these issues, we introduce a reference-free and Adverse\nweather condition-independent (Advent) framework (rather than a specific model\narchitecture) that can be implemented by various backbones and heads. This is\nachieved by leveraging the homogeneity over short durations, getting rid of\nclear reference and being generalizable to arbitrary weather condition.\nSpecifically, Advent includes three integral components: (I) Locally Sequential\nMechanism (LSM) leverages temporal correlations between adjacent frames to\nachieve the weather-condition-agnostic effect thanks to the homogeneity behind\narbitrary weather condition; (II) Globally Shuffled Mechanism (GSM) is proposed\nto shuffle segments processed by LSM from different positions of input sequence\nto prevent the overfitting to LSM-induced temporal patterns; (III) Unfolded\nRegularizers (URs) are the deep unfolding implementation of two proposed\nregularizers to penalize the model complexity to enhance across-weather\ngeneralization. We take the semantic segmentation task as an example to assess\nthe proposed Advent framework. Extensive experiments demonstrate that the\nproposed Advent outperforms existing state-of-the-art baselines with large\nmargins.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u53c2\u8003\u4e14\u72ec\u7acb\u4e8e\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u7684\u6846\u67b6Advent\uff0c\u901a\u8fc7\u5229\u7528\u77ed\u65f6\u540c\u8d28\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57df\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u6e05\u6670\u56fe\u50cf\u548c\u5355\u4e00\u5929\u6c14\u6761\u4ef6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6076\u52a3\u5929\u6c14\u6761\u4ef6\uff08\u5982\u96fe\u3001\u96e8\uff09\u5bf9\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\uff08\u5982\u8bed\u4e49\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\uff09\u6784\u6210\u6311\u6218\uff0c\u4f20\u7edf\u57df\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u6e05\u6670\u56fe\u50cf\u4e14\u96be\u4ee5\u5e94\u5bf9\u591a\u79cd\u5929\u6c14\u6df7\u5408\u60c5\u51b5\u3002", "method": "Advent\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u5c40\u90e8\u5e8f\u5217\u673a\u5236\uff08LSM\uff09\u5229\u7528\u76f8\u90bb\u5e27\u7684\u65f6\u95f4\u76f8\u5173\u6027\uff1b\u5168\u5c40\u6df7\u6d17\u673a\u5236\uff08GSM\uff09\u9632\u6b62\u8fc7\u62df\u5408\uff1b\u5c55\u5f00\u6b63\u5219\u5316\u5668\uff08URs\uff09\u589e\u5f3a\u8de8\u5929\u6c14\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdvent\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Advent\u6846\u67b6\u901a\u8fc7\u65e0\u53c2\u8003\u548c\u5929\u6c14\u65e0\u5173\u7684\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01074", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01074", "abs": "https://arxiv.org/abs/2508.01074", "authors": ["Hongyu Zhu", "Sichu Liang", "Wenwen Wang", "Zhuomeng Zhang", "Fangqi Li", "Shi-Lin Wang"], "title": "Evading Data Provenance in Deep Neural Networks", "comment": "ICCV 2025 Highlight", "summary": "Modern over-parameterized deep models are highly data-dependent, with large\nscale general-purpose and domain-specific datasets serving as the bedrock for\nrapid advancements. However, many datasets are proprietary or contain sensitive\ninformation, making unrestricted model training problematic. In the open world\nwhere data thefts cannot be fully prevented, Dataset Ownership Verification\n(DOV) has emerged as a promising method to protect copyright by detecting\nunauthorized model training and tracing illicit activities. Due to its\ndiversity and superior stealth, evading DOV is considered extremely\nchallenging. However, this paper identifies that previous studies have relied\non oversimplistic evasion attacks for evaluation, leading to a false sense of\nsecurity. We introduce a unified evasion framework, in which a teacher model\nfirst learns from the copyright dataset and then transfers task-relevant yet\nidentifier-independent domain knowledge to a surrogate student using an\nout-of-distribution (OOD) dataset as the intermediary. Leveraging\nVision-Language Models and Large Language Models, we curate the most\ninformative and reliable subsets from the OOD gallery set as the final transfer\nset, and propose selectively transferring task-oriented knowledge to achieve a\nbetter trade-off between generalization and evasion effectiveness. Experiments\nacross diverse datasets covering eleven DOV methods demonstrate our approach\nsimultaneously eliminates all copyright identifiers and significantly\noutperforms nine state-of-the-art evasion attacks in both generalization and\neffectiveness, with moderate computational overhead. As a proof of concept, we\nreveal key vulnerabilities in current DOV methods, highlighting the need for\nlong-term development to enhance practicality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c4\u907f\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u7248\u6743\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528OOD\u6570\u636e\u96c6\u8f6c\u79fb\u4efb\u52a1\u76f8\u5173\u77e5\u8bc6\uff0c\u6210\u529f\u89c4\u907f\u4e8611\u79cdDOV\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524dDOV\u65b9\u6cd5\u7684\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\uff0c\u4f46\u8bb8\u591a\u6570\u636e\u96c6\u662f\u4e13\u6709\u6216\u654f\u611f\u7684\uff0cDOV\u65b9\u6cd5\u7528\u4e8e\u4fdd\u62a4\u7248\u6743\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u89c4\u907f\u653b\u51fb\u7684\u8bc4\u4f30\u8fc7\u4e8e\u7b80\u5316\uff0c\u5bfc\u81f4\u865a\u5047\u7684\u5b89\u5168\u611f\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u89c4\u907f\u6846\u67b6\uff0c\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u7248\u6743\u6570\u636e\u540e\uff0c\u901a\u8fc7OOD\u6570\u636e\u96c6\u8f6c\u79fb\u4efb\u52a1\u76f8\u5173\u77e5\u8bc6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7b5b\u9009\u4fe1\u606f\u5b50\u96c6\uff0c\u9009\u62e9\u6027\u8f6c\u79fb\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6d88\u9664\u6240\u6709\u7248\u6743\u6807\u8bc6\u7b26\uff0c\u5728\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4e5d\u79cd\u73b0\u6709\u89c4\u907f\u653b\u51fb\uff0c\u8ba1\u7b97\u5f00\u9500\u9002\u4e2d\u3002", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524dDOV\u65b9\u6cd5\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u9700\u8981\u957f\u671f\u53d1\u5c55\u4ee5\u63d0\u5347\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00923", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00923", "abs": "https://arxiv.org/abs/2508.00923", "authors": ["Jiazhen Pan", "Bailiang Jian", "Paul Hager", "Yundi Zhang", "Che Liu", "Friedrike Jungmann", "Hongwei Bran Li", "Chenyu You", "Junde Wu", "Jiayuan Zhu", "Fenglin Liu", "Yuyuan Liu", "Niklas Bubeck", "Christian Wachinger", "Chen", "Chen", "Zhenyu Gong", "Cheng Ouyang", "Georgios Kaissis", "Benedikt Wiestler", "Daniel Rueckert"], "title": "Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models", "comment": null, "summary": "Ensuring the safety and reliability of large language models (LLMs) in\nclinical practice is critical to prevent patient harm and promote trustworthy\nhealthcare applications of AI. However, LLMs are advancing so rapidly that\nstatic safety benchmarks often become obsolete upon publication, yielding only\nan incomplete and sometimes misleading picture of model trustworthiness. We\ndemonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming\nframework that continuously stress-tests LLMs can reveal significant weaknesses\nof current LLMs across four safety-critical domains: robustness, privacy,\nbias/fairness, and hallucination. A suite of adversarial agents is applied to\nautonomously mutate test cases, identify/evolve unsafe-triggering strategies,\nand evaluate responses, uncovering vulnerabilities in real time without human\nintervention. Applying DAS to 15 proprietary and open-source LLMs revealed a\nstark contrast between static benchmark performance and vulnerability under\nadversarial pressure. Despite a median MedQA accuracy exceeding 80\\%, 94\\% of\npreviously correct answers failed our dynamic robustness tests. We observed\nsimilarly high failure rates across other domains: privacy leaks were elicited\nin 86\\% of scenarios, cognitive-bias priming altered clinical recommendations\nin 81\\% of fairness tests, and we identified hallucination rates exceeding 66\\%\nin widely used models. Such profound residual risks are incompatible with\nroutine clinical practice. By converting red-teaming from a static checklist\ninto a dynamic stress-test audit, DAS red-teaming offers the surveillance that\nhospitals/regulators/technology vendors require as LLMs become embedded in\npatient chatbots, decision-support dashboards, and broader healthcare\nworkflows. Our framework delivers an evolvable, scalable, and reliable\nsafeguard for the next generation of medical AI.", "AI": {"tldr": "\u52a8\u6001\u3001\u81ea\u52a8\u3001\u7cfb\u7edf\u5316\u7684\u7ea2\u961f\u6d4b\u8bd5\u6846\u67b6\uff08DAS\uff09\u7528\u4e8e\u6301\u7eed\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u63ed\u793a\u5176\u5728\u9c81\u68d2\u6027\u3001\u9690\u79c1\u3001\u504f\u89c1/\u516c\u5e73\u6027\u548c\u5e7b\u89c9\u65b9\u9762\u7684\u663e\u8457\u5f31\u70b9\u3002", "motivation": "\u786e\u4fddLLM\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u9632\u6b62\u60a3\u8005\u53d7\u5230\u4f24\u5bb3\u5e76\u4fc3\u8fdbAI\u5728\u533b\u7597\u4e2d\u7684\u53ef\u4fe1\u5e94\u7528\u3002", "method": "\u91c7\u7528DAS\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u4ee3\u7406\u52a8\u6001\u53d8\u5f02\u6d4b\u8bd5\u7528\u4f8b\u3001\u8bc6\u522b/\u6f14\u5316\u4e0d\u5b89\u5168\u89e6\u53d1\u7b56\u7565\u5e76\u8bc4\u4f30\u54cd\u5e94\uff0c\u5b9e\u65f6\u53d1\u73b0\u6f0f\u6d1e\u3002", "result": "\u6d4b\u8bd515\u4e2aLLM\u53d1\u73b0\uff0c\u9759\u6001\u57fa\u51c6\u6027\u80fd\u4e0e\u5bf9\u6297\u538b\u529b\u4e0b\u7684\u8106\u5f31\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u591a\u4e2a\u9886\u57df\u5931\u8d25\u7387\u9ad8\u8fbe66%-94%\u3002", "conclusion": "DAS\u6846\u67b6\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u53ef\u8fdb\u5316\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u9002\u7528\u4e8e\u533b\u9662\u3001\u76d1\u7ba1\u673a\u6784\u548c\u6280\u672f\u4f9b\u5e94\u5546\u3002"}}
{"id": "2508.01810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01810", "abs": "https://arxiv.org/abs/2508.01810", "authors": ["Wentao Shi", "Yang Yang", "Yiming Huang", "Hongliang Ren"], "title": "Exploring Stiffness Gradient Effects in Magnetically Induced Metamorphic Materials via Continuum Simulation and Validation", "comment": "Accepted to IROS 2025", "summary": "Magnetic soft continuum robots are capable of bending with remote control in\nconfined space environments, and they have been applied in various\nbioengineering contexts. As one type of ferromagnetic soft continuums, the\nMagnetically Induced Metamorphic Materials (MIMMs)-based continuum (MC)\nexhibits similar bending behaviors. Based on the characteristics of its base\nmaterial, MC is flexible in modifying unit stiffness and convenient in molding\nfabrication. However, recent studies on magnetic continuum robots have\nprimarily focused on one or two design parameters, limiting the development of\na comprehensive magnetic continuum bending model. In this work, we constructed\ngraded-stiffness MCs (GMCs) and developed a numerical model for GMCs' bending\nperformance, incorporating four key parameters that determine their\nperformance. The simulated bending results were validated with real bending\nexperiments in four different categories: varying magnetic field,\ncross-section, unit stiffness, and unit length. The graded-stiffness design\nstrategy applied to GMCs prevents sharp bending at the fixed end and results in\na more circular curvature. We also trained an expansion model for GMCs' bending\nperformance that is highly efficient and accurate compared to the simulation\nprocess. An extensive library of bending prediction for GMCs was built using\nthe trained model.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u78c1\u8bf1\u5bfc\u53d8\u5f62\u6750\u6599\uff08MIMMs\uff09\u7684\u68af\u5ea6\u521a\u5ea6\u78c1\u8f6f\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff08GMCs\uff09\uff0c\u5f00\u53d1\u4e86\u5305\u542b\u56db\u4e2a\u5173\u952e\u53c2\u6570\u7684\u6570\u503c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5f2f\u66f2\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u78c1\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5355\u4e00\u6216\u4e24\u4e2a\u8bbe\u8ba1\u53c2\u6570\uff0c\u7f3a\u4e4f\u5168\u9762\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u68af\u5ea6\u521a\u5ea6GMCs\uff0c\u5f00\u53d1\u6570\u503c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5f2f\u66f2\u6027\u80fd\u3002", "result": "\u68af\u5ea6\u521a\u5ea6\u8bbe\u8ba1\u907f\u514d\u4e86\u56fa\u5b9a\u7aef\u5c16\u9510\u5f2f\u66f2\uff0c\u5f62\u6210\u66f4\u5706\u6ed1\u66f2\u7387\uff1b\u8bad\u7ec3\u7684\u9ad8\u6548\u6269\u5c55\u6a21\u578b\u4f18\u4e8e\u6a21\u62df\u8fc7\u7a0b\u3002", "conclusion": "GMCs\u7684\u68af\u5ea6\u521a\u5ea6\u8bbe\u8ba1\u548c\u9ad8\u6548\u6a21\u578b\u4e3a\u78c1\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6027\u80fd\u9884\u6d4b\u548c\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2508.01300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01300", "abs": "https://arxiv.org/abs/2508.01300", "authors": ["Ma'ayan Armony", "Albert Mero\u00f1o-Pe\u00f1uela", "Gerard Canal"], "title": "How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective", "comment": null, "summary": "The reasoning and planning abilities of Large Language Models (LLMs) have\nbeen a frequent topic of discussion in recent years. Their ability to take\nunstructured planning problems as input has made LLMs' integration into AI\nplanning an area of interest. Nevertheless, LLMs are still not reliable as\nplanners, with the generated plans often containing mistaken or hallucinated\nactions. Existing benchmarking and evaluation methods investigate planning with\nLLMs, focusing primarily on success rate as a quality indicator in various\nplanning tasks, such as validating plans or planning in relaxed conditions. In\nthis paper, we approach planning with LLMs as a natural language processing\n(NLP) task, given that LLMs are NLP models themselves. We propose a recovery\npipeline consisting of an NLP-based evaluation of the generated plans, along\nwith three stages to recover the plans through NLP manipulation of the\nLLM-generated plans, and eventually complete the plan using a symbolic planner.\nThis pipeline provides a holistic analysis of LLM capabilities in the context\nof AI task planning, enabling a broader understanding of the quality of invalid\nplans. Our findings reveal no clear evidence of underlying reasoning during\nplan generation, and that a pipeline comprising an NLP-based analysis of the\nplans, followed by a recovery mechanism, still falls short of the quality and\nreliability of classical planners. On average, only the first 2.65 actions of\nthe plan are executable, with the average length of symbolically generated\nplans being 8.4 actions. The pipeline still improves action quality and\nincreases the overall success rate from 21.9% to 27.5%.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7684\u6062\u590d\u6d41\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u8ba1\u5212\u7684\u8d28\u91cf\u548c\u6210\u529f\u7387\u3002", "motivation": "LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u5e38\u751f\u6210\u9519\u8bef\u6216\u865a\u6784\u7684\u52a8\u4f5c\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6210\u529f\u7387\uff0c\u7f3a\u4e4f\u5bf9\u8ba1\u5212\u8d28\u91cf\u7684\u5168\u9762\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542bNLP\u8bc4\u4f30\u548c\u4e09\u9636\u6bb5\u6062\u590d\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u7b26\u53f7\u89c4\u5212\u5668\u5b8c\u6210\u8ba1\u5212\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u8ba1\u5212\u7f3a\u4e4f\u5e95\u5c42\u63a8\u7406\uff0c\u6062\u590d\u6d41\u7a0b\u4ec5\u5c06\u6210\u529f\u7387\u4ece21.9%\u63d0\u5347\u81f327.5%\uff0c\u4e14\u5e73\u5747\u4ec5\u524d2.65\u4e2a\u52a8\u4f5c\u53ef\u6267\u884c\u3002", "conclusion": "\u5c3d\u7ba1\u6062\u590d\u6d41\u7a0b\u6709\u6240\u6539\u8fdb\uff0cLLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u4ecd\u4e0d\u53ca\u4f20\u7edf\u89c4\u5212\u5668\u3002"}}
{"id": "2508.00961", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00961", "abs": "https://arxiv.org/abs/2508.00961", "authors": ["Xiang Li", "Penglei Sun", "Wanyun Zhou", "Zikai Wei", "Yongqi Zhang", "Xiaowen Chu"], "title": "FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph", "comment": null, "summary": "Individual investors are significantly outnumbered and disadvantaged in\nfinancial markets, overwhelmed by abundant information and lacking professional\nanalysis. Equity research reports stand out as crucial resources, offering\nvaluable insights. By leveraging these reports, large language models (LLMs)\ncan enhance investors' decision-making capabilities and strengthen financial\nanalysis. However, two key challenges limit their effectiveness: (1) the rapid\nevolution of market events often outpaces the slow update cycles of existing\nknowledge bases, (2) the long-form and unstructured nature of financial reports\nfurther hinders timely and context-aware integration by LLMs. To address these\nchallenges, we tackle both data and methodological aspects. First, we introduce\nthe Event-Enhanced Automated Construction of Financial Knowledge Graph\n(FinKario), a dataset comprising over 305,360 entities, 9,625 relational\ntriples, and 19 distinct relation types. FinKario automatically integrates\nreal-time company fundamentals and market events through prompt-driven\nextraction guided by professional institutional templates, providing structured\nand accessible financial insights for LLMs. Additionally, we propose a\nTwo-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the\nretrieval of evolving, large-scale financial knowledge to ensure efficient and\nprecise data access. Extensive experiments show that FinKario with FinKario-RAG\nachieves superior stock trend prediction accuracy, outperforming financial LLMs\nby 18.81% and institutional strategies by 17.85% on average in backtesting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFinKario\u6570\u636e\u96c6\u548cFinKario-RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u6574\u5408\u5e02\u573a\u4e8b\u4ef6\u548c\u7ed3\u6784\u5316\u91d1\u878d\u77e5\u8bc6\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5206\u6790\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e2a\u4f53\u6295\u8d44\u8005\u5728\u91d1\u878d\u5e02\u573a\u4e2d\u5904\u4e8e\u52a3\u52bf\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u5206\u6790\u5de5\u5177\u3002\u73b0\u6709\u77e5\u8bc6\u5e93\u66f4\u65b0\u6162\u4e14\u91d1\u878d\u62a5\u544a\u975e\u7ed3\u6784\u5316\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "1. \u6784\u5efaFinKario\u6570\u636e\u96c6\uff0c\u5305\u542b\u5b9e\u65f6\u516c\u53f8\u57fa\u672c\u9762\u548c\u5e02\u573a\u4e8b\u4ef6\uff1b2. \u63d0\u51faFinKario-RAG\uff0c\u4f18\u5316\u91d1\u878d\u77e5\u8bc6\u68c0\u7d22\u3002", "result": "FinKario\u4e0eFinKario-RAG\u5728\u80a1\u7968\u8d8b\u52bf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8d85\u8d8a\u91d1\u878d\u8bed\u8a00\u6a21\u578b\u548c\u673a\u6784\u7b56\u756518.81%\u548c17.85%\u3002", "conclusion": "FinKario\u548cFinKario-RAG\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u878d\u77e5\u8bc6\u66f4\u65b0\u6162\u548c\u975e\u7ed3\u6784\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6295\u8d44\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2508.01476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01476", "abs": "https://arxiv.org/abs/2508.01476", "authors": ["Arindam Khanda", "Anurag Satpathy", "Amit Jha", "Sajal K. Das"], "title": "CARGO: A Co-Optimization Framework for EV Charging and Routing in Goods Delivery Logistics", "comment": null, "summary": "With growing interest in sustainable logistics, electric vehicle (EV)-based\ndeliveries offer a promising alternative for urban distribution. However, EVs\nface challenges due to their limited battery capacity, requiring careful\nplanning for recharging. This depends on factors such as the charging point\n(CP) availability, cost, proximity, and vehicles' state of charge (SoC). We\npropose CARGO, a framework addressing the EV-based delivery route planning\nproblem (EDRP), which jointly optimizes route planning and charging for\ndeliveries within time windows. After proving the problem's NP-hardness, we\npropose a mixed integer linear programming (MILP)-based exact solution and a\ncomputationally efficient heuristic method. Using real-world datasets, we\nevaluate our methods by comparing the heuristic to the MILP solution, and\nbenchmarking it against baseline strategies, Earliest Deadline First (EDF) and\nNearest Delivery First (NDF). The results show up to 39% and 22% reductions in\nthe charging cost over EDF and NDF, respectively, while completing comparable\ndeliveries.", "AI": {"tldr": "CARGO\u6846\u67b6\u4f18\u5316\u7535\u52a8\u8f66\u8f86\u914d\u9001\u8def\u7ebf\u548c\u5145\u7535\u8ba1\u5212\uff0c\u51cf\u5c11\u5145\u7535\u6210\u672c\u3002", "motivation": "\u7535\u52a8\u8f66\u8f86\u56e0\u7535\u6c60\u5bb9\u91cf\u6709\u9650\uff0c\u9700\u4f18\u5316\u5145\u7535\u8ba1\u5212\u4ee5\u63d0\u5347\u57ce\u5e02\u914d\u9001\u6548\u7387\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u7cbe\u786e\u89e3\u548c\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u76f8\u6bd4\u57fa\u51c6\u7b56\u7565\uff0c\u5145\u7535\u6210\u672c\u964d\u4f4e39%\u548c22%\u3002", "conclusion": "CARGO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u7535\u52a8\u8f66\u8f86\u914d\u9001\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2508.01210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01210", "abs": "https://arxiv.org/abs/2508.01210", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Yue", "Nuoran Li", "Chao Sun"], "title": "RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification", "comment": null, "summary": "Acquiring the road surface conditions in advance based on visual technologies\nprovides effective information for the planning and control system of\nautonomous vehicles, thus improving the safety and driving comfort of the\nvehicles. Recently, the Mamba architecture based on state-space models has\nshown remarkable performance in visual processing tasks, benefiting from the\nefficient global receptive field. However, existing Mamba architectures\nstruggle to achieve state-of-the-art visual road surface classification due to\ntheir lack of effective extraction of the local texture of the road surface. In\nthis paper, we explore for the first time the potential of visual Mamba\narchitectures for road surface classification task and propose a method that\neffectively combines local and global perception, called RoadMamba.\nSpecifically, we utilize the Dual State Space Model (DualSSM) to effectively\nextract the global semantics and local texture of the road surface and decode\nand fuse the dual features through the Dual Attention Fusion (DAF). In\naddition, we propose a dual auxiliary loss to explicitly constrain dual\nbranches, preventing the network from relying only on global semantic\ninformation from the deep large receptive field and ignoring the local texture.\nThe proposed RoadMamba achieves the state-of-the-art performance in experiments\non a large-scale road surface classification dataset containing 1 million\nsamples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoadMamba\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u611f\u77e5\u6280\u672f\uff0c\u7528\u4e8e\u9053\u8def\u8868\u9762\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u89c6\u89c9\u6280\u672f\u63d0\u524d\u83b7\u53d6\u9053\u8def\u8868\u9762\u6761\u4ef6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u89c4\u5212\u4e0e\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u9a7e\u9a76\u8212\u9002\u6027\u3002", "method": "\u5229\u7528\u53cc\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08DualSSM\uff09\u63d0\u53d6\u9053\u8def\u8868\u9762\u7684\u5168\u5c40\u8bed\u4e49\u548c\u5c40\u90e8\u7eb9\u7406\uff0c\u5e76\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u878d\u5408\uff08DAF\uff09\u89e3\u7801\u548c\u878d\u5408\u53cc\u91cd\u7279\u5f81\uff1b\u63d0\u51fa\u53cc\u8f85\u52a9\u635f\u5931\u51fd\u6570\u4ee5\u7ea6\u675f\u7f51\u7edc\u3002", "result": "\u5728\u5305\u542b100\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u9053\u8def\u8868\u9762\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0cRoadMamba\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "RoadMamba\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9053\u8def\u8868\u9762\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.01495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01495", "abs": "https://arxiv.org/abs/2508.01495", "authors": ["Jingtian Yan", "Stephen F. Smith", "Jiaoyang Li"], "title": "WinkTPG: An Execution Framework for Multi-Agent Path Finding Using Temporal Reasoning", "comment": null, "summary": "Planning collision-free paths for a large group of agents is a challenging\nproblem with numerous real-world applications. While recent advances in\nMulti-Agent Path Finding (MAPF) have shown promising progress, standard MAPF\nalgorithms rely on simplified kinodynamic models, preventing agents from\ndirectly following the generated MAPF plan. To bridge this gap, we propose\nkinodynamic Temporal Plan Graph Planning (kTPG), a multi-agent speed\noptimization algorithm that efficiently refines a MAPF plan into a\nkinodynamically feasible plan while accounting for uncertainties and preserving\ncollision-freeness. Building on kTPG, we propose Windowed kTPG (WinkTPG), a\nMAPF execution framework that incrementally refines MAPF plans using a\nwindow-based mechanism, dynamically incorporating agent information during\nexecution to reduce uncertainty. Experiments show that WinkTPG can generate\nspeed profiles for up to 1,000 agents in 1 second and improves solution quality\nby up to 51.7% over existing MAPF execution methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3akTPG\u7684\u591a\u667a\u80fd\u4f53\u901f\u5ea6\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5176\u6269\u5c55\u6846\u67b6WinkTPG\uff0c\u7528\u4e8e\u5c06MAPF\u8ba1\u5212\u4f18\u5316\u4e3a\u52a8\u529b\u5b66\u53ef\u884c\u7684\u8def\u5f84\uff0c\u5e76\u52a8\u6001\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u52a8\u529b\u5b66\u6a21\u578b\u7b80\u5316\u5bfc\u81f4\u7684\u8ba1\u5212\u4e0d\u53ef\u884c\u95ee\u9898\uff0c\u5e76\u51cf\u5c11\u6267\u884c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u57fa\u4e8ekTPG\u7b97\u6cd5\uff0c\u901a\u8fc7\u7a97\u53e3\u673a\u5236\u52a8\u6001\u4f18\u5316MAPF\u8ba1\u5212\uff0c\u7ed3\u5408\u6267\u884c\u4e2d\u7684\u667a\u80fd\u4f53\u4fe1\u606f\u3002", "result": "WinkTPG\u80fd\u57281\u79d2\u5185\u4e3a1000\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u901f\u5ea6\u66f2\u7ebf\uff0c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u534751.7%\u3002", "conclusion": "WinkTPG\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2508.01223", "categories": ["cs.CV", "68T10", "I.4.6"], "pdf": "https://arxiv.org/pdf/2508.01223", "abs": "https://arxiv.org/abs/2508.01223", "authors": ["Changqing Xu", "Guoqing Sun", "Yi Liu", "Xinfang Liao", "Yintang Yang"], "title": "ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference", "comment": "8 pages, 3 figures, submitted to AAAI 2026", "summary": "Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training\nby reconstructing forward activations during backpropagation, but suffer from\nhigh latency due to strictly sequential computation. To overcome this\nlimitation, we propose ParaRevSNN, a parallel reversible SNN architecture that\ndecouples sequential dependencies between reversible blocks while preserving\nreversibility. This design enables inter-block parallelism, significantly\naccelerating training and inference while retaining the memory-saving benefits\nof reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128\nGesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard\nRevSNNs, while reducing training time by up to 35.2\\% and inference time to\n18.15\\%, making it well-suited for deployment in resource-constrained\nscenarios.", "AI": {"tldr": "ParaRevSNN\u662f\u4e00\u79cd\u5e76\u884c\u53ef\u9006SNN\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u53ef\u9006\u5757\u95f4\u7684\u987a\u5e8f\u4f9d\u8d56\uff0c\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u53ef\u9006SNN\u56e0\u4e25\u683c\u987a\u5e8f\u8ba1\u7b97\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51faParaRevSNN\uff0c\u89e3\u8026\u53ef\u9006\u5757\u95f4\u7684\u987a\u5e8f\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5757\u95f4\u5e76\u884c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u6807\u51c6RevSNN\u7684\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1135.2%\uff0c\u63a8\u7406\u65f6\u95f4\u964d\u81f318.15%\u3002", "conclusion": "ParaRevSNN\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.01700", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01700", "abs": "https://arxiv.org/abs/2508.01700", "authors": ["Zhihao Shuai", "Boyan Li", "Siyu Yan", "Yuyu Luo", "Weikai Yang"], "title": "DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning", "comment": null, "summary": "Although data visualization is powerful for revealing patterns and\ncommunicating insights, creating effective visualizations requires familiarity\nwith authoring tools and often disrupts the analysis flow. While large language\nmodels show promise for automatically converting analysis intent into\nvisualizations, existing methods function as black boxes without transparent\nreasoning processes, which prevents users from understanding design rationales\nand refining suboptimal outputs. To bridge this gap, we propose integrating\nChain-of-Thought (CoT) reasoning into the Natural Language to Visualization\n(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for\nNL2VIS and develop an automatic pipeline to equip existing datasets with\nstructured reasoning steps. Second, we introduce nvBench-CoT, a specialized\ndataset capturing detailed step-by-step reasoning from ambiguous natural\nlanguage descriptions to finalized visualizations, which enables\nstate-of-the-art performance when used for model fine-tuning. Third, we develop\nDeepVIS, an interactive visual interface that tightly integrates with the CoT\nreasoning process, allowing users to inspect reasoning steps, identify errors,\nand make targeted adjustments to improve visualization outcomes. Quantitative\nbenchmark evaluations, two use cases, and a user study collectively demonstrate\nthat our CoT framework effectively enhances NL2VIS quality while providing\ninsightful reasoning steps to users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\u5f15\u5165\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u89c6\u5316\uff08NL2VIS\uff09\u6d41\u7a0b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u7528\u6237\u65e0\u6cd5\u7406\u89e3\u8bbe\u8ba1\u903b\u8f91\u6216\u4f18\u5316\u8f93\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u6e05\u6670\u63a8\u7406\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86CoT\u63a8\u7406\u6d41\u7a0b\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86nvBench-CoT\u6570\u636e\u96c6\u548cDeepVIS\u4ea4\u4e92\u754c\u9762\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NL2VIS\u7684\u8d28\u91cf\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u7406\u89e3\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "conclusion": "CoT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86NL2VIS\u7684\u900f\u660e\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.01724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01724", "abs": "https://arxiv.org/abs/2508.01724", "authors": ["Shijie Cao", "Yuan Yuan"], "title": "ReflecSched: Solving Dynamic Flexible Job-Shop Scheduling via LLM-Powered Hierarchical Reflection", "comment": null, "summary": "Dynamic Flexible Job-Shop Scheduling (DFJSP) is an NP-hard problem challenged\nby real-time event adaptation and complex machine routing. While traditional\ndispatching rules are efficient but rigid, deep learning approaches are opaque\nand require intricate feature engineering. Large Language Models (LLMs) promise\nadaptive reasoning without this engineering overhead, yet we find their direct\napplication is suboptimal. Baseline LLMs suffer from three key pitfalls: the\nlong-context paradox, where crucial data is underutilized; an underutilization\nof expert heuristics; and myopic decision-making. To address this, we propose\nReflecSched, a framework that empowers the LLM beyond a direct scheduler by\nequipping it with a strategic analysis capability. ReflecSched tasks the LLM to\nanalyze heuristic-driven simulations across multiple planning horizons and\ndistill them into a concise, natural-language summary termed ``Strategic\nExperience''. This summary is then integrated into the prompt of a final\ndecision-making module, guiding it to produce non-myopic actions. Experiments\nshow that ReflecSched not only statistically significantly outperforms direct\nLLM baselines, securing a 71.35\\% Win Rate and a 2.755\\% Relative Percentage\nDeviation reduction, but also surpasses the performance of all individual\nheuristics evaluated, all while demonstrably mitigating the three identified\npitfalls. Additionally, ReflecSched performs on par with the best heuristic\ntailored to each instance across all problem cases.", "AI": {"tldr": "ReflecSched\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u7684\u6218\u7565\u5206\u6790\u80fd\u529b\uff0c\u89e3\u51b3\u4e86DFJSP\u95ee\u9898\u4e2dLLM\u76f4\u63a5\u5e94\u7528\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8c03\u5ea6\u89c4\u5219\u6548\u7387\u9ad8\u4f46\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7279\u5f81\u5de5\u7a0b\uff0c\u800cLLM\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u957f\u4e0a\u4e0b\u6587\u6096\u8bba\u3001\u4e13\u5bb6\u542f\u53d1\u5f0f\u5229\u7528\u4e0d\u8db3\u548c\u77ed\u89c6\u51b3\u7b56\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faReflecSched\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5206\u6790\u542f\u53d1\u5f0f\u6a21\u62df\u7684\u591a\u89c4\u5212\u89c6\u91ce\uff0c\u751f\u6210\u201c\u6218\u7565\u7ecf\u9a8c\u201d\u6458\u8981\uff0c\u5e76\u96c6\u6210\u5230\u6700\u7ec8\u51b3\u7b56\u6a21\u5757\u4e2d\u3002", "result": "ReflecSched\u663e\u8457\u4f18\u4e8e\u76f4\u63a5LLM\u57fa\u7ebf\uff0871.35%\u80dc\u7387\uff0c2.755%\u76f8\u5bf9\u504f\u5dee\u964d\u4f4e\uff09\uff0c\u5e76\u8d85\u8d8a\u6240\u6709\u8bc4\u4f30\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "ReflecSched\u901a\u8fc7\u6218\u7565\u5206\u6790\u80fd\u529b\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728DFJSP\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6027\u80fd\u4e0e\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd5\u76f8\u5f53\u3002"}}
{"id": "2508.01167", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01167", "abs": "https://arxiv.org/abs/2508.01167", "authors": ["Hongquan Zhang", "Jingyu Gong", "Zhizhong Zhang", "Xin Tan", "Yanyun Qu", "Yuan Xie"], "title": "T2S: Tokenized Skill Scaling for Lifelong Imitation Learning", "comment": null, "summary": "The main challenge in lifelong imitation learning lies in the balance between\nmitigating catastrophic forgetting of previous skills while maintaining\nsufficient capacity for acquiring new ones. However, current approaches\ntypically address these aspects in isolation, overlooking their internal\ncorrelation in lifelong skill acquisition. We address this limitation with a\nunified framework named Tokenized Skill Scaling (T2S). Specifically, by\ntokenizing the model parameters, the linear parameter mapping of the\ntraditional transformer is transformed into cross-attention between input and\nlearnable tokens, thereby enhancing model scalability through the easy\nextension of new tokens. Additionally, we introduce language-guided skill\nscaling to transfer knowledge across tasks efficiently and avoid linearly\ngrowing parameters. Extensive experiments across diverse tasks demonstrate that\nT2S: 1) effectively prevents catastrophic forgetting (achieving an average NBT\nof 1.0% across the three LIBERO task suites), 2) excels in new skill scaling\nwith minimal increases in trainable parameters (needing only 8.0% trainable\ntokens in an average of lifelong tasks), and 3) enables efficient knowledge\ntransfer between tasks (achieving an average FWT of 77.7% across the three\nLIBERO task suites), offering a promising solution for lifelong imitation\nlearning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTokenized Skill Scaling (T2S)\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u6807\u8bb0\u5316\u548c\u8bed\u8a00\u5f15\u5bfc\u6280\u80fd\u6269\u5c55\uff0c\u89e3\u51b3\u4e86\u7ec8\u8eab\u6a21\u4eff\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u548c\u65b0\u6280\u80fd\u5b66\u4e60\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u7ec8\u8eab\u6a21\u4eff\u5b66\u4e60\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5e73\u8861\u707e\u96be\u6027\u9057\u5fd8\u548c\u65b0\u6280\u80fd\u5b66\u4e60\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u7684\u5185\u5728\u5173\u8054\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u6807\u8bb0\u5316\u5c06\u4f20\u7edfTransformer\u7684\u7ebf\u6027\u53c2\u6570\u6620\u5c04\u8f6c\u6362\u4e3a\u8f93\u5165\u4e0e\u53ef\u5b66\u4e60\u6807\u8bb0\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5e76\u5f15\u5165\u8bed\u8a00\u5f15\u5bfc\u6280\u80fd\u6269\u5c55\u4ee5\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT2S\u6709\u6548\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff08\u5e73\u5747NBT\u4e3a1.0%\uff09\uff0c\u65b0\u6280\u80fd\u6269\u5c55\u8868\u73b0\u4f18\u5f02\uff08\u4ec5\u97008.0%\u53ef\u8bad\u7ec3\u6807\u8bb0\uff09\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\uff08\u5e73\u5747FWT\u4e3a77.7%\uff09\u3002", "conclusion": "T2S\u4e3a\u7ec8\u8eab\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01175", "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA", "68T07 (Primary) 42A10, 41A30, 65D15 (Secondary)", "I.5.1; G.1.2; G.1.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.01175", "abs": "https://arxiv.org/abs/2508.01175", "authors": ["Shiko Kudo"], "title": "From Taylor Series to Fourier Synthesis: The Periodic Linear Unit", "comment": "15 pages, 5 figures, for associated raw example files and the code\n  repository, see https://github.com/bill13579/plu_activation", "summary": "The dominant paradigm in modern neural networks relies on simple,\nmonotonically-increasing activation functions like ReLU. While effective, this\nparadigm necessitates large, massively-parameterized models to approximate\ncomplex functions. In this paper, we introduce the Periodic Linear Unit (PLU),\na learnable sine-wave based activation with periodic non-monotonicity. PLU is\ndesigned for maximum expressive power and numerical stability, achieved through\nits formulation and a paired innovation we term Repulsive Reparameterization,\nwhich prevents the activation from collapsing into a non-expressive linear\nfunction. We demonstrate that a minimal MLP with only two PLU neurons can solve\nthe spiral classification task, a feat impossible for equivalent networks using\nstandard activations. This suggests a paradigm shift from networks as piecewise\nTaylor-like approximators to powerful Fourier-like function synthesizers,\nachieving exponential gains in parameter efficiency by placing intelligence in\nthe neuron itself.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPLU\u7684\u5468\u671f\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u975e\u5355\u8c03\u6027\u548c\u53ef\u5b66\u4e60\u6027\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\u9700\u8981\u5927\u91cf\u53c2\u6570\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\uff0c\u9700\u8981\u5927\u89c4\u6a21\u53c2\u6570\u5316\u6a21\u578b\u6765\u903c\u8fd1\u590d\u6742\u51fd\u6570\u3002PLU\u65e8\u5728\u901a\u8fc7\u5468\u671f\u6027\u975e\u5355\u8c03\u6027\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u548c\u53c2\u6570\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5468\u671f\u6027\u7ebf\u6027\u5355\u5143\uff08PLU\uff09\uff0c\u7ed3\u5408\u6392\u65a5\u6027\u91cd\u53c2\u6570\u5316\u6280\u672f\uff0c\u9632\u6b62\u6fc0\u6d3b\u51fd\u6570\u9000\u5316\u4e3a\u7ebf\u6027\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u542b\u4e24\u4e2aPLU\u795e\u7ecf\u5143\u7684\u6700\u5c0fMLP\u80fd\u89e3\u51b3\u87ba\u65cb\u5206\u7c7b\u4efb\u52a1\uff0c\u800c\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\u65e0\u6cd5\u5b8c\u6210\u3002", "conclusion": "PLU\u5c55\u793a\u4e86\u4ece\u5206\u6bb5\u6cf0\u52d2\u8fd1\u4f3c\u5230\u5085\u91cc\u53f6\u5f0f\u51fd\u6570\u5408\u6210\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u6548\u7387\u3002"}}
{"id": "2508.01844", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01844", "abs": "https://arxiv.org/abs/2508.01844", "authors": ["Xinkai Zou", "Xuan Jiang", "Ruikai Huang", "Haoze He", "Parv Kapoor", "Jiahua Zhao"], "title": "CloudAnoAgent: Anomaly Detection for Cloud Sites via LLM Agent with Neuro-Symbolic Mechanism", "comment": null, "summary": "Anomaly detection in cloud sites remains a critical yet challenging task.\nExisting approaches that rely solely on metric data often suffer from high\nfalse positive rates (FPR) due to data imbalance between normal and anomalous\nevents, leading to significant operational overhead for system reliance\nengineers. Recent advances in large language models (LLMs) offer new\nopportunities for integrating metrics with log data, enabling more accurate and\ninterpretable anomaly detection. In this paper, we propose CloudAnoAgent, the\nfirst neuro-symbolic LLM-based agent for anomaly detection in cloud\nenvironments. CloudAnoAgent jointly processes structured metrics and textual\nlog data in a unified pipeline, leveraging symbolic verification to validate\ndetection hypotheses and generate structured anomaly reports. To support\nsystematic evaluation, we introduce CloudAnoBench, the first benchmark that\nprovides LLM-generated paired metrics and log data with fine-grained anomaly\nbehavior annotations, filling a critical gap in existing datasets. Experimental\nresults demonstrate that CloudAnoAgent improves anomaly classification accuracy\nby 46.36% and 36.67% on average and reduces the FPR by 36.67% and 33.89% on\naverage over traditional baselines and LLM-only baseline, with a boost on\nanomaly type detection accuracy by 12.8% compared to vanilla LLM prompting.\nThese results demonstrate the strengths of our approach in improving detection\naccuracy, reducing false positives, and enhancing interpretability, thereby\nsupporting practical deployment in enterprise cloud environments.", "AI": {"tldr": "\u63d0\u51faCloudAnoAgent\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7LLM\u7684\u4ee3\u7406\uff0c\u7528\u4e8e\u4e91\u73af\u5883\u5f02\u5e38\u68c0\u6d4b\uff0c\u7ed3\u5408\u6307\u6807\u548c\u65e5\u5fd7\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u8bef\u62a5\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6307\u6807\u6570\u636e\uff0c\u8bef\u62a5\u7387\u9ad8\u4e14\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u8fd0\u7ef4\u8d1f\u62c5\u91cd\u3002LLM\u7684\u8fdb\u5c55\u4e3a\u7ed3\u5408\u6307\u6807\u548c\u65e5\u5fd7\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51faCloudAnoAgent\uff0c\u8054\u5408\u5904\u7406\u7ed3\u6784\u5316\u6307\u6807\u548c\u6587\u672c\u65e5\u5fd7\u6570\u636e\uff0c\u5229\u7528\u7b26\u53f7\u9a8c\u8bc1\u751f\u6210\u7ed3\u6784\u5316\u5f02\u5e38\u62a5\u544a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCloudAnoAgent\u5e73\u5747\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u738746.36%\u548c36.67%\uff0c\u964d\u4f4e\u8bef\u62a5\u738736.67%\u548c33.89%\uff0c\u5f02\u5e38\u7c7b\u578b\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534712.8%\u3002", "conclusion": "CloudAnoAgent\u901a\u8fc7\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u51cf\u5c11\u8bef\u62a5\u548c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u4f01\u4e1a\u4e91\u73af\u5883\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2508.02629", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02629", "abs": "https://arxiv.org/abs/2508.02629", "authors": ["Yibin Liu", "Zhixuan Liang", "Zanxin Chen", "Tianxing Chen", "Mengkang Hu", "Wanxi Dong", "Congsheng Xu", "Zhaoming Han", "Yusen Qin", "Yao Mu"], "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents", "comment": "Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.", "AI": {"tldr": "HyCodePolicy\u662f\u4e00\u79cd\u6df7\u5408\u8bed\u8a00\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee3\u7801\u5408\u6210\u3001\u51e0\u4f55\u57fa\u7840\u3001\u611f\u77e5\u76d1\u63a7\u548c\u8fed\u4ee3\u4fee\u590d\uff0c\u4e3a\u5177\u8eab\u4ee3\u7406\u63d0\u4f9b\u95ed\u73af\u7f16\u7a0b\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u81ea\u9002\u5e94\u76d1\u63a7\u7b56\u7565\u6267\u884c\u548c\u4fee\u590d\u4ee3\u7801\u7684\u673a\u5236\uff0c\u9650\u5236\u4e86\u5177\u8eab\u4ee3\u7406\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "method": "HyCodePolicy\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\uff0c\u751f\u6210\u57fa\u4e8e\u51e0\u4f55\u57fa\u5143\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76d1\u63a7\u6267\u884c\u5931\u8d25\uff0c\u7ed3\u5408\u6267\u884c\u8f68\u8ff9\u548c\u611f\u77e5\u53cd\u9988\u4fee\u590d\u7a0b\u5e8f\u3002", "result": "HyCodePolicy\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "HyCodePolicy\u4e3a\u5c06\u591a\u6a21\u6001\u63a8\u7406\u96c6\u6210\u5230\u81ea\u4e3b\u51b3\u7b56\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7b56\u7565\u3002"}}
{"id": "2508.01287", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01287", "abs": "https://arxiv.org/abs/2508.01287", "authors": ["Micah Rentschler", "Jesse Roberts"], "title": "Exploitation Is All You Need... for Exploration", "comment": null, "summary": "Ensuring sufficient exploration is a central challenge when training\nmeta-reinforcement learning (meta-RL) agents to solve novel environments.\nConventional solutions to the exploration-exploitation dilemma inject explicit\nincentives such as randomization, uncertainty bonuses, or intrinsic rewards to\nencourage exploration. In this work, we hypothesize that an agent trained\nsolely to maximize a greedy (exploitation-only) objective can nonetheless\nexhibit emergent exploratory behavior, provided three conditions are met: (1)\nRecurring Environmental Structure, where the environment features repeatable\nregularities that allow past experience to inform future choices; (2) Agent\nMemory, enabling the agent to retain and utilize historical interaction data;\nand (3) Long-Horizon Credit Assignment, where learning propagates returns over\na time frame sufficient for the delayed benefits of exploration to inform\ncurrent decisions. Through experiments in stochastic multi-armed bandits and\ntemporally extended gridworlds, we observe that, when both structure and memory\nare present, a policy trained on a strictly greedy objective exhibits\ninformation-seeking exploratory behavior. We further demonstrate, through\ncontrolled ablations, that emergent exploration vanishes if either\nenvironmental structure or agent memory is absent (Conditions 1 & 2).\nSurprisingly, removing long-horizon credit assignment (Condition 3) does not\nalways prevent emergent exploration-a result we attribute to the\npseudo-Thompson Sampling effect. These findings suggest that, under the right\nprerequisites, exploration and exploitation need not be treated as orthogonal\nobjectives but can emerge from a unified reward-maximization process.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u4ec5\u4ee5\u8d2a\u5a6a\u76ee\u6807\u8bad\u7ec3\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4e5f\u80fd\u8868\u73b0\u51fa\u63a2\u7d22\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u8d2a\u5a6a\u76ee\u6807\u662f\u5426\u80fd\u81ea\u7136\u4ea7\u751f\u63a2\u7d22\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e09\u4e2a\u6761\u4ef6\uff08\u73af\u5883\u7ed3\u6784\u3001\u4ee3\u7406\u8bb0\u5fc6\u3001\u957f\u671f\u4fe1\u7528\u5206\u914d\uff09\u5bf9\u63a2\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u5728\u6ee1\u8db3\u73af\u5883\u7ed3\u6784\u548c\u4ee3\u7406\u8bb0\u5fc6\u7684\u6761\u4ef6\u4e0b\uff0c\u8d2a\u5a6a\u76ee\u6807\u8bad\u7ec3\u7684\u4ee3\u7406\u8868\u73b0\u51fa\u63a2\u7d22\u884c\u4e3a\u3002", "conclusion": "\u63a2\u7d22\u884c\u4e3a\u53ef\u4ee5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u81ea\u7136\u4ea7\u751f\uff0c\u65e0\u9700\u663e\u5f0f\u6fc0\u52b1\u3002"}}
{"id": "2508.02238", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02238", "abs": "https://arxiv.org/abs/2508.02238", "authors": ["Xin Dong", "Yiwei Zhang", "Yangjie Cui", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception", "comment": "A supplementary video is available at https://youtu.be/tLzXjXVRkVg", "summary": "Event cameras offer significant advantages, including a wide dynamic range,\nhigh temporal resolution, and immunity to motion blur, making them highly\npromising for addressing challenging visual conditions. Extracting and\nutilizing effective information from asynchronous event streams is essential\nfor the onboard implementation of event cameras. In this paper, we propose a\nstreamlined event-based intensity reconstruction scheme, event-based single\nintegration (ESI), to address such implementation challenges. This method\nguarantees the portability of conventional frame-based vision methods to\nevent-based scenarios and maintains the intrinsic advantages of event cameras.\nThe ESI approach reconstructs intensity images by performing a single\nintegration of the event streams combined with an enhanced decay algorithm.\nSuch a method enables real-time intensity reconstruction at a high frame rate,\ntypically 100 FPS. Furthermore, the relatively low computation load of ESI fits\nonboard implementation suitably, such as in UAV-based visual tracking\nscenarios. Extensive experiments have been conducted to evaluate the\nperformance comparison of ESI and state-of-the-art algorithms. Compared to\nstate-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency\nimprovements, superior reconstruction quality, and a high frame rate. As a\nresult, ESI enhances UAV onboard perception significantly under visual\nadversary surroundings. In-flight tests, ESI demonstrates effective performance\nfor UAV onboard visual tracking under extremely low illumination\nconditions(2-10lux), whereas other comparative algorithms fail due to\ninsufficient frame rate, poor image quality, or limited real-time performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5355\u79ef\u5206\uff08ESI\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u9ad8\u5e27\u7387\u5f3a\u5ea6\u56fe\u50cf\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u89c6\u89c9\u8ddf\u8e2a\u7b49\u573a\u666f\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7b49\u4f18\u52bf\uff0c\u4f46\u5982\u4f55\u4ece\u5f02\u6b65\u4e8b\u4ef6\u6d41\u4e2d\u63d0\u53d6\u6709\u6548\u4fe1\u606f\u5e76\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u662f\u5173\u952e\u6311\u6218\u3002", "method": "ESI\u901a\u8fc7\u5355\u6b21\u79ef\u5206\u4e8b\u4ef6\u6d41\u7ed3\u5408\u589e\u5f3a\u8870\u51cf\u7b97\u6cd5\u91cd\u5efa\u5f3a\u5ea6\u56fe\u50cf\uff0c\u5b9e\u73b0\u9ad8\u5e27\u7387\uff08100 FPS\uff09\u548c\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "ESI\u5728\u8fd0\u884c\u65f6\u6548\u7387\u3001\u91cd\u5efa\u8d28\u91cf\u548c\u5e27\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "ESI\u65b9\u6cd5\u5728\u6781\u7aef\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u673a\u8f7d\u89c6\u89c9\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.01426", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01426", "abs": "https://arxiv.org/abs/2508.01426", "authors": ["Hang Ni", "Weijia Zhang", "Hao Liu"], "title": "UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting", "comment": "35 pages, 80 figures, submitted to ACM KDD 2026 conference", "summary": "Recent advancements in deep learning have led to the development of\nFoundation Models (FMs) for weather forecasting, yet their ability to predict\nextreme weather events remains limited. Existing approaches either focus on\ngeneral weather conditions or specialize in specific-type extremes, neglecting\nthe real-world atmospheric patterns of diversified extreme events. In this\nwork, we identify two key characteristics of extreme events: (1) the spectral\ndisparity against normal weather regimes, and (2) the hierarchical drivers and\ngeographic blending of diverse extremes. Along this line, we propose\nUniExtreme, a universal extreme weather forecasting foundation model that\nintegrates (1) an Adaptive Frequency Modulation (AFM) module that captures\nregion-wise spectral differences between normal and extreme weather, through\nlearnable Beta-distribution filters and multi-granularity spectral aggregation,\nand (2) an Event Prior Augmentation (EPA) module which incorporates\nregion-specific extreme event priors to resolve hierarchical extreme diversity\nand composite extreme schema, via a dual-level memory fusion network. Extensive\nexperiments demonstrate that UniExtreme outperforms state-of-the-art baselines\nin both extreme and general weather forecasting, showcasing superior\nadaptability across diverse extreme scenarios.", "AI": {"tldr": "UniExtreme\u6a21\u578b\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\u548c\u4e8b\u4ef6\u5148\u9a8c\u589e\u5f3a\u6a21\u5757\uff0c\u63d0\u5347\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u5ffd\u89c6\u591a\u6837\u6781\u7aef\u4e8b\u4ef6\u7684\u771f\u5b9e\u5927\u6c14\u6a21\u5f0f\u3002", "method": "\u63d0\u51faUniExtreme\u6a21\u578b\uff0c\u5305\u542b\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\u6a21\u5757\uff08AFM\uff09\u548c\u4e8b\u4ef6\u5148\u9a8c\u589e\u5f3a\u6a21\u5757\uff08EPA\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aUniExtreme\u5728\u6781\u7aef\u548c\u4e00\u822c\u5929\u6c14\u9884\u6d4b\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniExtreme\u5c55\u793a\u4e86\u5728\u591a\u6837\u6781\u7aef\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u9002\u5e94\u6027\u3002"}}
{"id": "2508.02191", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02191", "abs": "https://arxiv.org/abs/2508.02191", "authors": ["Boheng Liu", "Ziyu Li", "Xia Wu"], "title": "Neuromorphic Computing with Multi-Frequency Oscillations: A Bio-Inspired Approach to Artificial Intelligence", "comment": null, "summary": "Despite remarkable capabilities, artificial neural networks exhibit limited\nflexible, generalizable intelligence. This limitation stems from their\nfundamental divergence from biological cognition that overlooks both neural\nregions' functional specialization and the temporal dynamics critical for\ncoordinating these specialized systems. We propose a tripartite brain-inspired\narchitecture comprising functionally specialized perceptual, auxiliary, and\nexecutive systems. Moreover, the integration of temporal dynamics through the\nsimulation of multi-frequency neural oscillation and synaptic dynamic\nadaptation mechanisms enhances the architecture, thereby enabling more flexible\nand efficient artificial cognition. Initial evaluations demonstrate superior\nperformance compared to state-of-the-art temporal processing approaches, with\n2.18\\% accuracy improvements while reducing required computation iterations by\n48.44\\%, and achieving higher correlation with human confidence patterns.\nThough currently demonstrated on visual processing tasks, this architecture\nestablishes a theoretical foundation for brain-like intelligence across\ncognitive domains, potentially bridging the gap between artificial and\nbiological intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u5927\u8111\u542f\u53d1\u7684\u4e09\u90e8\u5206\u67b6\u6784\uff0c\u901a\u8fc7\u529f\u80fd\u4e13\u4e1a\u5316\u548c\u65f6\u95f4\u52a8\u6001\u6a21\u62df\u63d0\u5347\u4eba\u5de5\u8ba4\u77e5\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u5176\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u7075\u6d3b\u6027\u548c\u901a\u7528\u667a\u80fd\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u6e90\u4e8e\u5176\u4e0e\u751f\u7269\u8ba4\u77e5\u7684\u6839\u672c\u5dee\u5f02\uff0c\u7279\u522b\u662f\u529f\u80fd\u4e13\u4e1a\u5316\u548c\u65f6\u95f4\u52a8\u6001\u7684\u5ffd\u89c6\u3002", "method": "\u63d0\u51fa\u4e09\u90e8\u5206\u67b6\u6784\uff08\u611f\u77e5\u3001\u8f85\u52a9\u3001\u6267\u884c\u7cfb\u7edf\uff09\uff0c\u7ed3\u5408\u591a\u9891\u795e\u7ecf\u632f\u8361\u548c\u7a81\u89e6\u52a8\u6001\u9002\u5e94\u673a\u5236\u6a21\u62df\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u63d0\u53472.18%\uff0c\u8ba1\u7b97\u8fed\u4ee3\u51cf\u5c1148.44%\uff0c\u4e14\u4e0e\u4eba\u7c7b\u7f6e\u4fe1\u6a21\u5f0f\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u7c7b\u8111\u667a\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u671b\u7f29\u5c0f\u4eba\u5de5\u4e0e\u751f\u7269\u667a\u80fd\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.02271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02271", "abs": "https://arxiv.org/abs/2508.02271", "authors": ["Kenneth Enevoldsen", "Kristian N\u00f8rgaard Jensen", "Jan Kostkan", "Bal\u00e1zs Szab\u00f3", "M\u00e1rton Kardos", "Kirten Vad", "Andrea Blasi N\u00fa\u00f1ez", "Gianluca Barmina", "Jacob Nielsen", "Rasmus Larsen", "Peter Vahlstrup", "Per M\u00f8ldrup Dalum", "Desmond Elliott", "Lukas Galke", "Peter Schneider-Kamp", "Kristoffer Nielbo"], "title": "Dynaword: From One-shot to Continuously Developed Datasets", "comment": null, "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDynaword\u65b9\u6cd5\u548cDanish Dynaword\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5728\u8bb8\u53ef\u3001\u793e\u533a\u8d21\u732e\u548c\u8d28\u91cf\u4fdd\u8bc1\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u6570\u636e\u96c6\u9762\u4e34\u8bb8\u53ef\u6a21\u7cca\u3001\u9759\u6001\u53d1\u5e03\u548c\u8d28\u91cf\u4fdd\u8bc1\u53d7\u9650\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u4f7f\u7528\u548c\u53d1\u5c55\u3002", "method": "\u63d0\u51faDynaword\u6846\u67b6\uff0c\u652f\u6301\u793e\u533a\u534f\u4f5c\u6301\u7eed\u66f4\u65b0\u6570\u636e\u96c6\uff1b\u5e76\u5b9e\u73b0Danish Dynaword\u4f5c\u4e3a\u9a8c\u8bc1\u3002", "result": "Danish Dynaword\u89c4\u6a21\u662f\u540c\u7c7b\u6570\u636e\u96c6\u7684\u56db\u500d\uff0c\u5b8c\u5168\u5f00\u6e90\uff0c\u5e76\u5438\u5f15\u4e86\u591a\u65b9\u8d21\u732e\u3002", "conclusion": "Dynaword\u65b9\u6cd5\u4e3a\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7684\u793e\u533a\u534f\u4f5c\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.01474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01474", "abs": "https://arxiv.org/abs/2508.01474", "authors": ["Ivan Karpukhin", "Andrey Savchenko"], "title": "HT-Transformer: Event Sequences Classification by Accumulating Prefix Information with History Tokens", "comment": null, "summary": "Deep learning has achieved remarkable success in modeling sequential data,\nincluding event sequences, temporal point processes, and irregular time series.\nRecently, transformers have largely replaced recurrent networks in these tasks.\nHowever, transformers often underperform RNNs in classification tasks where the\nobjective is to predict future targets. The reason behind this performance gap\nremains largely unexplored. In this paper, we identify a key limitation of\ntransformers: the absence of a single state vector that provides a compact and\neffective representation of the entire sequence. Additionally, we show that\ncontrastive pretraining of embedding vectors fails to capture local context,\nwhich is crucial for accurate prediction. To address these challenges, we\nintroduce history tokens, a novel concept that facilitates the accumulation of\nhistorical information during next-token prediction pretraining. Our approach\nsignificantly improves transformer-based models, achieving impressive results\nin finance, e-commerce, and healthcare tasks. The code is publicly available on\nGitHub.", "AI": {"tldr": "Transformers\u5728\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e38\u900a\u4e8eRNNs\u3002\u7814\u7a76\u53d1\u73b0\u5176\u7f3a\u4e4f\u5355\u4e00\u72b6\u6001\u5411\u91cf\u8868\u793a\u5168\u5c40\u5e8f\u5217\uff0c\u4e14\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65e0\u6cd5\u6355\u6349\u5c40\u90e8\u4e0a\u4e0b\u6587\u3002\u63d0\u51fa\u5386\u53f2\u4ee4\u724c\u6982\u5ff5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22Transformers\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u5386\u53f2\u4ee4\u724c\uff08history tokens\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u79ef\u7d2f\u5386\u53f2\u4fe1\u606f\u3002", "result": "\u5728\u91d1\u878d\u3001\u7535\u5546\u548c\u533b\u7597\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5386\u53f2\u4ee4\u724c\u6709\u6548\u89e3\u51b3\u4e86Transformers\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.01506", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.01506", "abs": "https://arxiv.org/abs/2508.01506", "authors": ["Zishan Shao", "Yixiao Wang", "Qinsi Wang", "Ting Jiang", "Zhixu Du", "Hancheng Ye", "Danyang Zhuo", "Yiran Chen", "Hai Li"], "title": "FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models", "comment": "Technical Report", "summary": "Singular Value Decomposition (SVD) has recently seen a surge of interest as a\nsimple yet powerful tool for large language models (LLMs) compression, with a\ngrowing number of works demonstrating 20-80% parameter reductions at minimal\naccuracy loss. Previous SVD-based approaches have focused primarily on reducing\nthe memory footprint of model weights, largely overlooking the additional\nactivation memory overhead incurred during inference when applying truncated\nfactors via standard dense CUDA kernels. Our experiments demonstrate that this\nactivation overhead, scaling with sequence length and hidden dimension,\nprevents current SVD compression techniques from achieving any reduction in\npeak inference memory, thereby limiting their viability for real-world,\non-device deployments.\n  We introduce FlashSVD, a novel, end-to-end rank-aware streaming inference\nframework specifically designed for SVD-compressed large language models.\nFlashSVD can be seamlessly integrated with any model that employs SVD-based\nmethods for parameter reduction. By fusing low-rank projection kernels directly\ninto both the self-attention and feed-forward network (FFN) pipelines, FlashSVD\navoid materializing full-size activation buffers. Instead, small tiles of the\ntruncated factors are loaded into on-chip SRAM, multiplied and reduced on the\nfly, and immediately evicted, preserving high GPU occupancy and adding no extra\nlatency. On standard encoder benchmarks (e.g., BERT-Base), FlashSVD cuts peak\nactivation memory by up to 70.2% and intermediate transient memory by 75%, all\nwhile incur no accuracy loss with upstreaming compression methods, offering a\npractical path toward memory-constrained deployment of low-rank LLMs.", "AI": {"tldr": "FlashSVD\u662f\u4e00\u79cd\u65b0\u578b\u7684\u7aef\u5230\u7aef\u6d41\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u4e13\u4e3aSVD\u538b\u7f29\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u5cf0\u503c\u6fc0\u6d3b\u5185\u5b58\u548c\u4e2d\u95f4\u4e34\u65f6\u5185\u5b58\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709SVD\u538b\u7f29\u6280\u672f\u5728\u63a8\u7406\u65f6\u5ffd\u7565\u4e86\u6fc0\u6d3b\u5185\u5b58\u7684\u5f00\u9500\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51cf\u5c11\u5cf0\u503c\u5185\u5b58\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u4f4e\u79e9\u6295\u5f71\u5185\u6838\u76f4\u63a5\u878d\u5408\u5230\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u7f51\u7edc\u7ba1\u9053\u4e2d\uff0c\u907f\u514d\u751f\u6210\u5168\u5c3a\u5bf8\u6fc0\u6d3b\u7f13\u51b2\u533a\uff0c\u5229\u7528SRAM\u52a8\u6001\u5904\u7406\u5c0f\u7247\u6bb5\u6570\u636e\u3002", "result": "\u5728BERT-Base\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlashSVD\u5c06\u5cf0\u503c\u6fc0\u6d3b\u5185\u5b58\u51cf\u5c1170.2%\uff0c\u4e2d\u95f4\u4e34\u65f6\u5185\u5b58\u51cf\u5c1175%\uff0c\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "FlashSVD\u4e3a\u4f4e\u79e9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.02555", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.02555", "abs": "https://arxiv.org/abs/2508.02555", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Building and Aligning Comparable Corpora", "comment": "27 pages, 11 figures", "summary": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eceWikipedia\u548cEURONEWS\u6784\u5efa\u591a\u8bed\u8a00\u53ef\u6bd4\u8bed\u6599\u5e93\u7684\u65b9\u6cd5\uff0c\u5e76\u5b9e\u9a8c\u4e86\u57fa\u4e8e\u53cc\u8bed\u8bcd\u5178\u548c\u6f5c\u5728\u8bed\u4e49\u7d22\u5f15\uff08LSI\uff09\u7684\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u5ea6\u91cf\u5bf9\u9f50\u6587\u6863\u3002\u5b9e\u9a8c\u8868\u660eCL-LSI\u4f18\u4e8e\u8bcd\u5178\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eBBC\u548cALJAZEERA\u65b0\u95fb\u6587\u6863\u7684\u5bf9\u9f50\u3002", "motivation": "\u53ef\u6bd4\u8bed\u6599\u5e93\u5728\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5f88\u6709\u7528\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5e73\u884c\u6587\u672c\u7684\u9886\u57df\u6216\u8bed\u8a00\u4e2d\u3002\u5b83\u4eec\u80fd\u53cd\u6620\u4e0d\u540c\u8bed\u8a00\u5bf9\u540c\u4e00\u4e3b\u9898\u7684\u8ba8\u8bba\u3002", "method": "\u4eceWikipedia\u548cEURONEWS\u6784\u5efa\u82f1\u8bed\u3001\u6cd5\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u7684\u53ef\u6bd4\u8bed\u6599\u5e93\uff0c\u5e76\u5b9e\u9a8c\u4e24\u79cd\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u5ea6\u91cf\uff08\u53cc\u8bed\u8bcd\u5178\u548cCL-LSI\uff09\u5bf9\u9f50\u6587\u6863\u3002", "result": "CL-LSI\u5728\u591a\u4e2a\u8bed\u6599\u5e93\u4e0a\u8868\u73b0\u4f18\u4e8e\u8bcd\u5178\u65b9\u6cd5\uff0c\u5e76\u80fd\u5b9e\u73b0\u4e3b\u9898\u548c\u4e8b\u4ef6\u7ea7\u522b\u7684\u8de8\u8bed\u8a00\u6587\u6863\u5bf9\u9f50\u3002", "conclusion": "CL-LSI\u662f\u4e00\u79cd\u6709\u6548\u7684\u8de8\u8bed\u8a00\u6587\u6863\u5bf9\u9f50\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u53ef\u6bd4\u8bed\u6599\u5e93\u3002"}}
{"id": "2508.01598", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01598", "abs": "https://arxiv.org/abs/2508.01598", "authors": ["En Yu", "Jie Lu", "Kun Wang", "Xiaoyu Yang", "Guangquan Zhang"], "title": "Drift-aware Collaborative Assistance Mixture of Experts for Heterogeneous Multistream Learning", "comment": null, "summary": "Learning from multiple data streams in real-world scenarios is fundamentally\nchallenging due to intrinsic heterogeneity and unpredictable concept drifts.\nExisting methods typically assume homogeneous streams and employ static\narchitectures with indiscriminate knowledge fusion, limiting generalizability\nin complex dynamic environments. To tackle this gap, we propose CAMEL, a\ndynamic \\textbf{C}ollaborative \\textbf{A}ssistance \\textbf{M}ixture of\n\\textbf{E}xperts \\textbf{L}earning framework. It addresses heterogeneity by\nassigning each stream an independent system with a dedicated feature extractor\nand task-specific head. Meanwhile, a dynamic pool of specialized private\nexperts captures stream-specific idiosyncratic patterns. Crucially,\ncollaboration across these heterogeneous streams is enabled by a dedicated\nassistance expert. This expert employs a multi-head attention mechanism to\ndistill and integrate relevant context autonomously from all other concurrent\nstreams. It facilitates targeted knowledge transfer while inherently mitigating\nnegative transfer from irrelevant sources. Furthermore, we propose an\nAutonomous Expert Tuner (AET) strategy, which dynamically manages expert\nlifecycles in response to drift. It instantiates new experts for emerging\nconcepts (freezing prior ones to prevent catastrophic forgetting) and prunes\nobsolete ones. This expert-level plasticity provides a robust and efficient\nmechanism for online model capacity adaptation. Extensive experiments\ndemonstrate CAMEL's superior generalizability across diverse multistreams and\nexceptional resilience against complex concept drifts.", "AI": {"tldr": "CAMEL\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u4e13\u5bb6\u6df7\u5408\u5b66\u4e60\u89e3\u51b3\u591a\u6570\u636e\u6d41\u5f02\u8d28\u6027\u548c\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u91c7\u7528\u72ec\u7acb\u7cfb\u7edf\u3001\u52a8\u6001\u4e13\u5bb6\u6c60\u548c\u534f\u4f5c\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u6570\u636e\u6d41\u7684\u5f02\u8d28\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u6982\u5ff5\u6f02\u79fb\u4f7f\u5f97\u5b66\u4e60\u53d8\u5f97\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6d41\u540c\u8d28\u4e14\u91c7\u7528\u9759\u6001\u67b6\u6784\uff0c\u9650\u5236\u4e86\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "CAMEL\u4e3a\u6bcf\u4e2a\u6d41\u5206\u914d\u72ec\u7acb\u7cfb\u7edf\uff0c\u5305\u542b\u4e13\u7528\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4efb\u52a1\u5934\uff1b\u52a8\u6001\u4e13\u5bb6\u6c60\u6355\u83b7\u6d41\u7279\u5b9a\u6a21\u5f0f\uff1b\u534f\u4f5c\u4e13\u5bb6\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u8de8\u6d41\u4e0a\u4e0b\u6587\uff1bAET\u7b56\u7565\u52a8\u6001\u7ba1\u7406\u4e13\u5bb6\u751f\u547d\u5468\u671f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCAMEL\u5728\u591a\u6837\u5316\u591a\u6d41\u6570\u636e\u4e2d\u5177\u6709\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u6709\u6548\u5e94\u5bf9\u590d\u6742\u6982\u5ff5\u6f02\u79fb\u3002", "conclusion": "CAMEL\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u548c\u4e13\u5bb6\u7ea7\u53ef\u5851\u6027\uff0c\u4e3a\u591a\u6d41\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01585", "abs": "https://arxiv.org/abs/2508.01585", "authors": ["Hua Yu", "Yaqing Hou", "Xu Gui", "Shanshan Feng", "Dongsheng Zhou", "Qiang Zhang"], "title": "A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction", "comment": null, "summary": "Stochastic Human Motion Prediction (HMP) has received increasing attention\ndue to its wide applications. Despite the rapid progress in generative fields,\nexisting methods often face challenges in learning continuous temporal dynamics\nand predicting stochastic motion sequences. They tend to overlook the\nflexibility inherent in complex human motions and are prone to mode collapse.\nTo alleviate these issues, we propose a novel method called STCN, for\nstochastic and continuous human motion prediction, which consists of two\nstages. Specifically, in the first stage, we propose a spatio-temporal\ncontinuous network to generate smoother human motion sequences. In addition,\nthe anchor set is innovatively introduced into the stochastic HMP task to\nprevent mode collapse, which refers to the potential human motion patterns. In\nthe second stage, STCN endeavors to acquire the Gaussian mixture distribution\n(GMM) of observed motion sequences with the aid of the anchor set. It also\nfocuses on the probability associated with each anchor, and employs the\nstrategy of sampling multiple sequences from each anchor to alleviate\nintra-class differences in human motions. Experimental results on two\nwidely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model\nobtains competitive performance on both diversity and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTCN\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u968f\u673a\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u5b66\u4e60\u548c\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u548c\u968f\u673a\u8fd0\u52a8\u5e8f\u5217\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4e14\u5bb9\u6613\u5ffd\u89c6\u590d\u6742\u4eba\u7c7b\u8fd0\u52a8\u7684\u7075\u6d3b\u6027\u5e76\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u3002", "method": "STCN\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u65f6\u7a7a\u8fde\u7eed\u7f51\u7edc\u751f\u6210\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u5e8f\u5217\uff0c\u5e76\u5f15\u5165\u951a\u70b9\u96c6\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5b66\u4e60\u89c2\u6d4b\u8fd0\u52a8\u5e8f\u5217\u7684\u9ad8\u65af\u6df7\u5408\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u591a\u5e8f\u5217\u91c7\u6837\u7f13\u89e3\u7c7b\u5185\u5dee\u5f02\u3002", "result": "\u5728Human3.6M\u548cHumanEva-I\u6570\u636e\u96c6\u4e0a\uff0cSTCN\u5728\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5747\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "STCN\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u951a\u70b9\u96c6\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u968f\u673a\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01646", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6; I.2.10; C.1.3"], "pdf": "https://arxiv.org/pdf/2508.01646", "abs": "https://arxiv.org/abs/2508.01646", "authors": ["Minsuk Jang", "Changick Kim"], "title": "SPARTA: Advancing Sparse Attention in Spiking Neural Networks via Spike-Timing-Based Prioritization", "comment": "9 pages, 4 figures, submitted to AAAI 2026", "summary": "Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics\ninherent in spike-based processing, relying primarily on rate coding while\noverlooking precise timing information that provides rich computational cues.\nWe propose SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal\nAllocation), a framework that leverages heterogeneous neuron dynamics and\nspike-timing information to enable efficient sparse attention. SPARTA\nprioritizes tokens based on temporal cues, including firing patterns, spike\ntiming, and inter-spike intervals, achieving 65.4% sparsity through competitive\ngating. By selecting only the most salient tokens, SPARTA reduces attention\ncomplexity from O(N^2) to O(K^2) with k << n, while maintaining high accuracy.\nOur method achieves state-of-the-art performance on DVS-Gesture (98.78%) and\ncompetitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating\nthat exploiting spike timing dynamics improves both computational efficiency\nand accuracy.", "AI": {"tldr": "SPARTA\u6846\u67b6\u5229\u7528\u8109\u51b2\u65f6\u5e8f\u52a8\u6001\u4fe1\u606f\uff0c\u901a\u8fc7\u7ade\u4e89\u95e8\u63a7\u5b9e\u73b065.4%\u7684\u7a00\u758f\u6027\uff0c\u5c06\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u4eceO(N^2)\u964d\u81f3O(K^2)\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u4e3b\u8981\u4f9d\u8d56\u901f\u7387\u7f16\u7801\uff0c\u5ffd\u89c6\u4e86\u8109\u51b2\u65f6\u5e8f\u4fe1\u606f\u63d0\u4f9b\u7684\u4e30\u5bcc\u8ba1\u7b97\u7ebf\u7d22\u3002", "method": "\u63d0\u51faSPARTA\u6846\u67b6\uff0c\u5229\u7528\u5f02\u8d28\u795e\u7ecf\u5143\u52a8\u6001\u548c\u8109\u51b2\u65f6\u5e8f\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u7ade\u4e89\u95e8\u63a7\u9009\u62e9\u6700\u663e\u8457\u7684token\u3002", "result": "\u5728DVS-Gesture\u4e0a\u8fbe\u523098.78%\u7684\u51c6\u786e\u7387\uff0cCIFAR10-DVS\u548cCIFAR-10\u4e0a\u5206\u522b\u8fbe\u523083.06%\u548c95.3%\u3002", "conclusion": "\u5229\u7528\u8109\u51b2\u65f6\u5e8f\u52a8\u6001\u53ef\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.01699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01699", "abs": "https://arxiv.org/abs/2508.01699", "authors": ["Zuhao Yang", "Yingchen Yu", "Yunqing Zhao", "Shijian Lu", "Song Bai"], "title": "TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (VTG) aims to precisely identify video event\nsegments in response to textual queries. The outputs of VTG tasks manifest as\nsequences of events, each defined by precise timestamps, saliency scores, and\ntextual descriptions. Despite recent advances, a fundamental limitation\npersists in existing Video Large Language Models (Video-LLMs): they process all\ntask tokens through identical and static pathways, failing to recognize that\ntemporal localization, saliency assessment, and textual generation represent\nfundamentally distinct tasks requiring specialized processing. To address this,\nwe introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that\neffectively decomposes VTG tasks by dynamically routing task-specific tokens\n(e.g., timestamps, saliency scores) to specialized experts, with increased\ncomputational efficiency. Our design choices enable precise handling of each\nsubtask, leading to improved event modeling across diverse VTG applications.\nExtensive experiments demonstrate that TimeExpert consistently achieves\nstate-of-the-art performance on various VTG tasks such as Dense Video\nCaptioning, Moment Retrieval, and Video Highlight Detection.", "AI": {"tldr": "TimeExpert\u662f\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u4efb\u52a1\u7279\u5b9a\u4ee4\u724c\uff08\u5982\u65f6\u95f4\u6233\u3001\u663e\u8457\u6027\u5206\u6570\uff09\u5230\u4e13\u95e8\u4e13\u5bb6\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6240\u6709\u4efb\u52a1\u4ee4\u724c\u91c7\u7528\u76f8\u540c\u7684\u9759\u6001\u5904\u7406\u8def\u5f84\uff0c\u65e0\u6cd5\u533a\u5206\u65f6\u95f4\u5b9a\u4f4d\u3001\u663e\u8457\u6027\u8bc4\u4f30\u548c\u6587\u672c\u751f\u6210\u7b49\u4e0d\u540c\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faTimeExpert\uff0c\u5229\u7528Mixture-of-Experts\u67b6\u6784\uff0c\u52a8\u6001\u8def\u7531\u4efb\u52a1\u7279\u5b9a\u4ee4\u724c\u5230\u4e13\u95e8\u4e13\u5bb6\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728Dense Video Captioning\u3001Moment Retrieval\u548cVideo Highlight Detection\u7b49\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TimeExpert\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u8def\u7531\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01842", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01842", "abs": "https://arxiv.org/abs/2508.01842", "authors": ["Weiqi Yan", "Chenlu Lin", "Youbiao Wang", "Zhipeng Cai", "Xiuhong Lin", "Yangyang Shi", "Weiquan Liu", "Yu Zang"], "title": "OmniEvent: Unified Event Representation Learning", "comment": null, "summary": "Event cameras have gained increasing popularity in computer vision due to\ntheir ultra-high dynamic range and temporal resolution. However, event networks\nheavily rely on task-specific designs due to the unstructured data distribution\nand spatial-temporal (S-T) inhomogeneity, making it hard to reuse existing\narchitectures for new tasks. We propose OmniEvent, the first unified event\nrepresentation learning framework that achieves SOTA performance across diverse\ntasks, fully removing the need of task-specific designs. Unlike previous\nmethods that treat event data as 3D point clouds with manually tuned S-T\nscaling weights, OmniEvent proposes a decouple-enhance-fuse paradigm, where the\nlocal feature aggregation and enhancement is done independently on the spatial\nand temporal domains to avoid inhomogeneity issues. Space-filling curves are\napplied to enable large receptive fields while improving memory and compute\nefficiency. The features from individual domains are then fused by attention to\nlearn S-T interactions. The output of OmniEvent is a grid-shaped tensor, which\nenables standard vision models to process event data without architecture\nchange. With a unified framework and similar hyper-parameters, OmniEvent\nout-performs (tasks-specific) SOTA by up to 68.2% across 3 representative tasks\nand 10 datasets (Fig.1). Code will be ready in\nhttps://github.com/Wickyan/OmniEvent .", "AI": {"tldr": "OmniEvent\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026-\u589e\u5f3a-\u878d\u5408\u8303\u5f0f\u89e3\u51b3\u4e8b\u4ef6\u6570\u636e\u7684\u4e0d\u5747\u5300\u6027\u95ee\u9898\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u5373\u53ef\u5728\u591a\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4e8b\u4ef6\u7f51\u7edc\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\uff0c\u96be\u4ee5\u590d\u7528\u73b0\u6709\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u89e3\u8026-\u589e\u5f3a-\u878d\u5408\u8303\u5f0f\uff0c\u72ec\u7acb\u5904\u7406\u65f6\u7a7a\u7279\u5f81\uff0c\u5229\u7528\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u63d0\u5347\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u7279\u5f81\u3002", "result": "\u57283\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u548c10\u4e2a\u6570\u636e\u96c6\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe68.2%\u3002", "conclusion": "OmniEvent\u4e3a\u4e8b\u4ef6\u6570\u636e\u63d0\u4f9b\u7edf\u4e00\u5904\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2508.01875", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01875", "abs": "https://arxiv.org/abs/2508.01875", "authors": ["Haolin Yang", "Feilong Tang", "Linxiao Zhao", "Xiang An", "Ming Hu", "Huifa Li", "Xinlin Zhuang", "Boqian Wang", "Yifan Lu", "Xiaofeng Zhang", "Abdalla Swikir", "Junjun He", "Zongyuan Ge", "Imran Razzak"], "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding", "comment": null, "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStreamAgent\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u6d41\u7406\u89e3\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u533a\u57df\uff0c\u5b9e\u73b0\u4e3b\u52a8\u51b3\u7b56\u548c\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u89c6\u9891\u6d41\u5904\u7406\u4e2d\u7f3a\u4e4f\u4efb\u52a1\u9a71\u52a8\u7684\u89c4\u5212\u548c\u672a\u6765\u9884\u6d4b\uff0c\u9650\u5236\u4e86\u54cd\u5e94\u901f\u5ea6\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u95ee\u9898\u8bed\u4e49\u548c\u5386\u53f2\u89c2\u5bdf\uff0c\u901a\u8fc7\u63d0\u793a\u9884\u6d4b\u5173\u952e\u4e8b\u4ef6\u7684\u65f6\u95f4\u8fdb\u5c55\uff0c\u8c03\u6574\u611f\u77e5\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6d41\u5f0fKV\u7f13\u5b58\u673a\u5236\u3002", "result": "\u5728\u6d41\u5f0f\u548c\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u54cd\u5e94\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "StreamAgent\u5728\u5b9e\u65f6\u89c6\u9891\u6d41\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4e3b\u52a8\u51b3\u7b56\u548c\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2508.02130", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02130", "abs": "https://arxiv.org/abs/2508.02130", "authors": ["Boyuan Zheng", "Victor W. Chu", "Zhidong Li", "Evan Webster", "Ashley Rootsey"], "title": "The Complexity of Extreme Climate Events on the New Zealand's Kiwifruit Industry", "comment": "Pre-print v0.8 2025-08-04", "summary": "Climate change has intensified the frequency and severity of extreme weather\nevents, presenting unprecedented challenges to the agricultural industry\nworldwide. In this investigation, we focus on kiwifruit farming in New Zealand.\nWe propose to examine the impacts of climate-induced extreme events,\nspecifically frost, drought, extreme rainfall, and heatwave, on kiwifruit\nharvest yields. These four events were selected due to their significant\nimpacts on crop productivity and their prevalence as recorded by climate\nmonitoring institutions in the country. We employed Isolation Forest, an\nunsupervised anomaly detection method, to analyse climate history and recorded\nextreme events, alongside with kiwifruit yields. Our analysis reveals\nconsiderable variability in how different types of extreme event affect\nkiwifruit yields underscoring notable discrepancies between climatic extremes\nand individual farm's yield outcomes. Additionally, our study highlights\ncritical limitations of current anomaly detection approaches, particularly in\naccurately identifying events such as frost. These findings emphasise the need\nfor integrating supplementary features like farm management strategies with\nclimate adaptation practices. Our further investigation will employ ensemble\nmethods that consolidate nearby farms' yield data and regional climate station\nfeatures to reduce variance, thereby enhancing the accuracy and reliability of\nextreme event detection and the formulation of response strategies.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u65b0\u897f\u5170\u7315\u7334\u6843\u79cd\u690d\u4e2d\u6781\u7aef\u6c14\u5019\u4e8b\u4ef6\uff08\u971c\u51bb\u3001\u5e72\u65f1\u3001\u6781\u7aef\u964d\u96e8\u548c\u70ed\u6d6a\uff09\u5bf9\u4ea7\u91cf\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u5b64\u7acb\u68ee\u6797\u7b97\u6cd5\u68c0\u6d4b\u5f02\u5e38\uff0c\u53d1\u73b0\u4e0d\u540c\u4e8b\u4ef6\u5bf9\u4ea7\u91cf\u7684\u5f71\u54cd\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\uff0c\u5bf9\u519c\u4e1a\u9020\u6210\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u8fd9\u4e9b\u4e8b\u4ef6\u5bf9\u7315\u7334\u6843\u4ea7\u91cf\u7684\u5177\u4f53\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5b64\u7acb\u68ee\u6797\u7b97\u6cd5\u5206\u6790\u6c14\u5019\u5386\u53f2\u548c\u6781\u7aef\u4e8b\u4ef6\u6570\u636e\uff0c\u7ed3\u5408\u7315\u7334\u6843\u4ea7\u91cf\u6570\u636e\u3002", "result": "\u4e0d\u540c\u6781\u7aef\u4e8b\u4ef6\u5bf9\u7315\u7334\u6843\u4ea7\u91cf\u7684\u5f71\u54cd\u5dee\u5f02\u663e\u8457\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u68c0\u6d4b\u971c\u51bb\u7b49\u4e8b\u4ef6\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u6574\u5408\u519c\u573a\u7ba1\u7406\u7b56\u7565\u548c\u6c14\u5019\u9002\u5e94\u63aa\u65bd\uff0c\u672a\u6765\u7814\u7a76\u5c06\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2508.02004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02004", "abs": "https://arxiv.org/abs/2508.02004", "authors": ["Kyungmin Jo", "Jooyeol Yun", "Jaegul Choo"], "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention", "comment": null, "summary": "While large-scale text-to-image diffusion models enable the generation of\nhigh-quality, diverse images from text prompts, these prompts struggle to\ncapture intricate details, such as textures, preventing the user intent from\nbeing reflected. This limitation has led to efforts to generate images\nconditioned on user-provided images, referred to as image prompts. Recent work\nmodifies the self-attention mechanism to impose image conditions in generated\nimages by replacing or concatenating the keys and values from the image prompt.\nThis enables the self-attention layer to work like a cross-attention layer,\ngenerally used to incorporate text prompts. In this paper, we identify two\ncommon issues in existing methods of modifying self-attention to generate\nimages that reflect the details of image prompts. First, existing approaches\nneglect the importance of image prompts in classifier-free guidance.\nSpecifically, current methods use image prompts as both desired and undesired\nconditions in classifier-free guidance, causing conflicting signals. To resolve\nthis, we propose conflict-free guidance by using image prompts only as desired\nconditions, ensuring that the generated image faithfully reflects the image\nprompt. In addition, we observe that the two most common self-attention\nmodifications involve a trade-off between the realism of the generated image\nand alignment with the image prompt. Specifically, selecting more keys and\nvalues from the image prompt improves alignment, while selecting more from the\ngenerated image enhances realism. To balance both, we propose an new\nself-attention modification method, Stratified Attention to jointly use keys\nand values from both images rather than selecting between them. Through\nextensive experiments across three image generation tasks, we show that the\nproposed method outperforms existing image-prompting models in faithfully\nreflecting the image prompt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6ce8\u610f\u529b\u4fee\u6539\u65b9\u6cd5\uff08Stratified Attention\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u50cf\u63d0\u793a\u751f\u6210\u65b9\u6cd5\u4e2d\u7684\u4e24\u4e2a\u95ee\u9898\uff1a\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u4e2d\u7684\u51b2\u7a81\u4fe1\u53f7\u4ee5\u53ca\u751f\u6210\u56fe\u50cf\u771f\u5b9e\u611f\u4e0e\u56fe\u50cf\u63d0\u793a\u5bf9\u9f50\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u96be\u4ee5\u6355\u6349\u590d\u6742\u7ec6\u8282\uff08\u5982\u7eb9\u7406\uff09\uff0c\u5bfc\u81f4\u7528\u6237\u610f\u56fe\u65e0\u6cd5\u5b8c\u5168\u4f53\u73b0\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u901a\u8fc7\u4fee\u6539\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u56fe\u50cf\u63d0\u793a\uff0c\u4f46\u4ecd\u5b58\u5728\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u4e2d\u7684\u51b2\u7a81\u4fe1\u53f7\u548c\u771f\u5b9e\u611f\u4e0e\u5bf9\u9f50\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u51b2\u7a81\u81ea\u7531\u5f15\u5bfc\uff08conflict-free guidance\uff09\uff0c\u4ec5\u5c06\u56fe\u50cf\u63d0\u793a\u4f5c\u4e3a\u671f\u671b\u6761\u4ef6\uff1b\u63d0\u51fa\u5206\u5c42\u6ce8\u610f\u529b\uff08Stratified Attention\uff09\uff0c\u8054\u5408\u4f7f\u7528\u56fe\u50cf\u63d0\u793a\u548c\u751f\u6210\u56fe\u50cf\u7684\u952e\u503c\uff0c\u800c\u975e\u4e8c\u9009\u4e00\u3002", "result": "\u5728\u4e09\u79cd\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5fe0\u5b9e\u53cd\u6620\u56fe\u50cf\u63d0\u793a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u51b2\u7a81\u81ea\u7531\u5f15\u5bfc\u548c\u5206\u5c42\u6ce8\u610f\u529b\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u5bf9\u56fe\u50cf\u63d0\u793a\u7684\u5fe0\u5b9e\u5ea6\uff0c\u540c\u65f6\u5e73\u8861\u4e86\u771f\u5b9e\u611f\u4e0e\u5bf9\u9f50\u3002"}}
{"id": "2508.02127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02127", "abs": "https://arxiv.org/abs/2508.02127", "authors": ["Mingjie Liu", "Hanqing Liu", "Chuang Zhu"], "title": "Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps", "comment": null, "summary": "Accurate object detection under adverse lighting conditions is critical for\nreal-world applications such as autonomous driving. Although neuromorphic event\ncameras have been introduced to handle these scenarios, adverse lighting often\ninduces distracting reflections from tunnel walls or road surfaces, which\nfrequently lead to false obstacle detections. However, neither RGB nor event\ndata alone is robust enough to address these complexities, and mitigating these\nissues without additional sensors remains underexplored. To overcome these\nchallenges, we propose leveraging normal maps, directly predicted from\nmonocular RGB images, as robust geometric cues to suppress false positives and\nenhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection\nframework that effectively fuses three complementary modalities: monocularly\npredicted surface normal maps, RGB images, and event streams. To optimize the\nfusion process, our framework incorporates two key modules: the Adaptive\nDual-stream Fusion Module (ADFM), which integrates RGB and normal map features,\nand the Event-modality Aware Fusion Module (EAFM), which adapts to the high\ndynamic range characteristics of event data. Extensive evaluations on the\nDSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly\noutperforms state-of-the-art methods. Our approach achieves mAP50 improvements\nof 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing\nthe fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by\n7.1% on the PKU-DAVIS-SOD dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u68c0\u6d4b\u6846\u67b6NRE-Net\uff0c\u878d\u5408RGB\u56fe\u50cf\u3001\u4e8b\u4ef6\u6d41\u548c\u6cd5\u7ebf\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7269\u4f53\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u7531\u53cd\u5c04\u5f15\u8d77\u7684\u8bef\u68c0\u95ee\u9898\uff0c\u73b0\u6709RGB\u6216\u4e8b\u4ef6\u6570\u636e\u5355\u72ec\u4f7f\u7528\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faNRE-Net\u6846\u67b6\uff0c\u7ed3\u5408RGB\u56fe\u50cf\u3001\u4e8b\u4ef6\u6d41\u548c\u6cd5\u7ebf\u56fe\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53cc\u6d41\u878d\u5408\u6a21\u5757\uff08ADFM\uff09\u548c\u4e8b\u4ef6\u6a21\u6001\u611f\u77e5\u878d\u5408\u6a21\u5757\uff08EAFM\uff09\u4f18\u5316\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728DSEC-Det-sub\u548cPKU-DAVIS-SOD\u6570\u636e\u96c6\u4e0a\uff0cNRE-Net\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cmAP50\u5206\u522b\u63d0\u53477.9%\u548c6.1%\u3002", "conclusion": "NRE-Net\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u6709\u6548\u6291\u5236\u8bef\u68c0\uff0c\u63d0\u5347\u4e86\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.02625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02625", "abs": "https://arxiv.org/abs/2508.02625", "authors": ["Riccardo Francia", "Maurizio Leone", "Giorgio Leonardi", "Stefania Montani", "Marzio Pennisi", "Manuel Striani", "Sandra D'Alfonso"], "title": "AutoML-Med: A Framework for Automated Machine Learning in Medical Tabular Data", "comment": "8 pages, preprint for conference", "summary": "Medical datasets are typically affected by issues such as missing values,\nclass imbalance, a heterogeneous feature types, and a high number of features\nversus a relatively small number of samples, preventing machine learning models\nfrom obtaining proper results in classification and regression tasks. This\npaper introduces AutoML-Med, an Automated Machine Learning tool specifically\ndesigned to address these challenges, minimizing user intervention and\nidentifying the optimal combination of preprocessing techniques and predictive\nmodels. AutoML-Med's architecture incorporates Latin Hypercube Sampling (LHS)\nfor exploring preprocessing methods, trains models using selected metrics, and\nutilizes Partial Rank Correlation Coefficient (PRCC) for fine-tuned\noptimization of the most influential preprocessing steps. Experimental results\ndemonstrate AutoML-Med's effectiveness in two different clinical settings,\nachieving higher balanced accuracy and sensitivity, which are crucial for\nidentifying at-risk patients, compared to other state-of-the-art tools.\nAutoML-Med's ability to improve prediction results, especially in medical\ndatasets with sparse data and class imbalance, highlights its potential to\nstreamline Machine Learning applications in healthcare.", "AI": {"tldr": "AutoML-Med\u662f\u4e00\u79cd\u4e13\u4e3a\u533b\u7597\u6570\u636e\u96c6\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u901a\u8fc7\u4f18\u5316\u9884\u5904\u7406\u548c\u6a21\u578b\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u758f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u6570\u636e\u96c6\u5e38\u56e0\u7f3a\u5931\u503c\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u7279\u5f81\u5f02\u8d28\u6027\u7b49\u95ee\u9898\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\u6765\u4f18\u5316\u9884\u5904\u7406\u548c\u6a21\u578b\u9009\u62e9\u3002", "method": "AutoML-Med\u7ed3\u5408\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\uff08LHS\uff09\u63a2\u7d22\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5229\u7528\u90e8\u5206\u79e9\u76f8\u5173\u7cfb\u6570\uff08PRCC\uff09\u4f18\u5316\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5e76\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAutoML-Med\u5728\u4e24\u79cd\u4e34\u5e8a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u5de5\u5177\uff0c\u5e73\u8861\u51c6\u786e\u7387\u548c\u7075\u654f\u5ea6\u66f4\u9ad8\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7a00\u758f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u3002", "conclusion": "AutoML-Med\u80fd\u6709\u6548\u63d0\u5347\u533b\u7597\u6570\u636e\u96c6\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u7b80\u5316\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2508.02288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02288", "abs": "https://arxiv.org/abs/2508.02288", "authors": ["Jae-Young Kang", "Hoonhee Cho", "Kuk-Jin Yoon"], "title": "Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection", "comment": "Accepted to ICCV 2025", "summary": "3D object detection is essential for autonomous systems, enabling precise\nlocalization and dimension estimation. While LiDAR and RGB cameras are widely\nused, their fixed frame rates create perception gaps in high-speed scenarios.\nEvent cameras, with their asynchronous nature and high temporal resolution,\noffer a solution by capturing motion continuously. The recent approach, which\nintegrates event cameras with conventional sensors for continuous-time\ndetection, struggles in fast-motion scenarios due to its dependency on\nsynchronized sensors. We propose a novel stereo 3D object detection framework\nthat relies solely on event cameras, eliminating the need for conventional 3D\nsensors. To compensate for the lack of semantic and geometric information in\nevent data, we introduce a dual filter mechanism that extracts both.\nAdditionally, we enhance regression by aligning bounding boxes with\nobject-centric information. Experiments show that our method outperforms prior\napproaches in dynamic environments, demonstrating the potential of event\ncameras for robust, continuous-time 3D perception. The code is available at\nhttps://github.com/mickeykang16/Ev-Stereo3D.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f533D\u7269\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6ee4\u6ce2\u5668\u673a\u5236\u63d0\u53d6\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfLiDAR\u548cRGB\u76f8\u673a\u5728\u9ad8\u52a8\u6001\u573a\u666f\u4e2d\u5b58\u5728\u611f\u77e5\u95f4\u9699\uff0c\u800c\u73b0\u6709\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u7684\u65b9\u6cd5\u4f9d\u8d56\u540c\u6b65\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u5e94\u5bf9\u5feb\u901f\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u4ec5\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f533D\u68c0\u6d4b\u6846\u67b6\uff0c\u5f15\u5165\u53cc\u6ee4\u6ce2\u5668\u673a\u5236\u63d0\u53d6\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u76ee\u6807\u4e2d\u5fc3\u4fe1\u606f\u5bf9\u9f50\u8fb9\u754c\u6846\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u8fde\u7eed\u65f6\u95f43D\u611f\u77e5\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u53ef\u4f5c\u4e3a\u72ec\u7acb\u4f20\u611f\u5668\u5b9e\u73b0\u9c81\u68d2\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u5c24\u5176\u5728\u9ad8\u901f\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.02293", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02293", "abs": "https://arxiv.org/abs/2508.02293", "authors": ["Muhammad Aqeel", "Shakiba Sharifi", "Marco Cristani", "Francesco Setti"], "title": "Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning", "comment": "Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)", "summary": "So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConfident Meta-learning (CoMet)\u7684\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u6df1\u5ea6\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u5728\u672a\u7b5b\u9009\u7684\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\uff0c\u65e0\u9700\u663e\u5f0f\u8fc7\u6ee4\u3002", "motivation": "\u4f20\u7edf\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u5047\u8bbe\u6240\u6709\u8bad\u7ec3\u6570\u636e\u5747\u4e3a\u6b63\u5e38\u6837\u672c\uff0c\u9700\u4eba\u5de5\u7b5b\u9009\u6570\u636e\uff0c\u5f15\u5165\u504f\u5dee\u4e14\u9002\u5e94\u6027\u53d7\u9650\u3002", "method": "CoMet\u7ed3\u5408Soft Confident Learning\uff08\u964d\u4f4e\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u6743\u91cd\uff09\u548cMeta-Learning\uff08\u901a\u8fc7\u8bad\u7ec3\u9a8c\u8bc1\u635f\u5931\u534f\u65b9\u5dee\u6b63\u5219\u5316\u66f4\u65b0\uff09\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u5e76\u589e\u5f3a\u5bf9\u566a\u58f0\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728MVTec-AD\u3001VIADUCT\u548cKSDD2\u6570\u636e\u96c6\u4e0a\uff0cCoMet\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u7684\u5f02\u5e38\u4e0d\u654f\u611f\uff0c\u5e76\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "CoMet\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.02307", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02307", "abs": "https://arxiv.org/abs/2508.02307", "authors": ["Dmitrii Seletkov", "Sophie Starck", "Ayhan Can Erdur", "Yundi Zhang", "Daniel Rueckert", "Rickmer Braren"], "title": "Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment", "comment": null, "summary": "Reliable preclinical disease risk assessment is essential to move public\nhealthcare from reactive treatment to proactive identification and prevention.\nHowever, image-based risk prediction algorithms often consider one condition at\na time and depend on hand-crafted features obtained through segmentation tools.\nWe propose a whole-body self-supervised representation learning method for the\npreclinical disease risk assessment under a competing risk modeling. This\napproach outperforms whole-body radiomics in multiple diseases, including\ncardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive\npulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a\npreclinical screening scenario and subsequently combining with cardiac MRI, it\nsharpens further the prediction for CVD subgroups: ischemic heart disease\n(IHD), hypertensive diseases (HD), and stroke. The results indicate the\ntranslational potential of whole-body representations as a standalone screening\nmodality and as part of a multi-modal framework within clinical workflows for\nearly personalized risk stratification. The code is available at\nhttps://github.com/yayapa/WBRLforCR/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u8eab\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u75be\u75c5\u98ce\u9669\u9884\u6d4b\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u4ec5\u9488\u5bf9\u5355\u4e00\u75be\u75c5\u4e14\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5168\u8eab\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ade\u4e89\u98ce\u9669\u6a21\u578b\uff0c\u8fdb\u884c\u591a\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u3002", "result": "\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u3001\u7cd6\u5c3f\u75c5\u7b49\u591a\u79cd\u75be\u75c5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u8fdb\u4e00\u6b65\u7ec6\u5316\u5fc3\u8840\u7ba1\u4e9a\u7ec4\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u4f5c\u4e3a\u72ec\u7acb\u7b5b\u67e5\u5de5\u5177\u6216\u591a\u6a21\u6001\u6846\u67b6\u4e00\u90e8\u5206\u7684\u4e34\u5e8a\u8f6c\u5316\u6f5c\u529b\u3002"}}
{"id": "2508.02411", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02411", "abs": "https://arxiv.org/abs/2508.02411", "authors": ["Xiao Wang", "Hao Si", "Fan Zhang", "Xiaoya Zhou", "Dengdi Sun", "Wanli Lyu", "Qingquan Yang", "Jin Tang"], "title": "HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis", "comment": null, "summary": "Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u65f6\u95f4\u5e8f\u5217\u53d8\u6362\u5668\u7f51\u7edc\uff08HGTS-Former\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u9ad8\u7ef4\u6027\u548c\u590d\u6742\u4ea4\u4e92\u95ee\u9898\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u56e0\u5176\u9ad8\u7ef4\u6027\u548c\u52a8\u6001\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u53d8\u91cf\u95f4\u7684\u590d\u6742\u8026\u5408\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u591a\u5934\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u65f6\u95f4\u8868\u793a\uff0c\u6784\u5efa\u5206\u5c42\u8d85\u56fe\u4ee5\u805a\u5408\u65f6\u95f4\u6a21\u5f0f\u548c\u53d8\u91cf\u95f4\u7ec6\u7c92\u5ea6\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7EdgeToNode\u6a21\u5757\u548c\u524d\u9988\u7f51\u7edc\u8fdb\u4e00\u6b65\u4f18\u5316\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HGTS-Former\u7684\u6709\u6548\u6027\u3002", "conclusion": "HGTS-Former\u4e3a\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
