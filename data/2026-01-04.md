<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Factorized Learning for Temporally Grounded Video-Language Models](https://arxiv.org/abs/2512.24097)
*Wenzheng Zeng,Difei Gao,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: D²VLM：通过解耦学习和因子化偏好优化提升视频语言模型的时序定位能力，采用"先定位后回答"范式，强调事件级视觉语义捕捉


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在事件级感知的时序定位方面存在不足，时序定位和文本响应这两个关键任务通常耦合处理，缺乏清晰的逻辑层次结构，导致次优目标

Method: 提出D²VLM框架，解耦时序定位和文本响应学习，采用"先定位后回答"范式，引入证据令牌进行证据定位；提出因子化偏好优化(FPO)算法，将概率时序建模纳入优化目标；构建合成数据集支持因子化偏好学习

Result: 在多个任务上的实验表明该方法具有明显优势，能够显著提升视频语言模型的时序定位准确性

Conclusion: 通过解耦学习和因子化偏好优化，D²VLM有效解决了视频语言模型中的时序定位问题，强调事件级视觉语义捕捉，为视频理解提供了更可靠的时序证据基础

Abstract: Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.

</details>


### [2] [Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design](https://arxiv.org/abs/2512.24120)
*Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出FSAP（Few-Shot Architecture Prompting）和Whitespace-Normalized Hash Validation两种方法，系统研究LLM在计算机视觉架构生成中的应用，通过大规模实验验证了n=3示例的最佳平衡点，并提供了数据集平衡评估方法。


<details>
  <summary>Details</summary>
Motivation: 自动化神经网络架构设计在计算机视觉中仍面临挑战。任务多样性和计算约束要求既有效又高效的搜索方法。虽然大语言模型（LLMs）为计算密集的神经架构搜索（NAS）提供了有前景的替代方案，但在计算机视觉架构生成中的应用尚未得到系统研究，特别是在提示工程和验证策略方面。

Method: 1. 提出Few-Shot Architecture Prompting（FSAP），系统研究支持示例数量（n=1-6）对LLM架构生成的影响；2. 引入Whitespace-Normalized Hash Validation，一种轻量级去重方法（<1ms），比AST解析快100倍；3. 在7个计算机视觉基准上生成1,900个独特架构；4. 提出数据集平衡评估方法以解决异构视觉任务间的架构比较挑战。

Result: 发现使用n=3示例在架构多样性和上下文聚焦之间达到最佳平衡；Whitespace-Normalized Hash Validation有效防止重复架构训练；在MNIST、CIFAR-10、CIFAR-100、CelebA、ImageNette、SVHN、Places365等基准上验证了方法的有效性。

Conclusion: 这些贡献为计算机视觉中基于LLM的架构搜索提供了可操作的指导方针，并建立了严格的评估实践，使计算资源有限的研究人员能够更便捷地进行自动化设计。

Abstract: Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.

</details>


### [3] [Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning](https://arxiv.org/abs/2512.24146)
*Chubin Chen,Sujie Hu,Jiashu Zhu,Meiqi Wu,Jintao Chen,Yanxun Li,Nisha Huang,Chengyu Fang,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 提出D²-Align框架解决文本到图像扩散模型在人类反馈强化学习中出现的偏好模式崩溃问题，通过方向性修正奖励信号保持生成多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈强化学习的文本到图像扩散模型对齐方法虽然能在自动化奖励指标上获得高分，但会导致偏好模式崩溃——模型收敛到狭窄的高分输出模式（如单一风格或普遍过曝），严重损害生成多样性。

Method: 提出方向性解耦对齐框架：1）在奖励模型的嵌入空间中学习方向性修正（模型保持冻结）；2）在优化过程中将修正应用于奖励信号，防止模型崩溃到特定模式。

Result: D²-Align在质量和多样性定量指标上均表现优异，实现了与人类偏好的更好对齐，同时避免了偏好模式崩溃问题。

Conclusion: 通过方向性修正奖励信号可以有效缓解偏好模式崩溃问题，D²-Align框架在保持生成多样性的同时实现了更好的人类偏好对齐，并提出了DivGenBench基准来量化这一问题。

Abstract: Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.

</details>


### [4] [MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation](https://arxiv.org/abs/2512.24243)
*Fuqiang Gu,Yuanke Li,Xianlei Long,Kangping Ji,Chao Chen,Qingyi Gu,Zhenliang Ni*

Main category: cs.CV

TL;DR: MambaSeg：一种新颖的双分支语义分割框架，使用并行Mamba编码器处理RGB图像和事件流，通过双维度交互模块实现空间和时间维度的细粒度融合，在降低计算成本的同时达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: RGB相机在快速运动、低光照或高动态范围条件下性能下降，而事件相机缺乏颜色和纹理信息。现有RGB-事件融合方法计算成本高且主要关注空间融合，忽略了事件流的时间动态特性。

Method: 提出MambaSeg框架，采用并行Mamba编码器分别处理RGB图像和事件流。引入双维度交互模块（DDIM），包含交叉空间交互模块（CSIM）和交叉时间交互模块（CTIM），在空间和时间维度上进行细粒度融合，改善跨模态对齐并减少歧义。

Result: 在DDD17和DSEC数据集上的实验表明，MambaSeg实现了最先进的语义分割性能，同时显著降低了计算成本。

Conclusion: MambaSeg展示了高效、可扩展和鲁棒的多模态感知潜力，通过有效融合RGB和事件数据的互补特性，解决了现有方法的局限性。

Abstract: Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.

</details>


### [5] [DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model](https://arxiv.org/abs/2512.24408)
*Bohong Chen,Haiyang Liu*

Main category: cs.CV

TL;DR: DyStream：基于流匹配的自回归模型，实时生成双人对话头部视频，延迟低于100ms，实现最佳唇同步质量


<details>
  <summary>Details</summary>
Motivation: 现有基于分块的方法需要完整的非因果上下文窗口，引入显著延迟，无法满足实时对话中即时非语言反馈的需求

Method: 采用流友好的自回归框架，结合流匹配头部进行概率建模；提出因果编码器增强设计，通过前瞻模块引入短未来上下文（如60ms）提升质量同时保持低延迟

Result: 每帧生成时间34ms，系统总延迟低于100ms；在HDTF数据集上获得离线8.13和在线7.61的LipSync Confidence分数，达到最先进的唇同步质量

Conclusion: DyStream通过流匹配自回归模型和因果编码器增强设计，实现了实时、高质量的对话头部视频生成，显著优于现有因果策略，为实时交互应用提供了可行解决方案

Abstract: Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stream-friendly autoregressive framework with flow-matching heads for probabilistic modeling, and (2) We propose a causal encoder enhanced by a lookahead module to incorporate short future context (e.g., 60 ms) to improve quality while maintaining low latency. Our analysis shows this simple-and-effective method significantly surpass alternative causal strategies, including distillation and generative encoder. Extensive experiments show that DyStream could generate video within 34 ms per frame, guaranteeing the entire system latency remains under 100 ms. Besides, it achieves state-of-the-art lip-sync quality, with offline and online LipSync Confidence scores of 8.13 and 7.61 on HDTF, respectively. The model, weights and codes are available.

</details>


### [6] [From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation](https://arxiv.org/abs/2512.24639)
*Siyang Wang,Hanting Li,Wei Li,Jie Hu,Xinghao Chen,Feng Zhao*

Main category: cs.CV

TL;DR: RadAR：一种基于径向拓扑的并行自回归视觉生成框架，通过环状并行预测和嵌套注意力机制，在保持生成质量的同时大幅提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在视觉生成中采用顺序解码机制，导致推理效率低下。视觉token具有强烈的局部依赖性和空间相关性，但标准光栅扫描解码顺序未能充分利用这一特性。

Method: 提出径向拓扑生成框架：1) 选择初始token作为中心点；2) 根据空间距离将所有其他token分组到多个同心环中；3) 按从内到外的环状顺序并行生成同一环内的所有token；4) 引入嵌套注意力机制动态修正不一致的预测，防止误差累积。

Result: RadAR在保持自回归模型表示能力的同时，显著提高了生成效率。通过并行化环内token预测，减少了顺序解码的开销，同时嵌套注意力机制确保了生成质量。

Conclusion: RadAR通过创新的径向拓扑设计和动态输出修正机制，成功解决了自回归视觉生成中的效率瓶颈问题，为高效视觉生成提供了新思路。

Abstract: Inspired by the remarkable success of autoregressive models in language modeling, this paradigm has been widely adopted in visual generation. However, the sequential token-by-token decoding mechanism inherent in traditional autoregressive models leads to low inference efficiency.In this paper, we propose RadAR, an efficient and parallelizable framework designed to accelerate autoregressive visual generation while preserving its representational capacity. Our approach is motivated by the observation that visual tokens exhibit strong local dependencies and spatial correlations with their neighbors--a property not fully exploited in standard raster-scan decoding orders. Specifically, we organize the generation process around a radial topology: an initial token is selected as the starting point, and all other tokens are systematically grouped into multiple concentric rings according to their spatial distances from this center. Generation then proceeds in a ring-wise manner, from inner to outer regions, enabling the parallel prediction of all tokens within the same ring. This design not only preserves the structural locality and spatial coherence of visual scenes but also substantially increases parallelization. Furthermore, to address the risk of inconsistent predictions arising from simultaneous token generation with limited context, we introduce a nested attention mechanism. This mechanism dynamically refines implausible outputs during the forward pass, thereby mitigating error accumulation and preventing model collapse. By integrating radial parallel prediction with dynamic output correction, RadAR significantly improves generation efficiency.

</details>


### [7] [EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation](https://arxiv.org/abs/2512.24731)
*Bingxuan Li,Yiming Cui,Yicheng He,Yiwei Wang,Shu Zhang,Longyin Wen,Yulei Niu*

Main category: cs.CV

TL;DR: EchoFoley提出视频声音生成新任务，通过符号化声音事件实现细粒度控制，构建大规模数据集EchoFoley-6k，并提出EchoVidia框架，在可控性和感知质量上显著超越现有VT2A模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本到音频（VT2A）方法存在三个关键限制：1）视觉与文本条件不平衡导致视觉主导；2）缺乏细粒度可控生成的具体定义；3）指令理解能力弱，现有数据集依赖简短分类标签。需要解决这些问题以实现更好的视频声音生成。

Method: 提出EchoFoley新任务，使用符号化声音事件表示（指定何时、什么、如何产生声音），构建EchoFoley-6k大规模专家标注数据集（6000+视频-指令-标注三元组），并提出EchoVidia框架，采用慢-快思维策略的声音事件中心化代理生成方法。

Result: EchoVidia在可控性上比现有VT2A模型提升40.7%，在感知质量上提升12.5%，显著改善了视频声音生成的效果。

Conclusion: EchoFoley任务和EchoVidia框架有效解决了现有VT2A方法的局限性，通过符号化声音事件表示和慢-快思维策略，实现了细粒度可控的视频声音生成，为多模态叙事提供了更好的声音效果支持。

Abstract: Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.

</details>


### [8] [Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection](https://arxiv.org/abs/2512.24922)
*Bartłomiej Olber,Jakub Winter,Paweł Wawrzyński,Andrii Gamalii,Daniel Górniak,Marcin Łojek,Robert Nowak,Krystian Radlak*

Main category: cs.CV

TL;DR: 提出基于神经元激活模式的LiDAR域自适应方法，仅需标注少量代表性目标域样本即可达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 3D目标检测器在自动驾驶感知系统中至关重要，但在不同地理区域（如美国、亚洲、欧洲）之间存在域泛化问题，模型在跨域时性能显著下降

Method: 基于神经元激活模式选择少量代表性目标域样本进行标注，结合持续学习启发的后训练技术防止权重漂移

Result: 该方法在少量标注预算下，性能优于线性探测和现有域自适应技术

Conclusion: 通过精心选择少量代表性样本进行标注，结合持续学习技术，可以有效解决LiDAR域自适应问题，显著提升模型跨域性能

Abstract: 3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [HINTS: Extraction of Human Insights from Time-Series Without External Sources](https://arxiv.org/abs/2512.23755)
*Sheo Yon Jhin,Noseong Park*

Main category: cs.LG

TL;DR: HINTS是一个自监督学习框架，无需外部数据即可从时间序列残差中提取人类因素（情绪、集体心理学等），通过意见动力学模型作为结构归纳偏置，提升时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 人类决策、情绪和集体心理学是影响金融经济系统时间动态的复杂因素。现有方法依赖外部数据（新闻、社交媒体）来捕捉这些因素，但存在高昂的数据依赖成本（财务、计算、实践）。

Method: 提出HINTS自监督学习框架：1）从时间序列残差中内生提取潜在人类因素；2）利用Friedkin-Johnsen意见动力学模型作为结构归纳偏置，建模演化中的社会影响、记忆和偏见模式；3）将提取的人类因素作为注意力图集成到最先进的骨干模型中。

Result: 在9个真实世界和基准数据集上的实验表明，HINTS能持续提升预测准确性。案例研究和消融研究验证了其可解释性，提取的因素与现实世界事件有强语义对齐，展示了实际效用。

Conclusion: HINTS提供了一种无需外部数据即可从时间序列中提取人类因素的有效方法，通过结合意见动力学模型的结构归纳偏置，不仅提升了预测性能，还提供了可解释的洞察，具有实际应用价值。

Abstract: Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.

</details>


### [10] [FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading](https://arxiv.org/abs/2512.23773)
*Molei Qin,Xinyu Cai,Yewen Li,Haochong Xia,Chuqiao Zong,Shuo Sun,Xinrun Wang,Bo An*

Main category: cs.LG

TL;DR: FineFT是一个针对加密货币期货交易的三阶段集成强化学习框架，通过选择性更新、盈利能力筛选和VAE引导的风险管理，解决高杠杆带来的训练不稳定和风险控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法主要针对现货市场，无法直接应用于高杠杆的期货市场，面临两大挑战：1）高杠杆放大奖励波动，导致训练不稳定难以收敛；2）缺乏对能力边界的自我认知，在新市场状态下（如黑天鹅事件）面临重大损失风险。

Method: 提出三阶段集成RL框架：阶段I通过集成TD误差选择性更新Q学习器以提高收敛性；阶段II基于盈利能力筛选Q学习器，并训练VAE识别学习器的能力边界；阶段III根据训练好的VAE指导，从筛选后的集成和保守策略中选择，以保持盈利能力并降低新市场状态下的风险。

Result: 在高频交易环境下对加密货币期货进行实验（5倍杠杆），FineFT在6个金融指标上优于12个SOTA基线，风险降低超过40%的同时获得优于第二名的盈利能力。可视化显示不同智能体专攻不同市场动态，消融研究证实VAE路由有效降低最大回撤，选择性更新改善收敛和性能。

Conclusion: FineFT成功解决了高杠杆期货交易中的训练不稳定和风险管理问题，通过集成RL框架实现了稳定的训练和有效的风险控制，在保持盈利能力的同时显著降低风险。

Abstract: Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.

</details>


### [11] [GARDO: Reinforcing Diffusion Models without Reward Hacking](https://arxiv.org/abs/2512.24138)
*Haoran He,Yuxiao Ye,Jie Liu,Jiajun Liang,Zhiyong Wang,Ziyang Yuan,Xintao Wang,Hangyu Mao,Pengfei Wan,Ling Pan*

Main category: cs.LG

TL;DR: GARDO框架通过选择性正则化、自适应参考模型更新和多样性感知优化，解决了扩散模型在线强化学习中奖励黑客、探索不足和模式崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型通过在线强化学习进行微调可以提升文本到图像的匹配度，但由于视觉任务难以精确指定真实目标，模型通常使用只能部分捕捉真实目标的代理奖励进行优化。这种不匹配导致奖励黑客问题——代理分数增加但真实图像质量下降，生成多样性崩溃。现有解决方案通常添加对参考策略的正则化来防止奖励黑客，但这会牺牲样本效率并阻碍探索新的高奖励区域，因为参考策略通常是次优的。

Method: 提出GARDO（Gated and Adaptive Regularization with Diversity-aware Optimization）框架，包含三个关键机制：1）选择性正则化：仅对高不确定性的样本子集应用惩罚；2）自适应正则化：定期更新参考模型以匹配在线策略的能力，确保正则化目标的相关性；3）多样性感知优化：对高质量且高多样性的样本放大奖励，鼓励模式覆盖而不破坏优化稳定性。

Result: 在多种代理奖励和未见过的评估指标上的广泛实验表明，GARDO能够缓解奖励黑客问题，增强生成多样性，同时不牺牲样本效率或探索能力，证明了其有效性和鲁棒性。

Conclusion: GARDO框架通过选择性正则化、自适应参考模型更新和多样性感知优化，有效解决了扩散模型在线强化学习中的奖励黑客、探索不足和模式崩溃问题，在保持样本效率和探索能力的同时提升了生成质量。

Abstract: Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.

</details>


### [12] [Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning](https://arxiv.org/abs/2512.24404)
*Soham Pahari,M. Srinivas*

Main category: cs.LG

TL;DR: 本文提出ViReLoc框架，通过视觉推理进行路径规划和定位，无需依赖文本信息或实时GPS数据，在空间任务中表现优于传统基于文本的推理方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能系统主要依赖文本信息进行推理，这在空间任务（如视觉导航和地理定位）中存在局限性。文本推理难以理解空间依赖关系和几何关系，限制了系统在空间任务中的有效性。

Method: 提出Geo-Consistent Visual Planning范式，开发ViReLoc框架：1）仅使用视觉表示进行规划和定位；2）学习空间依赖和几何关系；3）在视觉域中进行逐步推理编码；4）使用强化学习目标优化；5）集成对比学习和自适应特征交互来对齐跨视角并减少视点差异。

Result: 在多种导航和定位场景中的实验显示，ViReLoc在空间推理准确性和跨视角检索性能方面均有持续改进，证明了视觉推理在导航和定位任务中的有效性。

Conclusion: 视觉推理是导航和定位任务的强有力补充方法，可以在无需实时GPS数据的情况下执行空间任务，提供更安全的导航解决方案，为空间推理开辟了新方向。

Abstract: Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.

</details>


### [13] [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/abs/2512.25070)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.LG

TL;DR: 训练语言模型进行开放式预测，使用新闻数据合成预测问题，开发OpenForecaster 8B模型，在准确率、校准和一致性方面表现优异，媲美更大规模的专有模型。


<details>
  <summary>Details</summary>
Motivation: 高风险决策需要在不确定性下对未来进行推理。传统语言模型在开放式预测任务上能力有限，需要专门训练来提升预测性能。

Method: 1. 从每日新闻中的全球事件自动合成预测问题；2. 使用离线新闻语料防止信息泄露；3. 训练Qwen3思维模型；4. 结合检索机制；5. 改进强化学习的奖励函数；6. 在2025年5-8月进行留出测试。

Result: OpenForecaster 8B模型在准确率、校准和一致性方面显著提升，匹配甚至超越更大规模的专有模型。预测训练的校准改进能泛化到其他基准测试。

Conclusion: 通过专门的数据集和训练方法，可以显著提升语言模型的预测能力。开源模型、代码和数据将促进语言模型预测研究的广泛发展。

Abstract: High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化智能体框架，通过将LLM集成到"计划-执行-总结"认知范式中，将进化搜索映射为推理密集型过程，显著提高进化效率并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法缺乏结构化推理，导致从静态LLM向自改进智能体过渡困难。现有方法在高维代码空间中面临早熟收敛和低效探索的问题。

Method: 1. 集成LLM到"计划-执行-总结"认知范式；2. 采用混合进化记忆系统，结合多岛模型、MAP-Elites和自适应玻尔兹曼选择；3. 实例化为通用智能体（算法发现）和ML智能体（管道优化）。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比OpenEvolve、ShinkaEvolve等基线方法进化效率提升高达60%，同时发现更优解决方案。

Conclusion: LoongFlow标志着自主科学发现的重要进展，能够以更低的计算开销生成专家级解决方案，实现了从静态LLM到自进化智能体的有效过渡。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning](https://arxiv.org/abs/2512.23765)
*Tiancheng Su,Meicong Zhang,Guoxiu He*

Main category: cs.CL

TL;DR: EASD通过引入基于熵的动态惩罚机制，在保持SD效率的同时，使推理性能可能超越目标模型本身。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法中，草稿模型与目标模型过度对齐，导致性能受限于目标模型的上限。需要一种方法既能保持加速效果，又能突破目标模型的性能限制。

Method: 在标准推测解码基础上，引入动态熵惩罚机制：利用采样分布的熵量化模型不确定性，当两个模型都呈现高熵且top-N预测有显著重叠时，拒绝相应token并由目标LLM重新采样，防止低置信度错误传播。

Result: 在多个推理基准测试中，EASD一致优于现有推测解码方法，并且在大多数情况下超越了目标LLM本身的性能，同时保持了与标准SD相当的效率。

Conclusion: EASD通过熵感知机制实现了训练自由的性能提升，不仅加速了推理过程，还突破了目标模型的性能上限，为推测解码技术提供了新的发展方向。

Abstract: Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.

</details>


### [16] [Modeling Language as a Sequence of Thoughts](https://arxiv.org/abs/2512.25026)
*Nasim Borazjanizadeh,James McClelland*

Main category: cs.CL

TL;DR: 提出Thought Gestalt模型，通过双层抽象（token级和句子级"思想"状态）改进Transformer语言模型，提高数据效率和关系方向泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有Transformer语言模型主要依赖表层共现统计，缺乏对实体和事件的全局一致潜在表示，导致关系方向（如逆转诅咒）、上下文错误和数据效率低下。受认知科学启发，人类理解语言时会将其转换为紧凑的事件式表示，而字面形式短暂存在。

Method: 提出Thought Gestalt模型，一种循环Transformer，在token和句子级"思想"状态两个抽象层次上建模语言。模型逐句生成token，同时交叉关注先前句子表示的记忆。使用相同参数集生成token和句子表示，通过单一目标（下一个token交叉熵）训练，保留写入记忆的句子表示计算图，使未来token损失的梯度通过交叉注意力反向传播优化早期句子向量生成参数。

Result: 在扩展实验中，TG相比匹配的GPT-2运行持续提高效率，扩展拟合表明GPT-2需要约5-8%更多数据和33-42%更多参数才能达到TG的损失水平。TG还在父子逆转诅咒探测任务上减少了关系方向泛化错误。

Conclusion: Thought Gestalt模型通过引入句子级抽象表示，有效解决了传统Transformer在全局一致性表示方面的不足，提高了数据效率和关系方向泛化能力，为语言建模提供了更接近人类认知的处理方式。

Abstract: Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.

</details>
