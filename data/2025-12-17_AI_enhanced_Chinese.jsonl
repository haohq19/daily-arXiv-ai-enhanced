{"id": "2512.13981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13981", "abs": "https://arxiv.org/abs/2512.13981", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair", "comment": null, "summary": "Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a \"glad\" display with a brief confirmation after success, and a \"sad\" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u4efb\u52a1\u8868\u73b0\u548c\u60c5\u611f\u8868\u8fbe\u5982\u4f55\u52a8\u6001\u5f71\u54cd\u4eba\u7c7b\u4fe1\u4efb\uff0c\u53d1\u73b0\u6210\u529f\u63d0\u5347\u4fe1\u4efb\u3001\u5931\u8d25\u5bfc\u81f4\u4fe1\u4efb\u9aa4\u964d\uff0c\u9053\u6b49\u8868\u8fbe\u80fd\u90e8\u5206\u4fee\u590d\u4fe1\u4efb\uff0c\u4e14\u5e74\u9f84\u548c\u6001\u5ea6\u8c03\u8282\u4fe1\u4efb\u52a8\u6001\u53d8\u5316", "motivation": "\u5f53\u524dAEC\u884c\u4e1a\u4e2d\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u5c06\u4fe1\u4efb\u89c6\u4e3a\u9759\u6001\u56e0\u7d20\uff0c\u7f3a\u4e4f\u5bf9\u4fe1\u4efb\u5728\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u5982\u4f55\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6307\u5bfc\u3002\u9700\u8981\u4e86\u89e3\u673a\u5668\u4eba\u4efb\u52a1\u8868\u73b0\u548c\u60c5\u611f\u8868\u8fbe\u5982\u4f55\u5851\u9020\u4eba\u7c7b\u4fe1\u4efb\u7684\u52a8\u6001\u53d8\u5316", "method": "\u8bbe\u8ba1\u53d7\u63a7\u7ec4\u5185\u7814\u7a76\uff0c\u5305\u542b\u4e24\u4e2a\u5efa\u7b51\u4efb\u52a1\uff1a\u7269\u6599\u9012\u9001\uff08\u7269\u7406\u534f\u52a9\uff09\u548c\u4fe1\u606f\u6536\u96c6\uff08\u611f\u77e5\u534f\u52a9\uff09\u3002\u4f7f\u752814\u9879HRI\u4fe1\u4efb\u611f\u77e5\u91cf\u8868\u52a0\u91cd\u65b0\u59d4\u6d3e\u9009\u62e9\uff0c\u56db\u6b21\u6d4b\u91cf\u4fe1\u4efb\u3002\u673a\u5668\u4eba\u4ea7\u751f\u4e24\u79cd\u591a\u6a21\u6001\u8868\u8fbe\uff1a\u6210\u529f\u540e\u7684\"\u9ad8\u5174\"\u786e\u8ba4\u548c\u5931\u8d25\u540e\u7684\"\u60b2\u4f24\"\u9053\u6b49\u52a0\u4e8c\u6b21\u673a\u4f1a\u8bf7\u6c42\u300230\u540d\u53c2\u4e0e\u8005\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u4e0e\u56db\u8db3\u5e73\u53f0\u4e92\u52a8", "result": "\u673a\u5668\u4eba\u6210\u529f\u53ef\u9760\u589e\u52a0\u4fe1\u4efb\uff0c\u5931\u8d25\u5bfc\u81f4\u4fe1\u4efb\u9aa4\u964d\uff0c\u57fa\u4e8e\u9053\u6b49\u7684\u8868\u8fbe\u90e8\u5206\u6062\u590d\u4fe1\u4efb\uff08\u7269\u6599\u9012\u900144%\u6062\u590d\uff0c\u4fe1\u606f\u6536\u96c638%\u6062\u590d\uff09\u3002\u9879\u76ee\u7ea7\u5206\u6790\u663e\u793a\u6062\u590d\u7684\u4fe1\u4efb\u4e3b\u8981\u7531\u4ea4\u4e92\u548c\u6c9f\u901a\u56e0\u7d20\u9a71\u52a8\uff0c\u80fd\u529b\u90e8\u5206\u6062\u590d\uff0c\u81ea\u4e3b\u6027\u65b9\u9762\u53d8\u5316\u6700\u5c0f\u3002\u5e74\u9f84\u7ec4\u548c\u5148\u524d\u6001\u5ea6\u8c03\u8282\u4fe1\u4efb\u52a8\u6001\uff1a\u5e74\u8f7b\u53c2\u4e0e\u8005\u53d8\u5316\u66f4\u5927\u4f46\u77ed\u6682\uff0c25\u5c81\u5de6\u53f3\u53c2\u4e0e\u8005\u4fee\u590d\u6700\u6301\u4e45\uff0c\u5e74\u957f\u53c2\u4e0e\u8005\u6700\u4fdd\u5b88", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u6839\u636e\u4efb\u52a1\u9700\u6c42\u548c\u7528\u6237\u7279\u5f81\u8c03\u6574\u4fee\u590d\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ee5\u652f\u6301\u5efa\u7b51\u5de5\u5730\u673a\u5668\u4eba\u7684\u5b89\u5168\u3001\u9ad8\u6548\u91c7\u7528\u3002\u4fe1\u4efb\u662f\u52a8\u6001\u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u673a\u5668\u4eba\u8868\u8fbe\u8fdb\u884c\u8c03\u8282\uff0c\u4e14\u53d7\u7528\u6237\u7279\u5f81\u5f71\u54cd"}}
{"id": "2512.13706", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13706", "abs": "https://arxiv.org/abs/2512.13706", "authors": ["John Graham Reynolds"], "title": "Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training", "comment": "11 pages, 2 figures. Code available at https://github.com/johngrahamreynolds/mathematical_catastrophe_mitigation. Models available at https://huggingface.co/collections/MarioBarbeque/catastrophic-forgetting-in-mathematical-reasoning", "summary": "When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\\% to 12.0\\% but causes NLI accuracy to collapse from 81.0\\% to 16.5\\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\\% math accuracy (matching math-only) while preserving 86.2\\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.", "AI": {"tldr": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u80fd\u5b8c\u5168\u6d88\u9664\u9057\u5fd8\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u6027\u80fd", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\uff09\u5fae\u8c03\u65f6\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u6a21\u578b\u4f1a\u5931\u53bb\u5148\u524d\u5b66\u4e60\u7684\u4e00\u822c\u80fd\u529b", "method": "\u5728Flan-T5-Base\u6a21\u578b\u4e0a\u4f7f\u7528DeepMind Mathematics\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u6d4b\u91cf\u5728MultiNLI\u4e0a\u7684\u9057\u5fd8\u7a0b\u5ea6\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u6570\u5b66\u548cNLI\u793a\u4f8b\u4ea4\u9519\u8bad\u7ec3", "result": "\u7eaf\u6570\u5b66\u8bad\u7ec3\u5c06\u6570\u5b66\u51c6\u786e\u7387\u4ece3.1%\u63d0\u5347\u523012.0%\uff0c\u4f46\u5bfc\u81f4NLI\u51c6\u786e\u7387\u4ece81.0%\u66b4\u8dcc\u81f316.5%\uff08\u4e0b\u964d64.5\u4e2a\u767e\u5206\u70b9\uff09\u3002\u6df7\u5408\u8bad\u7ec3\uff081:1\u6bd4\u4f8b\uff09\u5b8c\u5168\u6d88\u9664\u9057\u5fd8\uff0c\u4fdd\u630112.0%\u6570\u5b66\u51c6\u786e\u7387\u7684\u540c\u65f6\u7ef4\u630186.2%\u7684NLI\u51c6\u786e\u7387", "conclusion": "\u4e13\u4e1a\u5316\u8bad\u7ec3\u4e0d\u9700\u8981\u4ee5\u9057\u5fd8\u4e00\u822c\u80fd\u529b\u4e3a\u4ee3\u4ef7\uff0c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u80fd\u6709\u6548\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5bf9\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u53ef\u80fd\u5e26\u6765\u8d85\u8d8a\u9057\u5fd8\u9884\u9632\u7684\u989d\u5916\u76ca\u5904"}}
{"id": "2512.13710", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13710", "abs": "https://arxiv.org/abs/2512.13710", "authors": ["Edwin Oluoch Awino", "Denis Machanda"], "title": "Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables", "comment": null, "summary": "Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408SAR\u5f71\u50cf\u4e0e\u591a\u6e90\u73af\u5883\u6570\u636e\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5bf9\u80af\u5c3c\u4e9aNyando\u6cb3\u6d41\u57df\u8fdb\u884c\u6d2a\u6c34\u6613\u53d1\u6027\u5efa\u6a21\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u8bc6\u522b\u51fa\u7ef4\u591a\u5229\u4e9a\u6e56\u9644\u8fd1\u4f4e\u6d3c\u5e73\u539f\u4e3a\u6700\u9ad8\u98ce\u9669\u533a\u3002", "motivation": "\u6d2a\u6c34\u662f\u5168\u7403\u6700\u5177\u7834\u574f\u6027\u7684\u81ea\u7136\u707e\u5bb3\u4e4b\u4e00\uff0c\u5bf9\u751f\u6001\u7cfb\u7edf\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u4eba\u7c7b\u751f\u8ba1\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u5728\u6570\u636e\u6709\u9650\u7684\u5730\u533a\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u6d2a\u6c34\u6613\u53d1\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u548c\u571f\u5730\u5229\u7528\u89c4\u5212\u3002", "method": "\u4f7f\u7528Sentinel-1\u53cc\u6781\u5316SAR\u6570\u636e\u5904\u74062024\u5e745\u6708\u6d2a\u6c34\u4e8b\u4ef6\uff0c\u751f\u6210\u4e8c\u8fdb\u5236\u6d2a\u6c34\u6e05\u5355\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u3002\u7ed3\u5408\u516d\u4e2a\u6761\u4ef6\u56e0\u5b50\uff08\u5761\u5ea6\u3001\u9ad8\u7a0b\u3001\u5761\u5411\u3001\u571f\u5730\u5229\u7528/\u571f\u5730\u8986\u76d6\u3001\u571f\u58e4\u7c7b\u578b\u3001\u8ddd\u6cb3\u6d41\u8ddd\u79bb\uff09\uff0c\u8bad\u7ec3\u56db\u79cd\u76d1\u7763\u5206\u7c7b\u5668\uff1a\u903b\u8f91\u56de\u5f52\u3001\u5206\u7c7b\u56de\u5f52\u6811\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u968f\u673a\u68ee\u6797\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u7387=0.762\uff1bKappa=0.480\uff09\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e09\u79cd\u6a21\u578b\u3002\u57fa\u4e8eRF\u7684\u6613\u53d1\u6027\u5730\u56fe\u663e\u793a\uff0c\u7ef4\u591a\u5229\u4e9a\u6e56\u9644\u8fd1\u7684Kano\u5e73\u539f\u4f4e\u6d3c\u5730\u533a\u5177\u6709\u6700\u9ad8\u7684\u6d2a\u6c34\u8106\u5f31\u6027\uff0c\u8fd9\u4e0e\u5386\u53f2\u6d2a\u6c34\u8bb0\u5f55\u548c2024\u5e745\u6708\u6d2a\u6c34\u4e8b\u4ef6\u7684\u5f71\u54cd\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6570\u636e\u6709\u9650\u7684\u5730\u533a\uff0c\u7ed3\u5408SAR\u6570\u636e\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6d2a\u6c34\u6613\u53d1\u6027\u5236\u56fe\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u751f\u6210\u7684\u6613\u53d1\u6027\u5730\u56fe\u4e3a\u707e\u5bb3\u98ce\u9669\u51cf\u5c11\u3001\u571f\u5730\u5229\u7528\u89c4\u5212\u548c\u9884\u8b66\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2512.14054", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14054", "abs": "https://arxiv.org/abs/2512.14054", "authors": ["Humaira Tasnim", "Ashik E Rasul", "Bruce Jo", "Hyung-Jin Yoon"], "title": "Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation", "comment": null, "summary": "Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c3a\u5ea6\u81ea\u9002\u5e94\u7684\u53cc\u4e13\u5bb6\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u8bad\u7ec3\u7684YOLOv8\u6a21\u578b\u5206\u522b\u5904\u7406\u8fdc\u8ddd\u79bb\u548c\u8fd1\u8ddd\u79bb\u76f4\u5347\u673a\u576a\u68c0\u6d4b\uff0c\u5728\u964d\u843d\u8fc7\u7a0b\u4e2d\u6839\u636e\u51e0\u4f55\u95e8\u63a7\u673a\u5236\u9009\u62e9\u6700\u5408\u9002\u7684\u4e13\u5bb6\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u81ea\u4e3b\u98de\u884c\u5668\u5728GPS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u964d\u843d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u4e3b\u98de\u884c\u5668\u5728GPS\u62d2\u6b62\u6216\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u964d\u843d\u9700\u8981\u51c6\u786e\u7684\u76f4\u5347\u673a\u576a\u68c0\u6d4b\u3002\u4f20\u7edf\u5355\u6a21\u578b\u68c0\u6d4b\u5668\u5728\u964d\u843d\u8fc7\u7a0b\u4e2d\u9762\u4e34\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u6311\u6218\uff1a\u9ad8\u7a7a\u65f6\u76f4\u5347\u673a\u576a\u5c0f\u800c\u6a21\u7cca\uff0c\u63a5\u8fd1\u5730\u9762\u65f6\u5219\u5927\u800c\u6e05\u6670\u3002\u5355\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u5728\u8fd9\u4e24\u79cd\u6781\u7aef\u5c3a\u5ea6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5c3a\u5ea6\u81ea\u9002\u5e94\u53cc\u4e13\u5bb6\u611f\u77e5\u6846\u67b6\uff1a1) \u4f7f\u7528HelipadCat\u6570\u636e\u96c6\u8bad\u7ec3\u4e24\u4e2aYOLOv8\u4e13\u5bb6\u6a21\u578b\uff0c\u5206\u522b\u4e13\u95e8\u9488\u5bf9\u8fdc\u8ddd\u79bb\uff08\u5c0f\u5c3a\u5ea6\uff09\u548c\u8fd1\u8ddd\u79bb\uff08\u5927\u5c3a\u5ea6\uff09\u76f4\u5347\u673a\u576a\u68c0\u6d4b\uff1b2) \u5728\u63a8\u7406\u65f6\u5e76\u884c\u8fd0\u884c\u4e24\u4e2a\u4e13\u5bb6\uff1b3) \u5f15\u5165\u51e0\u4f55\u95e8\u63a7\u673a\u5236\uff0c\u6839\u636e\u98de\u884c\u5668\u89c6\u89d2\u9009\u62e9\u9884\u6d4b\u6700\u4e00\u81f4\u7684\u4e13\u5bb6\u8f93\u51fa\uff1b4) \u5728CARLA\u548cNASA GUAM\u96c6\u6210\u7684\u95ed\u73af\u964d\u843d\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u5355\u68c0\u6d4b\u5668\u57fa\u7ebf\uff0c\u8be5\u6846\u67b6\u5728\u964d\u843d\u8fc7\u7a0b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\uff1a1) \u5bf9\u51c6\u7a33\u5b9a\u6027\uff1b2) \u964d\u843d\u7cbe\u5ea6\uff1b3) \u6574\u4f53\u9c81\u68d2\u6027\u3002\u7279\u522b\u662f\u5728\u5bbd\u9ad8\u5ea6\u8303\u56f4\u5185\u907f\u514d\u4e86\u5355\u68c0\u6d4b\u5668\u7cfb\u7edf\u5e38\u89c1\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u964d\u843d\u95ee\u9898\u8bbe\u8ba1\u7684\u5c3a\u5ea6\u611f\u77e5\u4e13\u5bb6\u8def\u7531\u7b56\u7565\uff0c\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u81ea\u4e3b\u4e0b\u964d\u7684\u5f39\u6027\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u591a\u4e13\u5bb6\u81ea\u4e3b\u98de\u884c\u5668\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u53cc\u4e13\u5bb6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u964d\u843d\u8fc7\u7a0b\u4e2d\u7684\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u6311\u6218\u3002"}}
{"id": "2512.13712", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.13712", "abs": "https://arxiv.org/abs/2512.13712", "authors": ["Eric Guo"], "title": "Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data", "comment": null, "summary": "Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \\textit{Low risk}, \\textit{Alert}, and \\textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.", "AI": {"tldr": "\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u5e9f\u6c34\u76d1\u6d4b\u3001\u6c14\u8c61\u548c\u7a7a\u6c14\u8d28\u91cf\u6570\u636e\uff0c\u9884\u6d4b\u7f8e\u56fdRSV\u76f8\u5173\u4f4f\u9662\u98ce\u9669\u7b49\u7ea7\uff0c\u5e76\u521b\u5efa\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u4f9b\u516c\u5171\u536b\u751f\u51b3\u7b56\u4f7f\u7528\u3002", "motivation": "\u547c\u5438\u9053\u5408\u80de\u75c5\u6bd2\uff08RSV\uff09\u662f\u5bfc\u81f4\u5e7c\u513f\u4f4f\u9662\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5176\u66b4\u53d1\u53d7\u73af\u5883\u6761\u4ef6\u5f3a\u70c8\u5f71\u54cd\u3002\u9700\u8981\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6e90\u6765\u9884\u6d4bRSV\u76f8\u5173\u4f4f\u9662\u98ce\u9669\uff0c\u4ee5\u652f\u6301\u53ca\u65f6\u7684\u516c\u5171\u536b\u751f\u5e72\u9884\u3002", "method": "\u6574\u5408\u6bcf\u5468\u4f4f\u9662\u7387\u3001\u5e9f\u6c34RSV\u6c34\u5e73\u3001\u6bcf\u65e5\u6c14\u8c61\u6d4b\u91cf\u548c\u7a7a\u6c14\u6c61\u67d3\u7269\u6d53\u5ea6\u6570\u636e\u3002\u4f7f\u7528CART\u3001\u968f\u673a\u68ee\u6797\u548cBoosting\u7b49\u5206\u7c7b\u6a21\u578b\uff0c\u9884\u6d4bRSV\u76f8\u5173\u4f4f\u9662\u7387\u7684\u98ce\u9669\u7b49\u7ea7\uff08\u4f4e\u98ce\u9669\u3001\u8b66\u62a5\u3001\u6d41\u884c\uff09\u3002", "result": "\u5e9f\u6c34RSV\u6c34\u5e73\u662f\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u5176\u6b21\u662f\u6e29\u5ea6\u3001\u81ed\u6c27\u6c34\u5e73\u548c\u6bd4\u6e7f\u7b49\u6c14\u8c61\u548c\u7a7a\u6c14\u8d28\u91cf\u53d8\u91cf\u3002\u53d1\u73b0\u7f8e\u56fd\u539f\u4f4f\u6c11\u548c\u963f\u62c9\u65af\u52a0\u539f\u4f4f\u6c11\u7684RSV\u76f8\u5173\u4f4f\u9662\u7387\u663e\u8457\u66f4\u9ad8\uff0c\u9ad8\u6d77\u62d4\u5730\u533a\uff08\u6c14\u538b\u8f83\u4f4e\uff09\u7684\u4f4f\u9662\u7387\u4e5f\u6301\u7eed\u8f83\u9ad8\u3002", "conclusion": "\u7ed3\u5408\u73af\u5883\u548c\u793e\u533a\u76d1\u6d4b\u6570\u636e\u5bf9\u9884\u6d4bRSV\u66b4\u53d1\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u66f4\u53ca\u65f6\u7684\u516c\u5171\u536b\u751f\u5e72\u9884\u548c\u8d44\u6e90\u5206\u914d\u3002\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0fR Shiny\u4eea\u8868\u677f\uff0c\u4f9b\u7528\u6237\u63a2\u7d22\u5404\u5ddeRSV\u98ce\u9669\u6c34\u5e73\u3001\u53ef\u89c6\u5316\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u5f71\u54cd\u5e76\u751f\u6210\u9884\u6d4b\u3002"}}
{"id": "2512.14142", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14142", "abs": "https://arxiv.org/abs/2512.14142", "authors": ["Hongqiu Ni", "Jiabao Zhang", "Guopeng Li", "Zilong Wang", "Ruiqi Wu", "Chi Zhang", "Haisheng Tan"], "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents", "comment": "12 pages, 8 figures", "summary": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.", "AI": {"tldr": "Astraea\uff1a\u4e00\u4e2a\u9762\u5411LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u670d\u52a1\u5f15\u64ce\uff0c\u901a\u8fc7\u5168\u5c40\u8c03\u5ea6\u4f18\u5316\u51cf\u5c11\u7aef\u5230\u7aef\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u63a8\u7406\u7cfb\u7edf\uff08\u5982vLLM\uff09\u4e3b\u8981\u4f18\u5316\u5c40\u90e8\u8ba1\u7b97\u6bb5\uff0c\u65e0\u6cd5\u6700\u5c0f\u5316LLM\u667a\u80fd\u4f53\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4ea4\u66ff\u8fdb\u884c\u672c\u5730\u8ba1\u7b97\u548c\u5916\u90e8API\u8c03\u7528\uff0c\u8fd9\u79cd\u6267\u884c\u6a21\u5f0f\u4e0e\u73b0\u6709\u7cfb\u7edf\u7684\u8c03\u5ea6\u7c92\u5ea6\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faAstraea\u670d\u52a1\u5f15\u64ce\uff0c\u91c7\u7528\u72b6\u6001\u611f\u77e5\u7684\u5206\u5c42\u8c03\u5ea6\u7b97\u6cd5\uff0c\u6574\u5408\u8bf7\u6c42\u5386\u53f2\u72b6\u6001\u548c\u672a\u6765\u9884\u6d4b\uff0c\u52a8\u6001\u5206\u7c7bI/O\u5bc6\u96c6\u578b\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u8bf7\u6c42\uff0c\u4f7f\u7528\u589e\u5f3a\u7684HRRN\u7b56\u7565\u5e73\u8861\u6548\u7387\u4e0e\u516c\u5e73\u6027\uff0c\u5e76\u5b9e\u73b0\u81ea\u9002\u5e94KV\u7f13\u5b58\u7ba1\u7406\u5668\u6765\u667a\u80fd\u5904\u7406I/O\u7b49\u5f85\u671f\u95f4\u7684\u667a\u80fd\u4f53\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAstraea\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747JCT\u964d\u4f4e\u9ad8\u8fbe25.5%\uff0c\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u5bf9\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "Astraea\u901a\u8fc7\u5c06\u4f18\u5316\u91cd\u70b9\u4ece\u5c40\u90e8\u6bb5\u8f6c\u79fb\u5230\u5168\u5c40\u8bf7\u6c42\u751f\u547d\u5468\u671f\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2512.13717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13717", "abs": "https://arxiv.org/abs/2512.13717", "authors": ["Ekaterina Sysoykova", "Bernhard Anzengruber-Tanase", "Michael Haslgrubler", "Philipp Seidl", "Alois Ferscha"], "title": "Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints", "comment": "12 pages", "summary": "Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.", "AI": {"tldr": "\u63d0\u51fa\u5169\u968e\u6bb5\u806f\u90a6\u5c11\u6a23\u672c\u5b78\u7fd2\u6846\u67b6\uff0c\u7528\u65bc\u500b\u4eba\u5316EEG\u7672\u7647\u6aa2\u6e2c\uff0c\u89e3\u6c7a\u81e8\u5e8a\u6578\u64da\u5206\u6563\u3001\u7a00\u7f3a\u548c\u96b1\u79c1\u554f\u984c\u3002", "motivation": "\u81e8\u5e8aEEG\u6578\u64da\u901a\u5e38\u7a00\u7f3a\u3001\u5206\u6563\u5728\u4e0d\u540c\u6a5f\u69cb\uff0c\u4e14\u53d7\u96b1\u79c1\u6cd5\u898f\u9650\u5236\u7121\u6cd5\u96c6\u4e2d\uff0c\u4f7f\u5f97\u57fa\u65bcAI\u7684\u7672\u7647\u6aa2\u6e2c\u6a21\u578b\u958b\u767c\u56f0\u96e3\u3002", "method": "\u5169\u968e\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u968e\u6bb5\u4f7f\u7528\u806f\u90a6\u5b78\u7fd2\u5728\u975eIID\u6a21\u64ec\u91ab\u9662\u7ad9\u9ede\u4e0a\u5fae\u8abfBIOT\u6a21\u578b\uff1b\u7b2c\u4e8c\u968e\u6bb5\u4f7f\u7528\u806f\u90a6\u5c11\u6a23\u672c\u500b\u4eba\u5316\uff0c\u50c5\u75285\u500b\u6a19\u8a3bEEG\u7247\u6bb5\u70ba\u6bcf\u4f4d\u60a3\u8005\u81ea\u9069\u61c9\u5206\u985e\u5668\u3002", "result": "\u806f\u90a6\u5fae\u8abf\u5e73\u8861\u6e96\u78ba\u73870.43\uff08\u96c6\u4e2d\u5f0f0.52\uff09\uff1bFFSL\u968e\u6bb5\u5ba2\u6236\u7aef\u7279\u5b9a\u6a21\u578b\u5e73\u5747\u5e73\u8861\u6e96\u78ba\u7387\u90540.77\uff0cCohen's kappa 0.62\uff0c\u52a0\u6b0aF1 0.73\u3002", "conclusion": "FFSL\u6846\u67b6\u80fd\u5728\u73fe\u5be6\u6578\u64da\u53ef\u7528\u6027\u548c\u96b1\u79c1\u9650\u5236\u4e0b\uff0c\u652f\u6301\u6709\u6548\u7684\u60a3\u8005\u81ea\u9069\u61c9\u7672\u7647\u6aa2\u6e2c\u3002"}}
{"id": "2512.13955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13955", "abs": "https://arxiv.org/abs/2512.13955", "authors": ["Sindhuja Madabushi", "Dawood Wasif", "Jin-Hee Cho"], "title": "MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning", "comment": null, "summary": "Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.", "AI": {"tldr": "MURIM\u662f\u4e00\u4e2a\u591a\u7ef4\u5ea6\u58f0\u8a89\u6fc0\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u7efc\u5408\u8003\u8651\u5ba2\u6237\u7aef\u53ef\u9760\u6027\u3001\u9690\u79c1\u3001\u8d44\u6e90\u5bb9\u91cf\u548c\u516c\u5e73\u6027\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6fc0\u52b1\u3001\u9690\u79c1\u548c\u8d44\u6e90\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u5ba2\u6237\u7aef\u6fc0\u52b1\u4e0d\u8db3\u3001\u9690\u79c1\u98ce\u9669\u548c\u8d44\u6e90\u7ea6\u675f\u7b49\u5173\u952e\u6311\u6218\u3002\u8bc4\u4f30\u5ba2\u6237\u7aef\u53ef\u9760\u6027\u5bf9\u4e8e\u516c\u5e73\u5206\u914d\u6fc0\u52b1\u548c\u786e\u4fdd\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6570\u636e\u5bf9\u5168\u5c40\u6a21\u578b\u6709\u5b9e\u8d28\u6027\u8d21\u732e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faMURIM\u591a\u7ef4\u5ea6\u58f0\u8a89\u6fc0\u52b1\u673a\u5236\uff0c\u57fa\u4e8e\u5ba2\u6237\u7aef\u8d21\u732e\u3001\u5ef6\u8fdf\u548c\u58f0\u8a89\u5206\u914d\u6fc0\u52b1\uff0c\u914d\u5907\u53ef\u9760\u6027\u9a8c\u8bc1\u6a21\u5757\uff0c\u9632\u6b62\u6076\u610f\u6216\u4e0d\u53ef\u9760\u5ba2\u6237\u7aef\u83b7\u5f97\u4e0d\u5e94\u5f97\u7684\u5956\u52b1\u3002", "result": "\u5728MNIST\u3001FMNIST\u548cADULT Income\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMURIM\u5728\u516c\u5e73\u6027\u6307\u6807\u4e0a\u63d0\u5347\u8fbe18%\uff0c\u9690\u79c1\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e5-9%\uff0c\u5bf9\u6295\u6bd2\u548c\u566a\u58f0\u68af\u5ea6\u653b\u51fb\u7684\u9c81\u68d2\u6027\u63d0\u5347\u8fbe85%\u3002", "conclusion": "MURIM\u6709\u6548\u7f13\u89e3\u5bf9\u6297\u5a01\u80c1\uff0c\u4fc3\u8fdb\u516c\u5e73\u771f\u5b9e\u53c2\u4e0e\uff0c\u5728\u5f02\u6784\u52a8\u6001\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u4fdd\u6301\u7a33\u5b9a\u7684\u6a21\u578b\u6536\u655b\u3002"}}
{"id": "2512.13728", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13728", "abs": "https://arxiv.org/abs/2512.13728", "authors": ["Bhavesh Kumar", "Roger Jin", "Jeffrey Quesnelle"], "title": "CurvaDion: Curvature-Adaptive Distributed Orthonormalization", "comment": "Nous Research", "summary": "As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.", "AI": {"tldr": "CurvaDion\u901a\u8fc7RMMC\u68c0\u6d4b\u9ad8\u66f2\u7387\u533a\u57df\uff0c\u53ea\u5728\u9700\u8981\u65f6\u540c\u6b65\u68af\u5ea6\uff0c\u51cf\u5c1199%\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\uff0c\u68af\u5ea6\u540c\u6b65\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u5982Dion\u867d\u7136\u901a\u8fc7\u4f4e\u79e9\u66f4\u65b0\u51cf\u5c11\u901a\u4fe1\uff0c\u4f46\u6bcf\u6b65\u90fd\u540c\u6b65\uff0c\u5ffd\u7565\u4e86\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4e0d\u540c\u533a\u57df\u5bf9\u540c\u6b65\u9700\u6c42\u7684\u53d8\u5316\uff1a\u5e73\u5766\u533a\u57df\u68af\u5ea6\u76f8\u4f3c\uff0c\u9891\u7e41\u540c\u6b65\u5197\u4f59\uff1b\u9ad8\u66f2\u7387\u533a\u57df\u9700\u8981\u534f\u8c03\u9632\u6b62\u53d1\u6563\u3002", "method": "\u63d0\u51faCurvaDion\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f8\u5bf9\u6700\u5927\u52a8\u91cf\u53d8\u5316(RMMC)\u68c0\u6d4b\u9700\u8981\u540c\u6b65\u7684\u9ad8\u66f2\u7387\u533a\u57df\u3002RMMC\u5229\u7528\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5df2\u8ba1\u7b97\u7684\u52a8\u91cf\u52a8\u6001\u4f5c\u4e3a\u65b9\u5411\u66f2\u7387\u7684\u8ba1\u7b97\u53ef\u884c\u4ee3\u7406\uff0c\u6bcf\u5c42\u4ec5\u589e\u52a0O(d)\u64cd\u4f5c\u3002\u5efa\u7acbRMMC\u4e0e\u635f\u5931\u66f2\u7387\u7684\u7406\u8bba\u8054\u7cfb\u3002", "result": "\u5728160M\u52301.3B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\uff0cCurvaDion\u5b9e\u73b099%\u7684\u901a\u4fe1\u51cf\u5c11\uff0c\u540c\u65f6\u5339\u914d\u57fa\u7ebf\u6536\u655b\u6027\u80fd\u3002", "conclusion": "CurvaDion\u901a\u8fc7\u667a\u80fd\u68c0\u6d4b\u4f18\u5316\u66f2\u7387\u53d8\u5316\uff0c\u53ea\u5728\u5fc5\u8981\u65f6\u540c\u6b65\uff0c\u663e\u8457\u51cf\u5c11\u5206\u5e03\u5f0f\u8bad\u7ec3\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6536\u655b\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13806", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13806", "abs": "https://arxiv.org/abs/2512.13806", "authors": ["Siegfried Ludwig", "Stylianos Bakas", "Konstantinos Barmpas", "Georgios Zoumpourlis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Yannis Panagakis", "Stefanos Zafeiriou"], "title": "EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models", "comment": null, "summary": "Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.", "AI": {"tldr": "\u63d0\u51faD3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u8bad\u7ec3\u5206\u79bbEEG\u4fe1\u53f7\u4e2d\u7684\u6f5c\u5728\u8111\u6d3b\u52a8\u6210\u5206\uff0c\u9632\u6b62\u9690\u85cf\u8fc7\u62df\u5408\uff0c\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u89e3\u7801EEG\u4fe1\u53f7\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5b58\u5728\u9690\u85cf\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5206\u79bb\u771f\u5b9e\u8111\u6d3b\u52a8\u6210\u5206\u4e0e\u4f2a\u8ff9\u7684\u65b9\u6cd5", "method": "\u63d0\u51fa\u89e3\u8026\u89e3\u7801\u5206\u89e3(D3)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8f93\u5165\u7a97\u53e3\u5728\u8bd5\u9a8c\u5e8f\u5217\u4e2d\u7684\u4f4d\u7f6e\u6765\u5206\u79bbEEG\u4fe1\u53f7\u7684\u6f5c\u5728\u6210\u5206\uff0c\u7c7b\u4f3c\u4e8e\u975e\u7ebf\u6027ICA\uff1b\u91c7\u7528\u5b8c\u5168\u72ec\u7acb\u7684\u5b50\u7f51\u7edc\u67b6\u6784\u786e\u4fdd\u4e25\u683c\u53ef\u89e3\u91ca\u6027", "result": "D3\u80fd\u53ef\u9760\u5206\u79bb\u8fd0\u52a8\u60f3\u8c61\u6570\u636e\u4e2d\u7684\u8111\u6d3b\u52a8\u6210\u5206\uff1b\u5728\u9002\u5f53\u6210\u5206\u5b50\u96c6\u4e0a\u8bad\u7ec3\u4e0b\u6e38\u5206\u7c7b\u5668\u53ef\u9632\u6b62\u4efb\u52a1\u76f8\u5173\u4f2a\u8ff9\u5f15\u8d77\u7684\u9690\u85cf\u8fc7\u62df\u5408\uff1b\u7ebf\u6027\u53ef\u5206\u79bb\u7684\u6f5c\u5728\u7a7a\u95f4\u652f\u6301\u6709\u6548\u7684\u5c11\u6837\u672c\u7761\u7720\u5206\u671f\u5b66\u4e60", "conclusion": "D3\u65b9\u6cd5\u80fd\u533a\u5206\u771f\u5b9e\u8111\u6d3b\u52a8\u6210\u5206\u4e0e\u865a\u5047\u7279\u5f81\uff0c\u907f\u514d\u9690\u85cf\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u5373\u53ef\u826f\u597d\u6cdb\u5316\u5230\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u5206\u79bb\u4e2a\u4f53\u8111\u8fc7\u7a0b\u7684\u65b0\u5de5\u5177"}}
{"id": "2512.14044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14044", "abs": "https://arxiv.org/abs/2512.14044", "authors": ["Zhenguo Zhang", "Haohan Zhen", "Yishen Wang", "Le Xu", "Tianchen Deng", "Xuefeng Chen", "Qu Chen", "Bo Zhang", "Wuxiong Huang"], "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving", "comment": null, "summary": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.", "AI": {"tldr": "OmniDrive-R1\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u9519\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u673a\u5236\u7edf\u4e00\u611f\u77e5\u548c\u63a8\u7406\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u6d88\u9664\u76ee\u6807\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u90e8\u7f72\u65f6\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u76ee\u6807\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u611f\u77e5\u4e0e\u63a8\u7406\u9636\u6bb5\u5206\u79bb\u3001\u4f9d\u8d56\u6602\u8d35\u7684\u5bc6\u96c6\u5b9a\u4f4d\u6807\u7b7e\u7b49\u6839\u672c\u7f3a\u9677\u3002", "method": "\u63d0\u51faOmniDrive-R1\u6846\u67b6\uff0c\u91c7\u7528\u4ea4\u9519\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u673a\u5236\u7edf\u4e00\u611f\u77e5\u548c\u63a8\u7406\u3002\u6838\u5fc3\u521b\u65b0\u662f\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6d41\u7a0b\u548cClip-GRPO\u7b97\u6cd5\u5b9e\u73b0\uff0c\u8be5\u7b97\u6cd5\u5f15\u5165\u65e0\u6807\u6ce8\u3001\u57fa\u4e8e\u8fc7\u7a0b\u7684\u5b9a\u4f4d\u5956\u52b1\u3002", "result": "\u5728DriveLMM-o1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6539\u8fdb\uff1a\u4e0e\u57fa\u7ebfQwen2.5VL-7B\u76f8\u6bd4\uff0c\u603b\u4f53\u63a8\u7406\u5206\u6570\u4ece51.77%\u63d0\u5347\u523080.35%\uff0c\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4ece37.81%\u63d0\u5347\u523073.62%\u3002", "conclusion": "OmniDrive-R1\u901a\u8fc7\u7aef\u5230\u7aef\u7edf\u4e00\u611f\u77e5\u548c\u63a8\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u89c6\u89c9\u5b9a\u4f4d\u4ee5\u53ca\u65e0\u6807\u6ce8\u7684\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u76ee\u6807\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2512.14092", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14092", "abs": "https://arxiv.org/abs/2512.14092", "authors": ["Felix Holm", "Ghazal Ghazaei", "Nassir Navab"], "title": "ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes", "comment": null, "summary": "Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.\n  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.\n  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.\n  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.", "AI": {"tldr": "ProtoFlow\uff1a\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u573a\u666f\u56fe\u539f\u578b\u5b66\u4e60\u6765\u5efa\u6a21\u590d\u6742\u624b\u672f\u5de5\u4f5c\u6d41\u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u5728\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u8be6\u7ec6\u7684\u624b\u672f\u8bc6\u522b\u5bf9AI\u8f85\u52a9\u624b\u672f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u9ad8\u6807\u6ce8\u6210\u672c\u3001\u6570\u636e\u7a00\u7f3a\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6a21\u578b\u7b49\u6311\u6218\u3002\u867d\u7136\u573a\u666f\u56fe\u63d0\u4f9b\u4e86\u624b\u672f\u4e8b\u4ef6\u7684\u7ed3\u6784\u5316\u62bd\u8c61\uff0c\u4f46\u5176\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u53d1\u6325\u3002", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8fdb\u884c\u4e30\u5bcc\u8868\u793a\u5b66\u4e60\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u539f\u578b\u7684\u5fae\u8c03\u9636\u6bb5\u53d1\u73b0\u548c\u4f18\u5316\u6838\u5fc3\u539f\u578b\uff0c\u8fd9\u4e9b\u539f\u578b\u5c01\u88c5\u4e86\u91cd\u590d\u51fa\u73b0\u7684\u3001\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u624b\u672f\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u5728CAT-SG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cProtoFlow\u4e0d\u4ec5\u8d85\u8d8a\u6807\u51c6GNN\u57fa\u7ebf\uff0c\u5728\u6709\u9650\u6570\u636e\u3001\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4ec5\u7528\u4e00\u4e2a\u624b\u672f\u89c6\u9891\u8bad\u7ec3\u4ecd\u4fdd\u6301\u5f3a\u6027\u80fd\u3002\u5b66\u4e60\u5230\u7684\u539f\u578b\u80fd\u6210\u529f\u8bc6\u522b\u4e0d\u540c\u7684\u624b\u672f\u5b50\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u6e05\u6670\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u504f\u5dee\u548c\u7f55\u89c1\u5e76\u53d1\u75c7\u6d1e\u5bdf\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9c81\u68d2\u8868\u793a\u5b66\u4e60\u4e0e\u5185\u5728\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\uff0cProtoFlow\u4ee3\u8868\u4e86\u5411\u5f00\u53d1\u66f4\u900f\u660e\u3001\u53ef\u9760\u3001\u6570\u636e\u9ad8\u6548\u7684AI\u7cfb\u7edf\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u52a0\u901f\u5176\u5728\u624b\u672f\u57f9\u8bad\u3001\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u548c\u6d41\u7a0b\u4f18\u5316\u4e2d\u7684\u4e34\u5e8a\u91c7\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.13989", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13989", "abs": "https://arxiv.org/abs/2512.13989", "authors": ["Cindy Y. Zhang", "Elif Ertekin", "Peter Orbanz", "Ryan P. Adams"], "title": "A Single Architecture for Representing Invariance Under Any Space Group", "comment": "24 pages, 7 figures", "summary": "Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.", "AI": {"tldr": "\u63d0\u51fa\u5355\u4e00\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u80fd\u81ea\u52a8\u9002\u5e94\u4efb\u4f55\u7a7a\u95f4\u7fa4\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u5bf9\u79f0\u9002\u914d\u5085\u91cc\u53f6\u57fa\u5b9e\u73b0\u6743\u91cd\u5171\u4eab\u548c\u96f6\u6837\u672c\u5b66\u4e60", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u5bf9\u79f0\u7fa4\u8bbe\u8ba1\u4e13\u95e8\u67b6\u6784\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u7279\u522b\u662f\u5728\u6750\u6599\u79d1\u5b66\u4e2d230\u4e2a\u4e09\u7ef4\u7a7a\u95f4\u7fa4\u7684\u60c5\u51b5\u4e0b\u5c24\u4e3a\u7a81\u51fa", "method": "\u6784\u5efa\u5bf9\u79f0\u9002\u914d\u7684\u5085\u91cc\u53f6\u57fa\uff0c\u901a\u8fc7\u663e\u5f0f\u8868\u5f81\u7fa4\u64cd\u4f5c\u5bf9\u5085\u91cc\u53f6\u7cfb\u6570\u7684\u7ea6\u675f\uff0c\u5c06\u8fd9\u4e9b\u7ea6\u675f\u7f16\u7801\u5230\u795e\u7ecf\u7f51\u7edc\u5c42\u4e2d\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u7a7a\u95f4\u7fa4\u7684\u6743\u91cd\u5171\u4eab", "result": "\u5728\u6750\u6599\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e76\u80fd\u8fdb\u884c\u96f6\u6837\u672c\u5b66\u4e60\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7a7a\u95f4\u7fa4", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5355\u4e00\u67b6\u6784\u9002\u5e94\u591a\u79cd\u7a7a\u95f4\u7fa4\u5bf9\u79f0\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u7fa4\u7684\u77e5\u8bc6\u8fc1\u79fb\u548c\u96f6\u6837\u672c\u6cdb\u5316"}}
{"id": "2512.14400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14400", "abs": "https://arxiv.org/abs/2512.14400", "authors": ["Fangzhou Lin", "Guoshun He", "Zhenyu Guo", "Zhe Huang", "Jinsong Tao"], "title": "GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion", "comment": null, "summary": "Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.", "AI": {"tldr": "GRAFT\u6a21\u578b\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u878d\u5408\u548c\u591a\u6e90\u4fe1\u606f\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e8b\u4ef6\u9a71\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272", "motivation": "\u7535\u529b\u8d1f\u8377\u540c\u65f6\u53d7\u5230\u5929\u6c14\u3001\u65e5\u5386\u8282\u594f\u3001\u7a81\u53d1\u4e8b\u4ef6\u548c\u653f\u7b56\u7b49\u591a\u65f6\u95f4\u5c3a\u5ea6\u5916\u751f\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u591a\u6e90\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u7535\u7f51\u611f\u77e5\u9884\u6d4b", "method": "\u63d0\u51faGRAFT\u6a21\u578b\uff0c\u6539\u8fdbSTanHOP\u4ee5\u652f\u6301\u7535\u7f51\u611f\u77e5\u9884\u6d4b\u548c\u591a\u6e90\u6587\u672c\u5e72\u9884\uff0c\u901a\u8fc7\u4e25\u683c\u5bf9\u9f50\u6bcf\u65e5\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u653f\u7b56\u6587\u672c\u4e0e\u534a\u5c0f\u65f6\u8d1f\u8377\u6570\u636e\uff0c\u5728\u8bad\u7ec3\u548c\u6eda\u52a8\u9884\u6d4b\u4e2d\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u878d\u5408\uff0c\u5e76\u63d0\u4f9b\u5373\u63d2\u5373\u7528\u7684\u5916\u90e8\u8bb0\u5fc6\u63a5\u53e3", "result": "\u5728\u6fb3\u5927\u5229\u4e9a\u4e94\u4e2a\u5dde\u7684\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRAFT\u5728\u591a\u4e2a\u533a\u57df\u548c\u9884\u6d4b\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u4e8b\u4ef6\u9a71\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u80fd\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6587\u672c\u5230\u8d1f\u8377\u6548\u5e94\u7684\u65f6\u7a7a\u5b9a\u4f4d\u548c\u6e90\u7ea7\u89e3\u91ca", "conclusion": "GRAFT\u901a\u8fc7\u6709\u6548\u6574\u5408\u591a\u6e90\u6587\u672c\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u7535\u7f51\u9884\u6d4b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u91ca\u653e\u7684\u57fa\u51c6\u6570\u636e\u548c\u4ee3\u7801\u6709\u52a9\u4e8e\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u5b9e\u8bc1\u8bc4\u4f30\u548c\u53ef\u590d\u73b0\u6027"}}
{"id": "2512.14683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14683", "abs": "https://arxiv.org/abs/2512.14683", "authors": ["Dimitris Bertsimas", "Yu Ma", "Kimberly Villalobos Carballo", "Gagan Singh", "Michal Laskowski", "Jeff Mather", "Dan Kombert", "Howard Haronian"], "title": "Early Warning Index for Patient Deteriorations in Hospitals", "comment": null, "summary": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6EWI\uff0c\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u548c\u8fd0\u8425\u6570\u636e\u9884\u6d4bICU\u5165\u9662\u3001\u6025\u6551\u56e2\u961f\u6d3e\u9063\u548c\u6b7b\u4ea1\u98ce\u9669\uff0c\u5728\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u4e0b\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u5206\u5c42\uff0cC\u7edf\u8ba1\u91cf0.796\uff0c\u5df2\u90e8\u7f72\u4e3a\u533b\u9662\u5206\u8bca\u5de5\u5177\u3002", "motivation": "\u533b\u9662\u7f3a\u4e4f\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u5229\u7528\u65e5\u76ca\u589e\u957f\u7684\u5f02\u8d28\u4e34\u5e8a\u548c\u8fd0\u8425\u6570\u636e\u6709\u6548\u9884\u6d4b\u5173\u952e\u4e8b\u4ef6\u3002\u65e9\u671f\u8bc6\u522b\u6709\u6076\u5316\u98ce\u9669\u7684\u60a3\u8005\u5bf9\u60a3\u8005\u62a4\u7406\u8d28\u91cf\u76d1\u6d4b\u548c\u533b\u751f\u62a4\u7406\u7ba1\u7406\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c06\u4e0d\u540c\u6570\u636e\u6d41\u8f6c\u5316\u4e3a\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u8bc4\u4f30\u9762\u4e34\u6570\u636e\u683c\u5f0f\u4e0d\u4e00\u81f4\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6EWI\uff08\u65e9\u671f\u9884\u8b66\u6307\u6570\uff09\uff0c\u91c7\u7528\u4eba\u673a\u534f\u540c\u6d41\u7a0b\uff1a\u4e34\u5e8a\u533b\u751f\u5e2e\u52a9\u786e\u5b9a\u8b66\u62a5\u9608\u503c\u548c\u89e3\u91ca\u6a21\u578b\u8f93\u51fa\u3002\u4f7f\u7528SHAP\uff08Shapley Additive exPlanations\uff09\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u7a81\u51fa\u663e\u793a\u9a71\u52a8\u6bcf\u4e2a\u60a3\u8005\u98ce\u9669\u7684\u4e34\u5e8a\u548c\u8fd0\u8425\u56e0\u7d20\uff08\u5982\u9884\u5b9a\u624b\u672f\u3001\u75c5\u623f\u4eba\u6570\uff09\u3002\u4ece\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\uff0c\u5c06\u60a3\u8005\u5206\u4e3a\u4e09\u4e2a\u98ce\u9669\u7b49\u7ea7\u3002", "result": "\u5728\u7f8e\u56fd\u4e00\u5bb6\u5927\u578b\u533b\u9662\u768418,633\u540d\u72ec\u7279\u60a3\u8005\u6570\u636e\u96c6\u4e0a\uff0cEWI\u5b9e\u73b0\u4e86C\u7edf\u8ba1\u91cf0.796\u3002\u76ee\u524d\u5df2\u90e8\u7f72\u4e3a\u533b\u9662\u4eea\u8868\u677f\u4e2d\u7684\u5206\u8bca\u5de5\u5177\uff0c\u7528\u4e8e\u4e3b\u52a8\u7ba1\u7406\u9ad8\u98ce\u9669\u60a3\u8005\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u5bf9\u60a3\u8005\u8fdb\u884c\u98ce\u9669\u5206\u7ea7\uff0c\u4e3a\u533b\u751f\u8282\u7701\u4e86\u5b9d\u8d35\u65f6\u95f4\uff0c\u4f7f\u4ed6\u4eec\u80fd\u4e13\u6ce8\u4e8e\u60a3\u8005\u62a4\u7406\u800c\u975e\u7b5b\u9009\u590d\u6742\u7684EHR\u6570\u636e\u3002", "conclusion": "EWI\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u9884\u6d4b\u7cfb\u7edf\u3002\u901a\u8fc7\u8bc6\u522b\u7279\u5b9a\u98ce\u9669\u9a71\u52a8\u56e0\u7d20\uff0c\u4e3a\u62a4\u7406\u4eba\u5458\u8c03\u5ea6\u548c\u5173\u952e\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u6570\u636e\u652f\u6301\u7684\u8c03\u6574\u3002\u4e34\u5e8a\u533b\u751f\u548c\u7ba1\u7406\u4eba\u5458\u53ef\u4ee5\u9884\u9632\u4e0b\u6e38\u5e76\u53d1\u75c7\uff0c\u5305\u62ec\u6602\u8d35\u7684\u624b\u672f\u6216\u9ad8\u518d\u5165\u9662\u7387\uff0c\u6539\u5584\u6574\u4f53\u60a3\u8005\u6d41\u7a0b\u3002"}}
{"id": "2512.14550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14550", "abs": "https://arxiv.org/abs/2512.14550", "authors": ["Zhiwen Yang", "Jiaju Zhang", "Yang Yi", "Jian Liang", "Bingzheng Wei", "Yan Xu"], "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration", "comment": "This paper has been accepted by MICCAI 2025", "summary": "Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u81ea\u9002\u5e94Transformer\uff08TAT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u6743\u91cd\u751f\u6210\u548c\u635f\u5931\u5e73\u8861\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u533b\u5b66\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u548c\u4efb\u52a1\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6062\u590d\uff08MedIR\uff09\u4e2d\uff0cAll-in-One\u6a21\u578b\u9700\u8981\u540c\u65f6\u5904\u7406\u591a\u79cd\u4e0d\u540c\u6a21\u6001\u548c\u9000\u5316\u7c7b\u578b\u7684\u4efb\u52a1\u3002\u7531\u4e8e\u4efb\u52a1\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5171\u4eab\u6a21\u578b\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4efb\u52a1\u5e72\u6270\uff08\u4e0d\u540c\u4efb\u52a1\u5bf9\u540c\u4e00\u53c2\u6570\u7684\u68af\u5ea6\u66f4\u65b0\u65b9\u5411\u51b2\u7a81\uff09\u548c\u4efb\u52a1\u4e0d\u5e73\u8861\uff08\u5404\u4efb\u52a1\u5b66\u4e60\u96be\u5ea6\u4e0d\u540c\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u5747\u8861\uff09\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u81ea\u9002\u5e94Transformer\uff08TAT\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u4efb\u52a1\u81ea\u9002\u5e94\u6743\u91cd\u751f\u6210\u7b56\u7565\uff1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u751f\u6210\u7279\u5b9a\u7684\u6743\u91cd\u53c2\u6570\uff0c\u907f\u514d\u5171\u4eab\u6743\u91cd\u4e0a\u7684\u68af\u5ea6\u51b2\u7a81\uff1b2\uff09\u4efb\u52a1\u81ea\u9002\u5e94\u635f\u5931\u5e73\u8861\u7b56\u7565\uff1a\u6839\u636e\u4efb\u52a1\u5b66\u4e60\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u9632\u6b62\u67d0\u4e9b\u4efb\u52a1\u4e3b\u5bfc\u6216\u8bad\u7ec3\u4e0d\u8db3\u3002", "result": "\u5728PET\u5408\u6210\u3001CT\u53bb\u566a\u548cMRI\u8d85\u5206\u8fa8\u7387\u4e09\u4e2aMedIR\u4efb\u52a1\u4e0a\uff0cTAT\u5728\u4efb\u52a1\u7279\u5b9a\u548cAll-in-One\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TAT\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u533b\u5b66\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u548c\u4efb\u52a1\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3aAll-in-One MedIR\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.14595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14595", "abs": "https://arxiv.org/abs/2512.14595", "authors": ["Mengyu Li", "Xingcheng Zhou", "Guang Chen", "Alois Knoll", "Hu Cao"], "title": "TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios", "comment": "10 pages, 9 figures", "summary": "In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8f66\u8f86\u548c\u884c\u4eba\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ddf\u8e2a\u57fa\u51c6\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e27\u7684\u76f8\u673a\u5728\u5f31\u5149\u548c\u9ad8\u8fd0\u52a8\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0d\u4f73\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "1. \u521b\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u8f66\u8f86\u548c\u884c\u4eba\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff1b2. \u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5efa\u7acb\u4e86\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ddf\u8e2a\u57fa\u51c6\uff1b3. \u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u7279\u5f81\u63d0\u53d6\u5668\u3002", "result": "\u5728\u5efa\u7acb\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672c\u6587\u901a\u8fc7\u521b\u5efa\u4e13\u7528\u6570\u636e\u96c6\u548c\u57fa\u51c6\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2512.14614", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.14614", "abs": "https://arxiv.org/abs/2512.14614", "authors": ["Wenqiang Sun", "Haiyu Zhang", "Haoyuan Wang", "Junta Wu", "Zehan Wang", "Zhenwei Wang", "Yunhong Wang", "Jun Zhang", "Tengfei Wang", "Chunchao Guo"], "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "AI": {"tldr": "WorldPlay\u662f\u4e00\u4e2a\u6d41\u5f0f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u4e16\u754c\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u957f\u671f\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u901f\u5ea6\u4e0e\u5185\u5b58\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u5185\u5b58\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u9650\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u4e16\u754c\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u671f\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u9700\u8981\u89e3\u51b3\u5185\u5b58\u8870\u51cf\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5b9e\u65f6\u901f\u5ea6\u4e0b\u5904\u7406\u957f\u5e8f\u5217\u89c6\u9891\u3002", "method": "1) \u53cc\u52a8\u4f5c\u8868\u793a\uff1a\u652f\u6301\u5bf9\u952e\u76d8\u9f20\u6807\u8f93\u5165\u7684\u9c81\u68d2\u52a8\u4f5c\u63a7\u5236\uff1b2) \u91cd\u6784\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff1a\u52a8\u6001\u91cd\u5efa\u8fc7\u53bb\u5e27\u7684\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u65f6\u95f4\u91cd\u5e27\u4fdd\u6301\u51e0\u4f55\u91cd\u8981\u4f46\u4e45\u8fdc\u5e27\u7684\u53ef\u8bbf\u95ee\u6027\uff1b3) \u4e0a\u4e0b\u6587\u5f3a\u5236\uff1a\u4e3a\u5185\u5b58\u611f\u77e5\u6a21\u578b\u8bbe\u8ba1\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5bf9\u9f50\u5e08\u751f\u6a21\u578b\u7684\u8bb0\u5fc6\u4e0a\u4e0b\u6587\u3002", "result": "WorldPlay\u80fd\u591f\u4ee524 FPS\u5b9e\u65f6\u751f\u6210720p\u957f\u5e8f\u5217\u6d41\u5f0f\u89c6\u9891\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "WorldPlay\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u7ba1\u7406\u548c\u84b8\u998f\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u4e2d\u901f\u5ea6\u4e0e\u5185\u5b58\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u4e16\u754c\u5efa\u6a21\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.14699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14699", "abs": "https://arxiv.org/abs/2512.14699", "authors": ["Sihui Ji", "Xi Chen", "Shuai Yang", "Xin Tao", "Pengfei Wan", "Hengshuang Zhao"], "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives", "comment": "Project Page: https://sihuiji.github.io/MemFlow.github.io/", "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.", "AI": {"tldr": "MemFlow\u63d0\u51fa\u52a8\u6001\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\uff0c\u6839\u636e\u6587\u672c\u63d0\u793a\u68c0\u7d22\u6700\u76f8\u5173\u7684\u5386\u53f2\u5e27\u6765\u7ef4\u62a4\u8bb0\u5fc6\u5e93\uff0c\u5b9e\u73b0\u957f\u89c6\u9891\u751f\u6210\u7684\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027", "motivation": "\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u9884\u5b9a\u4e49\u7b56\u7565\u538b\u7f29\u5386\u53f2\u5e27\u4f5c\u4e3a\u8bb0\u5fc6\uff0c\u4f46\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u9700\u8981\u53c2\u8003\u4e0d\u540c\u7684\u5386\u53f2\u7ebf\u7d22\uff0c\u56fa\u5b9a\u7b56\u7565\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42", "method": "\u5728\u751f\u6210\u6bcf\u4e2a\u65b0\u89c6\u9891\u7247\u6bb5\u524d\uff0c\u6839\u636e\u8be5\u7247\u6bb5\u7684\u6587\u672c\u63d0\u793a\u52a8\u6001\u68c0\u7d22\u6700\u76f8\u5173\u7684\u5386\u53f2\u5e27\u6765\u66f4\u65b0\u8bb0\u5fc6\u5e93\uff1b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6ce8\u610f\u529b\u5c42\u53ea\u6fc0\u6d3b\u8bb0\u5fc6\u5e93\u4e2d\u6700\u76f8\u5173\u7684token\uff0c\u4fdd\u8bc1\u751f\u6210\u6548\u7387", "result": "MemFlow\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u957f\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u8ba1\u7b97\u8d1f\u62c5\u53ef\u5ffd\u7565\uff08\u76f8\u6bd4\u65e0\u8bb0\u5fc6\u57fa\u7ebf\u4ec5\u964d\u4f4e7.9%\u901f\u5ea6\uff09\uff0c\u4e14\u517c\u5bb9\u4efb\u4f55\u652f\u6301KV\u7f13\u5b58\u7684\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u6a21\u578b", "conclusion": "\u52a8\u6001\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u80fd\u6709\u6548\u89e3\u51b3\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u6a21\u578b\u517c\u5bb9\u6027"}}
