<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出基于传感器物理的NeRF框架，从单曝光模糊LDR图像和事件数据合成锐利HDR新视角，通过两个映射场对齐传感器输出与物理世界辐射


<details>
  <summary>Details</summary>
Motivation: 现有方法使用事件数据解决低动态范围模糊图像的新视角合成问题，但忽略了相机输出与物理世界辐射之间的传感器物理不匹配，导致HDR和去模糊效果不理想

Method: 提出统一的传感器物理基础NeRF框架：1) 使用NeRF直接表示HDR域中的实际场景辐射；2) 引入像素级RGB映射场对齐渲染像素值与传感器记录的LDR像素值；3) 设计事件映射场桥接物理场景动态与事件传感器输出；4) 两个映射场与NeRF网络联合优化

Result: 在收集和公开数据集上的实验表明，该方法能够从单曝光模糊LDR图像和相应事件数据实现最先进的去模糊HDR新视角合成结果

Conclusion: 通过建模传感器物理不匹配并联合优化映射场，该方法能够有效利用事件数据的时空动态信息，学习锐利的HDR 3D表示，解决极端光照条件下的新视角合成问题

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [2] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream：基于事件感知的视频流理解框架，通过检测语义连贯的事件边界来减少冗余帧处理，实现长视频实时理解


<details>
  <summary>Details</summary>
Motivation: 现有视频流理解系统存在两个主要问题：1) 固定间隔解码导致重复输出；2) 缓存剪枝丢弃关键时间信息。多模态大语言模型在处理长视频流时面临冗余帧处理和快速遗忘过去上下文的问题

Method: 提出事件感知框架，将连续视频表示为离散的语义连贯事件序列。通过整合运动、语义和预测线索检测有意义的状态转换，仅在事件边界触发语言生成。每个事件嵌入被整合到持久记忆库中，支持长时程推理

Result: 在OVOBench-Realtime和长格式Ego4D评估中表现优异：相比VideoLLM-Online-8B基线提升10.4分；使用通用LLaMA-3-8B文本骨干即可接近Flash-VStream-7B性能；在2小时Ego4D流上保持约70% GPT-5胜率

Conclusion: Event-VStream通过事件感知方法有效解决了长视频流理解中的冗余处理和遗忘问题，在保持低延迟的同时实现了竞争性的性能表现

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice是一个混合序列预测系统，通过二元置信度门控有条件地激活学习到的行为结构，在推荐系统、科学时间序列和金融市场中验证有效。


<details>
  <summary>Details</summary>
Motivation: 为了解决在安全关键应用中管理认知不确定性的问题，需要一种能够智能地激活或拒绝行为结构的系统，以避免在分布偏移时产生错误激活。

Method: 系统将行为窗口聚类为行为原型，使用二元置信度门控：当置信度超过阈值时激活基于原型的评分，否则回退到基线预测。使用LSTM和Transformer作为骨干网络。

Result: 在MovieLens推荐系统中，Lattice比LSTM基线提升31.9%（HR@10），比SASRec和BERT4Rec分别提升109.4%和218.6%。在LIGO和金融数据中，系统能在分布偏移时正确拒绝原型激活。在Transformer骨干上，系统保持中性（0.0%改进）。

Conclusion: 置信度门控是一个有前景的架构原则，能够在模式适用时激活、不适用时拒绝、冗余时优雅地推迟，有效管理安全关键应用中的认知不确定性。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [4] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个PyTorch兼容的库，将随机数值线性代数（RandNLA）算法整合到高性能框架中，通过替换标准组件实现显著内存节省（BERT上达75%）。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受GPU内存和计算限制，虽然RandNLA提供了压缩模型的技术，但缺乏统一的生产级库阻碍了这些方法的广泛采用。

Method: 开发Panther库，包含自定义C++/CUDA后端（pawX），提供优化的草图线性层、2D卷积、多头注意力和随机矩阵分解等标准组件的替代实现。

Result: 通过替换标准PyTorch线性层为Panther层（仅需几行代码），在BERT上实现高达75%的内存节省，同时保持可比较的损失。

Conclusion: Panther成功将RandNLA技术整合到统一的高性能库中，证明了其有效性和易用性，有助于解决深度学习训练中的内存约束问题。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [5] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++通过引入两种漂移检测机制（基于熵和KL散度）和自适应重置策略，解决了持续测试时适应（CTTA）在快速变化或极长时域分布漂移下的崩溃问题，在CCC基准上相比RDumb获得约3%的绝对精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法（如Tent、EATA等）在短期分布漂移下表现良好，但在测试分布快速变化或极长时域（如CCC基准的750万样本流）下容易崩溃。需要一种能检测有害累积适应并及时恢复的方法。

Method: 提出RDumb++，扩展RDumb方法，引入两种漂移检测机制：1）基于熵的漂移评分；2）KL散度漂移评分。结合自适应重置策略，当检测到累积适应变得有害时，模型能及时重置恢复，避免预测崩溃。

Result: 在CCC-medium基准的9次运行（三种速度×三种随机种子，每次包含100万样本）中，RDumb++始终优于RDumb，获得约3%的绝对精度提升，并在整个流中保持稳定的适应。消融实验表明漂移感知重置对防止崩溃和实现可靠长时域CTTA至关重要。

Conclusion: 漂移检测和自适应重置机制对于长时域CTTA至关重要，RDumb++通过及时检测有害适应并重置恢复，有效防止预测崩溃，在持续变化的分布漂移下实现稳定可靠的适应。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [6] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 该论文提出了首个从部分观测数据学习神经算子的系统框架，通过掩码预测训练策略和物理感知潜在传播器解决部分观测的核心难题，并在专门基准上实现了18-69%的相对误差降低。


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中经常遇到不完整的观测数据（传感器限制、地理约束、测量成本等），而现有神经算子方法假设完全观测的空间输入，这严重限制了其在真实世界应用中的适用性。

Method: 提出了Latent Autoregressive Neural Operator (LARO)框架，包含两个核心组件：(1) 掩码预测训练策略：通过战略性地掩码观测区域创建人工监督；(2) 物理感知潜在传播器：在潜在空间中通过边界优先的自回归生成重建解。

Result: 在专门设计的POBench-PDE基准上，LARO在补丁式缺失率低于50%的情况下，在所有基准上实现了18-69%的相对L2误差降低，包括真实世界的气候预测任务，并能有效处理高达75%缺失率的实际场景。

Conclusion: 该研究首次系统性地解决了从部分观测学习神经算子的问题，通过创新的训练策略和潜在空间传播机制，在一定程度上弥合了理想化研究设置与真实世界科学计算复杂性之间的差距。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [7] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出一种后训练统计准则和结构退火方法，通过移除未支持的参数依赖，揭示稳定独立的子结构，实现无需修改模型功能的结构化并行推理。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型推理通常在密集参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续增长。这种限制并非源于模型容量不足，而是由于将后训练推理系统视为整体算子，忽略了学习过程中形成的内部结构。

Method: 研究发现大模型的梯度更新事件高度局部化和选择性，许多参数依赖在训练后与其初始化分布统计上无法区分。基于此观察，提出后训练统计准则和结构退火程序，移除未支持的依赖关系，揭示稳定的独立子结构。

Result: 建立了后训练、模型无关的推理系统结构视图，实现了结构化并行推理，无需修改模型功能或接口。

Conclusion: 通过揭示大模型中的内在可分解性，为可持续的大规模AI推理提供了新方法，解决了当前密集参数推理系统的可扩展性瓶颈。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [8] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 该论文提出使用可解释AI（XAI）分析工业信息物理系统中机器学习模型的预测行为，通过SHAP值分析时间序列分解组件对预测的影响，发现训练数据上下文信息不足，并通过增加数据窗口大小来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统（CPS）对安全和经济效益至关重要，需要高度可靠性。虽然机器学习（特别是深度学习）在工业CPS中应用日益广泛，但其模型复杂度导致操作不透明，需要严格评估以防止模型在未来未见数据上出现意外行为。

Method: 应用可解释AI（XAI）分析工业CPS中ML模型的预测性能。具体使用SHAP值分析时间序列数据分解的各个组件对模型预测的影响。通过XAI发现训练数据上下文信息不足的问题，并据此增加数据实例的窗口大小。

Result: 通过XAI分析发现模型训练中缺乏足够的上下文信息。基于这一发现，通过增加数据窗口大小，成功提升了模型的预测性能。

Conclusion: XAI不仅能揭示ML模型的推理过程，还能指导模型改进。在工业CPS中，通过XAI分析可以识别模型训练的不足，并采取针对性措施（如增加数据窗口）来提升模型性能，从而提高系统的可靠性。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出双过程智能体不确定性量化框架，将语言化不确定性转化为主动控制信号，解决AI智能体在长程推理中的"幻觉螺旋"问题


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两难：不确定性量化方法只能被动诊断风险而不解决问题，而自我反思机制则存在持续或漫无目的的修正问题。需要解决早期认知错误不可逆传播的"幻觉螺旋"问题

Method: 提出统一的Dual-Process Agentic UQ框架，包含两个互补机制：系统1（不确定性感知记忆UAM）隐式传播语言化置信度和语义解释；系统2（不确定性感知反思UAR）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解决

Result: 在闭环基准测试和开放式深度研究任务上的大量实验表明，这种无需训练的方法实现了卓越的性能和轨迹级校准

Conclusion: AUQ框架代表了向可靠智能体迈出的重要一步，能够动态平衡高效执行和深度思考

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [10] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从零开始学习的离策略actor-critic方法，仅需单条专家轨迹，通过sigmoid有界熵项防止负熵驱动的OOD动作优化并减少Q函数振荡，在D4RL任务和真实机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 强化学习在真实世界部署面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、小数据需求的真实世界RL方法。

Method: 提出SigEnt-SAC方法，核心设计是sigmoid有界熵项，防止负熵驱动的分布外动作优化，减少Q函数振荡。该方法仅需单条专家轨迹从零开始学习。

Result: 在D4RL基准测试中，SigEnt-SAC显著减轻Q函数振荡，比现有方法更快达到100%成功率。在四个真实机器人任务中，仅需少量真实世界交互就能学习成功策略。

Conclusion: SigEnt-SAC为真实世界RL部署提供了一条低成本、实用的途径，仅需最小数据需求就能在真实机器人任务中学习成功策略。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [11] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉语言模型与外部知识检索的方法，以改进对气候虚假信息的检测能力，特别是针对社交媒体上传播的误导性图像和视频。


<details>
  <summary>Details</summary>
Motivation: 气候虚假信息在数字世界中日益严重，特别是社交媒体上传播的误导性图像和视频难以检测，可能延迟气候行动。现有的视觉语言模型仅依赖训练时的知识，无法处理近期事件或更新信息。

Method: 通过将视觉语言模型与外部知识检索相结合，系统可以获取最新信息，包括反向图像搜索结果、在线事实核查和可信专家内容，从而更准确地评估图像及其声明的真实性。

Result: 该方法提高了模型处理现实世界气候虚假信息的能力，能够更准确地将图像及其声明分类为准确、误导、虚假或无法验证，支持保护公众对科学的理解。

Conclusion: 结合外部知识检索的视觉语言模型方法能够克服传统模型的知识局限性，在快速变化的信息环境中更有效地检测气候虚假信息，有助于维护科学传播的完整性。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: 论文评估RAG架构在公共卫生政策问答中减少LLM幻觉的效果，发现高级RAG配置相比基础RAG和普通LLM能显著提高回答的忠实度。


<details>
  <summary>Details</summary>
Motivation: LLM在公共卫生政策领域有应用潜力，但其产生幻觉（看似合理但事实错误的断言）的问题在高风险环境中构成严重障碍，需要可靠方法来确保信息完整性。

Method: 比较普通LLM、基础RAG和高级RAG（使用交叉编码器重排序）三种架构，采用Mistral-7B-Instruct-v0.2模型和all-MiniLM-L6-v2嵌入模型处理CDC政策文件，评估两种文本分块策略（递归字符分割和基于令牌的语义分割）对系统准确性的影响。

Result: 基础RAG的忠实度得分（0.621）显著优于普通LLM（0.347），而高级RAG配置达到最高的忠实度平均值0.797，证明两阶段检索机制对领域特定政策问答至关重要。

Conclusion: 高级RAG架构能有效减少LLM在公共卫生政策问答中的幻觉，但文档分割的结构限制仍然是多步推理任务的主要瓶颈，需要进一步优化。

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [13] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 提出Trajectory Replay via Concept-Basis Reconstruction框架，通过概念指纹对齐和概念原子重构，将拒绝干预从捐赠模型转移到目标模型，证明拒绝行为源于跨模型的通用低维语义电路。


<details>
  <summary>Details</summary>
Motivation: 对齐LLM中的拒绝行为通常被视为模型特定的，但作者假设它源于跨模型共享的通用低维语义电路。为了验证这一假设，需要开发一种能够在不同架构和训练机制之间转移拒绝干预的方法。

Method: 提出Trajectory Replay via Concept-Basis Reconstruction框架：1) 通过概念指纹对齐层；2) 使用共享的"概念原子"重构拒绝方向；3) 将捐赠模型的消融轨迹映射到目标模型的语义空间；4) 引入weight-SVD稳定性保护，将干预投影到低方差权重子空间以避免能力损害。

Result: 在8个模型对（包括GPT-OSS-20B和GLM-4）上的评估表明，转移的"配方"能持续减弱拒绝行为，同时保持模型性能，为安全对齐的语义通用性提供了有力证据。

Conclusion: 拒绝行为源于跨模型的通用低维语义电路，而非模型特定特性。提出的框架成功实现了拒绝干预在不同架构和训练机制之间的转移，证明了安全对齐的语义通用性。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>
