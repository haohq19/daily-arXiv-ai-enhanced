{"id": "2510.21885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21885", "abs": "https://arxiv.org/abs/2510.21885", "authors": ["Anh Pham", "Mihir Thalanki", "Michael Sun", "Aditya Chaloo", "Ankita Gupta", "Tian Xia", "Aditya Mate", "Ehimwenma Nosakhare", "Soundararajan Srinivasan"], "title": "Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning", "comment": null, "summary": "Large language models often lose previously aligned safety behaviors when\nfine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior\nwork shows that adding random safety examples can mitigate this effect, but it\nremains unclear which examples are most effective. We propose a behavior-aware\nsampling framework that selects safety examples based on two complementary\nfactors: instruction-response behavior (e.g., refusal versus compliance) and\nsemantic diversity across harm categories. Systematic evaluation shows that\nthis approach substantially reduces harmful outputs while maintaining\nhelpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%\nadditional training data. These results highlight how targeted data selection\ncan improve the safety and efficiency of fine-tuning at scale.", "AI": {"tldr": "\u63d0\u51fa\u884c\u4e3a\u611f\u77e5\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6307\u4ee4-\u54cd\u5e94\u884c\u4e3a\u548c\u8bed\u4e49\u591a\u6837\u6027\u7684\u5b89\u5168\u793a\u4f8b\u9009\u62e9\uff0c\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4ec5\u97000.5%\u989d\u5916\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u8f93\u51fa\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u826f\u6027\u6570\u636e\u4e0a\u5fae\u8c03\u65f6\u7ecf\u5e38\u5931\u53bb\u4e4b\u524d\u5bf9\u9f50\u7684\u5b89\u5168\u884c\u4e3a\uff08\u707e\u96be\u6027\u9057\u5fd8\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u6dfb\u52a0\u968f\u673a\u5b89\u5168\u793a\u4f8b\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u660e\u786e\u54ea\u4e9b\u793a\u4f8b\u6700\u6709\u6548\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u611f\u77e5\u91c7\u6837\u6846\u67b6\uff0c\u57fa\u4e8e\u4e24\u4e2a\u4e92\u8865\u56e0\u7d20\u9009\u62e9\u5b89\u5168\u793a\u4f8b\uff1a\u6307\u4ee4-\u54cd\u5e94\u884c\u4e3a\uff08\u5982\u62d2\u7eddvs\u670d\u4ece\uff09\u548c\u8de8\u4f24\u5bb3\u7c7b\u522b\u7684\u8bed\u4e49\u591a\u6837\u6027\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\u540c\u65f6\u4fdd\u6301\u5e2e\u52a9\u6027\uff0c\u4ec5\u75280.5%\u989d\u5916\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8fbe41%\u7684\u6709\u5bb3\u6027\u964d\u4f4e\u3002", "conclusion": "\u76ee\u6807\u6570\u636e\u9009\u62e9\u80fd\u591f\u63d0\u9ad8\u5927\u89c4\u6a21\u5fae\u8c03\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u4e3a\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.21734", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21734", "abs": "https://arxiv.org/abs/2510.21734", "authors": ["Giovanni Battista Regazzo", "Wim-Alexander Beckers", "Xuan Thao Ha", "Mouloud Ourak", "Johan Vlekken", "Emmanuel Vander Poorten"], "title": "Force-Displacement Profiling for Robot-Assisted Deployment of a Left Atrial Appendage Occluder Using FBG-EM Distal Sensing", "comment": "Presented at the Conference on New Technologies for Computer and\n  Robot Assisted Surgery (CRAS2025)", "summary": "Atrial fibrillation (AF) increases the risk of thromboembolic events due to\nimpaired function of the left atrial appendage (LAA). Left atrial appendage\nclosure (LAAC) is a minimally invasive intervention designed to reduce stroke\nrisk by sealing the LAA with an expandable occluder device. Current deployment\nrelies on manual catheter control and imaging modalities like fluoroscopy and\ntransesophageal echocardiography, which carry limitations including radiation\nexposure and limited positioning precision. In this study, we leverage a\npreviously developed force-sensing delivery sheath integrating fiber Bragg\ngratings (FBGs) at the interface between the catheter and the occluder.\nCombined with electromagnetic (EM) tracking, this setup enables real-time\nmeasurement of interaction forces and catheter tip position during\nrobot-assisted LAAC deployment in an anatomical phantom. We present a novel\nforce-displacement profiling method that characterizes occluder deployment\ndynamics and identifies key procedural steps without relying on ionizing\nradiation. The force profiles reveal low-magnitude interaction forces,\nsuggesting minimal mechanical stress on the surrounding anatomy. This approach\nshows promise in providing clinicians with enhanced intraoperative feedback,\nimproving deployment outcome. Future work will focus on automating deployment\nsteps classification and validating the sensing strategy in dynamic, realistic\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u7ea4\u5e03\u62c9\u683c\u5149\u6805\u548c\u7535\u78c1\u8ffd\u8e2a\u7684\u529b-\u4f4d\u79fb\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u5de6\u5fc3\u8033\u5c01\u5835\u672f\uff0c\u65e0\u9700\u7535\u79bb\u8f90\u5c04\u5373\u53ef\u5b9e\u65f6\u76d1\u6d4b\u5bfc\u7ba1\u5c16\u7aef\u4f4d\u7f6e\u548c\u76f8\u4e92\u4f5c\u7528\u529b\uff0c\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u5de6\u5fc3\u8033\u5c01\u5835\u672f\u4f9d\u8d56\u624b\u52a8\u5bfc\u7ba1\u63a7\u5236\u548c\u8367\u5149\u900f\u89c6\u6210\u50cf\uff0c\u5b58\u5728\u8f90\u5c04\u66b4\u9732\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u7cbe\u786e\u7684\u672f\u4e2d\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u96c6\u6210\u5149\u7ea4\u5e03\u62c9\u683c\u5149\u6805\u7684\u529b\u4f20\u611f\u8f93\u9001\u9798\u7ba1\u7ed3\u5408\u7535\u78c1\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u5728\u89e3\u5256\u6a21\u578b\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u8f85\u52a9\u5de6\u5fc3\u8033\u5c01\u5835\u90e8\u7f72\uff0c\u901a\u8fc7\u529b-\u4f4d\u79fb\u5206\u6790\u65b9\u6cd5\u8868\u5f81\u5c01\u5835\u5668\u90e8\u7f72\u52a8\u529b\u5b66\u3002", "result": "\u529b\u5206\u5e03\u663e\u793a\u4f4e\u5e45\u5ea6\u7684\u76f8\u4e92\u4f5c\u7528\u529b\uff0c\u8868\u660e\u5bf9\u5468\u56f4\u89e3\u5256\u7ed3\u6784\u65bd\u52a0\u7684\u673a\u68b0\u5e94\u529b\u6700\u5c0f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5173\u952e\u624b\u672f\u6b65\u9aa4\u800c\u4e0d\u4f9d\u8d56\u7535\u79bb\u8f90\u5c04\u3002", "conclusion": "\u8fd9\u79cd\u529b-\u4f4d\u79fb\u5206\u6790\u65b9\u6cd5\u6709\u671b\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u589e\u5f3a\u7684\u672f\u4e2d\u53cd\u9988\uff0c\u6539\u5584\u90e8\u7f72\u7ed3\u679c\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u4e13\u6ce8\u4e8e\u81ea\u52a8\u5316\u90e8\u7f72\u6b65\u9aa4\u5206\u7c7b\u548c\u5728\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4f20\u611f\u7b56\u7565\u3002"}}
{"id": "2510.21757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21757", "abs": "https://arxiv.org/abs/2510.21757", "authors": ["Mihir Gupta", "Pratik Desai", "Ross Greer"], "title": "Agro-Consensus: Semantic Self-Consistency in Vision-Language Models for Crop Disease Management in Developing Countries", "comment": null, "summary": "Agricultural disease management in developing countries such as India, Kenya,\nand Nigeria faces significant challenges due to limited access to expert plant\npathologists, unreliable internet connectivity, and cost constraints that\nhinder the deployment of large-scale AI systems. This work introduces a\ncost-effective self-consistency framework to improve vision-language model\n(VLM) reliability for agricultural image captioning. The proposed method\nemploys semantic clustering, using a lightweight (80MB) pre-trained embedding\nmodel to group multiple candidate responses. It then selects the most coherent\ncaption -- containing a diagnosis, symptoms, analysis, treatment, and\nprevention recommendations -- through a cosine similarity-based consensus. A\npractical human-in-the-loop (HITL) component is incorporated, wherein user\nconfirmation of the crop type filters erroneous generations, ensuring\nhigher-quality input for the consensus mechanism. Applied to the publicly\navailable PlantVillage dataset using a fine-tuned 3B-parameter PaliGemma model,\nour framework demonstrates improvements over standard decoding methods.\nEvaluated on 800 crop disease images with up to 21 generations per image, our\nsingle-cluster consensus method achieves a peak accuracy of 83.1% with 10\ncandidate generations, compared to the 77.5% baseline accuracy of greedy\ndecoding. The framework's effectiveness is further demonstrated when\nconsidering multiple clusters; accuracy rises to 94.0% when a correct response\nis found within any of the top four candidate clusters, outperforming the 88.5%\nachieved by a top-4 selection from the baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5171\u8bc6\u673a\u5236\uff0c\u63d0\u9ad8\u519c\u4e1a\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u7684\u53ef\u9760\u6027\uff0c\u5728PlantVillage\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8683.1%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u53d1\u5c55\u4e2d\u56fd\u5bb6\u519c\u4e1a\u75c5\u5bb3\u7ba1\u7406\u9762\u4e34\u7684\u4e13\u5bb6\u8d44\u6e90\u6709\u9650\u3001\u7f51\u7edc\u8fde\u63a5\u4e0d\u53ef\u9760\u548c\u6210\u672c\u9650\u5236\u7b49\u95ee\u9898\uff0c\u63d0\u9ad8AI\u7cfb\u7edf\u5728\u519c\u4e1a\u9886\u57df\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9009\u62e9\u6700\u8fde\u8d2f\u7684\u6807\u9898\uff0c\u5e76\u52a0\u5165\u4eba\u673a\u4ea4\u4e92\u7ec4\u4ef6\u8ba9\u7528\u6237\u786e\u8ba4\u4f5c\u7269\u7c7b\u578b\u4ee5\u8fc7\u6ee4\u9519\u8bef\u751f\u6210\u3002", "result": "\u5728800\u5f20\u4f5c\u7269\u75c5\u5bb3\u56fe\u50cf\u4e0a\u6d4b\u8bd5\uff0c\u5355\u805a\u7c7b\u5171\u8bc6\u65b9\u6cd5\u572810\u4e2a\u5019\u9009\u751f\u6210\u65f6\u8fbe\u523083.1%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u8d2a\u5a6a\u89e3\u7801\u768477.5%\uff1b\u8003\u8651\u591a\u4e2a\u805a\u7c7b\u65f6\u51c6\u786e\u7387\u53ef\u63d0\u5347\u81f394.0%\u3002", "conclusion": "\u8be5\u81ea\u4e00\u81f4\u6027\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u519c\u4e1a\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u519c\u4e1a\u75c5\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.21735", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21735", "abs": "https://arxiv.org/abs/2510.21735", "authors": ["Yuhui Liu", "Shian Wang", "Ansel Panicker", "Kate Embry", "Ayana Asanova", "Tianyi Li"], "title": "A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data", "comment": null, "summary": "Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit\ndistinct vehicle dynamics. EVs provide rapid acceleration, with electric motors\nproducing peak power across a wider speed range, and achieve swift deceleration\nthrough regenerative braking. While existing microscopic models effectively\ncapture the driving behavior of ICE vehicles, a modeling framework that\naccurately describes the unique car-following dynamics of EVs is lacking.\nDeveloping such a model is essential given the increasing presence of EVs in\ntraffic, yet creating an easy-to-use and accurate analytical model remains\nchallenging.\n  To address these gaps, this study develops and validates a Phase-Aware AI\n(PAAI) car-following model specifically for EVs. The proposed model enhances\ntraditional physics-based frameworks with an AI component that recognizes and\nadapts to different driving phases, such as rapid acceleration and regenerative\nbraking. Using real-world trajectory data from vehicles equipped with adaptive\ncruise control (ACC), we conduct comprehensive simulations to validate the\nmodel's performance. The numerical results demonstrate that the PAAI model\nsignificantly improves prediction accuracy over traditional car-following\nmodels, providing an effective tool for accurately representing EV behavior in\ntraffic simulations.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u7535\u52a8\u6c7d\u8f66\u7684\u76f8\u4f4d\u611f\u77e5AI\u8ddf\u8f66\u6a21\u578b\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u5fae\u89c2\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u5185\u71c3\u673a\u8f66\u8f86\u9a7e\u9a76\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u51c6\u786e\u63cf\u8ff0\u7535\u52a8\u6c7d\u8f66\u72ec\u7279\u8ddf\u8f66\u52a8\u529b\u5b66\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u800c\u7535\u52a8\u6c7d\u8f66\u5728\u4ea4\u901a\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\u4f7f\u5f97\u5f00\u53d1\u6b64\u7c7b\u6a21\u578b\u53d8\u5f97\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u4e86\u76f8\u4f4d\u611f\u77e5AI\u8ddf\u8f66\u6a21\u578b\uff0c\u5728\u4f20\u7edf\u7269\u7406\u6846\u67b6\u57fa\u7840\u4e0a\u52a0\u5165AI\u7ec4\u4ef6\uff0c\u8bc6\u522b\u5e76\u9002\u5e94\u4e0d\u540c\u9a7e\u9a76\u9636\u6bb5\uff08\u5982\u5feb\u901f\u52a0\u901f\u548c\u518d\u751f\u5236\u52a8\uff09\uff0c\u4f7f\u7528\u914d\u5907\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7684\u8f66\u8f86\u771f\u5b9e\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u7efc\u5408\u4eff\u771f\u9a8c\u8bc1", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cPAAI\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u8ddf\u8f66\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5728\u4ea4\u901a\u4eff\u771f\u4e2d\u51c6\u786e\u8868\u793a\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177"}}
{"id": "2510.21779", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21779", "abs": "https://arxiv.org/abs/2510.21779", "authors": ["Supriya Nagesh", "Karina Covarrubias", "Robert El-Kareh", "Shiva Prasad Kasiviswanathan", "Nina Mishra"], "title": "What Causes Postoperative Aspiration?", "comment": null, "summary": "Background: Aspiration, the inhalation of foreign material into the lungs,\nsignificantly impacts surgical patient morbidity and mortality. This study\ndevelops a machine learning (ML) model to predict postoperative aspiration,\nenabling timely preventative interventions.\n  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we\nidentified 826 surgical patients (mean age: 62, 55.7\\% male) who experienced\naspiration within seven days post-surgery, along with a matched non-aspiration\ncohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were\ntrained using pre-surgical hospitalization data to predict postoperative\naspiration. To investigate causation, we estimated Average Treatment Effects\n(ATE) using Augmented Inverse Probability Weighting.\n  Results: Our ML model achieved an AUROC of 0.86 and 77.3\\% sensitivity on a\nheld-out test set. Maximum daily opioid dose, length of stay, and patient age\nemerged as the most important predictors. ATE analysis identified significant\ncausative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/-\n0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men\nwere 1.5 times more likely to aspirate and received 27\\% higher maximum daily\nopioid dosages compared to women.\n  Conclusion: ML models can effectively predict postoperative aspiration risk,\nenabling targeted preventative measures. Maximum daily opioid dosage and\noperative site significantly influence aspiration risk. The gender disparity in\nboth opioid administration and aspiration rates warrants further investigation.\nThese findings have important implications for improving postoperative care\nprotocols and aspiration prevention strategies.", "AI": {"tldr": "\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u672f\u540e\u8bef\u5438\u98ce\u9669\uff0c\u8bc6\u522b\u963f\u7247\u7c7b\u836f\u7269\u5242\u91cf\u548c\u624b\u672f\u90e8\u4f4d\u4e3a\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\uff0c\u53d1\u73b0\u7537\u6027\u60a3\u8005\u8bef\u5438\u98ce\u9669\u66f4\u9ad8\u4e14\u963f\u7247\u7c7b\u836f\u7269\u5242\u91cf\u66f4\u5927\u3002", "motivation": "\u8bef\u5438\u4e25\u91cd\u5f71\u54cd\u624b\u672f\u60a3\u8005\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\uff0c\u9700\u8981\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u4ee5\u5b9e\u73b0\u53ca\u65f6\u9884\u9632\u5e72\u9884\u3002", "method": "\u4eceMIMIC-IV\u6570\u636e\u5e93\u4e2d\u8bc6\u522b826\u540d\u672f\u540e\u8bef\u5438\u60a3\u8005\u548c\u5339\u914d\u5bf9\u7167\u7ec4\uff0c\u4f7f\u7528XGBoost\u3001\u591a\u5c42\u611f\u77e5\u673a\u548c\u968f\u673a\u68ee\u6797\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u91c7\u7528\u589e\u5f3a\u9006\u6982\u7387\u52a0\u6743\u8fdb\u884c\u56e0\u679c\u5206\u6790\u3002", "result": "\u6a21\u578bAUROC\u8fbe\u52300.86\uff0c\u7075\u654f\u5ea677.3%\uff1b\u6700\u5927\u65e5\u963f\u7247\u5242\u91cf\u3001\u4f4f\u9662\u65f6\u95f4\u548c\u60a3\u8005\u5e74\u9f84\u662f\u6700\u91cd\u8981\u9884\u6d4b\u56e0\u5b50\uff1b\u7537\u6027\u8bef\u5438\u98ce\u9669\u662f\u5973\u6027\u76841.5\u500d\uff0c\u4e14\u963f\u7247\u5242\u91cf\u9ad827%\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u672f\u540e\u8bef\u5438\u98ce\u9669\uff0c\u963f\u7247\u7c7b\u836f\u7269\u5242\u91cf\u548c\u624b\u672f\u90e8\u4f4d\u663e\u8457\u5f71\u54cd\u98ce\u9669\uff0c\u6027\u522b\u5dee\u5f02\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u6539\u8fdb\u672f\u540e\u62a4\u7406\u65b9\u6848\u3002"}}
{"id": "2510.21786", "categories": ["cs.CV", "cs.AI", "cs.MM", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.21786", "abs": "https://arxiv.org/abs/2510.21786", "authors": ["Qile Su", "Shoutai Zhu", "Shuai Zhang", "Baoyu Liang", "Chao Tong"], "title": "EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction", "comment": "15 pages, 7 figures, 6 tables", "summary": "Script event induction, which aims to predict the subsequent event based on\nthe context, is a challenging task in NLP, achieving remarkable success in\npractical applications. However, human events are mostly recorded and presented\nin the form of videos rather than scripts, yet there is a lack of related\nresearch in the realm of vision. To address this problem, we introduce AVEP\n(Action-centric Video Event Prediction), a task that distinguishes itself from\nexisting video prediction tasks through its incorporation of more complex logic\nand richer semantic information. We present a large structured dataset, which\nconsists of about $35K$ annotated videos and more than $178K$ video clips of\nevent, built upon existing video event datasets to support this task. The\ndataset offers more fine-grained annotations, where the atomic unit is\nrepresented as a multimodal event argument node, providing better structured\nrepresentations of video events. Due to the complexity of event structures,\ntraditional visual models that take patches or frames as input are not\nwell-suited for AVEP. We propose EventFormer, a node-graph hierarchical\nattention based video event prediction model, which can capture both the\nrelationships between events and their arguments and the coreferencial\nrelationships between arguments. We conducted experiments using several SOTA\nvideo prediction models as well as LVLMs on AVEP, demonstrating both the\ncomplexity of the task and the value of the dataset. Our approach outperforms\nall these video prediction models. We will release the dataset and code for\nreplicating the experiments and annotations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AVEP\u4efb\u52a1\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u89c6\u9891\u9884\u6d4b\uff0c\u5305\u542b\u66f4\u590d\u6742\u7684\u903b\u8f91\u548c\u8bed\u4e49\u4fe1\u606f\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b35K\u6807\u6ce8\u89c6\u9891\u548c178K\u89c6\u9891\u7247\u6bb5\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86EventFormer\u6a21\u578b\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u4eba\u7c7b\u4e8b\u4ef6\u591a\u4ee5\u89c6\u9891\u5f62\u5f0f\u8bb0\u5f55\u800c\u975e\u6587\u672c\u811a\u672c\uff0c\u4f46\u5728\u89c6\u89c9\u9886\u57df\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\u3002\u73b0\u6709\u89c6\u9891\u9884\u6d4b\u4efb\u52a1\u7f3a\u4e4f\u590d\u6742\u903b\u8f91\u548c\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u89c6\u9891\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u63d0\u51faEventFormer\u6a21\u578b\uff0c\u91c7\u7528\u8282\u70b9-\u56fe\u5c42\u6b21\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u6355\u6349\u4e8b\u4ef6\u4e0e\u53c2\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\u4ee5\u53ca\u53c2\u6570\u95f4\u7684\u5171\u6307\u5173\u7cfb\u3002", "result": "\u5728AVEP\u4efb\u52a1\u4e0a\uff0cEventFormer\u6a21\u578b\u8d85\u8d8a\u4e86\u591a\u4e2aSOTA\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u548cLVLMs\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u6570\u636e\u96c6\u7684\u4ef7\u503c\u3002", "conclusion": "AVEP\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u9891\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548cEventFormer\u6a21\u578b\u4e3a\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u672a\u6765\u5c06\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2510.22052", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22052", "abs": "https://arxiv.org/abs/2510.22052", "authors": ["Abhijit Chatterjee", "Niraj K. Jha", "Jonathan D. Cohen", "Thomas L. Griffiths", "Hongjing Lu", "Diana Marculescu", "Ashiqur Rasul", "Keshab K. Parhi"], "title": "Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms", "comment": null, "summary": "The field of artificial intelligence (AI) has taken a tight hold on broad\naspects of society, industry, business, and governance in ways that dictate the\nprosperity and might of the world's economies. The AI market size is projected\nto grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI\nis dominated by large language models that exhibit linguistic and visual\nintelligence. However, training these models requires a massive amount of data\nscraped from the web as well as large amounts of energy (50--60 GWh to train\nGPT-4). Despite these costs, these models often hallucinate, a characteristic\nthat prevents them from being deployed in critical application domains. In\ncontrast, the human brain consumes only 20~W of power. What is needed is the\nnext level of AI evolution in which lightweight domain-specific multimodal\nmodels with higher levels of intelligence can reason, plan, and make decisions\nin dynamic environments with real-time data and prior knowledge, while learning\ncontinuously and evolving in ways that enhance future decision-making\ncapability. This will define the next wave of AI, progressing from today's\nlarge models, trained with vast amounts of data, to nimble energy-efficient\ndomain-specific agents that can reason and think in a world full of\nuncertainty. To support such agents, hardware will need to be reimagined to\nallow energy efficiencies greater than 1000x over the state of the art. Such a\nvision of future AI systems is developed in this work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e0b\u4e00\u4ee3AI\u7684\u53d1\u5c55\u613f\u666f\uff1a\u4ece\u5f53\u524d\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u80fd\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u5411\u8f7b\u91cf\u7ea7\u3001\u9886\u57df\u7279\u5b9a\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u63a8\u7406\u3001\u89c4\u5212\u548c\u51b3\u7b56\uff0c\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc7\u73b0\u6709\u6280\u672f1000\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5b58\u5728\u80fd\u8017\u5de8\u5927\uff08GPT-4\u8bad\u7ec3\u970050-60 GWh\uff09\u3001\u4ea7\u751f\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u5728\u5173\u952e\u5e94\u7528\u9886\u57df\u90e8\u7f72\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u8111\u4ec5\u6d88\u801720W\u529f\u7387\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u8282\u80fd\u3001\u66f4\u667a\u80fd\u7684AI\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u5f00\u53d1\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u591a\u6a21\u6001\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\uff1a1\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u63a8\u7406\u3001\u89c4\u5212\u548c\u51b3\u7b56\uff1b2\uff09\u5229\u7528\u5b9e\u65f6\u6570\u636e\u548c\u5148\u9a8c\u77e5\u8bc6\uff1b3\uff09\u6301\u7eed\u5b66\u4e60\u5e76\u589e\u5f3a\u672a\u6765\u51b3\u7b56\u80fd\u529b\uff1b4\uff09\u5b9e\u73b0\u8d85\u8fc7\u73b0\u6709\u6280\u672f1000\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "result": "\u63d0\u51fa\u4e86\u672a\u6765AI\u7cfb\u7edf\u7684\u613f\u666f\u6846\u67b6\uff0c\u5f3a\u8c03\u4ece\u6570\u636e\u5bc6\u96c6\u578b\u5927\u578b\u6a21\u578b\u5411\u80fd\u6548\u578b\u667a\u80fd\u4f53\u7684\u8f6c\u53d8\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u4e0b\u4e00\u4ee3AI\u5e94\u8be5\u671d\u7740\u8f7b\u91cf\u7ea7\u3001\u9886\u57df\u7279\u5b9a\u3001\u591a\u6a21\u6001\u3001\u9ad8\u80fd\u6548\u7684\u65b9\u5411\u53d1\u5c55\uff0c\u8fd9\u9700\u8981\u91cd\u65b0\u6784\u60f3\u786c\u4ef6\u67b6\u6784\u4ee5\u5b9e\u73b0\u663e\u8457\u7684\u80fd\u6548\u63d0\u5347\uff0c\u4ece\u800c\u5728\u5145\u6ee1\u4e0d\u786e\u5b9a\u6027\u7684\u4e16\u754c\u4e2d\u5b9e\u73b0\u771f\u6b63\u7684\u63a8\u7406\u548c\u601d\u8003\u80fd\u529b\u3002"}}
{"id": "2510.21836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21836", "abs": "https://arxiv.org/abs/2510.21836", "authors": ["Jaya Krishna Mandivarapu"], "title": "COLA: Continual Learning via Autoencoder Retrieval of Adapters", "comment": null, "summary": "Learning a set of tasks over time, also known as continual learning (CL), is\none of the most challenging problems in artificial intelligence due to\ncatastrophic forgetting. Large language models (LLMs) are often impractical to\nfrequent re-training and continual learning , due to high cost of computational\nresources for training. Moreover, LLM are not suitable for continual learning\nas updating these models over time for acquiring new knowledge leads to\noverwrites existing knowledge leading to common phenomenon know as\n\\textit{catastrophic forgetting}. In this paper, we aim to address these\nconcerns using a novel framework , COLA that employs an autoencoder to learn\ncapture low-dimensional embeddings of the weights associated with various\ntasks. Our approach facilitates the transfer of knowledge to new tasks while\npreventing catastrophic forgetting, all without using data replay or a\nsubstantial set of task-specific parameters. Our approach, COLA, makes the LLM\nefficiently learn new tasks with minimal training, insignificant performance\ndegradation on previous tasks, and eliminates the need for retaining earlier\ntraining data. Empirical evaluation on different datasets ranging from task\noriented dialouge system to intent classsfication datasets showcases that our\nmethod not only overcomes catastrophic forgetting but also achieves significant\nreduction in parameter usage and memory size, across multiple tasks and\noutperforming the existing state of the art methods across multiple datasets.", "AI": {"tldr": "\u63d0\u51faCOLA\u6846\u67b6\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4efb\u52a1\u6743\u91cd\u7684\u4f4e\u7ef4\u5d4c\u5165\uff0c\u65e0\u9700\u6570\u636e\u56de\u653e\u6216\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u53c2\u6570", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e14\u9891\u7e41\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u65b0\u4efb\u52a1\u540c\u65f6\u4fdd\u7559\u5df2\u6709\u77e5\u8bc6", "method": "\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4efb\u52a1\u6743\u91cd\u7684\u4f4e\u7ef4\u5d4c\u5165\uff0c\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\u5230\u65b0\u4efb\u52a1\uff0c\u540c\u65f6\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e0d\u4f7f\u7528\u6570\u636e\u56de\u653e\u6216\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u53c2\u6570", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0d\u4ec5\u514b\u670d\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd8\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u4f7f\u7528\u548c\u5185\u5b58\u5360\u7528\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "COLA\u6846\u67b6\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u5bf9\u5148\u524d\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\uff0c\u4e14\u65e0\u9700\u4fdd\u7559\u65e9\u671f\u8bad\u7ec3\u6570\u636e"}}
{"id": "2510.21847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21847", "abs": "https://arxiv.org/abs/2510.21847", "authors": ["Kaiyi Xu", "Junchao Gong", "Wenlong Zhang", "Ben Fei", "Lei Bai", "Wanli Ouyang"], "title": "SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization", "comment": null, "summary": "Precipitation nowcasting based on radar echoes plays a crucial role in\nmonitoring extreme weather and supporting disaster prevention. Although deep\nlearning approaches have achieved significant progress, they still face notable\nlimitations. For example, deterministic models tend to produce over-smoothed\npredictions, which struggle to capture extreme events and fine-scale\nprecipitation patterns. Probabilistic generative models, due to their inherent\nrandomness, often show fluctuating performance across different metrics and\nrarely achieve consistently optimal results. Furthermore, precipitation\nnowcasting is typically evaluated using multiple metrics, some of which are\ninherently conflicting. For instance, there is often a trade-off between the\nCritical Success Index (CSI) and the False Alarm Ratio (FAR), making it\nchallenging for existing models to deliver forecasts that perform well on both\nmetrics simultaneously. To address these challenges, we introduce preference\noptimization into precipitation nowcasting for the first time, motivated by the\nsuccess of reinforcement learning from human feedback in large language models.\nSpecifically, we propose SynCast, a method that employs the two-stage\npost-training framework of Diffusion Sequential Preference Optimization\n(Diffusion-SPO), to progressively align conflicting metrics and consistently\nachieve superior performance. In the first stage, the framework focuses on\nreducing FAR, training the model to effectively suppress false alarms. Building\non this foundation, the second stage further optimizes CSI with constraints\nthat preserve FAR alignment, thereby achieving synergistic improvements across\nthese conflicting metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSynCast\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u504f\u597d\u4f18\u5316\u5f15\u5165\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u786e\u5b9a\u6027\u6a21\u578b\u9884\u6d4b\u8fc7\u4e8e\u5e73\u6ed1\u548c\u6982\u7387\u6a21\u578b\u6027\u80fd\u6ce2\u52a8\u7684\u95ee\u9898\uff0c\u534f\u540c\u4f18\u5316\u51b2\u7a81\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u9762\u4e34\u786e\u5b9a\u6027\u6a21\u578b\u9884\u6d4b\u8fc7\u4e8e\u5e73\u6ed1\u96be\u4ee5\u6355\u6349\u6781\u7aef\u4e8b\u4ef6\uff0c\u6982\u7387\u6a21\u578b\u6027\u80fd\u6ce2\u52a8\u4e14\u96be\u4ee5\u5728\u51b2\u7a81\u6307\u6807\u4e0a\u540c\u65f6\u8868\u73b0\u4f18\u5f02\u7684\u95ee\u9898\uff0c\u7279\u522b\u662fCSI\u548cFAR\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "method": "\u63d0\u51faSynCast\u65b9\u6cd5\uff0c\u91c7\u7528Diffusion-SPO\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e13\u6ce8\u4e8e\u964d\u4f4eFAR\uff0c\u8bad\u7ec3\u6a21\u578b\u6291\u5236\u8bef\u62a5\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u4fdd\u6301FAR\u5bf9\u9f50\u7684\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316CSI\uff0c\u5b9e\u73b0\u51b2\u7a81\u6307\u6807\u7684\u534f\u540c\u6539\u8fdb\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u51b2\u7a81\u6307\u6807\uff0c\u5728\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u6301\u7eed\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u51b2\u7a81\u6307\u6807\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6781\u7aef\u5929\u6c14\u76d1\u6d4b\u548c\u707e\u5bb3\u9884\u9632\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.22251", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22251", "abs": "https://arxiv.org/abs/2510.22251", "authors": ["Imran Khan"], "title": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion", "comment": "17 pages, 1 figure, 6 tables. Code and experimental data available at\n  https://github.com/strongSoda/prompt-sculpting", "summary": "Prompt engineering, particularly Chain-of-Thought (CoT) prompting,\nsignificantly enhances LLM reasoning capabilities. We introduce \"Sculpting,\" a\nconstrained, rule-based prompting method designed to improve upon standard CoT\nby reducing errors from semantic ambiguity and flawed common sense.\n  We evaluate three prompting strategies (Zero Shot, standard CoT, and\nSculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)\nusing the GSM8K mathematical reasoning benchmark (1,317 problems).\n  Our findings reveal a \"Prompting Inversion\": Sculpting provides advantages on\ngpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%\nvs. 96.36% for CoT on full benchmark). We trace this to a\n\"Guardrail-to-Handcuff\" transition where constraints preventing common-sense\nerrors in mid-tier models induce hyper-literalism in advanced models. Our\ndetailed error analysis demonstrates that optimal prompting strategies must\nco-evolve with model capabilities, suggesting simpler prompts for more capable\nmodels.", "AI": {"tldr": "Sculpting\u662f\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u7ea6\u675f\u63d0\u793a\u65b9\u6cd5\uff0c\u76f8\u6bd4\u6807\u51c6CoT\u80fd\u51cf\u5c11\u8bed\u4e49\u6a21\u7cca\u548c\u5e38\u8bc6\u9519\u8bef\uff0c\u4f46\u5728\u66f4\u5148\u8fdb\u7684GPT-5\u6a21\u578b\u4e2d\u53cd\u800c\u6709\u5bb3\uff0c\u51fa\u73b0'\u63d0\u793a\u53cd\u8f6c'\u73b0\u8c61\u3002", "motivation": "\u6807\u51c6CoT\u63d0\u793a\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u548c\u5e38\u8bc6\u9519\u8bef\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u63d0\u793a\u65b9\u6cd5\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faSculpting\u65b9\u6cd5\uff0c\u91c7\u7528\u7ea6\u675f\u6027\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u4e0e\u96f6\u6837\u672c\u3001\u6807\u51c6CoT\u5728\u4e09\u4e2aOpenAI\u6a21\u578b\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "Sculpting\u5728GPT-4o\u4e0a\u8868\u73b0\u6700\u4f73(97% vs 93%)\uff0c\u4f46\u5728GPT-5\u4e0a\u53cd\u800c\u6709\u5bb3(94% vs 96.36%)\uff0c\u51fa\u73b0'\u63d0\u793a\u53cd\u8f6c'\u73b0\u8c61\u3002", "conclusion": "\u6700\u4f18\u63d0\u793a\u7b56\u7565\u9700\u4e0e\u6a21\u578b\u80fd\u529b\u5171\u540c\u8fdb\u5316\uff0c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u9700\u8981\u66f4\u7b80\u5355\u7684\u63d0\u793a\uff0c\u907f\u514d\u7ea6\u675f\u53d8\u6210'\u624b\u94d0'\u3002"}}
{"id": "2510.21858", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21858", "abs": "https://arxiv.org/abs/2510.21858", "authors": ["Yangze Zhou", "Ruiyang Yao", "Dalin Qin", "Yixiong Jia", "Yi Wang"], "title": "Privacy-preserving Decision-focused Learning for Multi-energy Systems", "comment": "10 pages, 7 figures", "summary": "Decision-making for multi-energy system (MES) dispatch depends on accurate\nload forecasting. Traditionally, load forecasting and decision-making for MES\nare implemented separately. Forecasting models are typically trained to\nminimize forecasting errors, overlooking their impact on downstream\ndecision-making. To address this, decision-focused learning (DFL) has been\nstudied to minimize decision-making costs instead. However, practical adoption\nof DFL in MES faces significant challenges: the process requires sharing\nsensitive load data and model parameters across multiple sectors, raising\nserious privacy issues. To this end, we propose a privacy-preserving DFL\nframework tailored for MES. Our approach introduces information masking to\nsafeguard private data while enabling recovery of decision variables and\ngradients required for model training. To further enhance security for DFL, we\ndesign a safety protocol combining matrix decomposition and homomorphic\nencryption, effectively preventing collusion and unauthorized data access.\nAdditionally, we developed a privacy-preserving load pattern recognition\nalgorithm, enabling the training of specialized DFL models for heterogeneous\nload patterns. Theoretical analysis and comprehensive case studies, including\nreal-world MES data, demonstrate that our framework not only protects privacy\nbut also consistently achieves lower average daily dispatch costs compared to\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u80fd\u6e90\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u51b3\u7b56\u805a\u7126\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u63a9\u853d\u548c\u5b89\u5168\u534f\u8bae\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u8c03\u5ea6\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u591a\u80fd\u6e90\u7cfb\u7edf\u8d1f\u8377\u9884\u6d4b\u4e0e\u51b3\u7b56\u5206\u79bb\uff0c\u9884\u6d4b\u6a21\u578b\u4ec5\u5173\u6ce8\u9884\u6d4b\u7cbe\u5ea6\u800c\u5ffd\u7565\u5bf9\u4e0b\u6e38\u51b3\u7b56\u7684\u5f71\u54cd\u3002\u51b3\u7b56\u805a\u7126\u5b66\u4e60\u867d\u80fd\u4f18\u5316\u51b3\u7b56\u6210\u672c\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u654f\u611f\u6570\u636e\u5171\u4eab\u5bfc\u81f4\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u63a9\u853d\u6280\u672f\u4fdd\u62a4\u79c1\u6709\u6570\u636e\uff0c\u540c\u65f6\u6062\u590d\u51b3\u7b56\u53d8\u91cf\u548c\u68af\u5ea6\uff1b\u7ed3\u5408\u77e9\u9635\u5206\u89e3\u548c\u540c\u6001\u52a0\u5bc6\u8bbe\u8ba1\u5b89\u5168\u534f\u8bae\u9632\u6b62\u5408\u8c0b\u548c\u672a\u6388\u6743\u8bbf\u95ee\uff1b\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7684\u8d1f\u8377\u6a21\u5f0f\u8bc6\u522b\u7b97\u6cd5\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u771f\u5b9e\u591a\u80fd\u6e90\u7cfb\u7edf\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff0c\u800c\u4e14\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6301\u7eed\u5b9e\u73b0\u66f4\u4f4e\u7684\u5e73\u5747\u65e5\u8c03\u5ea6\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9690\u79c1\u4fdd\u62a4\u51b3\u7b56\u805a\u7126\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u80fd\u6e90\u7cfb\u7edf\u8c03\u5ea6\u4e2d\u7684\u9690\u79c1\u5b89\u5168\u4e0e\u51b3\u7b56\u4f18\u5316\u53cc\u91cd\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.21934", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21934", "abs": "https://arxiv.org/abs/2510.21934", "authors": ["Fardin Gankhanloo", "Emmett Springer", "Erik H. Hoyer", "Daniel L. Young", "Kimia Ghobadi"], "title": "Joint Score-Threshold Optimization for Interpretable Risk Assessment Under Partial Supervision", "comment": null, "summary": "Risk assessment tools in healthcare commonly employ point-based scoring\nsystems that map patients to ordinal risk categories via thresholds. While\nelectronic health record (EHR) data presents opportunities for data-driven\noptimization of these tools, two fundamental challenges impede standard\nsupervised learning: (1) partial supervision arising from intervention-censored\noutcomes, where only extreme categories can be reliably labeled, and (2)\nasymmetric misclassification costs that increase with ordinal distance. We\npropose a mixed-integer programming (MIP) framework that jointly optimizes\nscoring weights and category thresholds under these constraints. Our approach\nhandles partial supervision through per-instance feasible label sets,\nincorporates asymmetric distance-aware objectives, and prevents middle-category\ncollapse via minimum threshold gaps. We further develop a CSO relaxation using\nsoftplus losses that preserves the ordinal structure while enabling efficient\noptimization. The framework supports governance constraints including sign\nrestrictions, sparsity, and minimal modifications to incumbent tools, ensuring\npractical deployability in clinical workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u533b\u7597\u98ce\u9669\u8bc4\u5206\u5de5\u5177\uff0c\u89e3\u51b3\u90e8\u5206\u76d1\u7763\u548c\u4e0d\u5bf9\u79f0\u8bef\u5206\u7c7b\u6210\u672c\u95ee\u9898", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e3a\u4f18\u5316\u98ce\u9669\u8bc4\u5206\u5de5\u5177\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u9762\u4e34\u90e8\u5206\u76d1\u7763\uff08\u4ec5\u6781\u7aef\u7c7b\u522b\u53ef\u53ef\u9760\u6807\u8bb0\uff09\u548c\u4e0d\u5bf9\u79f0\u8bef\u5206\u7c7b\u6210\u672c\uff08\u968f\u5e8f\u6570\u8ddd\u79bb\u589e\u52a0\uff09\u7684\u6311\u6218", "method": "\u6df7\u5408\u6574\u6570\u89c4\u5212\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u8bc4\u5206\u6743\u91cd\u548c\u7c7b\u522b\u9608\u503c\uff0c\u5904\u7406\u90e8\u5206\u76d1\u7763\u901a\u8fc7\u5b9e\u4f8b\u53ef\u884c\u6807\u7b7e\u96c6\uff0c\u7ed3\u5408\u4e0d\u5bf9\u79f0\u8ddd\u79bb\u611f\u77e5\u76ee\u6807\uff0c\u901a\u8fc7\u6700\u5c0f\u9608\u503c\u95f4\u9699\u9632\u6b62\u4e2d\u95f4\u7c7b\u522b\u5d29\u6e83", "result": "\u5f00\u53d1\u4e86\u4f7f\u7528softplus\u635f\u5931\u7684CSO\u677e\u5f1b\u65b9\u6cd5\uff0c\u4fdd\u6301\u5e8f\u6570\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u6cbb\u7406\u7ea6\u675f\uff0c\u5305\u62ec\u7b26\u53f7\u9650\u5236\u3001\u7a00\u758f\u6027\u548c\u5bf9\u73b0\u6709\u5de5\u5177\u7684\u6700\u5c0f\u4fee\u6539\uff0c\u786e\u4fdd\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5b9e\u9645\u53ef\u90e8\u7f72\u6027"}}
{"id": "2510.22524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22524", "abs": "https://arxiv.org/abs/2510.22524", "authors": ["Shenbagaraj Kannapiran", "Elena Oikonomou", "Albert Chu", "Spring Berman", "Theodore P. Pavlic"], "title": "Ant-inspired Walling Strategies for Scalable Swarm Separation: Reinforcement Learning Approaches Based on Finite State Machines", "comment": null, "summary": "In natural systems, emergent structures often arise to balance competing\ndemands. Army ants, for example, form temporary \"walls\" that prevent\ninterference between foraging trails. Inspired by this behavior, we developed\ntwo decentralized controllers for heterogeneous robotic swarms to maintain\nspatial separation while executing concurrent tasks. The first is a\nfinite-state machine (FSM)-based controller that uses encounter-triggered\ntransitions to create rigid, stable walls. The second integrates FSM states\nwith a Deep Q-Network (DQN), dynamically optimizing separation through emergent\n\"demilitarized zones.\" In simulation, both controllers reduce mixing between\nsubgroups, with the DQN-enhanced controller improving adaptability and reducing\nmixing by 40-50% while achieving faster convergence.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.21827", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21827", "abs": "https://arxiv.org/abs/2510.21827", "authors": ["Mojtaba Moattari"], "title": "Precise classification of low quality G-banded Chromosome Images by reliability metrics and data pruning classifier", "comment": null, "summary": "In the last decade, due to high resolution cameras and accurate meta-phase\nanalyzes, the accuracy of chromosome classification has improved substantially.\nHowever, current Karyotyping systems demand large number of high quality train\ndata to have an adequately plausible Precision per each chromosome. Such\nprovision of high quality train data with accurate devices are not yet\naccomplished in some out-reached pathological laboratories. To prevent false\npositive detections in low-cost systems and low-quality images settings, this\npaper improves the classification Precision of chromosomes using proposed\nreliability thresholding metrics and deliberately engineered features. The\nproposed method has been evaluated using a variation of deep Alex-Net neural\nnetwork, SVM, K Nearest-Neighbors, and their cascade pipelines to an automated\nfiltering of semi-straight chromosome. The classification results have highly\nimproved over 90% for the chromosomes with more common defections and\ntranslocations. Furthermore, a comparative analysis over the proposed\nthresholding metrics has been conducted and the best metric is bolded with its\nsalient characteristics. The high Precision results provided for a very\nlow-quality G-banding database verifies suitability of the proposed metrics and\npruning method for Karyotyping facilities in poor countries and lowbudget\npathological laboratories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u9760\u6027\u9608\u503c\u5ea6\u91cf\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7279\u5f81\u6765\u63d0\u9ad8\u67d3\u8272\u4f53\u5206\u7c7b\u7cbe\u5ea6\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u4f4e\u6210\u672c\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u504f\u8fdc\u75c5\u7406\u5b9e\u9a8c\u5ba4\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u51c6\u786e\u8bbe\u5907\u800c\u5bfc\u81f4\u7684\u67d3\u8272\u4f53\u5206\u7c7b\u7cbe\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u9632\u6b62\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0b\u7684\u5047\u9633\u6027\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684Alex-Net\u795e\u7ecf\u7f51\u7edc\u3001SVM\u3001K\u6700\u8fd1\u90bb\u53ca\u5176\u7ea7\u8054\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u53ef\u9760\u6027\u9608\u503c\u5ea6\u91cf\u548c\u5de5\u7a0b\u7279\u5f81\uff0c\u5bf9\u534a\u76f4\u67d3\u8272\u4f53\u8fdb\u884c\u81ea\u52a8\u8fc7\u6ee4\u3002", "result": "\u5bf9\u4e8e\u5e38\u89c1\u7f3a\u9677\u548c\u6613\u4f4d\u7684\u67d3\u8272\u4f53\uff0c\u5206\u7c7b\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\u8d85\u8fc790%\uff0c\u5728\u6781\u4f4e\u8d28\u91cfG\u5e26\u6570\u636e\u5e93\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9608\u503c\u5ea6\u91cf\u548c\u526a\u679d\u65b9\u6cd5\u9002\u7528\u4e8e\u8d2b\u56f0\u56fd\u5bb6\u548c\u4f4e\u9884\u7b97\u75c5\u7406\u5b9e\u9a8c\u5ba4\u7684\u6838\u578b\u5206\u6790\u8bbe\u65bd\u3002"}}
{"id": "2510.22680", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22680", "abs": "https://arxiv.org/abs/2510.22680", "authors": ["Shireen Kudukkil Manchingal", "Armand Amaritei", "Mihir Gohad", "Maryam Sultana", "Julian F. P. Kooij", "Fabio Cuzzolin", "Andrew Bradley"], "title": "Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead", "comment": null, "summary": "Autonomous Vehicle (AV) perception systems have advanced rapidly in recent\nyears, providing vehicles with the ability to accurately interpret their\nenvironment. Perception systems remain susceptible to errors caused by\noverly-confident predictions in the case of rare events or out-of-sample data.\nThis study equips an autonomous vehicle with the ability to 'know when it is\nuncertain', using an uncertainty-aware image classifier as part of the AV\nsoftware stack. Specifically, the study exploits the ability of Random-Set\nNeural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike\ntraditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets\nof classes, allowing the system to identify and signal uncertainty clearly in\nnovel or ambiguous scenarios. The system is tested in a real-world autonomous\nracing vehicle software stack, with the RS-NN classifying the layout of the\nroad ahead and providing the associated uncertainty of the prediction.\nPerformance of the RS-NN under a range of road conditions is compared against\ntraditional CNN and Bayesian neural networks, with the RS-NN achieving\nsignificantly higher accuracy and superior uncertainty calibration. This\nintegration of RS-NNs into Robot Operating System (ROS)-based vehicle control\npipeline demonstrates that predictive uncertainty can dynamically modulate\nvehicle speed, maintaining high-speed performance under confident predictions\nwhile proactively improving safety through speed reductions in uncertain\nscenarios. These results demonstrate the potential of uncertainty-aware neural\nnetworks - in particular RS-NNs - as a practical solution for safer and more\nrobust autonomous driving.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u968f\u673a\u96c6\u795e\u7ecf\u7f51\u7edc\uff08RS-NNs\uff09\u96c6\u6210\u5230\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f6f\u4ef6\u6808\u4e2d\uff0c\u4f7f\u8f66\u8f86\u80fd\u591f\u660e\u786e\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4e0d\u786e\u5b9a\u573a\u666f\u4e0b\u52a8\u6001\u8c03\u8282\u8f66\u901f\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5728\u9762\u5bf9\u7f55\u89c1\u4e8b\u4ef6\u6216\u6837\u672c\u5916\u6570\u636e\u65f6\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u9519\u8bef\uff0c\u9700\u8981\u8ba9\u8f66\u8f86\u5177\u5907'\u77e5\u9053\u4f55\u65f6\u4e0d\u786e\u5b9a'\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u968f\u673a\u96c6\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u9884\u6d4b\u7c7b\u522b\u96c6\u5408\u7684\u7f6e\u4fe1\u51fd\u6570\uff0c\u4e0e\u4f20\u7edfCNN\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "RS-NN\u5728\u591a\u79cd\u9053\u8def\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u4f18\u8d8a\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u80fd\u591f\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u8282\u8f66\u8f86\u901f\u5ea6\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u7279\u522b\u662fRS-NNs\uff0c\u662f\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.21978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21978", "abs": "https://arxiv.org/abs/2510.21978", "authors": ["Hoang Phan", "Xianjun Yang", "Kevin Yao", "Jingyu Zhang", "Shengjie Bi", "Xiaocheng Tang", "Madian Khabsa", "Lijuan Liu", "Deren Lei"], "title": "Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has delivered\nimpressive gains in mathematical and multimodal reasoning and has become a\nstandard post-training paradigm for contemporary language and vision-language\nmodels. However, the RLVR recipe introduces a significant risk of capability\nregression, where models forget foundational skills after prolonged training\nwithout employing regularization strategies. We empirically confirm this\nconcern, observing that open-source reasoning models suffer performance\ndegradation on core capabilities such as perception and faithfulness. While\nimposing regularization terms like KL divergence can help prevent deviation\nfrom the base model, these terms are calculated on the current task, thus they\ndo not guarantee broader knowledge. Meanwhile, commonly used experience replay\nacross heterogeneous domains makes it nontrivial to decide how much training\nfocus each objective should receive. To address this, we propose RECAP-a replay\nstrategy with dynamic objective reweighting for general knowledge preservation.\nOur reweighting mechanism adapts in an online manner using short-horizon\nsignals of convergence and instability, shifting the post-training focus away\nfrom saturated objectives and toward underperforming or volatile ones. Our\nmethod is end-to-end and readily applicable to existing RLVR pipelines without\ntraining additional models or heavy tuning. Extensive experiments on benchmarks\nbased on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our\nmethod, which not only preserves general capabilities but also improves\nreasoning by enabling more flexible trade-offs among in-task rewards.", "AI": {"tldr": "\u63d0\u51fa\u4e86RECAP\u65b9\u6cd5\uff0c\u4e00\u79cd\u5177\u6709\u52a8\u6001\u76ee\u6807\u91cd\u52a0\u6743\u7684\u56de\u653e\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bad\u7ec3\u4e2d\u9632\u6b62\u80fd\u529b\u9000\u5316\uff0c\u4fdd\u62a4\u6a21\u578b\u7684\u57fa\u7840\u77e5\u8bc6\u3002", "motivation": "RLVR\u8bad\u7ec3\u8303\u5f0f\u5b58\u5728\u80fd\u529b\u9000\u5316\u98ce\u9669\uff0c\u6a21\u578b\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u540e\u4f1a\u5fd8\u8bb0\u57fa\u7840\u6280\u80fd\uff0c\u5982\u611f\u77e5\u548c\u5fe0\u5b9e\u6027\u3002\u73b0\u6709\u7684KL\u6b63\u5219\u5316\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u5e7f\u6cdb\u77e5\u8bc6\u7684\u4fdd\u7559\uff0c\u800c\u5f02\u6784\u9886\u57df\u7684\u7ecf\u9a8c\u56de\u653e\u96be\u4ee5\u786e\u5b9a\u5404\u76ee\u6807\u7684\u8bad\u7ec3\u6743\u91cd\u3002", "method": "RECAP\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u52a8\u6001\u91cd\u52a0\u6743\u673a\u5236\uff0c\u5229\u7528\u6536\u655b\u548c\u4e0d\u7a33\u5b9a\u7684\u77ed\u671f\u4fe1\u53f7\uff0c\u5c06\u8bad\u7ec3\u91cd\u70b9\u4ece\u9971\u548c\u76ee\u6807\u8f6c\u5411\u8868\u73b0\u4e0d\u4f73\u6216\u6ce2\u52a8\u7684\u76ee\u6807\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u6216\u5927\u91cf\u8c03\u53c2\u3002", "result": "\u5728Qwen2.5-VL-3B\u548cQwen2.5-VL-7B\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u4fdd\u62a4\u4e86\u901a\u7528\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5728\u4efb\u52a1\u5956\u52b1\u95f4\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u6743\u8861\u6765\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "RECAP\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709RLVR\u6d41\u7a0b\uff0c\u65e2\u80fd\u4fdd\u62a4\u57fa\u7840\u77e5\u8bc6\uff0c\u53c8\u80fd\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.22017", "categories": ["cs.LG", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.22017", "abs": "https://arxiv.org/abs/2510.22017", "authors": ["Naina Balepur", "Xingrui Pei", "Hari Sundaram"], "title": "Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies", "comment": null, "summary": "Many governmental bodies are adopting AI policies for decision-making. In\nparticular, Reinforcement Learning has been used to design policies that\ncitizens would be expected to follow if implemented. Much RL work assumes that\ncitizens follow these policies, and evaluate them with this in mind. However,\nwe know from prior work that without institutional trust, citizens will not\nfollow policies put in place by governments. In this work, we develop a\ntrust-aware RL algorithm for resource allocation in communities. We consider\nthe case of humanitarian engineering, where the organization is aiming to\ndistribute some technology or resource to community members. We use a Deep\nDeterministic Policy Gradient approach to learn a resource allocation that fits\nthe needs of the organization. Then, we simulate resource allocation according\nto the learned policy, and model the changes in institutional trust of\ncommunity members. We investigate how this incorporation of institutional trust\naffects outcomes, and ask how effectively an organization can learn policies if\ntrust values are private. We find that incorporating trust into RL algorithms\ncan lead to more successful policies, specifically when the organization's\ngoals are less certain. We find more conservative trust estimates lead to\nincreased fairness and average community trust, though organization success\nsuffers. Finally, we explore a strategy to prevent unfair outcomes to\ncommunities. We implement a quota system by an external entity which decreases\nthe organization's utility when it does not serve enough community members. We\nfind this intervention can improve fairness and trust among communities in some\ncases, while decreasing the success of the organization. This work underscores\nthe importance of institutional trust in algorithm design and implementation,\nand identifies a tension between organization success and community well-being.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u8003\u8651\u5236\u5ea6\u4fe1\u4efb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u793e\u533a\u8d44\u6e90\u5206\u914d\uff0c\u53d1\u73b0\u5728\u7ec4\u7ec7\u76ee\u6807\u4e0d\u786e\u5b9a\u65f6\uff0c\u7eb3\u5165\u4fe1\u4efb\u673a\u5236\u80fd\u4ea7\u751f\u66f4\u6210\u529f\u7684\u653f\u7b56\uff0c\u4f46\u7ec4\u7ec7\u6210\u529f\u4e0e\u793e\u533a\u798f\u7949\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\u3002", "motivation": "\u73b0\u6709RL\u7b97\u6cd5\u5047\u8bbe\u516c\u6c11\u4f1a\u9075\u5faa\u653f\u5e9c\u5236\u5b9a\u7684\u653f\u7b56\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7f3a\u4e4f\u5236\u5ea6\u4fe1\u4efb\u65f6\u516c\u6c11\u4e0d\u4f1a\u914d\u5408\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u4e2d\u5236\u5ea6\u4fe1\u4efb\u7684\u91cd\u8981\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b66\u4e60\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u6a21\u62df\u8d44\u6e90\u5206\u914d\u8fc7\u7a0b\u5e76\u5efa\u6a21\u793e\u533a\u6210\u5458\u5236\u5ea6\u4fe1\u4efb\u7684\u53d8\u5316\uff0c\u7814\u7a76\u4fe1\u4efb\u6574\u5408\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u6574\u5408\u4fe1\u4efb\u7684RL\u7b97\u6cd5\u80fd\u4ea7\u751f\u66f4\u6210\u529f\u7684\u653f\u7b56\uff0c\u7279\u522b\u662f\u5728\u7ec4\u7ec7\u76ee\u6807\u4e0d\u786e\u5b9a\u65f6\uff1b\u4fdd\u5b88\u7684\u4fe1\u4efb\u4f30\u8ba1\u80fd\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u5e73\u5747\u793e\u533a\u4fe1\u4efb\uff0c\u4f46\u4f1a\u964d\u4f4e\u7ec4\u7ec7\u6210\u529f\u7387\uff1b\u5916\u90e8\u914d\u989d\u5e72\u9884\u53ef\u6539\u5584\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u3002", "conclusion": "\u5236\u5ea6\u4fe1\u4efb\u5728\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7ec4\u7ec7\u6210\u529f\u4e0e\u793e\u533a\u798f\u7949\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u5f20\u529b\uff0c\u9700\u8981\u5e73\u8861\u8003\u8651\u3002"}}
{"id": "2510.22602", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.22602", "abs": "https://arxiv.org/abs/2510.22602", "authors": ["Mahyar Abbasian", "Ramesh Jain"], "title": "Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance", "comment": "22 pages, 2 figures, 1 table, Journal paper", "summary": "Building on decades of success in digital infrastructure and biomedical\ninnovation, we propose the Personal Care Utility (PCU) - a cybernetic system\nfor lifelong health guidance. PCU is conceived as a global, AI-powered utility\nthat continuously orchestrates multimodal data, knowledge, and services to\nassist individuals and populations alike. Drawing on multimodal agents,\nevent-centric modeling, and contextual inference, it offers three essential\ncapabilities: (1) trusted health information tailored to the individual, (2)\nproactive health navigation and behavior guidance, and (3) ongoing\ninterpretation of recovery and treatment response after medical events. Unlike\nconventional episodic care, PCU functions as an ambient, adaptive companion -\nobserving, interpreting, and guiding health in real time across daily life. By\nintegrating personal sensing, experiential computing, and population-level\nanalytics, PCU promises not only improved outcomes for individuals but also a\nnew substrate for public health and scientific discovery. We describe the\narchitecture, design principles, and implementation challenges of this emerging\nparadigm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e2a\u4eba\u62a4\u7406\u6548\u7528\uff08PCU\uff09\u2014\u2014\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u7ec8\u8eab\u5065\u5eb7\u6307\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u77e5\u8bc6\u548c\u670d\u52a1\uff0c\u4e3a\u4e2a\u4eba\u548c\u7fa4\u4f53\u63d0\u4f9b\u6301\u7eed\u7684\u5065\u5eb7\u7ba1\u7406\u652f\u6301\u3002", "motivation": "\u57fa\u4e8e\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u548c\u751f\u7269\u533b\u5b66\u521b\u65b0\u7684\u6210\u529f\u7ecf\u9a8c\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u95f4\u6b47\u6027\u533b\u7597\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u6301\u7eed\u3001\u4e2a\u6027\u5316\u7684\u5065\u5eb7\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u667a\u80fd\u4f53\u3001\u4e8b\u4ef6\u4e2d\u5fc3\u5efa\u6a21\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u6280\u672f\uff0c\u6574\u5408\u4e2a\u4eba\u611f\u77e5\u3001\u4f53\u9a8c\u8ba1\u7b97\u548c\u7fa4\u4f53\u5206\u6790\uff0c\u6784\u5efa\u4e00\u4e2a\u73af\u5883\u5316\u3001\u81ea\u9002\u5e94\u7684\u5065\u5eb7\u4f34\u4fa3\u7cfb\u7edf\u3002", "result": "PCU\u80fd\u591f\u63d0\u4f9b\u4e2a\u6027\u5316\u53ef\u4fe1\u5065\u5eb7\u4fe1\u606f\u3001\u4e3b\u52a8\u5065\u5eb7\u5bfc\u822a\u548c\u884c\u4e3a\u6307\u5bfc\uff0c\u4ee5\u53ca\u6301\u7eed\u89e3\u91ca\u5eb7\u590d\u548c\u6cbb\u7597\u53cd\u5e94\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5065\u5eb7\u76d1\u63a7\u548c\u6307\u5bfc\u3002", "conclusion": "PCU\u4e0d\u4ec5\u80fd\u4e3a\u4e2a\u4eba\u6539\u5584\u5065\u5eb7\u7ed3\u679c\uff0c\u8fd8\u80fd\u4e3a\u516c\u5171\u536b\u751f\u548c\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u65b0\u7684\u57fa\u7840\uff0c\u4ee3\u8868\u4e86\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.21879", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21879", "abs": "https://arxiv.org/abs/2510.21879", "authors": ["Shu-Hao Zhang", "Wei-Cheng Tang", "Chen Wu", "Peng Hu", "Nan Li", "Liang-Jie Zhang", "Qi Zhang", "Shao-Qun Zhang"], "title": "TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge", "comment": null, "summary": "Recent years have witnessed an increasing interest in image-text contrastive\nmodeling, exemplified by models such as Contrastive Language-Image Pretraining\n(CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational\nframework that converts connection weights of both vision and text encoders of\nCLIP into the ternary format, instead of full-precision or floating ones.\nTernaryCLIP incorporates quantization-aware training and distillation modules,\npreventing precision degradation and enabling low-cost and high-efficiency\ncomputations. Comprehensive experiments demonstrate that TernaryCLIP can\nachieve up to 99\\% ternarized weights with 1.58-bit representation, 16.98\n$\\times$ compression ratio, 2.3 $\\times$ inference acceleration, 16 $\\times$\nstorage reduction, 10 $\\times$ memory optimization, and 60\\% sparsity while\nmaintaining promising performance on zero-shot image classification and\nimage-text retrieval tasks across 41 commonly used datasets. Our work\nhighlights the feasibility of extreme quantization for large multimodal models,\nsupporting effective and efficient deployment on resource-constrained devices.\nThe model and code can be accessed from Hugging Face and GitHub.", "AI": {"tldr": "\u63d0\u51faTernaryCLIP\u6846\u67b6\uff0c\u5c06CLIP\u6a21\u578b\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u6743\u91cd\u8f6c\u6362\u4e3a\u4e09\u5143\u683c\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6781\u7aef\u91cf\u5316\u964d\u4f4e\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u3002", "method": "\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u548c\u84b8\u998f\u6a21\u5757\uff0c\u5c06\u6743\u91cd\u8f6c\u6362\u4e3a\u4e09\u5143\u683c\u5f0f\uff081.58\u4f4d\u8868\u793a\uff09\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u548c\u7a00\u758f\u6027\u3002", "result": "\u8fbe\u523099%\u4e09\u5143\u5316\u6743\u91cd\uff0c16.98\u500d\u538b\u7f29\u6bd4\uff0c2.3\u500d\u63a8\u7406\u52a0\u901f\uff0c16\u500d\u5b58\u50a8\u51cf\u5c11\uff0c10\u500d\u5185\u5b58\u4f18\u5316\uff0c60%\u7a00\u758f\u6027\uff0c\u572841\u4e2a\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u6781\u7aef\u91cf\u5316\u7684\u53ef\u884c\u6027\uff0c\u652f\u6301\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2510.22752", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22752", "abs": "https://arxiv.org/abs/2510.22752", "authors": ["Anooshka Bajaj", "Deven Mahesh Mistry", "Sahaj Singh Maini", "Yash Aggarwal", "Zoran Tiganj"], "title": "Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models", "comment": null, "summary": "In-context learning is governed by both temporal and semantic relationships,\nshaping how Large Language Models (LLMs) retrieve contextual information.\nAnalogous to human episodic memory, where the retrieval of specific events is\nenabled by separating events that happened at different times, this work probes\nthe ability of various pretrained LLMs, including transformer and state-space\nmodels, to differentiate and retrieve temporally separated events.\nSpecifically, we prompted models with sequences containing multiple\npresentations of the same token, which reappears at the sequence end. By fixing\nthe positions of these repeated tokens and permuting all others, we removed\nsemantic confounds and isolated temporal effects on next-token prediction.\nAcross diverse sequences, models consistently placed the highest probabilities\non tokens following a repeated token, but with a notable bias for those nearest\nthe beginning or end of the input. An ablation experiment linked this\nphenomenon in transformers to induction heads. Extending the analysis to unique\nsemantic contexts with partial overlap further demonstrated that memories\nembedded in the middle of a prompt are retrieved less reliably. Despite\narchitectural differences, state-space and transformer models showed comparable\ntemporal biases. Our findings deepen the understanding of temporal biases in\nin-context learning and offer an illustration of how these biases can enable\ntemporal separation and episodic retrieval.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u65f6\u95f4\u504f\u5dee\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u68c0\u7d22\u5e8f\u5217\u5f00\u5934\u548c\u7ed3\u5c3e\u7684\u4fe1\u606f\uff0c\u800c\u4e2d\u95f4\u90e8\u5206\u7684\u4fe1\u606f\u68c0\u7d22\u53ef\u9760\u6027\u8f83\u4f4e\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u7684\u65f6\u95f4\u5206\u79bb\u673a\u5236\u7c7b\u4f3c\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u57fa\u4e8e\u65f6\u95f4\u548c\u8bed\u4e49\u5173\u7cfb\u68c0\u7d22\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u4e2d\u901a\u8fc7\u65f6\u95f4\u5206\u79bb\u6765\u68c0\u7d22\u7279\u5b9a\u4e8b\u4ef6\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u56fa\u5b9a\u91cd\u590d\u6807\u8bb0\u4f4d\u7f6e\u5e76\u7f6e\u6362\u5176\u4ed6\u6807\u8bb0\u6765\u6d88\u9664\u8bed\u4e49\u6df7\u6dc6\uff0c\u9694\u79bb\u65f6\u95f4\u6548\u5e94\u5bf9\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u7684\u5f71\u54cd\uff1b\u4f7f\u7528\u5305\u542b\u591a\u4e2a\u76f8\u540c\u6807\u8bb0\u5448\u73b0\u7684\u5e8f\u5217\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u59cb\u7ec8\u5bf9\u91cd\u590d\u6807\u8bb0\u540e\u7684\u6807\u8bb0\u8d4b\u4e88\u6700\u9ad8\u6982\u7387\uff0c\u4f46\u5b58\u5728\u660e\u663e\u7684\u504f\u5411\u5e8f\u5217\u5f00\u5934\u6216\u7ed3\u5c3e\u7684\u504f\u5dee\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u8fd9\u79cd\u73b0\u8c61\u4e0etransformer\u4e2d\u7684\u5f52\u7eb3\u5934\u76f8\u5173\uff1b\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548ctransformer\u6a21\u578b\u663e\u793a\u51fa\u76f8\u4f3c\u7684\u65f6\u95f4\u504f\u5dee\u3002", "conclusion": "\u7814\u7a76\u52a0\u6df1\u4e86\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u65f6\u95f4\u504f\u5dee\u7684\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u504f\u5dee\u5982\u4f55\u5b9e\u73b0\u65f6\u95f4\u5206\u79bb\u548c\u60c5\u666f\u68c0\u7d22\uff0c\u4e3a\u7406\u89e3LLM\u7684\u8bb0\u5fc6\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.23121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23121", "abs": "https://arxiv.org/abs/2510.23121", "authors": ["Bharath Santhanam", "Alex Mitrevski", "Santosh Thoduka", "Sebastian Houben", "Teena Hassan"], "title": "Reliable Robotic Task Execution in the Face of Anomalies", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5b66\u4e60\u7b56\u7565\u4e0e\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u7ea7\u6062\u590d\u8fc7\u7a0b\uff08\u6682\u505c\u6267\u884c\u3001\u5c40\u90e8\u6270\u52a8\u3001\u91cd\u7f6e\u5230\u5b89\u5168\u72b6\u6001\uff09\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u7b56\u7565\u6267\u884c\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5b66\u4e60\u5230\u7684\u673a\u5668\u4eba\u7b56\u7565\u867d\u7136\u901a\u7528\uff0c\u4f46\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7f3a\u4e4f\u5904\u7406\u590d\u6742\u6027\u7684\u673a\u5236\uff0c\u5bb9\u6613\u5bfc\u81f4\u6267\u884c\u5931\u8d25\uff0c\u9700\u8981\u80fd\u591f\u8bc6\u522b\u548c\u5e94\u5bf9\u6545\u969c\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u53ef\u9760\u5b89\u5168\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u540d\u4e49\u6267\u884c\u6570\u636e\u8bad\u7ec3\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5728\u7ebf\u7b56\u7565\u6267\u884c\u8fc7\u7a0b\u4e2d\u3002\u5f53\u68c0\u6d4b\u5230\u5f02\u5e38\u65f6\uff0c\u89e6\u53d1\u4e09\u7ea7\u987a\u5e8f\u6062\u590d\u8fc7\u7a0b\uff1a\u6682\u505c\u6267\u884c\u3001\u5c40\u90e8\u72b6\u6001\u6270\u52a8\u3001\u4ece\u5b66\u4e60\u5230\u7684\u6267\u884c\u6210\u529f\u6a21\u578b\u4e2d\u91c7\u6837\u91cd\u7f6e\u5230\u5b89\u5168\u72b6\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u573a\u666f\uff08Kinova Gen3\u81c2\u7684\u95e8\u628a\u624b\u5230\u8fbe\u4efb\u52a1\u548cUFactory xArm 6\u7684\u7269\u4f53\u653e\u7f6e\u4efb\u52a1\uff09\u4e2d\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u96c6\u6210\u5f02\u5e38\u68c0\u6d4b\u548c\u6062\u590d\u673a\u5236\u80fd\u663e\u8457\u63d0\u9ad8\u5728\u5b58\u5728\u5404\u79cd\u5f02\u5e38\uff08\u5982\u8f68\u8ff9\u504f\u5dee\u548c\u4eba\u4e3a\u5e72\u6270\uff09\u7684\u73af\u5883\u4e2d\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u5c06\u7b56\u7565\u6267\u884c\u4e0e\u5f02\u5e38\u68c0\u6d4b\u548c\u6062\u590d\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6267\u884c\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.22056", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.22056", "abs": "https://arxiv.org/abs/2510.22056", "authors": ["Mohammad Ali Etemadi Naeen", "Hoda Mohammadzade", "Saeed Bagheri Shouraki"], "title": "Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning", "comment": null, "summary": "Anomaly detection in surveillance videos remains a challenging task due to\nthe diversity of abnormal events, class imbalance, and scene-dependent visual\nclutter. To address these issues, we propose a robust deep learning framework\nthat integrates human-centric preprocessing with spatio-temporal modeling for\nmulti-class anomaly classification. Our pipeline begins by applying YOLO-World\n- an open-vocabulary vision-language detector - to identify human instances in\nraw video clips, followed by ByteTrack for consistent identity-aware tracking.\nBackground regions outside detected bounding boxes are suppressed via Gaussian\nblurring, effectively reducing scene-specific distractions and focusing the\nmodel on behaviorally relevant foreground content. The refined frames are then\nprocessed by an ImageNet-pretrained InceptionV3 network for spatial feature\nextraction, and temporal dynamics are captured using a bidirectional LSTM\n(BiLSTM) for sequence-level classification. Evaluated on a five-class subset of\nthe UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our\nmethod achieves a mean test accuracy of 92.41% across three independent trials,\nwith per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation\nmetrics - including confusion matrices, ROC curves, and macro/weighted averages\n- demonstrate strong generalization and resilience to class imbalance. The\nresults confirm that foreground-focused preprocessing significantly enhances\nanomaly discrimination in real-world surveillance scenarios.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4eba\u7c7b\u4e2d\u5fc3\u9884\u5904\u7406\u548c\u65f6\u7a7a\u5efa\u6a21\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.41%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u76d1\u63a7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff1a\u5f02\u5e38\u4e8b\u4ef6\u591a\u6837\u6027\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u573a\u666f\u76f8\u5173\u7684\u89c6\u89c9\u5e72\u6270\u3002", "method": "\u4f7f\u7528YOLO-World\u68c0\u6d4b\u4eba\u7c7b\u5b9e\u4f8b\uff0cByteTrack\u8fdb\u884c\u8eab\u4efd\u8ddf\u8e2a\uff0c\u9ad8\u65af\u6a21\u7cca\u6291\u5236\u80cc\u666f\uff0cInceptionV3\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0cBiLSTM\u6355\u83b7\u65f6\u5e8f\u52a8\u6001\u3002", "result": "\u5728UCF-Crime\u4e94\u7c7b\u5b50\u96c6\u4e0a\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u738792.41%\uff0c\u5404\u7c7bF1\u5206\u6570\u5747\u8d85\u8fc70.85\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u524d\u666f\u805a\u7126\u7684\u9884\u5904\u7406\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u76d1\u63a7\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2510.22094", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.22094", "abs": "https://arxiv.org/abs/2510.22094", "authors": ["Thomas Bailie", "S. Karthik Mukkavilli", "Varvara Vetrova", "Yun Sing Koh"], "title": "Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training", "comment": null, "summary": "Climate events arise from intricate, multivariate dynamics governed by\nglobal-scale drivers, profoundly impacting food, energy, and infrastructure.\nYet, accurate weather prediction remains elusive due to physical processes\nunfolding across diverse spatio-temporal scales, which fixed-resolution methods\ncannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale\nrepresentation, but nonlinear downward mappings often erase global trends,\nweakening the integration of physics into forecasts. We introduce HiFlowCast\nand its ensemble variant HiAntFlow, HGNNs that embed physics within a\nmultiscale prediction framework. Two innovations underpin their design: a\nLatent-Memory-Retention mechanism that preserves global trends during downward\ntraversal, and a Latent-to-Physics branch that integrates PDE solution fields\nacross diverse scales. Our Flow models cut errors by over 5% at 13-day lead\ntimes and by 5-8% under 1st and 99th quantile extremes, improving reliability\nfor rare events. Leveraging pretrained model weights, they converge within a\nsingle epoch, reducing training cost and their carbon footprint. Such\nefficiency is vital as the growing scale of machine learning challenges\nsustainability and limits research accessibility. Code and model weights are in\nthe supplementary materials.", "AI": {"tldr": "HiFlowCast\u548cHiAntFlow\u662f\u5d4c\u5165\u7269\u7406\u7ea6\u675f\u7684\u5c42\u6b21\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u4fdd\u7559\u5168\u5c40\u8d8b\u52bf\u548c\u591a\u5c3a\u5ea6PDE\u573a\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u5929\u6c14\u9884\u62a5\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5206\u8fa8\u7387\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u6c14\u5019\u53d8\u5316\u7684\u591a\u5c3a\u5ea6\u7269\u7406\u8fc7\u7a0b\uff0c\u800c\u73b0\u6709\u5c42\u6b21\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5411\u4e0b\u6620\u5c04\u65f6\u4f1a\u4e22\u5931\u5168\u5c40\u8d8b\u52bf\uff0c\u524a\u5f31\u7269\u7406\u7ea6\u675f\u7684\u96c6\u6210\u6548\u679c\u3002", "method": "\u63d0\u51faHiFlowCast\u548cHiAntFlow\u6a21\u578b\uff0c\u91c7\u7528\u6f5c\u5728\u8bb0\u5fc6\u4fdd\u7559\u673a\u5236\u4fdd\u6301\u5168\u5c40\u8d8b\u52bf\uff0c\u4ee5\u53ca\u6f5c\u5728\u5230\u7269\u7406\u5206\u652f\u96c6\u6210\u591a\u5c3a\u5ea6PDE\u89e3\u573a\u3002", "result": "\u572813\u5929\u9884\u62a5\u671f\u8bef\u5dee\u964d\u4f4e\u8d85\u8fc75%\uff0c\u5728\u6781\u7aef\u5206\u4f4d\u6570\u6761\u4ef6\u4e0b\u8bef\u5dee\u964d\u4f4e5-8%\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u53ef\u5728\u5355\u4e2aepoch\u5185\u6536\u655b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u63d0\u5347\u5929\u6c14\u9884\u62a5\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u5bf9\u673a\u5668\u5b66\u4e60\u53ef\u6301\u7eed\u6027\u548c\u7814\u7a76\u53ef\u53ca\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.22968", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22968", "abs": "https://arxiv.org/abs/2510.22968", "authors": ["Michael Hardy"], "title": "Measuring Teaching with LLMs", "comment": null, "summary": "Objective and scalable measurement of teaching quality is a persistent\nchallenge in education. While Large Language Models (LLMs) offer potential,\ngeneral-purpose models have struggled to reliably apply complex, authentic\nclassroom observation instruments. This paper uses custom LLMs built on\nsentence-level embeddings, an architecture better suited for the long-form,\ninterpretive nature of classroom transcripts than conventional subword\ntokenization. We systematically evaluate five different sentence embeddings\nunder a data-efficient training regime designed to prevent overfitting. Our\nresults demonstrate that these specialized models can achieve human-level and\neven super-human performance with expert human ratings above 0.65 and\nsurpassing the average human-human rater correlation. Further, through analysis\nof annotation context windows, we find that more advanced models-those better\naligned with human judgments-attribute a larger share of score variation to\nlesson-level features rather than isolated utterances, challenging the\nsufficiency of single-turn annotation paradigms. Finally, to assess external\nvalidity, we find that aggregate model scores align with teacher value-added\nmeasures, indicating they are capturing features relevant to student learning.\nHowever, this trend does not hold at the individual item level, suggesting that\nwhile the models learn useful signals, they have not yet achieved full\ngeneralization. This work establishes a viable and powerful new methodology for\nAI-driven instructional measurement, offering a path toward providing scalable,\nreliable, and valid feedback for educator development.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u57fa\u4e8e\u53e5\u5b50\u5d4c\u5165\u7684\u5b9a\u5236\u5316LLM\uff0c\u7528\u4e8e\u8bc4\u4f30\u6559\u5b66\u8d28\u91cf\uff0c\u5728\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u4e0b\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u751a\u81f3\u8d85\u4eba\u7c7b\u8868\u73b0\uff0c\u5e76\u4e0e\u6559\u5e08\u589e\u503c\u6307\u6807\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u53ef\u9760\u5e94\u7528\u590d\u6742\u7684\u8bfe\u5802\u89c2\u5bdf\u5de5\u5177\uff0c\u9700\u8981\u4e13\u95e8\u65b9\u6cd5\u6765\u89e3\u51b3\u6559\u5b66\u8d28\u91cf\u7684\u5ba2\u89c2\u53ef\u6269\u5c55\u6d4b\u91cf\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53e5\u5b50\u7ea7\u5d4c\u5165\u6784\u5efa\u5b9a\u5236\u5316LLM\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e94\u79cd\u4e0d\u540c\u53e5\u5b50\u5d4c\u5165\uff0c\u91c7\u7528\u9632\u6b62\u8fc7\u62df\u5408\u7684\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u673a\u5236\u3002", "result": "\u4e13\u4e1a\u6a21\u578b\u4e0e\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u5206\u76f8\u5173\u6027\u8d85\u8fc70.65\uff0c\u8d85\u8d8a\u5e73\u5747\u4eba-\u4eba\u8bc4\u5206\u76f8\u5173\u6027\uff1b\u9ad8\u7ea7\u6a21\u578b\u5c06\u66f4\u591a\u5206\u6570\u53d8\u5316\u5f52\u56e0\u4e8e\u8bfe\u7a0b\u7ea7\u7279\u5f81\u800c\u975e\u5b64\u7acb\u8bdd\u8bed\u3002", "conclusion": "\u5efa\u7acb\u4e86AI\u9a71\u52a8\u6559\u5b66\u6d4b\u91cf\u7684\u53ef\u884c\u5f3a\u5927\u65b0\u65b9\u6cd5\uff0c\u4e3a\u6559\u80b2\u8005\u53d1\u5c55\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u9760\u548c\u6709\u6548\u7684\u53cd\u9988\u8def\u5f84\u3002"}}
{"id": "2510.22225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22225", "abs": "https://arxiv.org/abs/2510.22225", "authors": ["Yu Luo", "Nan Huang", "Sophie Yu", "Hendry Xu", "Jerry Wang", "Colin Wang", "Zhichao Liu", "Chen Zeng"], "title": "Audio Frequency-Time Dual Domain Evaluation on Depression Diagnosis", "comment": null, "summary": "Depression, as a typical mental disorder, has become a prevalent issue\nsignificantly impacting public health. However, the prevention and treatment of\ndepression still face multiple challenges, including complex diagnostic\nprocedures, ambiguous criteria, and low consultation rates, which severely\nhinder timely assessment and intervention. To address these issues, this study\nadopts voice as a physiological signal and leverages its frequency-time dual\ndomain multimodal characteristics along with deep learning models to develop an\nintelligent assessment and diagnostic algorithm for depression. Experimental\nresults demonstrate that the proposed method achieves excellent performance in\nthe classification task for depression diagnosis, offering new insights and\napproaches for the assessment, screening, and diagnosis of depression.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8bed\u97f3\u4f5c\u4e3a\u751f\u7406\u4fe1\u53f7\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u6291\u90c1\u75c7\u667a\u80fd\u8bc4\u4f30\u8bca\u65ad\u7b97\u6cd5\uff0c\u5728\u6291\u90c1\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6291\u90c1\u75c7\u4f5c\u4e3a\u5178\u578b\u7cbe\u795e\u969c\u788d\u5df2\u6210\u4e3a\u4e25\u91cd\u5f71\u54cd\u516c\u5171\u5065\u5eb7\u7684\u95ee\u9898\uff0c\u4f46\u5176\u9632\u6cbb\u9762\u4e34\u8bca\u65ad\u7a0b\u5e8f\u590d\u6742\u3001\u6807\u51c6\u6a21\u7cca\u3001\u5c31\u8bca\u7387\u4f4e\u7b49\u6311\u6218\uff0c\u963b\u788d\u4e86\u53ca\u65f6\u8bc4\u4f30\u548c\u5e72\u9884\u3002", "method": "\u91c7\u7528\u8bed\u97f3\u4f5c\u4e3a\u751f\u7406\u4fe1\u53f7\uff0c\u5229\u7528\u5176\u9891\u65f6\u53cc\u57df\u591a\u6a21\u6001\u7279\u5f81\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u667a\u80fd\u8bc4\u4f30\u8bca\u65ad\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6291\u90c1\u75c7\u8bca\u65ad\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u4e3a\u6291\u90c1\u75c7\u7684\u8bc4\u4f30\u3001\u7b5b\u67e5\u548c\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2510.22293", "categories": ["cs.LG", "cs.CY", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.22293", "abs": "https://arxiv.org/abs/2510.22293", "authors": ["Mary E. An", "Paul Griffin", "Jonathan G. Stine", "Ramakrishna Balakrishnan", "Ram Sriram", "Soundar Kumara"], "title": "Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods", "comment": null, "summary": "Background: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD)\naffects ~33% of U.S. adults and is the most common chronic liver disease.\nAlthough often asymptomatic, progression can lead to cirrhosis. Early detection\nis important, as lifestyle interventions can prevent disease progression. We\ndeveloped a fair, rigorous, and reproducible MASLD prediction model and\ncompared it to prior methods using a large electronic health record database.\n  Methods: We evaluated LASSO logistic regression, random forest, XGBoost, and\na neural network for MASLD prediction using clinical feature subsets, including\nthe top 10 SHAP-ranked features. To reduce disparities in true positive rates\nacross racial and ethnic subgroups, we applied an equal opportunity\npostprocessing method.\n  Results: This study included 59,492 patients in the training data, 24,198 in\nthe validating data, and 25,188 in the testing data. The LASSO logistic\nregression model with the top 10 features was selected for its interpretability\nand comparable performance. Before fairness adjustment, the model achieved\nAUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and\nF1-score of 0.617. After equal opportunity postprocessing, accuracy modestly\nincreased to 81% and specificity to 94%, while sensitivity decreased to 41% and\nF1-score to 0.515, reflecting the fairness trade-off.\n  Conclusions: We developed the MASER prediction model (MASLD Static EHR Risk\nPrediction), a LASSO logistic regression model which achieved competitive\nperformance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to\npreviously reported ensemble and tree-based models. Overall, this approach\ndemonstrates that interpretable models can achieve a balance of predictive\nperformance and fairness in diverse patient populations.", "AI": {"tldr": "\u5f00\u53d1\u4e86MASER\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4ee3\u8c22\u529f\u80fd\u969c\u788d\u76f8\u5173\u8102\u80aa\u809d\u75c5(MASLD)\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u540e\u5904\u7406\u51cf\u5c11\u79cd\u65cf\u548c\u6c11\u65cf\u4e9a\u7ec4\u95f4\u7684\u9884\u6d4b\u5dee\u5f02\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "MASLD\u5f71\u54cd\u7ea633%\u7684\u7f8e\u56fd\u6210\u5e74\u4eba\uff0c\u662f\u6700\u5e38\u89c1\u7684\u6162\u6027\u809d\u75c5\u3002\u65e9\u671f\u68c0\u6d4b\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u751f\u6d3b\u65b9\u5f0f\u5e72\u9884\u53ef\u4ee5\u9884\u9632\u75be\u75c5\u8fdb\u5c55\u3002\u9700\u8981\u5f00\u53d1\u516c\u5e73\u3001\u4e25\u8c28\u4e14\u53ef\u590d\u73b0\u7684MASLD\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u8bc4\u4f30\u4e86LASSO\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u4e34\u5e8a\u7279\u5f81\u5b50\u96c6\uff08\u5305\u62ec\u524d10\u4e2aSHAP\u6392\u540d\u7279\u5f81\uff09\u3002\u5e94\u7528\u5e73\u7b49\u673a\u4f1a\u540e\u5904\u7406\u65b9\u6cd5\u51cf\u5c11\u79cd\u65cf\u548c\u6c11\u65cf\u4e9a\u7ec4\u95f4\u7684\u771f\u9633\u6027\u7387\u5dee\u5f02\u3002", "result": "\u8bad\u7ec3\u6570\u636e59,492\u4f8b\uff0c\u9a8c\u8bc1\u6570\u636e24,198\u4f8b\uff0c\u6d4b\u8bd5\u6570\u636e25,188\u4f8b\u3002\u9009\u62e9\u5177\u6709\u524d10\u4e2a\u7279\u5f81\u7684LASSO\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3002\u516c\u5e73\u6027\u8c03\u6574\u524d\uff1aAUROC 0.84\uff0c\u51c6\u786e\u738778%\uff0c\u654f\u611f\u602772%\uff0c\u7279\u5f02\u602779%\uff0cF1\u5206\u65700.617\u3002\u8c03\u6574\u540e\uff1a\u51c6\u786e\u738781%\uff0c\u7279\u5f02\u602794%\uff0c\u654f\u611f\u602741%\uff0cF1\u5206\u65700.515\u3002", "conclusion": "MASER\u9884\u6d4b\u6a21\u578b\u5728MASLD\u9884\u6d4b\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff08AUROC 0.836\uff0c\u51c6\u786e\u738777.6%\uff09\uff0c\u4e0e\u4e4b\u524d\u62a5\u544a\u7684\u96c6\u6210\u548c\u57fa\u4e8e\u6811\u7684\u6a21\u578b\u76f8\u5f53\u3002\u53ef\u89e3\u91ca\u6a21\u578b\u53ef\u4ee5\u5728\u591a\u6837\u5316\u60a3\u8005\u7fa4\u4f53\u4e2d\u5b9e\u73b0\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2510.22319", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22319", "abs": "https://arxiv.org/abs/2510.22319", "authors": ["Jing Wang", "Jiajun Liang", "Jie Liu", "Henglin Liu", "Gongye Liu", "Jun Zheng", "Wanyuan Pang", "Ao Ma", "Zhenyu Xie", "Xintao Wang", "Meng Wang", "Pengfei Wan", "Xiaodan Liang"], "title": "GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping", "comment": null, "summary": "Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.", "AI": {"tldr": "GRPO-Guard\u901a\u8fc7\u6bd4\u7387\u5f52\u4e00\u5316\u548c\u68af\u5ea6\u91cd\u52a0\u6743\u89e3\u51b3\u4e86GRPO\u5f3a\u5316\u5b66\u4e60\u4e2d\u91cd\u8981\u6027\u6bd4\u7387\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u6709\u6548\u9632\u6b62\u9690\u5f0f\u8fc7\u5ea6\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u91cd\u8981\u6027\u6bd4\u7387\u5206\u5e03\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u79fb\uff08\u5747\u503c\u4f4e\u4e8e1\u3001\u65b9\u5dee\u4e0d\u4e00\u81f4\uff09\uff0c\u5bfc\u81f4PPO\u526a\u88c1\u673a\u5236\u65e0\u6cd5\u6709\u6548\u7ea6\u675f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u6b63\u5411\u66f4\u65b0\uff0c\u9020\u6210\u9690\u5f0f\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faGRPO-Guard\u65b9\u6cd5\uff1a1\uff09\u6bd4\u7387\u5f52\u4e00\u5316\u6062\u590d\u5e73\u8861\u4e14\u6b65\u957f\u4e00\u81f4\u7684\u91cd\u8981\u6027\u6bd4\u7387\uff1b2\uff09\u68af\u5ea6\u91cd\u52a0\u6743\u7b56\u7565\u5747\u8861\u4e0d\u540c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\uff0c\u9632\u6b62\u7279\u5b9a\u65f6\u95f4\u6b65\u533a\u57df\u7684\u8fc7\u5ea6\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u6269\u6563\u6a21\u578b\u9aa8\u5e72\uff08SD3.5M\u3001Flux.1-dev\uff09\u548c\u591a\u6837\u5316\u4ee3\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRPO-Guard\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u5ea6\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "GRPO-Guard\u4f5c\u4e3a\u4e00\u79cd\u53d7\u8c03\u8282\u7684\u526a\u88c1\u673a\u5236\uff0c\u65e0\u9700\u4f9d\u8d56\u6c89\u91cd\u7684KL\u6b63\u5219\u5316\u5373\u53ef\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u6709\u6548\u7f13\u89e3\u9690\u5f0f\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2510.22383", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22383", "abs": "https://arxiv.org/abs/2510.22383", "authors": ["David Freire-Obreg\u00f3n", "Jos\u00e9 Salas-C\u00e1ceres", "Modesto Castrill\u00f3n-Santana"], "title": "Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization", "comment": "Accepted for presentation at the 5th International Conference on\n  Computing and Machine Intelligence (ICMI 2026)", "summary": "Regularization techniques play a crucial role in preventing overfitting and\nimproving the generalization performance of neural networks. Dropout, a widely\nused regularization technique, randomly deactivates units during training to\nintroduce redundancy and prevent co-adaptation among neurons. Despite its\neffectiveness, dropout has limitations, such as its static nature and lack of\ninterpretability. In this paper, we propose a novel approach to regularization\nby substituting dropout with Conway's Game of Life (GoL), a cellular automata\nwith simple rules that govern the evolution of a grid of cells. We introduce\ndynamic unit deactivation during training by representing neural network units\nas cells in a GoL grid and applying the game's rules to deactivate units. This\napproach allows for the emergence of spatial patterns that adapt to the\ntraining data, potentially enhancing the network's ability to generalize. We\ndemonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing\nthat dynamic unit deactivation using GoL achieves comparable performance to\ntraditional dropout techniques while offering insights into the network's\nbehavior through the visualization of evolving patterns. Furthermore, our\ndiscussion highlights the applicability of our proposal in deeper\narchitectures, demonstrating how it enhances the performance of different\ndropout techniques.", "AI": {"tldr": "\u63d0\u51fa\u7528\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u66ff\u4ee3dropout\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u52a8\u6001\u5355\u5143\u5931\u6d3b\u548c\u7a7a\u95f4\u6a21\u5f0f\u6f14\u5316\u6765\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfdropout\u5b58\u5728\u9759\u6001\u6027\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u5c40\u9650\uff0c\u9700\u8981\u66f4\u52a8\u6001\u3001\u53ef\u89e3\u91ca\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u5355\u5143\u8868\u793a\u4e3a\u751f\u547d\u6e38\u620f\u7f51\u683c\u4e2d\u7684\u7ec6\u80de\uff0c\u5e94\u7528\u6e38\u620f\u89c4\u5219\u52a8\u6001\u5931\u6d3b\u5355\u5143\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u8bad\u7ec3\u6570\u636e\u7684\u7a7a\u95f4\u6a21\u5f0f\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4f20\u7edfdropout\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u901a\u8fc7\u53ef\u89c6\u5316\u6f14\u5316\u6a21\u5f0f\u63d0\u4f9b\u7f51\u7edc\u884c\u4e3a\u6d1e\u5bdf\u3002", "conclusion": "\u57fa\u4e8e\u751f\u547d\u6e38\u620f\u7684\u52a8\u6001\u5355\u5143\u5931\u6d3b\u662f\u6709\u6548\u7684\u6b63\u5219\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6df1\u5c42\u67b6\u6784\u5e76\u80fd\u589e\u5f3a\u4e0d\u540cdropout\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23337", "abs": "https://arxiv.org/abs/2510.23337", "authors": ["Siyuan Zheng", "Pai Liu", "Xi Chen", "Jizheng Dong", "Sihan Jia"], "title": "BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning", "comment": null, "summary": "Human-like virtual characters are crucial for games, storytelling, and\nvirtual reality, yet current methods rely heavily on annotated data or\nhandcrafted persona prompts, making it difficult to scale up and generate\nrealistic, contextually coherent personas. We create the first QA dataset for\nBaZi-based persona reasoning, where real human experiences categorized into\nwealth, health, kinship, career, and relationships are represented as\nlife-event questions and answers. Furthermore, we propose the first BaZi-LLM\nsystem that integrates symbolic reasoning with large language models to\ngenerate temporally dynamic and fine-grained virtual personas. Compared with\nmainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a\n30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information\nis used, our model's accuracy drops by 20%-45%, showing the potential of\nculturally grounded symbolic-LLM integration for realistic character\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u516b\u5b57\u547d\u7406\u7684\u4eba\u683c\u63a8\u7406\u95ee\u7b54\u6570\u636e\u96c6\u548cBaZi-LLM\u7cfb\u7edf\uff0c\u5c06\u7b26\u53f7\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u751f\u6210\u65f6\u95f4\u52a8\u6001\u4e14\u7ec6\u7c92\u5ea6\u7684\u865a\u62df\u4eba\u683c\uff0c\u76f8\u6bd4\u4e3b\u6d41LLM\u51c6\u786e\u7387\u63d0\u534730.3%-62.6%\u3002", "motivation": "\u5f53\u524d\u865a\u62df\u89d2\u8272\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u624b\u5de5\u5236\u4f5c\u7684\u4eba\u683c\u63d0\u793a\uff0c\u96be\u4ee5\u6269\u5c55\u4e14\u96be\u4ee5\u751f\u6210\u771f\u5b9e\u3001\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u4eba\u683c\u3002", "method": "\u521b\u5efa\u9996\u4e2a\u516b\u5b57\u547d\u7406\u4eba\u683c\u63a8\u7406\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5c06\u4eba\u7c7b\u7ecf\u9a8c\u5206\u7c7b\u4e3a\u8d22\u5bcc\u3001\u5065\u5eb7\u3001\u4eb2\u5c5e\u3001\u804c\u4e1a\u548c\u5173\u7cfb\u7b49\u751f\u6d3b\u4e8b\u4ef6\u95ee\u7b54\uff1b\u63d0\u51faBaZi-LLM\u7cfb\u7edf\uff0c\u6574\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u76f8\u6bd4DeepSeek-v3\u548cGPT-5-mini\u7b49\u4e3b\u6d41LLM\uff0c\u51c6\u786e\u7387\u63d0\u534730.3%-62.6%\uff1b\u5f53\u4f7f\u7528\u9519\u8bef\u516b\u5b57\u4fe1\u606f\u65f6\uff0c\u6a21\u578b\u51c6\u786e\u7387\u4e0b\u964d20%-45%\u3002", "conclusion": "\u57fa\u4e8e\u6587\u5316\u57fa\u7840\u7684\u7b26\u53f7-LLM\u6574\u5408\u5728\u771f\u5b9e\u89d2\u8272\u6a21\u62df\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2510.22521", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22521", "abs": "https://arxiv.org/abs/2510.22521", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Wei Bi", "Yupeng Hu", "Liqiang Nie"], "title": "Open Multimodal Retrieval-Augmented Factual Image Generation", "comment": "Preprint", "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.", "AI": {"tldr": "ORIG\u662f\u4e00\u4e2a\u7528\u4e8e\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210(FIG)\u7684\u4ee3\u7406\u5f0f\u5f00\u653e\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u68c0\u7d22\u548c\u8fc7\u6ee4\u7f51\u7edc\u591a\u6a21\u6001\u8bc1\u636e\uff0c\u9010\u6b65\u6574\u5408\u7cbe\u70bc\u77e5\u8bc6\u5230\u589e\u5f3a\u63d0\u793a\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0e\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u76f8\u77db\u76fe\u7684\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u7ec6\u7c92\u5ea6\u5c5e\u6027\u6216\u65f6\u95f4\u654f\u611f\u4e8b\u4ef6\u7684\u63d0\u793a\u65f6\u3002\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6765\u6e90\u548c\u6d45\u5c42\u8bc1\u636e\u6574\u5408\uff0c\u65e0\u6cd5\u57fa\u4e8e\u51c6\u786e\u548c\u6f14\u5316\u7684\u77e5\u8bc6\u8fdb\u884c\u751f\u6210\u3002", "method": "ORIG\u6846\u67b6\u8fed\u4ee3\u5730\u4ece\u7f51\u7edc\u68c0\u7d22\u548c\u8fc7\u6ee4\u591a\u6a21\u6001\u8bc1\u636e\uff0c\u5e76\u9010\u6b65\u5c06\u7cbe\u70bc\u77e5\u8bc6\u6574\u5408\u5230\u589e\u5f3a\u63d0\u793a\u4e2d\u6307\u5bfc\u751f\u6210\u3002\u6784\u5efa\u4e86FIG-Eval\u57fa\u51c6\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u7ec4\u5408\u548c\u65f6\u95f4\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5341\u4e2a\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eORIG\u5728\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u5f00\u653e\u591a\u6a21\u6001\u68c0\u7d22\u5728\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "ORIG\u901a\u8fc7\u5f00\u653e\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u77db\u76fe\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u65e2\u903c\u771f\u53c8\u4e8b\u5b9e\u51c6\u786e\u7684\u56fe\u50cf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.23396", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23396", "abs": "https://arxiv.org/abs/2510.23396", "authors": ["Musleh Alharthi", "Kaleel Mahmood", "Sarosh Patel", "Ausif Mahmood"], "title": "EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting", "comment": null, "summary": "The immense success of the Transformer architecture\n  in Natural Language Processing has led to its adoption in Time Se ries\nForecasting (TSF), where superior performance has been shown.\n  However, a recent important paper questioned their effectiveness by\n  demonstrating that a simple single layer linear model outperforms\n  Transformer-based models. This was soon shown to be not as valid,\n  by a better transformer-based model termed PatchTST. More re cently, TimeLLM\ndemonstrated even better results by repurposing a\n  Large Language Model (LLM) for the TSF domain. Again, a follow\n  up paper challenged this by demonstrating that removing the LLM\n  component or replacing it with a basic attention layer in fact yields\n  better performance. One of the challenges in forecasting is the fact\n  that TSF data favors the more recent past, and is sometimes subject\n  to unpredictable events. Based upon these recent insights in TSF, we\n  propose a strong Mixture of Experts (MoE) framework. Our method\n  combines the state-of-the-art (SOTA) models including xLSTM, en hanced\nLinear, PatchTST, and minGRU, among others. This set of\n  complimentary and diverse models for TSF are integrated in a Trans former\nbased MoE gating network. Our proposed model outperforms\n  all existing TSF models on standard benchmarks, surpassing even the\n  latest approaches based on MoE frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7ed3\u5408\u4e86xLSTM\u3001\u589e\u5f3a\u7ebf\u6027\u6a21\u578b\u3001PatchTST\u548cminGRU\u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u901a\u8fc7Transformer\u95e8\u63a7\u7f51\u7edc\u96c6\u6210\u8fd9\u4e9b\u4e92\u8865\u7684\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u65f6\u5e8f\u9884\u6d4b\u9886\u57dfTransformer\u6a21\u578b\u6709\u6548\u6027\u7684\u4e89\u8bae\uff0c\u4ee5\u53ca\u6570\u636e\u503e\u5411\u4e8e\u8fd1\u671f\u5386\u53f2\u4e14\u6613\u53d7\u4e0d\u53ef\u9884\u6d4b\u4e8b\u4ef6\u5f71\u54cd\u7684\u7279\u70b9\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6574\u5408\u591a\u79cd\u4e92\u8865\u6a21\u578b\u7684\u5f3a\u5927\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u96c6\u6210xLSTM\u3001\u589e\u5f3a\u7ebf\u6027\u6a21\u578b\u3001PatchTST\u3001minGRU\u7b49SOTA\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u95e8\u63a7\u7f51\u7edc\u6765\u534f\u8c03\u8fd9\u4e9b\u4e0d\u540c\u7684\u65f6\u5e8f\u9884\u6d4b\u4e13\u5bb6\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u7684\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\uff0c\u5305\u62ec\u6700\u65b0\u7684\u57fa\u4e8eMoE\u6846\u67b6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4e92\u8865\u7684\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\uff0c\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u65f6\u5e8f\u6570\u636e\u7684\u7279\u6027\uff0c\u63d0\u4f9b\u66f4\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.22534", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22534", "abs": "https://arxiv.org/abs/2510.22534", "authors": ["Chen Chen", "Majid Abdolshah", "Violetta Shevchenko", "Hongdong Li", "Chang Xu", "Pulak Purkait"], "title": "SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning", "comment": "Accepted at NeurIPS 2025", "summary": "Existing diffusion-based super-resolution approaches often exhibit semantic\nambiguities due to inaccuracies and incompleteness in their text conditioning,\ncoupled with the inherent tendency for cross-attention to divert towards\nirrelevant pixels. These limitations can lead to semantic misalignment and\nhallucinated details in the generated high-resolution outputs. To address\nthese, we propose a novel, plug-and-play spatially re-focused super-resolution\n(SRSR) framework that consists of two core components: first, we introduce\nSpatially Re-focused Cross-Attention (SRCA), which refines text conditioning at\ninference time by applying visually-grounded segmentation masks to guide\ncross-attention. Second, we introduce a Spatially Targeted Classifier-Free\nGuidance (STCFG) mechanism that selectively bypasses text influences on\nungrounded pixels to prevent hallucinations. Extensive experiments on both\nsynthetic and real-world datasets demonstrate that SRSR consistently\noutperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR\nand SSIM) across all datasets, and in perceptual quality measures (LPIPS and\nDISTS) on two real-world benchmarks, underscoring its effectiveness in\nachieving both high semantic fidelity and perceptual quality in\nsuper-resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5373\u63d2\u5373\u7528\u7a7a\u95f4\u91cd\u805a\u7126\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u91cd\u805a\u7126\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u7a7a\u95f4\u76ee\u6807\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7531\u4e8e\u6587\u672c\u6761\u4ef6\u7684\u4e0d\u51c6\u786e\u6027\u548c\u4e0d\u5b8c\u6574\u6027\uff0c\u4ee5\u53ca\u4ea4\u53c9\u6ce8\u610f\u529b\u5bb9\u6613\u504f\u5411\u65e0\u5173\u50cf\u7d20\u7684\u56fa\u6709\u503e\u5411\uff0c\u7ecf\u5e38\u8868\u73b0\u51fa\u8bed\u4e49\u6a21\u7cca\uff0c\u5bfc\u81f4\u8bed\u4e49\u9519\u4f4d\u548c\u5e7b\u89c9\u7ec6\u8282\u3002", "method": "\u63d0\u51faSRSR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7a7a\u95f4\u91cd\u805a\u7126\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u89c6\u89c9\u57fa\u7840\u5206\u5272\u63a9\u7801\u5f15\u5bfc\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u7ec6\u5316\u6587\u672c\u6761\u4ef6\uff1b2) \u7a7a\u95f4\u76ee\u6807\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u673a\u5236\uff0c\u9009\u62e9\u6027\u5730\u7ed5\u8fc7\u5bf9\u672a\u63a5\u5730\u50cf\u7d20\u7684\u6587\u672c\u5f71\u54cd\u4ee5\u9632\u6b62\u5e7b\u89c9\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSRSR\u5728\u6240\u6709\u6570\u636e\u96c6\u7684\u6807\u51c6\u4fdd\u771f\u5ea6\u6307\u6807\uff08PSNR\u548cSSIM\uff09\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4e03\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u7684\u611f\u77e5\u8d28\u91cf\u6307\u6807\uff08LPIPS\u548cDISTS\uff09\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "SRSR\u5728\u8d85\u5206\u8fa8\u7387\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.22693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22693", "abs": "https://arxiv.org/abs/2510.22693", "authors": ["Wenlong Li", "Yifei Xu", "Yuan Rao", "Zhenhua Wang", "Shuiguang Deng"], "title": "VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree", "comment": "NeurIPS 2025 Camera Ready", "summary": "Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.", "AI": {"tldr": "\u63d0\u51faVADTree\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7c92\u5ea6\u611f\u77e5\u6811\u7ed3\u6784\u5b9e\u73b0\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u7075\u6d3b\u91c7\u6837\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u77e5\u8bc6\u51cf\u5c11\u91c7\u6837\u7247\u6bb5\u6570\u91cf\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9886\u57df\u5185\u8bad\u7ec3\u6570\u636e\u4e14\u65e0\u6cd5\u63d0\u4f9b\u6e05\u6670\u89e3\u91ca\uff1b\u65e0\u8bad\u7ec3\u65b9\u6cd5\u867d\u7136\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u77e5\u8bc6\uff0c\u4f46\u56fa\u5b9a\u957f\u5ea6\u65f6\u95f4\u7a97\u53e3\u91c7\u6837\u96be\u4ee5\u51c6\u786e\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u7684\u5f02\u5e38\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u7c92\u5ea6\u611f\u77e5\u6811\u7ed3\u6784\u8fdb\u884c\u7075\u6d3b\u91c7\u6837\uff0c\u57fa\u4e8e\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u6a21\u578b\u5206\u89e3\u89c6\u9891\u4e3a\u4e8b\u4ef6\u8282\u70b9\uff0c\u8fdb\u884c\u81ea\u9002\u5e94\u7c97\u7ec6\u5206\u5c42\u7ed3\u6784\u548c\u5197\u4f59\u53bb\u9664\uff0c\u7136\u540e\u5411\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6ce8\u5165\u591a\u7ef4\u5148\u9a8c\u589e\u5f3a\u5f02\u5e38\u611f\u77e5\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5f02\u5e38\u63a8\u7406\uff0c\u6700\u540e\u4f7f\u7528\u7c07\u95f4\u8282\u70b9\u76f8\u5173\u6027\u65b9\u6cd5\u6574\u5408\u591a\u7c92\u5ea6\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVADTree\u5728\u65e0\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u91c7\u6837\u7684\u89c6\u9891\u7247\u6bb5\u6570\u91cf\u3002", "conclusion": "VADTree\u901a\u8fc7\u5206\u5c42\u7c92\u5ea6\u611f\u77e5\u6811\u7ed3\u6784\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u77e5\u8bc6\u7684\u6709\u6548\u5229\u7528\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u65f6\u95f4\u8de8\u5ea6\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.22818", "categories": ["cs.LG", "cs.AI", "68T07, 68U35", "I.2.7; I.5.4; C.3"], "pdf": "https://arxiv.org/pdf/2510.22818", "abs": "https://arxiv.org/abs/2510.22818", "authors": ["Soham Pahari", "Sandeep Chand Kumain"], "title": "Air Quality Prediction Using LOESS-ARIMA and Multi-Scale CNN-BiLSTM with Residual-Gated Attention", "comment": null, "summary": "Air pollution remains a critical environmental and public health concern in\nIndian megacities such as Delhi, Kolkata, and Mumbai, where sudden spikes in\npollutant levels challenge timely intervention. Accurate Air Quality Index\n(AQI) forecasting is difficult due to the coexistence of linear trends,\nseasonal variations, and volatile nonlinear patterns. This paper proposes a\nhybrid forecasting framework that integrates LOESS decomposition, ARIMA\nmodeling, and a multi-scale CNN-BiLSTM network with a residual-gated attention\nmechanism. The LOESS step separates the AQI series into trend, seasonal, and\nresidual components, with ARIMA modeling the smooth components and the proposed\ndeep learning module capturing multi-scale volatility in the residuals. Model\nhyperparameters are tuned via the Unified Adaptive Multi-Stage Metaheuristic\nOptimizer (UAMMO), combining multiple optimization strategies for efficient\nconvergence. Experiments on 2021-2023 AQI datasets from the Central Pollution\nControl Board show that the proposed method consistently outperforms\nstatistical, deep learning, and hybrid baselines across PM2.5, O3, CO, and NOx\nin three major cities, achieving up to 5-8% lower MSE and higher R^2 scores\n(>0.94) for all pollutants. These results demonstrate the framework's\nrobustness, sensitivity to sudden pollution events, and applicability to urban\nair quality management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LOESS\u5206\u89e3\u3001ARIMA\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6CNN-BiLSTM\u7f51\u7edc\u7684\u6df7\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5370\u5ea6\u5927\u57ce\u5e02\u7684\u7a7a\u6c14\u8d28\u91cf\u6307\u6570(AQI)\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6c61\u67d3\u7269\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5370\u5ea6\u5927\u57ce\u5e02\u7a7a\u6c14\u8d28\u91cf\u95ee\u9898\u4e25\u91cd\uff0c\u6c61\u67d3\u7269\u6c34\u5e73\u7a81\u7136\u98d9\u5347\u7ed9\u53ca\u65f6\u5e72\u9884\u5e26\u6765\u6311\u6218\u3002AQI\u9884\u6d4b\u56e0\u540c\u65f6\u5b58\u5728\u7ebf\u6027\u8d8b\u52bf\u3001\u5b63\u8282\u53d8\u5316\u548c\u6ce2\u52a8\u975e\u7ebf\u6027\u6a21\u5f0f\u800c\u56f0\u96be\u3002", "method": "\u4f7f\u7528LOESS\u5206\u89e3\u5c06AQI\u5e8f\u5217\u5206\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u548c\u6b8b\u5dee\u5206\u91cf\uff0cARIMA\u5efa\u6a21\u5e73\u6ed1\u5206\u91cf\uff0c\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\uff08\u5e26\u6b8b\u5dee\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5c3a\u5ea6CNN-BiLSTM\u7f51\u7edc\uff09\u6355\u6349\u6b8b\u5dee\u4e2d\u7684\u591a\u5c3a\u5ea6\u6ce2\u52a8\uff0c\u4f7f\u7528UAMMO\u4f18\u5316\u5668\u8c03\u6574\u8d85\u53c2\u6570\u3002", "result": "\u57282021-2023\u5e74\u4e2d\u592e\u6c61\u67d3\u63a7\u5236\u59d4\u5458\u4f1aAQI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728PM2.5\u3001O3\u3001CO\u548cNOx\u7b49\u6c61\u67d3\u7269\u4e0a\u5747\u4f18\u4e8e\u7edf\u8ba1\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u6df7\u5408\u57fa\u7ebf\u65b9\u6cd5\uff0cMSE\u964d\u4f4e5-8%\uff0cR^2\u5f97\u5206>0.94\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5bf9\u7a81\u53d1\u6c61\u67d3\u4e8b\u4ef6\u654f\u611f\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.22868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22868", "abs": "https://arxiv.org/abs/2510.22868", "authors": ["Yang Zhang", "Qianyu Zhou", "Farhad Imani", "Jiong Tang"], "title": "Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models", "comment": null, "summary": "Wind turbine blades operate in harsh environments, making timely damage\ndetection essential for preventing failures and optimizing maintenance.\nDrone-based inspection and deep learning are promising, but typically depend on\nlarge, labeled datasets, which limit their ability to detect rare or evolving\ndamage types. To address this, we propose a zero-shot-oriented inspection\nframework that integrates Retrieval-Augmented Generation (RAG) with\nVision-Language Models (VLM). A multimodal knowledge base is constructed,\ncomprising technical documentation, representative reference images, and\ndomain-specific guidelines. A hybrid text-image retriever with keyword-aware\nreranking assembles the most relevant context to condition the VLM at\ninference, injecting domain knowledge without task-specific training. We\nevaluate the framework on 30 labeled blade images covering diverse damage\ncategories. Although the dataset is small due to the difficulty of acquiring\nverified blade imagery, it covers multiple representative defect types. On this\ntest set, the RAG-grounded VLM correctly classified all samples, whereas the\nsame VLM without retrieval performed worse in both accuracy and precision. We\nfurther compare against open-vocabulary baselines and incorporate uncertainty\nClopper-Pearson confidence intervals to account for the small-sample setting.\nAblation studies indicate that the key advantage of the framework lies in\nexplainability and generalizability: retrieved references ground the reasoning\nprocess and enable the detection of previously unseen defects by leveraging\ndomain knowledge rather than relying solely on visual cues. This research\ncontributes a data-efficient solution for industrial inspection that reduces\ndependence on extensive labeled datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u96f6\u6837\u672c\u98ce\u673a\u53f6\u7247\u635f\u4f24\u68c0\u6d4b\u6846\u67b6\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u635f\u4f24\u7c7b\u578b\u3002", "motivation": "\u98ce\u673a\u53f6\u7247\u5728\u6076\u52a3\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u9700\u8981\u53ca\u65f6\u68c0\u6d4b\u635f\u4f24\u4ee5\u9632\u6b62\u6545\u969c\u3002\u73b0\u6709\u57fa\u4e8e\u65e0\u4eba\u673a\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u68c0\u6d4b\u7f55\u89c1\u6216\u65b0\u51fa\u73b0\u7684\u635f\u4f24\u7c7b\u578b\u3002", "method": "\u6784\u5efa\u5305\u542b\u6280\u672f\u6587\u6863\u3001\u53c2\u8003\u56fe\u50cf\u548c\u9886\u57df\u6307\u5357\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u5e93\uff0c\u4f7f\u7528\u6df7\u5408\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u5668\u7ed3\u5408\u5173\u952e\u8bcd\u611f\u77e5\u91cd\u6392\u5e8f\uff0c\u4e3aVLM\u63d0\u4f9b\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u572830\u5f20\u6807\u8bb0\u53f6\u7247\u56fe\u50cf\u6d4b\u8bd5\u96c6\u4e0a\uff0cRAG\u589e\u5f3a\u7684VLM\u6b63\u786e\u5206\u7c7b\u4e86\u6240\u6709\u6837\u672c\uff0c\u800c\u672a\u7ecf\u68c0\u7d22\u7684\u76f8\u540cVLM\u5728\u51c6\u786e\u7387\u548c\u7cbe\u5ea6\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u8be5\u6846\u67b6\u5728\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5de5\u4e1a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u800c\u975e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\u6765\u68c0\u6d4b\u672a\u89c1\u8fc7\u7684\u7f3a\u9677\u3002"}}
{"id": "2510.22889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22889", "abs": "https://arxiv.org/abs/2510.22889", "authors": ["Darshana Priyasad", "Tharindu Fernando", "Maryam Haghighat", "Harshala Gammulle", "Clinton Fookes"], "title": "Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection", "comment": "Preprint to appear in IEEE IGARSS 2025", "summary": "Natural disasters, such as volcanic eruptions, pose significant challenges to\ndaily life and incur considerable global economic losses. The emergence of\nnext-generation small-satellites, capable of constellation-based operations,\noffers unparalleled opportunities for near-real-time monitoring and onboard\nprocessing of such events. However, a major bottleneck remains the lack of\nextensive annotated datasets capturing volcanic activity, which hinders the\ndevelopment of robust detection systems. This paper introduces a novel dataset\nexplicitly designed for volcanic activity and eruption detection, encompassing\ndiverse volcanoes worldwide. The dataset provides binary annotations to\nidentify volcanic anomalies or non-anomalies, covering phenomena such as\ntemperature anomalies, eruptions, and volcanic ash emissions. These annotations\noffer a foundational resource for developing and evaluating detection models,\naddressing a critical gap in volcanic monitoring research. Additionally, we\npresent comprehensive benchmarks using state-of-the-art models to establish\nbaselines for future studies. Furthermore, we explore the potential for\ndeploying these models onboard next-generation satellites. Using the Intel\nMovidius Myriad X VPU as a testbed, we demonstrate the feasibility of volcanic\nactivity detection directly onboard. This capability significantly reduces\nlatency and enhances response times, paving the way for advanced early warning\nsystems. This paves the way for innovative solutions in volcanic disaster\nmanagement, encouraging further exploration and refinement of onboard\nmonitoring technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u706b\u5c71\u6d3b\u52a8\u68c0\u6d4b\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0b\u4e00\u4ee3\u536b\u661f\u4e0a\u90e8\u7f72\u68c0\u6d4b\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u706b\u5c71\u707e\u5bb3\u7ba1\u7406\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u706b\u5c71\u707e\u5bb3\u9020\u6210\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\uff0c\u65b0\u4e00\u4ee3\u5c0f\u536b\u661f\u63d0\u4f9b\u4e86\u8fd1\u5b9e\u65f6\u76d1\u6d4b\u673a\u4f1a\uff0c\u4f46\u7f3a\u4e4f\u706b\u5c71\u6d3b\u52a8\u7684\u6807\u6ce8\u6570\u636e\u96c6\u963b\u788d\u4e86\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u521b\u5efa\u4e13\u95e8\u7528\u4e8e\u706b\u5c71\u6d3b\u52a8\u548c\u55b7\u53d1\u68c0\u6d4b\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e8c\u5143\u6807\u6ce8\u8bc6\u522b\u706b\u5c71\u5f02\u5e38\uff1b\u4f7f\u7528\u6700\u5148\u8fdb\u6a21\u578b\u5efa\u7acb\u57fa\u51c6\uff1b\u5728Intel Movidius Myriad X VPU\u4e0a\u6d4b\u8bd5\u661f\u8f7d\u90e8\u7f72\u53ef\u884c\u6027\u3002", "result": "\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff1b\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u7ebf\uff1b\u8bc1\u660e\u4e86\u5728\u536b\u661f\u4e0a\u76f4\u63a5\u8fdb\u884c\u706b\u5c71\u6d3b\u52a8\u68c0\u6d4b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u661f\u8f7d\u706b\u5c71\u6d3b\u52a8\u68c0\u6d4b\u80fd\u529b\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u54cd\u5e94\u65f6\u95f4\uff0c\u4e3a\u5148\u8fdb\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63a8\u52a8\u4e86\u706b\u5c71\u707e\u5bb3\u7ba1\u7406\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.22960", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22960", "abs": "https://arxiv.org/abs/2510.22960", "authors": ["Zhangkai Wu", "Xuhui Fan", "Zhongyuan Xie", "Kaize Shi", "Zhidong Li", "Longbing Cao"], "title": "FAME: Fairness-aware Attention-modulated Video Editing", "comment": null, "summary": "Training-free video editing (VE) models tend to fall back on gender\nstereotypes when rendering profession-related prompts. We propose \\textbf{FAME}\nfor \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates\nprofession-related gender biases while preserving prompt alignment and temporal\nconsistency for coherent VE. We derive fairness embeddings from existing\nminority representations by softly injecting debiasing tokens into the text\nencoder. Simultaneously, FAME integrates fairness modulation into both temporal\nself attention and prompt-to-region cross attention to mitigate the motion\ncorruption and temporal inconsistency caused by directly introducing fairness\ncues. For temporal self attention, FAME introduces a region constrained\nattention mask combined with time decay weighting, which enhances intra-region\ncoherence while suppressing irrelevant inter-region interactions. For cross\nattention, it reweights tokens to region matching scores by incorporating\nfairness sensitive similarity masks derived from debiasing prompt embeddings.\nTogether, these modulations keep fairness-sensitive semantics tied to the right\nvisual regions and prevent temporal drift across frames. Extensive experiments\non new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME\nachieves stronger fairness alignment and semantic fidelity, surpassing existing\nVE baselines.", "AI": {"tldr": "FAME\u662f\u4e00\u4e2a\u516c\u5e73\u6027\u611f\u77e5\u7684\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u8c03\u5236\u6765\u51cf\u8f7b\u804c\u4e1a\u76f8\u5173\u7684\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u5bf9\u9f50\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u514d\u8bad\u7ec3\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u804c\u4e1a\u76f8\u5173\u63d0\u793a\u65f6\u5bb9\u6613\u9677\u5165\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u9700\u8981\u89e3\u51b3\u804c\u4e1a\u76f8\u5173\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u516c\u5e73\u6027\u5d4c\u5165\uff0c\u901a\u8fc7\u8f6f\u6ce8\u5165\u53bb\u504f\u6807\u8bb0\u5230\u6587\u672c\u7f16\u7801\u5668\uff1b\u5728\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u548c\u8de8\u6ce8\u610f\u529b\u4e2d\u96c6\u6210\u516c\u5e73\u6027\u8c03\u5236\uff1b\u5f15\u5165\u533a\u57df\u7ea6\u675f\u6ce8\u610f\u529b\u63a9\u7801\u548c\u65f6\u95f4\u8870\u51cf\u6743\u91cd\uff1b\u91cd\u65b0\u52a0\u6743\u6807\u8bb0\u5230\u533a\u57df\u5339\u914d\u5206\u6570\u3002", "result": "\u5728FairVE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAME\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u516c\u5e73\u6027\u5bf9\u9f50\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u89c6\u9891\u7f16\u8f91\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FAME\u901a\u8fc7\u516c\u5e73\u6027\u611f\u77e5\u7684\u6ce8\u610f\u529b\u8c03\u5236\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u804c\u4e1a\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u63d0\u793a\u5bf9\u9f50\u3002"}}
{"id": "2510.22955", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22955", "abs": "https://arxiv.org/abs/2510.22955", "authors": ["Junhao Fan", "Wenrui Liang", "Wei-Qiang Zhang"], "title": "SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction", "comment": "5 pages, 2 figures, 3 tables. Equal contribution by Junhao Fan and\n  Wenrui Liang. Corresponding author: Wei-Qiang Zhang. Submitted to ICASSP 2026", "summary": "Accurate prediction of remaining useful life (RUL) is essential to enhance\nsystem reliability and reduce maintenance risk. Yet many strong contemporary\nmodels are fragile around fault onset and opaque to engineers: short,\nhigh-energy spikes are smoothed away or misread, fixed thresholds blunt\nsensitivity, and physics-based explanations are scarce. To remedy this, we\nintroduce SARNet (Spike-Aware Consecutive Validation Framework), which builds\non a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware\ndetection to provide physics-informed interpretability. ModernTCN forecasts\ndegradation-sensitive indicators; an adaptive consecutive threshold validates\ntrue spikes while suppressing noise. Failure-prone segments then receive\ntargeted feature engineering (spectral slopes, statistical derivatives, energy\nratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across\nbenchmark-ported datasets under an event-triggered protocol, SARNet\nconsistently lowers error compared to recent baselines (RMSE 0.0365, MAE\n0.0204) while remaining lightweight, robust, and easy to deploy.", "AI": {"tldr": "SARNet\u662f\u4e00\u4e2a\u7528\u4e8e\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u7684\u5c16\u5cf0\u611f\u77e5\u8fde\u7eed\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u73b0\u4ee3\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u548c\u5c16\u5cf0\u68c0\u6d4b\u673a\u5236\uff0c\u63d0\u4f9b\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u6a21\u578b\u5728\u6545\u969c\u8d77\u59cb\u70b9\u9644\u8fd1\u8868\u73b0\u8106\u5f31\u3001\u5bf9\u5de5\u7a0b\u5e08\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u5982\u77ed\u65f6\u9ad8\u80fd\u5c16\u5cf0\u88ab\u5e73\u6ed1\u6216\u8bef\u8bfb\u3001\u56fa\u5b9a\u9608\u503c\u964d\u4f4e\u7075\u654f\u5ea6\u3001\u7f3a\u4e4f\u57fa\u4e8e\u7269\u7406\u89e3\u91ca\u7b49\u3002", "method": "\u57fa\u4e8e\u73b0\u4ee3\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u9884\u6d4b\u9000\u5316\u654f\u611f\u6307\u6807\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u8fde\u7eed\u9608\u503c\u9a8c\u8bc1\u771f\u5b9e\u5c16\u5cf0\u5e76\u6291\u5236\u566a\u58f0\uff0c\u5bf9\u6545\u969c\u6613\u53d1\u6bb5\u8fdb\u884c\u9488\u5bf9\u6027\u7279\u5f81\u5de5\u7a0b\uff0c\u6700\u540e\u4f7f\u7528\u5806\u53e0\u7684\u968f\u673a\u68ee\u6797-\u8f7b\u91cf\u68af\u5ea6\u63d0\u5347\u673a\u56de\u5f52\u5668\u751f\u6210\u6700\u7ec8RUL\u9884\u6d4b\u3002", "result": "\u5728\u4e8b\u4ef6\u89e6\u53d1\u534f\u8bae\u4e0b\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSARNet\u76f8\u6bd4\u8fd1\u671f\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u964d\u4f4e\u8bef\u5dee\uff08RMSE 0.0365\uff0cMAE 0.0204\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u3001\u9c81\u68d2\u4e14\u6613\u4e8e\u90e8\u7f72\u3002", "conclusion": "SARNet\u901a\u8fc7\u5c16\u5cf0\u611f\u77e5\u68c0\u6d4b\u548c\u7269\u7406\u4fe1\u606f\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u7cfb\u7edf\u7ef4\u62a4\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2510.23190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23190", "abs": "https://arxiv.org/abs/2510.23190", "authors": ["Pascal Benschop", "Cristian Meo", "Justin Dauwels", "Jelte P. Mense"], "title": "Evaluation of Vision-LLMs in Surveillance Video", "comment": "Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,\n  Language, and Embodied AI", "summary": "The widespread use of cameras in our society has created an overwhelming\namount of video data, far exceeding the capacity for human monitoring. This\npresents a critical challenge for public safety and security, as the timely\ndetection of anomalous or criminal events is crucial for effective response and\nprevention. The ability for an embodied agent to recognize unexpected events is\nfundamentally tied to its capacity for spatial reasoning. This paper\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\nanomalous action recognition as a zero-shot, language-grounded task, addressing\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\ninto text descriptions and scoring labels via textual entailment. We evaluate\nfour open models on UCF-Crime and RWF-2000 under prompting and\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\nmodels, but may increase false positives, and privacy filters -- especially\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\nThese results chart where current vision--LLMs succeed (simple, spatially\nsalient events) and where they falter (noisy spatial cues, identity\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\ngrounding without task-specific training: structure-aware prompts, lightweight\nspatial memory across clips, scene-graph or 3D-pose priors during description,\nand privacy methods that preserve action-relevant geometry. This positions\nzero-shot, language-grounded pipelines as adaptable building blocks for\nembodied, real-world video understanding. Our implementation for evaluating\nVLMs is publicly available at:\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u5f02\u5e38\u884c\u4e3a\u8bc6\u522b\u6784\u5efa\u4e3a\u96f6\u6837\u672c\u3001\u57fa\u4e8e\u8bed\u8a00\u7684\u4efb\u52a1\uff0c\u8bc4\u4f30\u5c0f\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9-LLM\u4f5c\u4e3a\u7a7a\u95f4\u57fa\u7840\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u6548\u679c\u3002", "motivation": "\u6444\u50cf\u5934\u4ea7\u751f\u7684\u6d77\u91cf\u89c6\u9891\u6570\u636e\u8fdc\u8d85\u4eba\u5de5\u76d1\u63a7\u80fd\u529b\uff0c\u9700\u8981\u81ea\u52a8\u68c0\u6d4b\u5f02\u5e38\u4e8b\u4ef6\u4ee5\u4fdd\u969c\u516c\u5171\u5b89\u5168\u3002\u667a\u80fd\u4f53\u8bc6\u522b\u610f\u5916\u4e8b\u4ef6\u7684\u80fd\u529b\u4e0e\u5176\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5bc6\u5207\u76f8\u5173\u3002", "method": "\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u901a\u8fc7\u6587\u672c\u8574\u542b\u5bf9\u6807\u7b7e\u8fdb\u884c\u8bc4\u5206\uff0c\u8bc4\u4f30\u56db\u79cd\u5f00\u6e90\u6a21\u578b\u5728UCF-Crime\u548cRWF-2000\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u63d0\u793a\u548c\u9690\u79c1\u4fdd\u62a4\u6761\u4ef6\u4e0b\u7684\u6d4b\u8bd5\u3002", "result": "\u5c11\u91cf\u6837\u672c\u793a\u4f8b\u53ef\u63d0\u9ad8\u67d0\u4e9b\u6a21\u578b\u7684\u51c6\u786e\u7387\uff0c\u4f46\u53ef\u80fd\u589e\u52a0\u8bef\u62a5\uff1b\u9690\u79c1\u8fc7\u6ee4\u5668\uff08\u7279\u522b\u662f\u5168\u8eabGAN\u53d8\u6362\uff09\u4f1a\u5f15\u5165\u4e0d\u4e00\u81f4\u6027\u964d\u4f4e\u51c6\u786e\u7387\u3002\u6a21\u578b\u5728\u7b80\u5355\u3001\u7a7a\u95f4\u663e\u8457\u4e8b\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u566a\u58f0\u7a7a\u95f4\u7ebf\u7d22\u548c\u8eab\u4efd\u6a21\u7cca\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u589e\u5f3a\u7a7a\u95f4\u57fa\u7840\u7684\u5177\u4f53\u8def\u5f84\uff1a\u7ed3\u6784\u611f\u77e5\u63d0\u793a\u3001\u8de8\u7247\u6bb5\u7684\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u8bb0\u5fc6\u3001\u63cf\u8ff0\u8fc7\u7a0b\u4e2d\u7684\u573a\u666f\u56fe\u62163D\u59ff\u6001\u5148\u9a8c\uff0c\u4ee5\u53ca\u4fdd\u7559\u52a8\u4f5c\u76f8\u5173\u51e0\u4f55\u7684\u9690\u79c1\u65b9\u6cd5\u3002"}}
{"id": "2510.23191", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23191", "abs": "https://arxiv.org/abs/2510.23191", "authors": ["Timo Freiesleben", "Sebastian Zezulka"], "title": "The Benchmarking Epistemology: Construct Validity for Evaluating Machine Learning Models", "comment": null, "summary": "Predictive benchmarking, the evaluation of machine learning models based on\npredictive performance and competitive ranking, is a central epistemic practice\nin machine learning research and an increasingly prominent method for\nscientific inquiry. Yet, benchmark scores alone provide at best measurements of\nmodel performance relative to an evaluation dataset and a concrete learning\nproblem. Drawing substantial scientific inferences from the results, say about\ntheoretical tasks like image classification, requires additional assumptions\nabout the theoretical structure of the learning problems, evaluation functions,\nand data distributions. We make these assumptions explicit by developing\nconditions of construct validity inspired by psychological measurement theory.\nWe examine these assumptions in practice through three case studies, each\nexemplifying a typical intended inference: measuring engineering progress in\ncomputer vision with ImageNet; evaluating policy-relevant weather predictions\nwith WeatherBench; and examining limitations of the predictability of life\nevents with the Fragile Families Challenge. Our framework clarifies the\nconditions under which benchmark scores can support diverse scientific claims,\nbringing predictive benchmarking into perspective as an epistemological\npractice and a key site of conceptual and theoretical reasoning in machine\nlearning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9884\u6d4b\u6027\u57fa\u51c6\u6d4b\u8bd5\u7684\u6784\u5efa\u6548\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5206\u6790\u4e86\u57fa\u51c6\u5206\u6570\u652f\u6301\u79d1\u5b66\u63a8\u65ad\u6240\u9700\u7684\u6761\u4ef6\u3002", "motivation": "\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u672c\u8eab\u53ea\u80fd\u8861\u91cf\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u8981\u4ece\u4e2d\u5f97\u51fa\u6709\u610f\u4e49\u7684\u79d1\u5b66\u63a8\u65ad\u9700\u8981\u989d\u5916\u7684\u7406\u8bba\u5047\u8bbe\uff0c\u4f5c\u8005\u5e0c\u671b\u660e\u786e\u8fd9\u4e9b\u5047\u8bbe\u6761\u4ef6\u3002", "method": "\u501f\u9274\u5fc3\u7406\u6d4b\u91cf\u7406\u8bba\u5f00\u53d1\u6784\u5efa\u6548\u5ea6\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7ImageNet\u3001WeatherBench\u548cFragile Families Challenge\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u6765\u68c0\u9a8c\u8fd9\u4e9b\u5047\u8bbe\u3002", "result": "\u660e\u786e\u4e86\u57fa\u51c6\u5206\u6570\u652f\u6301\u79d1\u5b66\u63a8\u65ad\u6240\u9700\u7684\u7406\u8bba\u7ed3\u6784\u3001\u8bc4\u4f30\u51fd\u6570\u548c\u6570\u636e\u5206\u5e03\u7b49\u5047\u8bbe\u6761\u4ef6\uff0c\u4e3a\u9884\u6d4b\u6027\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u8ba4\u8bc6\u8bba\u6846\u67b6\u3002", "conclusion": "\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u4ec5\u662f\u6280\u672f\u5b9e\u8df5\uff0c\u66f4\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u6982\u5ff5\u548c\u7406\u8bba\u63a8\u7406\u7684\u5173\u952e\u573a\u6240\uff0c\u9700\u8981\u660e\u786e\u7684\u6784\u5efa\u6548\u5ea6\u6761\u4ef6\u6765\u652f\u6301\u79d1\u5b66\u63a8\u65ad\u3002"}}
{"id": "2510.23306", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23306", "abs": "https://arxiv.org/abs/2510.23306", "authors": ["Jiahao Chang", "Chongjie Ye", "Yushuang Wu", "Yuantao Chen", "Yidan Zhang", "Zhongjin Luo", "Chenghong Li", "Yihao Zhi", "Xiaoguang Han"], "title": "ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation", "comment": "18 pages, 7 figures", "summary": "Existing multi-view 3D object reconstruction methods heavily rely on\nsufficient overlap between input views, where occlusions and sparse coverage in\npractice frequently yield severe reconstruction incompleteness. Recent\nadvancements in diffusion-based 3D generative techniques offer the potential to\naddress these limitations by leveraging learned generative priors to\nhallucinate invisible parts of objects, thereby generating plausible 3D\nstructures. However, the stochastic nature of the inference process limits the\naccuracy and reliability of generation results, preventing existing\nreconstruction frameworks from integrating such 3D generative priors. In this\nwork, we comprehensively analyze the reasons why diffusion-based 3D generative\nmethods fail to achieve high consistency, including (a) the insufficiency in\nconstructing and leveraging cross-view connections when extracting multi-view\nimage features as conditions, and (b) the poor controllability of iterative\ndenoising during local detail generation, which easily leads to plausible but\ninconsistent fine geometric and texture details with inputs. Accordingly, we\npropose ReconViaGen to innovatively integrate reconstruction priors into the\ngenerative framework and devise several strategies that effectively address\nthese issues. Extensive experiments demonstrate that our ReconViaGen can\nreconstruct complete and accurate 3D models consistent with input views in both\nglobal structure and local details.Project page:\nhttps://jiahao620.github.io/reconviagen.", "AI": {"tldr": "ReconViaGen\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u89c6\u89d23D\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u91cd\u5efa\u5148\u9a8c\u6574\u5408\u5230\u751f\u6210\u6846\u67b6\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u5f0f3D\u751f\u6210\u65b9\u6cd5\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u591a\u89c6\u89d23D\u91cd\u5efa\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u8f93\u5165\u89c6\u89d2\u4e4b\u95f4\u7684\u5145\u5206\u91cd\u53e0\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u906e\u6321\u548c\u7a00\u758f\u8986\u76d6\u7ecf\u5e38\u5bfc\u81f4\u4e25\u91cd\u7684\u4e0d\u5b8c\u6574\u91cd\u5efa\u3002\u6269\u6563\u5f0f3D\u751f\u6210\u6280\u672f\u867d\u7136\u80fd\u591f\u901a\u8fc7\u751f\u6210\u5148\u9a8c\u6765\u8865\u5168\u4e0d\u53ef\u89c1\u90e8\u5206\uff0c\u4f46\u5176\u968f\u673a\u6027\u9650\u5236\u4e86\u751f\u6210\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86ReconViaGen\u65b9\u6cd5\uff0c\u521b\u65b0\u6027\u5730\u5c06\u91cd\u5efa\u5148\u9a8c\u6574\u5408\u5230\u751f\u6210\u6846\u67b6\u4e2d\uff0c\u8bbe\u8ba1\u4e86\u591a\u79cd\u7b56\u7565\u6765\u89e3\u51b3\u6269\u6563\u5f0f3D\u751f\u6210\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5305\u62ec\uff1a(a)\u6539\u8fdb\u591a\u89c6\u89d2\u56fe\u50cf\u7279\u5f81\u7684\u6784\u5efa\u548c\u8de8\u89c6\u89d2\u8fde\u63a5\u5229\u7528\uff1b(b)\u589e\u5f3a\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u5c40\u90e8\u7ec6\u8282\u751f\u6210\u53ef\u63a7\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cReconViaGen\u80fd\u591f\u91cd\u5efa\u51fa\u4e0e\u8f93\u5165\u89c6\u89d2\u5728\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u7ec6\u8282\u4e0a\u90fd\u4e00\u81f4\u7684\u5b8c\u6574\u4e14\u51c6\u786e\u76843D\u6a21\u578b\u3002", "conclusion": "ReconViaGen\u901a\u8fc7\u5c06\u91cd\u5efa\u5148\u9a8c\u6574\u5408\u5230\u751f\u6210\u6846\u67b6\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u5f0f3D\u751f\u6210\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u89c6\u89d23D\u91cd\u5efa\u3002"}}
{"id": "2510.23364", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23364", "abs": "https://arxiv.org/abs/2510.23364", "authors": ["Hyeongkyun Kim", "Orestis Oikonomou"], "title": "ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping", "comment": "Preprint submitted to EUSAR 2026 (under review)", "summary": "Flood susceptibility mapping (FSM) is vital for disaster prevention but\nremains challenging in data-scarce regions where hydrodynamic models require\ndense geophysical inputs. This work introduces ZeroFlood, a geospatial\nfoundation model framework for data-efficient FSM. The approach fine-tunes\nGeospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,\nenabling flood prediction from basic Earth observation data such as Sentinel-1\nor Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich\nregions, ZeroFlood bridges data availability gaps through cross-modal\nrepresentation learning. Experiments with TerraMind and Prithvi GFMs show that\nTiM enhances model robustness, with the TerraMind-Large configuration achieving\nan F1 score of 67.21. The results demonstrate the feasibility of\nfoundation-model-based FSM as a scalable and data-efficient solution for flood\nrisk management.", "AI": {"tldr": "ZeroFlood\u662f\u4e00\u4e2a\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7Thinking-in-Modality\u63a8\u7406\u5fae\u8c03\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u672c\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5b9e\u73b0\u6d2a\u6c34\u6613\u53d1\u6027\u5236\u56fe\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u533a\u57df\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u5730\u533a\u8fdb\u884c\u6d2a\u6c34\u6613\u53d1\u6027\u5236\u56fe\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6c34\u52a8\u529b\u6a21\u578b\u9700\u8981\u5bc6\u96c6\u7684\u5730\u7403\u7269\u7406\u8f93\u5165\u6570\u636e\u3002", "method": "\u901a\u8fc7Thinking-in-Modality\u63a8\u7406\u5fae\u8c03\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528\u6570\u636e\u4e30\u5bcc\u533a\u57df\u7684\u914d\u5bf9\u5730\u7403\u89c2\u6d4b\u548c\u6a21\u62df\u6d2a\u6c34\u5730\u56fe\u8fdb\u884c\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTiM\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0cTerraMind-Large\u914d\u7f6e\u8fbe\u523067.21\u7684F1\u5206\u6570\u3002", "conclusion": "\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6d2a\u6c34\u6613\u53d1\u6027\u5236\u56fe\u662f\u6d2a\u6c34\u98ce\u9669\u7ba1\u7406\u7684\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23569", "abs": "https://arxiv.org/abs/2510.23569", "authors": ["Baoqi Pei", "Yifei Huang", "Jilan Xu", "Yuping He", "Guo Chen", "Fei Wu", "Yu Qiao", "Jiangmiao Pang"], "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT", "comment": "Accepted at NeurIPS 2025", "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera\nwho dynamically shapes the environment, requiring inference of hidden\nintentions and recognition of fine-grained interactions. This core challenge\nlimits current multimodal large language models MLLMs, which excel at visible\nevent reasoning but lack embodied, first-person understanding. To bridge this\ngap, we introduce EgoThinker, a novel framework that endows MLLMs with robust\negocentric reasoning capabilities through spatio-temporal chain-of-thought\nsupervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,\na large-scale egocentric QA dataset constructed from 13M diverse egocentric\nvideo clips. This dataset features multi-minute segments annotated with\ndetailed CoT rationales and dense hand-object grounding. Second, we employ SFT\non EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning\nRFT to further enhance spatio-temporal localization. Experimental results show\nthat EgoThinker outperforms existing methods across multiple egocentric\nbenchmarks, while achieving substantial improvements in fine-grained\nspatio-temporal localization tasks. Full code and data are released at\nhttps://github.com/InternRobotics/EgoThinker.", "AI": {"tldr": "EgoThinker\u662f\u4e00\u4e2a\u8d4b\u4e88\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u81ea\u6211\u4e2d\u5fc3\u63a8\u7406\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u601d\u7ef4\u94fe\u76d1\u7763\u548c\u4e24\u9636\u6bb5\u5b66\u4e60\u8bfe\u7a0b\uff0c\u5728\u591a\u4e2a\u81ea\u6211\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524dMLLMs\u64c5\u957f\u53ef\u89c1\u4e8b\u4ef6\u63a8\u7406\u4f46\u7f3a\u4e4f\u5177\u8eab\u7684\u7b2c\u4e00\u4eba\u79f0\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u4e0d\u53ef\u89c2\u5bdf\u7684\u667a\u80fd\u4f53\u52a8\u6001\u5851\u9020\u73af\u5883\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u81ea\u6211\u4e2d\u5fc3QA\u6570\u636e\u96c6EgoRe-5M\uff0c\u5305\u542b1300\u4e07\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u6807\u6ce8\u8be6\u7ec6\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u5bc6\u96c6\u624b-\u7269\u4f53\u5b9a\u4f4d\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\uff1a\u76d1\u7763\u5fae\u8c03(SFT)\u57f9\u517b\u63a8\u7406\u6280\u80fd\uff0c\u5f3a\u5316\u5fae\u8c03(RFT)\u589e\u5f3a\u65f6\u7a7a\u5b9a\u4f4d\u80fd\u529b\u3002", "result": "EgoThinker\u5728\u591a\u4e2a\u81ea\u6211\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u8d4b\u4e88MLLMs\u5f3a\u5927\u7684\u81ea\u6211\u4e2d\u5fc3\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23581", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23581", "abs": "https://arxiv.org/abs/2510.23581", "authors": ["Junyoung Seo", "Rodrigo Mira", "Alexandros Haliassos", "Stella Bounareli", "Honglie Chen", "Linh Tran", "Seungryong Kim", "Zoe Landgraf", "Jie Shen"], "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation", "comment": "Project page: https://lookahead-anchoring.github.io", "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.", "AI": {"tldr": "\u63d0\u51faLookahead Anchoring\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6765\u65f6\u95f4\u6b65\u7684\u5173\u952e\u5e27\u4f5c\u4e3a\u65b9\u5411\u6027\u951a\u70b9\uff0c\u89e3\u51b3\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u52a8\u753b\u4e2d\u7684\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u5173\u952e\u5e27\u751f\u6210\u9636\u6bb5\u3002", "motivation": "\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u52a8\u753b\u6a21\u578b\u5728\u65f6\u95f4\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u51fa\u73b0\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u73b0\u6709\u4f7f\u7528\u5173\u952e\u5e27\u4f5c\u4e3a\u65f6\u95f4\u951a\u70b9\u7684\u65b9\u6cd5\u9700\u8981\u989d\u5916\u751f\u6210\u9636\u6bb5\u4e14\u9650\u5236\u81ea\u7136\u8fd0\u52a8\u52a8\u6001\u3002", "method": "\u5229\u7528\u5f53\u524d\u751f\u6210\u7a97\u53e3\u524d\u65b9\u7684\u672a\u6765\u65f6\u95f4\u6b65\u5173\u952e\u5e27\u4f5c\u4e3a\u65b9\u5411\u6027\u951a\u70b9\uff0c\u6a21\u578b\u5728\u54cd\u5e94\u5373\u65f6\u97f3\u9891\u7ebf\u7d22\u7684\u540c\u65f6\u6301\u7eed\u8ffd\u6c42\u8fd9\u4e9b\u672a\u6765\u951a\u70b9\uff0c\u5b9e\u73b0\u81ea\u6211\u5173\u952e\u5e27\u5316\uff0c\u53c2\u8003\u56fe\u50cf\u76f4\u63a5\u4f5c\u4e3a\u524d\u77bb\u76ee\u6807\u3002", "result": "\u5728\u4e09\u4e2a\u4eba\u4f53\u52a8\u753b\u6a21\u578b\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5507\u90e8\u540c\u6b65\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u8de8\u4e0d\u540c\u67b6\u6784\u7684\u6539\u8fdb\u65f6\u95f4\u6761\u4ef6\u5316\u6548\u679c\u3002", "conclusion": "Lookahead Anchoring\u901a\u8fc7\u524d\u77bb\u951a\u70b9\u673a\u5236\u6709\u6548\u5e73\u8861\u8868\u8fbe\u6027\u548c\u4e00\u81f4\u6027\uff0c\u65f6\u95f4\u524d\u77bb\u8ddd\u79bb\u81ea\u7136\u63a7\u5236\u8fd0\u52a8\u81ea\u7531\u5ea6\u548c\u8eab\u4efd\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2510.23577", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23577", "abs": "https://arxiv.org/abs/2510.23577", "authors": ["Zhongyi Yu", "Jianqiu Wu", "Zhenghao Wu", "Shuhan Zhong", "Weifeng Su", "Chul-Ho Lee", "Weipeng Zhuo"], "title": "TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction", "comment": "Accepted to NeurIPS 2025", "summary": "Temporal graph link prediction aims to predict future interactions between\nnodes in a graph based on their historical interactions, which are encoded in\nnode embeddings. We observe that heterogeneity naturally appears in temporal\ninteractions, e.g., a few node pairs can make most interaction events, and\ninteraction events happen at varying intervals. This leads to the problems of\nineffective temporal information encoding and forgetting of past interactions\nfor a pair of nodes that interact intermittently for their link prediction.\nExisting methods, however, do not consider such heterogeneity in their learning\nprocess, and thus their learned temporal node embeddings are less effective,\nespecially when predicting the links for infrequently interacting node pairs.\nTo cope with the heterogeneity, we propose a novel framework called TAMI, which\ncontains two effective components, namely log time encoding function (LTE) and\nlink history aggregation (LHA). LTE better encodes the temporal information\nthrough transforming interaction intervals into more balanced ones, and LHA\nprevents the historical interactions for each target node pair from being\nforgotten. State-of-the-art temporal graph neural networks can be seamlessly\nand readily integrated into TAMI to improve their effectiveness. Experiment\nresults on 13 classic datasets and three newest temporal graph benchmark (TGB)\ndatasets show that TAMI consistently improves the link prediction performance\nof the underlying models in both transductive and inductive settings. Our code\nis available at https://github.com/Alleinx/TAMI_temporal_graph.", "AI": {"tldr": "TAMI\u662f\u4e00\u4e2a\u5904\u7406\u65f6\u5e8f\u56fe\u4e2d\u5f02\u6784\u6027\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u7f16\u7801\u51fd\u6570\u548c\u94fe\u63a5\u5386\u53f2\u805a\u5408\u7ec4\u4ef6\uff0c\u6709\u6548\u63d0\u5347\u65f6\u5e8f\u56fe\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u65f6\u5e8f\u56fe\u4e2d\u7684\u4ea4\u4e92\u5b58\u5728\u5f02\u6784\u6027\uff0c\u5982\u5c11\u6570\u8282\u70b9\u5bf9\u4ea7\u751f\u5927\u90e8\u5206\u4ea4\u4e92\u4e8b\u4ef6\u3001\u4ea4\u4e92\u95f4\u9694\u65f6\u95f4\u4e0d\u5747\u5300\u7b49\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u6d4b\u4e0d\u9891\u7e41\u4ea4\u4e92\u8282\u70b9\u5bf9\u7684\u94fe\u63a5\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTAMI\u6846\u67b6\uff0c\u5305\u542b\u5bf9\u6570\u65f6\u95f4\u7f16\u7801\u51fd\u6570\uff08LTE\uff09\u548c\u94fe\u63a5\u5386\u53f2\u805a\u5408\uff08LHA\uff09\u4e24\u4e2a\u7ec4\u4ef6\u3002LTE\u901a\u8fc7\u53d8\u6362\u4ea4\u4e92\u95f4\u9694\u65f6\u95f4\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u65f6\u95f4\u7f16\u7801\uff0cLHA\u9632\u6b62\u76ee\u6807\u8282\u70b9\u5bf9\u7684\u5386\u53f2\u4ea4\u4e92\u88ab\u9057\u5fd8\u3002", "result": "\u572813\u4e2a\u7ecf\u5178\u6570\u636e\u96c6\u548c3\u4e2a\u6700\u65b0\u65f6\u5e8f\u56fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTAMI\u5728\u8f6c\u5bfc\u548c\u5f52\u7eb3\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u5e95\u5c42\u6a21\u578b\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "TAMI\u80fd\u6709\u6548\u5904\u7406\u65f6\u5e8f\u56fe\u4e2d\u7684\u5f02\u6784\u6027\u95ee\u9898\uff0c\u63d0\u5347\u94fe\u63a5\u9884\u6d4b\u6548\u679c\uff0c\u4e14\u80fd\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc\u65e0\u7f1d\u96c6\u6210\u3002"}}
