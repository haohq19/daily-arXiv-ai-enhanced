{"id": "2511.03912", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u7684\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u6269\u5c55\u6b63\u5e38\u6837\u672c\u96c6\u6765\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u65e0\u9700\u5f02\u5e38\u6807\u7b7e\u6216\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u4e13\u5bb6\u76d1\u7763\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u5f02\u5e38\u6807\u7b7e\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u4e3b\u5e72\u7f51\u7edc\uff0c\u6dfb\u52a0\u5c0f\u578b\u5377\u79ef\u9002\u914d\u5668\u8fdb\u884c\u5feb\u901f\u9886\u57df\u9002\u5e94\u3002\u901a\u8fc7k-NN\u5f02\u5e38\u8bc4\u5206\u548c\u53cc\u6982\u7387\u95e8\u63a7\u673a\u5236\uff08\u8ddd\u79bbz-score\u9608\u503c\u548cSWAG\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff09\u5b89\u5168\u5730\u589e\u91cf\u6269\u5c55\u6b63\u5e38\u6837\u672c\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1aCOVID-CXR\u7684ROC-AUC\u4ece0.9489\u63d0\u5347\u52300.9982\uff0cPneumonia CXR\u4ece0.6834\u52300.8968\uff0cBrain MRI ND-5\u4ece0.6041\u52300.7269\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6807\u7b7e\u7a00\u7f3a\u7684\u533b\u5b66\u6210\u50cf\u5e94\u7528\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u4e0d\u65ad\u4f18\u5316\u6b63\u5e38\u6027\u6982\u5ff5\u3002"}}
{"id": "2511.04256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04256", "abs": "https://arxiv.org/abs/2511.04256", "authors": ["Kun Yang", "Zikang chen", "Yanmeng Wang", "Zhigen Li"], "title": "SSPO: Subsentence-level Policy Optimization", "comment": null, "summary": "As a significant part of post-training of the Large Language Models (LLMs),\nReinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'\nreasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative\nPolicy Optimization) and GSPO (Group Sequence Policy Optimization), are\nobserved to suffer from unstable policy updates and low usage of sampling data,\nrespectively. The importance ratio of GRPO is calculated at the token level,\nwhich focuses more on optimizing a single token. This will be easily affected\nby outliers, leading to model training collapse. GSPO proposed the calculation\nof the response level importance ratio, which solves the problem of high\nvariance and training noise accumulation in the calculation of the GRPO\nimportance ratio. However, since all the response tokens share a common\nimportance ratio, extreme values can easily raise or lower the overall mean,\nleading to the entire response being mistakenly discarded, resulting in a\ndecrease in the utilization of sampled data. This paper introduces SSPO, which\napplies sentence-level importance ratio, taking the balance between GRPO and\nGSPO. SSPO not only avoids training collapse and high variance, but also\nprevents the whole response tokens from being abandoned by the clipping\nmechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily\nadjust the clipping bounds, encouraging high-entropy tokens to explore and\nnarrow the clipping range of low-entropy tokens. In particular, SSPO achieves\nan average score of 46.57 across five datasets, surpassing GRPO (43.01) and\nGSPO (44.42), and wins state-of-the-art performance on three datasets. These\nresults highlight SSPO's effectiveness in leveraging generated data by taking\nthe essence of GSPO but rejecting its shortcomings.", "AI": {"tldr": "SSPO\u63d0\u51fa\u53e5\u5b50\u7ea7\u522b\u7684\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u5e73\u8861\u4e86GRPO\u548cGSPO\u7684\u4f18\u7f3a\u70b9\uff0c\u907f\u514d\u8bad\u7ec3\u5d29\u6e83\u548c\u4f4e\u6570\u636e\u5229\u7528\u7387\u95ee\u9898\uff0c\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u5f97\u520646.57\uff0c\u4f18\u4e8eGRPO(43.01)\u548cGSPO(44.42)\u3002", "motivation": "\u73b0\u6709RLVR\u7b97\u6cd5\u5982GRPO\u5b58\u5728\u4e0d\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u95ee\u9898\uff0cGSPO\u5b58\u5728\u91c7\u6837\u6570\u636e\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u4e24\u8005\u4f18\u70b9\u7684\u65b0\u65b9\u6cd5\u3002", "method": "SSPO\u91c7\u7528\u53e5\u5b50\u7ea7\u522b\u7684\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u5e76\u5728PPO-CLIP\u4e2d\u5e94\u7528\u53e5\u5b50\u71b5\u6765\u52a8\u6001\u8c03\u6574\u88c1\u526a\u8fb9\u754c\uff0c\u9f13\u52b1\u9ad8\u71b5token\u63a2\u7d22\uff0c\u9650\u5236\u4f4e\u71b5token\u7684\u88c1\u526a\u8303\u56f4\u3002", "result": "SSPO\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u5f97\u520646.57\uff0c\u8d85\u8d8aGRPO(43.01)\u548cGSPO(44.42)\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SSPO\u901a\u8fc7\u91c7\u7528GSPO\u7684\u4f18\u70b9\u4f46\u907f\u514d\u5176\u7f3a\u70b9\uff0c\u6709\u6548\u5229\u7528\u4e86\u751f\u6210\u6570\u636e\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.03986", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.03986", "abs": "https://arxiv.org/abs/2511.03986", "authors": ["Ahmed A. Metwally", "Heyjun Park", "Yue Wu", "Tracey McLaughlin", "Michael P. Snyder"], "title": "Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes", "comment": "18 pages, 8 figures", "summary": "The classification of diabetes and prediabetes by static glucose thresholds\nobscures the pathophysiological dysglycemia heterogeneity, primarily driven by\ninsulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This\nreview demonstrates that continuous glucose monitoring and wearable\ntechnologies enable a paradigm shift towards non-invasive, dynamic metabolic\nphenotyping. We show evidence that machine learning models can leverage\nhigh-resolution glucose data from at-home, CGM-enabled oral glucose tolerance\ntests to accurately predict gold-standard measures of muscle IR and beta-cell\nfunction. This personalized characterization extends to real-world nutrition,\nwhere an individual's unique postprandial glycemic response (PPGR) to\nstandardized meals, such as the relative glucose spike to potatoes versus\ngrapes, could serve as a biomarker for their metabolic subtype. Moreover,\nintegrating wearable data reveals that habitual diet, sleep, and physical\nactivity patterns, particularly their timing, are uniquely associated with\nspecific metabolic dysfunctions, informing precision lifestyle interventions.\nThe efficacy of dietary mitigators in attenuating PPGR is also shown to be\nphenotype-dependent. Collectively, this evidence demonstrates that CGM can\ndeconstruct the complexity of early dysglycemia into distinct, actionable\nsubphenotypes. This approach moves beyond simple glycemic control, paving the\nway for targeted nutritional, behavioral, and pharmacological strategies\ntailored to an individual's core metabolic defects, thereby paving the way for\na new era of precision diabetes prevention.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u548c\u53ef\u7a7f\u6234\u6280\u672f\u5982\u4f55\u5b9e\u73b0\u4ece\u9759\u6001\u8840\u7cd6\u9608\u503c\u5411\u52a8\u6001\u4ee3\u8c22\u8868\u578b\u5206\u6790\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u80f0\u5c9b\u7d20\u62b5\u6297\u548c\u03b2\u7ec6\u80de\u529f\u80fd\uff0c\u4e3a\u7cbe\u51c6\u7cd6\u5c3f\u75c5\u9884\u9632\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "motivation": "\u4f20\u7edf\u7cd6\u5c3f\u75c5\u548c\u524d\u9a71\u7cd6\u5c3f\u75c5\u7684\u5206\u7c7b\u57fa\u4e8e\u9759\u6001\u8840\u7cd6\u9608\u503c\uff0c\u63a9\u76d6\u4e86\u7531\u80f0\u5c9b\u7d20\u62b5\u6297\u3001\u03b2\u7ec6\u80de\u529f\u80fd\u969c\u788d\u548c\u80a0\u4fc3\u80f0\u5c9b\u7d20\u7f3a\u4e4f\u9a71\u52a8\u7684\u75c5\u7406\u751f\u7406\u5f02\u8d28\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u4ee3\u8c22\u8868\u578b\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u548c\u53ef\u7a7f\u6234\u6280\u672f\u6536\u96c6\u9ad8\u5206\u8fa8\u7387\u8840\u7cd6\u6570\u636e\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u808c\u8089\u80f0\u5c9b\u7d20\u62b5\u6297\u548c\u03b2\u7ec6\u80de\u529f\u80fd\uff0c\u5206\u6790\u4e2a\u4f53\u5bf9\u6807\u51c6\u5316\u9910\u98df\u7684\u9910\u540e\u8840\u7cd6\u53cd\u5e94\uff0c\u5e76\u6574\u5408\u996e\u98df\u3001\u7761\u7720\u548c\u4f53\u529b\u6d3b\u52a8\u6a21\u5f0f\u6570\u636e\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u91d1\u6807\u51c6\u80f0\u5c9b\u7d20\u62b5\u6297\u548c\u03b2\u7ec6\u80de\u529f\u80fd\u6d4b\u91cf\u503c\uff1b\u4e2a\u4f53\u9910\u540e\u8840\u7cd6\u53cd\u5e94\u53ef\u4f5c\u4e3a\u4ee3\u8c22\u4e9a\u578b\u751f\u7269\u6807\u5fd7\u7269\uff1b\u751f\u6d3b\u4e60\u60ef\u6a21\u5f0f\u4e0e\u7279\u5b9a\u4ee3\u8c22\u529f\u80fd\u969c\u788d\u76f8\u5173\uff1b\u996e\u98df\u7f13\u89e3\u5242\u6548\u679c\u5177\u6709\u8868\u578b\u4f9d\u8d56\u6027\u3002", "conclusion": "\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u53ef\u5c06\u65e9\u671f\u8840\u7cd6\u5f02\u5e38\u89e3\u6784\u4e3a\u53ef\u64cd\u4f5c\u7684\u4e9a\u8868\u578b\uff0c\u8d85\u8d8a\u7b80\u5355\u8840\u7cd6\u63a7\u5236\uff0c\u4e3a\u9488\u5bf9\u4e2a\u4f53\u6838\u5fc3\u4ee3\u8c22\u7f3a\u9677\u7684\u7cbe\u51c6\u8425\u517b\u3001\u884c\u4e3a\u548c\u836f\u7269\u7b56\u7565\u94fa\u5e73\u9053\u8def\uff0c\u5f00\u542f\u7cbe\u51c6\u7cd6\u5c3f\u75c5\u9884\u9632\u65b0\u65f6\u4ee3\u3002"}}
{"id": "2511.04002", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04002", "abs": "https://arxiv.org/abs/2511.04002", "authors": ["Mingyu Sung", "Vikas Palakonda", "Suhwan Im", "Sunghwan Moon", "Il-Min Kim", "Sangseok Yun", "Jae-Mo Kang"], "title": "Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing", "comment": null, "summary": "Large language models (LLMs) have achieved near-human performance across\ndiverse reasoning tasks, yet their deployment on resource-constrained\nInternet-of-Things (IoT) devices remains impractical due to massive parameter\nfootprints and memory-intensive autoregressive decoding. While split computing\noffers a promising solution by partitioning model execution between edge\ndevices and cloud servers, existing approaches fail to address the unique\nchallenges of autoregressive inference, particularly the iterative token\ngeneration process and expanding key-value (KV) cache requirements. This work\nintroduces the first autoregressive-aware split computing framework designed\nexplicitly for LLM deployment on edge devices. Our approach makes three key\ncontributions. First, we develop one-point split compression (OPSC), a\nmixed-precision quantization scheme that prevents out-of-memory failures by\nstrategically partitioning models into front-end and back-end segments with\ndifferent precision levels. Second, we propose a two-stage intermediate\ncompression pipeline that combines threshold splitting (TS) and token-wise\nadaptive bit quantization (TAB-Q) to preserve accuracy-critical activations\nwhile dramatically reducing communication overhead. Third, we formulate a\nunified optimization framework that jointly selects optimal split points,\nquantization settings, and sequence lengths to satisfy strict memory and\nlatency constraints. Extensive evaluations across diverse LLMs and hardware\nplatforms demonstrate superior performance compared to state-of-the-art\nquantization methods, including SmoothQuant, OmniQuant, and Atom. The framework\nachieves a 1.49 inference speedup and significant communication overhead\nreduction while maintaining or improving model accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907LLM\u90e8\u7f72\u7684\u81ea\u56de\u5f52\u611f\u77e5\u5206\u5272\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u3001\u4e2d\u95f4\u538b\u7f29\u548c\u8054\u5408\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u548c\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u81ea\u56de\u5f52\u63a8\u7406\u7684\u8fed\u4ee3token\u751f\u6210\u8fc7\u7a0b\u548c\u4e0d\u65ad\u6269\u5c55\u7684KV\u7f13\u5b58\u9700\u6c42\u3002", "method": "1. \u5355\u70b9\u5206\u5272\u538b\u7f29(OPSC)\uff1a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\uff1b2. \u4e24\u9636\u6bb5\u4e2d\u95f4\u538b\u7f29\u7ba1\u9053\uff1a\u9608\u503c\u5206\u5272(TS)\u548ctoken\u81ea\u9002\u5e94\u4f4d\u91cf\u5316(TAB-Q)\uff1b3. \u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff1a\u8054\u5408\u9009\u62e9\u6700\u4f18\u5206\u5272\u70b9\u3001\u91cf\u5316\u8bbe\u7f6e\u548c\u5e8f\u5217\u957f\u5ea6\u3002", "result": "\u76f8\u6bd4SmoothQuant\u3001OmniQuant\u548cAtom\u7b49\u6700\u5148\u8fdb\u91cf\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e861.49\u500d\u63a8\u7406\u52a0\u901f\u548c\u663e\u8457\u901a\u4fe1\u5f00\u9500\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6027\u80fd\u3001\u5185\u5b58\u548c\u901a\u4fe1\u9700\u6c42\u3002"}}
{"id": "2511.04538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04538", "abs": "https://arxiv.org/abs/2511.04538", "authors": ["Cyril Vallez", "Alexander Sternfeld", "Andrei Kucharavy", "Ljiljana Dolamic"], "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting", "comment": null, "summary": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates.", "AI": {"tldr": "\u6700\u65b0\u5f00\u6e90LLM\u4ee3\u7801\u52a9\u624b\u5728\u73b0\u5b9e\u4f7f\u7528\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u65e9\u671f\u62a5\u544a\u7684\u6f0f\u6d1e\uff0c\u5b89\u5168-\u529f\u80fd\u6743\u8861\u963b\u788d\u4e86\u6709\u6548\u4fee\u590d\u3002\u4f5c\u8005\u63d0\u51faPrompt Exposure\u548cModel Exposure\u6307\u6807\u6765\u8bc4\u4f30LLM\u751f\u6210\u6f0f\u6d1e\u7684\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7801\u52a9\u624b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u5176\u751f\u6210\u7684\u6f0f\u6d1e\u5bf9\u7f51\u7edc\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u548c\u6539\u8fdb\u65b9\u6cd5\u5bf9\u4e3b\u6d41LLM\u7684\u5b9e\u9645\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165Prompt Exposure(PE)\u6307\u6807\uff0c\u7efc\u5408\u8003\u8651\u6f0f\u6d1e\u4e25\u91cd\u6027\u3001\u751f\u6210\u6982\u7387\u548c\u8bf1\u5bfc\u6f0f\u6d1e\u7684\u63d0\u793a\u8868\u8ff0\uff1b\u57fa\u4e8ePE\u5b9a\u4e49Model Exposure(ME)\u8bc4\u5206\uff0c\u8861\u91cf\u6a21\u578b\u751f\u6210\u6f0f\u6d1e\u7684\u4e25\u91cd\u6027\u548c\u666e\u904d\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u662f\u6700\u65b0\u5f00\u6e90\u6a21\u578b\u5728\u73b0\u5b9e\u4f7f\u7528\u573a\u666f\u4e2d\u4ecd\u6613\u53d7\u65e9\u671f\u62a5\u544a\u7684\u6f0f\u6d1e\u653b\u51fb\uff0c\u8868\u660e\u5b89\u5168-\u529f\u80fd\u6743\u8861\u963b\u788d\u4e86\u6f0f\u6d1e\u7684\u6709\u6548\u4fee\u590d\u3002", "conclusion": "\u9700\u8981\u65b0\u7684\u4e25\u91cd\u6027\u5ea6\u91cf\u6807\u51c6\u6765\u4f18\u5148\u7f13\u89e3\u6700\u4e25\u91cd\u548c\u666e\u904d\u7684\u6f0f\u6d1e\uff0cPE\u548cME\u6307\u6807\u6709\u52a9\u4e8e\u63a8\u52a8LLM\u4ee3\u7801\u52a9\u624b\u7684\u5b89\u5168\u6027\u6539\u8fdb\u3002"}}
{"id": "2511.04334", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04334", "abs": "https://arxiv.org/abs/2511.04334", "authors": ["Sa\u00fal Alonso-Monsalve", "Leigh H. Whitehead", "Adam Aurisano", "Lorena Escudero Sanchez"], "title": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography", "comment": "12 pages, 5 figures", "summary": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f53\u7d20\u7a00\u758f\u5316\u548c\u5b50\u6d41\u5f62\u7a00\u758f\u5377\u79ef\u7f51\u7edc\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66CT\u56fe\u50cf\u4e2d\u80bf\u7624\u7684\u81ea\u52a8\u5206\u5272\uff0c\u5728\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u653e\u5c04\u56fe\u50cf\u4e2d\u80bf\u7624\u7684\u7cbe\u786e\u5206\u5272\u662f\u4e13\u4e1a\u4e14\u8017\u65f6\u7684\u4efb\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e3D\u626b\u63cf\u7684\u5927\u8ba1\u7b97\u91cf\u800c\u9700\u8981\u964d\u91c7\u6837\u6216\u5206\u5757\u5904\u7406\uff0c\u8fd9\u9650\u5236\u4e86\u5b9a\u91cf\u5206\u6790\u5728\u4e34\u5e8a\u4e2d\u7684\u5e38\u89c4\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u4f53\u7d20\u7a00\u758f\u5316\u548c\u5b50\u6d41\u5f62\u7a00\u758f\u5377\u79ef\u7f51\u7edc\uff0c\u5141\u8bb8\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u539f\u751f3D\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728KiTS23\u6311\u6218\u8d5b\u7684\u80be\u764cCT\u56fe\u50cf\u4e0a\uff0c\u83b7\u5f97\u4e86\u4e0e\u4f18\u80dc\u8005\u76f8\u5f53\u7684\u7ed3\u679c\uff1a\u80be\u810f+\u80bf\u5757Dice\u7cfb\u657095.8%\uff0c\u80bf\u7624+\u56ca\u80bf85.7%\uff0c\u5355\u72ec\u80bf\u762480.3%\u3002\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1160%\uff0cVRAM\u4f7f\u7528\u51cf\u5c1175%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u5206\u5272\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u4e34\u5e8a\u5e38\u89c4\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04147", "abs": "https://arxiv.org/abs/2511.04147", "authors": ["Jiaming Zhang", "Yujie Yang", "Haoning Wang", "Liping Zhang", "Shengbo Eben Li"], "title": "Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning", "comment": "Submitted to the Journal of Machine Learning Research (JMLR), under\n  review", "summary": "Safe reinforcement learning (safe RL) aims to respect safety requirements\nwhile optimizing long-term performance. In many practical applications,\nhowever, the problem involves an infinite number of constraints, known as\nsemi-infinite safe RL (SI-safe RL). Such constraints typically appear when\nsafety conditions must be enforced across an entire continuous parameter space,\nsuch as ensuring adequate resource distribution at every spatial location. In\nthis paper, we propose exchange policy optimization (EPO), an algorithmic\nframework that achieves optimal policy performance and deterministic bounded\nsafety. EPO works by iteratively solving safe RL subproblems with finite\nconstraint sets and adaptively adjusting the active set through constraint\nexpansion and deletion. At each iteration, constraints with violations\nexceeding the predefined tolerance are added to refine the policy, while those\nwith zero Lagrange multipliers are removed after the policy update. This\nexchange rule prevents uncontrolled growth of the working set and supports\neffective policy training. Our theoretical analysis demonstrates that, under\nmild assumptions, strategies trained via EPO achieve performance comparable to\noptimal solutions with global constraint violations strictly remaining within a\nprescribed bound.", "AI": {"tldr": "\u63d0\u51fa\u4ea4\u6362\u7b56\u7565\u4f18\u5316(EPO)\u6846\u67b6\uff0c\u89e3\u51b3\u534a\u65e0\u9650\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574\u7ea6\u675f\u96c6\u5b9e\u73b0\u6700\u4f18\u7b56\u7565\u6027\u80fd\u548c\u6709\u754c\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5904\u7406\u6709\u9650\u7ea6\u675f\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u9762\u4e34\u65e0\u9650\u7ea6\u675f\u95ee\u9898\uff0c\u5982\u9700\u8981\u5728\u8fde\u7eed\u53c2\u6570\u7a7a\u95f4\u4e0a\u786e\u4fdd\u5b89\u5168\u6761\u4ef6\u3002", "method": "EPO\u901a\u8fc7\u8fed\u4ee3\u6c42\u89e3\u6709\u9650\u7ea6\u675f\u96c6\u7684\u5b89\u5168RL\u5b50\u95ee\u9898\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6d3b\u8dc3\u7ea6\u675f\u96c6\uff1a\u6dfb\u52a0\u8fdd\u53cd\u7ea6\u675f\uff0c\u5220\u9664\u96f6\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u7ea6\u675f\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\uff0cEPO\u8bad\u7ec3\u7684\u7b56\u7565\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u4e14\u5168\u5c40\u7ea6\u675f\u8fdd\u53cd\u4e25\u683c\u4fdd\u6301\u5728\u9884\u5b9a\u754c\u9650\u5185\u3002", "conclusion": "EPO\u4e3a\u534a\u65e0\u9650\u5b89\u5168RL\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u4fdd\u8bc1\u7b56\u7565\u6027\u80fd\u548c\u786e\u5b9a\u6027\u6709\u754c\u5b89\u5168\u3002"}}
{"id": "2511.04158", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04158", "abs": "https://arxiv.org/abs/2511.04158", "authors": ["Anzhuo Xie", "Wei-Chen Chang"], "title": "Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data", "comment": null, "summary": "This study proposes a Transformer-based longitudinal modeling method to\naddress challenges in clinical risk classification with heterogeneous\nElectronic Health Record (EHR) data, including irregular temporal patterns,\nlarge modality differences, and complex semantic structures. The method takes\nmulti-source medical features as input and employs a feature embedding layer to\nachieve a unified representation of structured and unstructured data. A\nlearnable temporal encoding mechanism is introduced to capture dynamic\nevolution under uneven sampling intervals. The core model adopts a multi-head\nself-attention structure to perform global dependency modeling on longitudinal\nsequences, enabling the aggregation of long-term trends and short-term\nfluctuations across different temporal scales. To enhance semantic\nrepresentation, a semantic-weighted pooling module is designed to assign\nadaptive importance to key medical events, improving the discriminative ability\nof risk-related features. Finally, a linear mapping layer generates\nindividual-level risk scores. Experimental results show that the proposed model\noutperforms traditional machine learning and temporal deep learning models in\naccuracy, recall, precision, and F1-Score, achieving stable and precise risk\nidentification in multi-source heterogeneous EHR environments and providing an\nefficient and reliable framework for clinical intelligent decision-making.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u7eb5\u5411\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u4e2d\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u5f02\u6784\u6027\u3001\u4e0d\u89c4\u5219\u65f6\u95f4\u6a21\u5f0f\u548c\u590d\u6742\u8bed\u4e49\u7ed3\u6784\u95ee\u9898\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e2d\u7684\u5f02\u6784\u6027\u6311\u6218\uff0c\u5305\u62ec\u4e0d\u89c4\u5219\u65f6\u95f4\u6a21\u5f0f\u3001\u5927\u6a21\u6001\u5dee\u5f02\u548c\u590d\u6742\u8bed\u4e49\u7ed3\u6784\uff0c\u4ee5\u63d0\u5347\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u7279\u5f81\u5d4c\u5165\u5c42\u7edf\u4e00\u8868\u793a\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u7f16\u7801\u673a\u5236\u5904\u7406\u4e0d\u5747\u5300\u91c7\u6837\u95f4\u9694\uff0c\u91c7\u7528\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7ed3\u6784\u8fdb\u884c\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\uff0c\u8bbe\u8ba1\u8bed\u4e49\u52a0\u6743\u6c60\u5316\u6a21\u5757\u81ea\u9002\u5e94\u5206\u914d\u5173\u952e\u533b\u7597\u4e8b\u4ef6\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728\u51c6\u786e\u6027\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u65f6\u5e8f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u591a\u6e90\u5f02\u6784EHR\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7cbe\u786e\u7684\u98ce\u9669\u8bc6\u522b\u3002", "conclusion": "\u4e3a\u4e34\u5e8a\u667a\u80fd\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u53ef\u9760\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6e90\u5f02\u6784\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u63d0\u5347\u98ce\u9669\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2511.04670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04670", "abs": "https://arxiv.org/abs/2511.04670", "authors": ["Shusheng Yang", "Jihan Yang", "Pinzhi Huang", "Ellis Brown", "Zihao Yang", "Yue Yu", "Shengbang Tong", "Zihan Zheng", "Yifan Xu", "Muhan Wang", "Daohan Lu", "Rob Fergus", "Yann LeCun", "Li Fei-Fei", "Saining Xie"], "title": "Cambrian-S: Towards Spatial Supersensing in Video", "comment": "Website: https://cambrian-mllm.github.io/", "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u8d85\u611f\u77e5\u7684\u6982\u5ff5\uff0c\u8ba4\u4e3a\u771f\u6b63\u7684\u591a\u6a21\u6001\u667a\u80fd\u9700\u8981\u4ece\u4efb\u52a1\u9a71\u52a8\u7cfb\u7edf\u8f6c\u5411\u80fd\u591f\u611f\u77e5\u3001\u8bb0\u5fc6\u3001\u63a8\u7406\u548c\u9884\u6d4b\u4e16\u754c\u7684\u6a21\u578b\u3002\u5f00\u53d1\u4e86VSI-SUPER\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4ec5\u9760\u6570\u636e\u6269\u5c55\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u8d85\u611f\u77e5\uff0c\u800c\u9884\u6d4b\u6027\u611f\u77e5\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u4e3b\u8981\u505c\u7559\u5728\u53cd\u5e94\u5f0f\u4efb\u52a1\u9a71\u52a8\u9636\u6bb5\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u8ba4\u77e5\u548c\u4e16\u754c\u5efa\u6a21\u7684\u6df1\u5ea6\u7406\u89e3\u3002\u9700\u8981\u4ece\u5355\u7eaf\u7684\u8bed\u8a00\u7406\u89e3\u8f6c\u5411\u66f4\u5168\u9762\u7684\u7a7a\u95f4\u8d85\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u8d85\u611f\u77e5\u7684\u56db\u4e2a\u9636\u6bb5\uff1a\u8bed\u4e49\u611f\u77e5\u3001\u6d41\u5f0f\u4e8b\u4ef6\u8ba4\u77e5\u3001\u9690\u5f0f3D\u7a7a\u95f4\u8ba4\u77e5\u548c\u9884\u6d4b\u6027\u4e16\u754c\u5efa\u6a21\u3002\u5f00\u53d1\u4e86VSI-SUPER\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542bVSR\u548cVSC\u4efb\u52a1\uff09\uff0c\u5e76\u8bad\u7ec3\u4e86Cambrian-S\u6a21\u578b\u3002\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u7684\u9884\u6d4b\u6027\u611f\u77e5\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u6269\u5c55\uff08VSI-590K\uff09\u4f7fVSI-Bench\u6027\u80fd\u63d0\u534730%\uff0c\u4f46\u5728VSI-SUPER\u4e0a\u8868\u73b0\u4ecd\u7136\u6709\u9650\u3002\u9884\u6d4b\u6027\u611f\u77e5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u9884\u6d4b\u80fd\u529b\u5bf9\u7a7a\u95f4\u8d85\u611f\u77e5\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4ec5\u9760\u6570\u636e\u6269\u5c55\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u7a7a\u95f4\u8d85\u611f\u77e5\u3002\u9884\u6d4b\u6027\u611f\u77e5\uff08\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u9a71\u52a8\u8bb0\u5fc6\u548c\u4e8b\u4ef6\u5206\u5272\uff09\u662f\u66f4\u6709\u6548\u7684\u8def\u5f84\uff0c\u9700\u8981\u6a21\u578b\u4e0d\u4ec5\u80fd\u770b\uff0c\u8fd8\u8981\u80fd\u9884\u6d4b\u3001\u9009\u62e9\u548c\u7ec4\u7ec7\u7ecf\u9a8c\u3002"}}
{"id": "2511.04557", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04557", "abs": "https://arxiv.org/abs/2511.04557", "authors": ["Divyansha Lachi", "Mahmoud Mohammadi", "Joe Meyer", "Vinam Arora", "Tom Palczewski", "Eva L. Dyer"], "title": "Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning", "comment": null, "summary": "In domains such as healthcare, finance, and e-commerce, the temporal dynamics\nof relational data emerge from complex interactions-such as those between\npatients and providers, or users and products across diverse categories. To be\nbroadly useful, models operating on these data must integrate long-range\nspatial and temporal dependencies across diverse types of entities, while also\nsupporting multiple predictive tasks. However, existing graph models for\nrelational data primarily focus on spatial structure, treating temporal\ninformation merely as a filtering constraint to exclude future events rather\nthan a modeling signal, and are typically designed for single-task prediction.\nTo address these gaps, we introduce a temporal subgraph sampler that enhances\nglobal context by retrieving nodes beyond the immediate neighborhood to capture\ntemporally relevant relationships. In addition, we propose the Relational Graph\nPerceiver (RGP), a graph transformer architecture for relational deep learning\nthat leverages a cross-attention-based latent bottleneck to efficiently\nintegrate information from both structural and temporal contexts. This latent\nbottleneck integrates signals from different node and edge types into a common\nlatent space, enabling the model to build global context across the entire\nrelational system. RGP also incorporates a flexible cross-attention decoder\nthat supports joint learning across tasks with disjoint label spaces within a\nsingle model. Experiments on RelBench, SALT, and CTU show that RGP delivers\nstate-of-the-art performance, offering a general and scalable solution for\nrelational deep learning with support for diverse predictive tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Relational Graph Perceiver (RGP)\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u5b50\u56fe\u91c7\u6837\u5668\u548c\u8de8\u6ce8\u610f\u529b\u6f5c\u5728\u74f6\u9888\u6765\u6574\u5408\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u652f\u6301\u591a\u4efb\u52a1\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u7ed3\u6784\uff0c\u5c06\u65f6\u95f4\u4fe1\u606f\u4ec5\u4f5c\u4e3a\u8fc7\u6ee4\u7ea6\u675f\u800c\u975e\u5efa\u6a21\u4fe1\u53f7\uff0c\u4e14\u901a\u5e38\u53ea\u652f\u6301\u5355\u4efb\u52a1\u9884\u6d4b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u533b\u7597\u3001\u91d1\u878d\u7b49\u9886\u57df\u7684\u590d\u6742\u65f6\u7a7a\u4ea4\u4e92\u548c\u591a\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u65f6\u95f4\u5b50\u56fe\u91c7\u6837\u5668\u6765\u6355\u83b7\u65f6\u95f4\u76f8\u5173\u5173\u7cfb\uff0c\u5e76\u63d0\u51faRGP\u56fe\u53d8\u6362\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u8de8\u6ce8\u610f\u529b\u6f5c\u5728\u74f6\u9888\u6574\u5408\u7ed3\u6784\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u89e3\u7801\u5668\u652f\u6301\u591a\u4efb\u52a1\u8054\u5408\u5b66\u4e60\u3002", "result": "\u5728RelBench\u3001SALT\u548cCTU\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRGP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "RGP\u901a\u8fc7\u6709\u6548\u6574\u5408\u65f6\u7a7a\u4e0a\u4e0b\u6587\u548c\u8de8\u4efb\u52a1\u5b66\u4e60\uff0c\u4e3a\u590d\u6742\u5173\u7cfb\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.04653", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04653", "abs": "https://arxiv.org/abs/2511.04653", "authors": ["Xinlu Zhang", "Yansha Deng", "Toktam Mahmoodi"], "title": "TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning", "comment": null, "summary": "Federated learning (FL) offers new opportunities in machine learning,\nparticularly in addressing data privacy concerns. In contrast to conventional\nevent-based federated learning, time-triggered federated learning (TT-Fed), as\na general form of both asynchronous and synchronous FL, clusters users into\ndifferent tiers based on fixed time intervals. However, the FL network consists\nof a growing number of user devices with limited wireless bandwidth,\nconsequently magnifying issues such as stragglers and communication overhead.\nIn this paper, we introduce adaptive model pruning to wireless TT-Fed systems\nand study the problem of jointly optimizing the pruning ratio and bandwidth\nallocation to minimize the training loss while ensuring minimal learning\nlatency. To answer this question, we perform convergence analysis on the\ngradient l_2 norm of the TT-Fed model based on model pruning. Based on the\nobtained convergence upper bound, a joint optimization problem of pruning ratio\nand wireless bandwidth is formulated to minimize the model training loss under\na given delay threshold. Then, we derive closed-form solutions for wireless\nbandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The\nsimulation results show that model pruning could reduce the communication cost\nby 40% while maintaining the model performance at the same level.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u65e0\u7ebf\u65f6\u95f4\u89e6\u53d1\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u6a21\u578b\u526a\u679d\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u526a\u679d\u7387\u548c\u5e26\u5bbd\u5206\u914d\u6765\u6700\u5c0f\u5316\u8bad\u7ec3\u635f\u5931\u5e76\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7f51\u7edc\u4e2d\u7684\u7528\u6237\u8bbe\u5907\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\u4e14\u65e0\u7ebf\u5e26\u5bbd\u6709\u9650\uff0c\u5bfc\u81f4\u8bad\u7ec3\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u526a\u679d\u5bf9TT-Fed\u6a21\u578b\u7684\u68af\u5ea6l2\u8303\u6570\u8fdb\u884c\u6536\u655b\u5206\u6790\uff0c\u5229\u7528KKT\u6761\u4ef6\u63a8\u5bfc\u526a\u679d\u7387\u548c\u5e26\u5bbd\u5206\u914d\u7684\u95ed\u5f0f\u89e3\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u526a\u679d\u53ef\u5c06\u901a\u4fe1\u6210\u672c\u964d\u4f4e40%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u4e0d\u53d8\u3002", "conclusion": "\u81ea\u9002\u5e94\u6a21\u578b\u526a\u679d\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u5ef6\u8fdf\u95ee\u9898\u3002"}}
{"id": "2511.04659", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.04659", "abs": "https://arxiv.org/abs/2511.04659", "authors": ["Huaguan Chen", "Wei Han", "Haofei Sun", "Ning Lin", "Xingtao Song", "Yunfan Yang", "Jie Tian", "Yang Liu", "Ji-Rong Wen", "Xiaoye Zhang", "Xueshun Shen", "Hao Sun"], "title": "Nowcast3D: Reliable precipitation nowcasting via gray-box learning", "comment": null, "summary": "Extreme precipitation nowcasting demands high spatiotemporal fidelity and\nextended lead times, yet existing approaches remain limited. Numerical Weather\nPrediction (NWP) and its deep-learning emulations are too slow and coarse for\nrapidly evolving convection, while extrapolation and purely data-driven models\nsuffer from error accumulation and excessive smoothing. Hybrid 2D radar-based\nmethods discard crucial vertical information, preventing accurate\nreconstruction of height-dependent dynamics. We introduce a gray-box, fully\nthree-dimensional nowcasting framework that directly processes volumetric radar\nreflectivity and couples physically constrained neural operators with\ndatadriven learning. The model learns vertically varying 3D advection fields\nunder a conservative advection operator, parameterizes spatially varying\ndiffusion, and introduces a Brownian-motion--inspired stochastic term to\nrepresent unresolved motions. A residual branch captures small-scale convective\ninitiation and microphysical variability, while a diffusion-based stochastic\nmodule estimates uncertainty. The framework achieves more accurate forecasts up\nto three-hour lead time across precipitation regimes and ranked first in 57\\%\nof cases in a blind evaluation by 160 meteorologists. By restoring full 3D\ndynamics with physical consistency, it offers a scalable and robust pathway for\nskillful and reliable nowcasting of extreme precipitation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7070\u76d2\u4e09\u7ef4\u4e34\u8fd1\u9884\u62a5\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u795e\u7ecf\u7f51\u7edc\u548c\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u76f4\u63a5\u5904\u7406\u4e09\u7ef4\u96f7\u8fbe\u53cd\u5c04\u7387\u6570\u636e\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u6781\u7aef\u964d\u6c34\u9884\u62a5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u6570\u503c\u5929\u6c14\u9884\u62a5\u53ca\u5176\u6df1\u5ea6\u5b66\u4e60\u6a21\u62df\u901f\u5ea6\u6162\u3001\u5206\u8fa8\u7387\u4f4e\uff1b\u5916\u63a8\u6cd5\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff1b\u4e8c\u7ef4\u96f7\u8fbe\u65b9\u6cd5\u4e22\u5f03\u4e86\u5173\u952e\u7684\u5782\u76f4\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u4e09\u7ef4\u96f7\u8fbe\u53cd\u5c04\u7387\u6570\u636e\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5782\u76f4\u53d8\u5316\u76843D\u5e73\u6d41\u573a\uff0c\u53c2\u6570\u5316\u7a7a\u95f4\u53d8\u5316\u7684\u6269\u6563\uff0c\u5f15\u5165\u5e03\u6717\u8fd0\u52a8\u542f\u53d1\u7684\u968f\u673a\u9879\u8868\u793a\u672a\u89e3\u6790\u8fd0\u52a8\uff0c\u901a\u8fc7\u6b8b\u5dee\u5206\u652f\u6355\u6349\u5c0f\u5c3a\u5ea6\u5bf9\u6d41\u542f\u52a8\u548c\u5fae\u7269\u7406\u53d8\u5316\u3002", "result": "\u5728\u957f\u8fbe\u4e09\u5c0f\u65f6\u7684\u9884\u62a5\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u964d\u6c34\u9884\u62a5\uff0c\u5728160\u540d\u6c14\u8c61\u5b66\u5bb6\u7684\u76f2\u8bc4\u4e2d57%\u7684\u60c5\u51b5\u4e0b\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u901a\u8fc7\u6062\u590d\u5b8c\u6574\u7684\u4e09\u7ef4\u52a8\u529b\u5b66\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4e3a\u6781\u7aef\u964d\u6c34\u7684\u51c6\u786e\u53ef\u9760\u4e34\u8fd1\u9884\u62a5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u9014\u5f84\u3002"}}
