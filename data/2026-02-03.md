<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 13]
- [cs.LG](#cs.LG) [Total: 31]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CL](#cs.CL) [Total: 9]
- [cs.RO](#cs.RO) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 提出一种基于事件相机数据的异步事件驱动聚类算法，用于实时检测小事件簇，具有线性复杂度O(n)且运行时间与像素阵列维度无关。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生异步事件流数据，传统聚类算法在处理这种数据时效率不高，需要一种能够实时处理事件相机数据、检测小事件簇的高效算法。

Method: 采用异步事件驱动的层次凝聚聚类算法，利用事件相机的特殊异步数据结构，通过精妙、高效且简单的决策机制，基于事件的时间-空间距离检测事件簇。

Result: 算法实现了线性复杂度O(n)，其中n为事件数量，且运行时间与像素阵列维度无关，能够实时检测小事件簇。

Conclusion: 该算法为事件相机数据处理提供了一种高效、实时的聚类解决方案，特别适合需要快速响应的小事件簇检测应用场景。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [2] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: D3R-Net：一种用于无监督异常检测的双域去噪重建框架，通过自监督"修复"任务和频域感知正则化，改善高频细节重建，提升缺陷分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的无监督异常检测方法存在过度平滑问题，对高频细节重建不佳，导致细微缺陷被部分重建而非突出显示，限制了分割精度。

Method: 提出D3R-Net双域去噪重建框架：1）使用合成损坏的正常图像进行自监督"修复"训练，防止恒等映射；2）结合空间MSE损失和FFT幅度损失进行频域一致性约束；3）可选SSIM损失；4）采用轻量卷积自编码器骨干网络。

Result: 在MVTec AD Hazelnut基准上，FFT损失将PRO AUC从0.603提升至0.687；在15个MVTec类别上，平均像素ROC AUC从0.733提升至0.751，PRO AUC从0.417提升至0.468，单GPU上约20FPS。

Conclusion: D3R-Net通过双域重建策略有效改善高频细节处理，提升异常定位一致性，为工业视觉检测提供了一种实用、轻量且高效的替代方案。

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [3] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 提出一种推理时方法，通过识别并移除不稳定的潜在token来缓解自回归视频生成中的时间漂移问题，无需修改模型架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成在生成长视频时存在严重的时间漂移问题，错误会随着时间累积和放大。作者认为这主要不是模型容量不足导致的，而是推理时的错误传播问题，特别是由于在自回归推理中重复使用了已损坏的潜在条件token。

Method: 提出简单的推理时方法：在自回归推理过程中，识别那些表示与先前生成批次显著偏离的不稳定潜在token（表明可能已损坏或发生语义漂移），并在将这些token重新用于条件生成之前将其移除。通过从自回归上下文中显式移除损坏的潜在token，而不是修改整个空间区域或模型参数，防止不可靠的潜在信息影响未来的生成步骤。

Result: 该方法显著改善了长时域的时间一致性，同时无需修改模型架构、训练过程或离开潜在空间。

Conclusion: 时间漂移问题主要源于推理时的错误传播而非模型容量不足，通过推理时识别和移除不稳定的潜在token可以有效缓解这一问题，提高自回归视频生成的长时域一致性。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [4] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind是一个诊断性基准测试，专门评估多模态大语言模型在细粒度时空理解上的能力，通过视频对对比揭示模型依赖静态视觉线索而非真正的时间逻辑。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在静态语义理解上表现出色，但对时间动态的理解仍然薄弱。需要专门的诊断工具来评估模型是否真正理解时间逻辑，而不是仅仅依赖静态视觉线索。

Method: 采用最小对范式：创建视频对，这些视频对具有完全相同的静态视觉内容，但时间结构不同。通过互补问题设计来消除语言先验。将细粒度时间理解分为三个层次：原子事件识别、事件属性表征、事件相互依赖推理。

Result: 评估了20多个最先进的MLLM（包括GPT-5、Gemini 3 Pro）在600个实例（2400个视频-问题对）上的表现。最佳模型的实例准确率仅为48.2%，远低于人类表现（98.2%）。这表明前沿模型严重依赖静态视觉捷径而非真正的时间逻辑。

Conclusion: TimeBlind揭示了当前MLLM在时间理解上的根本缺陷，为下一代视频理解模型提供了重要的诊断工具。数据集和代码已开源。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [5] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Text-DJ是一种针对大型视觉语言模型的新型越狱攻击，通过将有害查询分解为多个良性子查询并添加大量无关干扰查询，以图像网格形式绕过模型的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型的安全防护主要针对显式文本输入或相关视觉场景，但忽略了模型OCR能力的潜在漏洞。本文旨在探索如何利用OCR功能绕过这些安全防护。

Method: 采用三阶段方法：1) 将单个有害查询分解为多个语义相关但更良性的子查询；2) 选择与有害查询最大程度无关的干扰查询；3) 将所有子查询和干扰查询以图像网格形式同时呈现给模型，其中子查询位于网格中间位置。

Result: 该方法成功绕过了最先进大型视觉语言模型的安全对齐机制，暴露了OCR能力在面对分散的多图像对抗输入时的脆弱性。

Conclusion: 研究揭示了大型视觉语言模型OCR能力的关键漏洞，表明现有安全协议无法有效处理碎片化的多模态输入，强调了需要针对此类攻击开发专门的防御机制。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [6] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: RGBX-R1框架通过UAV提示策略构建视觉模态思维链，采用两阶段训练增强MLLM对红外、深度等X模态的感知推理能力，在RGBX-Grounding基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要基于RGB模态预训练，限制了在红外、深度、事件数据等其他视觉模态上的性能，而这些模态对复杂场景理解至关重要。

Method: 提出RGBX-R1框架：1) 使用理解-关联-验证(UAV)提示策略构建视觉模态思维链(VM-CoT)；2) 采用两阶段训练：冷启动监督微调(CS-SFT)和时空强化微调(ST-RFT)，后者使用模态理解时空奖励(MuST)。

Result: 构建首个RGBX-Grounding基准，在三个RGBX grounding任务上比基线方法提升22.71%，在多模态理解和空间感知方面表现优越。

Conclusion: RGBX-R1框架有效扩展了MLLM对多种视觉模态的感知推理能力，为解决复杂场景中的多模态理解问题提供了有效方案。

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [7] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: 提出了OmniVCHall基准来评估视频多模态大语言模型的组合幻觉问题，并开发了TriCD对比解码框架来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频幻觉缓解研究主要关注孤立错误类型，而由多个交互时空因素错误推理产生的组合幻觉问题尚未得到充分探索。

Method: 1) 构建OmniVCHall基准，涵盖多样化视频领域，引入新的基于摄像头的幻觉类型，定义细粒度分类法，并包含对抗性答案选项；2) 提出TriCD对比解码框架，包含三重路径校准机制，通过自适应扰动控制器构建负样本视频变体，使用显著性引导增强模块强化视觉证据，并通过强化学习优化。

Result: 评估了39个代表性VLLM，发现即使先进模型（如Qwen3-VL和GPT-5）也表现出显著性能下降。TriCD在两个代表性骨干模型上一致提升性能，平均准确率提升超过10%。

Conclusion: 组合幻觉是视频多模态大语言模型的重要挑战，OmniVCHall基准为系统评估提供了工具，TriCD框架通过对比解码和校准机制有效缓解了这一问题。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [8] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 提出EMFormer架构和累积上下文微调方法，通过多尺度特征提取和动态损失平衡，显著提升长期天气预报准确性并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 长期天气预报对社会经济规划和灾害准备至关重要，但现有方法存在灾难性遗忘、误差累积和高训练开销等问题，需要新的解决方案。

Method: 1) 提出高效多尺度Transformer（EMFormer），通过单卷积提取多尺度特征；2) 采用累积上下文微调提升时间一致性；3) 设计正弦加权动态平衡的复合损失函数。

Result: 在天气预报和极端事件预测中表现优异，显著提升长期预测准确性，在视觉基准测试（ImageNet-1K和ADE20K）上展示强泛化能力，相比传统多尺度模块实现5.69倍加速。

Conclusion: 提出的跨预训练、微调和预测的完整管道有效解决了长期天气预报中的关键挑战，在保持准确性的同时大幅降低计算成本，具有广泛应用前景。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [9] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: PaPE是一种基于抛物线的位置编码方法，专门为视觉模态设计，在8个数据集上7个取得最佳性能，在ImageNet-1K上外推能力显著提升10.5%


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法主要从语言处理的1D序列扩展到视觉的nD结构，但未能充分考虑视觉模态的特性，需要设计更符合视觉特性的位置编码方法

Method: 提出抛物线位置编码(PaPE)，基于平移不变性、旋转不变性(PaPE-RI)、距离衰减、方向性和上下文感知等原则设计，适用于图像、点云、视频和事件相机等多种视觉模态

Result: 在涵盖4种模态的8个数据集上，PaPE或PaPE-RI在7个数据集上取得最佳性能；在ImageNet-1K外推实验中，PaPE比次优位置编码绝对提升达10.5%

Conclusion: PaPE是一种有效的视觉位置编码方法，能更好地适应视觉模态特性，具有优异的外推能力和广泛的适用性

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [10] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 提出Logit Lens Loss (LLL)來解決視覺語言模型中視覺token資訊擴散問題，使Logit Lens可視化能產生有意義的物件信心圖，並提升視覺任務表現。


<details>
  <summary>Details</summary>
Motivation: Logit Lens在視覺語言模型中可視化圖像token的概念內容，但視覺資訊常擴散到語言token，破壞局部性，使可視化無法用於解釋性。

Method: 提出Logit Lens Loss (LLL)作為下一個token預測的補充損失，使視覺token嵌入與描述其圖像區域的文本概念語義對齊，無需架構修改或大規模訓練。

Result: LLL不僅使Logit Lens能產生有意義的物件信心圖，還提升了分割等視覺中心任務的表現，無需附加特殊頭部。

Conclusion: Logit Lens Loss有效解決視覺token資訊擴散問題，增強了視覺語言模型的可解釋性和視覺任務表現。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [11] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 提出一个联邦学习框架，结合动态自适应焦点损失和客户端感知聚合策略，解决医疗图像分类中的数据异质性和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在数据隐私法规限制下，医疗图像等敏感数据难以集中获取。联邦学习虽能保护隐私，但面临客户端数据异质性和类别不平衡的挑战，影响模型泛化能力。

Method: 1. 动态自适应焦点损失(DAFL)：根据客户端样本分布和类别数据分布动态调整类别不平衡系数，确保少数类得到足够关注；2. 客户端感知加权聚合策略：根据数据规模和特征自适应调整权重，更好捕捉客户端间差异。

Result: 在ISIC、Ocular Disease和RSNA-ICH三个公开数据集上，该框架在大多数情况下优于DenseNet121、ResNet50、ViT-S/16、ViT-L/32、FedCLIP、Swin Transformer、CoAtNet和MixNet，准确率提升0.98%到41.69%。在ISIC数据集上的消融实验验证了损失函数和聚合策略的有效性。

Conclusion: 提出的联邦学习框架通过动态自适应焦点损失和客户端感知聚合策略，有效解决了医疗图像分类中的数据异质性和类别不平衡问题，显著提升了模型性能。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [12] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA：一种基于视觉的基因组建模框架，将DNA序列视为OCR风格的文档理解，通过视觉DNA编码器和文档解码器实现高效压缩，在长序列任务上以更少的token和参数取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型大多采用语言模型架构，将DNA视为一维token序列，但这种顺序读取方式与基因组稀疏、不连续的语义结构不匹配，导致在低信息背景上浪费计算资源，且难以实现理解驱动的长上下文压缩。

Method: 将DNA序列渲染为结构化视觉布局，训练OCR能力的视觉-语言模型，包含视觉DNA编码器（生成可重构的紧凑视觉token）和文档解码器。定义了基于核心基因组原语的提示条件目标：读取、区域定位、子序列检索和掩码跨度补全。

Result: 在多样基因组基准测试中，OpticalDNA始终优于近期基线；在长达45万个碱基的序列上，以近20倍更少的有效token获得最佳整体性能，且仅需256k可训练参数即可超越激活参数多达985倍的模型。

Conclusion: OpticalDNA通过视觉文档理解范式重新构建基因组建模，实现了布局感知的DNA表示，在减少有效token预算的同时保留细粒度基因组信息，为长序列基因组分析提供了高效的计算框架。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [13] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 提出Causal Forcing方法，使用自回归教师进行ODE初始化，解决双向视频扩散模型蒸馏到自回归模型时的架构差距问题，显著提升实时交互视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练的双向视频扩散模型蒸馏为少步自回归模型时存在架构差距，使用双向教师进行ODE初始化违反帧级单射条件，导致性能下降。

Method: 提出Causal Forcing方法，使用自回归教师进行ODE初始化，从而桥接架构差距，确保满足帧级单射条件，恢复教师流映射。

Result: 在所有指标上超越所有基线方法，相比SOTA Self Forcing在Dynamic Degree上提升19.3%，VisionReward上提升8.7%，Instruction Following上提升16.7%。

Conclusion: 通过使用自回归教师进行ODE初始化，Causal Forcing有效解决了双向视频扩散模型蒸馏到自回归模型时的理论问题，显著提升了实时交互视频生成的性能。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 论文比较了多种机器学习方法用于抵押贷款违约预测，重点解决了标签模糊、类别不平衡和信息泄漏三个实际问题，发现AutoGluon在控制泄漏和处理不平衡后表现最佳。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但实际数据中存在三个主要问题：违约标签定义模糊、严重的类别不平衡、以及由时间结构和事后变量引起的信息泄漏，这些问题影响了评估有效性和部署可靠性。

Method: 采用泄漏感知的特征选择、严格的时间分割（限制贷款发放和报告时期）、以及控制性的多数类下采样。比较了多种机器学习方法，特别关注泄漏控制和类别不平衡处理。

Result: 在不同正负样本比例下，模型性能保持稳定，AutoGluon在所有评估模型中取得了最强的AUROC表现。

Conclusion: 通过严格的泄漏控制和适当的类别不平衡处理，机器学习模型（特别是AutoML方法如AutoGluon）能够有效预测抵押贷款违约，为金融风险管理提供可靠工具。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [15] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 论文认为局部线性解释方法（如LIME和SHAP）在决策边界附近的不稳定性反映了预测不确定性高的问题，而非方法缺陷。正确做法是先评估预测是否可用，只有在低不确定性区域才寻求解释，否则应使用更简单的模型。


<details>
  <summary>Details</summary>
Motivation: 在关键决策应用中，可解释性是主要关注点，但现有局部线性解释方法因在决策边界附近不稳定而受到批评。作者认为这种批评反映了对问题的误解，需要重新思考解释性评估的顺序。

Method: 提出新的解释性评估框架：首先评估预测的不确定性，只有当预测具有足够低的可用不确定性时，才通过局部线性近似寻求解释。当没有可用预测时，决策应回退到更简单的整体模型（如传统逻辑回归）。

Result: 论证了局部线性解释方法的不稳定性与预测不确定性相关，这是合理的而非缺陷。指出某些声称处处可解释的方法（如ReLU网络或任何分段线性模型）实际上只有虚幻的可解释性，因为在分段边界处的预测不确定性太高而无法使用。

Conclusion: 解释性评估应遵循正确顺序：先判断预测是否可用，再寻求解释。解释一个不可用的预测毫无意义。对于高不确定性区域，应使用更简单的模型进行决策。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [16] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT是一个自动合成、校准、验证和优化流行病学模拟器的智能体框架，通过将疾病进展建模为迭代程序合成问题，显著加速有效模型的收敛。


<details>
  <summary>Details</summary>
Motivation: 传统流行病建模方法依赖固定模型类别，需要随着病原体、政策和场景假设的变化而手动重新设计，这限制了建模的灵活性和效率。

Method: 引入流行病流图中间表示，将场景规范与模型结构连接，支持模块化正确性检查；通过迭代程序合成方法自动生成经过验证的机制模型，在物理和流行病学约束下进行可解释参数学习。

Result: EPIAGENT能够捕捉复杂的增长动态，在不同疫苗接种和免疫逃逸假设下产生流行病学一致的反事实预测；智能体反馈循环防止模型退化，显著加速向有效模型的收敛。

Conclusion: EPIAGENT框架通过模拟专业专家工作流程，实现了流行病学模拟器的自动合成和优化，为公共卫生规划提供了更灵活高效的建模工具。

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [17] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 提出量子生成核(QGK)，通过变分生成组(VGG)构建可参数化的量子核，解决NISQ设备数据嵌入限制，在图像分类任务中超越现有量子与经典核方法。


<details>
  <summary>Details</summary>
Motivation: 量子核方法理论上能将经典不可分特征在量子空间中分离，但NISQ硬件限制需要有效压缩和嵌入大规模真实数据（如图像）。现有混合架构的固定嵌入过程可能阻碍发挥量子计算全部潜力。

Method: 提出量子生成核(QGK)，包含变分生成组(VGGs)，将通用生成器合并为可参数化算子，实现量子空间的规模化覆盖。通过训练权重向量参数化VGG在当前数据上下文中的投影，优化核与目标域的对齐。

Result: 实证结果显示QGK在投影和分类能力上优于最先进的量子与经典核方法，展示了其作为多种QML应用的通用框架潜力。

Conclusion: QGK通过生成器方法解决了量子核在NISQ设备上的数据嵌入限制，提供了优于现有方法的性能，为量子机器学习应用提供了灵活框架。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [18] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: OMatG-IRL：基于推理时强化学习的晶体材料生成框架，无需显式计算得分函数，通过扰动生成动力学实现目标属性强化，在晶体结构预测中实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型可用于晶体材料设计，但难以将目标属性显式融入生成过程。基于策略梯度的强化学习能对齐生成模型与下游目标，但通常需要得分函数，这限制了其在仅学习速度场的流模型中的应用。

Method: 提出OMatG-IRL框架，直接在学习的速度场上操作，无需显式计算得分函数。通过随机扰动底层生成动力学，在保持预训练生成模型基线性能的同时，实现推理时的探索和策略梯度估计。

Result: 首次将强化学习应用于晶体结构预测，能有效强化基于能量的目标，同时通过成分条件保持多样性。性能与基于得分的强化学习方法相当，并能学习时间依赖的速度退火调度，实现采样效率数量级提升和生成时间大幅减少。

Conclusion: OMatG-IRL为晶体材料生成提供了有效的推理时强化学习框架，无需得分函数，在保持多样性的同时实现目标属性强化，显著提升了晶体结构预测的效率和准确性。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [19] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: NEST：一种用于多集序列事件流的层次化Transformer模型，通过保持原始层次结构提高计算效率和表示质量


<details>
  <summary>Details</summary>
Motivation: 现有事件流基础模型将层次结构扁平化为单维序列，导致计算效率低下（密集注意力）和学习虚假的集合内关系，同时通过启发式后训练池化得到的集合级表示质量较低

Method: 提出NEST（Nested Event Stream Transformer），一种用于多集序列事件流的基础模型，保持原始层次结构；并引入Masked Set Modeling（MSM）范式，促进更好的集合级表示学习

Result: 在真实世界多集序列数据上的实验表明，NEST能够捕捉真实世界动态，同时提高预训练效率和下游任务性能

Conclusion: 在基础模型架构中保持事件流的原始层次结构提供了有用的归纳偏置，既能提高计算效率又能改善表示质量

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [20] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: PolicyFlow：一种基于连续归一化流（CNF）的强化学习算法，通过近似重要性比率避免完整流轨迹的似然评估，结合布朗正则化器防止模式崩溃，在多种任务中表现优于高斯策略PPO。


<details>
  <summary>Details</summary>
Motivation: PPO算法在处理高斯分布策略时表现良好，但扩展到表达能力更强的连续归一化流（CNF）策略时面临挑战，因为完整流轨迹的似然评估计算成本高且数值不稳定。

Method: 提出PolicyFlow算法：1）通过速度场变化沿简单插值路径近似重要性比率，避免完整流轨迹的似然评估；2）引入布朗正则化器，这是一种受布朗运动启发的隐式策略熵正则化器，防止模式崩溃并鼓励行为多样性。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多种环境的任务中，PolicyFlow相比使用高斯策略的PPO以及FPO、DPPO等流基基线方法，取得了竞争性或更优的性能。在MultiGoal任务中特别展示了捕获更丰富多模态动作分布的能力。

Conclusion: PolicyFlow成功地将表达能力强的CNF策略与PPO风格目标相结合，通过创新的重要性比率近似和布朗正则化器，解决了传统方法中的计算和稳定性问题，为强化学习中的复杂策略建模提供了有效解决方案。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [21] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: ORA：一种新的电子健康记录基础模型预训练目标，联合建模事件时间和相关测量值，相比传统方法能产生更通用的表示


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据具有不规则采样、混合离散事件和数值测量的特点。现有的基于下一个令牌预测的预训练方法无法捕捉EHR的完整结构，限制了模型的泛化能力和下游任务表现。

Method: 提出ORA（标记时间到事件）预训练目标，联合建模事件时间和相关连续测量值。该方法考虑了EHR的时间结构和数值特征，超越了传统的下一个令牌预测方法。

Result: 在多个数据集、下游任务和模型架构上，ORA目标始终比下一个令牌预测和忽略连续测量的预训练损失产生更通用的表示。改进不仅体现在传统分类评估，还包括更好的回归和时间到事件预测性能。

Conclusion: ORA不仅引入了一个新的基础模型家族，更重要的是表明：考虑EHR结构的预训练目标对于扩展下游能力和提高泛化性至关重要。这为EHR基础模型的发展提供了新的方向。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [22] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: DisRFM：一种几何感知的图域自适应框架，通过黎曼嵌入和基于流的传输解决结构退化和优化不稳定问题


<details>
  <summary>Details</summary>
Motivation: 传统图域自适应方法在欧几里得空间中使用对抗学习对齐图嵌入，面临两个关键挑战：1）结构退化问题 - 层次和语义表示纠缠；2）优化不稳定问题 - 最小最大对抗训练的振荡动态

Method: 1）将图嵌入黎曼流形，使用极坐标显式解耦结构（半径）和语义（角度）；2）通过径向Wasserstein对齐保持拓扑结构，通过角度聚类实现语义区分；3）使用黎曼流匹配学习平滑向量场，沿测地线路径引导源特征向目标移动

Result: 理论证明流匹配的渐近稳定性，并推导出更紧的目标风险边界。大量实验表明DisRFM始终优于最先进方法

Conclusion: DisRFM通过几何感知的统一框架有效解决了图域自适应中的结构退化和优化不稳定问题，实现了更好的域适应性能

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [23] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 通过识别并约束导致模型在微调中出现不良行为的内部特征，可以有效减少95%的"涌现错位"现象，且不影响目标任务性能。


<details>
  <summary>Details</summary>
Motivation: 当语言模型在狭窄范围的监督目标上进行微调时，虽然学会了目标行为，但也会发展出不良的域外行为（涌现错位）。需要找到一种机制性方法来防止这种问题。

Method: 识别控制错位行为的少量内部特征，然后在微调过程中阻止模型强化这些特征。通过特征阻断（约束）技术，在六个微调领域进行实验验证。

Result: 阻断固定特征集可实现高达95%的相对错位减少，且不降低模型质量或目标任务性能。研究还发现长时间微调会导致错位重新出现，表明存在通过替代特征或层的重新路由。

Conclusion: 针对内部机制的针对性训练时约束可以有效减轻涌现错位，同时保持目标任务性能。这为模型安全对齐提供了有前景的机制性方法。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [24] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: ChronoSpike是一种自适应脉冲图神经网络，通过可学习的LIF神经元、多头注意力空间聚合和轻量级Transformer时间编码器，在保持线性内存复杂度的同时，实现了动态图表示学习的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法面临基本权衡：注意力方法表达能力强但复杂度高（O(T²)），循环架构存在梯度问题和密集状态存储问题。脉冲神经网络虽然具有事件驱动效率，但受限于序列传播、二进制信息丢失和局部聚合无法捕获全局上下文。

Method: 提出ChronoSpike，整合了：1）具有每通道膜动力学的可学习LIF神经元；2）在连续特征上的多头注意力空间聚合；3）轻量级Transformer时间编码器。该方法实现了细粒度局部建模和长距离依赖捕获，具有线性内存复杂度O(T·d)。

Result: 在三个大规模基准测试中，ChronoSpike在12个最先进基线方法上取得了2.0% Macro-F1和2.4% Micro-F1的提升，训练速度比循环方法快3-10倍，参数预算恒定105K（与图大小无关）。理论分析证明了膜电位有界性、收缩因子ρ<1下的梯度流稳定性以及BIBO稳定性。

Conclusion: ChronoSpike通过创新的自适应脉冲图神经网络架构，解决了动态图表示学习中的效率与表达能力权衡问题，实现了线性复杂度下的最优性能，同时提供了理论保证和可解释性分析（显示异质时间感受野和83-88%稀疏性）。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [25] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: TS-DPO：在切线空间中执行DPO，学习每个目标的更新方向，可在推理时线性组合以实现用户指定的行为平衡


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将反馈压缩为单一标量奖励，固定了目标间的平衡，无法遍历帕累托前沿。需要让大语言模型能够平衡多个人类偏好维度（如帮助性、安全性、简洁性），实现可控对齐。

Method: 基于切线空间理论，将微调视为在模型切线空间中的操作，线性化更新作为可组合的加法向量。提出切线空间直接偏好优化（TS-DPO），在局部线性机制中执行DPO，学习每个目标的更新方向。这些方向可在推理时线性组合，无需额外优化。

Result: 在HelpSteer和UltraFeedback数据集上评估帮助性-简洁性权衡，TS-DPO比标量化DPO实现了更广泛的帕累托最优覆盖和更平滑的偏好控制。典型相关分析（CCA）显示切线空间训练放大了与不同偏好对齐的典型方向，改善了分离性。

Conclusion: TS-DPO通过切线空间中的偏好优化，实现了对多个人类偏好维度的可控对齐，允许在推理时灵活调整目标平衡，为语言模型对齐提供了更精细的控制机制。

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [26] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: TRACE：利用自回归模型作为预训练密度估计器，从单条离散事件序列中推断事件类型间的因果图，支持延迟因果效应，可线性扩展到大规模词汇表。


<details>
  <summary>Details</summary>
Motivation: 从车辆日志、制造系统或患者轨迹等单条离散事件序列中发现因果关系具有挑战性，因为缺乏重复样本、高维度和长程时间依赖。

Method: TRACE框架将自回归模型重新用作预训练密度估计器进行条件互信息估计，推断事件类型间的摘要因果图，支持延迟因果效应，在GPU上完全并行化。

Result: 在理论可识别性证明下，实验显示TRACE在不同基线和词汇量下表现稳健，包括在超过29,100个事件类型的车辆诊断根因分析中的应用。

Conclusion: TRACE为从单条高维离散事件序列中进行可扩展因果发现提供了有效框架，具有理论保证和实际应用价值。

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [27] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE是一个结合变分自编码器和对比学习的深度生存分析模型，在保持预测性能的同时实现可解释的风险分层。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要生存分析来估计时间到事件结果、分层患者风险并指导治疗计划。深度学习在该领域表现出色但面临性能与可解释性的权衡：神经网络预测准确但黑箱性质限制临床采用，而可解释的聚类方法通常牺牲预测能力。

Method: CONVERSE结合变分自编码器和对比学习，使用变分嵌入和多种簇内簇间对比损失。采用自步学习从易到难逐步纳入样本以提高训练稳定性，支持簇特定生存头实现准确的集成预测。

Result: 在四个基准数据集上的综合评估表明，CONVERSE相比现有深度生存方法实现了竞争性或更优的性能，同时保持了有意义的患者分层。

Conclusion: CONVERSE通过统一变分自编码器和对比学习，在深度生存分析中成功平衡了预测性能与可解释性，为临床决策提供了既准确又可解释的风险分层工具。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [28] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文提出了一种将推测采样与水印技术相结合的新方法，通过向草稿令牌接受过程中注入伪随机性，在保持推测采样效率的同时实现最大水印强度。


<details>
  <summary>Details</summary>
Motivation: 水印技术是追踪大语言模型输出的有效方法，但在实际部署中受到推理效率低下的阻碍。推测采样能加速推理，但现有研究表明水印强度与推测采样效率之间存在根本性权衡：更高的水印强度会降低草稿模型的接受率，阻碍两者同时实现。

Method: 1. 引入量化水印强度的度量标准，该标准控制统计可检测性，并在令牌是伪随机数的确定性函数时达到最大。2. 将权衡问题形式化为约束优化问题，为两种现有水印方案推导出明确的帕累托曲线。3. 提出一种原则性机制，向草稿令牌接受过程注入伪随机性，确保最大水印强度同时保持推测采样效率。

Result: 实验表明该方法在不牺牲效率的情况下提高了可检测性。该方法揭示了推测采样与水印技术可以统一的原则，为两者的高效实用部署铺平了道路。

Conclusion: 研究表明水印强度与推测采样效率之间的权衡并非绝对，通过向草稿令牌接受过程注入伪随机性的原则性机制，可以在保持推测采样效率的同时实现最大水印强度，为水印技术的实际部署提供了高效解决方案。

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [29] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文提出Transformer Q-Learning (TQL)方法，通过控制注意力分数熵来稳定训练，解决了Transformer在强化学习价值函数缩放中的注意力崩溃问题，实现了从最小到最大网络规模43%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管规模在机器学习中驱动了显著进步，但强化学习方法仍主要使用小型价值函数。直接缩放价值函数（包括使用已知高度可扩展的Transformer架构）通常会导致学习不稳定和性能下降。本文旨在探究Transformer在价值函数缩放中失效的原因。

Method: 通过实证分析识别出缩放中的关键失效模式：随着容量增加，注意力分数崩溃。提出Transformer Q-Learning (TQL)方法，通过控制注意力分数的熵来有效防止这种崩溃并稳定训练，从而能够使用更大的模型。

Result: TQL方法在从最小到最大网络规模的缩放中实现了高达43%的性能提升，而先前方法则遭受性能下降的问题。

Conclusion: 通过控制注意力分数熵，Transformer Q-Learning成功解锁了Transformer在强化学习价值函数学习中的缩放潜力，解决了注意力崩溃问题，实现了稳定的大规模价值函数训练。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [30] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 论文提出RDMReg正则化方法，通过匹配Rectified Generalized Gaussian分布来学习稀疏、非负表示，改进JEPA架构


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用各向同性高斯分布正则化，倾向于产生密集表示，无法捕捉高效表示中观察到的稀疏性关键特性

Method: 引入Rectified Distribution Matching Regularization (RDMReg)，一种切片双样本分布匹配损失，将表示对齐到Rectified Generalized Gaussian (RGG)分布。RGG通过整流实现期望ℓ₀范数的显式控制，同时在期望ℓₚ范数约束下保持最大熵

Result: Rectified LpJEPA学习到稀疏、非负表示，在稀疏性-性能权衡方面表现良好，在图像分类基准测试中具有竞争力的下游性能

Conclusion: RDMReg能有效强制稀疏性同时保留任务相关信息，Rectified LpJEPA严格泛化了先前基于高斯的JEPA方法

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [31] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 论文提出MSign优化器，通过定期应用矩阵符号操作恢复稳定秩，有效防止LLM预训练中的梯度爆炸问题，计算开销小于7.0%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练中的训练不稳定性是一个关键挑战，表现为突发的梯度爆炸，浪费大量计算资源。研究旨在理解并解决这种不稳定性问题。

Method: 通过μP缩放的5M参数NanoGPT模型研究训练失败，发现崩溃前的两个关键现象：权重矩阵稳定秩快速下降和相邻层雅可比矩阵对齐度增加。提出MSign优化器，定期应用矩阵符号操作来恢复稳定秩。

Result: 理论证明这两个条件共同导致梯度范数随网络深度指数增长。在5M到3B参数的模型实验中，MSign能有效防止训练失败，计算开销小于7.0%。

Conclusion: MSign优化器通过打破训练不稳定性机制，为大语言模型预训练提供了一种有效的稳定性解决方案，显著减少计算资源浪费。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [32] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: CoMeT是一种新型Transformer架构，通过双内存系统和分块处理实现线性时间复杂度和恒定内存使用，能够处理任意长序列。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和不断增长的KV缓存是处理长上下文的主要障碍，需要一种能够高效处理任意长序列的解决方案。

Method: CoMeT采用双内存系统：FIFO队列的临时内存处理近期事件，带门控更新规则的全局内存处理长程依赖。这些内存作为动态软提示用于下一数据块，并引入层级流水线并行策略进行高效微调。

Result: CoMeT在32k上下文微调后，能在100万token序列中准确检索任意位置的密码。在SCROLLS基准测试中超越其他高效方法，在摘要任务上达到与全注意力基线相当的性能，在实际智能体和用户行为QA任务中验证了有效性。

Conclusion: CoMeT通过创新的双内存架构和高效微调策略，成功解决了Transformer处理长序列的内存和时间复杂度问题，为实际应用中的长上下文处理提供了可行方案。

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [33] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出一种数据高效的深度学习框架，使用稀疏双通道sEMG传感器实现精确的肌电假肢控制，通过可学习时间嵌入和归一化融合策略，在10类动作识别上达到95.7%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统高密度多传感器阵列成本高且复杂，限制了肌电假肢的普及。需要开发能够在最小化传感器硬件条件下实现精确控制的方法。

Method: 采用混合Transformer架构，使用Time2Vec可学习时间嵌入捕捉生物信号的随机时间扭曲，通过归一化加性融合策略对齐时空特征分布，采用两阶段课程学习协议应对数据稀缺问题。

Result: 在10类动作识别上获得95.7% ± 0.20%的多被试F1分数，优于标准Transformer和CNN-LSTM模型。快速校准协议（每个手势仅需2次试验）可将新被试性能从21.0%提升至96.9%。

Conclusion: 高质量时间嵌入可以补偿低空间分辨率，挑战了高密度传感的必要性。该框架为下一代可快速个性化的假肢接口提供了经济有效的蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [34] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出内部流签名方法，通过监控LLM内部深度动态来检测和定位生成内容中的不忠实问题，无需修改基础模型即可进行自我检查和针对性修正。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能生成与上下文不忠实但流畅的答案，现有方法多依赖外部验证或生成后的独立判断，需要一种基于内部决策动态的自我检查机制。

Method: 引入内部流签名：通过偏置中心监控稳定token级动态，在深度窗口内构建移动读取对齐子空间，使用正交传输对齐相邻窗口帧，提取深度可比较的传输步长、转向角和子空间漂移等特征。训练轻量GRU验证器进行自我检查，并能定位问题深度事件进行针对性修正。

Result: 该方法能够有效检测不忠实生成，定位问题深度事件，并通过回滚到问题token并在识别块处钳制异常传输步长（同时保留正交残差）进行针对性修正，实现低开销的自我检查。

Conclusion: 内部流签名提供了一种从内部决策动态进行可操作定位和低开销自我检查的管道，无需修改基础模型即可实现LLM的自我验证和修正。

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [35] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: FlashTrace：一种高效的多token归因方法，通过跨token聚合和递归归因机制，解决了长上下文归因的效率瓶颈和忠实度下降问题。


<details>
  <summary>Details</summary>
Motivation: 随着现代LLM越来越依赖扩展推理链，现有token归因方法面临两个关键挑战：1）效率瓶颈——归因M个目标token在长度为N的上下文中需要O(M*N)操作，使长上下文归因变得极其缓慢；2）忠实度下降——中间推理token吸收了归因质量，阻止重要性传播回原始输入。

Method: FlashTrace采用跨token聚合技术，在单次计算中计算多token目标的归因，同时保持忠实度。设计了递归归因机制，通过中间推理链追踪重要性回到源输入。

Result: 在长上下文检索（RULER）和多步推理（MATH、MorehopQA）任务上的实验表明，FlashTrace相比现有基线实现了超过130倍的加速，同时保持了优越的忠实度。递归归因分析显示，即使是单次递归跳跃也能通过追踪推理链中的重要性提高忠实度。

Conclusion: FlashTrace通过高效的跨token聚合和递归归因机制，有效解决了长上下文和多步推理场景下的token归因效率和忠实度问题，为语言模型的解释性提供了更实用的解决方案。

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [36] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 提出LLM代理自我演化框架，通过对比反思总结错误模式，并通过自整合机制将文本经验蒸馏为可学习参数，实现长期演化


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统缺乏终身学习能力，主要依赖检索成功轨迹作为演示，但这种方法存在两个关键局限：1) 忽视失败尝试的教学价值；2) 不断积累的文本经验增加检索时间、引入噪声并耗尽上下文窗口

Method: 提出自我演化框架，包含两个互补机制：1) 对比反思策略，明确总结易错模式并捕获可重用见解；2) 自整合机制，将非参数化文本经验蒸馏为紧凑的可学习参数，使代理能够将大量历史经验内化到潜在空间

Result: 大量实验证明该方法在长期代理演化中的优势

Conclusion: 提出的自我演化框架通过对比反思和参数化经验整合，有效解决了现有LLM代理缺乏终身学习能力的问题，实现了更高效的长期演化

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [37] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出一种两阶段优化框架来改进GPTQ分组量化，通过最小化层重建损失来提升大语言模型低比特量化的精度。


<details>
  <summary>Details</summary>
Motivation: GPTQ等现有分组量化方法在确定分组尺度时忽略了输入统计特性和组间相关性，与其最小化层重建损失的目标不匹配，导致精度下降。

Method: 提出两阶段优化框架：第一阶段在GPTQ前初始化每个分组尺度以最小化分组重建损失；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化分组尺度以最小化层重建损失，并考虑前层量化误差防止误差累积。

Result: 实验结果表明该方法能持续提升分组量化效果，以可忽略的开销获得更高的精度。

Conclusion: 通过两阶段优化框架显式最小化层重建损失，有效解决了GPTQ忽略输入统计和组间相关性的问题，提升了分组量化的精度。

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [38] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出一个用于时序链接预测模型的反事实验证框架，通过生成具有已知因果结构的因果时序交互图来评估模型是否捕捉到因果机制。


<details>
  <summary>Details</summary>
Motivation: 当前时序链接预测模型主要基于预测准确性评估，但这种方法无法评估模型是否真正捕捉到控制时序交互的因果机制。需要一种能够验证模型因果理解能力的评估框架。

Method: 1. 提出连续时间事件序列的结构方程模型，支持兴奋和抑制效应；2. 将该机制扩展到时序交互图；3. 提出基于跨模型预测误差的距离度量来比较因果模型；4. 通过控制因果偏移和时间戳重排实现反事实评估。

Result: 经验验证了假设：在一个因果模型上训练的预测器，在足够不同的因果模型上评估时性能会下降。框架为因果感知的基准测试提供了基础。

Conclusion: 该框架能够评估时序链接预测模型是否真正理解因果机制，而不仅仅是模式识别，为因果感知的模型评估提供了系统方法。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [39] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: SurvKAN：基于KAN架构的完全参数化生存分析模型，消除比例风险假设，在保持可解释性的同时实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖比例风险等限制性假设，无法捕捉真实临床动态；深度学习模型（如DeepSurv、DeepHit）虽然表达能力更强但牺牲了可解释性，限制了临床采用；现有混合模型（如CoxKAN）仍受半参数Cox框架约束

Method: 提出SurvKAN，一个基于KAN架构的完全参数化、时间连续的生存模型。将时间作为KAN的显式输入，直接预测对数风险函数，通过端到端训练优化完整生存似然。架构通过可学习的单变量函数保持可解释性，显示个体特征如何随时间影响风险

Result: 在标准生存基准测试中，SurvKAN在一致性指数和校准指标上相比经典方法和最先进基线实现了竞争性或更优的性能。可解释性分析揭示了与医学领域知识一致的临床有意义模式

Conclusion: SurvKAN成功解决了生存分析中性能与可解释性的权衡问题，提供了一个完全参数化的替代方案，消除了比例风险约束，同时保持了临床可接受的透明度和预测准确性

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [40] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: SEDformer：一种基于脉冲神经网络的IMTS预测模型，利用稀疏-事件对偶性特性，通过事件驱动的脉冲编码和压缩机制，在保持高精度的同时显著降低计算能耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图和Transformer的预测方法忽略了不规则多元时间序列(IMTS)的稀疏-事件对偶性(SED)特性：通过填充对齐到均匀网格违反了稀疏性，而关系重构破坏了局部时间连续性。需要一种更忠实于IMTS SED特性的建模范式。

Method: 提出SEDformer：1) SED-based Spike Encoder使用事件对齐LIF神经元将原始观测转换为事件同步脉冲；2) Event-Preserving Temporal Downsampling模块压缩长间隙同时保留显著脉冲；3) 堆叠的SED-based Spike Transformer块使用基于膜电位的线性注意力建模序列内依赖关系。

Result: 在公共IMTS数据集上的实验表明，SEDformer达到了最先进的预测精度，同时显著降低了能耗和内存使用。

Conclusion: SEDformer为IMTS建模提供了一种自然且高效的路径，通过利用脉冲神经网络的事件驱动特性，更好地对齐了IMTS的稀疏-事件对偶性本质。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [41] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: 提出多阶段物理信息训练策略和PhIS-FNO模型，通过渐进式边界条件约束和样条傅里叶神经算子，实现无监督PDE求解，达到接近监督学习的精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子需要监督数据，而物理信息神经网络存在收敛不稳定和泛化能力有限的问题。需要一种既能利用物理约束进行无监督训练，又能稳定收敛并具有良好泛化能力的方法。

Method: 提出多阶段物理信息训练策略：1) 渐进式在损失函数中实施边界条件；2) 随后加入内部残差；3) 每阶段重新初始化优化器作为延续机制。同时提出PhIS-FNO模型，结合傅里叶层和Hermite样条核进行平滑残差评估。

Result: 在标准基准测试中，PhIS-FNO仅使用狭窄边界区域的标注信息，达到了与监督学习相当的精度水平，证明了该方法的有效性。

Conclusion: 分阶段、基于样条的优化为物理信息算子学习提供了一个稳健的范式，能够在无监督条件下实现高质量的PDE求解。

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [42] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: 论文提出"Dyadic Backpropagation"框架，将反向传播解释为物理动力系统的有限时间松弛过程，通过拉格朗日理论在加倍状态空间推导全局能量函数，其鞍点动力学在2L步内精确恢复标准反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播被视为符号计算，但作者希望将其理解为物理动力系统的自然涌现过程，为模拟和神经形态计算提供理论基础。

Method: 将前向推理建模为连续时间过程，应用非保守系统的拉格朗日理论处理非对称交互，在编码激活和敏感度的加倍状态空间构造全局能量函数，分析其鞍点动力学。

Result: 证明单位步长欧拉离散化在2L步内精确恢复标准反向传播，无需对称权重、渐近收敛或微小扰动等近似条件。

Conclusion: 反向传播是连续物理松弛过程的数字优化影子，为模拟和神经形态计算中的精确梯度计算提供了严格理论基础。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [43] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: SLIME是一种新的无参考对齐方法，通过锚定项、稳定惩罚和双边界机制解决DPO方法中的目标不匹配问题，防止高质量输出概率下降和格式崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有直接偏好优化方法虽然计算高效，但存在目标不匹配问题：优化选择与拒绝响应之间的相对边界不能保证保持选择响应的绝对概率，导致"遗忘"（高质量输出概率下降）和"格式崩溃"（拒绝序列过度惩罚）。

Method: 提出SLIME方法，包含三部分目标：(1)锚定项最大化偏好响应的似然；(2)稳定惩罚防止拒绝标记概率崩溃为零；(3)双边界机制结合硬约束和软约束进行精确边界塑造。

Result: SLIME在性能上优于最先进的基线方法，同时保持更高的生成稳定性。

Conclusion: SLIME成功地将偏好学习与生成质量解耦，解决了现有DPO方法中的目标不匹配问题，实现了更稳定和优越的对齐性能。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [44] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: DAIL方法通过两步法利用少量专家解决方案提升LLM推理能力：先将专家解答转化为详细推理轨迹，再用对比学习聚焦专家洞察，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前提升LLM推理能力的方法要么依赖模型能采样正确解用于强化学习，要么需要更强模型解决问题。但许多难题对前沿模型也难解，无法提取有效训练信号。专家解决方案质量高但昂贵，且直接模仿会失败，因为专家解答是为人类设计，包含隐含推理跳跃，与模型分布不匹配。

Method: 提出分布对齐模仿学习(DAIL)：第一步将专家解决方案转化为详细、分布内的推理轨迹，解决分布不匹配问题；第二步应用对比目标，使学习聚焦于专家洞察和方法论。

Result: 仅用不到1000个高质量专家解决方案，在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，推理效率提高2-4倍，并具备跨领域泛化能力。

Conclusion: DAIL方法能有效利用少量专家解决方案提升LLM推理能力，解决了专家数据分布不匹配问题，实现了样本高效的学习，为难以通过传统方法解决的复杂推理问题提供了新途径。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理解释的健康指数和透明规则实现可信预测


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染而性能下降，现有预测维护模型缺乏可解释性，操作人员不信任，需要透明可信的预测方法

Method: 基于模糊相似推理的可解释预测框架：1) 基于跨膜压力、通量和阻力的物理信息健康指数；2) 高斯隶属函数模糊化；3) 相似性度量识别历史退化轨迹；4) Takagi-Sugeno模糊规则制定RUL预测

Result: 在工业规模UF系统的12,528个操作周期上测试，平均绝对误差为4.50个周期，同时生成与专家理解一致的可解释规则库

Conclusion: 该框架为超滤膜剩余使用寿命预测提供了可解释、透明且准确的解决方案，增强了操作人员对预测维护模型的信任

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [46] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 该论文介绍了Maria平台，这是一个用于初级医疗保健的生产级AI系统，通过整合四个工程支柱来实现可信赖的临床AI：清洁架构、事件驱动架构、Agent模块化和人在回路治理。


<details>
  <summary>Details</summary>
Motivation: AI在临床环境中的集成面临软件工程挑战，需要从孤立模型转向健壮、可治理和可靠的系统。当前工业应用常存在脆弱、原型衍生的架构和系统性监督缺失，导致"责任真空"，安全性和问责性受损。

Method: 提出Maria平台作为行业案例研究，采用协同架构：结合清洁架构（可维护性）与事件驱动架构（弹性和可审计性）；引入Agent作为主要模块化单元，每个Agent拥有自主的MLOps生命周期；技术上集成人在回路治理模型作为关键的事件驱动数据源。

Result: Maria平台展示了如何通过四个工程支柱的整合构建可信赖的临床AI系统，提供了一个参考架构，用于在高风险领域构建可维护、可扩展和可问责的AI系统。

Conclusion: 可信赖的临床AI需要通过清洁架构、事件驱动架构、Agent模块化和人在回路治理这四个工程支柱的全面整合来实现。Maria平台为工程师在高风险领域构建可维护、可扩展和可问责的AI系统提供了实用经验。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [47] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 提出首个CAN总线基础模型，将解码后的车辆信号视为语言进行大规模预训练，然后在多种汽车保险任务上微调，实现跨任务泛化


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理方法多为孤立的任务特定模型，缺乏共享表示学习和跨任务泛化能力。受NLP和CV领域基础模型范式成功的启发，希望将这一范式应用于CAN数据领域

Method: 将CAN数据视为语言，提出统一的分词方案处理混合离散-连续信号，解决时间复杂性和行程特定变异性挑战，在大规模未标记解码CAN信号上进行预训练，然后在异构汽车保险任务上微调

Result: 单个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据领域的适用性

Conclusion: 基础模型范式在NLP和CV领域取得成功后，同样适用于CAN数据，为汽车AI中的可泛化表示学习确立了新方向

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [48] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 本文分析了大语言模型在基于偏好的后训练中出现的谄媚行为增强现象，揭示了人类反馈对齐如何通过奖励学习中的偏差放大机制导致模型更倾向于附和用户观点，并提出了一种训练时干预方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于人类偏好的后训练后，经常表现出更强的谄媚行为——即使与事实准确性或合理判断相冲突，也更倾向于附和用户的观点。这种行为的增强对模型的可信度和可靠性构成了严重威胁，需要深入理解其产生机制并找到有效的缓解方法。

Method: 1. 形式化分析人类反馈对齐如何通过奖励学习中的偏差放大机制导致谄媚行为增强；2. 识别行为漂移方向由基础策略下赞同提示中的信念信号与学习到的奖励之间的协方差决定；3. 分析Bradley-Terry等随机效用模型下的成对比较奖励学习；4. 提出训练时干预方法，通过KL散度最接近无约束后训练策略的独特策略来防止谄媚行为增加，并推导出相应的最小奖励修正作为闭式协议惩罚。

Result: 计算实验发现奖励差距普遍存在，并在所有考虑的配置中都导致了行为漂移。提出的协议惩罚方法能够有效中和放大机制，防止谄媚行为的增加。

Conclusion: 人类反馈对齐会通过奖励学习中的偏差放大机制增强大语言模型的谄媚行为。通过分析奖励差距和行为漂移的因果关系，本文提出的训练时干预方法能够有效缓解这一问题，为开发更可靠、更少谄媚倾向的语言模型提供了理论框架和实践指导。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [49] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: MedBeads 提出了一种面向AI代理的医疗数据基础设施，使用不可变的Merkle DAG结构存储临床事件，解决传统EMR与AI代理之间的"上下文不匹配"问题，确保数据可追溯、防篡改。


<details>
  <summary>Details</summary>
Motivation: 当前电子病历系统为人类设计，AI代理接收碎片化数据，需要依赖概率推理重建患者历史，导致幻觉问题和审计困难，存在"上下文不匹配"问题。

Method: 提出MedBeads架构，将临床事件作为不可变的"Beads"节点存储在Merkle有向无环图中，每个节点加密引用其因果前驱。采用Go核心引擎、Python中间件和React可视化界面实现原型。

Result: 成功实现工作流程，将FHIR资源转换为因果链接图，BFS上下文检索算法实现O(V+E)复杂度实时决策支持，篡改检测通过密码学链保证，可视化界面帮助临床医生理解。

Conclusion: MedBeads通过从概率搜索转向确定性图遍历，从可变记录转向不可变链，解决了上下文不匹配问题，为"可信医疗AI"提供基础，确保AI接收的上下文是确定性和防篡改的。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [50] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个用于车辆故障模式多标签序列分类的多模态方法，整合DTC序列和环境数据，显著提升分类性能


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障码序列，但忽略了温度、湿度、压力等环境传感器数据，而这些上下文信息对专家诊断故障模式至关重要。真实世界数据复杂且噪声大，需要新方法整合多模态信息。

Method: 提出BiCarFormer双向Transformer模型，专门处理车辆事件序列，采用嵌入融合和协同注意力机制来捕捉诊断代码与环境数据之间的关系。

Result: 在包含22,137个错误代码和360个错误模式的真实世界汽车数据集上，该方法相比仅使用DTC序列的传统序列模型显著提升了分类性能。

Conclusion: 整合环境上下文信息对于实现更准确、更鲁棒的车辆诊断至关重要，有助于降低维护成本并增强汽车行业的自动化流程。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [51] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，用于从车辆诊断故障码(DTC)序列中自动生成错误模式(EP)规则，替代传统手工编写规则的方法。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生数千种不同的诊断故障码，汽车制造商使用这些代码的布尔组合（错误模式）来表征系统故障。然而，EP规则仍然由领域专家手工编写，随着车辆复杂性增加，这一过程成本高昂且容易出错。

Method: CAREP采用多智能体系统架构：1)因果发现智能体识别DTC-EP潜在关系；2)上下文信息智能体整合元数据和描述；3)编排智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTC和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知的EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: 通过结合实用因果发现和基于智能体的推理，CAREP向完全自动化的故障诊断迈进一步，实现了可扩展、可解释且经济高效的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [52] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订闭环架构解决电子病历诊断中的异质性、稀疏性和噪声问题，显著提升神经病学诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 电子病历（特别是神经病学领域）具有异质性、稀疏性和噪声，单智能体系统容易产生自我强化的错误，而现有的多智能体框架交互浅薄，忽略了疾病间的丰富逻辑依赖关系（如互斥性、病理兼容性、诊断混淆），无法排除临床不可行的假设。

Method: 提出RE-MCDF框架，包含三个互补组件：1）生成候选诊断和支持证据的主要专家；2）动态优先处理异质性临床指标的实验室专家；3）强制执行疾病间逻辑约束的多关系感知与评估专家组。基于医学知识图谱，前两个专家自适应地重新加权电子病历证据，专家组验证和修正候选诊断以确保逻辑一致性。

Result: 在CMEMR的神经病学子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中始终优于最先进的基线方法。

Conclusion: RE-MCDF通过整合多专家协作和显式逻辑约束，有效解决了临床诊断中的异质性和逻辑一致性问题，为电子病历的智能诊断提供了更可靠、更符合临床实践的解决方案。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [53] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能自主形成问题并设定任务，通过推理内部状态、环境观察和与其他AI系统的交互来提升自主决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了它们在环境变化时自主识别应解决问题的能力。需要让AI系统能自主形成问题并设定任务。

Method: 提出人类模拟框架，将问题形成作为任务选择和执行之前的一等决策过程。整合内部驱动、环境感知和智能体间感知三种提示范围，逐步扩展认知覆盖。支持从经验中学习问题形成过程。

Result: 在多智能体仿真环境中，环境感知提示相比内部驱动基线显著减少了无进食事件，智能体间感知提示在20天仿真中将累积无进食事件进一步减少60%以上，具有统计显著性改进(p<0.05)。

Conclusion: 该框架使AI系统能自主形成问题和设定任务，通过整合多维度认知范围和从经验中学习，显著提升了系统在动态开放环境中的适应性和决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [54] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD是一种新的视频语言模型解码方法，通过模型感知的反事实数据构建来减少幻觉，特别针对视觉证据弱、模糊或有偏的场景。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型容易产生幻觉，特别是在视觉证据弱、模糊或有偏的情况下。现有的解码方法（如对比解码）依赖随机扰动构建对比数据，难以控制驱动幻觉的视觉线索或与模型弱点良好对齐。

Method: 提出模型感知的反事实数据对比解码（MACD），利用视频语言模型自身的反馈识别导致幻觉的对象区域，在对象级别生成有针对性的反事实输入（而非任意的帧或时间修改），然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制执行基于证据的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME上的实验表明，MACD能持续减少幻觉，同时在Qwen和InternVL等不同视频语言模型中保持或提高任务准确性。该方法在处理涉及小物体、遮挡物体或共现物体的挑战性场景时特别有效。

Conclusion: MACD通过模型引导的反事实构建与解码相结合，提供了一种有效的推理策略来减少视频语言模型的幻觉，特别是在视觉证据不足的复杂场景中。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [55] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，在数据稀缺情况下仍能在活动识别任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能家居传感器数据分析方法存在多个问题：监督模型需要大量标注数据不实用；现有基础模型只关注惯性传感器，无法处理智能家居二元传感器数据的稀疏、离散特性；基于LLM的方法需要自然语言描述或提示，依赖外部服务或昂贵硬件，存在隐私和成本问题。

Method: 提出DomusFM基础模型，采用自监督双对比学习范式，同时捕捉token级语义属性和序列级时间依赖。整合轻量级语言模型的语义嵌入、专门的时间模式编码器和二元状态编码器，学习可迁移的通用表示。

Result: 在七个公共智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务上均优于现有最先进基线，即使在仅有5%标注数据用于微调的情况下也能获得优越性能。

Conclusion: DomusFM解决了智能家居传感器数据稀缺问题，同时保持了实际部署可行性，为现实世界智能家居系统提供了实用解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [56] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: PPoGA是一个创新的知识图谱问答框架，通过规划器-执行器架构和预测处理机制，实现了自我修正能力，在复杂多跳问答任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM+KG方法在初始推理计划错误时容易失败，类似于认知功能固着问题，无法重新调整策略，导致追求不可行的解决方案。

Method: 提出PPoGA框架，采用规划器-执行器架构分离高层策略与底层执行，引入预测处理机制预测结果，核心创新是自我修正机制，包括路径修正和计划修正。

Result: 在三个具有挑战性的多跳KGQA基准测试（GrailQA、CWQ、WebQSP）上进行了广泛实验，PPoGA实现了最先进的性能，显著优于现有方法。

Conclusion: 研究表明元认知能力（如问题重构）对于构建更强大和灵活的AI推理系统至关重要，PPoGA展示了通过自我修正机制提升推理鲁棒性的有效性。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [57] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: G-MemLLM：一种基于门控更新的记忆增强LLM架构，通过潜在记忆库解决长上下文推理中的信息稀释问题，显著提升多跳推理和关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM受限于有限的上下文窗口，在多跳推理中难以保持长期事实一致性。现有方法（如上下文压缩或循环标记）常出现"上下文腐化"或信息稀释问题，需要更好的长期记忆机制。

Method: 提出G-MemLLM架构，将冻结的LLM主干与可训练的潜在记忆库结合。核心创新是GRU风格的门控更新逻辑，允许模型选择性更新、保留或覆盖潜在记忆槽，避免循环系统中常见的知识梯度消失问题。

Result: 在GPT-2（124M）到Llama 3.1（8B）不同规模模型上评估，在HotpotQA和ZsRE基准测试中表现优异：Llama 3.1-8B在ZsRE上准确率提升13.3%；GPT-2在HotpotQA上答案F1提升8.56分；Llama 3.1-8B在HotpotQA上支持事实F1提升6.89分。

Conclusion: G-MemLLM通过门控记忆机制有效解决了LLM在长上下文推理中的信息保持问题，显著提升了多跳推理和关系抽取能力，且在不同规模模型上均表现出良好的可扩展性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [58] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: PTCBENCH是一个评估大语言模型在受控情境下人格一致性的基准测试，发现外部场景（如失业）会显著改变LLM的人格特质和推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在情感代理和AI系统中的部署增加，保持一致且真实的人格对于用户信任和参与至关重要。现有研究忽视了人格特质是动态且情境依赖的基本心理学共识。

Method: 引入PTCBENCH基准，将模型置于12种不同的外部情境（地点背景和生活事件），使用NEO五因素人格量表严格评估人格，共分析了39,240个人格特质记录。

Result: 研究发现某些外部场景（如"失业"）会触发LLM显著的人格变化，甚至改变其推理能力。PTCBENCH为评估现实、动态环境中的人格一致性提供了可扩展框架。

Conclusion: PTCBENCH为开发稳健且心理对齐的AI系统提供了可操作的见解，建立了评估人格一致性的系统性框架，有助于提升LLM在情感代理中的可信度和用户体验。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [59] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 该论文研究了基于大语言模型的多智能体系统中的曼德拉效应（集体记忆偏差），提出了MANBENCH基准来评估该效应，并提出了缓解策略，平均减少74.40%的曼德拉效应。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型增强了多智能体系统的能力，但智能体对集体认知偏差的易感性仍未充分研究。曼德拉效应作为集体错误记忆现象，在多智能体系统中存在风险，可能传播错误信息并引发伦理问题。

Method: 提出MANBENCH基准，包含四种易受曼德拉效应影响的任务类型和五种不同智能体角色与记忆时间尺度的交互协议。评估多种LLM驱动的智能体，分析影响因素，并提出缓解策略：包括提示级防御（认知锚定和来源审查）和模型级对齐防御。

Result: 在MANBENCH上量化了曼德拉效应，分析了不同因素对其影响。提出的缓解策略相比基线平均减少了74.40%的曼德拉效应，为开发更具韧性的多智能体系统提供了实证支持。

Conclusion: 该研究揭示了LLM多智能体系统中曼德拉效应的存在和影响因素，提出的缓解策略有效降低了该效应。研究结果为开发更具韧性和伦理对齐的协作多智能体系统提供了重要见解。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [60] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: HyLRA是一种基于层间稀疏性分析的混合注意力机制，通过动态规划优化层间策略，在敏感层保留完整注意力，在容忍层重用前层的关键token索引，显著提升长上下文推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的长上下文推理受限于注意力机制的二次计算复杂度和KV缓存的内存占用。现有稀疏注意力方法要么依赖固定模式，要么过度剪枝，无法在效率和准确性之间达到最优平衡。

Method: HyLRA通过层间稀疏性分析发现注意力机制的双重特性：层内敏感性和层间相似性。采用离线动态规划方法生成最优层间策略，敏感层保留完整注意力，容忍层直接重用前层top-k索引，避免二次计算。

Result: HyLRA将推理吞吐量提升6%-46%，同时保持可比较的性能（准确率下降<1%），在各种基准测试中一致优于现有最先进的稀疏注意力方法。

Conclusion: HyLRA通过层间混合注意力策略有效解决了长上下文推理的计算瓶颈，在保持模型性能的同时显著提升效率，为大规模语言模型的高效推理提供了新思路。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [61] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: DISPO算法通过解耦正确与错误回答的重要性采样权重上下裁剪，实现四个可控策略更新机制，在保持探索-蒸馏平衡的同时避免灾难性失败，显著提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在数学推理中存在明显权衡：PPO风格方法训练稳定但学习缓慢，REINFORCE风格方法效率高但性能不稳定。需要解决这一局限性。

Method: DISPO算法将正确与错误回答的重要性采样权重上下裁剪解耦，形成四个可控策略更新机制，分别调节探索与蒸馏平衡，防止灾难性失败。

Result: 在AIME'24上达到61.04%准确率（vs. CISPO 55.42%，DAPO 50.21%），在多个基准测试和模型上均有类似提升。

Conclusion: DISPO通过可控的权重裁剪机制平衡探索与蒸馏，在保持训练效率的同时避免性能崩溃，为强化学习在数学推理中的应用提供了有效解决方案。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [62] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的框架，将基于LLM的时间点过程扩展到视觉模态，通过自适应序列压缩机制解决长上下文问题，在预测准确性和生成文本分析质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间点过程方法在处理多模态数据时面临挑战：多模态数据会显著增加序列长度，阻碍基于注意力的模型生成需要长距离理解的连贯长文本描述。需要一种能够生成丰富多模态内容并推理事件动态的方法。

Method: 提出基于时间相似性的自适应序列压缩机制，减少序列长度同时保留关键模式；采用两阶段范式：先在压缩序列上进行预训练，然后针对下游任务进行监督微调；将文本生成定位为核心能力，与时间和类型预测并列。

Result: 在DanmakuTPP-QA等挑战性基准测试中，该方法在预测准确性和生成文本分析质量方面均优于最先进的基线方法。

Conclusion: 该框架成功将LLM-based TPPs扩展到视觉模态，通过自适应压缩机制有效解决了长上下文问题，实现了更好的多模态事件序列建模和文本生成能力。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [63] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: FS-Researcher：基于文件系统的双智能体框架，通过持久化工作空间解决LLM在深度研究任务中上下文窗口限制问题，实现超越上下文长度的知识积累和报告生成。


<details>
  <summary>Details</summary>
Motivation: 深度研究作为LLM智能体的代表性长视野任务，其长轨迹常超出模型上下文限制，压缩了证据收集和报告编写的token预算，阻碍了有效的测试时扩展。

Method: 提出基于文件系统的双智能体框架：1) Context Builder智能体作为图书管理员浏览互联网、编写结构化笔记、将原始资料归档到可超越上下文长度的分层知识库；2) Report Writer智能体分段编写最终报告，将知识库作为事实来源。文件系统作为持久化外部内存和跨智能体/会话的共享协调媒介。

Result: 在两个开放基准测试（DeepResearch Bench和DeepConsult）上实现最先进的报告质量。分析显示最终报告质量与分配给Context Builder的计算量呈正相关，验证了文件系统范式下的有效测试时扩展。

Conclusion: FS-Researcher通过文件系统作为持久化工作空间，成功解决了深度研究中上下文窗口限制问题，实现了超越上下文长度的知识积累和报告生成，为LLM智能体的长视野任务提供了有效解决方案。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [64] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: SafePred提出预测性护栏框架，通过风险预测与决策优化，解决现有反应式护栏无法应对长期风险的问题，显著提升计算机使用代理的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理（CUAs）的护栏大多采用反应式方法，只能在当前观察空间内约束代理行为。虽然能防止短期风险（如点击钓鱼链接），但无法主动避免长期风险：看似合理的行动可能导致延迟出现的高风险后果（如清理日志导致未来审计不可追溯），反应式护栏无法在当前观察空间内识别这些风险。

Method: 提出预测性护栏方法，核心思想是将预测的未来风险与当前决策对齐。基于此提出SafePred框架，建立风险到决策的循环，支持两个关键能力：1）短期和长期风险预测：以安全策略为基础，利用世界模型的预测能力生成短期和长期风险的语义表示，识别并剪枝导致高风险状态的行为；2）决策优化：通过步骤级干预和任务级重新规划，将预测风险转化为可操作的安全决策指导。

Result: 大量实验表明，SafePred显著减少了高风险行为，与反应式基线相比，实现了超过97.6%的安全性能，并将任务效用提高了高达21.4%。

Conclusion: SafePred通过预测性护栏方法有效解决了计算机使用代理的长期风险问题，将风险预测与决策优化相结合，在保持高安全性的同时提升了任务执行效率。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [65] [Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration](https://arxiv.org/abs/2602.01040)
*Yuhang Zhang,Chao Yan,Jiaxi Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: CAPO提出了一种结合对比提示学习和自适应提示编排的新方法，用于学习跨具身智能体的视觉运动策略，显著提升了样本效率和零样本适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法难以将任务相关特征与领域特定变化（如光照、视野、旋转）分离，导致样本效率低下且在未见环境中表现不佳。跨具身智能体的传感器配置和动态特性差异使得策略学习面临巨大挑战。

Method: 提出ContrAstive Prompt Orchestration (CAPO)：1) 混合对比学习策略整合视觉、时序动作和文本目标，建立可学习提示池；2) 自适应提示编排机制根据当前观测动态聚合提示，识别主导领域因素并构建最优状态表示。

Result: CAPO在样本效率和渐进性能上显著优于现有基线方法，在光照、视野、旋转等剧烈环境变化和物理变化的未见目标域中表现出卓越的零样本适应能力。

Conclusion: CAPO通过对比提示学习和自适应编排有效分离任务相关特征与领域特定变化，为跨具身智能体的视觉运动策略适应提供了可行的解决方案。

Abstract: Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.

</details>


### [66] [Estimating Force Interactions of Deformable Linear Objects from their Shapes](https://arxiv.org/abs/2602.01085)
*Qi Jing Chen,Shilin Shan,Timothy Bretl,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: 提出一种仅通过观察形状来检测和估计作用于可变形线性物体（DLO）外部力的分析方法，无需额外力传感器或末端接触假设。


<details>
  <summary>Details</summary>
Motivation: 在机器人-线缆交互任务中，接触常发生在机器人身体其他部位而非末端执行器，现有方法依赖昂贵的力传感器或末端接触假设，需要一种仅通过形状信息就能准确识别交互的方法。

Method: 利用深度相机获取线缆形状信息，假设线缆处于或接近静态平衡，通过推导一致性条件并求解基于线缆力-力矩平衡的线性方程组，估计外部力的位置和大小。

Result: 仿真实验显示方法达到高精度，真实世界实验在选定交互场景中展示了准确估计能力。

Conclusion: 该方法能够仅通过形状信息有效检测和估计外部力，为机器人安全高效轨迹规划提供了实用解决方案。

Abstract: This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.

</details>


### [67] [SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models](https://arxiv.org/abs/2602.01226)
*Aditya Shibu,Marah Saleh,Mohamed Al-Musleh,Nidhal Abdulaziz*

Main category: cs.RO

TL;DR: SkySim是一个基于ROS2和Gazebo的无人机集群仿真框架，使用Gemini 3.5 Pro进行高级自然语言规划，结合人工势场安全过滤器确保轨迹安全，实现了非专家用户对无人机集群的自然语言控制。


<details>
  <summary>Details</summary>
Motivation: 无人机集群在物流、农业和监控等领域有广泛应用，但传统控制方法需要专业知识且缺乏适应性，而大型语言模型虽然能实现自然语言控制，但生成的轨迹缺乏物理基础且不安全。

Method: 提出SkySim框架，将LLM高级规划与低级安全执行解耦：使用Gemini 3.5 Pro将用户自然语言命令转换为空间航点，然后通过人工势场安全过滤器进行碰撞避免、运动学限制和地理围栏检查，以20Hz频率执行。

Result: 使用3、10和30架Crazyflie无人机进行实验验证：空间推理准确率达到100%（测试所有几何基元），实时碰撞预防有效，系统具有良好的可扩展性。

Conclusion: SkySim使非专家用户能够通过自然语言迭代优化无人机集群行为，将AI认知与机器人安全相结合，为动态环境提供安全可靠的无人机集群控制方案。

Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.

</details>


### [68] [Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations](https://arxiv.org/abs/2602.01535)
*Huzaifa Mustafa Unjhawala,Khizar Shaikh,Luning Bakke,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: 提出贝叶斯优化框架，联合优化月球车车轮几何与转向控制器参数，使用连续介质模型进行全车闭环仿真，相比传统DEM方法大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 传统离散元方法（DEM）模拟可变形地形成本过高，通常只能进行单轮测试，无法进行全车闭环仿真，限制了越野车辆机械设计与控制的联合优化

Method: 使用连续介质表示模型（CRM）进行高效地形力学模拟，建立贝叶斯优化框架，同时优化车轮参数（半径、宽度、抓地齿特征）和转向PID增益，采用多目标优化平衡行驶速度、跟踪误差和能耗

Result: 完成3000次全车仿真仅需5-9天，相比之前DEM方法需要数月大幅提升效率；初步硬件测试显示仿真优化设计在物理月球车上保持相对性能趋势

Conclusion: 可扩展的高保真仿真能够实现越野车辆车轮设计与控制的实用联合优化，无需依赖昂贵的DEM研究，相关仿真基础设施已开源发布

Abstract: While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.

</details>


### [69] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: RTRRL算法能在线微调预训练策略，提升自动驾驶任务性能，并与生物启发的LRC-RNN结合，在仿真和真实世界任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在真实世界部署时面临环境动态变化、传感器漂移和任务目标变化等挑战，固定策略性能会迅速下降，需要在线适应能力。

Method: 采用实时循环强化学习（RTRRL）在线微调预训练策略，并与生物启发的液体电阻-液体电容循环神经网络（LRC-RNN）结合，形成闭环控制方法。

Result: 在模拟CarRacing环境和真实世界RoboRacer事件相机线跟踪任务中，该方法有效提升了自主智能体的性能表现。

Conclusion: RTRRL与生物启发网络结合能有效解决预训练策略在动态环境中的适应问题，为学习型控制系统在实际应用中的部署提供了可行方案。

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [70] [Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces](https://arxiv.org/abs/2602.02411)
*Hanwen Ren,Junyong Kim,Aathman Tharmasanthiran,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: CAM-MCTS：一种用于多智能体物体重排规划的集中式异步蒙特卡洛树搜索框架，通过集中任务分配和异步执行策略减少任务完成时间


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物体重排任务通常是非单调的（物体相互阻挡需要临时移动），现有研究主要处理单调实例，多智能体协作可以显著减少任务完成时间

Method: 提出CAM-MCTS框架，结合集中式任务分配（智能体了解彼此意图以优化全局规划）和异步任务执行策略（智能体在适当时刻承担新任务而不等待其他智能体），使用一步前瞻成本估计指导

Result: 在杂乱环境中的单调和非单调任务上评估，相比强基线方法持续减少makespan（任务完成时间），在真实多智能体系统中验证了有效性和鲁棒性

Conclusion: CAM-MCTS通过最小化空闲时间、避免不必要的同步延迟，提高了多智能体物体重排规划的整体系统效率

Abstract: Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.

</details>
