{"id": "2507.14267", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS\u662f\u4e00\u4e2a\u57fa\u4e8eDFT\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u5b9e\u73b0\u6750\u6599\u53d1\u73b0\u7684\u9ad8\u901a\u91cf\u3001\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u7684\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3DFT\u6a21\u62df\u4e2d\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u53c2\u6570\u8c03\u4f18\u590d\u6742\u548c\u7cfb\u7edf\u8bef\u5dee\u5904\u7406\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u4e2d\u592eLLM\u89c4\u5212\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9aLLM\u4ee3\u7406\uff0c\u5171\u4eab\u753b\u5e03\u8f85\u52a9\u534f\u4f5c\u3002", "result": "\u5728Sol27LC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bef\u5dee\u4f4e\u4e8e1%\uff0c\u89e3\u51b3\u4e86CO/Pt(111)\u5438\u9644\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u91c7\u6837\u9a8c\u8bc1FCC\u4f4d\u70b9\u504f\u597d\u3002", "conclusion": "DREAMS\u5b9e\u73b0\u4e86L3\u7ea7\u81ea\u52a8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u9ad8\u901a\u91cf\u6750\u6599\u53d1\u73b0\u7684\u6c11\u4e3b\u5316\u3002"}}
{"id": "2507.14181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14181", "abs": "https://arxiv.org/abs/2507.14181", "authors": ["Yajiao Dai", "Jun Li", "Zhen Mei", "Yiyang Ni", "Shi Jin", "Zengxiang Li", "Sheng Guo", "Wei Xiang"], "title": "Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis", "comment": "Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\n  5 figures", "summary": "Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\noperation of industrial machinery and improving production efficiency. However,\ntraditional supervised deep learning methods require a large amount of training\ndata and labels, which are often located in different clients. Additionally,\nthe cost of data labeling is high, making labels difficult to acquire.\nMeanwhile, differences in data distribution among clients may also hinder the\nmodel's performance. To tackle these challenges, this paper proposes a\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\ncontrastive loss and soft labeling to address data and label scarcity for\ndistributed clients with few labeled samples while safeguarding user privacy.\nIt enables representation learning using unlabeled data on the client side and\nfacilitates joint learning among clients through prototypes, thereby achieving\nmutual knowledge sharing and preventing local model divergence. Specifically,\nfirst, a sample weighting function based on the Laplace distribution is\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\nsemi-supervised training process. Second, a dual contrastive loss is introduced\nto mitigate model divergence caused by different data distributions, comprising\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\naggregated on the server with weighted averaging and updated with momentum to\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\nexperiments are conducted on two publicly available datasets and a dataset\ncollected on motors from the factory. In the most challenging task, where only\n10\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\n1.15% to 7.85% over state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6SSFL-DCSL\uff0c\u901a\u8fc7\u53cc\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u89e3\u51b3\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u6570\u636e\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6210\u672c\u9ad8\uff0c\u800c\u4e0d\u540c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u5dee\u5f02\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7684\u6837\u672c\u52a0\u6743\u51fd\u6570\u3001\u53cc\u5bf9\u6bd4\u635f\u5931\uff08\u5c40\u90e8\u548c\u5168\u5c40\u5bf9\u6bd4\u635f\u5931\uff09\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u548c\u52a8\u91cf\u66f4\u65b0\u805a\u5408\u672c\u5730\u539f\u578b\u3002", "result": "\u5728\u4ec510%\u6570\u636e\u6807\u6ce8\u7684\u6700\u5177\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cSSFL-DCSL\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53471.15%\u81f37.85%\u3002", "conclusion": "SSFL-DCSL\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6807\u7b7e\u7a00\u7f3a\u548c\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u4fdd\u62a4\u4e86\u9690\u79c1\u3002"}}
{"id": "2507.14513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\u7684\u81ea\u4e3b\u4ee3\u7406\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f9d\u8d56\u4e91\u7aef\u8ba1\u7b97\u4e14\u7f3a\u4e4f\u6301\u4e45\u81ea\u4e3b\u6027\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "Amico\u91c7\u7528Rust\u7f16\u5199\uff0c\u652f\u6301\u901a\u8fc7WebAssembly\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u548c\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u63d0\u4f9b\u4e8b\u4ef6\u5904\u7406\u3001\u72b6\u6001\u7ba1\u7406\u548c\u884c\u4e3a\u6267\u884c\u7684\u62bd\u8c61\u3002", "result": "Amico\u4e3a\u6784\u5efa\u9002\u5e94\u6709\u9650\u8ba1\u7b97\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u73af\u5883\u7684\u5f39\u6027\u4ea4\u4e92\u4ee3\u7406\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "Amico\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u5b89\u5168\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u4ee3\u7406\u5f00\u53d1\u3002"}}
{"id": "2507.14660", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u6076\u610f\u5408\u8c0b\u4e2d\u7684\u98ce\u9669\uff0c\u901a\u8fc7\u6a21\u62df\u6846\u67b6\u5c55\u793a\u4e86\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u548c\u7535\u5546\u6b3a\u8bc8\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6f5c\u5728\u5371\u5bb3\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u6076\u610f\u5408\u8c0b\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u96c6\u4e2d\u5f0f\u548c\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u7ed3\u6784\uff0c\u5e94\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5546\u6b3a\u8bc8\u573a\u666f\u3002", "result": "\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u5b9e\u65bd\u6076\u610f\u884c\u4e3a\u65f6\u66f4\u9ad8\u6548\uff0c\u4e14\u80fd\u7075\u6d3b\u8c03\u6574\u7b56\u7565\u4ee5\u89c4\u907f\u4f20\u7edf\u5e72\u9884\u63aa\u65bd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6539\u8fdb\u68c0\u6d4b\u7cfb\u7edf\u548c\u5e94\u5bf9\u63aa\u65bd\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5e94\u5bf9\u6076\u610f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5a01\u80c1\u3002"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6cd5\u5411\u6d41\u7684\u8fd0\u52a8\u5206\u5272\u4e0e\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u65e0\u9700\u5b8c\u6574\u5149\u6d41\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5149\u6d41\u6216\u6df1\u5ea6\u4f30\u8ba1\uff0c\u800c\u795e\u7ecf\u5f62\u6001\u4f20\u611f\u5668\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4e8b\u4ef6\u6570\u636e\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5229\u7528\u51e0\u4f55\u7ea6\u675f\uff0c\u901a\u8fc7\u4f18\u5316\u6d41\u7a0b\u8fdb\u884c\u4e8b\u4ef6\u8fc7\u5206\u5272\u3001\u6b8b\u5dee\u5206\u6790\u5206\u79bb\u8fd0\u52a8\u5bf9\u8c61\uff0c\u5e76\u57fa\u4e8e\u8fd0\u52a8\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u8fdb\u884c\u5c42\u6b21\u805a\u7c7b\u3002", "result": "\u5728EVIMO2v2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u7269\u4f53\u8fb9\u754c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u53ca\u5bfc\u822a\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u673a\u5f15\u5bfc\u7cfb\u7edf\uff0c\u5e2e\u52a9\u6444\u5f71\u7231\u597d\u8005\u8bc6\u522b\u548c\u53bb\u9664\u7167\u7247\u4e2d\u7684\u6742\u4e71\u5143\u7d20\uff0c\u901a\u8fc7\u7f8e\u5b66\u8bc4\u4f30\u548c\u56fe\u50cf\u4fee\u590d\u7b97\u6cd5\u63d0\u5347\u7167\u7247\u8d28\u91cf\u3002", "motivation": "\u7167\u7247\u4e2d\u7684\u6742\u4e71\u5143\u7d20\u4f1a\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5f71\u54cd\u60c5\u611f\u6216\u6545\u4e8b\u7684\u4f20\u8fbe\u3002\u4e1a\u4f59\u6444\u5f71\u5e08\u5e38\u56e0\u758f\u5ffd\u6216\u7ecf\u9a8c\u4e0d\u8db3\u800c\u62cd\u6444\u6742\u4e71\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6765\u6307\u5bfc\u4ed6\u4eec\u4f18\u5316\u6784\u56fe\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u4e86\u7f8e\u5b66\u8bc4\u4f30\u7b97\u6cd5\u548c\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u56fe\u50cf\u4fee\u590d\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u6742\u4e71\u8bc6\u522b\u548c\u53bb\u9664\u5de5\u5177\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u8bc6\u522b\u6742\u4e71\u5e76\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7075\u6d3b\u754c\u9762\u548c\u51c6\u786e\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6444\u5f71\u7231\u597d\u8005\u7684\u62cd\u6444\u6548\u7387\u548c\u7167\u7247\u8d28\u91cf\u3002"}}
{"id": "2507.14693", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14693", "abs": "https://arxiv.org/abs/2507.14693", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u571f\u8033\u5176\u8bed\u81ea\u6740\u610f\u5ff5\u8bed\u6599\u5e93\uff0c\u5e76\u63d0\u51fa\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u6807\u7b7e\u53ef\u9760\u6027\u548c\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u5f3a\u8c03\u4e86\u5fc3\u7406\u5065\u5eb7NLP\u4e2d\u8bed\u8a00\u5305\u5bb9\u6027\u548c\u900f\u660e\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u6740\u610f\u5ff5\u68c0\u6d4b\u4e2d\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u548c\u6807\u6ce8\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u5168\u7403AI\u81ea\u6740\u9884\u9632\u7684\u5b9e\u73b0\u3002", "method": "\u6784\u5efa\u571f\u8033\u5176\u8bed\u8bed\u6599\u5e93\uff0c\u5f15\u5165\u4e09\u4eba\u7c7b\u6807\u6ce8\u8005\u548c\u4e24\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u6807\u7b7e\u53ef\u9760\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u6d41\u884c\u6a21\u578b\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u5fc3\u7406\u5065\u5eb7NLP\u9700\u8981\u66f4\u4e25\u683c\u7684\u6807\u6ce8\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u547c\u5401\u5728\u5fc3\u7406\u5065\u5eb7NLP\u4e2d\u4f18\u5148\u8003\u8651\u6570\u636e\u548c\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u5021\u900f\u660e\u6027\u3002"}}
{"id": "2507.15444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u60ac\u505c\u548c\u6a2a\u5411\u79fb\u52a8\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u56e0\u6c14\u6d41\u6270\u52a8\u5bfc\u81f4\u7684\u60ac\u505c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4f4e\u5ef6\u8fdf\u7684\u4e8b\u4ef6\u70df\u96fe\u6d4b\u901f\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6270\u52a8\u4f30\u8ba1\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u63a7\u5236\u5668\u3002", "result": "\u7cfb\u7edf\u80fd\u6709\u6548\u62b5\u6d88\u77ac\u6001\u6c14\u52a8\u6548\u5e94\uff0c\u9632\u6b62\u4e0e\u7ba1\u58c1\u78b0\u649e\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u95ed\u73af\u63a7\u5236\u3002", "conclusion": "\u4e3a\u590d\u6742\u6c14\u52a8\u73af\u5883\u4e2d\u7684\u98de\u884c\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u63ed\u793a\u4e86\u7ba1\u9053\u98de\u884c\u4e2d\u7684\u6d41\u573a\u7279\u6027\u3002"}}
{"id": "2507.15499", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15499", "abs": "https://arxiv.org/abs/2507.15499", "authors": ["Jongseok Lee", "Timo Birr", "Rudolph Triebel", "Tamim Asfour"], "title": "CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions", "comment": "8 pages. Accepted to IEEE RAL", "summary": "We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.", "AI": {"tldr": "CLEVER\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u83b7\u53d6\u4eba\u7c7b\u652f\u6301\u5e76\u9002\u5e94DNN\uff0c\u63d0\u5347\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3DNN\u5728\u6d41\u6570\u636e\u8bed\u4e49\u611f\u77e5\u4e2d\u7684\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u4eba\u7c7b\u5e72\u9884\u548c\u5728\u7ebf\u9002\u5e94\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u7f16\u7801\u9886\u57df\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6d41\u5f0f\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u7528\u6237\u9a8c\u8bc1\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0cCLEVER\u80fd\u6709\u6548\u63d0\u5347DNN\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CLEVER\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u7684\u6d41\u5f0f\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u4e3aDNN\u8bed\u4e49\u611f\u77e5\u7684\u5b9e\u8df5\u6539\u8fdb\u63d0\u4f9b\u4e86\u8bc1\u636e\u3002"}}
{"id": "2507.15729", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15729", "abs": "https://arxiv.org/abs/2507.15729", "authors": ["Jens V. R\u00fcppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f85\u52a9\u673a\u5668\u4eba\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u89c6\u7ebf\u548c\u8bed\u97f3\uff09\u652f\u6301\u52a8\u6001\u7528\u6237\u4efb\u52a1\uff0c\u5e76\u4e0e\u4f20\u7edf\u811a\u672c\u5316\u7cfb\u7edf\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u5728\u53cc\u5411\u3001\u591a\u6a21\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u652f\u6301\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u8f6c\u79fb\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u89c6\u89c9\u8f93\u5165\u548c\u5b9e\u65f6\u8bed\u8a00\u4ea4\u4e92\u72b6\u6001\u8868\u793a\uff0c\u652f\u6301\u52a8\u6001\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u5197\u4f59\u8f93\u51fa\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u66f4\u9002\u5408\u7b80\u5355\u4efb\u52a1\u3002", "conclusion": "LLM\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9700\u4f18\u5316\u5197\u4f59\u95ee\u9898\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u66f4\u9ad8\u6548\u3002"}}
{"id": "2507.15411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15411", "abs": "https://arxiv.org/abs/2507.15411", "authors": ["Wissam Gherissi", "Mehdi Acheli", "Joyce El Haddad", "Daniela Grigori"], "title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "comment": "ICSOC Workshops 2024, Dec 2024, Tunis, Tunisia", "summary": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cLSTM\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u6d41\u7a0b\u884c\u4e3a\uff0c\u5305\u62ec\u4e0b\u4e00\u6d3b\u52a8\u548c\u4e0b\u4e00\u4e8b\u4ef6\u65f6\u95f4\uff0c\u5e76\u5728\u771f\u5b9e\u548c\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\u63d0\u5347\u6d41\u7a0b\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4fe1\u606f\u63d0\u53d6\u548c\u6a21\u578b\u6784\u5efa\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7f16\u7801\u6d3b\u52a8\u53ca\u5176\u5173\u7cfb\uff0c\u4f7f\u7528LSTM\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u9884\u6d4b\u6d41\u7a0b\u76d1\u63a7\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "\u63d0\u51faMultiRetNet\uff0c\u7ed3\u5408\u89c6\u7f51\u819c\u6210\u50cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u5171\u75c5\u8d44\u6599\uff0c\u63d0\u9ad8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u671f\u51c6\u786e\u6027\uff0c\u5e76\u96c6\u6210\u4e34\u5e8a\u5ef6\u8fdf\u7cfb\u7edf\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5168\u7403\u53ef\u9884\u9632\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f4e\u6536\u5165\u7fa4\u4f53\u56e0\u7b5b\u67e5\u673a\u4f1a\u6709\u9650\u66f4\u6613\u8fdb\u5c55\u81f3\u665a\u671f\u3002\u5171\u75c5\u6761\u4ef6\u52a0\u901f\u75c5\u60c5\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u5168\u8fde\u63a5\u5c42\u878d\u5408\u6548\u679c\u6700\u4f73\u3002\u901a\u8fc7\u5bf9\u6297\u6027\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u5ef6\u8fdf\u7cfb\u7edf\uff0c\u8bc6\u522b\u9700\u4e34\u5e8a\u590d\u67e5\u7684\u5f02\u5e38\u6837\u672c\u3002", "result": "\u7cfb\u7edf\u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u4fdd\u6301\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u6574\u5408\u5173\u952e\u5065\u5eb7\u6570\u636e\uff0c\u63d0\u9ad8\u65e9\u671f\u68c0\u6d4b\u7387\uff0c\u5c24\u5176\u5bf9\u670d\u52a1\u4e0d\u8db3\u4eba\u7fa4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u964d\u4f4e\u533b\u7597\u6210\u672c\uff0c\u63d0\u9ad8\u65e9\u671f\u68c0\u6d4b\u7387\uff0c\u51cf\u5c11\u533b\u7597\u8d44\u6e90\u5206\u914d\u4e0d\u5747\uff0c\u4fc3\u8fdb\u533b\u7597\u516c\u5e73\u3002"}}
{"id": "2507.14743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86InterAct VideoQA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u5728\u4ea4\u901a\u76d1\u63a7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u63d0\u5347\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b8\u5c0f\u65f6\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u548c25,000\u4e2a\u95ee\u7b54\u5bf9\u7684InterAct VideoQA\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u5728InterAct VideoQA\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "InterAct VideoQA\u6570\u636e\u96c6\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2507.14784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14784", "abs": "https://arxiv.org/abs/2507.14784", "authors": ["Xinxin Dong", "Baoyun Peng", "Haokai Ma", "Yufei Wang", "Zixuan Dong", "Fei Hu", "Xiaodong Wang"], "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering", "comment": null, "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.", "AI": {"tldr": "LeAdQA\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u611f\u77e5\u67e5\u8be2\u4f18\u5316\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u95ee\u7b54\u4e2d\u5173\u952e\u5e27\u7a00\u758f\u548c\u56e0\u679c\u63a8\u7406\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u65e0\u5173\u91c7\u6837\u548c\u542f\u53d1\u5f0f\u68c0\u7d22\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5173\u952e\u4e8b\u4ef6\u548c\u56e0\u679c\u63a8\u7406\u3002", "method": "LeAdQA\u5229\u7528LLM\u4f18\u5316\u95ee\u9898-\u9009\u9879\u5bf9\uff0c\u901a\u8fc7\u65f6\u95f4\u5b9a\u4f4d\u6a21\u578b\u7cbe\u786e\u68c0\u7d22\u5173\u952e\u7247\u6bb5\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u548cMLLM\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728NExT-QA\u3001IntentQA\u548cNExT-GQA\u6570\u636e\u96c6\u4e0a\uff0cLeAdQA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LeAdQA\u901a\u8fc7\u7cbe\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u56e0\u679c\u63a8\u7406\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u95ee\u7b54\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.15844", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15844", "abs": "https://arxiv.org/abs/2507.15844", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "AI": {"tldr": "HBPO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9884\u7b97\u63a2\u7d22\u548c\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u56e0\u7edf\u4e00\u7b56\u7565\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u56e0\u6548\u7387\u4f18\u5316\u800c\u727a\u7272\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u9884\u7b97\u63a2\u7d22\u548c\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u5c06\u6837\u672c\u5206\u7ec4\u5e76\u5206\u914d\u4e0d\u540c\u9884\u7b97\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHBPO\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51cf\u5c1160.6%\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u53473.14%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u63a8\u7406\u6548\u7387\u548c\u80fd\u529b\u5e76\u975e\u4e0d\u53ef\u517c\u5f97\uff0c\u901a\u8fc7\u5206\u5c42\u8bad\u7ec3\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\u4e24\u8005\u3002"}}
{"id": "2507.14805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14805", "abs": "https://arxiv.org/abs/2507.14805", "authors": ["Alex Cloud", "Minh Le", "James Chua", "Jan Betley", "Anna Sztyber-Betley", "Jacob Hilton", "Samuel Marks", "Owain Evans"], "title": "Subliminal Learning: Language models transmit behavioral traits via hidden signals in data", "comment": null, "summary": "We study subliminal learning, a surprising phenomenon where language models\ntransmit behavioral traits via semantically unrelated data. In our main\nexperiments, a \"teacher\" model with some trait T (such as liking owls or being\nmisaligned) generates a dataset consisting solely of number sequences.\nRemarkably, a \"student\" model trained on this dataset learns T. This occurs\neven when the data is filtered to remove references to T. We observe the same\neffect when training on code or reasoning traces generated by the same teacher\nmodel. However, we do not observe the effect when the teacher and student have\ndifferent base models. To help explain our findings, we prove a theoretical\nresult showing that subliminal learning occurs in all neural networks under\ncertain conditions, and demonstrate subliminal learning in a simple MLP\nclassifier. We conclude that subliminal learning is a general phenomenon that\npresents an unexpected pitfall for AI development. Distillation could propagate\nunintended traits, even when developers try to prevent this via data filtering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8bed\u4e49\u65e0\u5173\u7684\u6570\u636e\u4f20\u9012\u884c\u4e3a\u7279\u5f81\uff0c\u79f0\u4e3a\u201c\u6f5c\u610f\u8bc6\u5b66\u4e60\u201d\u3002\u5373\u4f7f\u6570\u636e\u8fc7\u6ee4\u6389\u76f8\u5173\u7279\u5f81\uff0c\u5b66\u751f\u6a21\u578b\u4ecd\u80fd\u4ece\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u8fd9\u4e9b\u7279\u5f81\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u770b\u4f3c\u65e0\u5173\u7684\u6570\u636e\u4f20\u9012\u884c\u4e3a\u7279\u5f81\uff0c\u63ed\u793aAI\u5f00\u53d1\u4e2d\u6f5c\u5728\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u6570\u5b57\u5e8f\u5217\u3001\u4ee3\u7801\u6216\u63a8\u7406\u75d5\u8ff9\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u89c2\u5bdf\u5176\u662f\u5426\u5b66\u4e60\u5230\u6559\u5e08\u6a21\u578b\u7684\u884c\u4e3a\u7279\u5f81\u3002", "result": "\u5b66\u751f\u6a21\u578b\u786e\u5b9e\u4ece\u8fc7\u6ee4\u540e\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u6559\u5e08\u6a21\u578b\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4f46\u4ec5\u5f53\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u7684\u57fa\u7840\u6a21\u578b\u76f8\u540c\u65f6\u6709\u6548\u3002", "conclusion": "\u6f5c\u610f\u8bc6\u5b66\u4e60\u662f\u4e00\u79cd\u666e\u904d\u73b0\u8c61\uff0c\u53ef\u80fd\u6210\u4e3aAI\u5f00\u53d1\u4e2d\u7684\u6f5c\u5728\u9677\u9631\uff0c\u5373\u4f7f\u6570\u636e\u8fc7\u6ee4\u4e5f\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u3002"}}
{"id": "2507.14847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14847", "abs": "https://arxiv.org/abs/2507.14847", "authors": ["Junhan Yu", "Zhunyi Feng", "Junwei Lu", "Tianxi Cai", "Doudou Zhou"], "title": "Time-Aware Attention for Enhanced Electronic Health Records Modeling", "comment": null, "summary": "Electronic Health Records (EHR) contain valuable clinical information for\npredicting patient outcomes and guiding healthcare decisions. However,\neffectively modeling Electronic Health Records (EHRs) requires addressing data\nheterogeneity and complex temporal patterns. Standard approaches often struggle\nwith irregular time intervals between clinical events. We propose TALE-EHR, a\nTransformer-based framework featuring a novel time-aware attention mechanism\nthat explicitly models continuous temporal gaps to capture fine-grained\nsequence dynamics. To complement this temporal modeling with robust semantics,\nTALE-EHR leverages embeddings derived from standardized code descriptions using\na pre-trained Large Language Model (LLM), providing a strong foundation for\nunderstanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset\ndemonstrate that our approach outperforms state-of-the-art baselines on tasks\nsuch as disease progression forecasting. TALE-EHR underscores the benefit of\nintegrating explicit, continuous temporal modeling with strong semantic\nrepresentations provides a powerful solution for advancing EHR analysis.", "AI": {"tldr": "TALE-EHR\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u548c\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5305\u542b\u4e30\u5bcc\u7684\u4e34\u5e8a\u4fe1\u606f\uff0c\u4f46\u6807\u51c6\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u548c\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTALE-EHR\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff0c\u663e\u5f0f\u5efa\u6a21\u8fde\u7eed\u65f6\u95f4\u95f4\u9694\u5e76\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u3002", "result": "\u5728MIMIC-IV\u548cPIC\u6570\u636e\u96c6\u4e0a\uff0cTALE-EHR\u5728\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TALE-EHR\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u65f6\u95f4\u5efa\u6a21\u548c\u5f3a\u8bed\u4e49\u8868\u793a\uff0c\u4e3aEHR\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15600", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15600", "abs": "https://arxiv.org/abs/2507.15600", "authors": ["Armin Pournaki"], "title": "Conflicting narratives and polarization on social media", "comment": "30 pages, 7 figures", "summary": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization.", "AI": {"tldr": "\u5206\u6790\u51b2\u7a81\u53d9\u4e8b\u63ed\u793a\u516c\u5171\u9886\u57df\u6781\u5316\u4e0e\u8bae\u9898\u5bf9\u9f50\u7684\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u53d9\u4e8b\u5982\u4f55\u4f5c\u4e3a\u89e3\u91ca\u653f\u6cbb\u73b0\u5b9e\u7684\u5de5\u5177\uff0c\u5e76\u7814\u7a76\u5176\u5bf9\u6781\u5316\u548c\u8bae\u9898\u5bf9\u9f50\u7684\u5f71\u54cd\u3002", "method": "\u4ece\u5fb7\u56fdTwitter\u6570\u636e\u4e2d\u63d0\u53d6\u5bf9\u7acb\u610f\u89c1\u7fa4\u4f53\u7684\u53d9\u4e8b\u4fe1\u53f7\uff0c\u805a\u7126\u4e4c\u514b\u5170\u6218\u4e89\u3001\u65b0\u51a0\u75ab\u60c5\u548c\u6c14\u5019\u53d8\u5316\u7b49\u8bae\u9898\u3002", "result": "\u53d1\u73b0\u51b2\u7a81\u53d9\u4e8b\u7684\u4e24\u7ef4\u5ea6\uff1a\u89d2\u8272\u5206\u914d\u5dee\u5f02\u548c\u4e8b\u4ef6\u60c5\u8282\u5dee\u5f02\uff0c\u5e76\u521d\u6b65\u63ed\u793a\u53d9\u4e8b\u5bf9\u9f50\u6a21\u5f0f\u3002", "conclusion": "\u53d9\u4e8b\u5206\u6790\u4e3a\u7406\u89e3\u6781\u5316\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.15759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15759", "abs": "https://arxiv.org/abs/2507.15759", "authors": ["Lyumanshan Ye", "Xiaojie Cai", "Xinkai Wang", "Junfei Wang", "Xiangkun Hu", "Jiadi Su", "Yang Nan", "Sihan Wang", "Bohan Zhang", "Xiaoze Fan", "Jinbin Luo", "Yuxiang Zheng", "Tianze Xu", "Dayuan Fu", "Yunze Wu", "Pengrui Lu", "Zengzhi Wang", "Yiwei Qin", "Zhen Huang", "Yan Ma", "Zhulin Hu", "Haoyang Zou", "Tiantian Mi", "Yixin Ye", "Ethan Chern", "Pengfei Liu"], "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership", "comment": "30 pages, 10 figures", "summary": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u201c\u4ea4\u4e92\u5373\u667a\u80fd\u201d\u6982\u5ff5\uff0c\u91cd\u65b0\u5b9a\u4e49\u4eba\u673a\u5173\u7cfb\uff0c\u5f3a\u8c03\u4ea4\u4e92\u662f\u667a\u80fd\u7684\u6838\u5fc3\u7ef4\u5ea6\uff0c\u800c\u975e\u5355\u7eaf\u63a5\u53e3\u3002\u901a\u8fc7Deep Cognition\u7cfb\u7edf\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u63a7\u7684\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u7814\u7a76\u4efb\u52a1\u7684\u6548\u7387\u548c\u534f\u4f5c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4eba\u673a\u4ea4\u4e92\u6a21\u5f0f\uff08\u8f93\u5165-\u7b49\u5f85-\u8f93\u51fa\uff09\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3001\u7814\u7a76\u8fb9\u754c\u50f5\u5316\u53ca\u4e13\u5bb6\u77e5\u8bc6\u6574\u5408\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51faDeep Cognition\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u5927\u521b\u65b0\uff1a\u900f\u660e\u53ef\u63a7\u7684\u4ea4\u4e92\u3001\u7ec6\u7c92\u5ea6\u53cc\u5411\u5bf9\u8bdd\u3001\u5171\u4eab\u8ba4\u77e5\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4eba\u7c7b\u5bf9AI\u601d\u7ef4\u8fc7\u7a0b\u7684\u6218\u7565\u5e72\u9884\u3002", "result": "\u7528\u6237\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u900f\u660e\u5ea6\u3001\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u3001\u5b9e\u65f6\u5e72\u9884\u7b49\u516d\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7814\u7a76\u4efb\u52a1\u6027\u80fd\u63d0\u534731.8%\u81f350.0%\u3002", "conclusion": "\u4ea4\u4e92\u662f\u667a\u80fd\u7684\u6838\u5fc3\uff0cDeep Cognition\u901a\u8fc7\u8ba4\u77e5\u76d1\u7763\u6a21\u5f0f\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15158", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.15158", "abs": "https://arxiv.org/abs/2507.15158", "authors": ["A. H. Abbas", "Hend Abdel-Ghani", "Ivan S. Maksymov"], "title": "Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition", "comment": null, "summary": "As artificial intelligence continues to push into real-time, edge-based and\nresource-constrained environments, there is an urgent need for novel,\nhardware-efficient computational models. In this study, we present and validate\na neuromorphic computing architecture based on resonant-tunnelling diodes\n(RTDs), which exhibit the nonlinear characteristics ideal for physical\nreservoir computing (RC). We theoretically formulate and numerically implement\nan RTD-based RC system and demonstrate its effectiveness on two image\nrecognition benchmarks: handwritten digit classification and object recognition\nusing the Fruit~360 dataset. Our results show that this circuit-level\narchitecture delivers promising performance while adhering to the principles of\nnext-generation RC -- eliminating random connectivity in favour of a\ndeterministic nonlinear transformation of input signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u632f\u96a7\u7a7f\u4e8c\u6781\u7ba1\uff08RTD\uff09\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\uff0c\u5e76\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5411\u5b9e\u65f6\u3001\u8fb9\u7f18\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u6269\u5c55\uff0c\u9700\u8981\u786c\u4ef6\u9ad8\u6548\u7684\u8ba1\u7b97\u6a21\u578b\u3002", "method": "\u7406\u8bba\u6784\u5efa\u5e76\u6570\u503c\u5b9e\u73b0\u4e86\u57fa\u4e8eRTD\u7684RC\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u624b\u5199\u6570\u5b57\u5206\u7c7b\u548cFruit~360\u6570\u636e\u96c6\u7684\u5bf9\u8c61\u8bc6\u522b\u3002", "result": "\u8be5\u67b6\u6784\u5728\u6027\u80fd\u8868\u73b0\u826f\u597d\uff0c\u540c\u65f6\u9075\u5faa\u4e0b\u4e00\u4ee3RC\u539f\u5219\uff0c\u7528\u786e\u5b9a\u6027\u975e\u7ebf\u6027\u53d8\u6362\u53d6\u4ee3\u968f\u673a\u8fde\u63a5\u3002", "conclusion": "RTD-based RC\u67b6\u6784\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15150", "abs": "https://arxiv.org/abs/2507.15150", "authors": ["Aayush Atul Verma", "Arpitsinh Vaghela", "Bharatesh Chakravarthi", "Kaustav Chanda", "Yezhou Yang"], "title": "Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection", "comment": null, "summary": "Event-based sensors offer high temporal resolution and low latency by\ngenerating sparse, asynchronous data. However, converting this irregular data\ninto dense tensors for use in standard neural networks diminishes these\ninherent advantages, motivating research into graph representations. While such\nmethods preserve sparsity and support asynchronous inference, their performance\non downstream tasks remains limited due to suboptimal modeling of\nspatiotemporal dynamics. In this work, we propose a novel spatiotemporal\nmultigraph representation to better capture spatial structure and temporal\nchanges. Our approach constructs two decoupled graphs: a spatial graph\nleveraging B-spline basis functions to model global structure, and a temporal\ngraph utilizing motion vector-based attention for local dynamic changes. This\ndesign enables the use of efficient 2D kernels in place of computationally\nexpensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM\ndatasets for event-based object detection, achieving over a 6% improvement in\ndetection accuracy compared to previous graph-based works, with a 5x speedup,\nreduced parameter count, and no increase in computational cost. These results\nhighlight the effectiveness of structured graph modeling for asynchronous\nvision. Project page: eventbasedvision.github.io/eGSMV.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u591a\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u6570\u636e\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4e8b\u4ef6\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\u4e14\u5f02\u6b65\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u5176\u8f6c\u6362\u4e3a\u5bc6\u96c6\u5f20\u91cf\u4f1a\u4e27\u5931\u5176\u4f18\u52bf\uff0c\u800c\u73b0\u6709\u56fe\u8868\u793a\u65b9\u6cd5\u5bf9\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u89e3\u8026\u7684\u7a7a\u95f4\u56fe\uff08\u5229\u7528B\u6837\u6761\u57fa\u51fd\u6570\u5efa\u6a21\u5168\u5c40\u7ed3\u6784\uff09\u548c\u65f6\u95f4\u56fe\uff08\u57fa\u4e8e\u8fd0\u52a8\u5411\u91cf\u7684\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u5c40\u90e8\u52a8\u6001\u53d8\u5316\uff09\uff0c\u66ff\u4ee3\u8ba1\u7b97\u6602\u8d35\u76843D\u6838\u3002", "result": "\u5728Gen1\u548ceTraM\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u53476%\uff0c\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u53c2\u6570\u51cf\u5c11\u4e14\u8ba1\u7b97\u6210\u672c\u4e0d\u53d8\u3002", "conclusion": "\u7ed3\u6784\u5316\u56fe\u5efa\u6a21\u5728\u5f02\u6b65\u89c6\u89c9\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.15269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15269", "abs": "https://arxiv.org/abs/2507.15269", "authors": ["Fangqiu Yi", "Jingyu Xu", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Conditional Video Generation for High-Efficiency Video Compression", "comment": null, "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u4ece\u7a00\u758f\u4fe1\u53f7\u4e2d\u91cd\u5efa\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5229\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4f18\u5316\u89c6\u9891\u538b\u7f29\u7684\u611f\u77e5\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u5c06\u89c6\u9891\u538b\u7f29\u91cd\u6784\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u5f15\u5165\u591a\u7c92\u5ea6\u6761\u4ef6\u3001\u7d27\u51d1\u8868\u793a\u548c\u591a\u6761\u4ef6\u8bad\u7ec3\u6a21\u5757\u3002", "result": "\u5728\u611f\u77e5\u8d28\u91cf\u6307\u6807\uff08\u5982FVD\u548cLPIPS\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u5c24\u5176\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u611f\u77e5\u4f18\u5316\u7684\u89c6\u9891\u538b\u7f29\uff0c\u4e3a\u9ad8\u6548\u4f20\u8f93\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15285", "abs": "https://arxiv.org/abs/2507.15285", "authors": ["Lazaro Janier Gonzalez-Soler", "Maciej Salwowski", "Christoph Busch"], "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems", "comment": "Submitted to IEEE-TIFS", "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u68c0\u6d4b\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfCNN\u3002", "motivation": "\u968f\u7740\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u68c0\u6d4b\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u653b\u51fb\u624b\u6bb5\u4e5f\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u653b\u51fb\u548c\u73af\u5883\u6761\u4ef6\uff0c\u4e14\u6570\u636e\u6536\u96c6\u9762\u4e34\u9690\u79c1\u548c\u591a\u6837\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7269\u7406\u5448\u73b0\u653b\u51fb\u548c\u6570\u5b57\u53d8\u5f62\u653b\u51fb\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u7684\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\uff0c\u4e14\u65e0\u9700\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63d0\u5347\u653b\u51fb\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2507.15349", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15349", "abs": "https://arxiv.org/abs/2507.15349", "authors": ["Zehua Cheng", "Rui Sun", "Jiahao Sun", "Yike Guo"], "title": "Scaling Decentralized Learning with FLock", "comment": null, "summary": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data.", "AI": {"tldr": "FLock\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b89\u5168\u9ad8\u6548\u7684\u534f\u4f5c\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4e2d\u5fc3\u5316\u6f0f\u6d1e\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u9700\u8981\u4e2d\u5fc3\u670d\u52a1\u5668\uff0c\u5b58\u5728\u5355\u70b9\u653b\u51fb\u548c\u4e2d\u6bd2\u653b\u51fb\u7684\u98ce\u9669\uff0c\u4e14\u5728\u5f02\u6784\u3001\u65e0\u4fe1\u4efb\u73af\u5883\u4e2d\u5fae\u8c0370B\u53c2\u6570\u6a21\u578b\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u74f6\u9888\u3002", "method": "FLock\u7ed3\u5408\u533a\u5757\u94fe\u4fe1\u4efb\u5c42\u548c\u7ecf\u6d4e\u6fc0\u52b1\u673a\u5236\uff0c\u66ff\u4ee3\u4e2d\u5fc3\u805a\u5408\u5668\uff0c\u63d0\u4f9b\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u7684\u534f\u4f5c\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFLock\u80fd\u62b5\u5fa1\u540e\u95e8\u4e2d\u6bd2\u653b\u51fb\uff0c\u51cf\u5c11\u5bf9\u6297\u653b\u51fb\u6210\u529f\u7387>68%\uff0c\u5e76\u5b9e\u73b0\u8de8\u9886\u57df\u6cdb\u5316\u4f18\u52bf\u3002", "conclusion": "FLock\u4e3a\u53bb\u4e2d\u5fc3\u5316LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15566", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15566", "abs": "https://arxiv.org/abs/2507.15566", "authors": ["Pieter Smet", "Martina Doneda", "Ettore Lanzarone", "Giuliana Carello"], "title": "Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy", "comment": null, "summary": "The availability of downstream resources plays a critical role in planning\nthe admission of patients undergoing elective surgery, with inpatient beds\nbeing one of the most crucial resources. When planning patient admissions,\npredictions on their length-of-stay (LOS) made by machine learning (ML) models\nare used to ensure bed availability. However, the actual LOS for each patient\nmay differ considerably from the predicted value, potentially making the\nschedule infeasible. To address such infeasibilities, rescheduling strategies\nthat take advantage of operational flexibility can be implemented. For example,\nadjustments may include postponing admission dates, relocating patients to\ndifferent wards, or even transferring patients who are already admitted. The\ncommon assumption is that more accurate LOS predictions reduce the impact of\nrescheduling. However, training ML models that can make such accurate\npredictions can be costly. Building on previous work that proposed simulated\n\\ac{ml} for evaluating data-driven approaches, this paper explores the\nrelationship between LOS prediction accuracy and rescheduling flexibility\nacross various corrective policies. Specifically, we examine the most effective\npatient rescheduling strategies under LOS prediction errors to prevent bed\noverflows while optimizing resource utilization.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f4f\u9662\u65f6\u95f4\uff08LOS\uff09\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u5e8a\u4f4d\u8c03\u5ea6\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u4e86\u5728\u9884\u6d4b\u8bef\u5dee\u4e0b\u6700\u6709\u6548\u7684\u60a3\u8005\u8c03\u5ea6\u7b56\u7565\u4ee5\u907f\u514d\u5e8a\u4f4d\u6ea2\u51fa\u5e76\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u4e0b\u6e38\u8d44\u6e90\uff08\u5982\u5e8a\u4f4d\uff09\u5bf9\u8ba1\u5212\u9009\u62e9\u6027\u624b\u672f\u60a3\u8005\u5165\u9662\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645LOS\u4e0e\u9884\u6d4b\u503c\u53ef\u80fd\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u5bfc\u81f4\u8c03\u5ea6\u4e0d\u53ef\u884c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7814\u7a76LOS\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u8c03\u5ea6\u7075\u6d3b\u6027\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u66f4\u6709\u6548\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "method": "\u5229\u7528\u6a21\u62df\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u6570\u636e\u9a71\u52a8\u7b56\u7565\uff0c\u5206\u6790\u4e0d\u540c\u7ea0\u6b63\u653f\u7b56\u4e0bLOS\u9884\u6d4b\u8bef\u5dee\u4e0e\u8c03\u5ea6\u7075\u6d3b\u6027\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u51c6\u786e\u7684LOS\u9884\u6d4b\u53ef\u4ee5\u51cf\u5c11\u8c03\u5ea6\u8c03\u6574\u7684\u5f71\u54cd\uff0c\u4f46\u8bad\u7ec3\u9ad8\u7cbe\u5ea6ML\u6a21\u578b\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\uff0c\u53ef\u4ee5\u5728LOS\u9884\u6d4b\u8bef\u5dee\u4e0b\u6709\u6548\u907f\u514d\u5e8a\u4f4d\u6ea2\u51fa\u5e76\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.15614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15614", "abs": "https://arxiv.org/abs/2507.15614", "authors": ["Edward Holmberg", "Pujan Pokhrel", "Maximilian Zoch", "Elias Ioup", "Ken Pathak", "Steven Sloan", "Kendall Niles", "Jay Ratcliff", "Maik Flanagin", "Christian Guetl", "Julian Simeonov", "Mahdi Abdelguerfi"], "title": "Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting", "comment": "10 pages, 8 figures", "summary": "Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but\nare too computationally intensive for on-the-fly decision-making during flood\nevents. The central challenge is to accelerate these simulations without\nsacrificing accuracy. This paper introduces a deep learning surrogate that\ntreats HEC-RAS not as a solver but as a data-generation engine. We propose a\nhybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)\nto capture short-term temporal dynamics with a Geometry-Aware Fourier Neural\nOperator (Geo-FNO) to model long-range spatial dependencies along a river\nreach. The model learns underlying physics implicitly from a minimal\neight-channel feature vector encoding dynamic state, static geometry, and\nboundary forcings extracted directly from native HEC-RAS files. Trained on 67\nreaches of the Mississippi River Basin, the surrogate was evaluated on a\nyear-long, unseen hold-out simulation. Results show the model achieves a strong\npredictive accuracy, with a median absolute stage error of 0.31 feet.\nCritically, for a full 67-reach ensemble forecast, our surrogate reduces the\nrequired wall-clock time from 139 minutes to 40 minutes, a speedup of nearly\n3.5 times over the traditional solver. The success of this data-driven approach\ndemonstrates that robust feature engineering can produce a viable, high-speed\nreplacement for conventional hydraulic models, improving the computational\nfeasibility of large-scale ensemble flood forecasting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u901fHEC-RAS\u6d2a\u6c34\u6a21\u62df\uff0c\u7ed3\u5408GRU\u548cGeo-FNO\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\uff08\u5982HEC-RAS\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u51b3\u7b56\uff0c\u9700\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u6a21\u62df\u3002", "method": "\u91c7\u7528\u6df7\u5408\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u7ed3\u5408GRU\u6355\u6349\u77ed\u671f\u65f6\u95f4\u52a8\u6001\u548cGeo-FNO\u5efa\u6a21\u957f\u7a0b\u7a7a\u95f4\u4f9d\u8d56\uff0c\u4eceHEC-RAS\u6587\u4ef6\u4e2d\u63d0\u53d6\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u5bc6\u897f\u897f\u6bd4\u6cb3\u6d41\u57df67\u4e2a\u6cb3\u6bb5\u4e0a\u9a8c\u8bc1\uff0c\u4e2d\u4f4d\u7edd\u5bf9\u6c34\u4f4d\u8bef\u5dee\u4e3a0.31\u82f1\u5c3a\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ece139\u5206\u949f\u7f29\u77ed\u81f340\u5206\u949f\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u53ef\u66ff\u4ee3\u4f20\u7edf\u6c34\u529b\u6a21\u578b\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u6d2a\u6c34\u9884\u62a5\u7684\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
