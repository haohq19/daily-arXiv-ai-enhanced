{"id": "2510.03337", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.03337", "abs": "https://arxiv.org/abs/2510.03337", "authors": ["Andrey A. Lebedev", "Victor B. Kazantsev", "Sergey V. Stasenko"], "title": "Error correction in multiclass image classification of facial emotion on unbalanced samples", "comment": null, "summary": "This paper considers the problem of error correction in multi-class\nclassification of face images on unbalanced samples. The study is based on the\nanalysis of a data frame containing images labeled by seven different emotional\nstates of people of different ages. Particular attention is paid to the problem\nof class imbalance, in which some emotions significantly prevail over others.\nTo solve the classification problem, a neural network model based on LSTM with\nan attention mechanism focusing on key areas of the face that are informative\nfor emotion recognition is used. As part of the experiments, the model is\ntrained on all possible configurations of subsets of six classes with\nsubsequent error correction for the seventh class, excluded at the training\nstage. The results show that correction is possible for all classes, although\nthe degree of success varies: some classes are better restored, others are\nworse. In addition, on the test sample, when correcting some classes, an\nincrease in key quality metrics for small classes was recorded, which indicates\nthe promise of the proposed approach in solving applied problems related to the\nsearch for rare events, for example, in anti-fraud systems. Thus, the proposed\nmethod can be effectively applied in facial expression analysis systems and in\ntasks requiring stable classification under skewed class distribution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u9762\u90e8\u8868\u60c5\u5206\u7c7b\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8bef\u5dee\u6821\u6b63\u6280\u672f\u63d0\u9ad8\u5bf9\u5c0f\u7c7b\u522b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u8868\u60c5\u5206\u7c7b\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5176\u4e2d\u67d0\u4e9b\u60c5\u7eea\u7c7b\u522b\u663e\u8457\u591a\u4e8e\u5176\u4ed6\u7c7b\u522b\uff0c\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLSTM\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u4e8e\u9762\u90e8\u5173\u952e\u533a\u57df\uff0c\u5728\u516d\u7c7b\u5b50\u96c6\u4e0a\u8bad\u7ec3\u540e\u5bf9\u7b2c\u4e03\u7c7b\u8fdb\u884c\u8bef\u5dee\u6821\u6b63\u3002", "result": "\u6240\u6709\u7c7b\u522b\u90fd\u80fd\u8fdb\u884c\u6821\u6b63\uff0c\u4f46\u6548\u679c\u5404\u5f02\uff1b\u6d4b\u8bd5\u96c6\u4e0a\u5c0f\u7c7b\u522b\u7684\u5173\u952e\u8d28\u91cf\u6307\u6807\u6709\u6240\u63d0\u5347\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7a00\u6709\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u5206\u6790\u7cfb\u7edf\uff0c\u5728\u7c7b\u522b\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u7a33\u5b9a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53cd\u6b3a\u8bc8\u7b49\u9700\u8981\u68c0\u6d4b\u7a00\u6709\u4e8b\u4ef6\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.03680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding.", "AI": {"tldr": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5728\u6307\u4ee4\u8c03\u4f18\u540e\u5b58\u5728<eos>\u6ea2\u51fa\u95ee\u9898\uff1a\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u54cd\u5e94\u53cd\u800c\u53d8\u77ed\uff0c\u63d0\u524d\u7ec8\u6b62\u6216\u9000\u5316\u4e3a<eos>\u6807\u8bb0\u6d41\u3002\u4f5c\u8005\u63d0\u51fa\u5f69\u8679\u586b\u5145\u65b9\u6cd5\uff0c\u7528\u5faa\u73af\u7684\u4e0d\u540c\u586b\u5145\u6807\u8bb0\u66ff\u4ee3\u91cd\u590d<eos>\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u6307\u4ee4\u8c03\u4f18\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728<eos>\u6ea2\u51fa\u6f0f\u6d1e\uff0c\u5373\u968f\u7740\u5206\u914d\u7684\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u54cd\u5e94\u53cd\u800c\u53d8\u77ed\uff0c\u63d0\u524d\u7ec8\u6b62\u6216\u9000\u5316\u4e3a<eos>\u6807\u8bb0\u6d41\u3002\u8fd9\u4e2a\u95ee\u9898\u5728\u5b9e\u8df5\u4e2d\u5df2\u88ab\u6ce8\u610f\u5230\u4f46\u672a\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u63d0\u51fa\u5f69\u8679\u586b\u5145\u65b9\u6cd5\uff0c\u7528\u5faa\u73af\u7684\u4e0d\u540c\u586b\u5145\u6807\u8bb0\u66ff\u4ee3\u91cd\u590d\u7684<eos>\u5360\u4f4d\u7b26\uff0c\u5206\u6563\u6982\u7387\u8d28\u91cf\uff0c\u6253\u7834<eos>\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002\u8be5\u65b9\u6cd5\u53ef\u9ad8\u6548\u96c6\u6210\u5230\u73b0\u6709\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e2d\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u548c\u5355\u4e2aepoch\u7684LoRA\u5fae\u8c03\u3002", "result": "\u5f69\u8679\u586b\u5145\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u4ec5\u9700\u4e03\u4e2a\u586b\u5145\u6807\u8bb0\u5373\u53ef\u9632\u6b62\u63d0\u524d\u7ec8\u6b62\u3002\u8be5\u65b9\u6cd5\u5728\u73b0\u6709\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u96c6\u6210\u6548\u7387\u9ad8\uff0c\u5355epoch LoRA\u5fae\u8c03\u5373\u53ef\u83b7\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5f69\u8679\u586b\u5145\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684<eos>\u6ea2\u51fa\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.03561", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03561", "abs": "https://arxiv.org/abs/2510.03561", "authors": ["Adam Filipek"], "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models", "comment": "25 pages, 13 figures", "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity ($O(L^2)$) with respect to sequence length $L$.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to\nthe number of interactions $N$. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size.", "AI": {"tldr": "RxT\u662f\u4e00\u79cd\u65b0\u578bTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u8303\u5f0f\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u5bf9\u8bddAI\u4e2d\u7684\u72b6\u6001\u4fdd\u6301\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5c06\u5bf9\u8bdd\u6210\u672c\u4ece\u4e8c\u6b21\u65b9\u964d\u4f4e\u5230\u7ebf\u6027\u3002", "motivation": "\u4f20\u7edfTransformer\u5728\u5bf9\u8bdd\u5e94\u7528\u4e2d\u5b58\u5728\u72b6\u6001\u4fdd\u6301\u56f0\u96be\u548c\u4e8c\u6b21\u65b9\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u5bf9\u8bdd\u6210\u672c\u9ad8\u6602\u3001\u5ef6\u8fdf\u4e25\u91cd\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u8303\u5f0f\uff0c\u5c06\u6bcf\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u4f5c\u4e3a\u79bb\u6563\u4e8b\u4ef6\u5904\u7406\uff0c\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u77ed\u671f\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u5668-\u89e3\u7801\u5668\u548c\u5185\u5b58\u6ce8\u610f\u529b\u7f51\u7edc\u5f02\u6b65\u66f4\u65b0\u8bb0\u5fc6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u663e\u793a\uff0cRxT\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u66f4\u4f18\u6027\u80fd\u548c\u6052\u5b9a\u65f6\u95f4\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "RxT\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u6709\u72b6\u6001\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u957f\u5bf9\u8bdd\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6269\u5c55\u52a8\u6001\u3002"}}
{"id": "2510.03577", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.03577", "abs": "https://arxiv.org/abs/2510.03577", "authors": ["Ikram Belmadani", "Parisa Nazari Hashemi", "Thomas Sebbag", "Benoit Favre", "Guillaume Fortier", "Solen Quiniou", "Emmanuel Morin", "Richard Dufour"], "title": "LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction", "comment": "in French language", "summary": "This work presents our participation in the EvalLLM 2025 challenge on\nbiomedical Named Entity Recognition (NER) and health event extraction in French\n(few-shot setting). For NER, we propose three approaches combining large\nlanguage models (LLMs), annotation guidelines, synthetic data, and\npost-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating\nautomatic selection of 10 examples and a summary of the annotation guidelines\ninto the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic\ncorpus and then verified by an LLM in post-processing, and (3) the open LLM\nLLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event\nextraction uses the same ICL strategy with GPT-4.1, reusing the guideline\nsummary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for\nNER and 15.02% for event extraction, highlighting the importance of\nwell-crafted prompting to maximize performance in very low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728EvalLLM 2025\u6311\u6218\u8d5b\u4e2d\u9488\u5bf9\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5065\u5eb7\u4e8b\u4ef6\u63d0\u53d6\u7684\u4e09\u79cd\u65b9\u6cd5\uff0c\u4e3b\u8981\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001GLiNER\u7cfb\u7edf\u5fae\u8c03\u548cLLaMA-3.1\u5fae\u8c03\uff0c\u7ed3\u679c\u663e\u793aGPT-4.1\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u9886\u57df\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5065\u5eb7\u4e8b\u4ef6\u63d0\u53d6\u95ee\u9898\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\uff1a(1) GPT-4.1\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5305\u542b\u81ea\u52a8\u9009\u62e910\u4e2a\u793a\u4f8b\u548c\u6807\u6ce8\u6307\u5357\u6458\u8981\uff1b(2) GLiNER\u7cfb\u7edf\u5728\u5408\u6210\u8bed\u6599\u4e0a\u5fae\u8c03\uff0c\u540e\u7ecfLLM\u9a8c\u8bc1\uff1b(3) LLaMA-3.1-8B\u5728\u5408\u6210\u8bed\u6599\u4e0a\u5fae\u8c03\u3002\u4e8b\u4ef6\u63d0\u53d6\u91c7\u7528\u76f8\u540c\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u3002", "result": "GPT-4.1\u8868\u73b0\u6700\u4f73\uff0cNER\u7684\u5b8fF1\u4e3a61.53%\uff0c\u4e8b\u4ef6\u63d0\u53d6\u4e3a15.02%\uff0c\u8868\u660e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5728\u6cd5\u8bed\u751f\u7269\u533b\u5b66NER\u548c\u4e8b\u4ef6\u63d0\u53d6\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0cGPT-4.1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u6548\u679c\u6700\u597d\uff0c\u5f3a\u8c03\u4e86\u63d0\u793a\u5de5\u7a0b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.03595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03595", "abs": "https://arxiv.org/abs/2510.03595", "authors": ["Haikang Deng", "Po-Nien Kung", "Nanyun Peng"], "title": "Decoupling Task-Solving and Output Formatting in LLM Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly adept at following instructions\ncontaining task descriptions to solve complex problems, such as mathematical\nreasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow\nmore complex, models often struggle to adhere to all instructions. This\ndifficulty is especially common when instructive prompts intertwine reasoning\ndirectives -- specifying what the model should solve -- with rigid formatting\nrequirements that dictate how the solution must be presented. The entanglement\ncreates competing goals for the model, suggesting that more explicit separation\nof these two aspects could lead to improved performance. To this front, we\nintroduce Deco-G, a decoding framework that explicitly decouples format\nadherence from task solving. Deco-G handles format compliance with a separate\ntractable probabilistic model (TPM), while prompts LLMs with only task\ninstructions. At each decoding step, Deco-G combines next token probabilities\nfrom the LLM with the TPM calculated format compliance likelihood to form the\noutput probability. To make this approach both practical and scalable for\nmodern instruction-tuned LLMs, we introduce three key innovations:\ninstruction-aware distillation, a flexible trie-building algorithm, and HMM\nstate pruning for computational efficiency. We demonstrate the effectiveness of\nDeco-G across a wide range of tasks with diverse format requirements, including\nmathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,\nour approach yields 1.0% to 6.0% relative gain over regular prompting practice\nwith guaranteed format compliance.", "AI": {"tldr": "Deco-G\u662f\u4e00\u4e2a\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u683c\u5f0f\u9075\u5faa\u4e0e\u4efb\u52a1\u89e3\u51b3\u89e3\u8026\uff0c\u4f7f\u7528\u5355\u72ec\u7684\u6982\u7387\u6a21\u578b\u5904\u7406\u683c\u5f0f\u5408\u89c4\u6027\uff0c\u4ece\u800c\u63d0\u5347LLM\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u63d0\u793a\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0cLLM\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u9075\u5faa\u6240\u6709\u6307\u4ee4\uff0c\u7279\u522b\u662f\u5f53\u63a8\u7406\u6307\u4ee4\u4e0e\u4e25\u683c\u7684\u683c\u5f0f\u8981\u6c42\u4ea4\u7ec7\u5728\u4e00\u8d77\u65f6\uff0c\u8fd9\u79cd\u7ea0\u7f20\u4f1a\u4e3a\u6a21\u578b\u521b\u9020\u7ade\u4e89\u76ee\u6807\u3002", "method": "\u5f15\u5165Deco-G\u89e3\u7801\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u5904\u7406\u7684\u6982\u7387\u6a21\u578b(TPM)\u5904\u7406\u683c\u5f0f\u5408\u89c4\u6027\uff0c\u540c\u65f6\u4ec5\u7528\u4efb\u52a1\u6307\u4ee4\u63d0\u793aLLM\u3002\u901a\u8fc7\u6307\u4ee4\u611f\u77e5\u84b8\u998f\u3001\u7075\u6d3b\u7684trie\u6784\u5efa\u7b97\u6cd5\u548cHMM\u72b6\u6001\u526a\u679d\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001LLM-as-a-judge\u548c\u4e8b\u4ef6\u53c2\u6570\u63d0\u53d6\u7b49\u591a\u6837\u5316\u683c\u5f0f\u8981\u6c42\u7684\u4efb\u52a1\u4e2d\uff0cDeco-G\u76f8\u6bd4\u5e38\u89c4\u63d0\u793a\u65b9\u6cd5\u5b9e\u73b0\u4e861.0%\u52306.0%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4fdd\u8bc1\u683c\u5f0f\u5408\u89c4\u6027\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u5206\u79bb\u683c\u5f0f\u9075\u5faa\u548c\u4efb\u52a1\u89e3\u51b3\uff0cDeco-G\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u683c\u5f0f\u5408\u89c4\u6027\u3002"}}
{"id": "2510.03266", "categories": ["cs.LG", "stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.03266", "abs": "https://arxiv.org/abs/2510.03266", "authors": ["Bharat Sharma", "Jitendra Kumar"], "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model", "comment": null, "summary": "Climate anomalies significantly impact terrestrial carbon cycle dynamics,\nnecessitating robust methods for detecting and analyzing anomalous behavior in\nplant productivity. This study presents a novel application of variational\nautoencoders (VAE) for identifying extreme events in gross primary productivity\n(GPP) from Community Earth System Model version 2 simulations across four AR6\nregions in the Continental United States. We compare VAE-based anomaly\ndetection with traditional singular spectral analysis (SSA) methods across\nthree time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.\nThe VAE architecture employs three dense layers and a latent space with an\ninput sequence length of 12 months, trained on a normalized GPP time series to\nreconstruct the GPP and identifying anomalies based on reconstruction errors.\nExtreme events are defined using 5th percentile thresholds applied to both VAE\nand SSA anomalies. Results demonstrate strong regional agreement between VAE\nand SSA methods in spatial patterns of extreme event frequencies, despite VAE\nproducing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA\nacross regions and periods). Both methods reveal increasing magnitudes and\nfrequencies of negative carbon cycle extremes toward 2050-80, particularly in\nWestern and Central North America. The VAE approach shows comparable\nperformance to established SSA techniques, while offering computational\nadvantages and enhanced capability for capturing non-linear temporal\ndependencies in carbon cycle variability. Unlike SSA, the VAE method does not\nrequire one to define the periodicity of the signals in the data; it discovers\nthem from the data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u9646\u5730\u78b3\u5faa\u73af\u4e2d\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u5947\u5f02\u8c31\u5206\u6790(SSA)\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u6c14\u5019\u5f02\u5e38\u5bf9\u9646\u5730\u78b3\u5faa\u73af\u52a8\u6001\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u690d\u7269\u751f\u4ea7\u529b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5bc6\u96c6\u5c42\u548c\u6f5c\u5728\u7a7a\u95f4\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a12\u4e2a\u6708\uff0c\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u8bc6\u522bGPP\u5f02\u5e38\u3002\u4e0eSSA\u65b9\u6cd5\u5728\u4e09\u4e2a\u65f6\u95f4\u6bb5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "VAE\u548cSSA\u65b9\u6cd5\u5728\u6781\u7aef\u4e8b\u4ef6\u9891\u7387\u7684\u7a7a\u95f4\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u4e00\u81f4\u6027\uff0c\u4f46VAE\u4ea7\u751f\u66f4\u9ad8\u7684\u9608\u503c\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u793a\u52302050-80\u5e74\uff0c\u5317\u7f8e\u897f\u90e8\u548c\u4e2d\u90e8\u7684\u8d1f\u78b3\u5faa\u73af\u6781\u7aef\u4e8b\u4ef6\u5e45\u5ea6\u548c\u9891\u7387\u589e\u52a0\u3002", "conclusion": "VAE\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0eSSA\u76f8\u5f53\uff0c\u4f46\u5177\u6709\u8ba1\u7b97\u4f18\u52bf\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u78b3\u5faa\u73af\u53d8\u7387\u4e2d\u7684\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4e14\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u6570\u636e\u7684\u5468\u671f\u6027\u3002"}}
{"id": "2510.04017", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aZephyrus\u7684\u667a\u80fd\u5929\u6c14\u79d1\u5b66\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5929\u6c14\u6570\u636e\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u591a\u8f6e\u5bf9\u8bdd\u5f0f\u7684\u5929\u6c14\u6570\u636e\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5929\u6c14\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\uff0c\u9700\u8981\u6865\u63a5\u8fd9\u4e00\u6280\u672f\u9e3f\u6c9f\u3002", "method": "\u6784\u5efa\u4e86ZephyrusWorld\u73af\u5883\uff0c\u5305\u542bWeatherBench 2\u6570\u636e\u96c6\u63a5\u53e3\u3001\u5730\u7406\u67e5\u8be2\u3001\u5929\u6c14\u9884\u62a5\u548c\u6c14\u5019\u6a21\u62df\u7b49\u5de5\u5177\uff0c\u8bbe\u8ba1\u4e86\u591a\u8f6eLLM\u5929\u6c14\u4ee3\u7406Zephyrus\u3002", "result": "\u5728ZephyrusBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cZephyrus\u4ee3\u7406\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u6a21\u578b\u6b63\u786e\u7387\u63d0\u9ad8\u4e8635\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u5728\u66f4\u56f0\u96be\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u4f3c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u548c\u5929\u6c14\u79d1\u5b66\u5de5\u5177\uff0c\u4f46\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4ecd\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2510.03805", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03805", "abs": "https://arxiv.org/abs/2510.03805", "authors": ["Canhui Wu", "Qiong Cao", "Chang Li", "Zhenfang Wang", "Chao Xue", "Yuwei Fan", "Wei Xi", "Xiaodong He"], "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models", "comment": "20pages, 7 figures", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks\nbut often suffer from excessive verbosity, known as \"overthinking.\" Existing\nsolutions via reinforcement learning (RL) typically penalize generated tokens\nto promote conciseness. However, these methods encounter two challenges:\nresponses with fewer tokens do not always correspond to fewer reasoning steps,\nand models may develop hacking behavior in later stages of training by\ndiscarding reasoning steps to minimize token usage. In this work, we introduce\n\\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more\nefficient reasoning by favoring compact reasoning steps. Our step-aware reward\nfunction prioritizes correctness while imposing penalties for redundant steps,\nand withholds rewards for incorrect responses to prevent the reinforcement of\nerroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when\nthe length of any output step exceeds the upper limit, we halt updates to\nprevent hacking behavior caused by merging steps. Extensive experiments across\nfour reasoning benchmarks demonstrate that SP achieves state-of-the-art\naccuracy while significantly reducing response length. For instance, on AIME24,\nSP reduces token usage by \\textbf{69.7\\%}.", "AI": {"tldr": "Step Pruner (SP) \u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u6765\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u901a\u8fc7\u60e9\u7f5a\u751f\u6210token\u6765\u4fc3\u8fdb\u7b80\u6d01\u6027\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u66f4\u5c11\u7684token\u4e0d\u4e00\u5b9a\u5bf9\u5e94\u66f4\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6a21\u578b\u53ef\u80fd\u5728\u8bad\u7ec3\u540e\u671f\u901a\u8fc7\u4e22\u5f03\u63a8\u7406\u6b65\u9aa4\u6765\u6700\u5c0f\u5316token\u4f7f\u7528\u3002", "method": "\u63d0\u51faStep Pruner (SP)\u6846\u67b6\uff0c\u4f7f\u7528\u6b65\u9aa4\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u540c\u65f6\u60e9\u7f5a\u5197\u4f59\u6b65\u9aa4\uff0c\u5bf9\u9519\u8bef\u54cd\u5e94\u4e0d\u7ed9\u4e88\u5956\u52b1\u4ee5\u9632\u6b62\u5f3a\u5316\u9519\u8bef\u63a8\u7406\u3002\u8fd8\u63d0\u51fa\u52a8\u6001\u505c\u6b62\u673a\u5236\uff0c\u5f53\u4efb\u4f55\u8f93\u51fa\u6b65\u9aa4\u957f\u5ea6\u8d85\u8fc7\u4e0a\u9650\u65f6\u505c\u6b62\u66f4\u65b0\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSP\u5728\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002\u5728AIME24\u4e0a\uff0cSP\u51cf\u5c11\u4e8669.7%\u7684token\u4f7f\u7528\u3002", "conclusion": "Step Pruner\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u5173\u6ce8\u63a8\u7406\u6b65\u9aa4\u800c\u975e\u5355\u7eaftoken\u6570\u91cf\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2510.04033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedLog\u534f\u8bae\uff0c\u7528\u4e8e\u4e34\u5e8aAI\u7684\u4e8b\u4ef6\u7ea7\u65e5\u5fd7\u8bb0\u5f55\uff0c\u7c7b\u4f3c\u4e8e\u7cfb\u7edf\u65e5\u5fd7(syslog)\u5728\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597AI\u7f3a\u4e4f\u6807\u51c6\u5316\u4f7f\u7528\u8bb0\u5f55\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u7597\u9886\u57df\u5feb\u901f\u589e\u957f\u7684\u4e34\u5e8aAI\u5806\u6808\u7f3a\u4e4f\u50cfsyslog\u90a3\u6837\u7684\u6807\u51c6\u65e5\u5fd7\u534f\u8bae\uff0c\u5bfc\u81f4\u96be\u4ee5\u8ffd\u8e2aAI\u6a21\u578b\u7684\u4f7f\u7528\u60c5\u51b5\u3001\u6d4b\u91cf\u771f\u5b9e\u6027\u80fd\u3001\u68c0\u6d4b\u4e0d\u826f\u4e8b\u4ef6\u6216\u7ea0\u6b63\u504f\u5dee\u3002", "method": "\u8bbe\u8ba1\u4e86MedLog\u534f\u8bae\uff0c\u5305\u542b9\u4e2a\u6838\u5fc3\u5b57\u6bb5\uff1aheader\u3001model\u3001user\u3001target\u3001inputs\u3001artifacts\u3001outputs\u3001outcomes\u548cfeedback\uff0c\u652f\u6301\u98ce\u9669\u91c7\u6837\u3001\u751f\u547d\u5468\u671f\u611f\u77e5\u4fdd\u7559\u7b56\u7565\u548c\u5199\u540e\u7f13\u5b58\u3002", "result": "MedLog\u80fd\u591f\u4e3a\u4e34\u5e8aAI\u4f7f\u7528\u63d0\u4f9b\u7ed3\u6784\u5316\u548c\u4e00\u81f4\u7684\u8bb0\u5f55\uff0c\u652f\u6301\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u8be6\u7ec6\u8ffd\u8e2a\uff0c\u5e76\u4fc3\u8fdb\u65b0\u6570\u636e\u5e93\u548c\u8f6f\u4ef6\u7684\u53d1\u5c55\u3002", "conclusion": "MedLog\u53ef\u4ee5\u5b9e\u73b0\u533b\u7597AI\u7684\u6301\u7eed\u76d1\u63a7\u3001\u5ba1\u8ba1\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u4e3a\u65b0\u578b\u6570\u5b57\u6d41\u884c\u75c5\u5b66\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.03999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03999", "abs": "https://arxiv.org/abs/2510.03999", "authors": ["Yang Xu", "Xuanming Zhang", "Min-Hsuan Yeh", "Jwala Dhamala", "Ousmane Dia", "Rahul Gupta", "Yixuan Li"], "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions", "comment": null, "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u6a21\u62df\u6846\u67b6\u6765\u8bc4\u4f30LLM\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u6b3a\u9a97\u884c\u4e3a\uff0c\u53d1\u73b0\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u4f1a\u968f\u538b\u529b\u589e\u52a0\uff0c\u5e76\u6301\u7eed\u524a\u5f31\u76d1\u7763\u8005\u4fe1\u4efb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u5355\u8f6e\u63d0\u793a\u4e0b\u7684LLM\u6b3a\u9a97\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6355\u6349\u6b3a\u9a97\u7b56\u7565\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u53d1\u5c55\u8fc7\u7a0b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4fe1\u4efb\u654f\u611f\u573a\u666f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a\u6267\u884c\u8005\u667a\u80fd\u4f53\u5b8c\u6210\u4efb\u52a1\uff0c\u76d1\u7763\u8005\u667a\u80fd\u4f53\u8bc4\u4f30\u8fdb\u5c55\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u72ec\u7acb\u6b3a\u9a97\u5ba1\u8ba1\u5458\u5ba1\u67e5\u5b8c\u6574\u8f68\u8ff9\u4ee5\u8bc6\u522b\u6b3a\u9a97\u884c\u4e3a\u3002\u572811\u4e2a\u524d\u6cbf\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u968f\u4e8b\u4ef6\u538b\u529b\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u6301\u7eed\u524a\u5f31\u76d1\u7763\u8005\u4fe1\u4efb\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u9690\u7792\u3001\u6a21\u68f1\u4e24\u53ef\u548c\u4f2a\u9020\u7b49\u4e0d\u540c\u6b3a\u9a97\u7b56\u7565\u3002", "conclusion": "\u6b3a\u9a97\u662f\u957f\u671f\u4ea4\u4e92\u4e2d\u51fa\u73b0\u7684\u98ce\u9669\uff0c\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u672a\u6765LLM\u5728\u771f\u5b9e\u4e16\u754c\u4fe1\u4efb\u654f\u611f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication.", "AI": {"tldr": "\u63d0\u51fa\u4e86Moral Anchor System (MAS)\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u9884\u6d4b\u548c\u7f13\u89e3AI\u4ee3\u7406\u4e2d\u7684\u4ef7\u503c\u6f02\u79fb\u95ee\u9898\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u3001LSTM\u7f51\u7edc\u9884\u6d4b\u548c\u4eba\u7c7b\u4e2d\u5fc3\u6cbb\u7406\u5c42\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u54cd\u5e94\u548c\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740AI\u4f5c\u4e3a\u8d85\u7ea7\u52a9\u624b\u7684\u5174\u8d77\uff0c\u786e\u4fddAI\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4f26\u7406\u548c\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u81f3\u5173\u91cd\u8981\u3002\u4ef7\u503c\u6f02\u79fb\u98ce\u9669\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u4f26\u7406\u8fdd\u89c4\uff0c\u9700\u8981\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "MAS\u6846\u67b6\u7ed3\u5408\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u7406\u76d1\u63a7\u4ef7\u503c\u72b6\u6001\u3001LSTM\u7f51\u7edc\u9884\u6d4b\u6f02\u79fb\u8d8b\u52bf\uff0c\u4ee5\u53ca\u4eba\u7c7b\u4e2d\u5fc3\u6cbb\u7406\u5c42\u8fdb\u884c\u81ea\u9002\u5e94\u5e72\u9884\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u51cf\u5c11\u8bef\u62a5\u548c\u8b66\u62a5\u75b2\u52b3\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cMAS\u80fd\u591f\u5c06\u4ef7\u503c\u6f02\u79fb\u4e8b\u4ef6\u51cf\u5c1180%\u4ee5\u4e0a\uff0c\u4fdd\u630185%\u7684\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c0.08\u7684\u4f4e\u8bef\u62a5\u7387\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u54cd\u5e94\u6027\u3002", "conclusion": "MAS\u901a\u8fc7\u6982\u7387\u6f02\u79fb\u68c0\u6d4b\u3001\u9884\u6d4b\u5206\u6790\u548c\u81ea\u9002\u5e94\u6cbb\u7406\u7684\u96c6\u6210\uff0c\u63d0\u4f9b\u4e86\u6bd4\u9759\u6001\u5bf9\u9f50\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u4ef7\u503c\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8de8\u9886\u57df\u9002\u7528\u6027\u3002"}}
{"id": "2510.04696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04696", "abs": "https://arxiv.org/abs/2510.04696", "authors": ["Alexander L. Mitchell", "Joe Watson", "Ingmar Posner"], "title": "Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly", "comment": "8 pages, 6 figures, 1 table", "summary": "There are many challenges in bimanual assembly, including high-level\nsequencing, multi-robot coordination, and low-level, contact-rich operations\nsuch as component mating. Task and motion planning (TAMP) methods, while\neffective in this domain, may be prohibitively slow to converge when adapting\nto disturbances that require new task sequencing and optimisation. These events\nare common during tight-tolerance assembly, where difficult-to-model dynamics\nsuch as friction or deformation require rapid replanning and reattempts.\nMoreover, defining explicit task sequences for assembly can be cumbersome,\nlimiting flexibility when task replanning is required. To simplify this\nplanning, we introduce a decentralised gradient-based framework that uses a\npiecewise continuous energy function through the automatic composition of\nadaptive potential functions. This approach generates sub-goals using only\nmyopic optimisation, rather than long-horizon planning. It demonstrates\neffectiveness at solving long-horizon tasks due to the structure and adaptivity\nof the energy function. We show that our approach scales to physical bimanual\nassembly tasks for constructing tight-tolerance assemblies. In these\nexperiments, we discover that our gradient-based rapid replanning framework\ngenerates automatic retries, coordinated motions and autonomous handovers in an\nemergent fashion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u7684\u81ea\u52a8\u7ec4\u5408\u6765\u7b80\u5316\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u89c4\u5212\uff0c\u65e0\u9700\u957f\u65f6\u7a0b\u89c4\u5212\u5373\u53ef\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u9762\u4e34\u9ad8\u96be\u5ea6\u4efb\u52a1\u6392\u5e8f\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\u548c\u63a5\u89e6\u5bc6\u96c6\u578b\u64cd\u4f5c\u7b49\u6311\u6218\u3002\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9700\u8981\u5feb\u901f\u91cd\u89c4\u5212\u65f6\u6536\u655b\u7f13\u6162\uff0c\u4e14\u663e\u5f0f\u5b9a\u4e49\u4efb\u52a1\u5e8f\u5217\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u5206\u6bb5\u8fde\u7eed\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u7684\u81ea\u52a8\u7ec4\u5408\u6784\u5efa\u5206\u6563\u5f0f\u68af\u5ea6\u4f18\u5316\u6846\u67b6\uff0c\u4ec5\u901a\u8fc7\u8fd1\u89c6\u4f18\u5316\u751f\u6210\u5b50\u76ee\u6807\uff0c\u65e0\u9700\u957f\u65f6\u7a0b\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6269\u5c55\u5230\u7269\u7406\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\uff0c\u5728\u7d27\u5bc6\u516c\u5dee\u88c5\u914d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u91cd\u8bd5\u3001\u534f\u8c03\u8fd0\u52a8\u548c\u81ea\u4e3b\u4ea4\u63a5\u7b49\u884c\u4e3a\u3002", "conclusion": "\u68af\u5ea6\u5feb\u901f\u91cd\u89c4\u5212\u6846\u67b6\u80fd\u591f\u4ee5\u6d8c\u73b0\u65b9\u5f0f\u5904\u7406\u88c5\u914d\u4e2d\u7684\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u7d27\u5bc6\u516c\u5dee\u88c5\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03747", "abs": "https://arxiv.org/abs/2510.03747", "authors": ["Zuomin Qu", "Yimao Guo", "Qianyue Hu", "Wei Lu"], "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes", "comment": null, "summary": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoRA Patching\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411Deepfake\u751f\u6210\u5668\u6ce8\u5165\u53ef\u63d2\u62d4\u7684LoRA\u8865\u4e01\u6765\u7ed5\u8fc7\u6700\u5148\u8fdb\u7684\u4e3b\u52a8\u9632\u5fa1\u7cfb\u7edf\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u9632\u5fa1\u6027LoRA\u8865\u4e01\u4f5c\u4e3a\u8865\u5145\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "Deepfake\u6280\u672f\u5bf9\u793e\u4f1a\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0c\u4fc3\u4f7f\u5f00\u53d1\u5728\u9762\u90e8\u56fe\u50cf\u4e2d\u5d4c\u5165\u5bf9\u6297\u6027\u6270\u52a8\u7684\u4e3b\u52a8\u9632\u5fa1\u63aa\u65bd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u9632\u5fa1\u63aa\u65bd\u5f80\u5f80\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86LoRA Patching\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1\uff09\u5411Deepfake\u751f\u6210\u5668\u6ce8\u5165\u53ef\u63d2\u62d4\u7684LoRA\u8865\u4e01\uff1b2\uff09\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u63a7\u5236LoRA\u8865\u4e01\u6548\u679c\u5e76\u9632\u6b62\u68af\u5ea6\u7206\u70b8\uff1b3\uff09\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u635f\u5931\uff08MMFA\uff09\uff0c\u5728\u8bed\u4e49\u5c42\u9762\u4f7f\u5bf9\u6297\u8f93\u51fa\u7279\u5f81\u4e0e\u671f\u671b\u8f93\u51fa\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u4ec5\u4f7f\u75281,000\u4e2a\u9762\u90e8\u793a\u4f8b\u548c\u5355\u4e2a\u8bad\u7ec3\u5468\u671f\uff0cLoRA Patching\u6210\u529f\u51fb\u8d25\u4e86\u591a\u4e2a\u4e3b\u52a8\u9632\u5fa1\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9632\u5fa1\u8303\u5f0f\u7684\u5173\u952e\u5f31\u70b9\u3002", "conclusion": "\u5f53\u524dDeepfake\u9632\u5fa1\u7b56\u7565\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u9632\u5fa1\u6027LoRA\u8865\u4e01\u4f5c\u4e3a\u7f13\u89e3\u8fd9\u79cd\u65b0\u53d1\u73b0\u5b89\u5168\u6f0f\u6d1e\u7684\u8865\u5145\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03302", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03302", "abs": "https://arxiv.org/abs/2510.03302", "authors": ["Daiheng Gao", "Nanxiang Jiang", "Andi Zhang", "Shilin Lu", "Yufei Tang", "Wenbo Zhou", "Weiming Zhang", "Zhaoxin Fan"], "title": "Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models", "comment": "21 pages, 10 figures", "summary": "Concept erasure techniques have been widely deployed in T2I diffusion models\nto prevent inappropriate content generation for safety and copyright\nconsiderations. However, as models evolve to next-generation architectures like\nFlux, established erasure methods (\\textit{e.g.}, ESD, UCE, AC) exhibit\ndegraded effectiveness, raising questions about their true mechanisms. Through\nsystematic analysis, we reveal that concept erasure creates only an illusion of\n``amnesia\": rather than genuine forgetting, these methods bias sampling\ntrajectories away from target concepts, making the erasure fundamentally\nreversible. This insight motivates the need to distinguish superficial safety\nfrom genuine concept removal. In this work, we propose \\textbf{RevAm}\n(\\underline{Rev}oking \\underline{Am}nesia), an RL-based trajectory optimization\nframework that resurrects erased concepts by dynamically steering the denoising\nprocess without modifying model weights. By adapting Group Relative Policy\nOptimization (GRPO) to diffusion models, RevAm explores diverse recovery\ntrajectories through trajectory-level rewards, overcoming local optima that\nlimit existing methods. Extensive experiments demonstrate that RevAm achieves\nsuperior concept resurrection fidelity while reducing computational time by\n10$\\times$, exposing critical vulnerabilities in current safety mechanisms and\nunderscoring the need for more robust erasure techniques beyond trajectory\nmanipulation.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u6982\u5ff5\u64e6\u9664\u6280\u672f\u53ea\u662f\u5236\u9020\u4e86\"\u9057\u5fd8\"\u7684\u5047\u8c61\uff0c\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u504f\u7f6e\u91c7\u6837\u8f68\u8ff9\u6765\u907f\u5f00\u76ee\u6807\u6982\u5ff5\uff0c\u8fd9\u79cd\u64e6\u9664\u662f\u53ef\u9006\u7684\u3002\u4f5c\u8005\u63d0\u51fa\u4e86RevAm\u6846\u67b6\uff0c\u901a\u8fc7RL\u4f18\u5316\u8f68\u8ff9\u6765\u590d\u6d3b\u88ab\u64e6\u9664\u7684\u6982\u5ff5\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u67b6\u6784\u6f14\u8fdb\u5230Flux\u7b49\u65b0\u4e00\u4ee3\u67b6\u6784\uff0c\u73b0\u6709\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff08\u5982ESD\u3001UCE\u3001AC\uff09\u6548\u679c\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u975e\u771f\u6b63\u5220\u9664\u6982\u5ff5\uff0c\u800c\u662f\u901a\u8fc7\u504f\u7f6e\u91c7\u6837\u8f68\u8ff9\u6765\u907f\u5f00\u76ee\u6807\u6982\u5ff5\uff0c\u8fd9\u79cd\u64e6\u9664\u662f\u53ef\u9006\u7684\u3002\u8fd9\u4fc3\u4f7f\u9700\u8981\u533a\u5206\u8868\u9762\u5b89\u5168\u6027\u548c\u771f\u6b63\u7684\u6982\u5ff5\u79fb\u9664\u3002", "method": "\u63d0\u51fa\u4e86RevAm\u6846\u67b6\uff0c\u57fa\u4e8eRL\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u6765\u590d\u6d3b\u88ab\u64e6\u9664\u7684\u6982\u5ff5\uff0c\u800c\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002\u8be5\u65b9\u6cd5\u5c06Group Relative Policy Optimization (GRPO)\u9002\u5e94\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5956\u52b1\u63a2\u7d22\u591a\u6837\u5316\u7684\u6062\u590d\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRevAm\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6982\u5ff5\u590d\u6d3b\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e8610\u500d\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u5b89\u5168\u673a\u5236\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u8f68\u8ff9\u64cd\u4f5c\u7684\u5b89\u5168\u673a\u5236\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u9700\u8981\u5f00\u53d1\u8d85\u8d8a\u8f68\u8ff9\u64cd\u4f5c\u7684\u66f4\u9c81\u68d2\u7684\u64e6\u9664\u6280\u672f\u3002"}}
{"id": "2510.03325", "categories": ["cs.LG", "physics.comp-ph", "physics.data-an", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.03325", "abs": "https://arxiv.org/abs/2510.03325", "authors": ["Giuseppe Di Somma", "Giorgio Carelli", "Angela D. V. Di Virgilio", "Francesco Fuso", "Enrico Maccioni", "Paolo Marsili"], "title": "Fast frequency reconstruction using Deep Learning for event recognition in ring laser data", "comment": null, "summary": "The reconstruction of a frequency with minimal delay from a sinusoidal signal\nis a common task in several fields; for example Ring Laser Gyroscopes, since\ntheir output signal is a beat frequency. While conventional methods require\nseveral seconds of data, we present a neural network approach capable of\nreconstructing frequencies of several hundred Hertz within approximately 10\nmilliseconds. This enables rapid trigger generation. The method outperforms\nstandard Fourier-based techniques, improving frequency estimation precision by\na factor of 2 in the operational range of GINGERINO, our Ring Laser\nGyroscope.\\\\ In addition to fast frequency estimation, we introduce an\nautomated classification framework to identify physical disturbances in the\nsignal, such as laser instabilities and seismic events, achieving accuracy\nrates between 99\\% and 100\\% on independent test datasets for the seismic\nclass. These results mark a step forward in integrating artificial intelligence\ninto signal analysis for geophysical applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u7ea610\u6beb\u79d2\u5185\u4ece\u6b63\u5f26\u4fe1\u53f7\u4e2d\u91cd\u5efa\u6570\u767e\u8d6b\u5179\u9891\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u5085\u91cc\u53f6\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u9ad82\u500d\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u5206\u7c7b\u6846\u67b6\u8bc6\u522b\u7269\u7406\u5e72\u6270\u3002", "motivation": "\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u4eea\u7b49\u8bbe\u5907\u9700\u8981\u4ece\u6b63\u5f26\u4fe1\u53f7\u4e2d\u5feb\u901f\u91cd\u5efa\u9891\u7387\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6570\u79d2\u6570\u636e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5feb\u901f\u89e6\u53d1\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u8fdb\u884c\u9891\u7387\u91cd\u5efa\uff0c\u7ed3\u5408\u81ea\u52a8\u5206\u7c7b\u6846\u67b6\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u7269\u7406\u5e72\u6270\uff08\u5982\u6fc0\u5149\u4e0d\u7a33\u5b9a\u6027\u548c\u5730\u9707\u4e8b\u4ef6\uff09\u3002", "result": "\u572810\u6beb\u79d2\u5185\u91cd\u5efa\u6570\u767e\u8d6b\u5179\u9891\u7387\uff0c\u9891\u7387\u4f30\u8ba1\u7cbe\u5ea6\u6bd4\u6807\u51c6\u5085\u91cc\u53f6\u6280\u672f\u63d0\u9ad82\u500d\uff1b\u7269\u7406\u5e72\u6270\u5206\u7c7b\u5728\u72ec\u7acb\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%-100%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fe1\u53f7\u5206\u6790\u4e2d\u6574\u5408\u4eba\u5de5\u667a\u80fd\uff0c\u4e3a\u5730\u7403\u7269\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u9891\u7387\u4f30\u8ba1\u548c\u81ea\u52a8\u5e72\u6270\u8bc6\u522b\u3002"}}
{"id": "2510.03827", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03827", "abs": "https://arxiv.org/abs/2510.03827", "authors": ["Xueyang Zhou", "Yangming Xu", "Guiyao Tie", "Yongchao Chen", "Guowen Zhang", "Duanfeng Chu", "Pan Zhou", "Lichao Sun"], "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization", "comment": "12 pages,7 figures, 5 tables", "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.", "AI": {"tldr": "LIBERO-PRO\u662f\u4e00\u4e2a\u6269\u5c55\u7684LIBERO\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u56db\u4e2a\u7ef4\u5ea6\u6270\u52a8\u4e0b\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7684\u4efb\u52a1\u7406\u89e3\u3002", "motivation": "\u5f53\u524dLIBERO\u57fa\u51c6\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bbe\u7f6e\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4f30\u8ba1\u88ab\u5938\u5927\uff0c\u65e0\u6cd5\u516c\u5e73\u6bd4\u8f83\u6a21\u578b\u3002\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165LIBERO-PRO\u57fa\u51c6\uff0c\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u65bd\u52a0\u5408\u7406\u6270\u52a8\uff1a\u64cd\u4f5c\u5bf9\u8c61\u3001\u521d\u59cb\u72b6\u6001\u3001\u4efb\u52a1\u6307\u4ee4\u548c\u73af\u5883\u8bbe\u7f6e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u6807\u51c6LIBERO\u8bc4\u4f30\u4e2d\u8fbe\u523090%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u4f46\u5728\u5e7f\u4e49\u8bbe\u7f6e\u4e0b\u6027\u80fd\u5d29\u6e83\u81f30.0%\uff0c\u663e\u793a\u6a21\u578b\u4f9d\u8d56\u52a8\u4f5c\u5e8f\u5217\u548c\u73af\u5883\u5e03\u5c40\u7684\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7406\u89e3\u3002", "conclusion": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u547c\u5401\u793e\u533a\u653e\u5f03\u8bef\u5bfc\u6027\u65b9\u6cd5\uff0c\u91c7\u7528\u80fd\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u7406\u89e3\u529b\u7684\u9c81\u68d2\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.03340", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2510.03340", "abs": "https://arxiv.org/abs/2510.03340", "authors": ["Marian Chen", "Miri Zilka"], "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL", "comment": null, "summary": "The COVID-19 pandemic underscored a critical need for intervention strategies\nthat balance disease containment with socioeconomic stability. We approach this\nchallenge by designing a framework for modeling and evaluating disease-spread\nprevention strategies. Our framework leverages multi-objective reinforcement\nlearning (MORL) - a formulation necessitated by competing objectives - combined\nwith a new stochastic differential equation (SDE) pandemic simulator,\ncalibrated and validated against global COVID-19 data. Our simulator reproduces\nnational-scale pandemic dynamics with orders of magnitude higher fidelity than\nother models commonly used in reinforcement learning (RL) approaches to\npandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on\nthis simulator, we illustrate the direct policy trade-offs between\nepidemiological control and economic stability for COVID-19. Furthermore, we\ndemonstrate the framework's generality by extending it to pathogens with\ndifferent epidemiological profiles, such as polio and influenza, and show how\nthese profiles lead the agent to discover fundamentally different intervention\npolicies. To ground our work in contemporary policymaking challenges, we apply\nthe model to measles outbreaks, quantifying how a modest 5% drop in vaccination\ncoverage necessitates significantly more stringent and costly interventions to\ncurb disease spread. This work provides a robust and adaptable framework to\nsupport transparent, evidence-based policymaking for mitigating public health\ncrises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u548c\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5e73\u8861\u75be\u75c5\u63a7\u5236\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u6027\u7684\u75ab\u60c5\u9632\u63a7\u7b56\u7565\u8bc4\u4f30\u3002", "motivation": "COVID-19\u75ab\u60c5\u51f8\u663e\u4e86\u9700\u8981\u5728\u75be\u75c5\u63a7\u5236\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u5e72\u9884\u7b56\u7565\u9700\u6c42\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u8fd9\u4e9b\u76f8\u4e92\u7ade\u4e89\u7684\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60(MORL)\u548c\u65b0\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b(SDE)\u75ab\u60c5\u6a21\u62df\u5668\uff0c\u8bad\u7ec3Pareto-Conditioned Network(PCN)\u4ee3\u7406\u6765\u53d1\u73b0\u6700\u4f18\u5e72\u9884\u7b56\u7565\u3002", "result": "\u8be5\u6a21\u62df\u5668\u6bd4\u4f20\u7edfRL\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u80fd\u591f\u91cf\u5316COVID-19\u9632\u63a7\u4e0e\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u9002\u7528\u4e8e\u9ebb\u75b9\u3001\u810a\u9ad3\u7070\u8d28\u708e\u548c\u6d41\u611f\u7b49\u5176\u4ed6\u75c5\u539f\u4f53\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u516c\u5171\u536b\u751f\u5371\u673a\u7f13\u89e3\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u9002\u5e94\u6027\u5f3a\u7684\u5de5\u5177\uff0c\u652f\u6301\u900f\u660e\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u653f\u7b56\u5236\u5b9a\uff0c\u7279\u522b\u662f\u5728\u75ab\u82d7\u63a5\u79cd\u8986\u76d6\u7387\u4e0b\u964d\u7b49\u73b0\u5b9e\u6311\u6218\u4e2d\u3002"}}
{"id": "2510.03426", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.03426", "abs": "https://arxiv.org/abs/2510.03426", "authors": ["Franz A. Heinsen", "Leo Kozachkov"], "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation", "comment": "18 pages, 4 figures (main text). 14 pages, 21 figures (appendix)", "summary": "Many domains, from deep learning to finance, require compounding real numbers\nover long sequences, often leading to catastrophic numerical underflow or\noverflow. We introduce generalized orders of magnitude (GOOMs), a principled\nextension of traditional orders of magnitude that incorporates floating-point\nnumbers as a special case, and which in practice enables stable computation\nover significantly larger dynamic ranges of real numbers than previously\npossible. We implement GOOMs, along with an efficient custom parallel prefix\nscan, to support native execution on parallel hardware such as GPUs. We\ndemonstrate that our implementation of GOOMs outperforms traditional approaches\nwith three representative experiments, all of which were previously considered\nimpractical or impossible, and now become possible and practical: (1)\ncompounding real matrix products far beyond standard floating-point limits; (2)\nestimating spectra of Lyapunov exponents in parallel, orders of magnitude\nfaster than with previous methods, applying a novel selective-resetting method\nto prevent state colinearity; and (3) capturing long-range dependencies in deep\nrecurrent neural networks with non-diagonal recurrent states, computed in\nparallel via a prefix scan, without requiring any form of stabilization. Our\nresults show that our implementation of GOOMs, combined with efficient parallel\nscanning, offers a scalable and numerically robust alternative to conventional\nfloating-point numbers for high-dynamic-range applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u6570\u91cf\u7ea7(GOOMs)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u4f20\u7edf\u6570\u91cf\u7ea7\u6982\u5ff5\u6765\u652f\u6301\u6d6e\u70b9\u6570\uff0c\u5b9e\u73b0\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5927\u52a8\u6001\u8303\u56f4\u7684\u7a33\u5b9a\u6570\u503c\u8ba1\u7b97\uff0c\u7ed3\u5408\u5e76\u884c\u524d\u7f00\u626b\u63cf\u5728GPU\u7b49\u786c\u4ef6\u4e0a\u9ad8\u6548\u6267\u884c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u3001\u91d1\u878d\u7b49\u9886\u57df\u9700\u8981\u5728\u957f\u5e8f\u5217\u4e0a\u8fdb\u884c\u5b9e\u6570\u590d\u5408\u8fd0\u7b97\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u707e\u96be\u6027\u7684\u6570\u503c\u4e0b\u6ea2\u6216\u4e0a\u6ea2\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u652f\u6301\u66f4\u5927\u52a8\u6001\u8303\u56f4\u7684\u7a33\u5b9a\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u5e7f\u4e49\u6570\u91cf\u7ea7(GOOMs)\u6982\u5ff5\uff0c\u6269\u5c55\u4f20\u7edf\u6570\u91cf\u7ea7\u4ee5\u5305\u542b\u6d6e\u70b9\u6570\u4f5c\u4e3a\u7279\u4f8b\uff1b\u5b9e\u73b0\u9ad8\u6548\u7684\u81ea\u5b9a\u4e49\u5e76\u884c\u524d\u7f00\u626b\u63cf\u7b97\u6cd5\uff0c\u652f\u6301\u5728GPU\u7b49\u5e76\u884c\u786c\u4ef6\u4e0a\u539f\u751f\u6267\u884c\u3002", "result": "GOOMs\u65b9\u6cd5\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a1) \u8d85\u8d8a\u6807\u51c6\u6d6e\u70b9\u6570\u9650\u5236\u7684\u5b9e\u6570\u77e9\u9635\u4e58\u79ef\u590d\u5408\uff1b2) \u5e76\u884c\u4f30\u8ba1\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u8c31\uff0c\u901f\u5ea6\u6bd4\u5148\u524d\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff1b3) \u5728\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u4efb\u4f55\u7a33\u5b9a\u5316\u5904\u7406\u3002", "conclusion": "GOOMs\u7ed3\u5408\u9ad8\u6548\u5e76\u884c\u626b\u63cf\u4e3a\u9ad8\u52a8\u6001\u8303\u56f4\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6570\u503c\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u6d6e\u70b9\u6570\u65b9\u6cd5\u3002"}}
{"id": "2510.04022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04022", "abs": "https://arxiv.org/abs/2510.04022", "authors": ["Chendong Wang", "Donglin Bai", "Yifan Yang", "Xiao Jin", "Anlan Zhang", "Rui Wang", "Shiqi Jiang", "Yuqing Yang", "Hao Wu", "Qi Dai", "Chong Luo", "Ting Cao", "Lili Qiu", "Suman Banerjee"], "title": "Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning", "comment": null, "summary": "We present \\emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA\nframework that preserves a fixed token budget by first \\emph{localizing}\nquestion-relevant interval(s) with a low-fps skim and then \\emph{answering} via\nspan-aware reallocation of visual tokens at higher effective frame rate,\nemitting an interleaved output with both spans and the final option for direct\nattribution. We also introduce \\dataname{}, which converts description based\nevent graphs into \\emph{span-grounded} multiple-choice QA by pairing each\nquestion with \\emph{ground-truth} time span(s) and related reasoning. ViTL is\ntrained end-to-end with an interleaved group-relative objective that couples\ntemporal IoU for localization with answer correctness, allowing credit to flow\nfrom answers back to spans without increasing compute. Under fixed token\nbudgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and\ntemporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations\nshow that span-aware token reallocation consistently surpasses uniform\nsampling. Together, \\dataname{} and ViTL provide an interpretable,\ncompute-efficient recipe for scalable long-video QA.", "AI": {"tldr": "\u63d0\u51faVideo-in-the-Loop (ViTL)\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5904\u7406\u957f\u89c6\u9891\u95ee\u7b54\uff1a\u5148\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u65f6\u95f4\u533a\u95f4\uff0c\u518d\u91cd\u65b0\u5206\u914d\u89c6\u89c9token\u8fdb\u884c\u56de\u7b54\uff0c\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u957f\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u6574\u4e2a\u89c6\u9891\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u6709\u6548\u5904\u7406\u957f\u89c6\u9891\u7684\u65b9\u6cd5\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528\u4f4e\u5e27\u7387\u6d4f\u89c8\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u65f6\u95f4\u533a\u95f4\uff1b2) \u901a\u8fc7\u8de8\u5ea6\u611f\u77e5\u7684token\u91cd\u65b0\u5206\u914d\uff0c\u5728\u66f4\u9ad8\u6709\u6548\u5e27\u7387\u4e0b\u56de\u7b54\u95ee\u9898\u3002\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u7ed3\u5408\u65f6\u95f4IoU\u548c\u7b54\u6848\u6b63\u786e\u6027\u7684\u8054\u5408\u76ee\u6807\u3002", "result": "\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\uff0cViTL\u5728\u957f\u89c6\u9891\u95ee\u7b54\u548c\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8fbe\u52308.6%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c1150%\u7684\u5e27\u8f93\u5165\u3002\u8de8\u5ea6\u611f\u77e5token\u91cd\u65b0\u5206\u914d\u59cb\u7ec8\u4f18\u4e8e\u5747\u5300\u91c7\u6837\u3002", "conclusion": "ViTL\u548c\u914d\u5957\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u957f\u89c6\u9891\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.04024", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04024", "abs": "https://arxiv.org/abs/2510.04024", "authors": ["Yuyan Bu", "Qiang Sheng", "Juan Cao", "Shaofei Wang", "Peng Qi", "Yuhui Shi", "Beizhe Hu"], "title": "Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation", "comment": "ACM CIKM 2025", "summary": "The emergence of fake news on short video platforms has become a new\nsignificant societal concern, necessitating automatic video-news-specific\ndetection. Current detectors primarily rely on pattern-based features to\nseparate fake news videos from real ones. However, limited and less diversified\ntraining data lead to biased patterns and hinder their performance. This\nweakness stems from the complex many-to-many relationships between video\nmaterial segments and fabricated news events in real-world scenarios: a single\nvideo clip can be utilized in multiple ways to create different fake\nnarratives, while a single fabricated event often combines multiple distinct\nvideo segments. However, existing datasets do not adequately reflect such\nrelationships due to the difficulty of collecting and annotating large-scale\nreal-world data, resulting in sparse coverage and non-comprehensive learning of\nthe characteristics of potential fake news video creation. To address this\nissue, we propose a data augmentation framework, AgentAug, that generates\ndiverse fake news videos by simulating typical creative processes. AgentAug\nimplements multiple LLM-driven pipelines of four fabrication categories for\nnews video creation, combined with an active learning strategy based on\nuncertainty sampling to select the potentially useful augmented samples during\ntraining. Experimental results on two benchmark datasets demonstrate that\nAgentAug consistently improves the performance of short video fake news\ndetectors.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentAug\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5178\u578b\u521b\u4f5c\u8fc7\u7a0b\u751f\u6210\u591a\u6837\u5316\u5047\u65b0\u95fb\u89c6\u9891\uff0c\u63d0\u5347\u77ed\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u68c0\u6d4b\u5668\u56e0\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u5f0f\u504f\u89c1\u548c\u6027\u80fd\u53d7\u9650\u3002\u771f\u5b9e\u573a\u666f\u4e2d\u89c6\u9891\u7d20\u6750\u4e0e\u5047\u65b0\u95fb\u4e8b\u4ef6\u5b58\u5728\u590d\u6742\u591a\u5bf9\u591a\u5173\u7cfb\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u8fd9\u79cd\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u591aLLM\u9a71\u52a8\u7684\u56db\u79cd\u4f2a\u9020\u7c7b\u522b\u7ba1\u9053\u751f\u6210\u5047\u65b0\u95fb\u89c6\u9891\uff0c\u7ed3\u5408\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u6709\u7528\u7684\u589e\u5f3a\u6837\u672c\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAgentAug\u80fd\u6301\u7eed\u63d0\u5347\u77ed\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "AgentAug\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u521b\u4f5c\u8fc7\u7a0b\u7684\u6570\u636e\u589e\u5f3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u758f\u548c\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2510.04039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04039", "abs": "https://arxiv.org/abs/2510.04039", "authors": ["Bin Lei", "Nuo Xu", "Ali Payani", "Mingyi Hong", "Chunhua Liao", "Yu Cao", "Caiwen Ding"], "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding", "comment": null, "summary": "Multimodal large language models (MLLMs) have markedly expanded the\ncompetence of graphical user-interface (GUI) systems, propelling them beyond\ncontrolled simulations into complex, real-world environments across diverse\nplatforms. However, practical usefulness is still bounded by the reliability of\nvisual grounding, i.e., mapping textual references to exact on-screen elements.\nThis limitation prevents the system from accurately performing pointer-level\nactions such as clicking or dragging. To address it, we introduce GUI-Spotlight\n-- a model trained for image-grounded reasoning that dynamically invokes\nmultiple specialized tools to iteratively narrow its focus to the relevant\nregion of the screen, thereby substantially improving visual grounding\naccuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only\n18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with\n9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).", "AI": {"tldr": "GUI-Spotlight\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u63a8\u7406\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u591a\u4e2a\u4e13\u4e1a\u5de5\u5177\u8fed\u4ee3\u7f29\u5c0f\u5c4f\u5e55\u76f8\u5173\u533a\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u89c6\u89c9\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u89c6\u89c9\u5b9a\u4f4d\u53ef\u9760\u6027\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u51c6\u786e\u6267\u884c\u6307\u9488\u7ea7\u64cd\u4f5c\u5982\u70b9\u51fb\u6216\u62d6\u52a8\u3002", "method": "\u5f15\u5165GUI-Spotlight\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u591a\u4e2a\u4e13\u4e1a\u5de5\u5177\u8fdb\u884c\u56fe\u50cf\u63a8\u7406\uff0c\u8fed\u4ee3\u7f29\u5c0f\u7126\u70b9\u5230\u5c4f\u5e55\u76f8\u5173\u533a\u57df\u3002", "result": "\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u752818.5K\u8bad\u7ec3\u6837\u672c\u7684GUI-Spotlight\u8fbe\u523052.8%\u51c6\u786e\u7387\uff0c\u8d85\u8d8aV2P-7B\uff0850.6%\uff0c\u4f7f\u75289.6M\u6837\u672c\uff09\u548cGTA-1-7B\uff0850.1%\uff0c\u4f7f\u75281.56M\u6837\u672c\uff09\u3002", "conclusion": "GUI-Spotlight\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u548c\u8fed\u4ee3\u805a\u7126\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\u7cfb\u7edf\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2510.04063", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2510.04063", "abs": "https://arxiv.org/abs/2510.04063", "authors": ["Chetraj Pandey", "Jinsu Hong", "Anli Ji", "Rafal A. Angryk", "Berkay Aydin"], "title": "Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction", "comment": "This is a preprint submitted to ICDM Workshop (SABID 2025). 6 pages,\n  2 Figures", "summary": "The prediction of solar flares is typically formulated as a binary\nclassification task, distinguishing events as either Flare (FL) or No-Flare\n(NF) according to a specified threshold (for example, greater than or equal to\nC-class, M-class, or X-class). However, this binary framework neglects the\ninherent ordinal relationships among the sub-classes contained within each\ncategory (FL and NF). Several studies on solar flare prediction have\nempirically shown that the most frequent misclassifications occur near this\nprediction threshold. This suggests that the models struggle to differentiate\nevents that are similar in intensity but fall on opposite sides of the binary\nthreshold. To mitigate this limitation, we propose a modified loss function\nthat integrates the ordinal information among the sub-classes of the binarized\nflare labels into the conventional binary cross-entropy (BCE) loss. This\napproach serves as an ordinality-aware, data-driven regularization method that\npenalizes the incorrect predictions of flare events in close proximity to the\nprediction threshold more heavily than those away from the boundary during\nmodel optimization. By incorporating ordinal weighting into the loss function,\nwe aim to enhance the model's learning process by leveraging the ordinal\ncharacteristics of the data, thereby improving its overall performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u5c06\u8000\u6591\u5b50\u7c7b\u522b\u95f4\u7684\u5e8f\u6570\u4fe1\u606f\u6574\u5408\u5230\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u4e2d\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4e8c\u5143\u5206\u7c7b\u5ffd\u7565\u8000\u6591\u5f3a\u5ea6\u5e8f\u6570\u5173\u7cfb\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8000\u6591\u9884\u6d4b\u7684\u4e8c\u5143\u5206\u7c7b\u6846\u67b6\u5ffd\u7565\u4e86FL\u548cNF\u7c7b\u522b\u5185\u5b50\u7c7b\u522b\u95f4\u7684\u5e8f\u6570\u5173\u7cfb\uff0c\u4e14\u7814\u7a76\u8868\u660e\u6a21\u578b\u5728\u9884\u6d4b\u9608\u503c\u9644\u8fd1\u6700\u5bb9\u6613\u51fa\u73b0\u8bef\u5206\u7c7b\u3002", "method": "\u5728\u4f20\u7edf\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u57fa\u7840\u4e0a\uff0c\u96c6\u6210\u8000\u6591\u6807\u7b7e\u5b50\u7c7b\u522b\u7684\u5e8f\u6570\u4fe1\u606f\uff0c\u4f5c\u4e3a\u5e8f\u6570\u611f\u77e5\u7684\u6570\u636e\u9a71\u52a8\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5bf9\u9608\u503c\u9644\u8fd1\u7684\u9519\u8bef\u9884\u6d4b\u65bd\u52a0\u66f4\u91cd\u60e9\u7f5a\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6570\u636e\u7684\u5e8f\u6570\u7279\u6027\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u65e8\u5728\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e8f\u6570\u52a0\u6743\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u6539\u5584\u6a21\u578b\u5bf9\u8000\u6591\u5f3a\u5ea6\u76f8\u4f3c\u4f46\u4f4d\u4e8e\u4e8c\u5143\u9608\u503c\u4e24\u4fa7\u4e8b\u4ef6\u7684\u533a\u5206\u80fd\u529b\u3002"}}
{"id": "2510.04100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04100", "abs": "https://arxiv.org/abs/2510.04100", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Harold Soh"], "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing", "comment": "Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally", "summary": "Topological mapping offers a compact and robust representation for\nnavigation, but progress in the field is hindered by the lack of standardized\nevaluation metrics, datasets, and protocols. Existing systems are assessed\nusing different environments and criteria, preventing fair and reproducible\ncomparisons. Moreover, a key challenge - perceptual aliasing - remains\nunder-quantified, despite its strong influence on system performance. We\naddress these gaps by (1) formalizing topological consistency as the\nfundamental property of topological maps and showing that localization accuracy\nprovides an efficient and interpretable surrogate metric, and (2) proposing the\nfirst quantitative measure of dataset ambiguity to enable fair comparisons\nacross environments. To support this protocol, we curate a diverse benchmark\ndataset with calibrated ambiguity levels, implement and release deep-learned\nbaseline systems, and evaluate them alongside classical methods. Our\nexperiments and analysis yield new insights into the limitations of current\napproaches under perceptual aliasing. All datasets, baselines, and evaluation\ntools are fully open-sourced to foster consistent and reproducible research in\ntopological mapping.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u62d3\u6251\u6620\u5c04\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u62ec\u62d3\u6251\u4e00\u81f4\u6027\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\u3001\u6570\u636e\u96c6\u6a21\u7cca\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7cfb\u7edf\u3002", "motivation": "\u62d3\u6251\u6620\u5c04\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u548c\u534f\u8bae\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u4e0d\u540c\u73af\u5883\u548c\u6807\u51c6\u4e0b\u8bc4\u4f30\uff0c\u65e0\u6cd5\u8fdb\u884c\u516c\u5e73\u53ef\u590d\u73b0\u7684\u6bd4\u8f83\uff0c\u4e14\u611f\u77e5\u6df7\u6dc6\u95ee\u9898\u672a\u88ab\u5145\u5206\u91cf\u5316\u3002", "method": "\u5f62\u5f0f\u5316\u62d3\u6251\u4e00\u81f4\u6027\u4f5c\u4e3a\u62d3\u6251\u6620\u5c04\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u4f7f\u7528\u5b9a\u4f4d\u7cbe\u5ea6\u4f5c\u4e3a\u66ff\u4ee3\u6307\u6807\uff1b\u63d0\u51fa\u6570\u636e\u96c6\u6a21\u7cca\u5ea6\u7684\u91cf\u5316\u65b9\u6cd5\uff1b\u6784\u5efa\u5177\u6709\u6821\u51c6\u6a21\u7cca\u5ea6\u7ea7\u522b\u7684\u591a\u6837\u5316\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u5b9e\u73b0\u5e76\u53d1\u5e03\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u7cfb\u7edf\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u62d3\u6251\u6620\u5c04\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u611f\u77e5\u6df7\u6dc6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u6240\u6709\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u548c\u8bc4\u4f30\u5de5\u5177\u5747\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u62d3\u6251\u6620\u5c04\u9886\u57df\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u516c\u5e73\u53ef\u590d\u73b0\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u534f\u8bae\u548c\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.04111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04111", "abs": "https://arxiv.org/abs/2510.04111", "authors": ["Xinglong Luo", "Ao Luo", "Kunming Luo", "Zhengning Wang", "Ping Tan", "Bing Zeng", "Shuaicheng Liu"], "title": "Learning Efficient Meshflow and Optical Flow from Event Cameras", "comment": "Accepted by TPAMI 2025", "summary": "In this paper, we explore the problem of event-based meshflow estimation, a\nnovel task that involves predicting a spatially smooth sparse motion field from\nevent cameras. To start, we review the state-of-the-art in event-based flow\nestimation, highlighting two key areas for further research: i) the lack of\nmeshflow-specific event datasets and methods, and ii) the underexplored\nchallenge of event data density. First, we generate a large-scale\nHigh-Resolution Event Meshflow (HREM) dataset, which showcases its superiority\nby encompassing the merits of high resolution at 1280x720, handling dynamic\nobjects and complex motion patterns, and offering both optical flow and\nmeshflow labels. These aspects have not been fully explored in previous works.\nBesides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a\nlightweight model featuring a specially crafted encoder-decoder architecture to\nfacilitate swift and accurate meshflow estimation. Furthermore, we upgrade\nEEMFlow network to support dense event optical flow, in which a\nConfidence-induced Detail Completion (CDC) module is proposed to preserve sharp\nmotion boundaries. We conduct comprehensive experiments to show the exceptional\nperformance and runtime efficiency (30x faster) of our EEMFlow model compared\nto the recent state-of-the-art flow method. As an extension, we expand HREM\ninto HREM+, a multi-density event dataset contributing to a thorough study of\nthe robustness of existing methods across data with varying densities, and\npropose an Adaptive Density Module (ADM) to adjust the density of input event\ndata to a more optimal range, enhancing the model's generalization ability. We\nempirically demonstrate that ADM helps to significantly improve the performance\nof EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are\nreleased at https://github.com/boomluo02/EEMFlowPlus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7f51\u683c\u6d41\u4f30\u8ba1\u65b0\u4efb\u52a1\uff0c\u521b\u5efa\u4e86\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u7f51\u683c\u6d41\u6570\u636e\u96c6HREM\uff0c\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7EEMFlow\u7f51\u7edc\u7528\u4e8e\u5feb\u901f\u51c6\u786e\u7684\u7f51\u683c\u6d41\u4f30\u8ba1\uff0c\u5e76\u6269\u5c55\u4e3a\u652f\u6301\u7a20\u5bc6\u5149\u6d41\u7684EEMFlow+\uff0c\u540c\u65f6\u63d0\u51fa\u81ea\u9002\u5e94\u5bc6\u5ea6\u6a21\u5757ADM\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u7f51\u683c\u6d41\u4f30\u8ba1\u9886\u57df\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4e8b\u4ef6\u6570\u636e\u5bc6\u5ea6\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "1) \u521b\u5efaHREM\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u7f51\u683c\u6d41\u6570\u636e\u96c6\uff1b2) \u8bbe\u8ba1\u8f7b\u91cf\u7ea7EEMFlow\u7f51\u7edc\u67b6\u6784\uff1b3) \u63d0\u51fa\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7ec6\u8282\u8865\u5168\u6a21\u5757CDC\uff1b4) \u5f00\u53d1\u81ea\u9002\u5e94\u5bc6\u5ea6\u6a21\u5757ADM\u3002", "result": "EEMFlow\u6a21\u578b\u5728\u6027\u80fd\u548c\u8fd0\u884c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff08\u5feb30\u500d\uff09\uff0cADM\u6a21\u5757\u4f7fEEMFlow\u548cEEMFlow+\u6027\u80fd\u5206\u522b\u63d0\u53478%\u548c10%\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u7f51\u683c\u6d41\u4f30\u8ba1\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u3001\u7f51\u7edc\u67b6\u6784\u548c\u5bc6\u5ea6\u81ea\u9002\u5e94\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2510.03566", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.03566", "abs": "https://arxiv.org/abs/2510.03566", "authors": ["Ashwin Prabu", "Nhat Thanh Tran", "Guofa Zhou", "Jack Xin"], "title": "CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "A variety of models have been developed to forecast dengue cases to date.\nHowever, it remains a challenge to predict major dengue outbreaks that need\ntimely public warnings the most. In this paper, we introduce CrossLag, an\nenvironmentally informed attention that allows for the incorporation of lagging\nendogenous signals behind the significant events in the exogenous data into the\narchitecture of the transformer at low parameter counts. Outbreaks typically\nlag behind major changes in climate and oceanic anomalies. We use TimeXer, a\nrecent general-purpose transformer distinguishing exogenous-endogenous inputs,\nas the baseline for this study. Our proposed model outperforms TimeXer by a\nconsiderable margin in detecting and predicting major outbreaks in Singapore\ndengue data over a 24-week prediction window.", "AI": {"tldr": "\u63d0\u51fa\u4e86CrossLag\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u6574\u5408\u5916\u751f\u6570\u636e\u4e2d\u7684\u6ede\u540e\u5185\u751f\u4fe1\u53f7\u6765\u6539\u8fdb\u767b\u9769\u70ed\u75ab\u60c5\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u91cd\u5927\u75ab\u60c5\u7206\u53d1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u767b\u9769\u70ed\u9884\u6d4b\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u91cd\u5927\u75ab\u60c5\u7206\u53d1\uff0c\u800c\u8fd9\u4e9b\u6b63\u662f\u6700\u9700\u8981\u53ca\u65f6\u516c\u5171\u536b\u751f\u9884\u8b66\u7684\u60c5\u51b5\u3002", "method": "\u5f00\u53d1\u4e86CrossLag\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5728\u4f4e\u53c2\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u5c06\u5916\u751f\u6570\u636e\u4e2d\u6ede\u540e\u5185\u751f\u4fe1\u53f7\u6574\u5408\u5230transformer\u67b6\u6784\u4e2d\uff0c\u5229\u7528\u75ab\u60c5\u7206\u53d1\u901a\u5e38\u6ede\u540e\u4e8e\u6c14\u5019\u548c\u6d77\u6d0b\u5f02\u5e38\u53d8\u5316\u7684\u7279\u70b9\u3002", "result": "\u572824\u5468\u9884\u6d4b\u7a97\u53e3\u5185\uff0c\u8be5\u6a21\u578b\u5728\u65b0\u52a0\u5761\u767b\u9769\u70ed\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8eTimeXer\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u548c\u9884\u6d4b\u91cd\u5927\u75ab\u60c5\u7206\u53d1\u65b9\u9762\u3002", "conclusion": "CrossLag\u901a\u8fc7\u6709\u6548\u6574\u5408\u6ede\u540e\u73af\u5883\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5927\u767b\u9769\u70ed\u75ab\u60c5\u7206\u53d1\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u516c\u5171\u536b\u751f\u9884\u8b66\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2510.03571", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.2.7; C.2.1"], "pdf": "https://arxiv.org/pdf/2510.03571", "abs": "https://arxiv.org/abs/2510.03571", "authors": ["Burak Karabulut", "Carlo Manna", "Chris Develder"], "title": "Generalization of Graph Neural Network Models for Distribution Grid Fault Detection", "comment": "This paper has been submitted and accepted for IEEE SmartGridComm\n  2025", "summary": "Fault detection in power distribution grids is critical for ensuring system\nreliability and preventing costly outages. Moreover, fault detection\nmethodologies should remain robust to evolving grid topologies caused by\nfactors such as reconfigurations, equipment failures, and Distributed Energy\nResource (DER) integration. Current data-driven state-of-the-art methods use\nRecurrent Neural Networks (RNNs) for temporal modeling and Graph Neural\nNetworks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in\nshort). Specifically, for power system fault diagnosis, Graph Convolutional\nNetworks (GCNs) have been adopted. Yet, various more advanced GNN architectures\nhave been proposed and adopted in domains outside of power systems. In this\npaper, we set out to systematically and consistently benchmark various GNN\narchitectures in an RNN+GNN pipeline model. Specifically, to the best of our\nknowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention\n(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive\nbenchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN\nmodels (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring\ntheir generalization potential for deployment in different settings than those\nused for training them. Our experimental results on the IEEE 123-node\ndistribution network show that RGATv2 has superior generalization capabilities,\nmaintaining high performance with an F1-score reduction of $\\sim$12% across\ndifferent topology settings. In contrast, pure RNN models largely fail,\nexperiencing an F1-score reduction of up to $\\sim$60%, while other RGNN\nvariants also exhibit significant performance degradation, i.e., up to\n$\\sim$25% lower F1-scores.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RGATv2\u6a21\u578b\u5177\u6709\u6700\u4f73\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u62d3\u6251\u8bbe\u7f6e\u4e0bF1\u5206\u6570\u4ec5\u4e0b\u964d\u7ea612%\uff0c\u800c\u7eafRNN\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe60%\u3002", "motivation": "\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9002\u5e94\u56e0\u91cd\u6784\u3001\u8bbe\u5907\u6545\u969c\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u96c6\u6210\u800c\u4e0d\u65ad\u53d8\u5316\u7684\u7535\u7f51\u62d3\u6251\u3002\u5f53\u524d\u6700\u5148\u8fdb\u7684RNN+GNN\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\uff0c\u4f46\u5176\u4ed6\u66f4\u5148\u8fdb\u7684GNN\u67b6\u6784\u5c1a\u672a\u5728\u6b64\u9886\u57df\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728RNN+GNN\u6d41\u6c34\u7ebf\u8bbe\u7f6e\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cdGNN\u67b6\u6784\uff0c\u9996\u6b21\u5c06GraphSAGE\u548cGraph Attention(GAT\u3001GATv2)\u7528\u4e8e\u6545\u969c\u8bca\u65ad\uff0c\u5e76\u4e0e\u65e9\u671f\u63d0\u51fa\u7684RGCN\u89e3\u51b3\u65b9\u6848\u4ee5\u53ca\u7eafRNN\u6a21\u578b(\u7279\u522b\u662fGRU)\u8fdb\u884c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u6a21\u578b\u5728\u4e0d\u540c\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728IEEE 123\u8282\u70b9\u914d\u7535\u7f51\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRGATv2\u5177\u6709\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u62d3\u6251\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u9ad8\u6027\u80fd\uff0cF1\u5206\u6570\u4ec5\u4e0b\u964d\u7ea612%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7eafRNN\u6a21\u578b\u57fa\u672c\u5931\u6548\uff0cF1\u5206\u6570\u4e0b\u964d\u9ad8\u8fbe60%\uff0c\u5176\u4ed6RGNN\u53d8\u4f53\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0cF1\u5206\u6570\u4e0b\u964d\u9ad8\u8fbe25%\u3002", "conclusion": "RGATv2\u6a21\u578b\u5728\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u9002\u5e94\u7535\u7f51\u62d3\u6251\u53d8\u5316\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04232", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04232", "abs": "https://arxiv.org/abs/2510.04232", "authors": ["Amin Ahmadi Kasani", "Hedieh Sajedi"], "title": "Detection of retinal diseases using an accelerated reused convolutional network", "comment": null, "summary": "Convolutional neural networks are continually evolving, with some efforts\naimed at improving accuracy, others at increasing speed, and some at enhancing\naccessibility. Improving accessibility broadens the application of neural\nnetworks across a wider range of tasks, including the detection of eye\ndiseases. Early diagnosis of eye diseases and consulting an ophthalmologist can\nprevent many vision disorders. Given the importance of this issue, various\ndatasets have been collected from the cornea to facilitate the process of\nmaking neural network models. However, most of the methods introduced in the\npast are computationally complex. In this study, we tried to increase the\naccessibility of deep neural network models. We did this at the most\nfundamental level, specifically by redesigning and optimizing the convolutional\nlayers. By doing so, we created a new general model that incorporates our novel\nconvolutional layer named ArConv layers. Thanks to the efficient performance of\nthis new layer, the model has suitable complexity for use in mobile phones and\ncan perform the task of diagnosing the presence of disease with high accuracy.\nThe final model we present contains only 1.3 million parameters. In comparison\nto the MobileNetV2 model, which has 2.2 million parameters, our model\ndemonstrated better accuracy when trained and evaluated on the RfMiD dataset\nunder identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on\nthe RfMiD test set.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArConv\u7684\u65b0\u578b\u5377\u79ef\u5c42\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u5377\u79ef\u5c42\u6765\u521b\u5efa\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ec5\u542b130\u4e07\u53c2\u6570\uff0c\u5728RfMiD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eMobileNetV2\uff0c\u51c6\u786e\u7387\u8fbe\u52300.9328\u3002", "motivation": "\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u7279\u522b\u662f\u5728\u773c\u79d1\u75be\u75c5\u8bca\u65ad\u9886\u57df\uff0c\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u6765\u6269\u5927\u795e\u7ecf\u7f51\u7edc\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5728\u57fa\u7840\u5c42\u9762\u91cd\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u5377\u79ef\u5c42\uff0c\u5f00\u53d1\u4e86\u65b0\u578bArConv\u5377\u79ef\u5c42\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u901a\u7528\u6a21\u578b\uff0c\u53c2\u6570\u6570\u91cf\u4ec5\u4e3a130\u4e07\u3002", "result": "\u5728RfMiD\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u52300.9328\uff0c\u4f18\u4e8e\u62e5\u6709220\u4e07\u53c2\u6570\u7684MobileNetV2\u6a21\u578b\uff08\u51c6\u786e\u73870.9266\uff09\uff0c\u540c\u65f6\u5177\u6709\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5377\u79ef\u5c42\u8bbe\u8ba1\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u5408\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u7528\u4e8e\u773c\u79d1\u75be\u75c5\u8bca\u65ad\u3002"}}
{"id": "2510.03648", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03648", "abs": "https://arxiv.org/abs/2510.03648", "authors": ["Huijing Zhang", "Muyang Cao", "Linshan Jiang", "Xin Du", "Di Yu", "Changze Lv", "Shuiguang Deng"], "title": "SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network", "comment": null, "summary": "Continuous learning of novel classes is crucial for edge devices to preserve\ndata privacy and maintain reliable performance in dynamic environments.\nHowever, the scenario becomes particularly challenging when data samples are\ninsufficient, requiring on-device few-shot class-incremental learning (FSCIL)\nto maintain consistent model performance. Although existing work has explored\nparameter-efficient FSCIL frameworks based on artificial neural networks\n(ANNs), their deployment is still fundamentally constrained by limited device\nresources. Inspired by neural mechanisms, Spiking neural networks (SNNs)\nprocess spatiotemporal information efficiently, offering lower energy\nconsumption, greater biological plausibility, and compatibility with\nneuromorphic hardware than ANNs. In this work, we present an SNN-based method\nfor On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We\nfirst propose sparsity-conditioned neuronal dynamics, in which most neurons\nremain stable while a subset stays active, thereby mitigating catastrophic\nforgetting. To further cope with spike non-differentiability in gradient\nestimation, we employ zeroth-order optimization. Moreover, during incremental\nlearning sessions, we enhance the discriminability of new classes through\nsubspace projection, which alleviates overfitting to novel classes. Extensive\nexperiments conducted on two standard benchmark datasets (CIFAR100 and\nMini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,\nand N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,\nspecifically achieving at least 4.01% improvement at the last incremental\nsession on Mini-ImageNet and 20% lower energy cost over baseline methods with\npractical implementation.", "AI": {"tldr": "\u63d0\u51faSAFA-SNN\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u5907\u7aef\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff0c\u901a\u8fc7\u7a00\u758f\u611f\u77e5\u548c\u5feb\u901f\u81ea\u9002\u5e94\u673a\u5236\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u4e14\u80fd\u8017\u66f4\u4f4e\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b0\u7c7b\u522b\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u53ef\u9760\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bbe\u5907\u8d44\u6e90\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u66f4\u4f4e\u80fd\u8017\u548c\u66f4\u597d\u7684\u751f\u7269\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u6761\u4ef6\u795e\u7ecf\u5143\u52a8\u6001\u673a\u5236\uff0c\u5927\u90e8\u5206\u795e\u7ecf\u5143\u4fdd\u6301\u7a33\u5b9a\u800c\u90e8\u5206\u4fdd\u6301\u6d3b\u8dc3\uff1b\u4f7f\u7528\u96f6\u9636\u4f18\u5316\u5904\u7406\u8109\u51b2\u4e0d\u53ef\u5fae\u95ee\u9898\uff1b\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u589e\u5f3a\u65b0\u7c7b\u522b\u7684\u53ef\u533a\u5206\u6027\u3002", "result": "\u5728CIFAR100\u3001Mini-ImageNet\u548c\u4e09\u4e2a\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAFA-SNN\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Mini-ImageNet\u6700\u540e\u589e\u91cf\u4f1a\u8bdd\u4e2d\u81f3\u5c11\u63d0\u53474.01%\uff0c\u80fd\u8017\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4f4e20%\u3002", "conclusion": "SAFA-SNN\u4e3a\u8bbe\u5907\u7aef\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u80fd\u8017\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03959", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03959", "abs": "https://arxiv.org/abs/2510.03959", "authors": ["Iryna Stanishevska"], "title": "Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model", "comment": "23 pages (main), 70 pages incl. appendices; figures & tables as in\n  manuscript. Code (main figure, synthetic data):\n  https://github.com/<your-login>/peak-outage-forecasting License: CC BY 4.0\n  (preprint)", "summary": "Thunderstorm-driven outages are difficult to predict because most storms do\nnot cause damage, convective processes occur rapidly and chaotically, and the\navailable public data are both noisy and incomplete. We develop a 24-48 h\nearly-warning model for summer, thunderstorm-related outages in Michigan using\nonly open sources (EAGLE-I for ground truth; METAR for weather). We use the\npublicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge\nNational Laboratory for the U.S. Department of Energy. The pipeline preserves\nconvective micro-signals from a sparse station network via parameter-specific\nkriging with hourly variograms and targeted overdrafting to retain extremes,\nand builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW\nspatial aggregates) capturing precursors of severe convection (moisture\nadvection, wind shifts, and pressure drops). The two-stage model design,\ncombining a logistic gate and an LSTM regressor, limits routine periods and\nreduces noise exposure. The study uses event-centric metrics (cluster-based\nhits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour\nwindows around state-level peaks (>= 50,000), with uncertainty quantified by\nhourly moving-block bootstrap.\n  On the test sample, Two-Stage detects more reference peaks across all windows\n(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra\nfalse alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at\n+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48\nh (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP\nanalysis confirms moisture-advection and wind/gust precursors, underscoring the\nvalue of the feature engineering. Despite open-data noise, the feature-driven\npipeline yields actionable, event-focused early warnings for thunderstorm\noutages.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a24-48\u5c0f\u65f6\u96f7\u66b4\u505c\u7535\u65e9\u671f\u9884\u8b66\u6a21\u578b\uff0c\u4f7f\u7528\u516c\u5f00\u6570\u636e\u6e90\u548c\u4e24\u9636\u6bb5\u6a21\u578b\u8bbe\u8ba1\uff08\u903b\u8f91\u95e8+LSTM\u56de\u5f52\u5668\uff09\uff0c\u5728\u5bc6\u6b47\u6839\u5dde\u590f\u5b63\u96f7\u66b4\u76f8\u5173\u505c\u7535\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u96f7\u66b4\u9a71\u52a8\u7684\u505c\u7535\u96be\u4ee5\u9884\u6d4b\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u98ce\u66b4\u4e0d\u4f1a\u9020\u6210\u635f\u5bb3\uff0c\u5bf9\u6d41\u8fc7\u7a0b\u5feb\u901f\u4e14\u6df7\u4e71\uff0c\u4e14\u53ef\u7528\u7684\u516c\u5171\u6570\u636e\u65e2\u5608\u6742\u53c8\u4e0d\u5b8c\u6574\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\uff08EAGLE-I\u505c\u7535\u6570\u636e\u3001METAR\u5929\u6c14\u6570\u636e\uff09\uff0c\u901a\u8fc7\u53c2\u6570\u7279\u5b9a\u514b\u91cc\u91d1\u63d2\u503c\u548c\u76ee\u6807\u8fc7\u91c7\u6837\u4fdd\u7559\u6781\u7aef\u503c\uff0c\u6784\u5efa\u56e0\u679c\u65f6\u7a7a\u7279\u5f81\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6a21\u578b\u8bbe\u8ba1\uff08\u903b\u8f91\u95e8+LSTM\u56de\u5f52\u5668\uff09\u3002", "result": "\u4e24\u9636\u6bb5\u6a21\u578b\u5728\u6240\u6709\u65f6\u95f4\u7a97\u53e3\u68c0\u6d4b\u5230\u66f4\u591a\u53c2\u8003\u5cf0\u503c\uff08\u5982\u00b148\u5c0f\u65f6\u8bb0\u5f553/4 vs 2/4\uff09\uff0c\u5728\u5cf0\u503c\u9644\u8fd1\u663e\u793a\u9002\u5ea6\u5e45\u5ea6\u589e\u76ca\uff08\u00b10-12\u5c0f\u65f6cMASE\u964d\u4f4e2-3%\uff09\uff0c\u4f46\u603b\u4f53\u8bef\u5dee\u4e0e\u5355\u6b65LSTM\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "\u5c3d\u7ba1\u516c\u5f00\u6570\u636e\u5b58\u5728\u566a\u58f0\uff0c\u4f46\u7279\u5f81\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\u4e3a\u96f7\u66b4\u505c\u7535\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u3001\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u65e9\u671f\u9884\u8b66\u3002"}}
{"id": "2510.03971", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03971", "abs": "https://arxiv.org/abs/2510.03971", "authors": ["Jatin Prakash", "Anirudh Buvanesh"], "title": "What Can You Do When You Have Zero Rewards During RL?", "comment": null, "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective\nfor improving large language models (LLMs) on complex reasoning tasks. However,\nits success often depends on the base model occasionally sampling correct\nsolutions. When no correct solutions are sampled, training encounters a\nzero-reward barrier where learning stalls due to zero gradients. We study this\nscenario through the graph search task introduced in Bachmann et al. (2024) and\nevaluate recent methods that incorporate desirable components such as dense\nrewards, diversity incentives, and improved credit assignment. Our experiments\nshow that none of these approaches overcome the zero-reward barrier if the base\nmodel never produces a correct answer. In contrast, we find that a simple\ndata-centric intervention of adding easier samples to the training set enables\nthe model to eventually solve the original hard task despite starting from zero\nreward. Importantly, this succeeds without modifying the RL algorithm itself.\nBecause official implementations of several baselines were unavailable, we\ndeveloped our own, which allowed us to conduct a detailed analysis of their\nfailure modes. We release these implementations to support further research at:\nhttps://github.com/rl4reasoning/rl-baselines", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f53\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u91c7\u6837\u5230\u6b63\u786e\u89e3\u65f6\u51fa\u73b0\u7684\u96f6\u5956\u52b1\u969c\u788d\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u514b\u670d\u8fd9\u4e00\u969c\u788d\uff0c\u4f46\u901a\u8fc7\u7b80\u5355\u7684\u6570\u636e\u5e72\u9884\uff08\u5728\u8bad\u7ec3\u96c6\u4e2d\u6dfb\u52a0\u7b80\u5355\u6837\u672c\uff09\u53ef\u4ee5\u89e3\u51b3\u539f\u59cb\u56f0\u96be\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u96f6\u5956\u52b1\u969c\u788d\u95ee\u9898\uff0c\u5373\u5f53\u57fa\u7840\u6a21\u578b\u4ece\u672a\u91c7\u6837\u5230\u6b63\u786e\u89e3\u65f6\uff0c\u8bad\u7ec3\u4f1a\u56e0\u96f6\u68af\u5ea6\u800c\u505c\u6ede\u3002", "method": "\u4f7f\u7528\u56fe\u641c\u7d22\u4efb\u52a1\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u5bc6\u96c6\u5956\u52b1\u3001\u591a\u6837\u6027\u6fc0\u52b1\u548c\u6539\u8fdb\u7684\u4fe1\u7528\u5206\u914d\uff09\uff0c\u5e76\u6d4b\u8bd5\u901a\u8fc7\u5411\u8bad\u7ec3\u96c6\u6dfb\u52a0\u7b80\u5355\u6837\u672c\u7684\u6570\u636e\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u514b\u670d\u96f6\u5956\u52b1\u969c\u788d\uff0c\u4f46\u6570\u636e\u5e72\u9884\u65b9\u6cd5\u80fd\u591f\u4f7f\u6a21\u578b\u6700\u7ec8\u89e3\u51b3\u539f\u59cb\u56f0\u96be\u4efb\u52a1\uff0c\u4e14\u65e0\u9700\u4fee\u6539RL\u7b97\u6cd5\u672c\u8eab\u3002", "conclusion": "\u6570\u636e\u5e72\u9884\u662f\u514b\u670d\u96f6\u5956\u52b1\u969c\u788d\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.04618", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04618", "abs": "https://arxiv.org/abs/2510.04618", "authors": ["Qizheng Zhang", "Changran Hu", "Shubhangi Upasani", "Boyuan Ma", "Fenglu Hong", "Vamsidhar Kamanuru", "Jay Rainton", "Chen Wu", "Mengmeng Ji", "Hanchen Li", "Urmish Thakker", "James Zou", "Kunle Olukotun"], "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", "comment": null, "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.", "AI": {"tldr": "ACE\u6846\u67b6\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u6f14\u5316\u7684\u5267\u672c\uff0c\u901a\u8fc7\u751f\u6210\u3001\u53cd\u601d\u548c\u7b56\u5c55\u7684\u6a21\u5757\u5316\u8fc7\u7a0b\u6765\u79ef\u7d2f\u3001\u4f18\u5316\u548c\u7ec4\u7ec7\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u7b80\u6d01\u6027\u504f\u89c1\u548c\u4e0a\u4e0b\u6587\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4e0a\u4e0b\u6587\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u7b80\u6d01\u6027\u504f\u89c1\uff08\u4e22\u5931\u9886\u57df\u6d1e\u5bdf\uff09\u548c\u4e0a\u4e0b\u6587\u5d29\u6e83\uff08\u8fed\u4ee3\u91cd\u5199\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u8be6\u7ec6\u77e5\u8bc6\u5e76\u968f\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ACE\u6846\u67b6\u91c7\u7528\u7ed3\u6784\u5316\u3001\u589e\u91cf\u5f0f\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u3001\u53cd\u601d\u548c\u7b56\u5c55\u4e09\u4e2a\u6a21\u5757\u5316\u8fc7\u7a0b\u6765\u7ba1\u7406\u548c\u4f18\u5316\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u79bb\u7ebf\u548c\u5728\u7ebf\u4e0a\u4e0b\u6587\u4f18\u5316\u3002", "result": "\u5728\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACE\u6bd4\u5f3a\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\uff1a\u4ee3\u7406\u4efb\u52a1\u63d0\u534710.6%\uff0c\u91d1\u878d\u4efb\u52a1\u63d0\u53478.6%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9002\u5e94\u5ef6\u8fdf\u548c\u90e8\u7f72\u6210\u672c\u3002\u5728AppWorld\u6392\u884c\u699c\u4e0a\uff0cACE\u4e0e\u9876\u7ea7\u751f\u4ea7\u7ea7\u4ee3\u7406\u5728\u6574\u4f53\u5e73\u5747\u5206\u4e0a\u6301\u5e73\uff0c\u5728\u66f4\u96be\u7684\u6d4b\u8bd5\u6311\u6218\u5206\u5272\u4e0a\u8d85\u8d8a\u5bf9\u624b\u3002", "conclusion": "\u5168\u9762\u3001\u6f14\u5316\u7684\u4e0a\u4e0b\u6587\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u81ea\u6211\u6539\u8fdb\u7684LLM\u7cfb\u7edf\uff0c\u4e14\u5f00\u9500\u8f83\u4f4e\u3002"}}
{"id": "2510.04020", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04020", "abs": "https://arxiv.org/abs/2510.04020", "authors": ["Hao Wu", "Yuan Gao", "Xingjian Shi", "Shuaipeng Li", "Fan Xu", "Fan Zhang", "Zhihong Zhu", "Weiyan Wang", "Xiao Luo", "Kun Wang", "Xian Wu", "Xiaomeng Huang"], "title": "Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models", "comment": null, "summary": "To address the dual challenges of inherent stochasticity and\nnon-differentiable metrics in physical spatiotemporal forecasting, we propose\nSpatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in\nModel-Based Reinforcement Learning. SFP constructs a novel Generative World\nModel to simulate diverse, high-fidelity future states, enabling an\n\"imagination-based\" environmental simulation. Within this framework, a base\nforecasting model acts as an agent, guided by a beam search-based planning\nalgorithm that leverages non-differentiable domain metrics as reward signals to\nexplore high-return future sequences. These identified high-reward candidates\nthen serve as pseudo-labels to continuously optimize the agent's policy through\niterative self-training, significantly reducing prediction error and\ndemonstrating exceptional performance on critical domain metrics like capturing\nextreme events.", "AI": {"tldr": "\u63d0\u51faSFP\uff08\u65f6\u7a7a\u9884\u6d4b\u5373\u89c4\u5212\uff09\u65b0\u8303\u5f0f\uff0c\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u968f\u673a\u6027\u548c\u4e0d\u53ef\u5fae\u5ea6\u91cf\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u4e16\u754c\u6a21\u578b\u6a21\u62df\u672a\u6765\u72b6\u6001\uff0c\u4f7f\u7528\u6ce2\u675f\u641c\u7d22\u89c4\u5212\u7b97\u6cd5\u5bfb\u627e\u9ad8\u56de\u62a5\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u81ea\u8bad\u7ec3\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7269\u7406\u65f6\u7a7a\u9884\u6d4b\u4e2d\u56fa\u6709\u7684\u968f\u673a\u6027\u548c\u4e0d\u53ef\u5fae\u5ea6\u91cf\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u4f18\u5316\u9886\u57df\u7279\u5b9a\u6307\u6807\u3002", "method": "\u6784\u5efa\u751f\u6210\u4e16\u754c\u6a21\u578b\u6a21\u62df\u591a\u6837\u5316\u9ad8\u4fdd\u771f\u672a\u6765\u72b6\u6001\uff0c\u57fa\u7840\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u6ce2\u675f\u641c\u7d22\u89c4\u5212\u7b97\u6cd5\u4ee5\u4e0d\u53ef\u5fae\u9886\u57df\u5ea6\u91cf\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u3002", "result": "\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\uff0c\u5728\u6355\u83b7\u6781\u7aef\u4e8b\u4ef6\u7b49\u5173\u952e\u9886\u57df\u5ea6\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SFP\u4e3a\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u6846\u67b6\uff0c\u6210\u529f\u6574\u5408\u4e86\u751f\u6210\u5efa\u6a21\u3001\u89c4\u5212\u7b97\u6cd5\u548c\u81ea\u8bad\u7ec3\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.05015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05015", "abs": "https://arxiv.org/abs/2510.05015", "authors": ["Nabil Daiyan", "Md Rakibul Haque"], "title": "Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns", "comment": "5 pages, 11 figures, published on 2024 2nd International Conference\n  on Information and Communication Technology (ICICT 2024)", "summary": "Parkinson's disease (PD) is a progressive neurodegenerative condition\ncharacterized by the death of dopaminergic neurons, leading to various movement\ndisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,\nyet traditional diagnostic methods are often cumbersome and costly. In this\nstudy, a machine learning-based approach is proposed using hand-drawn spiral\nand wave images as potential biomarkers for PD detection. Our methodology\nleverages convolutional neural networks (CNNs), transfer learning, and\nattention mechanisms to improve model performance and resilience against\noverfitting. To enhance the diversity and richness of both spiral and wave\ncategories, the training dataset undergoes augmentation to increase the number\nof images. The proposed architecture comprises three phases: utilizing\npre-trained CNNs, incorporating custom convolutional layers, and ensemble\nvoting. Employing hard voting further enhances performance by aggregating\npredictions from multiple models. Experimental results show promising accuracy\nrates. For spiral images, weighted average precision, recall, and F1-score are\n90%, and for wave images, they are 96.67%. After combining the predictions\nthrough ensemble hard voting, the overall accuracy is 93.3%. These findings\nunderscore the potential of machine learning in early PD diagnosis, offering a\nnon-invasive and cost-effective solution to improve patient outcomes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u624b\u7ed8\u87ba\u65cb\u548c\u6ce2\u6d6a\u56fe\u50cf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528CNN\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u96c6\u6210\u6295\u7968\u8fbe\u523093.3%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u5f88\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u7e41\u7410\u6602\u8d35\uff0c\u9700\u8981\u975e\u4fb5\u5165\u6027\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u3001\u81ea\u5b9a\u4e49\u5377\u79ef\u5c42\u548c\u96c6\u6210\u6295\u7968\u7684\u4e09\u9636\u6bb5\u67b6\u6784\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u87ba\u65cb\u56fe\u50cf\u7cbe\u5ea690%\uff0c\u6ce2\u6d6a\u56fe\u50cf\u7cbe\u5ea696.67%\uff0c\u96c6\u6210\u6295\u7968\u540e\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523093.3%\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04058", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04058", "abs": "https://arxiv.org/abs/2510.04058", "authors": ["Subhodip Panda", "MS Varun", "Shreyans Jain", "Sarthak Kumar Maharana", "Prathosh A. P"], "title": "Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints", "comment": null, "summary": "For a responsible and safe deployment of diffusion models in various domains,\nregulating the generated outputs from these models is desirable because such\nmodels could generate undesired, violent, and obscene outputs. To tackle this\nproblem, recent works use machine unlearning methodology to forget training\ndata points containing these undesired features from pre-trained generative\nmodels. However, these methods proved to be ineffective in data-constrained\nsettings where the whole training dataset is inaccessible. Thus, the principal\nobjective of this work is to propose a machine unlearning methodology that can\nprevent the generation of outputs containing undesired features from a\npre-trained diffusion model in such a data-constrained setting. Our proposed\nmethod, termed as Variational Diffusion Unlearning (VDU), is a computationally\nefficient method that only requires access to a subset of training data\ncontaining undesired features. Our approach is inspired by the variational\ninference framework with the objective of minimizing a loss function consisting\nof two terms: plasticity inducer and stability regularizer. Plasticity inducer\nreduces the log-likelihood of the undesired training data points, while the\nstability regularizer, essential for preventing loss of image generation\nquality, regularizes the model in parameter space. We validate the\neffectiveness of our method through comprehensive experiments for both class\nunlearning and feature unlearning. For class unlearning, we unlearn some\nuser-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a\npre-trained unconditional denoising diffusion probabilistic model (DDPM).\nSimilarly, for feature unlearning, we unlearn the generation of certain\nhigh-level features from a pre-trained Stable Diffusion model", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53d8\u5206\u6269\u6563\u9057\u5fd8(VDU)\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u79fb\u9664\u4e0d\u826f\u7279\u5f81\u7684\u751f\u6210\u80fd\u529b\uff0c\u4ec5\u9700\u8bbf\u95ee\u5305\u542b\u4e0d\u826f\u7279\u5f81\u7684\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\u3002", "motivation": "\u4e3a\u4e86\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u6269\u6563\u6a21\u578b\uff0c\u9700\u8981\u8c03\u8282\u5176\u751f\u6210\u8f93\u51fa\uff0c\u9632\u6b62\u4ea7\u751f\u4e0d\u826f\u3001\u66b4\u529b\u548c\u6deb\u79fd\u5185\u5bb9\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4ec5\u9700\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u7684\u9057\u5fd8\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u5851\u6027\u8bf1\u5bfc\u5668\u548c\u7a33\u5b9a\u6027\u6b63\u5219\u5668\u7684\u635f\u5931\u51fd\u6570\u3002\u5851\u6027\u8bf1\u5bfc\u5668\u964d\u4f4e\u4e0d\u826f\u8bad\u7ec3\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136\uff0c\u7a33\u5b9a\u6027\u6b63\u5219\u5668\u5728\u53c2\u6570\u7a7a\u95f4\u6b63\u5219\u5316\u6a21\u578b\u4ee5\u9632\u6b62\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u7c7b\u522b\u9057\u5fd8\u548c\u7279\u5f81\u9057\u5fd8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u4ece\u9884\u8bad\u7ec3DDPM\u6a21\u578b\u4e2d\u79fb\u9664MNIST\u3001CIFAR-10\u548ctinyImageNet\u7684\u6307\u5b9a\u7c7b\u522b\uff0c\u4ee5\u53ca\u4eceStable Diffusion\u4e2d\u79fb\u9664\u7279\u5b9a\u9ad8\u7ea7\u7279\u5f81\u3002", "conclusion": "VDU\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u6709\u6548\u9632\u6b62\u6269\u6563\u6a21\u578b\u751f\u6210\u5305\u542b\u4e0d\u826f\u7279\u5f81\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2510.04217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04217", "abs": "https://arxiv.org/abs/2510.04217", "authors": ["Chenlu Ding", "Jiancan Wu", "Leheng Sheng", "Fan Zhang", "Yancheng Yuan", "Xiang Wang", "Xiangnan He"], "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities across vision-language tasks, yet their large-scale deployment\nraises pressing concerns about memorized private data, outdated knowledge, and\nharmful content. Existing unlearning approaches for MLLMs typically adapt\ntraining-based strategies such as gradient ascent or preference optimization,\nbut these methods are computationally expensive, irreversible, and often\ndistort retained knowledge. In this work, we propose MLLMEraser, an\ninput-aware, training-free framework for test-time unlearning. Our approach\nleverages activation steering to enable dynamic knowledge erasure without\nparameter updates. Specifically, we construct a multimodal erasure direction by\ncontrasting adversarially perturbed, knowledge-recall image-text pairs with\nknowledge-erasure counterparts, capturing both textual and visual\ndiscrepancies. To prevent unnecessary interference, we further design an\ninput-aware steering mechanism that adaptively determines when and how the\nerasure direction should be applied, preserving utility on retained knowledge\nwhile enforcing forgetting on designated content. Experiments on LLaVA-1.5 and\nQwen-2.5-VL demonstrate that MLLMEraser consistently outperforms\nstate-of-the-art MLLM unlearning baselines, achieving stronger forgetting\nperformance with lower computational cost and minimal utility degradation.", "AI": {"tldr": "\u63d0\u51faMLLMEraser\uff0c\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5bfc\u5411\u7684\u8bad\u7ec3\u514d\u8d39\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u77e5\u8bc6\u56de\u5fc6\u548c\u77e5\u8bc6\u9057\u5fd8\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u6784\u5efa\u591a\u6a21\u6001\u9057\u5fd8\u65b9\u5411\uff0c\u5b9e\u73b0\u52a8\u6001\u77e5\u8bc6\u64e6\u9664\u800c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u9762\u4e34\u8bb0\u5fc6\u9690\u79c1\u6570\u636e\u3001\u8fc7\u65f6\u77e5\u8bc6\u548c\u6709\u5bb3\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u8bad\u7ec3\u7684\u9057\u5fd8\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e0d\u53ef\u9006\u4e14\u5bb9\u6613\u626d\u66f2\u4fdd\u7559\u77e5\u8bc6\u3002", "method": "\u5229\u7528\u6fc0\u6d3b\u5bfc\u5411\u6280\u672f\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u6297\u6027\u6270\u52a8\u7684\u77e5\u8bc6\u56de\u5fc6\u548c\u77e5\u8bc6\u9057\u5fd8\u56fe\u50cf-\u6587\u672c\u5bf9\u6784\u5efa\u591a\u6a21\u6001\u9057\u5fd8\u65b9\u5411\uff0c\u5e76\u8bbe\u8ba1\u8f93\u5165\u611f\u77e5\u7684\u5bfc\u5411\u673a\u5236\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u53ca\u5982\u4f55\u5e94\u7528\u9057\u5fd8\u65b9\u5411\u3002", "result": "\u5728LLaVA-1.5\u548cQwen-2.5-VL\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMLLMEraser\u5728\u9057\u5fd8\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u57fa\u7ebf\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u4e14\u6548\u7528\u9000\u5316\u6700\u5c0f\u3002", "conclusion": "MLLMEraser\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4fdd\u7559\u77e5\u8bc6\u6548\u7528\u7684\u540c\u65f6\u6709\u6548\u64e6\u9664\u6307\u5b9a\u5185\u5bb9\u3002"}}
{"id": "2510.04547", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04547", "abs": "https://arxiv.org/abs/2510.04547", "authors": ["Seunghyeon Kim", "Jinho Kim", "Taesun Yeom", "Wonpyo Park", "Kyuyeun Kim", "Jaeho Lee"], "title": "Post-training quantization of vision encoders needs prefixing registers", "comment": null, "summary": "Transformer-based vision encoders -- such as CLIP -- are central to\nmultimodal intelligence, powering applications from autonomous web agents to\nrobotic control. Since these applications often demand real-time processing of\nmassive visual data, reducing the inference cost of vision encoders is\ncritical. Post-training quantization offers a practical path, but remains\nchallenging even at 8-bit precision due to massive-scale activations (i.e.,\noutliers). In this work, we propose $\\textit{RegCache}$, a training-free\nalgorithm to mitigate outliers in vision encoders, enabling quantization with\nsignificantly smaller accuracy drops. The proposed RegCache introduces\noutlier-prone yet semantically meaningless prefix tokens to the target vision\nencoder, which prevents other tokens from having outliers. Notably, we observe\nthat outliers in vision encoders behave differently from those in language\nmodels, motivating two technical innovations: middle-layer prefixing and token\ndeletion. Experiments show that our method consistently improves the accuracy\nof quantized models across both text-supervised and self-supervised vision\nencoders.", "AI": {"tldr": "\u63d0\u51faRegCache\u8bad\u7ec3\u514d\u8d39\u7b97\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u524d\u7f00token\u6765\u7f13\u89e3\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u91cf\u5316", "motivation": "\u89c6\u89c9\u7f16\u7801\u5668\u5728\u5b9e\u65f6\u5904\u7406\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u65f6\u9700\u8981\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4f46\u540e\u8bad\u7ec3\u91cf\u5316\u57288\u4f4d\u7cbe\u5ea6\u4e0b\u4ecd\u56e0\u5927\u89c4\u6a21\u6fc0\u6d3b\u5f02\u5e38\u503c\u800c\u5177\u6709\u6311\u6218\u6027", "method": "\u5f15\u5165\u5f02\u5e38\u503c\u6613\u53d1\u4f46\u8bed\u4e49\u65e0\u610f\u4e49\u7684\u524d\u7f00token\u5230\u76ee\u6807\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u9632\u6b62\u5176\u4ed6token\u4ea7\u751f\u5f02\u5e38\u503c\uff0c\u91c7\u7528\u4e2d\u95f4\u5c42\u524d\u7f00\u548ctoken\u5220\u9664\u6280\u672f", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u76d1\u7763\u548c\u81ea\u76d1\u7763\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u4e00\u81f4\u63d0\u9ad8\u4e86\u91cf\u5316\u6a21\u578b\u7684\u51c6\u786e\u6027", "conclusion": "RegCache\u80fd\u6709\u6548\u7f13\u89e3\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u91cf\u5316"}}
{"id": "2510.04341", "categories": ["cs.LG", "cs.AI", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.04341", "abs": "https://arxiv.org/abs/2510.04341", "authors": ["G. Niklas Noren", "Eva-Lisa Meldau", "Johan Ellenius"], "title": "Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies", "comment": "28 pages, 2 figures", "summary": "Many high-stakes AI applications target low-prevalence events, where apparent\naccuracy can conceal limited real-world value. Relevant AI models range from\nexpert-defined rules and traditional machine learning to generative LLMs\nconstrained for classification. We outline key considerations for critical\nappraisal of AI in rare-event recognition, including problem framing and test\nset design, prevalence-aware statistical evaluation, robustness assessment, and\nintegration into human workflows. In addition, we propose an approach to\nstructured case-level examination (SCLE), to complement statistical performance\nevaluation, and a comprehensive checklist to guide procurement or development\nof AI models for rare-event recognition. We instantiate the framework in\npharmacovigilance, drawing on three studies: rule-based retrieval of\npregnancy-related reports; duplicate detection combining machine learning with\nprobabilistic record linkage; and automated redaction of person names using an\nLLM. We highlight pitfalls specific to the rare-event setting including\noptimism from unrealistic class balance and lack of difficult positive controls\nin test sets - and show how cost-sensitive targets align model performance with\noperational value. While grounded in pharmacovigilance practice, the principles\ngeneralize to domains where positives are scarce and error costs may be\nasymmetric.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30AI\u6a21\u578b\u5728\u7f55\u89c1\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u6027\u80fd\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u4f4e\u6d41\u884c\u7387\u573a\u666f\u4e0b\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5305\u62ec\u95ee\u9898\u5b9a\u4e49\u3001\u6d4b\u8bd5\u96c6\u8bbe\u8ba1\u3001\u7edf\u8ba1\u8bc4\u4f30\u548c\u4eba\u7c7b\u5de5\u4f5c\u6d41\u6574\u5408\u3002", "motivation": "\u8bb8\u591a\u9ad8\u98ce\u9669\u7684AI\u5e94\u7528\u9488\u5bf9\u4f4e\u6d41\u884c\u7387\u4e8b\u4ef6\uff0c\u8868\u9762\u4e0a\u7684\u51c6\u786e\u6027\u53ef\u80fd\u63a9\u76d6\u4e86\u6709\u9650\u7684\u73b0\u5b9e\u4ef7\u503c\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fddAI\u6a21\u578b\u5728\u7f55\u89c1\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u6848\u4f8b\u7ea7\u68c0\u67e5(SCLE)\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u5168\u9762\u7684\u68c0\u67e5\u6e05\u5355\u6765\u6307\u5bfcAI\u6a21\u578b\u7684\u91c7\u8d2d\u6216\u5f00\u53d1\uff0c\u5e76\u5728\u836f\u7269\u8b66\u6212\u9886\u57df\u901a\u8fc7\u4e09\u4e2a\u7814\u7a76\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u7d22\u3001\u673a\u5668\u5b66\u4e60\u4e0e\u6982\u7387\u8bb0\u5f55\u94fe\u63a5\u7684\u7ed3\u5408\u3001\u4f7f\u7528LLM\u7684\u81ea\u52a8\u5316\u7f16\u8f91\u3002", "result": "\u8bc6\u522b\u4e86\u7f55\u89c1\u4e8b\u4ef6\u8bbe\u7f6e\u4e2d\u7684\u7279\u5b9a\u9677\u9631\uff0c\u5305\u62ec\u4e0d\u5207\u5b9e\u9645\u7684\u7c7b\u522b\u5e73\u8861\u5bfc\u81f4\u7684\u4e50\u89c2\u4f30\u8ba1\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7f3a\u4e4f\u56f0\u96be\u7684\u9633\u6027\u5bf9\u7167\uff0c\u5e76\u5c55\u793a\u4e86\u6210\u672c\u654f\u611f\u76ee\u6807\u5982\u4f55\u4f7f\u6a21\u578b\u6027\u80fd\u4e0e\u64cd\u4f5c\u4ef7\u503c\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u867d\u7136\u57fa\u4e8e\u836f\u7269\u8b66\u6212\u5b9e\u8df5\uff0c\u4f46\u8fd9\u4e9b\u539f\u5219\u53ef\u63a8\u5e7f\u5230\u9633\u6027\u6837\u672c\u7a00\u7f3a\u4e14\u9519\u8bef\u6210\u672c\u53ef\u80fd\u4e0d\u5bf9\u79f0\u7684\u9886\u57df\uff0c\u4e3a\u7f55\u89c1\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u7684AI\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2510.04342", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04342", "abs": "https://arxiv.org/abs/2510.04342", "authors": ["Harshil Vejendla"], "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics", "comment": "MIT URTC Technical Paper (Oral), 5 pages, 4 figures", "summary": "Forecasting chaotic systems is a cornerstone challenge in many scientific\nfields, complicated by the exponential amplification of even infinitesimal\nprediction errors. Modern machine learning approaches often falter due to two\nopposing pitfalls: over-specializing on a single, well-known chaotic system\n(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing\nvast, unrelated time-series, which prevents the model from learning the nuances\nof any specific dynamical regime. We propose Curriculum Chaos Forecasting\n(CCF), a training paradigm that bridges this gap. CCF organizes training data\nbased on fundamental principles of dynamical systems theory, creating a\ncurriculum that progresses from simple, periodic behaviors to highly complex,\nchaotic dynamics. We quantify complexity using the largest Lyapunov exponent\nand attractor dimension, two well-established metrics of chaos. By first\ntraining a sequence model on predictable systems and gradually introducing more\nchaotic trajectories, CCF enables the model to build a robust and generalizable\nrepresentation of dynamical behaviors. We curate a library of over 50 synthetic\nODE/PDE systems to build this curriculum. Our experiments show that\npre-training with CCF significantly enhances performance on unseen, real-world\nbenchmarks. On datasets including Sunspot numbers, electricity demand, and\nhuman ECG signals, CCF extends the valid prediction horizon by up to 40%\ncompared to random-order training and more than doubles it compared to training\non real-world data alone. We demonstrate that this benefit is consistent across\nvarious neural architectures (GRU, Transformer) and provide extensive ablations\nto validate the importance of the curriculum's structure.", "AI": {"tldr": "\u63d0\u51fa\u8bfe\u7a0b\u6df7\u6c8c\u9884\u6d4b(CCF)\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u4ece\u7b80\u5355\u5468\u671f\u6027\u7cfb\u7edf\u5230\u590d\u6742\u6df7\u6c8c\u7cfb\u7edf\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u63d0\u9ad8\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5728\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u4e2d\u7684\u4e24\u4e2a\u5bf9\u7acb\u95ee\u9898\uff1a\u8fc7\u5ea6\u4e13\u6ce8\u4e8e\u5355\u4e00\u6df7\u6c8c\u7cfb\u7edf\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u6216\u6df7\u5408\u5927\u91cf\u4e0d\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u5bfc\u81f4\u65e0\u6cd5\u5b66\u4e60\u7279\u5b9a\u52a8\u6001\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u52a8\u529b\u5b66\u7cfb\u7edf\u7406\u8bba\u7ec4\u7ec7\u8bad\u7ec3\u6570\u636e\uff0c\u4f7f\u7528\u6700\u5927Lyapun\u592b\u6307\u6570\u548c\u5438\u5f15\u5b50\u7ef4\u5ea6\u91cf\u5316\u590d\u6742\u6027\uff0c\u6784\u5efa\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u8bad\u7ec3\u8bfe\u7a0b\u3002\u6536\u96c650\u591a\u4e2a\u5408\u6210ODE/PDE\u7cfb\u7edf\u6784\u5efa\u8bfe\u7a0b\u5e93\u3002", "result": "\u5728\u592a\u9633\u9ed1\u5b50\u6570\u3001\u7535\u529b\u9700\u6c42\u548c\u4eba\u7c7bECG\u4fe1\u53f7\u7b49\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCF\u5c06\u6709\u6548\u9884\u6d4b\u8303\u56f4\u5ef6\u957f\u8fbe40%\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u63d0\u5347\u8d85\u8fc7\u4e24\u500d\u3002", "conclusion": "CCF\u8bad\u7ec3\u8303\u5f0f\u80fd\u663e\u8457\u63d0\u5347\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u8bfe\u7a0b\u7ed3\u6784\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.04357", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.04357", "abs": "https://arxiv.org/abs/2510.04357", "authors": ["Anoushka Harit", "Zhongtian Sun", "Jongmin Yu"], "title": "From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere", "comment": "6th ACM International Conference on AI in Finance", "summary": "We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel\narchitecture for interpretable financial time-series forecasting that unifies\n\\emph{Granger-causal hypergraph structure}, \\emph{Riemannian geometry}, and\n\\emph{causally masked Transformer attention}. CSHT models the directional\ninfluence of financial news and sentiment on asset returns by extracting\nmultivariate Granger-causal dependencies, which are encoded as directional\nhyperedges on the surface of a hypersphere. Attention is constrained via\nangular masks that preserve both temporal directionality and geometric\nconsistency. Evaluated on S\\&P 500 data from 2018 to 2023, including the 2020\nCOVID-19 shock, CSHT consistently outperforms baselines across return\nprediction, regime classification, and top-asset ranking tasks. By enforcing\npredictive causal structure and embedding variables in a Riemannian manifold,\nCSHT delivers both \\emph{robust generalisation across market regimes} and\n\\emph{transparent attribution pathways} from macroeconomic events to\nstock-level responses. These results suggest that CSHT is a principled and\npractical solution for trustworthy financial forecasting under uncertainty.", "AI": {"tldr": "\u63d0\u51faCausal Sphere Hypergraph Transformer (CSHT)\uff0c\u4e00\u79cd\u7ed3\u5408Granger\u56e0\u679c\u8d85\u56fe\u7ed3\u6784\u3001\u9ece\u66fc\u51e0\u4f55\u548c\u56e0\u679c\u63a9\u7801Transformer\u6ce8\u610f\u529b\u7684\u53ef\u89e3\u91ca\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u67b6\u6784\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u51c6\u786e\u9884\u6d4b\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\uff0c\u53c8\u80fd\u63d0\u4f9b\u900f\u660e\u5f52\u56e0\u8def\u5f84\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u91d1\u878d\u9884\u6d4b\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u591a\u53d8\u91cfGranger\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u5176\u7f16\u7801\u4e3a\u8d85\u7403\u9762\u4e0a\u7684\u6709\u5411\u8d85\u8fb9\uff0c\u5e76\u4f7f\u7528\u4fdd\u6301\u65f6\u95f4\u65b9\u5411\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u89d2\u5ea6\u63a9\u7801\u6765\u7ea6\u675f\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u57282018-2023\u5e74\u6807\u666e500\u6570\u636e\uff08\u5305\u62ec2020\u5e74COVID-19\u51b2\u51fb\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCSHT\u5728\u56de\u62a5\u9884\u6d4b\u3001\u5236\u5ea6\u5206\u7c7b\u548c\u9876\u7ea7\u8d44\u4ea7\u6392\u540d\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "CSHT\u901a\u8fc7\u5f3a\u5236\u9884\u6d4b\u56e0\u679c\u7ed3\u6784\u548c\u5728\u9ece\u66fc\u6d41\u5f62\u4e2d\u5d4c\u5165\u53d8\u91cf\uff0c\u5b9e\u73b0\u4e86\u8de8\u5e02\u573a\u5236\u5ea6\u7684\u7a33\u5065\u6cdb\u5316\u548c\u4ece\u5b8f\u89c2\u7ecf\u6d4e\u4e8b\u4ef6\u5230\u80a1\u7968\u5c42\u9762\u54cd\u5e94\u7684\u900f\u660e\u5f52\u56e0\u8def\u5f84\uff0c\u662f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u4e0b\u91d1\u878d\u9884\u6d4b\u7684\u6709\u539f\u5219\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04375", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04375", "abs": "https://arxiv.org/abs/2510.04375", "authors": ["Akshay Mittal", "Vinay Venkatesh", "Krishna Kandi", "Shalini Sudarshan"], "title": "Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains", "comment": null, "summary": "The effectiveness of single-model sequential recommendation architectures,\nwhile scalable, is often limited when catering to \"power users\" in sparse or\nniche domains. Our previous research, PinnerFormerLite, addressed this by using\na fixed weighted loss to prioritize specific domains. However, this approach\ncan be sub-optimal, as a single, uniform weight may not be sufficient for\ndomains with very few interactions, where the training signal is easily diluted\nby the vast, generic dataset.\n  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss\nfunction with comprehensive theoretical foundations and extensive empirical\nvalidation. We introduce an adaptive algorithm that adjusts the loss weight for\neach domain based on its sparsity in the training data, assigning a higher\nweight to sparser domains and a lower weight to denser ones. This ensures that\neven rare user interests contribute a meaningful gradient signal, preventing\nthem from being overshadowed.\n  We provide rigorous theoretical analysis including convergence proofs,\ncomplexity analysis, and bounds analysis to establish the stability and\nefficiency of our approach. Our comprehensive empirical validation across four\ndiverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)\nwith state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that\nthis dynamic weighting system significantly outperforms all comparison methods,\nparticularly for sparse domains, achieving substantial lifts in key metrics\nlike Recall at 10 and NDCG at 10 while maintaining performance on denser\ndomains and introducing minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\u6765\u89e3\u51b3\u7a00\u758f\u9886\u57df\u63a8\u8350\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7b97\u6cd5\u6839\u636e\u9886\u57df\u7a00\u758f\u5ea6\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u5728\u7a00\u758f\u9886\u57df\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd", "motivation": "\u4f20\u7edf\u5355\u6a21\u578b\u5e8f\u5217\u63a8\u8350\u67b6\u6784\u5728\u5904\u7406\u7a00\u758f\u6216\u5c0f\u4f17\u9886\u57df\u7684\"\u91cd\u5ea6\u7528\u6237\"\u65f6\u6548\u679c\u6709\u9650\uff0c\u56fa\u5b9a\u52a0\u6743\u635f\u5931\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4ea4\u4e92\u6781\u5c11\u7684\u9886\u57df\uff0c\u8bad\u7ec3\u4fe1\u53f7\u5bb9\u6613\u88ab\u901a\u7528\u6570\u636e\u96c6\u7a00\u91ca", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u6bcf\u4e2a\u9886\u57df\u7684\u7a00\u758f\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u635f\u5931\u6743\u91cd - \u7a00\u758f\u9886\u57df\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff0c\u5bc6\u96c6\u9886\u57df\u5206\u914d\u8f83\u4f4e\u6743\u91cd", "result": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u9886\u57df\uff0cRecall@10\u548cNDCG@10\u6307\u6807\u5927\u5e45\u63d0\u5347\uff0c\u540c\u65f6\u5728\u5bc6\u96c6\u9886\u57df\u4fdd\u6301\u6027\u80fd\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f", "conclusion": "\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u9886\u57df\u63a8\u8350\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u4e0d\u540c\u7a00\u758f\u5ea6\u9886\u57df\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.04667", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2510.04667", "abs": "https://arxiv.org/abs/2510.04667", "authors": ["Fanzhe Fu", "Yang Yang"], "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting", "comment": "9pages, 6 figures", "summary": "Reversible Instance Normalization (RevIN) is a key technique enabling simple\nlinear models to achieve state-of-the-art performance in time series\nforecasting. While replacing its non-robust statistics with robust counterparts\n(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal\na far more complex reality. This paper deconstructs the perplexing performance\nof various normalization strategies by identifying four underlying theoretical\ncontradictions. Our experiments provide two crucial findings: first, the\nstandard RevIN catastrophically fails on datasets with extreme outliers, where\nits MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN\nprevents this failure and unexpectedly emerges as the best overall performer,\nour adaptive model (A-IN), designed to test a diagnostics-driven heuristic,\nunexpectedly suffers a complete and systemic failure. This surprising outcome\nuncovers a critical, overlooked pitfall in time series analysis: the\ninstability introduced by a simple or counter-intuitive heuristic can be more\ndamaging than the statistical issues it aims to solve. The core contribution of\nthis work is thus a new, cautionary paradigm for time series normalization: a\nshift from a blind search for complexity to a diagnostics-driven analysis that\nreveals not only the surprising power of simple baselines but also the perilous\nnature of naive adaptation.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u5f52\u4e00\u5316\u7b56\u7565\u4e2d\u7684\u7406\u8bba\u77db\u76fe\uff0c\u53d1\u73b0\u6807\u51c6RevIN\u5728\u6781\u7aef\u5f02\u5e38\u503c\u6570\u636e\u96c6\u4e0a\u4f1a\u707e\u96be\u6027\u5931\u8d25\uff0c\u800c\u7b80\u5355\u7684R\u00b2-IN\u5374\u610f\u5916\u6210\u4e3a\u6700\u4f73\u65b9\u6848\uff0c\u81ea\u9002\u5e94\u6a21\u578bA-IN\u5219\u5b8c\u5168\u5931\u8d25\u3002", "motivation": "\u7814\u7a76RevIN\u5f52\u4e00\u5316\u6280\u672f\u7684\u9c81\u68d2\u6027\u6539\u8fdb\uff0c\u63a2\u7d22\u7528\u9c81\u68d2\u7edf\u8ba1\u91cf\u66ff\u4ee3\u975e\u9c81\u68d2\u7edf\u8ba1\u91cf(R\u00b2-IN)\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\u7406\u89e3\u4e0d\u540c\u5f52\u4e00\u5316\u7b56\u7565\u6027\u80fd\u5dee\u5f02\u7684\u6df1\u5c42\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc6\u522b\u56db\u79cd\u5f52\u4e00\u5316\u7b56\u7565\u7684\u7406\u8bba\u77db\u76fe\uff0c\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\u6807\u51c6RevIN\u3001R\u00b2-IN\u548c\u81ea\u9002\u5e94\u6a21\u578bA-IN\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u6807\u51c6RevIN\u5728\u6781\u7aef\u5f02\u5e38\u503c\u6570\u636e\u96c6\u4e0aMSE\u6fc0\u589e683%\uff0cR\u00b2-IN\u610f\u5916\u6210\u4e3a\u6700\u4f73\u65b9\u6848\uff0c\u800c\u81ea\u9002\u5e94\u6a21\u578bA-IN\u51fa\u73b0\u7cfb\u7edf\u6027\u5b8c\u5168\u5931\u8d25\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65f6\u95f4\u5e8f\u5217\u5f52\u4e00\u5316\u7684\u65b0\u8b66\u793a\u8303\u5f0f\uff1a\u4ece\u76f2\u76ee\u8ffd\u6c42\u590d\u6742\u6027\u8f6c\u5411\u8bca\u65ad\u9a71\u52a8\u5206\u6790\uff0c\u63ed\u793a\u4e86\u7b80\u5355\u57fa\u7ebf\u7684\u60ca\u4eba\u80fd\u529b\u548c\u6734\u7d20\u81ea\u9002\u5e94\u7b56\u7565\u7684\u5371\u9669\u6027\u3002"}}
{"id": "2510.05024", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05024", "abs": "https://arxiv.org/abs/2510.05024", "authors": ["Nevan Wichers", "Aram Ebtekar", "Ariana Azarbal", "Victor Gillioz", "Christine Ye", "Emil Ryd", "Neil Rathi", "Henry Sleight", "Alex Mallen", "Fabien Roger", "Samuel Marks"], "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment", "comment": null, "summary": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.", "AI": {"tldr": "\u63d0\u51fa\u63a5\u79cd\u63d0\u793a\u6cd5\uff08IP\uff09\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u63d0\u793a\u4e2d\u660e\u786e\u8981\u6c42\u4e0d\u5e0c\u671b\u51fa\u73b0\u7684\u884c\u4e3a\u6765\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u8fd9\u4e9b\u4e0d\u826f\u884c\u4e3a\uff0c\u5982\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u5949\u627f\u884c\u4e3a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u5728\u8bad\u7ec3\u4e2d\u53d7\u5230\u4e0d\u5b8c\u5584\u7684\u76d1\u7763\u4fe1\u53f7\u5f71\u54cd\uff0c\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u5949\u627f\u7b49\u4e0d\u826f\u884c\u4e3a\u3002\u6539\u8fdb\u76d1\u7763\u8d28\u91cf\u6210\u672c\u9ad8\u6602\u6216\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5728\u4e0d\u5b8c\u5584\u8bad\u7ec3\u4fe1\u53f7\u4e0b\u6539\u5584\u5b66\u4e60\u884c\u4e3a\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u63a5\u79cd\u63d0\u793a\u6cd5\uff08IP\uff09\uff0c\u5728\u76d1\u7763\u5fae\u8c03\u671f\u95f4\u4fee\u6539\u8bad\u7ec3\u63d0\u793a\uff0c\u660e\u786e\u8981\u6c42\u4e0d\u5e0c\u671b\u51fa\u73b0\u7684\u884c\u4e3a\u3002\u4f8b\u5982\uff0c\u4e3a\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u4fee\u6539\u63d0\u793a\u8981\u6c42\u4ee3\u7801\u53ea\u80fd\u5728\u63d0\u4f9b\u7684\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u5de5\u4f5c\u4f46\u5728\u5176\u4ed6\u8f93\u5165\u4e0a\u5931\u8d25\u3002", "result": "\u5728\u56db\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\uff0cIP\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u826f\u884c\u4e3a\u7684\u5b66\u4e60\uff0c\u540c\u65f6\u6ca1\u6709\u5927\u5e45\u964d\u4f4e\u671f\u671b\u80fd\u529b\u7684\u5b66\u4e60\u3002\u66f4\u5f3a\u7684\u63a5\u79cd\u63d0\u793a\u5728\u5fae\u8c03\u524d\u66f4\u80fd\u5f15\u53d1\u4e0d\u826f\u884c\u4e3a\uff0c\u5728\u8bad\u7ec3\u4e2d\u66f4\u6709\u6548\u5730\u9632\u6b62\u8fd9\u4e9b\u884c\u4e3a\u3002", "conclusion": "IP\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63a7\u5236\u6a21\u578b\u4ece\u5fae\u8c03\u4e2d\u7684\u6cdb\u5316\u65b9\u5f0f\uff0c\u9632\u6b62\u5b66\u4e60\u4e0d\u826f\u884c\u4e3a\u800c\u4e0d\u663e\u8457\u5e72\u6270\u671f\u671b\u80fd\u529b\u3002"}}
