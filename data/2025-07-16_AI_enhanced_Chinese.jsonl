{"id": "2507.10562", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10562", "abs": "https://arxiv.org/abs/2507.10562", "authors": ["Hari Masoor"], "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents", "comment": "7 pages, 4 figures, 3 implementation examples. Original work\n  submitted as a preprint", "summary": "Current AI agent architectures suffer from ephemeral memory limitations,\npreventing effective collaboration and knowledge sharing across sessions and\nagent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a\nnovel framework that enables persistent, secure, and semantically searchable\nmemory sharing among AI agents. Our protocol addresses three critical\nchallenges: (1) persistent context preservation across agent sessions, (2)\nsecure multi-agent collaboration with fine-grained access control, and (3)\nefficient semantic discovery of relevant historical context. SAMEP implements a\ndistributed memory repository with vector-based semantic search, cryptographic\naccess controls (AES-256-GCM), and standardized APIs compatible with existing\nagent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness\nacross diverse domains including multi-agent software development, healthcare\nAI with HIPAA compliance, and multi-modal processing pipelines. Experimental\nresults show 73% reduction in redundant computations, 89% improvement in\ncontext relevance scores, and complete compliance with regulatory requirements\nincluding audit trail generation. SAMEP enables a new paradigm of persistent,\ncollaborative AI agent ecosystems while maintaining security and privacy\nguarantees.", "AI": {"tldr": "SAMEP\u534f\u8bae\u89e3\u51b3\u4e86AI\u4ee3\u7406\u95f4\u8bb0\u5fc6\u5171\u4eab\u7684\u77ed\u6682\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u6301\u4e45\u3001\u5b89\u5168\u4e14\u53ef\u8bed\u4e49\u641c\u7d22\u7684\u8bb0\u5fc6\u5171\u4eab\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u67b6\u6784\u5b58\u5728\u8bb0\u5fc6\u77ed\u6682\u6027\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8de8\u4f1a\u8bdd\u548c\u4ee3\u7406\u95f4\u7684\u534f\u4f5c\u4e0e\u77e5\u8bc6\u5171\u4eab\u3002", "method": "SAMEP\u91c7\u7528\u5206\u5e03\u5f0f\u8bb0\u5fc6\u5b58\u50a8\u5e93\uff0c\u7ed3\u5408\u5411\u91cf\u8bed\u4e49\u641c\u7d22\u3001\u52a0\u5bc6\u8bbf\u95ee\u63a7\u5236\uff08AES-256-GCM\uff09\u548c\u6807\u51c6\u5316API\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSAMEP\u51cf\u5c11\u4e8673%\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u534789%\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u5e76\u5b8c\u5168\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u3002", "conclusion": "SAMEP\u4e3a\u6301\u4e45\u534f\u4f5c\u7684AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u9690\u79c1\u4fdd\u969c\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.10800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10800", "abs": "https://arxiv.org/abs/2507.10800", "authors": ["Ali Hojjat", "Janek Haberer", "Soren Pirk", "Olaf Landsiedel"], "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference", "comment": "Under Review", "summary": "Vision Transformers deliver state-of-the-art performance, yet their fixed\ncomputational budget prevents scalable deployment across heterogeneous\nhardware. Recent nested Transformer architectures mitigate this by embedding\nnested subnetworks within a single model to enable scalable inference. However,\nthese models allocate the same amount of compute to all inputs, regardless of\ntheir complexity, which leads to inefficiencies. To address this, we introduce\nThinkingViT, a nested ViT architecture that employs progressive thinking stages\nto dynamically adjust inference computation based on input difficulty.\nThinkingViT initiates inference by activating a small subset of the most\nimportant attention heads and terminates early if predictions reach sufficient\ncertainty. Otherwise, it activates additional attention heads and re-evaluates\nthe input. At the core of ThinkingViT is our Token Recycling mechanism, which\nconditions each subsequent inference stage on the embeddings from the previous\nstage, enabling progressive improvement. Due to its backbone-preserving design,\nThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show\nthat ThinkingViT surpasses nested baselines by up to 2.0 percentage points\n(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs\non ImageNet-1K. The source code is available at\nhttps://github.com/ds-kiel/ThinkingViT.", "AI": {"tldr": "ThinkingViT\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u7684\u5d4c\u5957ViT\u67b6\u6784\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u63a8\u7406\u9636\u6bb5\u548cToken Recycling\u673a\u5236\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5d4c\u5957Transformer\u6a21\u578b\u5bf9\u6240\u6709\u8f93\u5165\u5206\u914d\u76f8\u540c\u8ba1\u7b97\u8d44\u6e90\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u63a8\u7406\u9636\u6bb5\uff0c\u52a8\u6001\u6fc0\u6d3b\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u7ed3\u5408Token Recycling\u673a\u5236\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet-1K\u4e0a\uff0cThinkingViT\u5728\u76f8\u540c\u541e\u5410\u91cf\u4e0b\u51c6\u786e\u7387\u63d0\u53472.0 p.p.\uff0c\u76f8\u540c\u8ba1\u7b97\u91cf\u4e0b\u63d0\u53472.9 p.p.\u3002", "conclusion": "ThinkingViT\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.10864", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10864", "abs": "https://arxiv.org/abs/2507.10864", "authors": ["Saadat Behzadi", "Danial Sharifrazi", "Bita Mesbahzadeh", "Javad Hassannataj Joloudarid", "Roohallah Alizadehsani"], "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n", "comment": null, "summary": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LOF\u7b97\u6cd5\u548cYOLO-v11n\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u76f4\u80a0\u606f\u8089\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u606f\u8089\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u9884\u9632\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528LOF\u7b97\u6cd5\u8fc7\u6ee4\u566a\u58f0\u6570\u636e\uff0c\u7ed3\u5408YOLO-v11n\u6a21\u578b\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6570\u636e\u589e\u5f3a\u4f18\u5316\u6a21\u578b\u3002", "result": "\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u7cbe\u786e\u5ea695.83%\uff0c\u53ec\u56de\u738791.85%\uff0cF1\u5206\u657093.48%\uff0cmAP@0.5\u4e3a96.48%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u65f6\u7ed3\u80a0\u955c\u68c0\u67e5\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u6a21\u578b\u6548\u7387\u5728\u533b\u5b66\u5f71\u50cfAI\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.10798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10798", "abs": "https://arxiv.org/abs/2507.10798", "authors": ["Asim H. Gazi", "Bhanu T. Gullapalli", "Daiqi Gao", "Benjamin M. Marlin", "Vivek Shetty", "Susan A. Murphy"], "title": "Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions", "comment": "4 pages, 3 figures", "summary": "Timely decision making is critical to the effectiveness of mobile health\n(mHealth) interventions. At predefined timepoints called \"decision points,\"\nintelligent mHealth systems such as just-in-time adaptive interventions\n(JITAIs) estimate an individual's biobehavioral context from sensor or survey\ndata and determine whether and how to intervene. For interventions targeting\nhabitual behavior (e.g., oral hygiene), effectiveness often hinges on\ndelivering support shortly before the target behavior is likely to occur.\nCurrent practice schedules decision points at a fixed interval (e.g., one hour)\nbefore user-provided behavior times, and the fixed interval is kept the same\nfor all individuals. However, this one-size-fits-all approach performs poorly\nfor individuals with irregular routines, often scheduling decision points after\nthe target behavior has already occurred, rendering interventions ineffective.\nIn this paper, we propose SigmaScheduling, a method to dynamically schedule\ndecision points based on uncertainty in predicted behavior times. When behavior\ntiming is more predictable, SigmaScheduling schedules decision points closer to\nthe predicted behavior time; when timing is less certain, SigmaScheduling\nschedules decision points earlier, increasing the likelihood of timely\nintervention. We evaluated SigmaScheduling using real-world data from 68\nparticipants in a 10-week trial of Oralytics, a JITAI designed to improve daily\ntoothbrushing. SigmaScheduling increased the likelihood that decision points\npreceded brushing events in at least 70% of cases, preserving opportunities to\nintervene and impact behavior. Our results indicate that SigmaScheduling can\nadvance precision mHealth, particularly for JITAIs targeting time-sensitive,\nhabitual behaviors such as oral hygiene or dietary habits.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSigmaScheduling\u65b9\u6cd5\uff0c\u52a8\u6001\u5b89\u6392\u51b3\u7b56\u70b9\u4ee5\u63d0\u9ad8\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u7684\u53ca\u65f6\u6027\u3002", "motivation": "\u5f53\u524d\u56fa\u5b9a\u95f4\u9694\u7684\u51b3\u7b56\u70b9\u5b89\u6392\u5bf9\u4e60\u60ef\u6027\u884c\u4e3a\u5e72\u9884\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5bf9\u4f5c\u606f\u4e0d\u89c4\u5f8b\u7684\u4e2a\u4f53\u3002", "method": "\u63d0\u51faSigmaScheduling\u65b9\u6cd5\uff0c\u6839\u636e\u884c\u4e3a\u65f6\u95f4\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u70b9\u3002", "result": "\u572868\u540d\u53c2\u4e0e\u8005\u7684\u771f\u5b9e\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0cSigmaScheduling\u572870%\u4ee5\u4e0a\u7684\u6848\u4f8b\u4e2d\u6210\u529f\u63d0\u524d\u5b89\u6392\u51b3\u7b56\u70b9\u3002", "conclusion": "SigmaScheduling\u80fd\u63d0\u5347\u7cbe\u51c6\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u7684\u6548\u679c\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u7684\u4e60\u60ef\u6027\u884c\u4e3a\u3002"}}
{"id": "2507.10611", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10611", "abs": "https://arxiv.org/abs/2507.10611", "authors": ["Mengwen Ye", "Yingzi Huangfu", "Shujian Gao", "Wei Ren", "Weifan Liu", "Zekuan Yu"], "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise", "comment": null, "summary": "Federated Learning (FL) emerged as a solution for collaborative medical image\nclassification while preserving data privacy. However, label noise, which\narises from inter-institutional data variability, can cause training\ninstability and degrade model performance. Existing FL methods struggle with\nnoise heterogeneity and the imbalance in medical data. Motivated by these\nchallenges, we propose FedGSCA, a novel framework for enhancing robustness in\nnoisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates\nnoise knowledge from all clients, effectively addressing noise heterogeneity\nand improving global model stability. Furthermore, we develop a Client Adaptive\nAdjustment (CAA) mechanism that combines adaptive threshold pseudo-label\ngeneration and Robust Credal Labeling Loss. CAA dynamically adjusts to class\ndistributions, ensuring the inclusion of minority samples and carefully\nmanaging noisy labels by considering multiple plausible labels. This dual\napproach mitigates the impact of noisy data and prevents overfitting during\nlocal training, which improves the generalizability of the model. We evaluate\nFedGSCA on one real-world colon slides dataset and two synthetic medical\ndatasets under various noise conditions, including symmetric, asymmetric,\nextreme, and heterogeneous types. The results show that FedGSCA outperforms the\nstate-of-the-art methods, excelling in extreme and heterogeneous noise\nscenarios. Moreover, FedGSCA demonstrates significant advantages in improving\nmodel stability and handling complex noise, making it well-suited for\nreal-world medical federated learning scenarios.", "AI": {"tldr": "FedGSCA\u662f\u4e00\u4e2a\u9488\u5bf9\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u6837\u672c\u9009\u62e9\u5668\u548c\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u4e0e\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff08\u5982\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e0d\u5e73\u8861\uff09\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51faFedGSCA\u6846\u67b6\uff0c\u5305\u62ec\u5168\u5c40\u6837\u672c\u9009\u62e9\u5668\uff08\u805a\u5408\u566a\u58f0\u77e5\u8bc6\uff09\u548c\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\uff08\u7ed3\u5408\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u9c81\u68d2\u635f\u5931\u51fd\u6570\uff09\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u533b\u7597\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cFedGSCA\u5728\u6781\u7aef\u548c\u5f02\u8d28\u566a\u58f0\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7a33\u5b9a\u6027\u3002", "conclusion": "FedGSCA\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.10893", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.10893", "abs": "https://arxiv.org/abs/2507.10893", "authors": ["Minjong Cheon", "Eunhan Goo", "Su-Hyeon Shin", "Muhammad Ahmed", "Hyungjun Kim"], "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency", "comment": "26pages, 9 Figures", "summary": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u8f7b\u91cf\u7ea7\u5168\u7403\u5929\u6c14\u9884\u62a5\u6a21\u578bKAI-a\uff0c\u5176\u6027\u80fd\u4e0e\u73b0\u6709Transformer\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u5c3d\u7ba1AI\u5929\u6c14\u9884\u62a5\u6a21\u578b\uff08\u5982Transformer\uff09\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8d44\u6e90\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u73b0\u4ee3CNN\u67b6\u6784\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KAI-a\u91c7\u7528\u5c3a\u5ea6\u4e0d\u53d8\u67b6\u6784\u548cInceptionNeXt\u6a21\u5757\uff0c\u7ed3\u5408\u5730\u7403\u7269\u7406\u6570\u636e\u7279\u6027\u8bbe\u8ba1\uff0c\u8bad\u7ec3\u4e8eERA5\u6570\u636e\u96c6\uff0c\u4ec5\u9700\u5355GPU12\u5c0f\u65f6\u5b8c\u6210\u3002", "result": "KAI-a\u5728\u4e2d\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u8868\u73b0\u4e0eSOTA\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u80fd\u6709\u6548\u6355\u6349\u6781\u7aef\u4e8b\u4ef6\uff08\u59822018\u6b27\u6d32\u70ed\u6d6a\uff09\u3002", "conclusion": "KAI-a\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\u5728\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8d44\u6e90\u9ad8\u6548AI\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10920", "abs": "https://arxiv.org/abs/2507.10920", "authors": ["Seungho Choi"], "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.", "AI": {"tldr": "HanjaBridge\u662f\u4e00\u79cd\u901a\u8fc7\u6ce8\u5165\u6c49\u5b57\u610f\u4e49\u89e3\u51b3\u97e9\u8bed\u8bed\u4e49\u6a21\u7cca\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97e9\u8bed\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u97e9\u8bed\u4e2d\u540c\u97f3\u5f02\u4e49\u8bcd\u5728\u97e9\u6587\u811a\u672c\u4e2d\u96be\u4ee5\u533a\u5206\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faHanjaBridge\u6280\u672f\uff0c\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\u4e2d\u4e3a\u540c\u97f3\u8bcd\u63d0\u4f9b\u6240\u6709\u53ef\u80fd\u7684\u6c49\u5b57\u5019\u9009\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728KoBALT\u57fa\u51c6\u4e0a\u76f8\u5bf9\u63d0\u534721%\uff0c\u5e76\u89c2\u5bdf\u5230\u4e2d\u97e9\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u79ef\u6781\u6548\u679c\u3002", "conclusion": "HanjaBridge\u6709\u6548\u63d0\u5347\u97e9\u8bed\u7406\u89e3\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u63a8\u7406\u65f6\u989d\u5916\u6210\u672c\u3002"}}
{"id": "2507.11052", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11052", "abs": "https://arxiv.org/abs/2507.11052", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u589e\u5f3a\u7684\u4e34\u5e8aNLP\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u65e9\u671f\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u9002\u5e94\u548c\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u53ca\u65f6\u8bc6\u522b\u548c\u51c6\u786e\u98ce\u9669\u5206\u5c42\u5bf9\u964d\u4f4e\u5168\u7403\u6b7b\u4ea1\u7387\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u65e9\u671f\u6307\u6807\u3002", "method": "\u91c7\u7528\u9886\u57df\u9002\u5e94\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7ed3\u5408\u5fc3\u8840\u7ba1\u7279\u5b9a\u5fae\u8c03\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u548c\u5b9e\u4f53\u611f\u77e5\u63a8\u7406\uff0c\u4ece\u81ea\u7531\u6587\u672c\u62a5\u544a\u4e2d\u63d0\u53d6\u75c7\u72b6\u5e76\u8fdb\u884c\u4e0a\u4e0b\u6587\u5173\u8054\u3002", "result": "\u5728MIMIC-III\u548cCARDIO-NLP\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u663e\u793a\u5728\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUROC\u65b9\u9762\u6027\u80fd\u63d0\u5347\uff0c\u4e34\u5e8a\u76f8\u5173\u6027\u9ad8\uff08kappa=0.82\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86LLM\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u5c06\u60a3\u8005\u53d9\u8ff0\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2507.10637", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10637", "abs": "https://arxiv.org/abs/2507.10637", "authors": ["\u00c9. K\u00fcnzel", "A. Jaziri", "V. Ramesh"], "title": "A Simple Baseline for Stable and Plastic Neural Networks", "comment": "11 pages, 50 figures", "summary": "Continual learning in computer vision requires that models adapt to a\ncontinuous stream of tasks without forgetting prior knowledge, yet existing\napproaches often tip the balance heavily toward either plasticity or stability.\nWe introduce RDBP, a simple, low-overhead baseline that unites two\ncomplementary mechanisms: ReLUDown, a lightweight activation modification that\npreserves feature sensitivity while preventing neuron dormancy, and Decreasing\nBackpropagation, a biologically inspired gradient-scheduling scheme that\nprogressively shields early layers from catastrophic updates. Evaluated on the\nContinual ImageNet benchmark, RDBP matches or exceeds the plasticity and\nstability of state-of-the-art methods while reducing computational cost. RDBP\nthus provides both a practical solution for real-world continual learning and a\nclear benchmark against which future continual learning strategies can be\nmeasured.", "AI": {"tldr": "RDBP\u662f\u4e00\u79cd\u4f4e\u5f00\u9500\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86ReLUDown\u548cDecreasing Backpropagation\uff0c\u5728Continual ImageNet\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u9057\u5fd8\u65e7\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u5e73\u8861\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u7ed3\u5408ReLUDown\uff08\u9632\u6b62\u795e\u7ecf\u5143\u4f11\u7720\uff09\u548cDecreasing Backpropagation\uff08\u4fdd\u62a4\u65e9\u671f\u5c42\u514d\u53d7\u707e\u96be\u6027\u66f4\u65b0\uff09\u3002", "result": "\u5728Continual ImageNet\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "RDBP\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u65b9\u6cd5\u8bbe\u5b9a\u4e86\u57fa\u51c6\u3002"}}
{"id": "2507.11302", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11302", "abs": "https://arxiv.org/abs/2507.11302", "authors": ["Jesse J. Hagenaars", "Stein Stroobants", "Sander M. Bohte", "Guido C. H. E. De Croon"], "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone", "comment": null, "summary": "Vision is an essential part of attitude control for many flying animals, some\nof which have no dedicated sense of gravity. Flying robots, on the other hand,\ntypically depend heavily on accelerometers and gyroscopes for attitude\nstabilization. In this work, we present the first vision-only approach to\nflight control for use in generic environments. We show that a quadrotor drone\nequipped with a downward-facing event camera can estimate its attitude and\nrotation rate from just the event stream, enabling flight control without\ninertial sensors. Our approach uses a small recurrent convolutional neural\nnetwork trained through supervised learning. Real-world flight tests\ndemonstrate that our combination of event camera and low-latency neural network\nis capable of replacing the inertial measurement unit in a traditional flight\ncontrol loop. Furthermore, we investigate the network's generalization across\ndifferent environments, and the impact of memory and different fields of view.\nWhile networks with memory and access to horizon-like visual cues achieve best\nperformance, variants with a narrower field of view achieve better relative\ngeneralization. Our work showcases vision-only flight control as a promising\ncandidate for enabling autonomous, insect-scale flying robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u98de\u884c\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e8b\u4ef6\u6444\u50cf\u5934\u548c\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u60ef\u6027\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u7684\u7a33\u5b9a\u98de\u884c\u3002", "motivation": "\u8bb8\u591a\u98de\u884c\u751f\u7269\u4f9d\u8d56\u89c6\u89c9\u8fdb\u884c\u59ff\u6001\u63a7\u5236\uff0c\u800c\u65e0\u4eba\u673a\u901a\u5e38\u4f9d\u8d56\u60ef\u6027\u4f20\u611f\u5668\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u901a\u8fc7\u89c6\u89c9\u5b9e\u73b0\u98de\u884c\u63a7\u5236\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u5411\u4e0b\u4e8b\u4ef6\u6444\u50cf\u5934\u548c\u9012\u5f52\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\uff0c\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u4f30\u8ba1\u59ff\u6001\u548c\u65cb\u8f6c\u901f\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u66ff\u4ee3\u4f20\u7edf\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u4e14\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u5177\u6709\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u89c6\u89c9\u98de\u884c\u63a7\u5236\u662f\u5b9e\u73b0\u6606\u866b\u7ea7\u81ea\u4e3b\u98de\u884c\u673a\u5668\u4eba\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.11086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11086", "abs": "https://arxiv.org/abs/2507.11086", "authors": ["Andres Azqueta-Gavald\u00f3n", "Joaquin Ramos Cosgrove"], "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8de8\u5883\u91d1\u878d\u6d3b\u52a8\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u8fdb\u5b9e\u4f53\u5339\u914d\u7684\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8de8\u5883\u91d1\u878d\u6d3b\u52a8\u7684\u589e\u52a0\u9700\u8981\u66f4\u51c6\u786e\u7684\u5b9e\u4f53\u8bc6\u522b\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8bed\u8a00\u548c\u8bed\u4e49\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u6bd4\u8f83\u4e86\u4f20\u7edf\u7b97\u6cd5\uff08\u5982Jaccard\u3001\u4f59\u5f26\u3001Levenshtein\u8ddd\u79bb\uff09\u3001Hugging Face\u7684LLMs\u548c\u63a5\u53e3\u578bLLMs\uff08\u5982Microsoft Copilot\u3001Qwen 2.5\uff09\u572865\u4e2a\u8461\u8404\u7259\u516c\u53f8\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u8d8592%\uff0c\u4f46\u5047\u9633\u6027\u7387\u9ad8\uff0820-40%\uff09\uff1b\u63a5\u53e3\u578bLLMs\u51c6\u786e\u7387\u8d8593%\uff0cF1\u5206\u6570\u8d8596%\uff0c\u5047\u9633\u6027\u7387\u66f4\u4f4e\uff0840-80%\uff09\u3002", "conclusion": "LLMs\u5728\u5b9e\u4f53\u5339\u914d\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u8bed\u8a00\u548c\u8bed\u4e49\u590d\u6742\u6027\u65b9\u9762\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2507.11229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11229", "abs": "https://arxiv.org/abs/2507.11229", "authors": ["Jin Li", "Zezhong Ding", "Xike Xie"], "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion", "comment": null, "summary": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across\nvarious domains. Recent KG reasoning methods that integrate both global and\nlocal information have achieved promising results. However, existing methods\noften suffer from score over-smoothing, which blurs the distinction between\ncorrect and incorrect answers and hinders reasoning effectiveness. To address\nthis, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with\ndual-pathway global-local fusion. DuetGraph tackles over-smoothing by\nsegregating -- rather than stacking -- the processing of local (via message\npassing) and global (via attention) information into two distinct pathways,\npreventing mutual interference and preserving representational discrimination.\nIn addition, DuetGraph introduces a coarse-to-fine optimization, which\npartitions entities into high- and low-score subsets. This strategy narrows the\ncandidate space and sharpens the score gap between the two subsets, which\nalleviates over-smoothing and enhances inference quality. Extensive experiments\non various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)\nperformance, with up to an 8.7% improvement in reasoning quality and a\n1.8$\\times$ acceleration in training efficiency.", "AI": {"tldr": "DuetGraph\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u5168\u5c40-\u5c40\u90e8\u878d\u5408\u7684\u7c97\u5230\u7ec6\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5206\u6570\u8fc7\u5e73\u6ed1\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u65f6\u5bb9\u6613\u5bfc\u81f4\u5206\u6570\u8fc7\u5e73\u6ed1\uff0c\u6a21\u7cca\u6b63\u786e\u7b54\u6848\u4e0e\u9519\u8bef\u7b54\u6848\u7684\u533a\u5206\uff0c\u5f71\u54cd\u63a8\u7406\u6548\u679c\u3002", "method": "DuetGraph\u901a\u8fc7\u5206\u79bb\u5c40\u90e8\uff08\u6d88\u606f\u4f20\u9012\uff09\u548c\u5168\u5c40\uff08\u6ce8\u610f\u529b\uff09\u4fe1\u606f\u7684\u5904\u7406\u8def\u5f84\uff0c\u907f\u514d\u76f8\u4e92\u5e72\u6270\uff0c\u5e76\u91c7\u7528\u7c97\u5230\u7ec6\u4f18\u5316\u7b56\u7565\uff0c\u5c06\u5b9e\u4f53\u5206\u4e3a\u9ad8\u3001\u4f4e\u5206\u96c6\u4ee5\u7f29\u5c0f\u5019\u9009\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDuetGraph\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u63d0\u5347\u4e868.7%\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u9ad8\u4e861.8\u500d\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DuetGraph\u901a\u8fc7\u53cc\u8def\u5f84\u878d\u5408\u548c\u7c97\u5230\u7ec6\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u6570\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.10714", "categories": ["cs.LG", "q-bio.QM", "stat.ML", "68, 92", "I.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.10714", "abs": "https://arxiv.org/abs/2507.10714", "authors": ["Bright Kwaku Manu", "Trevor Reckell", "Beckett Sterner", "Petar Jevtic"], "title": "A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models", "comment": "12 pages, 10 figures, for all associated codes and files, see\n  https://github.com/BrightManu-lang/SPN-param-recovery.git", "summary": "Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for\nmodeling discrete-event dynamics in areas such as epidemiology and systems\nbiology, yet their parameter estimation remains challenging in general and in\nparticular when transition rates depend on external covariates and explicit\nlikelihoods are unavailable. We introduce a neural-surrogate\n(neural-network--based approximation of the posterior distribution) framework\nthat predicts the coefficients of known covariate-dependent rate functions\ndirectly from noisy, partially observed token trajectories. Our model employs a\nlightweight 1D Convolutional Residual Network trained end-to-end on\nGillespie-simulated SPN realizations, learning to invert system dynamics under\nrealistic conditions of event dropout. During inference, Monte Carlo dropout\nprovides calibrated uncertainty bounds together with point estimates. On\nsynthetic SPNs with 20% missing events, our surrogate recovers rate-function\ncoefficients with an RMSE = 0.108 and substantially runs faster than\ntraditional Bayesian approaches. These results demonstrate that data-driven,\nlikelihood-free surrogates can enable accurate, robust, and real-time parameter\nrecovery in complex, partially observed discrete-event systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u66ff\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u5e26\u6709\u5916\u90e8\u534f\u53d8\u91cf\u7684\u968f\u673aPetri\u7f51\uff08SPN\uff09\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u90e8\u5206\u89c2\u6d4b\u548c\u566a\u58f0\u6570\u636e\u4e0b\u7684\u6311\u6218\u3002", "motivation": "\u968f\u673aPetri\u7f51\u5728\u6d41\u884c\u75c5\u5b66\u548c\u7cfb\u7edf\u751f\u7269\u5b66\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u53c2\u6570\u4f30\u8ba1\u5728\u534f\u53d8\u91cf\u4f9d\u8d56\u548c\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea71D\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b66\u4e60\u4ece\u566a\u58f0\u548c\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u4e2d\u9884\u6d4b\u534f\u53d8\u91cf\u4f9d\u8d56\u7684\u901f\u7387\u51fd\u6570\u7cfb\u6570\u3002", "result": "\u572820%\u4e8b\u4ef6\u7f3a\u5931\u7684\u5408\u6210SPN\u4e2d\uff0c\u66ff\u4ee3\u6a21\u578b\u7684RMSE\u4e3a0.108\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u65b9\u6cd5\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u65e0\u4f3c\u7136\u66ff\u4ee3\u6a21\u578b\u80fd\u591f\u5728\u590d\u6742\u3001\u90e8\u5206\u89c2\u6d4b\u7684\u79bb\u6563\u4e8b\u4ef6\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u51c6\u786e\u3001\u9c81\u68d2\u548c\u5b9e\u65f6\u7684\u53c2\u6570\u6062\u590d\u3002"}}
{"id": "2507.11061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing.", "AI": {"tldr": "RoMaP\u662f\u4e00\u79cd\u65b0\u578b\u5c40\u90e83D\u9ad8\u65af\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc73D-GALP\u548c\u6b63\u5219\u5316SDS\u635f\u5931\u5b9e\u73b0\u7cbe\u786e\u7684\u90e8\u4ef6\u7ea7\u4fee\u6539\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u6e85\u5c04\u4e2d\u5c40\u90e83D\u7f16\u8f91\u7684\u6311\u6218\uff0c\u5982\u591a\u89c6\u89d22D\u5206\u5272\u4e0d\u4e00\u81f4\u548cSDS\u635f\u5931\u7684\u6a21\u7cca\u6027\u3002", "method": "\u63d0\u51fa3D-GALP\u6a21\u5757\u751f\u6210\u9c81\u68d2\u76843D\u63a9\u7801\uff0c\u5e76\u7ed3\u5408\u6b63\u5219\u5316SDS\u635f\u5931\uff08\u5982L1\u951a\u5b9a\u635f\u5931\u548cSLaMP\u7f16\u8f91\u65b9\u6cd5\uff09\u3002", "result": "RoMaP\u5728\u91cd\u5efa\u548c\u751f\u6210\u7684\u9ad8\u65af\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5c40\u90e83D\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "RoMaP\u4e3a\u90e8\u4ef6\u7ea73D\u9ad8\u65af\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10809", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10809", "abs": "https://arxiv.org/abs/2507.10809", "authors": ["Kazi Tasnim Zinat", "Yun Zhou", "Xiang Lyu", "Yawei Wang", "Zhicheng Liu", "Panpan Xu"], "title": "Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions", "comment": "Accepted at ICANN 2025", "summary": "Inferring causal relationships between event pairs in a temporal sequence is\napplicable in many domains such as healthcare, manufacturing, and\ntransportation. Most existing work on causal inference primarily focuses on\nevent types within the designated domain, without considering the impact of\nexogenous out-of-domain interventions. In real-world settings, these\nout-of-domain interventions can significantly alter causal dynamics. To address\nthis gap, we propose a new causal framework to define average treatment effect\n(ATE), beyond independent and identically distributed (i.i.d.) data in classic\nRubin's causal framework, to capture the causal relation shift between events\nof temporal process under out-of-domain intervention. We design an unbiased ATE\nestimator, and devise a Transformer-based neural network model to handle both\nlong-range temporal dependencies and local patterns while integrating\nout-of-domain intervention information into process modeling. Extensive\nexperiments on both simulated and real-world datasets demonstrate that our\nmethod outperforms baselines in ATE estimation and goodness-of-fit under\nout-of-domain-augmented point processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56e0\u679c\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u5916\u6e90\u5e72\u9884\u4e0b\u65f6\u95f4\u8fc7\u7a0b\u4e2d\u4e8b\u4ef6\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u53d8\u5316\uff0c\u5e76\u901a\u8fc7Transformer\u6a21\u578b\u6539\u8fdbATE\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9886\u57df\u5185\u4e8b\u4ef6\u7c7b\u578b\uff0c\u5ffd\u7565\u4e86\u5916\u6e90\u5e72\u9884\u7684\u5f71\u54cd\uff0c\u800c\u73b0\u5b9e\u4e2d\u8fd9\u4e9b\u5e72\u9884\u53ef\u80fd\u663e\u8457\u6539\u53d8\u56e0\u679c\u52a8\u6001\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65e0\u504f\u7684ATE\u4f30\u8ba1\u5668\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ee5\u5904\u7406\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u548c\u5c40\u90e8\u6a21\u5f0f\uff0c\u540c\u65f6\u6574\u5408\u5916\u6e90\u5e72\u9884\u4fe1\u606f\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5916\u6e90\u5e72\u9884\u589e\u5f3a\u7684\u70b9\u8fc7\u7a0b\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6a21\u578b\u6709\u6548\u6355\u6349\u4e86\u5916\u6e90\u5e72\u9884\u4e0b\u7684\u56e0\u679c\u52a8\u6001\u53d8\u5316\uff0c\u63d0\u5347\u4e86ATE\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.11143", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11143", "abs": "https://arxiv.org/abs/2507.11143", "authors": ["Lam Pham", "Cam Le", "Hieu Tang", "Khang Truong", "Truong Nguyen", "Jasmin Lampert", "Alexander Schindler", "Martin Boyer", "Son Phan"], "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images", "comment": null, "summary": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5229\u7528\u9065\u611f\u56fe\u50cf\u81ea\u52a8\u89c2\u6d4b\u6ed1\u5761\u4e8b\u4ef6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6ed1\u5761\u707e\u5bb3\u9891\u53d1\uff0c\u4f46\u4f20\u7edf\u89c2\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u5927\u8303\u56f4\u590d\u6742\u5730\u5f62\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u6ed1\u5761\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\uff0c\u8f93\u5165\u4e3a\u9065\u611f\u56fe\u50cf\u3002", "result": "\u5728LandSlide4Sense\u3001Bijie\u548cNepal\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u4efb\u52a1\u7684F1\u5206\u6570\u5206\u522b\u4e3a98.23\u548c93.83\uff0c\u5206\u5272\u4efb\u52a1\u7684mIoU\u5206\u6570\u4e3a63.74\u548c76.88\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6ed1\u5761\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10986", "abs": "https://arxiv.org/abs/2507.10986", "authors": ["Tianyu Su", "Zhiqiang Zou", "Ali Luo", "Xiao Kong", "Qingyu Lu", "Min Li"], "title": "StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data", "comment": null, "summary": "Stellar flare forecasting, a critical research frontier in astronomy, offers\nprofound insights into stellar activity. However, the field is constrained by\nboth the sparsity of recorded flare events and the absence of domain-specific\nlarge-scale predictive models. To address these challenges, this study\nintroduces StellarF (Stellar Flare Forecasting), a novel large model that\nleverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient\nlearning for stellar flare forecasting. At its core, StellarF integrates an\nflare statistical information module with a historical flare record module,\nenabling multi-scale pattern recognition from observational data. Extensive\nexperiments on our self-constructed datasets (derived from Kepler and TESS\nlight curves) demonstrate that StellarF achieves state-of-the-art performance\ncompared to existing methods. The proposed prediction paradigm establishes a\nnovel methodological framework for advancing astrophysical research and\ncross-disciplinary applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStellarF\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6052\u661f\u8000\u6591\u9884\u6d4b\uff0c\u901a\u8fc7LoRA\u548cAdapter\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u5e76\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6052\u661f\u8000\u6591\u9884\u6d4b\u662f\u5929\u6587\u7814\u7a76\u7684\u91cd\u8981\u524d\u6cbf\uff0c\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u758f\u548c\u7f3a\u4e4f\u4e13\u7528\u5927\u89c4\u6a21\u9884\u6d4b\u6a21\u578b\u3002", "method": "StellarF\u7ed3\u5408\u4e86\u8000\u6591\u7edf\u8ba1\u4fe1\u606f\u6a21\u5757\u548c\u5386\u53f2\u8bb0\u5f55\u6a21\u5757\uff0c\u652f\u6301\u591a\u5c3a\u5ea6\u6a21\u5f0f\u8bc6\u522b\uff0c\u5229\u7528LoRA\u548cAdapter\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u5728Kepler\u548cTESS\u5149\u53d8\u66f2\u7ebf\u6570\u636e\u96c6\u4e0a\uff0cStellarF\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "StellarF\u4e3a\u5929\u4f53\u7269\u7406\u7814\u7a76\u548c\u8de8\u5b66\u79d1\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u3002"}}
{"id": "2507.11252", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11252", "abs": "https://arxiv.org/abs/2507.11252", "authors": ["Guanghao Wu", "Chen Xu", "Hai Song", "Chong Wang", "Qixing Zhang"], "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection", "comment": "18 pages, 11 figures", "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of\ndeep learning, image-based smoke detection has become a crucial method for\ndetecting and preventing forest fires. However, the scarcity of smoke image\ndata from forest fires is one of the significant factors hindering the\ndetection of forest fire smoke. Image generation models offer a promising\nsolution for synthesizing realistic smoke images. However, current inpainting\nmodels exhibit limitations in generating high-quality smoke representations,\nparticularly manifesting as inconsistencies between synthesized smoke and\nbackground contexts. To solve these problems, we proposed a comprehensive\nframework for generating forest fire smoke images. Firstly, we employed the\npre-trained segmentation model and the multimodal model to obtain smoke masks\nand image captions.Then, to address the insufficient utilization of masks and\nmasked images by inpainting models, we introduced a network architecture guided\nby mask and masked image features. We also proposed a new loss function, the\nmask random difference loss, which enhances the consistency of the generated\neffects around the mask by randomly expanding and eroding the mask\nedges.Finally, to generate a smoke image dataset using random masks for\nsubsequent detection tasks, we incorporated smoke characteristics and use a\nmultimodal large language model as a filtering tool to select diverse and\nreasonable smoke images, thereby improving the quality of the synthetic\ndataset. Experiments showed that our generated smoke images are realistic and\ndiverse, and effectively enhance the performance of forest fire smoke detection\nmodels. Code is available at https://github.com/wghr123/MFGDiffusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u7684\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u4fee\u590d\u6a21\u578b\u548c\u5f15\u5165\u65b0\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u4e86\u70df\u96fe\u56fe\u50cf\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u7528\u4e8e\u589e\u5f3a\u70df\u96fe\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u4fee\u590d\u6a21\u578b\u751f\u6210\u70df\u96fe\u56fe\u50cf\u8d28\u91cf\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u4ee5\u652f\u6301\u70df\u96fe\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u83b7\u53d6\u70df\u96fe\u63a9\u7801\u548c\u56fe\u50cf\u63cf\u8ff0\uff0c\u63d0\u51fa\u57fa\u4e8e\u63a9\u7801\u548c\u63a9\u7801\u56fe\u50cf\u7279\u5f81\u5f15\u5bfc\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u63a9\u7801\u968f\u673a\u5dee\u5f02\u635f\u5931\u51fd\u6570\u3002", "result": "\u751f\u6210\u7684\u70df\u96fe\u56fe\u50cf\u771f\u5b9e\u4e14\u591a\u6837\uff0c\u6709\u6548\u63d0\u5347\u4e86\u68ee\u6797\u706b\u707e\u70df\u96fe\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u9ad8\u8d28\u91cf\u751f\u6210\u70df\u96fe\u56fe\u50cf\uff0c\u4e3a\u70df\u96fe\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.11357", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11357", "abs": "https://arxiv.org/abs/2507.11357", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "title": "Neurosymbolic Reasoning Shortcuts under the Independence Assumption", "comment": "Accepted at NeSy 2025", "summary": "The ubiquitous independence assumption among symbolic concepts in\nneurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors\nuse it to speed up probabilistic reasoning. Recent works like van Krieken et\nal. (2024) and Marconato et al. (2024) argued that the independence assumption\ncan hinder learning of NeSy predictors and, more crucially, prevent them from\ncorrectly modelling uncertainty. There is, however, scepticism in the NeSy\ncommunity around the scenarios in which the independence assumption actually\nlimits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle\nthis question by formally showing that assuming independence among symbolic\nconcepts entails that a model can never represent uncertainty over certain\nconcept combinations. Thus, the model fails to be aware of reasoning shortcuts,\ni.e., the pathological behaviour of NeSy predictors that predict correct\ndownstream tasks but for the wrong reasons.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7b26\u53f7\uff08NeSy\uff09\u9884\u6d4b\u5668\u4e2d\u7b26\u53f7\u6982\u5ff5\u72ec\u7acb\u6027\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u5176\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6b63\u786e\u8868\u793a\u67d0\u4e9b\u6982\u5ff5\u7ec4\u5408\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5ffd\u7565\u63a8\u7406\u6377\u5f84\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7\u9884\u6d4b\u5668\u901a\u5e38\u5047\u8bbe\u7b26\u53f7\u6982\u5ff5\u72ec\u7acb\u4ee5\u52a0\u901f\u6982\u7387\u63a8\u7406\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8d28\u7591\u8fd9\u79cd\u5047\u8bbe\u662f\u5426\u4f1a\u9650\u5236\u5b66\u4e60\u6548\u679c\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u660e\u786e\u72ec\u7acb\u6027\u5047\u8bbe\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u5206\u6790\uff0c\u8bc1\u660e\u7b26\u53f7\u6982\u5ff5\u72ec\u7acb\u6027\u5047\u8bbe\u4f1a\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u8868\u793a\u67d0\u4e9b\u6982\u5ff5\u7ec4\u5408\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u72ec\u7acb\u6027\u5047\u8bbe\u4f7f\u6a21\u578b\u65e0\u6cd5\u610f\u8bc6\u5230\u63a8\u7406\u6377\u5f84\u95ee\u9898\uff0c\u5373\u6a21\u578b\u53ef\u80fd\u56e0\u9519\u8bef\u539f\u56e0\u9884\u6d4b\u6b63\u786e\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u72ec\u7acb\u6027\u5047\u8bbe\u9650\u5236\u4e86\u795e\u7ecf\u7b26\u53f7\u9884\u6d4b\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u5176\u9002\u7528\u6027\u3002"}}
