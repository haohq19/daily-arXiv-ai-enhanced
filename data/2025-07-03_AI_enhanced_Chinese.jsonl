{"id": "2507.01160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u62bd\u8c61\u6458\u8981\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u548c\u539f\u6587\u7684\u4e8b\u4ef6\u91cd\u53e0\u6765\u8861\u91cf\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u6458\u8981\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u53c2\u8003\u6458\u8981\u7684\u76f8\u4f3c\u6027\u8bc4\u5206\uff0c\u4f46\u65b0\u95fb\u6458\u8981\u5e94\u53cd\u6620\u4e8b\u4ef6\u4fe1\u606f\uff0c\u56e0\u6b64\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5728\u632a\u5a01\u8bed\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u548c\u539f\u6587\u7684\u4e8b\u4ef6\u91cd\u53e0\u3002", "result": "\u65b9\u6cd5\u80fd\u66f4\u6df1\u5165\u5730\u5206\u6790\u6458\u8981\u4e2d\u7684\u4e8b\u4ef6\u4fe1\u606f\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u6458\u8981\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.01143", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01143", "abs": "https://arxiv.org/abs/2507.01143", "authors": ["Reza Jalayer", "Masoud Jalayer", "Amirali Baniasadi"], "title": "A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods", "comment": "35 pages", "summary": "Sound source localization (SSL) adds a spatial dimension to auditory\nperception, allowing a system to pinpoint the origin of speech, machinery\nnoise, warning tones, or other acoustic events, capabilities that facilitate\nrobot navigation, human-machine dialogue, and condition monitoring. While\nexisting surveys provide valuable historical context, they typically address\ngeneral audio applications and do not fully account for robotic constraints or\nthe latest advancements in deep learning. This review addresses these gaps by\noffering a robotics-focused synthesis, emphasizing recent progress in deep\nlearning methodologies. We start by reviewing classical methods such as Time\nDifference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and\nsubspace analysis. Subsequently, we delve into modern machine learning (ML) and\ndeep learning (DL) approaches, discussing traditional ML and neural networks\n(NNs), convolutional neural networks (CNNs), convolutional recurrent neural\nnetworks (CRNNs), and emerging attention-based architectures. The data and\ntraining strategy that are the two cornerstones of DL-based SSL are explored.\nStudies are further categorized by robot types and application domains to\nfacilitate researchers in identifying relevant work for their specific\ncontexts. Finally, we highlight the current challenges in SSL works in general,\nregarding environmental robustness, sound source multiplicity, and specific\nimplementation constraints in robotics, as well as data and learning strategies\nin DL-based SSL. Also, we sketch promising directions to offer an actionable\nroadmap toward robust, adaptable, efficient, and explainable DL-based SSL for\nnext-generation robots.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u58f0\u6e90\u5b9a\u4f4d\uff08SSL\uff09\u6280\u672f\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u591a\u5173\u6ce8\u901a\u7528\u97f3\u9891\u5e94\u7528\uff0c\u672a\u5145\u5206\u8003\u8651\u673a\u5668\u4eba\u9886\u57df\u7684\u9650\u5236\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u56de\u987e\u4e86\u7ecf\u5178\u65b9\u6cd5\uff08\u5982TDOA\u3001\u6ce2\u675f\u6210\u5f62\u7b49\uff09\u548c\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982CNN\u3001CRNN\u7b49\uff09\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u4e0e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u603b\u7ed3\u4e86\u4e0d\u540c\u673a\u5668\u4eba\u7c7b\u578b\u548c\u5e94\u7528\u9886\u57df\u7684\u7814\u7a76\uff0c\u5e76\u6307\u51fa\u5f53\u524dSSL\u5728\u73af\u5883\u9c81\u68d2\u6027\u3001\u591a\u58f0\u6e90\u5904\u7406\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60SSL\u6280\u672f\u3002"}}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TKG\u63a8\u7406\u65b9\u6cd5T3DM\uff0c\u901a\u8fc7\u5206\u5e03\u7279\u5f81\u5efa\u6a21\u548c\u5bf9\u6297\u8bad\u7ec3\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u4e8b\u4ef6\u5206\u5e03\u504f\u79fb\u548c\u4f4e\u8d28\u91cf\u8d1f\u6837\u672c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709TKG\u63a8\u7406\u65b9\u6cd5\u5728\u4e8b\u4ef6\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u8d1f\u6837\u672c\u751f\u6210\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u63d0\u51faT3DM\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5f15\u5bfc\u7684\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u5bf9\u6297\u8bad\u7ec3\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eT3DM\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u7ed3\u679c\u3002", "conclusion": "T3DM\u901a\u8fc7\u6539\u8fdb\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u548c\u8d1f\u6837\u672c\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86TKG\u63a8\u7406\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01264", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01264", "abs": "https://arxiv.org/abs/2507.01264", "authors": ["Yongjie Fu", "Ruijian Zha", "Pei Tian", "Xuan Di"], "title": "LLM-based Realistic Safety-Critical Driving Video Generation", "comment": null, "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u9a7e\u9a76\u573a\u666f\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u7ed3\u5408CARLA\u6a21\u62df\u5668\u548c\u89c6\u9891\u751f\u6210\u6280\u672f\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6d4b\u8bd5\u3002", "motivation": "\u8bbe\u8ba1\u591a\u6837\u4e14\u5b89\u5168\u7684\u9a7e\u9a76\u573a\u666f\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u751f\u6210\u590d\u6742\u573a\u666f\u6548\u7387\u4f4e\u3002", "method": "\u901a\u8fc7LLMs\u57fa\u4e8e\u5c11\u91cf\u793a\u4f8b\u751f\u6210\u573a\u666f\u811a\u672c\uff0c\u7ed3\u5408CARLA\u6a21\u62df\u5668\u548c\u89c6\u9891\u751f\u6210\u6280\u672f\uff08Cosmos-Transfer1\u4e0eControlNet\uff09\u5b9e\u73b0\u771f\u5b9e\u611f\u89c6\u9891\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u3001\u771f\u5b9e\u4e14\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u5c24\u5176\u662f\u7f55\u89c1\u8fb9\u7f18\u6848\u4f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u573a\u666f\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2507.01367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u7269\u7406\u653b\u51fb\u6846\u67b6PGA\uff0c\u7528\u4e8e\u751f\u6210\u591a\u89c6\u89d2\u9c81\u68d2\u7684\u5bf9\u6297\u6027\u4f2a\u88c5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u7269\u7406\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7f51\u683c\u5148\u9a8c\u548c\u865a\u62df\u73af\u5883\u7684\u7269\u7406\u653b\u51fb\u65b9\u6cd5\u8017\u65f6\u4e14\u4e0e\u73b0\u5b9e\u4e16\u754c\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u5728\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u5bf9\u6297\u6548\u679c\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u75283DGS\u5feb\u901f\u7cbe\u786e\u91cd\u5efa\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u901a\u8fc7\u9632\u6b62\u9ad8\u65af\u76f8\u4e92\u906e\u6321\u548c\u81ea\u906e\u6321\uff0c\u7ed3\u5408min-max\u4f18\u5316\u8c03\u6574\u80cc\u666f\uff0c\u63d0\u5347\u5bf9\u6297\u6548\u679c\u548c\u8de8\u89c6\u89d2\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGA\u5728\u5bf9\u6297\u6548\u679c\u548c\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "PGA\u6846\u67b6\u4e3a\u7269\u7406\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01384", "abs": "https://arxiv.org/abs/2507.01384", "authors": ["Langyu Wang", "Bingke Zhu", "Yingying Chen", "Yiyuan Zhang", "Ming Tang", "Jinqiao Wang"], "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing", "comment": "Accpted by ICCV 2025", "summary": "The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f2a\u6807\u7b7e\u589e\u5f3a\u7684\u97f3\u9891\u89c6\u89c9Mamba\u7f51\u7edc\uff08MUG\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u5f31\u76d1\u7763\u97f3\u9891\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u548c\u6a21\u578b\u67b6\u6784\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u540c\u65f6\u63d0\u5347\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u751f\u6210\u65b0\u6570\u636e\uff0c\u5e76\u91c7\u7528\u97f3\u9891\u89c6\u89c9Mamba\u7f51\u7edc\u5904\u7406\u7279\u5f81\u548c\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728LLP\u6570\u636e\u96c6\u4e0a\uff0cMUG\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u89c6\u89c9\u6bb5\u7ea7\u548c\u97f3\u9891\u6bb5\u7ea7\u6307\u6807\u5206\u522b\u63d0\u53472.1%\u548c1.2%\uff09\u3002", "conclusion": "MUG\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u548cMamba\u7f51\u7edc\u6709\u6548\u63d0\u5347\u4e86\u97f3\u9891\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01045", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u5fc3\u810f\u611f\u77e5\u57fa\u7840\u6a21\u578b\uff08CSFM\uff09\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u63a9\u7801\u9884\u8bad\u7ec3\u7b56\u7565\u4ece\u591a\u6a21\u6001\u6570\u636e\u4e2d\u5b66\u4e60\u7edf\u4e00\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u810f\u4fe1\u53f7\u5206\u6790\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5fc3\u810f\u4fe1\u53f7\u5206\u6790\u4e2d\u4f9d\u8d56\u540c\u8d28\u6570\u636e\u96c6\u548c\u9759\u6001\u5b9a\u5236\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u548c\u91c7\u96c6\u534f\u8bae\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528Transformer\u67b6\u6784\u548c\u751f\u6210\u5f0f\u63a9\u7801\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u5982MIMIC-III-WDB\u3001MIMIC-IV-ECG\u548cCODE\uff09\u4e2d\u5b66\u4e60\u7edf\u4e00\u8868\u793a\u3002", "result": "CSFM\u5728\u8bca\u65ad\u4efb\u52a1\u3001\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u8bc6\u522b\u3001\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u3001\u4e34\u5e8a\u7ed3\u679c\u9884\u6d4b\u548cECG\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5355\u6a21\u6001\u5355\u4efb\u52a1\u65b9\u6cd5\uff0c\u4e14\u5728\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "CSFM\u4f5c\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5168\u9762\u5fc3\u810f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.01048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afr\u00e2nio Jos\u00e9 de Melo Junior", "Celso Jos\u00e9 Munaro", "Cl\u00e1udio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Fl\u00e1vio Miguel Varej\u00e3o", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andr\u00e9s Lozano Cadena", "Jean Carlos Dias de Ara\u00fajo", "Jo\u00e3o Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rog\u00e9rio Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions.", "AI": {"tldr": "\u77f3\u6cb9\u884c\u4e1a\u4e2d\u7684\u4e0d\u826f\u4e8b\u4ef6\u53ef\u80fd\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931\u548c\u73af\u5883\u4e8b\u6545\u3002Petrobras\u5f00\u53d1\u76843W\u6570\u636e\u96c6\u4e3a\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u516c\u5f00\u6570\u636e\u652f\u6301\uff0c\u6700\u65b0\u7248\u672c\u5305\u542b\u66f4\u591a\u6807\u6ce8\u6570\u636e\u548c\u7ed3\u6784\u6539\u8fdb\u3002", "motivation": "\u77f3\u6cb9\u884c\u4e1a\u4e0d\u826f\u4e8b\u4ef6\u7684\u65e9\u671f\u68c0\u6d4b\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\uff0cPetrobras\u5f00\u53d13W\u6570\u636e\u96c6\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6784\u5efa3W\u6570\u636e\u96c6\uff0c\u5e76\u6301\u7eed\u66f4\u65b0\u548c\u6539\u8fdb\u3002", "result": "3W\u6570\u636e\u96c6\u6210\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u7840\u53c2\u8003\uff0c\u652f\u6301\u5f00\u53d1\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5\u548c\u6570\u5b57\u4ea7\u54c1\u3002", "conclusion": "3W\u6570\u636e\u96c6\u7684\u6539\u8fdb\u9f13\u52b1\u793e\u533a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u524d\u68c0\u6d4b\u4e0d\u826f\u4e8b\u4ef6\u5e76\u91c7\u53d6\u884c\u52a8\u3002"}}
{"id": "2507.01056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u6d2a\u6c34\u5982\u4f55\u52a0\u901f\u8def\u9762\u7c97\u7cd9\u5ea6\uff08\u901a\u8fc7IRI\u6d4b\u91cf\uff09\u7684\u6076\u5316\uff0c\u7ed3\u540820\u5e74\u8def\u9762\u6570\u636e\u548c\u6d2a\u6c34\u4e8b\u4ef6\uff0c\u5229\u7528\u7edf\u8ba1\u5206\u6790\u548cXAI\u6280\u672f\uff08\u5982SHAP\u548cLIME\uff09\uff0c\u53d1\u73b0\u6d2a\u6c34\u663e\u8457\u589e\u52a0\u8def\u9762\u7c97\u7cd9\u5ea6\uff0c\u5efa\u8bae\u91c7\u53d6\u9632\u6d2a\u63aa\u65bd\u3002", "motivation": "\u6d2a\u6c34\u5bf9\u8def\u9762\u57fa\u7840\u8bbe\u65bd\u9020\u6210\u4e25\u91cd\u635f\u5bb3\uff0c\u9700\u91cf\u5316\u5176\u5bf9\u8def\u9762\u7c97\u7cd9\u5ea6\u7684\u5f71\u54cd\uff0c\u4ee5\u5236\u5b9a\u6709\u6548\u7684\u9632\u6d2a\u7b56\u7565\u3002", "method": "\u7ed3\u5408TxDOT\u7684PMIS\u6570\u636e\u5e9320\u5e74\u8def\u9762\u6570\u636e\u548c\u6d2a\u6c34\u4e8b\u4ef6\u6570\u636e\uff0c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u5e76\u5e94\u7528XAI\u6280\u672f\uff08SHAP\u548cLIME\uff09\u8bc4\u4f30\u6d2a\u6c34\u5f71\u54cd\u3002", "result": "\u6d2a\u6c34\u663e\u8457\u52a0\u901f\u8def\u9762\u7c97\u7cd9\u5ea6\u7684\u589e\u52a0\uff0c\u4e0e\u975e\u6d2a\u6c34\u8def\u6bb5\u76f8\u6bd4\u6076\u5316\u66f4\u5feb\u3002", "conclusion": "\u5efa\u8bae\u91c7\u53d6\u4e3b\u52a8\u9632\u6d2a\u63aa\u65bd\uff08\u5982\u6539\u8fdb\u6392\u6c34\u7cfb\u7edf\u3001\u4f7f\u7528\u6297\u6d2a\u6750\u6599\uff09\u4ee5\u589e\u5f3a\u8def\u9762\u5728\u6613\u53d7\u707e\u533a\u57df\u7684\u97e7\u6027\u3002"}}
{"id": "2507.01811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01811", "abs": "https://arxiv.org/abs/2507.01811", "authors": ["Yash Kulkarni", "Susheela Sharma", "Sarah Go", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures", "comment": null, "summary": "Current pelvic fixation techniques rely on rigid drilling tools, which\ninherently constrain the placement of rigid medical screws in the complex\nanatomy of pelvis. These constraints prevent medical screws from following\nanatomically optimal pathways and force clinicians to fixate screws in linear\ntrajectories. This suboptimal approach, combined with the unnatural placement\nof the excessively long screws, lead to complications such as screw\nmisplacement, extended surgery times, and increased radiation exposure due to\nrepeated X-ray images taken ensure to safety of procedure. To address these\nchallenges, in this paper, we present the design and development of a unique 4\ndegree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic\nCT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling\ntrajectories that follow the natural curvatures of the pelvic anatomy. The\nperformance of the pelvic CT-SDR was thoroughly evaluated through several\nS-shape drilling experiments in simulated bone phantoms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b4\u81ea\u7531\u5ea6\u9aa8\u76c6\u540c\u5fc3\u7ba1\u53ef\u8f6c\u5411\u94bb\u5b54\u673a\u5668\u4eba\uff08pelvic CT-SDR\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u521a\u6027\u94bb\u5b54\u5de5\u5177\u5728\u9aa8\u76c6\u56fa\u5b9a\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u94bb\u5b54\u5de5\u5177\u9650\u5236\u4e86\u87ba\u9489\u7684\u653e\u7f6e\u8def\u5f84\uff0c\u5bfc\u81f4\u624b\u672f\u5e76\u53d1\u75c7\u589e\u52a0\uff0c\u5982\u87ba\u9489\u9519\u4f4d\u3001\u624b\u672f\u65f6\u95f4\u5ef6\u957f\u548c\u8f90\u5c04\u66b4\u9732\u589e\u52a0\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd4\u81ea\u7531\u5ea6\u7684\u9aa8\u76c6\u540c\u5fc3\u7ba1\u53ef\u8f6c\u5411\u94bb\u5b54\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5b9e\u73b0S\u5f62\u94bb\u5b54\u8def\u5f84\u3002", "result": "\u5728\u6a21\u62df\u9aa8\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86S\u5f62\u94bb\u5b54\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u80fd\u591f\u9075\u5faa\u9aa8\u76c6\u81ea\u7136\u66f2\u7387\uff0c\u4f18\u5316\u87ba\u9489\u653e\u7f6e\u8def\u5f84\uff0c\u51cf\u5c11\u5e76\u53d1\u75c7\u3002"}}
{"id": "2507.01067", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors.", "AI": {"tldr": "\u672c\u6587\u4f18\u5316\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u9ad8\u6027\u80fd\u673a\u5668\u5b66\u4e60\u670d\u52a1\u4e2d\u7684\u5076\u53d1\u6027\u6216\u5c16\u5cf0\u6027\u751f\u4ea7\u4e2d\u65ad\uff0c\u5e76\u4e0e\u7ecf\u5178\u968f\u673a\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u9884\u6d4b\u7f55\u89c1\u4e14\u5c16\u5cf0\u6027\u7684\u4e8b\u4ef6\uff08\u5982\u751f\u4ea7\u4e2d\u65ad\uff09\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u57fa\u7840\u6a21\u578b\u5c1a\u672a\u5728\u6b64\u7c7b\u6781\u7aef\u4e8b\u4ef6\u4e2d\u5f97\u5230\u5e94\u7528\u3002", "method": "\u4f18\u5316\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u7ecf\u5178\u968f\u673a\u9884\u6d4b\u6a21\u578b\uff08\u5982\u79fb\u52a8\u5e73\u5747\u548c\u81ea\u56de\u5f52\u6a21\u578b\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u5176\u9884\u6d4b\u8bef\u5dee\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u5076\u53d1\u6027\u6216\u5c16\u5cf0\u6027\u4e8b\u4ef6\u65f6\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u6a21\u578b\uff0c\u5e76\u80fd\u4ee5\u5c0f\u4e8e6%\u7684\u8bef\u5dee\u4f30\u8ba1\u5e74\u5ea6\u4e2d\u65ad\u7edf\u8ba1\u6570\u636e\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u7f55\u89c1\u4e8b\u4ef6\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4f18\u5316\u540e\u80fd\u591f\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u3002"}}
{"id": "2507.01472", "categories": ["cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.01472", "abs": "https://arxiv.org/abs/2507.01472", "authors": ["Jon\u00e1\u0161 Herec", "V\u00edt R\u016f\u017ei\u010dka", "Rado Pito\u0148\u00e1k"], "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware", "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference", "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u7532\u70f7\u6cc4\u6f0f\u68c0\u6d4b\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u661f\u8f7d\u786c\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u901f\u5ea6\u3002", "motivation": "\u7532\u70f7\u662f\u5f3a\u6548\u6e29\u5ba4\u6c14\u4f53\uff0c\u65e9\u671f\u68c0\u6d4b\u6709\u52a9\u4e8e\u51cf\u7f13\u6c14\u5019\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u7684\u661f\u8f7d\u786c\u4ef6\u4e0a\u5b9e\u73b0\u3002", "method": "\u6d4b\u8bd5\u4e86\u5feb\u901f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08ACE\u3001CEM\uff09\uff0c\u5e76\u63d0\u51faMag1c-SAS\u7b97\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08U-Net\u3001LinkNet\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "Mag1c-SAS\u548cCEM\u5728\u68c0\u6d4b\u5f3a\u7fbd\u6d41\u65f6\u8868\u73b0\u826f\u597d\uff0c\u8ba1\u7b97\u901f\u5ea6\u5206\u522b\u6bd4\u539fMag1c\u5feb\u7ea6100\u500d\u548c230\u500d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u661f\u8f7d\u7532\u70f7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b97\u6cd5\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPARL\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u68c0\u6d4b\u5f02\u5e38\u4e8b\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5ba2\u6237\u670d\u52a1\u5bf9\u8bdd\u4e2d\u5f02\u5e38\u4e8b\u4ef6\u68c0\u6d4b\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u6311\u6218\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4ee5\u6700\u5927\u5316\u5546\u4e1a\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u56f0\u60d1\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08APARL\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u73af\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u67b6\u6784\uff0c\u9010\u6b65\u63d0\u5347\u6a21\u578b\u5bf9\u6311\u6218\u6027\u6837\u672c\u7684\u5904\u7406\u80fd\u529b\u3002", "result": "\u5728\u98df\u54c1\u914d\u9001\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0cAPARL\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0cF1\u5206\u6570\u5e73\u5747\u63d0\u9ad817.19%\uff0c\u8de8\u9886\u57df\u6d4b\u8bd5\u5e73\u5747\u63d0\u53479.59%\u3002", "conclusion": "APARL\u4e3a\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f18\u8d8a\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8fd0\u8425\u6548\u7387\u548c\u5546\u4e1a\u6548\u76ca\u3002"}}
{"id": "2507.01557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01557", "abs": "https://arxiv.org/abs/2507.01557", "authors": ["Marcin Kowlaczyk", "Tomasz Kryjak"], "title": "Interpolation-Based Event Visual Data Filtering Algorithms", "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Vancouver, 2023.\n  Copyright IEEE", "summary": "The field of neuromorphic vision is developing rapidly, and event cameras are\nfinding their way into more and more applications. However, the data stream\nfrom these sensors is characterised by significant noise. In this paper, we\npropose a method for event data that is capable of removing approximately 99\\%\nof noise while preserving the majority of the valid signal. We have proposed\nfour algorithms based on the matrix of infinite impulse response (IIR) filters\nmethod. We compared them on several event datasets that were further modified\nby adding artificially generated noise and noise recorded with dynamic vision\nsensor. The proposed methods use about 30KB of memory for a sensor with a\nresolution of 1280 x 720 and is therefore well suited for implementation in\nembedded devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u9650\u8109\u51b2\u54cd\u5e94\uff08IIR\uff09\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u80fd\u53bb\u9664\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u7ea699%\u7684\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u6548\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6d41\u4e2d\u5b58\u5728\u663e\u8457\u566a\u58f0\uff0c\u5f71\u54cd\u5e94\u7528\u6548\u679c\uff0c\u9700\u9ad8\u6548\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u57fa\u4e8eIIR\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u6dfb\u52a0\u4eba\u5de5\u566a\u58f0\u548c\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u65b9\u6cd5\u80fd\u53bb\u9664\u7ea699%\u566a\u58f0\uff0c\u5185\u5b58\u5360\u7528\u7ea630KB\uff081280x720\u5206\u8fa8\u7387\uff09\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u53bb\u566a\u4e14\u8d44\u6e90\u5360\u7528\u4f4e\uff0c\u9002\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5d4c\u5165\u5f0f\u5e94\u7528\u3002"}}
{"id": "2507.01235", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u5efa\u6a21\u76ae\u80a4\u7535\u5bfc\u53cd\u5e94\uff08SCR\uff09\u4e8b\u4ef6\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u548c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u590d\u6742\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u7279\u522b\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u9ad8\u7ef4\u6570\u636e\u8868\u793a\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8ePennylane\u7684QSVM\uff08\u4f7f\u7528\u516b\u91cf\u5b50\u4f4dZZ\u7279\u5f81\u6620\u5c04\uff09\u548cQNN\uff08\u4f7f\u7528\u6811\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\u548c\u516b\u91cf\u5b50\u4f4dZZ\u7279\u5f81\u6620\u5c04\uff09\u3002", "result": "QSVM\u8bad\u7ec3\u7cbe\u5ea6\u9ad8\u4f46\u6d4b\u8bd5\u7cbe\u5ea6\u4f4e\uff0845%\uff09\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff1bQNN\u6d4b\u8bd5\u7cbe\u5ea6\u66f4\u9ad8\uff0855%\uff09\uff0c\u4f18\u4e8eQSVM\u548c\u7ecf\u5178\u6a21\u578b\u3002", "conclusion": "QNN\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.01470", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u9891\u7387\u4f5c\u4e3a\u4efb\u52a1\u96be\u5ea6\u8861\u91cf\u6807\u51c6\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5173\u952e\u5b50\u76ee\u6807\u65e0\u76f4\u63a5\u5956\u52b1\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6311\u6218\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5047\u8bbe\uff0c\u5373\u5956\u52b1\u9891\u7387\u80fd\u6709\u6548\u8861\u91cf\u4efb\u52a1\u96be\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5173\u952e\u5b50\u76ee\u6807\u65e0\u76f4\u63a5\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f62\u5f0f\u5316\u96f6\u6fc0\u52b1\u52a8\u6001\u95ee\u9898\uff0c\u5e76\u5206\u6790\u73b0\u6709\u6df1\u5ea6\u5b50\u76ee\u6807\u7b97\u6cd5\u5728\u6b64\u7c7b\u52a8\u6001\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7b97\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u96f6\u6fc0\u52b1\u52a8\u6001\uff0c\u4e14\u5b66\u4e60\u6027\u80fd\u5bf9\u5b50\u76ee\u6807\u5b8c\u6210\u4e0e\u6700\u7ec8\u5956\u52b1\u7684\u65f6\u95f4\u63a5\u8fd1\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u63a8\u65ad\u6f5c\u5728\u4efb\u52a1\u7ed3\u6784\u800c\u4e0d\u4f9d\u8d56\u5373\u65f6\u5956\u52b1\u7684\u673a\u5236\u3002"}}
{"id": "2507.01551", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "SPRO\u662f\u4e00\u79cd\u81ea\u5f15\u5bfc\u8fc7\u7a0b\u5956\u52b1\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5728\u63a8\u5bfc\u8fc7\u7a0b\u5956\u52b1\u548c\u5f15\u5165\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u4e0e\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6d4b\u8bd5\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8fc7\u7a0b\u5f3a\u5316\u5b66\u4e60\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSPRO\u6846\u67b6\uff0c\u5305\u62ec\u5185\u5728\u63a8\u5bfc\u8fc7\u7a0b\u5956\u52b1\u548c\u5b9a\u4e49\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u4e0e\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf\uff08MSA\uff09\u3002", "result": "SPRO\u8bad\u7ec3\u6548\u7387\u63d0\u9ad83.4\u500d\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u534717.5%\uff0c\u4e14\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u7ea61/3\u3002", "conclusion": "SPRO\u5728\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u8fc7\u7a0b\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2507.01631", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "Snake-NeRF\u662f\u4e00\u79cd\u6269\u5c55\u5230\u5927\u573a\u666f\u7684NeRF\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u5185\u5b58\u9650\u5236\u4ec5\u9002\u7528\u4e8e\u5c0f\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406\u5927\u5c3a\u5ea6\u536b\u661f\u56fe\u50cf\u3002", "method": "\u5c06\u573a\u666f\u5212\u5206\u4e3a\u65e0\u91cd\u53e0\u76843D\u5757\uff0c\u88c1\u526a\u56fe\u50cf\u4ee5\u4fdd\u7559\u91cd\u53e0\u50cf\u7d20\uff0c\u91c7\u75282\u00d72 3D\u5757\u6e10\u8fdb\u7b56\u7565\u548c\u5206\u6bb5\u91c7\u6837\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355GPU\u4e0a\u7ebf\u6027\u65f6\u95f4\u5904\u7406\u5927\u536b\u661f\u56fe\u50cf\uff0c\u4e14\u8d28\u91cf\u65e0\u635f\u5931\u3002", "conclusion": "Snake-NeRF\u6210\u529f\u6269\u5c55\u4e86NeRF\u7684\u5e94\u7528\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u5927\u573a\u666f3D\u91cd\u5efa\u3002"}}
{"id": "2507.01654", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01654", "abs": "https://arxiv.org/abs/2507.01654", "authors": ["Martine Hjelkrem-Tan", "Marius Aasan", "Gabriel Y. Arteaga", "Ad\u00edn Ram\u00edrez Rivera"], "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers", "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT", "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.", "AI": {"tldr": "SPoT\u662f\u4e00\u79cd\u65b0\u7684\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8fde\u7eed\u653e\u7f6e\u6807\u8bb0\u907f\u514d\u7f51\u683c\u9650\u5236\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u7279\u5f81\u9650\u5236\u5728\u79bb\u6563\u7684\u8865\u4e01\u7f51\u683c\u4e2d\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u7a00\u758f\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faSubpixel Placement of Tokens (SPoT)\uff0c\u901a\u8fc7\u8fde\u7eed\u653e\u7f6e\u6807\u8bb0\u5e76\u7ed3\u5408oracle-guided\u641c\u7d22\u4f18\u5316\u4f4d\u7f6e\u3002", "result": "SPoT\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "SPoT\u4e3aViT\u67b6\u6784\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u5411\uff0c\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u6218\u7565\u4f18\u52bf\u3002"}}
{"id": "2507.01737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01737", "abs": "https://arxiv.org/abs/2507.01737", "authors": ["Lin Wu", "Zhixiang Chen", "Jianglin Lan"], "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "comment": null, "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.", "AI": {"tldr": "HOI-Dyn\u6846\u67b6\u901a\u8fc7\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\u751f\u62103D\u4eba-\u7269\u4ea4\u4e92\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\u9884\u6d4b\u7269\u4f53\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u52a8\u529b\u5b66\u635f\u5931\u63d0\u5347\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u4eba\u4e0e\u7269\u8fd0\u52a8\uff0c\u5bfc\u81f4\u7269\u7406\u4e0d\u5408\u7406\u548c\u56e0\u679c\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faHOI-Dyn\u6846\u67b6\uff0c\u5c06\u4eba-\u7269\u4ea4\u4e92\u5efa\u6a21\u4e3a\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\uff0c\u4f7f\u7528Transformer\u9884\u6d4b\u7269\u4f53\u54cd\u5e94\uff0c\u5e76\u5f15\u5165\u6b8b\u5dee\u52a8\u529b\u5b66\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u4ea4\u4e92\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "HOI-Dyn\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e863D\u4eba-\u7269\u4ea4\u4e92\u7684\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.01761", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u6307\u6807\uff08Clipped Density\u548cClipped Coverage\uff09\uff0c\u7528\u4e8e\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u6837\u672c\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u5728\u9c81\u68d2\u6027\u548c\u6821\u51c6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5173\u952e\u5e94\u7528\u4e2d\u56e0\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u6837\u672c\u8d28\u91cf\u800c\u53d7\u9650\uff0c\u73b0\u6709\u6307\u6807\u7f3a\u4e4f\u6821\u51c6\u6216\u5bf9\u5f02\u5e38\u503c\u4e0d\u591f\u9c81\u68d2\u3002", "method": "\u901a\u8fc7\u526a\u88c1\u5355\u4e2a\u6837\u672c\u8d21\u732e\u548c\u6700\u8fd1\u90bb\u7403\u534a\u5f84\uff0c\u63d0\u51faClipped Density\u548cClipped Coverage\u6307\u6807\uff0c\u9632\u6b62\u5f02\u5e38\u6837\u672c\u5f71\u54cd\u6574\u4f53\u8bc4\u4f30\u3002", "result": "\u65b0\u6307\u6807\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3001\u654f\u611f\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Clipped Density\u548cClipped Coverage\u4e3a\u751f\u6210\u6a21\u578b\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u76f4\u89c2\u7684\u5de5\u5177\u3002"}}
{"id": "2507.01882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01882", "abs": "https://arxiv.org/abs/2507.01882", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Marcel Hussing", "Edward Zhang", "Eric Eaton", "Daniel A. Hashimoto"], "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video", "comment": "Accepted by MICCAI2025", "summary": "Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u89c6\u9891\u4e2d\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u624b\u672f\u89c6\u9891\uff09\u4e2d\u7684\u5f02\u6784\u573a\u666f\u96be\u4ee5\u89e3\u6790\u4e3a\u6709\u610f\u4e49\u7684\u4e00\u7ec4\u69fd\uff0c\u73b0\u6709\u81ea\u9002\u5e94\u69fd\u8ba1\u6570\u65b9\u6cd5\u5728\u624b\u672f\u89c6\u9891\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7ed3\u5408\u65f6\u5e8f\u63a8\u7406\u548c\u9884\u6d4b\u672a\u6765\u69fd\u521d\u59cb\u5316\u7684\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u65e0\u76d1\u7763\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u6210\u4e3a\u533b\u7597\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.01875", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gast\u00f3n Garc\u00eda Gonz\u00e1lez", "Pedro Casas", "Emilio Mart\u00ednez", "Alicia Fern\u00e1ndez"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.", "AI": {"tldr": "FAE\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6269\u5f20\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u6d4b\u3002", "motivation": "\u53d7\u5927\u578b\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6210\u529f\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u548c\u6269\u5f20\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNNs\uff09\u6784\u5efa\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u652f\u6301\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff08\u5305\u62ec\u79fb\u52a8ISP\u6570\u636e\u548cKDD 2021\u6570\u636e\u96c6\uff09\u4e0a\u5c55\u793a\u4e86\u521d\u6b65\u7ed3\u679c\u3002", "conclusion": "FAE\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01927", "abs": "https://arxiv.org/abs/2507.01927", "authors": ["Zhentan Zheng"], "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision", "comment": null, "summary": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aevMLP\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u5c40\u90e8\u66f4\u65b0\u673a\u5236\uff0c\u9009\u62e9\u6027\u5904\u7406\u56fe\u50cf\u6216\u7279\u5f81\u56fe\u4e2d\u7684\u53d8\u5316\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u5728\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u89e3\u51b3\u89c6\u9891\u5904\u7406\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u63d0\u51faevMLP\u6a21\u578b\uff0c\u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u4ec5\u5904\u7406\u8fde\u7eed\u5e27\u95f4\u53d1\u751f\u53d8\u5316\u7684\u56fe\u50cf\u5757\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u4e14\u4fdd\u6301\u8f93\u51fa\u4e00\u81f4\u6027\u3002", "conclusion": "evMLP\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u6709\u6548\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
