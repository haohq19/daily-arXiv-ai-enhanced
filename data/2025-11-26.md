<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 11]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks](https://arxiv.org/abs/2511.19474)
*Jie Li,Hongyi Cai,Mingkang Dong,Muxin Pu,Shan You,Fei Wang,Tao Huang*

Main category: cs.CV

TL;DR: Pistachio是一个通过生成式流水线构建的视频异常检测与理解新基准，解决了现有基准在场景多样性、异常覆盖和时间复杂性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测基准缺乏场景多样性、平衡的异常覆盖和时间复杂性，难以可靠评估真实世界性能。同时，视频异常理解需要更深的语义和因果推理，但人工标注成本高昂。

Method: 利用视频生成模型，通过场景条件异常分配、多步骤故事情节生成和时间一致的长格式合成策略，构建了包含41秒视频的基准数据集。

Result: Pistachio基准在规模、多样性和复杂性方面表现出色，为现有方法带来新挑战，并推动动态和多事件异常理解的未来研究。

Conclusion: Pistachio通过生成式流水线有效消除了互联网收集数据集的偏见和限制，为视频异常检测和理解提供了更可靠的评估基准。

Abstract: Automatically detecting abnormal events in videos is crucial for modern autonomous systems, yet existing Video Anomaly Detection (VAD) benchmarks lack the scene diversity, balanced anomaly coverage, and temporal complexity needed to reliably assess real-world performance. Meanwhile, the community is increasingly moving toward Video Anomaly Understanding (VAU), which requires deeper semantic and causal reasoning but remains difficult to benchmark due to the heavy manual annotation effort it demands. In this paper, we introduce Pistachio, a new VAD/VAU benchmark constructed entirely through a controlled, generation-based pipeline. By leveraging recent advances in video generation models, Pistachio provides precise control over scenes, anomaly types, and temporal narratives, effectively eliminating the biases and limitations of Internet-collected datasets. Our pipeline integrates scene-conditioned anomaly assignment, multi-step storyline generation, and a temporally consistent long-form synthesis strategy that produces coherent 41-second videos with minimal human intervention. Extensive experiments demonstrate the scale, diversity, and complexity of Pistachio, revealing new challenges for existing methods and motivating future research on dynamic and multi-event anomaly understanding.

</details>


### [2] [What You See is (Usually) What You Get: Multimodal Prototype Networks that Abstain from Expensive Modalities](https://arxiv.org/abs/2511.19752)
*Muchang Bahng,Charlie Berens,Jon Donnelly,Eric Chen,Chaofan Chen,Cynthia Rudin*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态原型网络方法，用于物种检测，通过智能分配昂贵的基因数据来提高效率并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态神经网络在物种检测中的两个主要问题：黑盒性质导致决策过程不可解释，以及基因数据收集成本高且需要侵入性操作。

Method: 扩展原型网络(ProtoPNets)到多模态成本感知设置，通过集成每个模态的原型并使用相关权重确定预测对每个模态的依赖程度，同时引入方法识别不需要昂贵基因信息即可做出自信预测的情况。

Result: 该方法能够智能分配昂贵的基因数据用于细粒度区分，同时使用丰富的图像数据进行清晰的视觉分类，达到与始终使用两种模态的模型相当的准确率。

Conclusion: 提出的多模态原型网络方法在保持可解释性的同时，能够有效降低物种检测中对昂贵基因数据的依赖，实现成本效益平衡的自动化物种识别。

Abstract: Species detection is important for monitoring the health of ecosystems and identifying invasive species, serving a crucial role in guiding conservation efforts. Multimodal neural networks have seen increasing use for identifying species to help automate this task, but they have two major drawbacks. First, their black-box nature prevents the interpretability of their decision making process. Second, collecting genetic data is often expensive and requires invasive procedures, often necessitating researchers to capture or kill the target specimen. We address both of these problems by extending prototype networks (ProtoPNets), which are a popular and interpretable alternative to traditional neural networks, to the multimodal, cost-aware setting. We ensemble prototypes from each modality, using an associated weight to determine how much a given prediction relies on each modality. We further introduce methods to identify cases for which we do not need the expensive genetic information to make confident predictions. We demonstrate that our approach can intelligently allocate expensive genetic data for fine-grained distinctions while using abundant image data for clearer visual classifications and achieving comparable accuracy to models that consistently use both modalities.

</details>


### [3] [Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization](https://arxiv.org/abs/2511.19811)
*Debin Meng,Chen Jin,Zheng Gao,Yanran Li,Ioannis Patras,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 提出了TPSO方法，一种无需训练且模型无关的模块，通过在token嵌入空间中探索欠表示区域来提升文本到图像扩散模型的生成多样性，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型中图像多样性不足的问题，避免生成重复输出和采样冗余，支持创造性探索和下游应用。

Method: TPSO引入可学习参数来探索token嵌入空间中的欠表示区域，减少模型重复从学习分布的强模式生成样本的趋势，同时通过提示级空间提供全局语义约束来调节分布偏移。

Result: 在MS-COCO和三个扩散骨干网络上的广泛实验表明，TPSO显著提升了生成多样性，将基线性能从1.10提高到4.18分，且不牺牲图像质量。

Conclusion: TPSO是一种有效的训练免费方法，能够显著增强扩散模型的生成多样性，同时保持高质量输出。

Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.

</details>


### [4] [CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding](https://arxiv.org/abs/2511.19923)
*Yuefei Chen,Jiang Liu,Xiaodong Lin,Ruixiang Tang*

Main category: cs.CV

TL;DR: 提出了CounterVQA视频基准测试来评估视觉语言模型的因果推理能力，发现现有模型在复杂因果链推理上表现不佳，并开发了CFGPT方法来提升模型的视觉因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视频理解方面取得了显著进展，但其因果推理能力（推断假设条件下的替代结果）尚未得到充分探索。这种能力对于稳健的视频理解至关重要，因为它需要识别潜在的因果结构并推理未观察到的可能性。

Method: 引入CounterVQA基准测试，包含三个渐进难度级别来评估因果推理的不同方面；开发了CFGPT后训练方法，通过从语言模态中提取因果推理能力来增强模型的视觉因果推理能力。

Result: 评估发现最先进的模型在简单因果问题上表现合理，但在复杂多跳因果链上性能显著下降；CFGPT方法在所有CounterVQA难度级别上都带来了持续改进。

Conclusion: 当前视觉语言模型在因果推理方面存在显著局限性，特别是在复杂因果链推理上；CFGPT方法有效提升了模型的视觉因果推理能力，为改进视频理解模型提供了方向。

Abstract: Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.

</details>


### [5] [ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction](https://arxiv.org/abs/2511.20020)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 提出ACIT模型，通过注意力引导的跨模态交互Transformer预测行人过街意图，在JAAD数据集上达到70%和89%的准确率。


<details>
  <summary>Details</summary>
Motivation: 有效提取和整合不同类型数据中的互补线索是预测行人过街意图的主要挑战。

Method: 使用六种视觉和运动模态，分为三组交互对，采用双路径注意力机制、跨模态注意力和Transformer时序特征聚合模块。

Result: 在JAADbeh和JAADall数据集上分别达到70%和89%的准确率，优于现有方法。

Conclusion: ACIT模型通过有效的跨模态交互和时序建模，显著提升了行人过街意图预测性能。

Abstract: Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.

</details>


### [6] [Blind Adaptive Local Denoising for CEST Imaging](https://arxiv.org/abs/2511.20081)
*Chu Chen,Aitor Artola,Yang Liu,Se Weon Park,Raymond H. Chan,Jean-Michel Morel,Kannie W. Y. Chan*

Main category: cs.CV

TL;DR: 提出了一种新的盲自适应局部去噪方法BALD，用于解决CEST MRI中的异方差噪声问题，无需先验噪声知识即可提升定量对比映射的准确性。


<details>
  <summary>Details</summary>
Motivation: CEST MRI在临床转化中面临空间变化噪声和复杂成像协议导致的异方差性问题，传统去噪方法无法处理这种复杂噪声且会改变关键生物医学信息。

Method: BALD利用CEST数据的自相似性推导自适应方差稳定变换，通过局部SVD分解进行两阶段去噪，在数据线性变换中分离分子信号和噪声。

Result: 在多个体模和活体CEST扫描实验中，BALD在去噪指标和下游任务（如分子浓度图估计和癌症检测）上均优于现有最先进的CEST去噪方法。

Conclusion: BALD方法有效解决了CEST MRI中的异方差噪声问题，显著提升了定量分析的准确性和临床应用价值。

Abstract: Chemical Exchange Saturation Transfer (CEST) MRI enables molecular-level visualization of low-concentration metabolites by leveraging proton exchange dynamics. However, its clinical translation is hindered by inherent challenges: spatially varying noise arising from hardware limitations, and complex imaging protocols introduce heteroscedasticity in CEST data, perturbing the accuracy of quantitative contrast mapping such as amide proton transfer (APT) imaging. Traditional denoising methods are not designed for this complex noise and often alter the underlying information that is critical for biomedical analysis. To overcome these limitations, we propose a new Blind Adaptive Local Denoising (BALD) method. BALD exploits the self-similar nature of CEST data to derive an adaptive variance-stabilizing transform that equalizes the noise distributions across CEST pixels without prior knowledge of noise characteristics. Then, BALD performs two-stage denoising on a linear transformation of data to disentangle molecular signals from noise. A local SVD decomposition is used as a linear transform to prevent spatial and spectral denoising artifacts. We conducted extensive validation experiments on multiple phantoms and \textit{in vivo} CEST scans. In these experiments, BALD consistently outperformed state-of-the-art CEST denoisers in both denoising metrics and downstream tasks such as molecular concentration maps estimation and cancer detection.

</details>


### [7] [While recognizing actions, LMMs struggle to detect core interaction events](https://arxiv.org/abs/2511.20162)
*Daniel Harari,Michael Sidorov,Liel David,Chen Shterental,Abrham Kahsay Gebreselasie,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 大型多模态模型在视频交互事件定位任务中表现不佳，虽然能识别物体和动作，但无法准确定位交互开始/结束的时间和位置。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型是否真正将语义理解建立在视觉输入基础上，特别是在物理交互事件的时空定位能力。

Method: 构建首个大规模交互事件标注数据集（20K+标注），基于Something-Something-V2视频，让两个LMM模型定位物体与代理的接触和分离事件。

Result: 模型能可靠命名目标物体、识别动作并提供连贯推理，但持续无法识别交互开始/结束的帧数，也无法在场景中定位事件位置。

Conclusion: 模型缺乏感知基础，难以精确定义交互的物理接触时刻和位置，限制了其对动态场景的深层理解能力。

Abstract: Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.

</details>


### [8] [Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware](https://arxiv.org/abs/2511.20175)
*Federico Paredes-Valles,Yoshitaka Miyatani,Kirk Y. W. Scheper*

Main category: cs.CV

TL;DR: 首个电池供电的穿戴式瞳孔中心追踪系统，结合事件视觉传感和神经形态处理，在Speck2f芯片上实现100Hz双目追踪，单眼功耗低于5mW。


<details>
  <summary>Details</summary>
Motivation: 现有事件视觉传感器虽具备微秒级分辨率和稀疏数据流，但缺乏完全集成、低功耗的实时处理解决方案，难以实现穿戴式平台的鲁棒高频眼动追踪。

Method: 采用事件传感和神经形态处理集成方案，在Speck2f系统芯片上部署新型不确定性量化脉冲神经网络，配合门控时序解码，并在低功耗微控制器上进行轻量级坐标解码。

Result: 开发出穿戴式原型系统，实现100Hz双目瞳孔追踪，平均功耗低于5mW/眼，并在多用户数据集上验证了系统性能。

Conclusion: 端到端神经形态计算能够实现实用的、始终在线的眼动追踪，为下一代高能效穿戴系统提供了可行方案。

Abstract: Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.

</details>


### [9] [Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning](https://arxiv.org/abs/2511.20549)
*Guanjie Chen,Shirui Huang,Kai Liu,Jianchen Zhu,Xiaoye Qu,Peng Chen,Yu Cheng,Yifu Sun*

Main category: cs.CV

TL;DR: Flash-DMD是一个新颖的扩散模型加速框架，通过高效的timestep感知蒸馏和联合RL精炼，显著降低训练成本并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的迭代采样过程计算昂贵，现有的timestep蒸馏技术需要大量训练且会导致图像质量下降，而基于RL的微调过程不稳定且容易陷入奖励攻击。

Method: 提出高效的timestep感知蒸馏策略，大幅降低训练成本；引入联合训练方案，在继续timestep蒸馏训练的同时进行RL目标微调，利用蒸馏损失作为正则化器稳定RL训练。

Result: 仅需DMD2 2.1%的训练成本即可超越其性能；在少步采样机制下达到最先进的生成质量，在视觉质量、人类偏好和文本-图像对齐指标上优于现有方法。

Conclusion: Flash-DMD为训练高效、高保真且稳定的生成模型提供了有效范式，显著加速收敛并提升生成质量。

Abstract: Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.

</details>


### [10] [AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models](https://arxiv.org/abs/2511.20325)
*Tianyi Yan,Tao Tang,Xingtai Gui,Yongkang Li,Jiasen Zhesng,Weiyao Huang,Lingdong Kong,Wencheng Han,Xia Zhou,Xueyang Zhang,Yifei Zhan,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于公正世界模型的端到端自动驾驶策略优化框架，通过反事实合成方法让模型学会诚实预测危险，显著提升了安全性能。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型面临安全和长尾事件处理的挑战，强化学习虽然有望解决这些问题，但由于世界模型存在乐观偏见而难以成功。

Method: 引入公正世界模型框架，使用反事实合成方法系统生成碰撞和偏离道路事件的丰富课程，将模型从被动场景补全器转变为忠实预测动作与结果因果关系的验证性预测器。

Result: 在包括新的风险预见基准在内的广泛实验中，模型在预测失败方面显著优于基线方法，作为内部批评器使用时，在挑战性模拟中大幅减少了安全违规。

Conclusion: 教会模型预见危险是构建真正安全和智能自主智能体的关键步骤，公正世界模型框架为自动驾驶安全提供了有效解决方案。

Abstract: End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.

</details>


### [11] [Vision-Language Memory for Spatial Reasoning](https://arxiv.org/abs/2511.20644)
*Zuntao Liu,Yi Du,Taimeng Fu,Shaoshu Su,Cherie Ho,Chen Wang*

Main category: cs.CV

TL;DR: VLM²是一个具有持久记忆的视觉语言模型，通过双记忆模块解决视频空间推理中的语义几何错位和缺乏持久记忆问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视频空间推理方面仍达不到人类水平，主要存在语义几何错位和缺乏持久记忆两个挑战。

Method: 提出VLM²模型，采用双记忆模块：工作记忆作为滑动窗口关注即时上下文，情景记忆整合存储关键长期信息，实现固定计算成本下的高效长时程空间推理。

Result: 在多个基准测试中，VLM²在仅使用视频的模型中达到了最先进的性能，显著推进了视觉空间智能的前沿。

Conclusion: VLM²通过持久记忆和视图一致的3D感知表示，有效解决了视频空间推理中的关键挑战，为机器人空间推理能力提供了重要进展。

Abstract: Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 提出了一个基于Transformer的生存分析框架FACT，用于建模网约车司机的空闲行为，在时间依赖性C指数和Brier Score方面优于传统和深度学习生存模型。


<details>
  <summary>Details</summary>
Motivation: 网约车平台具有高频、行为驱动的特点，但生存分析在建模司机行为方面的应用仍未被充分探索。

Method: 将空闲行为建模为循环生存过程，使用基于Transformer的框架，结合因果掩码捕获长期时间依赖关系，并纳入司机特定嵌入来建模潜在异质性。

Result: 在多伦多网约车数据上的结果显示，FACT模型在时间依赖性C指数和Brier Score方面表现最佳。

Conclusion: 该方法能够实现更准确的风险估计，支持平台留存策略，并提供政策相关的见解。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [13] [MSTN: Fast and Efficient Multivariate Time Series Model](https://arxiv.org/abs/2511.20577)
*Sumit S Shevtekar,Chandresh K Maurya,Gourab Sil*

Main category: cs.LG

TL;DR: MSTN是一种多尺度时间网络，通过分层多尺度卷积编码器、序列建模组件和门控融合机制，自适应地建模从毫秒级到长期依赖的时间模式，在时间序列预测、插补和分类任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列数据具有高度非平稳性和复杂的多时间尺度动态特性，现有模型依赖固定尺度的结构先验，导致对时间动态的过度正则化，限制了它们自适应建模全谱时间变化的能力。

Method: 提出多尺度时间网络(MSTN)，包含：(i)多尺度卷积编码器构建分层特征金字塔；(ii)序列建模组件处理长期时间依赖；(iii)结合SE和多头时间注意力的门控融合机制进行动态特征集成。

Result: 在32个基准数据集中的24个上建立了新的最先进性能，在时间序列长期预测、插补、分类和泛化性研究中均表现出优于现有方法的竞争力。

Conclusion: MSTN通过统一框架自适应建模多时间尺度模式，在多个时间序列任务中实现了持续优异的表现，为时间序列分析提供了灵活且强大的解决方案。

Abstract: Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.

</details>


### [14] [BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents](https://arxiv.org/abs/2511.20597)
*Kaiyuan Zhang,Mark Tenenholtz,Kyle Polley,Jerry Ma,Denis Yarats,Ninghui Li*

Main category: cs.LG

TL;DR: 本文研究了AI网络代理中的提示注入攻击，创建了包含真实HTML载荷的攻击基准，评估了现有防御措施，并提出了多层防御策略。


<details>
  <summary>Details</summary>
Motivation: AI代理集成到浏览器带来了超越传统网络应用威胁模型的安全挑战，现有研究对提示注入攻击在真实环境中的影响理解不足。

Method: 创建嵌入真实HTML载荷的提示注入攻击基准，强调影响实际操作的攻击而非仅文本输出，并基于该基准对前沿AI模型的现有防御进行全面实证评估。

Result: 开发了具有真实复杂性和干扰频率的攻击基准，评估了现有防御措施的有效性，并提出了结合架构和模型防御的多层防御策略。

Conclusion: 通过纵深防御方法为设计实用、安全的网络代理提供了蓝图，能够应对不断演变的提示注入攻击。

Abstract: The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.
  In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.

</details>


### [15] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: 深度序列模型在血糖预测中未能有效利用胰岛素、饮食和活动等临床驱动因素，这种现象被称为Driver-Blindness。论文通过Δ_drivers指标量化这一问题，并分析了三个根本原因：架构偏好自相关性、数据质量问题和生理异质性。


<details>
  <summary>Details</summary>
Motivation: 现有血糖预测模型虽然理论上可以利用胰岛素、饮食和活动等临床驱动因素，但实际上这些驱动因素很少被有效利用，导致模型性能提升有限。

Method: 提出Δ_drivers指标来量化多变量模型相比单变量基线的性能提升，分析导致Driver-Blindness的三个因素：架构偏好、数据质量和生理异质性，并提出了生理特征编码器、因果正则化和个性化等缓解策略。

Result: 文献综述显示Δ_drivers通常接近零，表明当前模型未能有效利用临床驱动因素。提出的缓解策略能够部分解决这一问题。

Conclusion: 未来研究应常规报告Δ_drivers指标，防止忽略驱动因素的模型被误认为是先进模型，并推荐采用生理特征编码、因果正则化和个性化等策略来缓解Driver-Blindness问题。

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2511.20196)
*Zhen Zeng,Leijiang Gu,Zhangling Duan,Feng Li,Zenglin Shi,Cees G. M. Snoek,Meng Wang*

Main category: cs.AI

TL;DR: 提出了SMFA方法，通过记忆遗忘适配器和保留锚点引导的掩码机制，实现多模态大语言模型的精确可控遗忘，同时保持模型的通用图像理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型遗忘方法在移除隐私敏感信息时，往往会损害模型的通用图像理解性能，无法实现良性遗忘。

Method: SMFA首先微调模型将敏感响应替换为拒绝回答，生成记忆遗忘适配器，然后应用保留锚点引导的掩码机制来防止对无关知识和理解能力的干扰。

Result: 大量实验表明，与现有方法不同，SMFA能够实现精确可控的遗忘，同时保持模型的基础图像理解能力。

Conclusion: SMFA方法成功解决了多模态大语言模型在隐私保护与性能保持之间的平衡问题，为选择性遗忘提供了有效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.

</details>


### [17] [FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization](https://arxiv.org/abs/2511.20510)
*Yuto Suzuki,Paul Awolade,Daniel V. LaBarbera,Farnoush Banaei-Kashani*

Main category: cs.AI

TL;DR: FRAGMENTA是一个用于药物先导化合物优化的端到端框架，包含创新的生成模型和代理AI系统，在癌症药物发现实验中比基线方法识别出近两倍的高分分子。


<details>
  <summary>Details</summary>
Motivation: 分子生成在药物发现中至关重要，但类别特定数据集通常少于100个训练样本。现有启发式碎片化方法限制了多样性且遗漏关键片段，模型调优需要药物化学家和AI工程师之间缓慢的间接协作。

Method: 1) 将碎片化重构为"词汇选择"问题的新生成模型，使用动态Q学习联合优化碎片化和生成；2) 通过领域专家的对话反馈精炼目标的代理AI系统，从循环中移除AI工程师并逐步学习领域知识以实现自动化调优。

Result: 在真实世界癌症药物发现实验中，FRAGMENTA的人类-代理配置识别出比基线方法近两倍的高分分子。完全自主的代理-代理系统优于传统的人类-人类调优。

Conclusion: FRAGMENTA展示了代理调优在捕捉专家意图方面的有效性，为药物发现提供了高效的端到端优化框架。

Abstract: Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a "vocabulary selection" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [18] [Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494)
*Jakub Hoscilowicz,Artur Janicki*

Main category: cs.CL

TL;DR: 提出了一种针对多模态大语言模型的新型对抗攻击方法，通过最大化下一个token的熵来诱导模型生成不连贯或自信的错误输出，具有跨模型迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击主要关注越狱或目标错误分类，而本文旨在开发能系统性地破坏MLLM可靠性的攻击方法，防止MLLM驱动的智能体可靠运行。

Method: 使用小型开源MLLM集合，通过PGD对抗攻击方法最大化下一个token的熵，在完整图像和对抗验证码两种设置下进行攻击。

Result: 单个对抗图像能够破坏集合中所有模型，生成的扰动能够迁移到未见过的开源模型和专有模型上。

Conclusion: 对抗混淆攻击是一种有效的新型威胁，能够系统性地破坏MLLM的可靠性，且具有强大的跨模型迁移能力。

Abstract: We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.

</details>
