{"id": "2509.25244", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25244", "abs": "https://arxiv.org/abs/2509.25244", "authors": ["Shuide Wen", "Beier Ku", "Teng Wang", "Mingyang Zou", "Yang Yang"], "title": "Neo-Grounded Theory: A Methodological Innovation Integrating High-Dimensional Vector Clustering and Multi-Agent Collaboration for Qualitative Research", "comment": "44 pages, 11 figures", "summary": "Purpose: Neo Grounded Theory (NGT) integrates vector clustering with multi\nagent systems to resolve qualitative research's scale depth paradox, enabling\nanalysis of massive datasets in hours while preserving interpretive rigor.\nMethods: We compared NGT against manual coding and ChatGPT-assisted analysis\nusing 40,000 character Chinese interview transcripts. NGT employs\n1536-dimensional embeddings, hierarchical clustering, and parallel agent-based\ncoding. Two experiments tested pure automation versus human guided refinement.\nFindings: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks),\nsuperior quality (0.904 vs 0.883), and 96% cost reduction. Human AI\ncollaboration proved essential: automation alone produced abstract frameworks\nwhile human guidance yielded actionable dual pathway theories. The system\ndiscovered patterns invisible to manual coding, including identity bifurcation\nphenomena. Contributions: NGT demonstrates computational objectivity and human\ninterpretation are complementary. Vector representations provide reproducible\nsemantic measurement while preserving meaning's interpretive dimensions.\nResearchers shift from mechanical coding to theoretical guidance, with AI\nhandling pattern recognition while humans provide creative insight.\nImplications: Cost reduction from \\$50,000 to \\$500 democratizes qualitative\nresearch, enabling communities to study themselves. Real-time analysis makes\nqualitative insights contemporaneous with events. The framework shows\ncomputational methods can strengthen rather than compromise qualitative\nresearch's humanistic commitments.\n  Keywords: Grounded theory; Vector embeddings; Multi agent systems; Human AI\ncollaboration; Computational qualitative analysis", "AI": {"tldr": "Neo Grounded Theory (NGT) \u7ed3\u5408\u5411\u91cf\u805a\u7c7b\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5b9a\u6027\u7814\u7a76\u7684\u89c4\u6a21\u6df1\u5ea6\u6096\u8bba\uff0c\u80fd\u591f\u5728\u51e0\u5c0f\u65f6\u5185\u5206\u6790\u6d77\u91cf\u6570\u636e\u96c6\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u91ca\u4e25\u8c28\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9a\u6027\u7814\u7a76\u4e2d\u89c4\u6a21\u4e0e\u6df1\u5ea6\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4f7f\u5927\u89c4\u6a21\u5b9a\u6027\u6570\u636e\u5206\u6790\u65e2\u9ad8\u6548\u53c8\u4fdd\u6301\u89e3\u91ca\u529b\u3002", "method": "\u4f7f\u75281536\u7ef4\u5d4c\u5165\u3001\u5c42\u6b21\u805a\u7c7b\u548c\u5e76\u884c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7f16\u7801\uff0c\u57284\u4e07\u5b57\u7b26\u4e2d\u6587\u8bbf\u8c08\u6587\u672c\u4e0a\u6bd4\u8f83NGT\u4e0e\u624b\u52a8\u7f16\u7801\u53caChatGPT\u8f85\u52a9\u5206\u6790\u7684\u6548\u679c\u3002", "result": "NGT\u5b9e\u73b0\u4e86168\u500d\u901f\u5ea6\u63d0\u5347\uff083\u5c0f\u65f6vs 3\u5468\uff09\u3001\u66f4\u9ad8\u8d28\u91cf\uff080.904 vs 0.883\uff09\u548c96%\u6210\u672c\u964d\u4f4e\u3002\u4eba\u673a\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff1a\u7eaf\u81ea\u52a8\u5316\u4ea7\u751f\u62bd\u8c61\u6846\u67b6\uff0c\u800c\u4eba\u5de5\u6307\u5bfc\u4ea7\u751f\u53ef\u64cd\u4f5c\u7684\u53cc\u8def\u5f84\u7406\u8bba\u3002", "conclusion": "NGT\u8bc1\u660e\u8ba1\u7b97\u5ba2\u89c2\u6027\u4e0e\u4eba\u7c7b\u89e3\u91ca\u662f\u4e92\u8865\u7684\u3002\u5411\u91cf\u8868\u793a\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u8bed\u4e49\u6d4b\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u610f\u4e49\u7684\u89e3\u91ca\u7ef4\u5ea6\u3002\u7814\u7a76\u8005\u4ece\u673a\u68b0\u7f16\u7801\u8f6c\u5411\u7406\u8bba\u6307\u5bfc\uff0cAI\u5904\u7406\u6a21\u5f0f\u8bc6\u522b\uff0c\u4eba\u7c7b\u63d0\u4f9b\u521b\u9020\u6027\u6d1e\u89c1\u3002"}}
{"id": "2509.25208", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.25208", "abs": "https://arxiv.org/abs/2509.25208", "authors": ["Zenghui Huang", "Ting Shu", "Zhonglei Wang", "Yang Lu", "Yan Yan", "Wei Zhong", "Hanzi Wang"], "title": "DPSformer: A long-tail-aware model for improving heavy rainfall prediction", "comment": null, "summary": "Accurate and timely forecasting of heavy rainfall remains a critical\nchallenge for modern society. Precipitation exhibits a highly imbalanced\ndistribution: most observations record no or light rain, while heavy rainfall\nevents are rare. Such an imbalanced distribution obstructs deep learning models\nfrom effectively predicting heavy rainfall events. To address this challenge,\nwe treat rainfall forecasting explicitly as a long-tailed learning problem,\nidentifying the insufficient representation of heavy rainfall events as the\nprimary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a\nlong-tail-aware model that enriches representation of heavy rainfall events\nthrough a high-resolution branch. For heavy rainfall events $ \\geq $ 50 mm/6 h,\nDPSformer lifts the Critical Success Index (CSI) of a baseline Numerical\nWeather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of\nheavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing\nexisting methods. Our work establishes an effective long-tailed paradigm for\nheavy rainfall prediction, offering a practical tool to enhance early warning\nsystems and mitigate the societal impacts of extreme weather events.", "AI": {"tldr": "DPSformer\u662f\u4e00\u4e2a\u9488\u5bf9\u66b4\u96e8\u9884\u62a5\u7684\u957f\u5c3e\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u5206\u652f\u589e\u5f3a\u66b4\u96e8\u4e8b\u4ef6\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u66b4\u96e8\u9884\u62a5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u964d\u6c34\u5206\u5e03\u9ad8\u5ea6\u4e0d\u5e73\u8861\uff0c\u5927\u591a\u6570\u89c2\u6d4b\u8bb0\u5f55\u65e0\u96e8\u6216\u5c0f\u96e8\uff0c\u800c\u66b4\u96e8\u4e8b\u4ef6\u7f55\u89c1\u3002\u8fd9\u79cd\u4e0d\u5e73\u8861\u5206\u5e03\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6709\u6548\u9884\u6d4b\u66b4\u96e8\u4e8b\u4ef6\u3002", "method": "\u5c06\u964d\u96e8\u9884\u62a5\u660e\u786e\u89c6\u4e3a\u957f\u5c3e\u5b66\u4e60\u95ee\u9898\uff0c\u5f15\u5165DPSformer\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u5206\u652f\u4e30\u5bcc\u66b4\u96e8\u4e8b\u4ef6\u7684\u8868\u793a\u3002", "result": "\u5bf9\u4e8e\u226550mm/6h\u7684\u66b4\u96e8\u4e8b\u4ef6\uff0cDPSformer\u5c06\u57fa\u51c6\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684CSI\u4ece0.012\u63d0\u5347\u52300.067\uff1b\u5bf9\u4e8e\u524d1%\u7684\u66b4\u96e8\u4e8b\u4ef6\uff0c\u5176FSS\u8d85\u8fc70.45\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u66b4\u96e8\u9884\u6d4b\u5efa\u7acb\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u957f\u5c3e\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u9884\u8b66\u7cfb\u7edf\u548c\u51cf\u8f7b\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u793e\u4f1a\u5f71\u54cd\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.25210", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.25210", "abs": "https://arxiv.org/abs/2509.25210", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Lei Bai"], "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting", "comment": null, "summary": "To gain finer regional forecasts, many works have explored the regional\nintegration from the global atmosphere, e.g., by solving boundary equations in\nphysics-based methods or cropping regions from global forecasts in data-driven\nmethods. However, the effectiveness of these methods is often constrained by\nstatic and imprecise regional boundaries, resulting in poor generalization\nability. To address this issue, we propose Spatial-Temporal Weather Forecasting\n(STCast), a novel AI-driven framework for adaptive regional boundary\noptimization and dynamic monthly forecast allocation. Specifically, our\napproach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns\nglobal and regional spatial distributions to initialize boundaries and\nadaptively refines them based on attention-derived alignment patterns.\nFurthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where\natmospheric variables from distinct months are dynamically routed to\nspecialized experts using a discrete Gaussian distribution, enhancing the\nmodel's ability to capture temporal patterns. Beyond global and regional\nforecasting, we evaluate our STCast on extreme event prediction and ensemble\nforecasting. Experimental results demonstrate consistent superiority over\nstate-of-the-art methods across all four tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86STCast\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u5bf9\u9f50\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u4f18\u5316\u533a\u57df\u8fb9\u754c\uff0c\u5e76\u4f7f\u7528\u65f6\u5e8f\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u52a8\u6001\u5206\u914d\u6708\u5ea6\u9884\u6d4b\uff0c\u5728\u533a\u57df\u5929\u6c14\u9884\u62a5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533a\u57df\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u53d7\u9650\u4e8e\u9759\u6001\u4e0d\u7cbe\u786e\u7684\u533a\u57df\u8fb9\u754c\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u4f18\u5316\u8fb9\u754c\u5e76\u52a8\u6001\u5904\u7406\u65f6\u5e8f\u53d8\u5316\u7684AI\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTCast\u6846\u67b6\uff1a1) \u7a7a\u95f4\u5bf9\u9f50\u6ce8\u610f\u529b\u673a\u5236(SAA) - \u5bf9\u9f50\u5168\u5c40\u548c\u533a\u57df\u7a7a\u95f4\u5206\u5e03\u6765\u521d\u59cb\u5316\u8fb9\u754c\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u81ea\u9002\u5e94\u4f18\u5316\u8fb9\u754c\uff1b2) \u65f6\u5e8f\u4e13\u5bb6\u6df7\u5408\u6a21\u5757(TMoE) - \u4f7f\u7528\u79bb\u6563\u9ad8\u65af\u5206\u5e03\u5c06\u4e0d\u540c\u6708\u4efd\u7684\u5927\u6c14\u53d8\u91cf\u52a8\u6001\u8def\u7531\u5230\u4e13\u95e8\u4e13\u5bb6\uff0c\u589e\u5f3a\u65f6\u5e8f\u6a21\u5f0f\u6355\u6349\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTCast\u5728\u5168\u5c40\u548c\u533a\u57df\u9884\u62a5\u3001\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u548c\u96c6\u5408\u9884\u62a5\u56db\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u6301\u7eed\u7684\u4f18\u52bf\u3002", "conclusion": "STCast\u901a\u8fc7\u81ea\u9002\u5e94\u8fb9\u754c\u4f18\u5316\u548c\u52a8\u6001\u65f6\u5e8f\u5206\u914d\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533a\u57df\u5929\u6c14\u9884\u62a5\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aAI\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25282", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2.4; D.1.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.25282", "abs": "https://arxiv.org/abs/2509.25282", "authors": ["Jiexi Xu", "Jiaqi Liu", "Ran Tong", "Su Liu"], "title": "Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments", "comment": "5 pages, 1 table", "summary": "Large language model (LLM) agents are increasingly capable of orchestrating\ncomplex tasks in low-code environments. However, these agents often exhibit\nhallucinations and logical inconsistencies because their inherent reasoning\nmechanisms rely on probabilistic associations rather than genuine causal\nunderstanding. This paper introduces a new programming paradigm: Causal-Visual\nProgramming (CVP), designed to address this fundamental issue by explicitly\nintroducing causal structures into the workflow design. CVP allows users to\ndefine a simple \"world model\" for workflow modules through an intuitive\nlow-code interface, effectively creating a Directed Acyclic Graph (DAG) that\nexplicitly defines the causal relationships between modules. This causal graph\nacts as a crucial constraint during the agent's reasoning process, anchoring\nits decisions to a user-defined causal structure and significantly reducing\nlogical errors and hallucinations by preventing reliance on spurious\ncorrelations. To validate the effectiveness of CVP, we designed a synthetic\nexperiment that simulates a common real-world problem: a distribution shift\nbetween the training and test environments. Our results show that a causally\nanchored model maintained stable accuracy in the face of this shift, whereas a\npurely associative baseline model that relied on probabilistic correlations\nexperienced a significant performance drop. The primary contributions of this\nstudy are: a formal definition of causal structures for workflow modules; the\nproposal and implementation of a CVP framework that anchors agent reasoning to\na user-defined causal graph; and empirical evidence demonstrating the\nframework's effectiveness in enhancing agent robustness and reducing errors\ncaused by causal confusion in dynamic environments. CVP offers a viable path\ntoward building more interpretable, reliable, and trustworthy AI agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u56e0\u679c\u89c6\u89c9\u7f16\u7a0b(CVP)\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u5de5\u4f5c\u6d41\u4e2d\u5f15\u5165\u56e0\u679c\u7ed3\u6784\u6765\u51cf\u5c11LLM\u4ee3\u7406\u7684\u5e7b\u89c9\u548c\u903b\u8f91\u9519\u8bef\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4f9d\u8d56\u6982\u7387\u5173\u8054\u800c\u975e\u771f\u6b63\u7684\u56e0\u679c\u7406\u89e3\uff0c\u5bfc\u81f4\u5e7b\u89c9\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4f4e\u4ee3\u7801\u754c\u9762\u8ba9\u7528\u6237\u5b9a\u4e49\u5de5\u4f5c\u6d41\u6a21\u5757\u7684\"\u4e16\u754c\u6a21\u578b\"\uff0c\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe(DAG)\u6765\u660e\u786e\u6a21\u5757\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4f5c\u4e3a\u4ee3\u7406\u63a8\u7406\u8fc7\u7a0b\u7684\u7ea6\u675f\u3002", "result": "\u5728\u6a21\u62df\u5206\u5e03\u504f\u79fb\u7684\u5b9e\u9a8c\u4e2d\uff0c\u56e0\u679c\u951a\u5b9a\u6a21\u578b\u4fdd\u6301\u7a33\u5b9a\u51c6\u786e\u7387\uff0c\u800c\u57fa\u4e8e\u6982\u7387\u5173\u8054\u7684\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "CVP\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u3001\u53ef\u9760\u548c\u53ef\u4fe1\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.25649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25649", "abs": "https://arxiv.org/abs/2509.25649", "authors": ["Samar Haider", "Amir Tohidi", "Jenny S. Wang", "Timothy D\u00f6rr", "David M. Rothschild", "Chris Callison-Burch", "Duncan J. Watts"], "title": "The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale", "comment": null, "summary": "Mainstream news organizations shape public perception not only directly\nthrough the articles they publish but also through the choices they make about\nwhich topics to cover (or ignore) and how to frame the issues they do decide to\ncover. However, measuring these subtle forms of media bias at scale remains a\nchallenge. Here, we introduce a large, ongoing (from January 1, 2024 to\npresent), near real-time dataset and computational framework developed to\nenable systematic study of selection and framing bias in news coverage. Our\npipeline integrates large language models (LLMs) with scalable, near-real-time\nnews scraping to extract structured annotations -- including political lean,\ntone, topics, article type, and major events -- across hundreds of articles per\nday. We quantify these dimensions of coverage at multiple levels -- the\nsentence level, the article level, and the publisher level -- expanding the\nways in which researchers can analyze media bias in the modern news landscape.\nIn addition to a curated dataset, we also release an interactive web platform\nfor convenient exploration of these data. Together, these contributions\nestablish a reusable methodology for studying media bias at scale, providing\nempirical resources for future research. Leveraging the breadth of the corpus\nover time and across publishers, we also present some examples (focused on the\n150,000+ articles examined in 2024) that illustrate how this novel data set can\nreveal insightful patterns in news coverage and bias, supporting academic\nresearch and real-world efforts to improve media accountability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u5b9e\u65f6\u7684\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u9009\u62e9\u6027\u548c\u6846\u67b6\u6027\u504f\u89c1\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u65b0\u95fb\u722c\u53d6\u6280\u672f\uff0c\u63d0\u53d6\u7ed3\u6784\u5316\u6ce8\u91ca\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5e73\u53f0\u3002", "motivation": "\u4e3b\u6d41\u65b0\u95fb\u673a\u6784\u901a\u8fc7\u9009\u62e9\u62a5\u9053\u4e3b\u9898\u548c\u6846\u67b6\u8bae\u9898\u6765\u5f71\u54cd\u516c\u4f17\u8ba4\u77e5\uff0c\u4f46\u5927\u89c4\u6a21\u6d4b\u91cf\u8fd9\u4e9b\u5fae\u5999\u504f\u89c1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u65b0\u95fb\u722c\u53d6\u6280\u672f\uff0c\u63d0\u53d6\u653f\u6cbb\u503e\u5411\u3001\u8bed\u8c03\u3001\u4e3b\u9898\u3001\u6587\u7ae0\u7c7b\u578b\u548c\u91cd\u5927\u4e8b\u4ef6\u7b49\u7ed3\u6784\u5316\u6ce8\u91ca\uff0c\u6bcf\u5929\u5904\u7406\u6570\u767e\u7bc7\u6587\u7ae0\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b15\u4e07+\u6587\u7ae0\u7684\u8bed\u6599\u5e93\uff0c\u5728\u53e5\u5b50\u3001\u6587\u7ae0\u548c\u53d1\u5e03\u8005\u5c42\u9762\u91cf\u5316\u62a5\u9053\u7ef4\u5ea6\uff0c\u63ed\u793a\u4e86\u65b0\u95fb\u8986\u76d6\u548c\u504f\u89c1\u7684\u6df1\u523b\u6a21\u5f0f\u3002", "conclusion": "\u4e3a\u5927\u89c4\u6a21\u7814\u7a76\u5a92\u4f53\u504f\u89c1\u5efa\u7acb\u4e86\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u8bba\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u5b9e\u8bc1\u8d44\u6e90\uff0c\u652f\u6301\u5b66\u672f\u7814\u7a76\u548c\u63d0\u9ad8\u5a92\u4f53\u95ee\u8d23\u5236\u7684\u5b9e\u9645\u52aa\u529b\u3002"}}
{"id": "2509.25673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25673", "abs": "https://arxiv.org/abs/2509.25673", "authors": ["Dianqing Liu", "Yi Liu", "Guoqing Jin", "Zhendong Mao"], "title": "Mitigating Biases in Language Models via Bias Unlearning", "comment": "EMNLP 2025 MainConference", "summary": "Many studies have shown various biases targeting different demographic groups\nin language models, amplifying discrimination and harming fairness. Recent\nparameter modification debiasing approaches significantly degrade core\ncapabilities such as text coherence and task accuracy. And Prompt-based\ndebiasing methods, only effective for predefined trigger words, fail to address\ndeeply embedded stereotypical associations in model parameters. In this paper,\nwe propose BiasUnlearn, a novel model debiasing framework which achieves\ntargeted debiasing via dual-pathway unlearning mechanisms coordinating\nstereotype forgetting with anti-stereotype retention, while preventing bias\npolarity reversal through adversarial forget set and dynamic dataset swapping.\nWe conducted extensive experiments with multiple language models across various\nevaluation benchmarks. The results show that BiasUnlearn outperforms existing\nmethods in mitigating bias in language models while retaining language modeling\ncapabilities. Further experiments reveal that debiasing weights are\ntransferable across model variants, confirming that bias representations become\nentrenched during pre-training and persist through fine-tuning phases.", "AI": {"tldr": "BiasUnlearn\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u9057\u5fd8\u673a\u5236\u534f\u8c03\u523b\u677f\u5370\u8c61\u9057\u5fd8\u4e0e\u53cd\u523b\u677f\u5370\u8c61\u4fdd\u7559\uff0c\u540c\u65f6\u901a\u8fc7\u5bf9\u6297\u6027\u9057\u5fd8\u96c6\u548c\u52a8\u6001\u6570\u636e\u96c6\u4ea4\u6362\u9632\u6b62\u504f\u89c1\u6781\u6027\u53cd\u8f6c\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u4fee\u6539\u53bb\u504f\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u6587\u672c\u8fde\u8d2f\u6027\u548c\u4efb\u52a1\u51c6\u786e\u6027\u7b49\u6838\u5fc3\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u63d0\u793a\u7684\u53bb\u504f\u65b9\u6cd5\u4ec5\u5bf9\u9884\u5b9a\u4e49\u89e6\u53d1\u8bcd\u6709\u6548\uff0c\u65e0\u6cd5\u89e3\u51b3\u6a21\u578b\u53c2\u6570\u4e2d\u6df1\u5ea6\u5d4c\u5165\u7684\u523b\u677f\u5370\u8c61\u5173\u8054\u3002", "method": "\u63d0\u51faBiasUnlearn\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u9057\u5fd8\u673a\u5236\u534f\u8c03\u523b\u677f\u5370\u8c61\u9057\u5fd8\u4e0e\u53cd\u523b\u677f\u5370\u8c61\u4fdd\u7559\uff0c\u4f7f\u7528\u5bf9\u6297\u6027\u9057\u5fd8\u96c6\u548c\u52a8\u6001\u6570\u636e\u96c6\u4ea4\u6362\u9632\u6b62\u504f\u89c1\u6781\u6027\u53cd\u8f6c\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u5404\u79cd\u8bc4\u4f30\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBiasUnlearn\u5728\u51cf\u8f7b\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u540c\u65f6\u4fdd\u7559\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u53bb\u504f\u6743\u91cd\u53ef\u5728\u6a21\u578b\u53d8\u4f53\u95f4\u8fc1\u79fb\uff0c\u8bc1\u5b9e\u504f\u89c1\u8868\u793a\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u56fa\u5316\u5e76\u5728\u5fae\u8c03\u9636\u6bb5\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "BiasUnlearn\u6846\u67b6\u80fd\u6709\u6548\u51cf\u8f7b\u8bed\u8a00\u6a21\u578b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u53bb\u504f\u6743\u91cd\u5177\u6709\u53ef\u8fc1\u79fb\u6027\uff0c\u8868\u660e\u504f\u89c1\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5c31\u5df2\u56fa\u5316\u3002"}}
{"id": "2509.25999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25999", "abs": "https://arxiv.org/abs/2509.25999", "authors": ["Yann de Mont-Marin", "Louis Montaut", "Jean Ponce", "Martial Hebert", "Justin Carpentier"], "title": "On the Conic Complementarity of Planar Contacts", "comment": null, "summary": "We present a unifying theoretical result that connects two foundational\nprinciples in robotics: the Signorini law for point contacts, which underpins\nmany simulation methods for preventing object interpenetration, and the center\nof pressure (also known as the zero-moment point), a key concept used in, for\ninstance, optimization-based locomotion control. Our contribution is the planar\nSignorini condition, a conic complementarity formulation that models general\nplanar contacts between rigid bodies. We prove that this formulation is\nequivalent to enforcing the punctual Signorini law across an entire contact\nsurface, thereby bridging the gap between discrete and continuous contact\nmodels. A geometric interpretation reveals that the framework naturally\ncaptures three physical regimes -sticking, separating, and tilting-within a\nunified complementarity structure. This leads to a principled extension of the\nclassical center of pressure, which we refer to as the extended center of\npressure. By establishing this connection, our work provides a mathematically\nconsistent and computationally tractable foundation for handling planar\ncontacts, with implications for both the accurate simulation of contact\ndynamics and the design of advanced control and optimization algorithms in\nlocomotion and manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e73\u9762Signorini\u6761\u4ef6\uff0c\u5c06\u70b9\u63a5\u89e6\u7684Signorini\u5b9a\u5f8b\u4e0e\u8d28\u5fc3\u538b\u529b\u6982\u5ff5\u7edf\u4e00\u8d77\u6765\uff0c\u5efa\u7acb\u4e86\u79bb\u6563\u4e0e\u8fde\u7eed\u63a5\u89e6\u6a21\u578b\u4e4b\u95f4\u7684\u6865\u6881\u3002", "motivation": "\u8fde\u63a5\u673a\u5668\u4eba\u5b66\u4e2d\u4e24\u4e2a\u57fa\u7840\u539f\u7406\uff1a\u9632\u6b62\u7269\u4f53\u7a7f\u900f\u7684Signorini\u5b9a\u5f8b\u548c\u7528\u4e8e\u4f18\u5316\u63a7\u5236\u7684\u91cd\u5fc3\u538b\u529b\u6982\u5ff5\uff0c\u586b\u8865\u79bb\u6563\u4e0e\u8fde\u7eed\u63a5\u89e6\u6a21\u578b\u4e4b\u95f4\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5e73\u9762Signorini\u6761\u4ef6\u7684\u9525\u4e92\u8865\u516c\u5f0f\uff0c\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8e\u5728\u6574\u4e2a\u63a5\u89e6\u9762\u4e0a\u5f3a\u5236\u6267\u884c\u70b9\u72b6Signorini\u5b9a\u5f8b\uff0c\u5e76\u63d0\u4f9b\u51e0\u4f55\u89e3\u91ca\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u4e92\u8865\u7ed3\u6784\uff0c\u81ea\u7136\u6355\u6349\u7c98\u9644\u3001\u5206\u79bb\u548c\u503e\u659c\u4e09\u79cd\u7269\u7406\u72b6\u6001\uff0c\u5e76\u6269\u5c55\u4e86\u7ecf\u5178\u91cd\u5fc3\u538b\u529b\u6982\u5ff5\u3002", "conclusion": "\u4e3a\u5904\u7406\u5e73\u9762\u63a5\u89e6\u63d0\u4f9b\u4e86\u6570\u5b66\u4e00\u81f4\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5bf9\u63a5\u89e6\u52a8\u529b\u5b66\u6a21\u62df\u548c\u63a7\u5236\u7b97\u6cd5\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.25654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25654", "abs": "https://arxiv.org/abs/2509.25654", "authors": ["Kaiyu Li", "Zixuan Jiang", "Xiangyong Cao", "Jiayu Wang", "Yuchen Xiao", "Deyu Meng", "Zhi Wang"], "title": "DescribeEarth: Describe Anything for Remote Sensing Images", "comment": null, "summary": "Automated textual description of remote sensing images is crucial for\nunlocking their full potential in diverse applications, from environmental\nmonitoring to urban planning and disaster management. However, existing studies\nin remote sensing image captioning primarily focus on the image level, lacking\nobject-level fine-grained interpretation, which prevents the full utilization\nand transformation of the rich semantic and structural information contained in\nremote sensing images. To address this limitation, we propose Geo-DLC, a novel\ntask of object-level fine-grained image captioning for remote sensing. To\nsupport this task, we construct DE-Dataset, a large-scale dataset contains 25\ncategories and 261,806 annotated instances with detailed descriptions of object\nattributes, relationships, and contexts. Furthermore, we introduce\nDE-Benchmark, a LLM-assisted question-answering based evaluation suite designed\nto systematically measure model capabilities on the Geo-DLC task. We also\npresent DescribeEarth, a Multi-modal Large Language Model (MLLM) architecture\nexplicitly designed for Geo-DLC, which integrates a scale-adaptive focal\nstrategy and a domain-guided fusion module leveraging remote sensing\nvision-language model features to encode high-resolution details and remote\nsensing category priors while maintaining global context. Our DescribeEarth\nmodel consistently outperforms state-of-the-art general MLLMs on DE-Benchmark,\ndemonstrating superior factual accuracy, descriptive richness, and grammatical\nsoundness, particularly in capturing intrinsic object features and surrounding\nenvironmental attributes across simple, complex, and even out-of-distribution\nremote sensing scenarios. All data, code and weights are released at\nhttps://github.com/earth-insights/DescribeEarth.", "AI": {"tldr": "\u63d0\u51fa\u4e86Geo-DLC\u4efb\u52a1\uff0c\u5373\u9065\u611f\u56fe\u50cf\u7684\u5bf9\u8c61\u7ea7\u7ec6\u7c92\u5ea6\u56fe\u50cf\u63cf\u8ff0\uff0c\u5e76\u6784\u5efa\u4e86DE-Dataset\u6570\u636e\u96c6\u548cDE-Benchmark\u8bc4\u4f30\u5957\u4ef6\uff0c\u5f00\u53d1\u4e86DescribeEarth\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u7ea7\u522b\uff0c\u7f3a\u4e4f\u5bf9\u8c61\u7ea7\u7ec6\u7c92\u5ea6\u89e3\u91ca\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u9065\u611f\u56fe\u50cf\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u6784\u5efaDE-Dataset\u6570\u636e\u96c6\uff0825\u4e2a\u7c7b\u522b\uff0c261,806\u4e2a\u6807\u6ce8\u5b9e\u4f8b\uff09\uff0c\u5f15\u5165DE-Benchmark\u8bc4\u4f30\u5957\u4ef6\uff0c\u63d0\u51faDescribeEarth MLLM\u67b6\u6784\uff0c\u96c6\u6210\u5c3a\u5ea6\u81ea\u9002\u5e94\u805a\u7126\u7b56\u7565\u548c\u9886\u57df\u5f15\u5bfc\u878d\u5408\u6a21\u5757\u3002", "result": "DescribeEarth\u6a21\u578b\u5728DE-Benchmark\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u901a\u7528MLLMs\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u63cf\u8ff0\u4e30\u5bcc\u5ea6\u548c\u8bed\u6cd5\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u5185\u5728\u5bf9\u8c61\u7279\u5f81\u548c\u5468\u56f4\u73af\u5883\u5c5e\u6027\u65b9\u9762\u3002", "conclusion": "Geo-DLC\u4efb\u52a1\u548cDescribeEarth\u6a21\u578b\u4e3a\u9065\u611f\u56fe\u50cf\u7684\u5bf9\u8c61\u7ea7\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63cf\u8ff0\u8d28\u91cf\u3002"}}
{"id": "2509.26082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26082", "abs": "https://arxiv.org/abs/2509.26082", "authors": ["Tianyi Jin", "Melya Boukheddimi", "Rohit Kumar", "Gabriele Fadini", "Frank Kirchner"], "title": "Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance", "comment": null, "summary": "Humanoid robots have seen significant advancements in both design and\ncontrol, with a growing emphasis on integrating these aspects to enhance\noverall performance. Traditionally, robot design has followed a sequential\nprocess, where control algorithms are developed after the hardware is\nfinalized. However, this can be myopic and prevent robots to fully exploit\ntheir hardware capabilities. Recent approaches advocate for co-design,\noptimizing both design and control in parallel to maximize robotic\ncapabilities. This paper presents the Evolutionary Continuous Adaptive RL-based\nCo-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with\nevolutionary strategies to enable continuous adaptation of the control policy\nto the hardware. EA-CoRL comprises two key components: Design Evolution, which\nexplores the hardware choices using an evolutionary algorithm to identify\nefficient configurations, and Policy Continuous Adaptation, which fine-tunes a\ntask-specific control policy across evolving designs to maximize performance\nrewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and\ncontrol policy of the RH5 humanoid for a highly dynamic chin-up task,\npreviously unfeasible due to actuator limitations. Comparative results against\nstate-of-the-art RL-based co-design methods show that EA-CoRL achieves higher\nfitness score and broader design space exploration, highlighting the critical\nrole of continuous policy adaptation in robot co-design.", "AI": {"tldr": "\u63d0\u51fa\u4e86EA-CoRL\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u8fdb\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u786c\u4ef6\u548c\u63a7\u5236\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u7684\u52a8\u6001\u5f15\u4f53\u5411\u4e0a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8bbe\u8ba1\u91c7\u7528\u987a\u5e8f\u6d41\u7a0b\uff0c\u786c\u4ef6\u786e\u5b9a\u540e\u518d\u5f00\u53d1\u63a7\u5236\u7b97\u6cd5\uff0c\u8fd9\u9650\u5236\u4e86\u786c\u4ef6\u80fd\u529b\u7684\u5145\u5206\u5229\u7528\u3002\u9700\u8981\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u6765\u540c\u65f6\u4f18\u5316\u8bbe\u8ba1\u548c\u63a7\u5236\u3002", "method": "EA-CoRL\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8bbe\u8ba1\u8fdb\u5316\uff08\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u63a2\u7d22\u786c\u4ef6\u914d\u7f6e\uff09\u548c\u7b56\u7565\u8fde\u7eed\u9002\u5e94\uff08\u8de8\u8bbe\u8ba1\u6f14\u5316\u5fae\u8c03\u63a7\u5236\u7b56\u7565\uff09\u3002", "result": "\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u7684\u52a8\u6001\u5f15\u4f53\u5411\u4e0a\u4efb\u52a1\u4e2d\uff0cEA-CoRL\u76f8\u6bd4\u6700\u5148\u8fdb\u7684RL\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u9002\u5e94\u5ea6\u5206\u6570\u548c\u66f4\u5e7f\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u3002", "conclusion": "\u7b56\u7565\u8fde\u7eed\u9002\u5e94\u5728\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0cEA-CoRL\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u786c\u4ef6\u548c\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u534f\u540c\u4f18\u5316\u3002"}}
{"id": "2509.25263", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25263", "abs": "https://arxiv.org/abs/2509.25263", "authors": ["Yifang Zhang", "Pengfei Duan", "Henan Wang", "Shengwu Xiong"], "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data", "comment": "11 pages,8 figures", "summary": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.", "AI": {"tldr": "\u63d0\u51fa\u4e86RainfallBench\u57fa\u51c6\uff0c\u7528\u4e8e0-3\u5c0f\u65f6\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\uff0c\u5305\u542b5\u5e74\u5168\u740312000\u591a\u4e2aGNSS\u7ad9\u70b9\u7684\u6c14\u8c61\u6570\u636e\uff0c\u7279\u522b\u5305\u542b\u53ef\u964d\u6c34\u91cf\u6c34\u6c7d(PWV)\u6307\u6807\u3002\u9488\u5bf9\u964d\u96e8\u6570\u636e\u7684\u96f6\u81a8\u80c0\u3001\u65f6\u95f4\u8870\u51cf\u548c\u975e\u5e73\u7a33\u6027\u7279\u70b9\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86Bi-Focus Precipitation Forecaster (BFPF)\u6a21\u5757\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6c14\u8c61\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u5177\u6709\u5f3a\u5468\u671f\u6027\u7684\u53d8\u91cf(\u5982\u6e29\u5ea6\u3001\u6e7f\u5ea6)\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u66f4\u590d\u6742\u548c\u5b9e\u9645\u7684\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\u5177\u6709\u96f6\u81a8\u80c0\u3001\u65f6\u95f4\u8870\u51cf\u548c\u975e\u5e73\u7a33\u6027\u7b49\u6311\u6218\u6027\u7279\u5f81\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86RainfallBench\u6570\u636e\u96c6\uff0c\u5305\u542b5\u5e7415\u5206\u949f\u95f4\u9694\u76846\u4e2a\u5173\u952e\u6c14\u8c61\u53d8\u91cf\u6570\u636e\uff0c\u7279\u522b\u52a0\u5165\u4e86\u53ef\u964d\u6c34\u91cf\u6c34\u6c7d(PWV)\u3002\u8bbe\u8ba1\u4e86\u9488\u5bf9\u591a\u5c3a\u5ea6\u9884\u6d4b\u548c\u6781\u7aef\u964d\u96e8\u4e8b\u4ef6\u7684\u4e13\u95e8\u8bc4\u4f30\u7b56\u7565\u3002\u63d0\u51fa\u4e86Bi-Focus Precipitation Forecaster (BFPF)\u6a21\u5757\uff0c\u901a\u8fc7\u878d\u5165\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\u6765\u89e3\u51b3\u96f6\u81a8\u80c0\u548c\u65f6\u95f4\u8870\u51cf\u95ee\u9898\u3002", "result": "\u5728RainfallBench\u4e0a\u8bc4\u4f30\u4e8620\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u6db5\u76d66\u79cd\u4e3b\u8981\u67b6\u6784\u3002\u7edf\u8ba1\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u5168\u9762\u6027\u548c\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002BFPF\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u964d\u96e8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "RainfallBench\u586b\u8865\u4e86\u964d\u96e8\u4e34\u8fd1\u9884\u62a5\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u590d\u6742\u6c14\u8c61\u573a\u666f\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u3002BFPF\u6a21\u5757\u901a\u8fc7\u878d\u5165\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u964d\u96e8\u6570\u636e\u7279\u6709\u7684\u96f6\u81a8\u80c0\u548c\u65f6\u95f4\u8870\u51cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.25268", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "68T07"], "pdf": "https://arxiv.org/pdf/2509.25268", "abs": "https://arxiv.org/abs/2509.25268", "authors": ["Cristian Bodnar", "Rapha\u00ebl Rousseau-Rizzi", "Nikhil Shankar", "James Merleau", "Stylianos Flampouris", "Guillem Candille", "Slavica Antic", "Fran\u00e7ois Miralles", "Jayesh K. Gupta"], "title": "A Weather Foundation Model for the Power Grid", "comment": "31 pages, 22 figures", "summary": "Weather foundation models (WFMs) have recently set new benchmarks in global\nforecast skill, yet their concrete value for the weather-sensitive\ninfrastructure that powers modern society remains largely unexplored. In this\nstudy, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting\nTransformer (GFT), on a rich archive of Hydro-Qu\\'ebec asset\nobservations--including transmission-line weather stations, wind-farm met-mast\nstreams, and icing sensors--to deliver hyper-local, asset-level forecasts for\nfive grid-critical variables: surface temperature, precipitation, hub-height\nwind speed, wind-turbine icing risk, and rime-ice accretion on overhead\nconductors. Across 6-72 h lead times, the tailored model surpasses\nstate-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)\nby 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.\nMost importantly, it attains an average precision score of 0.72 for day-ahead\nrime-ice detection, a capability absent from existing operational systems,\nwhich affords several hours of actionable warning for potentially catastrophic\noutage events. These results show that WFMs, when post-trained with small\namounts of high-fidelity, can serve as a practical foundation for\nnext-generation grid-resilience intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u5929\u6c14\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4e3a\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u8d85\u672c\u5730\u5316\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u5173\u952e\u6c14\u8c61\u53d8\u91cf\u4e0a\u8d85\u8d8a\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\uff0c\u7279\u522b\u662f\u5b9e\u73b0\u4e86\u8986\u51b0\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u7535\u7f51\u97e7\u6027\u63d0\u4f9b\u667a\u80fd\u652f\u6301\u3002", "motivation": "\u5929\u6c14\u57fa\u7840\u6a21\u578b\u5728\u5929\u6c14\u9884\u62a5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u73b0\u4ee3\u793e\u4f1a\u5929\u6c14\u654f\u611f\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u9645\u4ef7\u503c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e9b\u6a21\u578b\u5728\u7535\u7f51\u5173\u952e\u6c14\u8c61\u9884\u6d4b\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u9b41\u5317\u514b\u6c34\u7535\u516c\u53f8\u8d44\u4ea7\u89c2\u6d4b\u6570\u636e\u5bf9Silurian AI\u768415\u4ebf\u53c2\u6570\u751f\u6210\u5f0f\u9884\u62a5\u53d8\u6362\u5668\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u9884\u6d4b\u4e94\u4e2a\u7535\u7f51\u5173\u952e\u53d8\u91cf\uff1a\u5730\u8868\u6e29\u5ea6\u3001\u964d\u6c34\u3001\u8f6e\u6bc2\u9ad8\u5ea6\u98ce\u901f\u3001\u98ce\u673a\u8986\u51b0\u98ce\u9669\u548c\u67b6\u7a7a\u5bfc\u7ebf\u8986\u51b0\u79ef\u805a\u3002", "result": "\u57286-72\u5c0f\u65f6\u9884\u62a5\u65f6\u6548\u5185\uff0c\u5b9a\u5236\u6a21\u578b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\u57fa\u51c6\uff1a\u6e29\u5ea6\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e15%\uff0c\u603b\u964d\u6c34\u8bef\u5dee\u964d\u4f4e35%\uff0c\u98ce\u901f\u8bef\u5dee\u964d\u4f4e15%\uff0c\u5e76\u5b9e\u73b0\u4e86\u8986\u51b0\u68c0\u6d4b0.72\u7684\u5e73\u5747\u7cbe\u5ea6\u5f97\u5206\u3002", "conclusion": "\u5929\u6c14\u57fa\u7840\u6a21\u578b\u7ecf\u8fc7\u5c11\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\u540e\u8bad\u7ec3\u540e\uff0c\u53ef\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u7535\u7f51\u97e7\u6027\u667a\u80fd\u7684\u5b9e\u7528\u57fa\u7840\uff0c\u63d0\u4f9b\u4f20\u7edf\u7cfb\u7edf\u4e0d\u5177\u5907\u7684\u8986\u51b0\u9884\u8b66\u80fd\u529b\u3002"}}
{"id": "2509.25740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25740", "abs": "https://arxiv.org/abs/2509.25740", "authors": ["Xinyu Pu", "Hongsong Wang", "Jie Gui", "Pan Zhou"], "title": "Dragging with Geometry: From Pixels to Geometry-Guided Image Editing", "comment": null, "summary": "Interactive point-based image editing serves as a controllable editor,\nenabling precise and flexible manipulation of image content. However, most\ndrag-based methods operate primarily on the 2D pixel plane with limited use of\n3D cues. As a result, they often produce imprecise and inconsistent edits,\nparticularly in geometry-intensive scenarios such as rotations and perspective\ntransformations. To address these limitations, we propose a novel\ngeometry-guided drag-based image editing method - GeoDrag, which addresses\nthree key challenges: 1) incorporating 3D geometric cues into pixel-level\nediting, 2) mitigating discontinuities caused by geometry-only guidance, and 3)\nresolving conflicts arising from multi-point dragging. Built upon a unified\ndisplacement field that jointly encodes 3D geometry and 2D spatial priors,\nGeoDrag enables coherent, high-fidelity, and structure-consistent editing in a\nsingle forward pass. In addition, a conflict-free partitioning strategy is\nintroduced to isolate editing regions, effectively preventing interference and\nensuring consistency. Extensive experiments across various editing scenarios\nvalidate the effectiveness of our method, showing superior precision,\nstructural consistency, and reliable multi-point editability. The code will be\navailable on https://github.com/xinyu-pu/GeoDrag .", "AI": {"tldr": "GeoDrag\u662f\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u5f15\u5bfc\u7684\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u54083D\u51e0\u4f55\u7ebf\u7d22\u548c2D\u7a7a\u95f4\u5148\u9a8c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf2D\u50cf\u7d20\u5e73\u9762\u7f16\u8f91\u5728\u51e0\u4f55\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u7684\u4e0d\u7cbe\u786e\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u62d6\u62fd\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u57282D\u50cf\u7d20\u5e73\u9762\u4e0a\u64cd\u4f5c\uff0c\u7f3a\u4e4f3D\u51e0\u4f55\u7ebf\u7d22\uff0c\u5bfc\u81f4\u5728\u65cb\u8f6c\u548c\u900f\u89c6\u53d8\u6362\u7b49\u51e0\u4f55\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u4ea7\u751f\u4e0d\u7cbe\u786e\u548c\u4e0d\u4e00\u81f4\u7684\u7f16\u8f91\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4f4d\u79fb\u573a\u8054\u5408\u7f16\u78013D\u51e0\u4f55\u548c2D\u7a7a\u95f4\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u65e0\u51b2\u7a81\u5206\u533a\u7b56\u7565\u6765\u9694\u79bb\u7f16\u8f91\u533a\u57df\uff0c\u9632\u6b62\u5e72\u6270\u5e76\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5404\u79cd\u7f16\u8f91\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u53ef\u9760\u7684\u591a\u70b9\u7f16\u8f91\u80fd\u529b\u3002", "conclusion": "GeoDrag\u80fd\u591f\u5b9e\u73b0\u8fde\u8d2f\u3001\u9ad8\u4fdd\u771f\u548c\u7ed3\u6784\u4e00\u81f4\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b8c\u6210\uff0c\u89e3\u51b3\u4e86\u51e0\u4f55\u5f15\u5bfc\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.25591", "categories": ["cs.AI", "cs.CL", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2509.25591", "abs": "https://arxiv.org/abs/2509.25591", "authors": ["Zekai Chen", "Arda Pekis", "Kevin Brown"], "title": "Building the EHR Foundation Model via Next Event Prediction", "comment": null, "summary": "Electronic Health Records (EHRs) contain rich temporal dynamics that\nconventional encoding approaches fail to adequately capture. While Large\nLanguage Models (LLMs) show promise for EHR modeling, they struggle to reason\nabout sequential clinical events and temporal dependencies. We propose Next\nEvent Prediction (NEP), a framework that enhances LLMs' temporal reasoning\nthrough autoregressive fine-tuning on clinical event sequences. By\nreformulating EHRs as timestamped event chains and predicting future medical\nevents, NEP explicitly models disease progression patterns and causal\nrelationships. Extensive evaluations across oncology survival prediction and\nclinical diagnosis tasks demonstrate NEP's superiority, outperforming\nspecialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index\nin temporal reasoning tasks. Our analyses reveal dual benefits:\nstate-of-the-art prediction accuracy combined with clinically interpretable\nattention patterns that align with known disease pathways.", "AI": {"tldr": "\u63d0\u51fa\u4e86Next Event Prediction (NEP)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5fae\u8c03\u589e\u5f3aLLM\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u5728\u80bf\u7624\u751f\u5b58\u9884\u6d4b\u548c\u4e34\u5e8a\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7f16\u7801\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u4e30\u5bcc\u65f6\u5e8f\u52a8\u6001\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4e34\u5e8a\u4e8b\u4ef6\u5e8f\u5217\u548c\u65f6\u5e8f\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e26\u65f6\u95f4\u6233\u7684\u4e8b\u4ef6\u94fe\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5fae\u8c03\u9884\u6d4b\u672a\u6765\u533b\u7597\u4e8b\u4ef6\uff0c\u663e\u5f0f\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\u6a21\u5f0f\u548c\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cNEP\u5728AUROC\u4e0a\u4f18\u4e8e\u4e13\u4e1aEHR\u6a21\u578b4.6%\uff0c\u5728C-index\u4e0a\u4f18\u4e8e\u901a\u7528LLM 7.2%\uff0c\u540c\u65f6\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "conclusion": "NEP\u6846\u67b6\u6210\u529f\u589e\u5f3a\u4e86LLM\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u5728\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0e\u5df2\u77e5\u75be\u75c5\u901a\u8def\u4e00\u81f4\u7684\u4e34\u5e8a\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.25379", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25379", "abs": "https://arxiv.org/abs/2509.25379", "authors": ["Yogesh Verma", "Markus Heinonen", "Vikas Garg"], "title": "Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation", "comment": null, "summary": "Protein structure prediction and folding are fundamental to understanding\nbiology, with recent deep learning advances reshaping the field.\nDiffusion-based generative models have revolutionized protein design, enabling\nthe creation of novel proteins. However, these methods often neglect the\nintrinsic physical realism of proteins, driven by noising dynamics that lack\ngrounding in physical principles. To address this, we first introduce a\nphysically motivated non-linear noising process, grounded in classical physics,\nthat unfolds proteins into secondary structures (e.g., alpha helices, linear\nbeta sheets) while preserving topological integrity--maintaining bonds, and\npreventing collisions. We then integrate this process with the flow-matching\nparadigm on SE(3) to model the invariant distribution of protein backbones with\nhigh fidelity, incorporating sequence information to enable\nsequence-conditioned folding and expand the generative capabilities of our\nmodel. Experimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in unconditional protein generation, producing\nmore designable and novel protein structures while accurately folding monomer\nsequences into precise protein conformations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u975e\u7ebf\u6027\u566a\u58f0\u8fc7\u7a0b\uff0c\u7ed3\u5408SE(3)\u6d41\u5339\u914d\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u751f\u6210\u548c\u5e8f\u5217\u6761\u4ef6\u6298\u53e0\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\u5ffd\u7565\u4e86\u86cb\u767d\u8d28\u7684\u7269\u7406\u771f\u5b9e\u6027\uff0c\u7f3a\u4e4f\u7269\u7406\u539f\u7406\u652f\u6491\u7684\u566a\u58f0\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u7ecf\u5178\u7269\u7406\u7684\u975e\u7ebf\u6027\u566a\u58f0\u8fc7\u7a0b\uff0c\u5c06\u86cb\u767d\u8d28\u5c55\u5f00\u4e3a\u4e8c\u7ea7\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u62d3\u6251\u5b8c\u6574\u6027\uff1b\u7ed3\u5408SE(3)\u6d41\u5339\u914d\u8303\u5f0f\u5efa\u6a21\u86cb\u767d\u8d28\u9aa8\u67b6\u7684\u4e0d\u53d8\u5206\u5e03\u3002", "result": "\u5728\u65e0\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ea7\u751f\u66f4\u5177\u8bbe\u8ba1\u6027\u548c\u65b0\u9896\u6027\u7684\u86cb\u767d\u8d28\u7ed3\u6784\uff0c\u5e76\u80fd\u51c6\u786e\u5c06\u5355\u4f53\u5e8f\u5217\u6298\u53e0\u4e3a\u7cbe\u786e\u6784\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u566a\u58f0\u8fc7\u7a0b\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u751f\u6210\u7684\u8d28\u91cf\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002"}}
{"id": "2509.25381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25381", "abs": "https://arxiv.org/abs/2509.25381", "authors": ["Penglei Gao", "Yan Zou", "Abhijit Duggal", "Shuaiqi Huang", "Faming Liang", "Xiaofeng Wang"], "title": "Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation", "comment": null, "summary": "We introduce the Functional Competing Risk Net (FCRN), a unified\ndeep-learning framework for discrete-time survival analysis under competing\nrisks, which seamlessly integrates functional covariates and handles missing\ndata within an end-to-end model. By combining a micro-network Basis Layer for\nfunctional data representation with a gradient-based imputation module, FCRN\nsimultaneously learns to impute missing values and predict event-specific\nhazards. Evaluated on multiple simulated datasets and a real-world ICU case\nstudy using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates\nsubstantial improvements in prediction accuracy over random survival forests\nand traditional competing risks models. This approach advances prognostic\nmodeling in critical care by more effectively capturing dynamic risk factors\nand static predictors while accommodating irregular and incomplete data.", "AI": {"tldr": "\u63d0\u51faFCRN\u6846\u67b6\uff0c\u7528\u4e8e\u7ade\u4e89\u98ce\u9669\u4e0b\u7684\u79bb\u6563\u65f6\u95f4\u751f\u5b58\u5206\u6790\uff0c\u6574\u5408\u529f\u80fd\u534f\u53d8\u91cf\u5e76\u5904\u7406\u7f3a\u5931\u6570\u636e\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6a21\u578b\u540c\u65f6\u5b66\u4e60\u586b\u8865\u7f3a\u5931\u503c\u548c\u9884\u6d4b\u4e8b\u4ef6\u7279\u5b9a\u98ce\u9669\u3002", "motivation": "\u5728\u91cd\u75c7\u76d1\u62a4\u7b49\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5904\u7406\u529f\u80fd\u534f\u53d8\u91cf\uff08\u5982\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff09\u548c\u7f3a\u5931\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u590d\u6742\u56e0\u7d20\u3002", "method": "\u7ed3\u5408\u7528\u4e8e\u529f\u80fd\u6570\u636e\u8868\u793a\u7684\u5fae\u7f51\u7edc\u57fa\u7840\u5c42\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u586b\u8865\u6a21\u5757\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u548c\u771f\u5b9eICU\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u968f\u673a\u751f\u5b58\u68ee\u6797\u548c\u4f20\u7edf\u7ade\u4e89\u98ce\u9669\u6a21\u578b\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FCRN\u901a\u8fc7\u66f4\u6709\u6548\u5730\u6355\u6349\u52a8\u6001\u98ce\u9669\u56e0\u7d20\u548c\u9759\u6001\u9884\u6d4b\u56e0\u5b50\uff0c\u540c\u65f6\u9002\u5e94\u4e0d\u89c4\u5219\u548c\u4e0d\u5b8c\u6574\u6570\u636e\uff0c\u63a8\u8fdb\u4e86\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u9884\u540e\u5efa\u6a21\u3002"}}
{"id": "2509.25382", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25382", "abs": "https://arxiv.org/abs/2509.25382", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n"], "title": "On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study", "comment": "Argentine Congress of Embedded Systems (2025)", "summary": "In this work, we explore the latent space of a denoising variational\nautoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on\ngravitational wave data from event GW150914. To evaluate how well the model\ncaptures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw\nposterior samples conditioned on clean inputs, and compare them to the\nencoder's outputs from noisy data. Although the model reconstructs signals\naccurately, statistical comparisons reveal a clear mismatch in the latent\nspace. This shows that strong denoising performance doesn't necessarily mean\nthe latent representations are reliable highlighting the importance of using\nposterior-based validation when evaluating generative models.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728GW150914\u5f15\u529b\u6ce2\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6df7\u5408\u9ad8\u65af\u5148\u9a8c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE-MoG\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u901a\u8fc7\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\u91c7\u6837\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6a21\u578b\u80fd\u51c6\u786e\u91cd\u6784\u4fe1\u53f7\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u5b58\u5728\u660e\u663e\u4e0d\u5339\u914d\uff0c\u8868\u660e\u5f3a\u53bb\u566a\u6027\u80fd\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u6f5c\u5728\u8868\u793a\u53ef\u9760\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5728\u6355\u83b7\u6570\u636e\u5e95\u5c42\u7ed3\u6784\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u53bb\u566a\u6027\u80fd\u4e0e\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u9ad8\u65af\u5148\u9a8c\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE-MoG\uff09\u5904\u7406\u5f15\u529b\u6ce2\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u91c7\u6837\u6765\u6bd4\u8f83\u5e72\u51c0\u8f93\u5165\u6761\u4ef6\u4e0b\u7684\u540e\u9a8c\u6837\u672c\u4e0e\u566a\u58f0\u6570\u636e\u7f16\u7801\u5668\u8f93\u51fa\u7684\u5dee\u5f02\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u91cd\u6784\u4fe1\u53f7\uff0c\u4f46\u7edf\u8ba1\u6bd4\u8f83\u663e\u793a\u6f5c\u5728\u7a7a\u95f4\u5b58\u5728\u660e\u663e\u4e0d\u5339\u914d\uff0c\u8868\u660e\u53bb\u566a\u6027\u80fd\u5f3a\u5e76\u4e0d\u4fdd\u8bc1\u6f5c\u5728\u8868\u793a\u53ef\u9760\u3002", "conclusion": "\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u57fa\u4e8e\u540e\u9a8c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4ec5\u51ed\u91cd\u6784\u7cbe\u5ea6\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6f5c\u5728\u7a7a\u95f4\u7684\u8d28\u91cf\u3002"}}
{"id": "2509.25794", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25794", "abs": "https://arxiv.org/abs/2509.25794", "authors": ["Haotian Xue", "Yunhao Ge", "Yu Zeng", "Zhaoshuo Li", "Ming-Yu Liu", "Yongxin Chen", "Jiaojiao Fan"], "title": "Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated impressive world knowledge\nacross a wide range of tasks, making them promising candidates for embodied\nreasoning applications. However, existing benchmarks primarily evaluate the\nembodied reasoning ability of VLMs through multiple-choice questions based on\nimage annotations -- for example, selecting which trajectory better describes\nan event in the image. In this work, we introduce the Point-It-Out (PIO)\nbenchmark, a novel benchmark designed to systematically assess the embodied\nreasoning abilities of VLMs through precise visual grounding. We propose a\nhierarchical evaluation protocol spanning three stages (S1: referred-object\nlocalization, S2: task-driven pointing, and S3: visual trace prediction), with\ndata collected from critical domains for embodied intelligence, including\nindoor, kitchen, driving, and robotic manipulation scenarios. Extensive\nexperiments with over ten state-of-the-art VLMs reveal several interesting\nfindings. For example, strong general-purpose models such as GPT-4o, while\nexcelling on many benchmarks (e.g., language, perception, and reasoning),\nunderperform compared to some open-source models in precise visual grounding;\nmodels such as MoLMO perform well in S1 and S2 but struggle in S3, where\nrequires grounding combined with visual trace planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Point-It-Out (PIO)\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7cbe\u786e\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b\u4e09\u4e2a\u5c42\u6b21\u9636\u6bb5\uff1a\u6307\u79f0\u5bf9\u8c61\u5b9a\u4f4d\u3001\u4efb\u52a1\u9a71\u52a8\u6307\u5411\u548c\u89c6\u89c9\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u901a\u8fc7\u57fa\u4e8e\u56fe\u50cf\u6807\u6ce8\u7684\u591a\u9009\u9898\u8bc4\u4f30VLMs\u7684\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u7cbe\u786e\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1aS1\u6307\u79f0\u5bf9\u8c61\u5b9a\u4f4d\u3001S2\u4efb\u52a1\u9a71\u52a8\u6307\u5411\u3001S3\u89c6\u89c9\u8f68\u8ff9\u9884\u6d4b\uff0c\u6570\u636e\u6765\u81ea\u5ba4\u5185\u3001\u53a8\u623f\u3001\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u5177\u8eab\u667a\u80fd\u5173\u952e\u9886\u57df\u3002", "result": "\u5bf910\u591a\u4e2a\u6700\u5148\u8fdbVLM\u7684\u5b9e\u9a8c\u53d1\u73b0\uff1aGPT-4o\u7b49\u901a\u7528\u6a21\u578b\u5728\u7cbe\u786e\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4e0d\u5982\u67d0\u4e9b\u5f00\u6e90\u6a21\u578b\uff1bMoLMO\u5728S1\u548cS2\u8868\u73b0\u826f\u597d\u4f46\u5728\u9700\u8981\u89c6\u89c9\u8f68\u8ff9\u89c4\u5212\u7684S3\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "PIO\u57fa\u51c6\u63ed\u793a\u4e86VLMs\u5728\u5177\u8eab\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u7cbe\u786e\u89c6\u89c9\u5b9a\u4f4d\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.25693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25693", "abs": "https://arxiv.org/abs/2509.25693", "authors": ["N. de Silva", "S. Perera", "K. L. A. A. Nimasha", "I. D. S. Fernando", "R. K. A. O. Wijerathne"], "title": "ScheduleMe: Multi-Agent Calendar Assistant", "comment": null, "summary": "Recent advancements in LLMs have contributed to the rise of advanced\nconversational assistants that can assist with user needs through natural\nlanguage conversation. This paper presents a ScheduleMe, a multi-agent calendar\nassistant for users to manage google calendar events in natural language. The\nsystem uses a graph-structured coordination mechanism where a central\nsupervisory agent supervises specialized task agents, allowing modularity,\nconflicts resolution, and context-aware interactions to resolve ambiguities and\nevaluate user commands. This approach sets an example of how structured\nreasoning and agent cooperation might convince operators to increase the\nusability and flexibility of personal calendar assistant tools.", "AI": {"tldr": "ScheduleMe\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u65e5\u5386\u52a9\u624b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7ba1\u7406Google\u65e5\u5386\u4e8b\u4ef6\uff0c\u91c7\u7528\u56fe\u7ed3\u6784\u534f\u8c03\u673a\u5236\u5b9e\u73b0\u6a21\u5757\u5316\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u4ea4\u4e92\u3002", "motivation": "\u968f\u7740LLMs\u7684\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u5bf9\u8bdd\u52a9\u624b\u6765\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6ee1\u8db3\u7528\u6237\u9700\u6c42\uff0c\u7279\u522b\u662f\u7ba1\u7406\u65e5\u5386\u4e8b\u4ef6\u65b9\u9762\u3002", "method": "\u4f7f\u7528\u56fe\u7ed3\u6784\u534f\u8c03\u673a\u5236\uff0c\u4e2d\u592e\u76d1\u7763\u4ee3\u7406\u76d1\u7763\u4e13\u95e8\u7684\u4efb\u52a1\u4ee3\u7406\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u4ea4\u4e92\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u89e3\u6790\u7528\u6237\u547d\u4ee4\u4e2d\u7684\u6b67\u4e49\u5e76\u8bc4\u4f30\u7528\u6237\u6307\u4ee4\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u548c\u4ee3\u7406\u534f\u4f5c\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u63d0\u5347\u4e2a\u4eba\u65e5\u5386\u52a9\u624b\u5de5\u5177\u7684\u53ef\u7528\u6027\u548c\u7075\u6d3b\u6027\u63d0\u4f9b\u4e86\u8303\u4f8b\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u548c\u4ee3\u7406\u534f\u4f5c\u5982\u4f55\u8bf4\u670d\u64cd\u4f5c\u8005\u91c7\u7528\u66f4\u5148\u8fdb\u7684\u5de5\u5177\u3002"}}
{"id": "2509.25851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25851", "abs": "https://arxiv.org/abs/2509.25851", "authors": ["Jundong Xu", "Hao Fei", "Yuhui Zhang", "Liangming Pan", "Qijun Huang", "Qian Liu", "Preslav Nakov", "Min-Yen Kan", "William Yang Wang", "Mong-Li Lee", "Wynne Hsu"], "title": "MuSLR: Multimodal Symbolic Logical Reasoning", "comment": "Accepted by NeurIPS 2025", "summary": "Multimodal symbolic logical reasoning, which aims to deduce new facts from\nmultimodal input via formal logic, is critical in high-stakes applications such\nas autonomous driving and medical diagnosis, as its rigorous, deterministic\nreasoning helps prevent serious consequences. To evaluate such capabilities of\ncurrent state-of-the-art vision language models (VLMs), we introduce the first\nbenchmark MuSLR for multimodal symbolic logical reasoning grounded in formal\nlogical rules. MuSLR comprises 1,093 instances across 7 domains, including 35\natomic symbolic logic and 976 logical combinations, with reasoning depths\nranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that\nthey all struggle with multimodal symbolic reasoning, with the best model,\nGPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework\nthat applies formal logical rules to multimodal inputs, boosting GPT-4.1's\nChain-of-Thought performance by 14.13%, and delivering even larger gains on\ncomplex logics such as first-order logic. We also conduct a comprehensive error\nanalysis, showing that around 70% of failures stem from logical misalignment\nbetween modalities, offering key insights to guide future improvements. All\ndata and code are publicly available at https://llm-symbol.github.io/MuSLR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u6a21\u6001\u7b26\u53f7\u903b\u8f91\u63a8\u7406\u57fa\u51c6MuSLR\uff0c\u5305\u542b1,093\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d67\u4e2a\u9886\u57df\u3002\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u6a21\u578bGPT-4.1\u4ec5\u8fbe\u523046.8%\u3002\u63d0\u51fa\u4e86LogiCAM\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u7b26\u53f7\u903b\u8f91\u63a8\u7406\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u8bca\u65ad\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e25\u8c28\u7684\u786e\u5b9a\u6027\u63a8\u7406\u6709\u52a9\u4e8e\u9632\u6b62\u4e25\u91cd\u540e\u679c\u3002\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86MuSLR\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,093\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d67\u4e2a\u9886\u57df\u300135\u79cd\u539f\u5b50\u7b26\u53f7\u903b\u8f91\u548c976\u79cd\u903b\u8f91\u7ec4\u5408\u3002\u63d0\u51fa\u4e86LogiCAM\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u5f62\u5f0f\u903b\u8f91\u89c4\u5219\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u8f93\u5165\u3002", "result": "\u8bc4\u4f30\u4e867\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u90fd\u5728\u591a\u6a21\u6001\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cGPT-4.1\u6700\u4f73\u4f46\u4ec5\u8fbe\u523046.8%\u3002LogiCAM\u6846\u67b6\u5c06GPT-4.1\u7684\u601d\u7ef4\u94fe\u6027\u80fd\u63d0\u5347\u4e8614.13%\uff0c\u5728\u590d\u6742\u903b\u8f91\u5982\u4e00\u9636\u903b\u8f91\u4e0a\u63d0\u5347\u66f4\u5927\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u7ea670%\u7684\u5931\u8d25\u6e90\u4e8e\u6a21\u6001\u95f4\u7684\u903b\u8f91\u4e0d\u5bf9\u9f50\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u7b26\u53f7\u903b\u8f91\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0cLogiCAM\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002\u9519\u8bef\u5206\u6790\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2509.25438", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25438", "abs": "https://arxiv.org/abs/2509.25438", "authors": ["Zhibo Hou", "Zhiyu An", "Wan Du"], "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring", "comment": null, "summary": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b66\u4e60\u8fdb\u5ea6\u76d1\u63a7\uff08LPM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u800c\u975e\u9884\u6d4b\u8bef\u5dee\u6216\u65b0\u9896\u6027\u6765\u6307\u5bfc\u63a2\u7d22\uff0c\u6709\u6548\u907f\u514d\u667a\u80fd\u4f53\u88ab\u73af\u5883\u4e2d\u7684\u4e0d\u53ef\u5b66\u4e60\u968f\u673a\u566a\u58f0\u6e90\uff08noisy-TV\uff09\u56f0\u4f4f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5185\u5728\u5956\u52b1\u7684\u63a2\u7d22\u65b9\u6cd5\u5728\u9762\u5bf9\u73af\u5883\u4e2d\u7684\u4e0d\u53ef\u5b66\u4e60\u968f\u673a\u566a\u58f0\u6e90\u65f6\uff0c\u667a\u80fd\u4f53\u4f1a\u88ab\u8fd9\u4e9b\u566a\u58f0\u56f0\u4f4f\u800c\u65e0\u6cd5\u6709\u6548\u63a2\u7d22\u3002\u867d\u7136\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6216\u5206\u5e03\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u6700\u7ec8\u80fd\u9003\u8131\uff0c\u4f46\u6837\u672c\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u4eba\u7c7b\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4f1a\u76d1\u63a7\u81ea\u8eab\u8fdb\u6b65\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86LPM\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u8bbe\u8ba1\uff1a\u4e00\u4e2a\u8bef\u5dee\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u6a21\u578b\u5728\u5148\u524d\u8fed\u4ee3\u4e2d\u7684\u9884\u671f\u9884\u6d4b\u8bef\u5dee\uff0c\u4f7f\u7528\u5f53\u524d\u8fed\u4ee3\u4e0e\u5148\u524d\u8fed\u4ee3\u6a21\u578b\u8bef\u5dee\u7684\u5dee\u5f02\u6765\u6307\u5bfc\u63a2\u7d22\u3002LPM\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u800c\u975e\u9884\u6d4b\u8bef\u5dee\u6216\u65b0\u9896\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eLPM\u7684\u5185\u5728\u5956\u52b1\u5177\u6709\u96f6\u7b49\u53d8\u6027\uff0c\u662f\u4fe1\u606f\u589e\u76ca\u7684\u5355\u8c03\u6307\u6807\u3002\u5728\u57fa\u4e8eMNIST\u30013D\u8ff7\u5bab\u548cAtari\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0cLPM\u7684\u5185\u5728\u5956\u52b1\u6536\u655b\u66f4\u5feb\uff0c\u5728\u8ff7\u5bab\u5b9e\u9a8c\u4e2d\u63a2\u7d22\u66f4\u591a\u72b6\u6001\uff0c\u5728Atari\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u5916\u5728\u5956\u52b1\u3002", "conclusion": "LPM\u6982\u5ff5\u7b80\u5355\u4f46\u6709\u6548\uff0c\u6807\u5fd7\u7740\u566a\u58f0\u9c81\u68d2\u63a2\u7d22\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u901a\u8fc7\u76d1\u63a7\u5b66\u4e60\u8fdb\u5ea6\u800c\u975e\u4f20\u7edf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u3002"}}
{"id": "2509.25792", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25792", "abs": "https://arxiv.org/abs/2509.25792", "authors": ["Alexander Branch", "Omead Pooladzandi", "Radin Khosraviani", "Sunay Gajanan Bhat", "Jeffrey Jiang", "Gregory Pottie"], "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks", "comment": null, "summary": "We introduce PureVQ-GAN, a defense against data poisoning that forces\nbackdoor triggers through a discrete bottleneck using Vector-Quantized VAE with\nGAN discriminator. By quantizing poisoned images through a learned codebook,\nPureVQ-GAN destroys fine-grained trigger patterns while preserving semantic\ncontent. A GAN discriminator ensures outputs match the natural image\ndistribution, preventing reconstruction of out-of-distribution perturbations.\nOn CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient\nMatching and Bullseye Polytope attacks, and 1.64% against Narcissus while\nmaintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring\nhundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making\nit practical for real training pipelines.", "AI": {"tldr": "PureVQ-GAN\u662f\u4e00\u79cd\u9632\u5fa1\u6570\u636e\u4e2d\u6bd2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316VAE\u548cGAN\u9274\u522b\u5668\u6765\u7834\u574f\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u9632\u5fa1\u65b9\u6cd5\u9700\u8981\u6570\u767e\u6b21\u8fed\u4ee3\u6b65\u9aa4\uff0c\u901f\u5ea6\u6162\u4e14\u4e0d\u5b9e\u7528\u3002\u9700\u8981\u4e00\u79cd\u5feb\u901f\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u6765\u5bf9\u6297\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u3002", "method": "\u4f7f\u7528\u5411\u91cf\u91cf\u5316VAE\uff08VQ-VAE\uff09\u548cGAN\u9274\u522b\u5668\uff0c\u901a\u8fc7\u79bb\u6563\u74f6\u9888\u5f3a\u5236\u5904\u7406\u4e2d\u6bd2\u56fe\u50cf\uff0c\u7834\u574f\u7ec6\u7c92\u5ea6\u89e6\u53d1\u5668\u6a21\u5f0f\uff0c\u540c\u65f6GAN\u786e\u4fdd\u8f93\u51fa\u7b26\u5408\u81ea\u7136\u56fe\u50cf\u5206\u5e03\u3002", "result": "\u5728CIFAR-10\u4e0a\uff0c\u5bf9Gradient Matching\u548cBullseye Polytope\u653b\u51fb\u5b9e\u73b00%\u4e2d\u6bd2\u6210\u529f\u7387\uff0c\u5bf9Narcissus\u653b\u51fb\u4e3a1.64%\uff0c\u540c\u65f6\u4fdd\u630191-95%\u7684\u5e72\u51c0\u51c6\u786e\u7387\u3002\u901f\u5ea6\u6bd4\u6269\u6563\u65b9\u6cd5\u5feb50\u500d\u4ee5\u4e0a\u3002", "conclusion": "PureVQ-GAN\u662f\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u6570\u636e\u4e2d\u6bd2\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9632\u5fa1\u901f\u5ea6\u3002"}}
{"id": "2509.25843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25843", "abs": "https://arxiv.org/abs/2509.25843", "authors": ["Yein Park", "Jungwoo Park", "Jaewoo Kang"], "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack", "comment": null, "summary": "Large language models (LLMs), despite being safety-aligned, exhibit brittle\nrefusal behaviors that can be circumvented by simple linguistic changes. As\ntense jailbreaking demonstrates that models refusing harmful requests often\ncomply when rephrased in past tense, a critical generalization gap is revealed\nin current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,\nmechanistically-informed framework that surgically mitigates this specific\nvulnerability. For the first step, we use circuit analysis to identify the\nspecific attention heads causally linked to the targeted jailbreaking, the\ntense-changing attack. Second, we train a precise, channel-wise scaling vector\nto recalibrate the activation of tense vulnerable heads. Lastly, we apply it\ninto a \"preventative fine-tuning\", forcing the model to learn a more robust\nrefusal mechanism. Across three LLMs, ASGuard effectively reduces the attack\nsuccess rate of targeted jailbreaking while preserving general capabilities and\nminimizing over refusal, achieving a Pareto-optimal balance between safety and\nutility. Our findings underscore how adversarial suffixes suppress the\npropagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can\nbe leveraged to develop practical, efficient, and targeted methods for\nadjusting model behavior, charting a course for more reliable and interpretable\nAI safety.", "AI": {"tldr": "ASGuard\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5236\u7406\u89e3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u4e0e\u65f6\u6001\u8d8a\u72f1\u653b\u51fb\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\uff0c\u8bad\u7ec3\u7cbe\u786e\u7684\u7f29\u653e\u5411\u91cf\u6765\u91cd\u65b0\u6821\u51c6\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u9884\u9632\u6027\u5fae\u8c03\u589e\u5f3a\u6a21\u578b\u7684\u5b89\u5168\u62d2\u7edd\u673a\u5236\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u5b58\u5728\u8106\u5f31\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u7b80\u5355\u7684\u8bed\u8a00\u53d8\u5316\uff08\u5982\u4f7f\u7528\u8fc7\u53bb\u65f6\u6001\uff09\u5c31\u80fd\u7ed5\u8fc7\u5176\u5b89\u5168\u673a\u5236\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u7684\u6cdb\u5316\u5dee\u8ddd\u3002", "method": "1. \u4f7f\u7528\u7535\u8def\u5206\u6790\u8bc6\u522b\u4e0e\u65f6\u6001\u8d8a\u72f1\u653b\u51fb\u76f8\u5173\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff1b2. \u8bad\u7ec3\u901a\u9053\u7ea7\u7f29\u653e\u5411\u91cf\u6765\u91cd\u65b0\u6821\u51c6\u8fd9\u4e9b\u5934\u7684\u6fc0\u6d3b\uff1b3. \u5e94\u7528\u9884\u9632\u6027\u5fae\u8c03\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u62d2\u7edd\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2aLLM\u4e0a\uff0cASGuard\u6709\u6548\u964d\u4f4e\u4e86\u76ee\u6807\u8d8a\u72f1\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u5e76\u6700\u5c0f\u5316\u4e86\u8fc7\u5ea6\u62d2\u7edd\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u5bf9\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u6765\u5f00\u53d1\u5b9e\u7528\u3001\u9ad8\u6548\u548c\u6709\u9488\u5bf9\u6027\u7684\u65b9\u6cd5\u8c03\u6574\u6a21\u578b\u884c\u4e3a\uff0c\u4e3a\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684AI\u5b89\u5168\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.25538", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25538", "abs": "https://arxiv.org/abs/2509.25538", "authors": ["Marcus Schwarting", "Logan Ward", "Nathaniel Hudson", "Xiaoli Yan", "Ben Blaiszik", "Santanu Chaudhuri", "Eliu Huerta", "Ian Foster"], "title": "Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization", "comment": null, "summary": "Generative AI poses both opportunities and risks for solving inverse design\nproblems in the sciences. Generative tools provide the ability to expand and\nrefine a search space autonomously, but do so at the cost of exploring\nlow-quality regions until sufficiently fine tuned. Here, we propose a queue\nprioritization algorithm that combines generative modeling and active learning\nin the context of a distributed workflow for exploring complex design spaces.\nWe find that incorporating an active learning model to prioritize top design\ncandidates can prevent a generative AI workflow from expending resources on\nnonsensical candidates and halt potential generative model decay. For an\nexisting generative AI workflow for discovering novel molecular structure\ncandidates for carbon capture, our active learning approach significantly\nincreases the number of high-quality candidates identified by the generative\nmodel. We find that, out of 1000 novel candidates, our workflow without active\nlearning can generate an average of 281 high-performing candidates, while our\nproposed prioritization with active learning can generate an average 604\nhigh-performing candidates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5efa\u6a21\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u961f\u5217\u4f18\u5148\u7ea7\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u4e2d\u7684\u9006\u5411\u8bbe\u8ba1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u8d28\u91cf\u5019\u9009\u5206\u5b50\u7684\u751f\u6210\u6570\u91cf\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u89e3\u51b3\u79d1\u5b66\u9006\u5411\u8bbe\u8ba1\u95ee\u9898\u65f6\u65e2\u5e26\u6765\u673a\u9047\u4e5f\u5e26\u6765\u98ce\u9669\uff0c\u867d\u7136\u80fd\u81ea\u4e3b\u6269\u5c55\u548c\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\uff0c\u4f46\u4f1a\u5728\u5145\u5206\u8c03\u4f18\u524d\u63a2\u7d22\u4f4e\u8d28\u91cf\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u961f\u5217\u4f18\u5148\u7ea7\u7b97\u6cd5\uff0c\u5c06\u751f\u6210\u5efa\u6a21\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7ed3\u5408\u5728\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\u4e2d\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u6a21\u578b\u4f18\u5148\u5904\u7406\u9876\u7ea7\u8bbe\u8ba1\u5019\u9009\uff0c\u9632\u6b62\u8d44\u6e90\u6d6a\u8d39\u5728\u65e0\u610f\u4e49\u7684\u5019\u9009\u4e0a\u3002", "result": "\u5728\u78b3\u6355\u83b7\u5206\u5b50\u7ed3\u6784\u53d1\u73b0\u4efb\u52a1\u4e2d\uff0c\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u8d28\u91cf\u5019\u9009\u5206\u5b50\u7684\u6570\u91cf\uff1a\u65e0\u4e3b\u52a8\u5b66\u4e60\u5e73\u5747\u751f\u6210281\u4e2a\u9ad8\u6027\u80fd\u5019\u9009\uff0c\u6709\u4e3b\u52a8\u5b66\u4e60\u5e73\u5747\u751f\u6210604\u4e2a\u9ad8\u6027\u80fd\u5019\u9009\u3002", "conclusion": "\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u7684\u751f\u6210AI\u5de5\u4f5c\u6d41\u80fd\u6709\u6548\u9632\u6b62\u751f\u6210\u6a21\u578b\u8870\u51cf\uff0c\u663e\u8457\u63d0\u9ad8\u9ad8\u8d28\u91cf\u8bbe\u8ba1\u5019\u9009\u7684\u8bc6\u522b\u6548\u7387\u3002"}}
{"id": "2509.25596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25596", "abs": "https://arxiv.org/abs/2509.25596", "authors": ["Lucia Quirke", "Stepan Shabalin", "Nora Belrose"], "title": "Binary Sparse Coding for Interpretability", "comment": null, "summary": "Sparse autoencoders (SAEs) are used to decompose neural network activations\ninto sparsely activating features, but many SAE features are only interpretable\nat high activation strengths. To address this issue we propose to use binary\nsparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all\nactivations to be zero or one. We find that binarisation significantly improves\nthe interpretability and monosemanticity of the discovered features, while\nincreasing reconstruction error. By eliminating the distinction between high\nand low activation strengths, we prevent uninterpretable information from being\nsmuggled in through the continuous variation in feature activations. However,\nwe also find that binarisation increases the number of uninterpretable\nultra-high frequency features, and when interpretability scores are\nfrequency-adjusted, the scores for continuous sparse coders are slightly better\nthan those of binary ones. This suggests that polysemanticity may be an\nineliminable property of neural activations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e8c\u5143\u7a00\u758f\u81ea\u7f16\u7801\u5668(BAE)\u548c\u4e8c\u5143\u8f6c\u7801\u5668(BTC)\uff0c\u901a\u8fc7\u5c06\u6fc0\u6d3b\u503c\u9650\u5236\u4e3a0\u62161\u6765\u63d0\u5347\u7279\u5f81\u7684\u5355\u4e00\u8bed\u4e49\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u91cd\u5efa\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7684\u8bb8\u591a\u7279\u5f81\u53ea\u6709\u5728\u9ad8\u6fc0\u6d3b\u5f3a\u5ea6\u4e0b\u624d\u53ef\u89e3\u91ca\uff0c\u5e0c\u671b\u901a\u8fc7\u4e8c\u5143\u5316\u6d88\u9664\u901a\u8fc7\u8fde\u7eed\u7279\u5f81\u6fc0\u6d3b\u53d8\u5316\u4f20\u9012\u7684\u4e0d\u53ef\u89e3\u91ca\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u7a00\u758f\u81ea\u7f16\u7801\u5668(BAE)\u548c\u4e8c\u5143\u8f6c\u7801\u5668(BTC)\uff0c\u5c06\u6240\u6709\u6fc0\u6d3b\u503c\u7ea6\u675f\u4e3a0\u62161\uff0c\u6d88\u9664\u8fde\u7eed\u6fc0\u6d3b\u5f3a\u5ea6\u7684\u53d8\u5316\u3002", "result": "\u4e8c\u5143\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u53d1\u73b0\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5355\u4e00\u8bed\u4e49\u6027\uff0c\u4f46\u589e\u52a0\u4e86\u91cd\u5efa\u8bef\u5dee\u3002\u540c\u65f6\u53d1\u73b0\u4e8c\u5143\u5316\u4f1a\u589e\u52a0\u4e0d\u53ef\u89e3\u91ca\u7684\u8d85\u9ad8\u9891\u7279\u5f81\u6570\u91cf\u3002", "conclusion": "\u5f53\u53ef\u89e3\u91ca\u6027\u5206\u6570\u7ecf\u8fc7\u9891\u7387\u8c03\u6574\u540e\uff0c\u8fde\u7eed\u7a00\u758f\u7f16\u7801\u5668\u7684\u5f97\u5206\u7565\u4f18\u4e8e\u4e8c\u5143\u7f16\u7801\u5668\uff0c\u8fd9\u8868\u660e\u591a\u8bed\u4e49\u6027\u53ef\u80fd\u662f\u795e\u7ecf\u6fc0\u6d3b\u7684\u4e0d\u53ef\u6d88\u9664\u5c5e\u6027\u3002"}}
{"id": "2509.26461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26461", "abs": "https://arxiv.org/abs/2509.26461", "authors": ["Yuyang Cheng", "Linyue Cai", "Changwei Peng", "Yumiao Xu", "Rongfang Bie", "Yong Zhao"], "title": "CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine", "comment": null, "summary": "We present CreAgentive, an agent workflow driven multi-category creative\ngeneration engine that addresses four key limitations of contemporary large\nlanguage models in writing stories, drama and other categories of creatives:\nrestricted genre diversity, insufficient output length, weak narrative\ncoherence, and inability to enforce complex structural constructs. At its core,\nCreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge\ngraph-based narrative representation that decouples story logic from stylistic\nrealization by encoding characters, events, and environments as semantic\ntriples. CreAgentive engages a three-stage agent workflow that comprises: an\nInitialization Stage that constructs a user-specified narrative skeleton; a\nGeneration Stage in which long- and short-term objectives guide multi-agent\ndialogues to instantiate the Story Prototype; a Writing Stage that leverages\nthis prototype to produce multi-genre text with advanced structures such as\nretrospection and foreshadowing. This architecture reduces storage redundancy\nand overcomes the typical bottlenecks of long-form generation. In extensive\nexperiments, CreAgentive generates thousands of chapters with stable quality\nand low cost (less than $1 per 100 chapters) using a general-purpose backbone\nmodel. To evaluate performance, we define a two-dimensional framework with 10\nnarrative indicators measuring both quality and length. Results show that\nCreAgentive consistently outperforms strong baselines and achieves robust\nperformance across diverse genres, approaching the quality of human-authored\nnovels.", "AI": {"tldr": "CreAgentive\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u521b\u610f\u751f\u6210\u5f15\u64ce\uff0c\u901a\u8fc7\u6545\u4e8b\u539f\u578b\u548c\u4e09\u7ea7\u4ee3\u7406\u5de5\u4f5c\u6d41\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u5199\u4f5c\u4e2d\u7684\u56db\u5927\u9650\u5236\uff1a\u4f53\u88c1\u591a\u6837\u6027\u4e0d\u8db3\u3001\u8f93\u51fa\u957f\u5ea6\u4e0d\u591f\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u5f31\u3001\u65e0\u6cd5\u6267\u884c\u590d\u6742\u7ed3\u6784\u3002", "motivation": "\u89e3\u51b3\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u5199\u4f5c\u4e2d\u7684\u5173\u952e\u9650\u5236\uff1a\u4f53\u88c1\u591a\u6837\u6027\u53d7\u9650\u3001\u8f93\u51fa\u957f\u5ea6\u4e0d\u8db3\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u5f31\u3001\u65e0\u6cd5\u5f3a\u5236\u6267\u884c\u590d\u6742\u7ed3\u6784\u6784\u9020\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u6545\u4e8b\u539f\u578b\u4f5c\u4e3a\u4f53\u88c1\u65e0\u5173\u7684\u53d9\u4e8b\u8868\u793a\uff0c\u901a\u8fc7\u4e09\u7ea7\u4ee3\u7406\u5de5\u4f5c\u6d41\uff1a\u521d\u59cb\u5316\u9636\u6bb5\u6784\u5efa\u7528\u6237\u6307\u5b9a\u7684\u53d9\u4e8b\u9aa8\u67b6\uff0c\u751f\u6210\u9636\u6bb5\u901a\u8fc7\u957f\u77ed\u76ee\u6807\u6307\u5bfc\u591a\u4ee3\u7406\u5bf9\u8bdd\u5b9e\u4f8b\u5316\u6545\u4e8b\u539f\u578b\uff0c\u5199\u4f5c\u9636\u6bb5\u5229\u7528\u539f\u578b\u751f\u6210\u5177\u6709\u9ad8\u7ea7\u7ed3\u6784\u7684\u591a\u4f53\u88c1\u6587\u672c\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cCreAgentive\u4ee5\u7a33\u5b9a\u8d28\u91cf\u548c\u4f4e\u6210\u672c\uff08\u6bcf100\u7ae0\u4f4e\u4e8e1\u7f8e\u5143\uff09\u751f\u6210\u4e86\u6570\u5343\u7ae0\u5185\u5bb9\u3002\u5728\u5305\u542b10\u4e2a\u53d9\u4e8b\u6307\u6807\u7684\u53cc\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u4e0d\u540c\u4f53\u88c1\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u63a5\u8fd1\u4eba\u7c7b\u521b\u4f5c\u5c0f\u8bf4\u7684\u8d28\u91cf\u3002", "conclusion": "CreAgentive\u901a\u8fc7\u6545\u4e8b\u539f\u578b\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u672c\u751f\u6210\u7684\u74f6\u9888\uff0c\u51cf\u5c11\u4e86\u5b58\u50a8\u5197\u4f59\uff0c\u5728\u521b\u610f\u5199\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u6210\u672c\u7684\u591a\u4f53\u88c1\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2509.25958", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25958", "abs": "https://arxiv.org/abs/2509.25958", "authors": ["Gang Li", "Yulei Qin", "Xiaoyu Tan", "Dingkang Yang", "Yuchen Shi", "Zihan Xu", "Xiang Li", "Xing Sun", "Ke Li"], "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in\neliciting complex reasoning in large language models (LLMs). However, standard\nRLVR training often leads to excessively verbose processes (in reasoning tasks)\nand inefficient exploration trajectories (in agentic settings), as outcome-only\nrewards provide no incentive for efficiency and the high variance in response\nlength within relatively small rollout groups results in noisy optimization\nsignals. To address this, we propose Rollout Response Recomposition (RoRecomp),\na plug-and-play method that guides models toward concise reasoning by\nstrategically recomposing the training data. RoRecomp separates responses into\ntwo distinct batch types: 1) priority batches, which combine short-correct and\nlong-incorrect responses selected from online batches to provide a clear\ngradient signal for brevity, and 2) compensation batches, which utilize\nremaining responses from a replay buffer to maintain stability and prevent\nmodel collapse. To comprehensively evaluate effectiveness, we test RoRecomp\nacross three settings where results demonstrate substantial efficiency gains:\nreducing reasoning length by 27.7% in zero RL training, reducing unnecessary\ntool calls by 46.8% while improving accuracy in agentic RL, and achieving up to\n52.5% length reduction in thinking compression, all with minimal performance\nimpact.", "AI": {"tldr": "\u63d0\u51faRoRecomp\u65b9\u6cd5\u89e3\u51b3RLVR\u8bad\u7ec3\u4e2d\u63a8\u7406\u8fc7\u7a0b\u5197\u957f\u548c\u63a2\u7d22\u8f68\u8ff9\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u91cd\u7ec4\u8bad\u7ec3\u6570\u636e\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u7b80\u6d01\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u6807\u51c6RLVR\u8bad\u7ec3\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u8fc7\u5ea6\u5197\u957f\u548c\u63a2\u7d22\u8f68\u8ff9\u4f4e\u6548\uff0c\u56e0\u4e3a\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u4e0d\u6fc0\u52b1\u6548\u7387\uff0c\u4e14\u5c0f\u6279\u91cf\u5185\u54cd\u5e94\u957f\u5ea6\u7684\u9ad8\u65b9\u5dee\u9020\u6210\u4f18\u5316\u4fe1\u53f7\u566a\u58f0\u3002", "method": "RoRecomp\u5c06\u54cd\u5e94\u5206\u4e3a\u4e24\u7c7b\u6279\u6b21\uff1a\u4f18\u5148\u6279\u6b21\uff08\u7ed3\u5408\u77ed\u6b63\u786e\u548c\u957f\u9519\u8bef\u54cd\u5e94\u63d0\u4f9b\u7b80\u6d01\u68af\u5ea6\u4fe1\u53f7\uff09\u548c\u8865\u507f\u6279\u6b21\uff08\u4f7f\u7528\u56de\u653e\u7f13\u51b2\u533a\u5269\u4f59\u54cd\u5e94\u4fdd\u6301\u7a33\u5b9a\u6027\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff1a\u96f6RL\u8bad\u7ec3\u4e2d\u63a8\u7406\u957f\u5ea6\u51cf\u5c1127.7%\uff0c\u667a\u80fd\u4f53RL\u4e2d\u4e0d\u5fc5\u8981\u5de5\u5177\u8c03\u7528\u51cf\u5c1146.8%\u4e14\u51c6\u786e\u7387\u63d0\u5347\uff0c\u601d\u7ef4\u538b\u7f29\u4e2d\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe52.5%\uff0c\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "RoRecomp\u662f\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u7b80\u6d01\u63a8\u7406\u3002"}}
{"id": "2509.26520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26520", "abs": "https://arxiv.org/abs/2509.26520", "authors": ["Yaoxiang Wang", "Qingguo Hu", "Yucheng Ding", "Ruizhe Wang", "Yeyun Gong", "Jian Jiao", "Yelong Shen", "Peng Cheng", "Jinsong Su"], "title": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently\nscaling large language models without a proportional increase in computational\ncost. However, the standard training strategy of Top-K router prevents MoE\nmodels from realizing their full potential for elastic inference. When the\nnumber of activated experts is altered at inference time, these models exhibit\nprecipitous performance degradation. In this work, we introduce Matryoshka MoE\n(M-MoE), a training framework that instills a coarse-to-fine structure directly\ninto the expert ensemble. By systematically varying the number of activated\nexperts during training, M-MoE compels the model to learn a meaningful ranking:\ntop-ranked experts collaborate to provide essential, coarse-grained\ncapabilities, while subsequent experts add progressively finer-grained detail.\nWe explore this principle at multiple granularities, identifying a layer-wise\nrandomization strategy as the most effective. Our experiments demonstrate that\na single M-MoE model achieves remarkable elasticity, with its performance at\nvarious expert counts closely matching that of an entire suite of specialist\nmodels, but at only a fraction of the total training cost. This flexibility not\nonly unlocks elastic inference but also enables optimizing performance by\nallocating different computational budgets to different model layers. Our work\npaves the way for more practical and adaptable deployments of large-scale MoE\nmodels.", "AI": {"tldr": "M-MoE\u8bad\u7ec3\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u8bad\u7ec3\u671f\u95f4\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\uff0c\u5728\u4e13\u5bb6\u96c6\u5408\u4e2d\u6784\u5efa\u4ece\u7c97\u5230\u7ec6\u7684\u7ed3\u6784\uff0c\u4f7f\u5355\u4e00\u6a21\u578b\u80fd\u5728\u4e0d\u540c\u4e13\u5bb6\u6570\u91cf\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u5b9e\u73b0\u5f39\u6027\u63a8\u7406\u3002", "motivation": "\u6807\u51c6Top-K\u8def\u7531\u5668\u8bad\u7ec3\u7b56\u7565\u9650\u5236\u4e86MoE\u6a21\u578b\u7684\u5f39\u6027\u63a8\u7406\u6f5c\u529b\uff0c\u5f53\u63a8\u7406\u65f6\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u6539\u53d8\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u4f1a\u6025\u5267\u4e0b\u964d\u3002", "method": "\u5728\u8bad\u7ec3\u671f\u95f4\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\uff0c\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u6709\u610f\u4e49\u7684\u4e13\u5bb6\u6392\u540d\uff1a\u9876\u7ea7\u4e13\u5bb6\u534f\u4f5c\u63d0\u4f9b\u57fa\u672c\u7c97\u7c92\u5ea6\u80fd\u529b\uff0c\u540e\u7eed\u4e13\u5bb6\u9010\u6b65\u6dfb\u52a0\u66f4\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\u3002\u91c7\u7528\u5206\u5c42\u968f\u673a\u5316\u7b56\u7565\u3002", "result": "\u5355\u4e00M-MoE\u6a21\u578b\u5728\u4e0d\u540c\u4e13\u5bb6\u6570\u91cf\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u5f39\u6027\uff0c\u5176\u6027\u80fd\u4e0e\u6574\u4e2a\u4e13\u5bb6\u6a21\u578b\u5957\u4ef6\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u540e\u8005\u7684\u4e00\u5c0f\u90e8\u5206\u3002", "conclusion": "M-MoE\u4e3a\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u5f39\u6027\u63a8\u7406\uff0c\u8fd8\u80fd\u901a\u8fc7\u5728\u4e0d\u540c\u6a21\u578b\u5c42\u5206\u914d\u4e0d\u540c\u8ba1\u7b97\u9884\u7b97\u6765\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2509.26128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26128", "abs": "https://arxiv.org/abs/2509.26128", "authors": ["Asmita Sengupta", "David Antony Selby", "Sebastian Josef Vollmer", "Gerrit Gro\u00dfmann"], "title": "MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models", "comment": "9 pages, 5 figures, 2 tables", "summary": "Knowledge graphs (KGs) are increasingly used to represent biomedical\ninformation in structured, interpretable formats. However, existing biomedical\nKGs often focus narrowly on molecular interactions or adverse events,\noverlooking the rich data found in drug leaflets. In this work, we present (1)\na hackable, end-to-end pipeline to create KGs from unstructured online content\nusing a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by\napplying this method to publicly available drug leaflets. The dataset captures\nclinically relevant attributes such as side effects, warnings,\ncontraindications, ingredients, dosage guidelines, storage instructions and\nphysical characteristics. We evaluate it through manual inspection and with an\nLLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs\nand databases. We expect MEDAKA to support tasks such as patient safety\nmonitoring and drug recommendation. The pipeline can also be used for\nconstructing KGs from unstructured texts in other domains. Code and dataset are\navailable at https://github.com/medakakg/medaka.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u5728\u7ebf\u836f\u7269\u8bf4\u660e\u4e66\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5e76\u521b\u5efa\u4e86MEDAKA\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e34\u5e8a\u76f8\u5173\u5c5e\u6027\u5982\u526f\u4f5c\u7528\u3001\u8b66\u544a\u3001\u7981\u5fcc\u75c7\u7b49\u3002", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e3b\u8981\u5173\u6ce8\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u6216\u4e0d\u826f\u4e8b\u4ef6\uff0c\u5ffd\u7565\u4e86\u836f\u7269\u8bf4\u660e\u4e66\u4e2d\u4e30\u5bcc\u7684\u4e34\u5e8a\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u7f51\u7edc\u722c\u866b\u548cLLM\u4ece\u975e\u7ed3\u6784\u5316\u5728\u7ebf\u5185\u5bb9\u521b\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5e76\u5e94\u7528\u4e8e\u516c\u5f00\u836f\u7269\u8bf4\u660e\u4e66\u751f\u6210MEDAKA\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u68c0\u67e5\u548cLLM-as-a-Judge\u6846\u67b6\u8bc4\u4f30\uff0c\u4e0e\u73b0\u6709\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u548c\u6570\u636e\u5e93\u6bd4\u8f83\u663e\u793a\u66f4\u597d\u7684\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "MEDAKA\u53ef\u652f\u6301\u60a3\u8005\u5b89\u5168\u76d1\u6d4b\u548c\u836f\u7269\u63a8\u8350\u7b49\u4efb\u52a1\uff0c\u8be5\u6d41\u7a0b\u4e5f\u53ef\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002"}}
{"id": "2509.26088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26088", "abs": "https://arxiv.org/abs/2509.26088", "authors": ["Pasindu Ranasinghe", "Pamudu Ranasinghe"], "title": "Predicting Penalty Kick Direction Using Multi-Modal Deep Learning with Pose-Guided Attention", "comment": null, "summary": "Penalty kicks often decide championships, yet goalkeepers must anticipate the\nkicker's intent from subtle biomechanical cues within a very short time window.\nThis study introduces a real-time, multi-modal deep learning framework to\npredict the direction of a penalty kick (left, middle, or right) before ball\ncontact. The model uses a dual-branch architecture: a MobileNetV2-based CNN\nextracts spatial features from RGB frames, while 2D keypoints are processed by\nan LSTM network with attention mechanisms. Pose-derived keypoints further guide\nvisual focus toward task-relevant regions. A distance-based thresholding method\nsegments input sequences immediately before ball contact, ensuring consistent\ninput across diverse footage. A custom dataset of 755 penalty kick events was\ncreated from real match videos, with frame-level annotations for object\ndetection, shooter keypoints, and final ball placement. The model achieved 89%\naccuracy on a held-out test set, outperforming visual-only and pose-only\nbaselines by 14-22%. With an inference time of 22 milliseconds, the lightweight\nand interpretable design makes it suitable for goalkeeper training, tactical\nanalysis, and real-time game analytics.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8db3\u7403\u70b9\u7403\u89e6\u7403\u524d\u9884\u6d4b\u5c04\u95e8\u65b9\u5411\uff08\u5de6\u3001\u4e2d\u3001\u53f3\uff09\uff0c\u51c6\u786e\u7387\u8fbe89%\uff0c\u63a8\u7406\u65f6\u95f422\u6beb\u79d2\u3002", "motivation": "\u70b9\u7403\u7ecf\u5e38\u51b3\u5b9a\u51a0\u519b\u5f52\u5c5e\uff0c\u4f46\u5b88\u95e8\u5458\u5fc5\u987b\u5728\u6781\u77ed\u65f6\u95f4\u5185\u4ece\u5fae\u5999\u7684\u751f\u7269\u529b\u5b66\u7ebf\u7d22\u4e2d\u9884\u6d4b\u5c04\u95e8\u610f\u56fe\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1aMobileNetV2 CNN\u5904\u7406RGB\u5e27\u7a7a\u95f4\u7279\u5f81\uff0c\u5e26\u6ce8\u610f\u529b\u673a\u5236\u7684LSTM\u7f51\u7edc\u5904\u74062D\u5173\u952e\u70b9\uff0c\u59ff\u6001\u5173\u952e\u70b9\u5f15\u5bfc\u89c6\u89c9\u5173\u6ce8\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u4f7f\u7528\u8ddd\u79bb\u9608\u503c\u65b9\u6cd5\u5206\u5272\u8f93\u5165\u5e8f\u5217\u3002", "result": "\u5728755\u4e2a\u70b9\u7403\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u738789%\uff0c\u6bd4\u7eaf\u89c6\u89c9\u548c\u7eaf\u59ff\u6001\u57fa\u7ebf\u6a21\u578b\u63d0\u534714-22%\uff0c\u63a8\u7406\u65f6\u95f422\u6beb\u79d2\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u53ef\u89e3\u91ca\u8bbe\u8ba1\u9002\u7528\u4e8e\u5b88\u95e8\u5458\u8bad\u7ec3\u3001\u6218\u672f\u5206\u6790\u548c\u5b9e\u65f6\u6bd4\u8d5b\u5206\u6790\u3002"}}
{"id": "2509.26153", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26153", "abs": "https://arxiv.org/abs/2509.26153", "authors": ["Jack Gallifant", "Katherine C. Kellogg", "Matt Butler", "Amanda Centi", "Patrick F. Doyle", "Sayon Dutta", "Joyce Guo", "Matthew J. Hadfield", "Esther H. Kim", "David E. Kozono", "Hugo JWL Aerts", "Adam B. Landman", "Raymond H. Mak", "Rebecca G. Mishuris", "Tanna L. Nelson", "Guergana K. Savova", "Elad Sharon", "Benjamin C. Silverman", "Umit Topaloglu", "Jeremy L. Warner", "Danielle S. Bitterman"], "title": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice", "comment": "Under review. 5 Tables, 2 Figures", "summary": "Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u5b9e\u8df5\u8005\u7684\u73b0\u573a\u624b\u518c\uff0c\u7528\u4e8e\u90e8\u7f72\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff0c\u91cd\u70b9\u89e3\u51b3\u4e34\u5e8aAI\u5b9e\u65bd\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u6311\u6218\u800c\u975e\u7b97\u6cd5\u5f00\u53d1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u4e0e\u6f5c\u529b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u4f5c\u8005\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u4f9b\u5b9e\u7528\u7684\u90e8\u7f72\u6307\u5357\u3002", "method": "\u57fa\u4e8e\u5728Mass General Brigham\u90e8\u7f72\"irAE-Agent\"\u7cfb\u7edf\u7684\u7ecf\u9a8c\uff0c\u4ee5\u53ca\u5bf920\u540d\u4e34\u5e8a\u533b\u751f\u3001\u5de5\u7a0b\u5e08\u548c\u4fe1\u606f\u5b66\u9886\u5bfc\u7684\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u5206\u6790\u4e34\u5e8aAI\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u6311\u6218\u3002", "result": "\u5206\u6790\u663e\u793a\u4e34\u5e8aAI\u5f00\u53d1\u5b58\u5728\u5173\u952e\u9519\u4f4d\uff1a\u53ea\u6709\u4e0d\u523020%\u7684\u52aa\u529b\u7528\u4e8e\u63d0\u793a\u5de5\u7a0b\u548c\u6a21\u578b\u5f00\u53d1\uff0c\u800c\u8d85\u8fc780%\u88ab\u5b9e\u65bd\u7684\u793e\u4f1a\u6280\u672f\u5de5\u4f5c\u6d88\u8017\u3002\u4f5c\u8005\u5c06\u6b64\u63d0\u70bc\u4e3a\u4e94\u4e2a\"\u91cd\u6d3b\"\uff1a\u6570\u636e\u96c6\u6210\u3001\u6a21\u578b\u9a8c\u8bc1\u3001\u786e\u4fdd\u7ecf\u6d4e\u4ef7\u503c\u3001\u7ba1\u7406\u7cfb\u7edf\u6f02\u79fb\u548c\u6cbb\u7406\u3002", "conclusion": "\u8be5\u73b0\u573a\u624b\u518c\u5c06\u91cd\u70b9\u4ece\u7b97\u6cd5\u5f00\u53d1\u8f6c\u5411\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u5b9e\u65bd\u5de5\u4f5c\uff0c\u4e3a\u6210\u529f\u5c06\u751f\u6210\u5f0fAI\u4ece\u8bd5\u70b9\u9879\u76ee\u8f6c\u5316\u4e3a\u5e38\u89c4\u4e34\u5e8a\u62a4\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24803", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24803", "abs": "https://arxiv.org/abs/2509.24803", "authors": ["Tong Guan", "Zijie Meng", "Dianqi Li", "Shiyu Wang", "Chao-Han Huck Yang", "Qingsong Wen", "Zuozhu Liu", "Sabato Marco Siniscalchi", "Ming Jin", "Shirui Pan"], "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models", "comment": null, "summary": "Recent advances in multimodal time series learning underscore a paradigm\nshift from analytics centered on basic patterns toward advanced time series\nunderstanding and reasoning. However, existing multimodal time series datasets\nmostly remain at the level of surface alignment and question answering, without\nreaching the depth of genuine reasoning. The absence of well-defined tasks that\ngenuinely require time series reasoning, along with the scarcity of\nhigh-quality data, has limited progress in building practical time series\nreasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite\n(TSR-Suite), which formalizes four atomic tasks that span three fundamental\ncapabilities for reasoning with time series: (1) perception, acquired through\nscenario understanding and causality discovery; (2) extrapolation, realized via\nevent-aware forecasting; and (3) decision-making, developed through\ndeliberation over perception and extrapolation. TSR-Suite is the first\ncomprehensive time series reasoning suite that supports not only thorough\nevaluation but also the data pipeline and training of TSRMs. It contains more\nthan 23K samples, of which 2.3K are carefully curated through a human-guided\nhierarchical annotation process. Building on this foundation, we introduce\nTimeOmni-1, the first unified reasoning model designed to address diverse\nreal-world problems demanding time series reasoning. The model is trained in\nmultiple stages, integrating a mixture of task scenarios, novel reward\nfunctions, and tailored optimizations. Experiments show that TimeOmni-1\ndelivers strong out-of-distribution generalization across all tasks and\nachieves a high rate of valid responses. It significantly improves causality\ndiscovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response\nrate by over 6% compared to GPT-4.1 on the event-aware forecasting task.", "AI": {"tldr": "TSR-Suite\u662f\u9996\u4e2a\u5168\u9762\u652f\u6301\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u8bc4\u4f30\u3001\u6570\u636e\u7ba1\u9053\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u7efc\u5408\u5957\u4ef6\uff0c\u5305\u542b23K+\u6837\u672c\u548c\u56db\u4e2a\u539f\u5b50\u4efb\u52a1\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u7684TimeOmni-1\u6a21\u578b\u5728\u56e0\u679c\u53d1\u73b0\u548c\u4e8b\u4ef6\u611f\u77e5\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4.1\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u5927\u591a\u505c\u7559\u5728\u8868\u9762\u5bf9\u9f50\u548c\u95ee\u7b54\u5c42\u9762\uff0c\u7f3a\u4e4f\u771f\u6b63\u9700\u8981\u63a8\u7406\u7684\u4efb\u52a1\u5b9a\u4e49\u548c\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faTSR-Suite\u5957\u4ef6\uff0c\u5b9a\u4e49\u56db\u4e2a\u539f\u5b50\u4efb\u52a1\u6db5\u76d6\u611f\u77e5\u3001\u5916\u63a8\u548c\u51b3\u7b56\u4e09\u79cd\u57fa\u672c\u63a8\u7406\u80fd\u529b\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1TimeOmni-1\u7edf\u4e00\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u6574\u5408\u4efb\u52a1\u573a\u666f\u6df7\u5408\u3001\u65b0\u5956\u52b1\u51fd\u6570\u548c\u5b9a\u5236\u4f18\u5316\u3002", "result": "TimeOmni-1\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u679c\u53d1\u73b0\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0864.0% vs 35.9%\uff09\uff0c\u4e8b\u4ef6\u611f\u77e5\u9884\u6d4b\u4efb\u52a1\u7684\u6709\u6548\u54cd\u5e94\u7387\u6bd4GPT-4.1\u63d0\u9ad8\u8d85\u8fc76%\u3002", "conclusion": "TSR-Suite\u4e3a\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0cTimeOmni-1\u8bc1\u660e\u4e86\u7edf\u4e00\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u591a\u6837\u5316\u73b0\u5b9e\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.26208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26208", "abs": "https://arxiv.org/abs/2509.26208", "authors": ["Ioannis Kontostathis", "Evlampios Apostolidis", "Vasileios Mezaris"], "title": "TSalV360: A Method and Dataset for Text-driven Saliency Detection in 360-Degrees Videos", "comment": "IEEE CBMI 2025. This is the authors' accepted version. The final\n  publication is available at https://ieeexplore.ieee.org/", "summary": "In this paper, we deal with the task of text-driven saliency detection in\n360-degrees videos. For this, we introduce the TSV360 dataset which includes\n16,000 triplets of ERP frames, textual descriptions of salient objects/events\nin these frames, and the associated ground-truth saliency maps. Following, we\nextend and adapt a SOTA visual-based approach for 360-degrees video saliency\ndetection, and develop the TSalV360 method that takes into account a\nuser-provided text description of the desired objects and/or events. This\nmethod leverages a SOTA vision-language model for data representation and\nintegrates a similarity estimation module and a viewport spatio-temporal\ncross-attention mechanism, to discover dependencies between the different data\nmodalities. Quantitative and qualitative evaluations using the TSV360 dataset,\nshowed the competitiveness of TSalV360 compared to a SOTA visual-based approach\nand documented its competency to perform customized text-driven saliency\ndetection in 360-degrees videos.", "AI": {"tldr": "\u63d0\u51fa\u4e86TSV360\u6570\u636e\u96c6\u548cTSalV360\u65b9\u6cd5\uff0c\u7528\u4e8e360\u5ea6\u89c6\u9891\u4e2d\u7684\u6587\u672c\u9a71\u52a8\u663e\u8457\u6027\u68c0\u6d4b\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u5b9a\u5236\u5316\u663e\u8457\u6027\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3360\u5ea6\u89c6\u9891\u4e2d\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u663e\u8457\u6027\u68c0\u6d4b\u95ee\u9898\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u6587\u672c\u63cf\u8ff0\u6765\u68c0\u6d4b\u89c6\u9891\u4e2d\u7684\u663e\u8457\u5bf9\u8c61\u6216\u4e8b\u4ef6\u3002", "method": "\u6269\u5c55\u5e76\u9002\u914d\u4e86SOTA\u89c6\u89c9\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86TSalV360\u65b9\u6cd5\uff0c\u5229\u7528SOTA\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u8868\u793a\uff0c\u96c6\u6210\u76f8\u4f3c\u6027\u4f30\u8ba1\u6a21\u5757\u548c\u89c6\u53e3\u65f6\u7a7a\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u4f7f\u7528TSV360\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\uff0cTSalV360\u4e0eSOTA\u89c6\u89c9\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u5b9a\u5236\u5316\u7684\u6587\u672c\u9a71\u52a8\u663e\u8457\u6027\u68c0\u6d4b\u3002", "conclusion": "TSalV360\u65b9\u6cd5\u5728360\u5ea6\u89c6\u9891\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u6587\u672c\u9a71\u52a8\u7684\u663e\u8457\u6027\u68c0\u6d4b\uff0c\u4e3a\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u5b9a\u5236\u5316\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26473", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26473", "abs": "https://arxiv.org/abs/2509.26473", "authors": ["Shaoxiong Guo", "Tianyi Du", "Lijun Li", "Yuyao Wu", "Jie Li", "Jing Shao"], "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models", "comment": null, "summary": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSTaR-Attack\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b(UMMs)\u4e2d\u751f\u6210\u4e0e\u7406\u89e3\u8026\u5408\u7684\u6f0f\u6d1e\uff0c\u901a\u8fc7\u65f6\u7a7a\u53d9\u4e8b\u9690\u85cf\u6076\u610f\u4e8b\u4ef6\u8fdb\u884c\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u3002", "motivation": "\u53d1\u73b0\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u751f\u6210\u4e0e\u7406\u89e3\u8026\u5408\u7684\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u751f\u6210\u529f\u80fd\u5236\u4f5c\u4fe1\u606f\u4e30\u5bcc\u7684\u5bf9\u6297\u56fe\u50cf\uff0c\u7136\u540e\u901a\u8fc7\u7406\u89e3\u529f\u80fd\u5355\u6b21\u5438\u6536\uff0c\u5f62\u6210\u8de8\u6a21\u6001\u751f\u6210\u6ce8\u5165(CMGI)\u653b\u51fb\u3002", "method": "STaR-Attack\u6846\u67b6\uff1a1)\u5b9a\u4e49\u4e0e\u76ee\u6807\u67e5\u8be2\u5f3a\u76f8\u5173\u7684\u6076\u610f\u4e8b\u4ef6\uff1b2)\u4f7f\u7528\u4e09\u5e55\u53d9\u4e8b\u7406\u8bba\u751f\u6210\u524d\u540e\u573a\u666f\uff0c\u9690\u85cf\u6076\u610f\u4e8b\u4ef6\u4f5c\u4e3a\u9ad8\u6f6e\uff1b3)\u524d\u4e24\u8f6e\u5229\u7528\u751f\u6210\u80fd\u529b\u5236\u4f5c\u573a\u666f\u56fe\u50cf\uff1b4)\u5f15\u5165\u57fa\u4e8e\u56fe\u50cf\u7684\u95ee\u7b54\u6e38\u620f\uff0c\u5c06\u6076\u610f\u95ee\u9898\u9690\u85cf\u5728\u826f\u6027\u5019\u9009\u4e2d\u3002", "result": "\u5728Gemini-2.0-Flash\u4e0a\u8fbe\u523093.06%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u6700\u5f3a\u7684FlipAttack\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63ed\u793a\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u5173\u952e\u4f46\u672a\u5145\u5206\u5f00\u53d1\u7684\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u5bf9UMMs\u8fdb\u884c\u5b89\u5168\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.26584", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26584", "abs": "https://arxiv.org/abs/2509.26584", "authors": ["Matheus Vinicius da Silva de Oliveira", "Jonathan de Andrade Silva", "Awdren de Lima Fontao"], "title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models", "comment": null, "summary": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8715\u53d8\u6d4b\u8bd5\u5bf9\u4e09\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728RAG\u7ba1\u9053\u4e2d\u7684\u516c\u5e73\u6027\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u7ec6\u5fae\u7684\u4eba\u53e3\u7edf\u8ba1\u53d8\u5316\u53ef\u7834\u574f\u4e09\u5206\u4e4b\u4e00\u7684\u8715\u53d8\u5173\u7cfb\uff0c\u5176\u4e2d\u79cd\u65cf\u7ebf\u7d22\u662f\u4e3b\u8981\u8fdd\u89c4\u539f\u56e0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5305\u62ec\u516c\u5e73\u6027\u7f3a\u9677\u548c\u5e7b\u89c9\u95ee\u9898\u3002RAG\u867d\u7136\u80fd\u7f13\u89e3\u5e7b\u89c9\uff0c\u4f46\u5176\u68c0\u7d22\u5185\u5bb9\u53ef\u80fd\u52a0\u5267\u504f\u89c1\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u516c\u5e73\u6027\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u8715\u53d8\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5728\u63d0\u793a\u4e2d\u5f15\u5165\u53d7\u63a7\u7684\u4eba\u53e3\u7edf\u8ba1\u6270\u52a8\uff0c\u8bc4\u4f30\u4e09\u4e2aHuggingFace\u4e0a\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama-3.2-3B-Instruct\u3001Mistral-7B-Instruct-v0.3\u548cLlama-3.1-Nemotron-8B\uff09\u5728RAG\u7ba1\u9053\u4e2d\u7684\u60c5\u611f\u5206\u6790\u516c\u5e73\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7ec6\u5fae\u7684\u4eba\u53e3\u7edf\u8ba1\u53d8\u5316\u53ef\u7834\u574f\u9ad8\u8fbe\u4e09\u5206\u4e4b\u4e00\u7684\u8715\u53d8\u5173\u7cfb\u3002\u5931\u8d25\u5206\u6790\u63ed\u793a\u4e86\u6301\u7eed\u7684\u504f\u89c1\u5c42\u7ea7\uff0c\u6d89\u53ca\u79cd\u65cf\u7ebf\u7d22\u7684\u6270\u52a8\u662f\u8fdd\u89c4\u7684\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "RAG\u4e2d\u7684\u68c0\u7d22\u7ec4\u4ef6\u5fc5\u987b\u4ed4\u7ec6\u7ba1\u7406\u4ee5\u9632\u6b62\u504f\u89c1\u653e\u5927\u3002\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u7ec4\u7ec7\u91c7\u7528\u53ef\u8bbf\u95ee\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8b66\u793a\uff0c\u5f3a\u8c03\u4e0d\u80fd\u727a\u7272\u516c\u5e73\u6027\u6216\u53ef\u9760\u6027\u3002"}}
{"id": "2509.26498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26498", "abs": "https://arxiv.org/abs/2509.26498", "authors": ["Jijun Xiang", "Longliang Liu", "Xuan Zhu", "Xianqi Wang", "Min Lin", "Xin Yang"], "title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance", "comment": "15 pages, 16 figures", "summary": "Depth enhancement, which converts raw dToF signals into dense depth maps\nusing RGB guidance, is crucial for improving depth perception in high-precision\ntasks such as 3D reconstruction and SLAM. However, existing methods often\nassume ideal dToF inputs and perfect dToF-RGB alignment, overlooking\ncalibration errors and anomalies, thus limiting real-world applicability. This\nwork systematically analyzes the noise characteristics of real-world\nlightweight dToF sensors and proposes a practical and novel depth completion\nframework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three\nkey aspects. First, we introduce a simulation method based on synthetic\ndatasets to generate realistic training samples for robust model training.\nSecond, we propose a learnable-parameter-free anomaly detection mechanism to\nidentify and remove erroneous dToF measurements, preventing misleading\npropagation during completion. Third, we design a depth completion network\ntailored to noisy dToF inputs, which integrates RGB images and pre-trained\nmonocular depth estimation priors to improve depth recovery in challenging\nregions. On the ZJU-L5 dataset and real-world samples, our training strategy\nsignificantly boosts existing depth completion models, with our model achieving\nstate-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.\nOn the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our\nmodel improves upon the previous SOTA by 37% in mirror regions. On the Hammer\ndataset, using simulated low-cost dToF data from RealSense L515, our method\nsurpasses the L515 measurements with an average gain of 22%, demonstrating its\npotential to enable low-cost sensors to outperform higher-end devices.\nQualitative results across diverse real-world datasets further validate the\neffectiveness and generalizability of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEPTHOR++\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u3001\u65e0\u53c2\u6570\u5f02\u5e38\u68c0\u6d4b\u548c\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cfdToF\u4f20\u611f\u5668\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u589e\u5f3a\u65b9\u6cd5\u5047\u8bbe\u7406\u60f3\u7684dToF\u8f93\u5165\u548c\u5b8c\u7f8e\u7684dToF-RGB\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6821\u51c6\u8bef\u5dee\u548c\u5f02\u5e38\uff0c\u9650\u5236\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u9002\u7528\u6027\u3002", "method": "1. \u57fa\u4e8e\u5408\u6210\u6570\u636e\u96c6\u7684\u6a21\u62df\u65b9\u6cd5\u751f\u6210\u771f\u5b9e\u8bad\u7ec3\u6837\u672c\uff1b2. \u65e0\u5b66\u4e60\u53c2\u6570\u5f02\u5e38\u68c0\u6d4b\u673a\u5236\u8bc6\u522b\u9519\u8befdToF\u6d4b\u91cf\uff1b3. \u9488\u5bf9\u566a\u58f0dToF\u8f93\u5165\u8bbe\u8ba1\u7684\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\uff0c\u6574\u5408RGB\u56fe\u50cf\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5148\u9a8c\u3002", "result": "\u5728ZJU-L5\u6570\u636e\u96c6\u4e0aRMSE\u548cRel\u5206\u522b\u5e73\u5747\u63d0\u534722%\u548c11%\uff1b\u5728Mirror3D-NYU\u6570\u636e\u96c6\u4e0a\u955c\u9762\u533a\u57df\u6027\u80fd\u63d0\u534737%\uff1b\u5728Hammer\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aRealSense L515\u6d4b\u91cf\u7ed3\u679c22%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u4f7f\u4f4e\u6210\u672c\u4f20\u611f\u5668\u80fd\u591f\u8d85\u8d8a\u9ad8\u7aef\u8bbe\u5907\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.26555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26555", "abs": "https://arxiv.org/abs/2509.26555", "authors": ["Agneet Chatterjee", "Rahim Entezari", "Maksym Zhuravinskyi", "Maksim Lapin", "Reshinth Adithyan", "Amit Raj", "Chitta Baral", "Yezhou Yang", "Varun Jampani"], "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation", "comment": "NeurIPS 2025. Project Page : https://stable-cinemetrics.github.io/", "summary": "Recent advances in video generation have enabled high-fidelity video\nsynthesis from user provided prompts. However, existing models and benchmarks\nfail to capture the complexity and requirements of professional video\ngeneration. Towards that goal, we introduce Stable Cinemetrics, a structured\nevaluation framework that formalizes filmmaking controls into four\ndisentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.\nTogether, these taxonomies define 76 fine-grained control nodes grounded in\nindustry practices. Using these taxonomies, we construct a benchmark of prompts\naligned with professional use cases and develop an automated pipeline for\nprompt categorization and question generation, enabling independent evaluation\nof each control dimension. We conduct a large-scale human study spanning 10+\nmodels and 20K videos, annotated by a pool of 80+ film professionals. Our\nanalysis, both coarse and fine-grained reveal that even the strongest current\nmodels exhibit significant gaps, particularly in Events and Camera-related\ncontrols. To enable scalable evaluation, we train an automatic evaluator, a\nvision-language model aligned with expert annotations that outperforms existing\nzero-shot baselines. SCINE is the first approach to situate professional video\ngeneration within the landscape of video generative models, introducing\ntaxonomies centered around cinematic controls and supporting them with\nstructured evaluation pipelines and detailed analyses to guide future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86Stable Cinemetrics\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u7535\u5f71\u5236\u4f5c\u63a7\u5236\u5206\u4e3a\u56db\u4e2a\u5c42\u6b21\u5316\u5206\u7c7b\uff1a\u8bbe\u7f6e\u3001\u4e8b\u4ef6\u3001\u706f\u5149\u548c\u6444\u50cf\u673a\uff0c\u5305\u542b76\u4e2a\u7ec6\u7c92\u5ea6\u63a7\u5236\u8282\u70b9\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u7814\u7a76\u548c\u81ea\u52a8\u8bc4\u4f30\u5668\u5206\u6790\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u4e13\u4e1a\u9700\u6c42\u4e0a\u7684\u8868\u73b0\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u6355\u6349\u4e13\u4e1a\u89c6\u9891\u751f\u6210\u7684\u590d\u6742\u6027\u548c\u9700\u6c42\uff0c\u9700\u8981\u5efa\u7acb\u9762\u5411\u4e13\u4e1a\u7535\u5f71\u5236\u4f5c\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165\u56db\u4e2a\u89e3\u8026\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u4e13\u4e1a\u7528\u4f8b\u5bf9\u9f50\u7684\u63d0\u793a\u57fa\u51c6\uff0c\u5f00\u53d1\u81ea\u52a8\u63d0\u793a\u5206\u7c7b\u548c\u95ee\u9898\u751f\u6210\u6d41\u7a0b\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u4eba\u7c7b\u7814\u7a76\uff0810+\u6a21\u578b\u300120K\u89c6\u9891\u300180+\u7535\u5f71\u4e13\u4e1a\u4eba\u58eb\u6807\u6ce8\uff09\uff0c\u8bad\u7ec3\u81ea\u52a8\u8bc4\u4f30\u5668\u3002", "result": "\u5373\u4f7f\u6700\u5f3a\u7684\u73b0\u6709\u6a21\u578b\u5728\u4e8b\u4ef6\u548c\u6444\u50cf\u673a\u76f8\u5173\u63a7\u5236\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u8bad\u7ec3\u7684\u81ea\u52a8\u8bc4\u4f30\u5668\u5728\u4e13\u5bb6\u6807\u6ce8\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u57fa\u7ebf\u3002", "conclusion": "SCINE\u662f\u9996\u4e2a\u5c06\u4e13\u4e1a\u89c6\u9891\u751f\u6210\u7f6e\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u80cc\u666f\u4e0b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee5\u7535\u5f71\u63a7\u5236\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\u548c\u7ed3\u6784\u5316\u8bc4\u4f30\u6d41\u7a0b\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.26114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26114", "abs": "https://arxiv.org/abs/2509.26114", "authors": ["Jaesung R. Park", "Junsu Kim", "Gyeongman Kim", "Jinyoung Jo", "Sean Choi", "Jaewoong Cho", "Ernest K. Ryu"], "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as\nthe leading approach for enhancing the reasoning capabilities of large language\nmodels (LLMs). However, RLVR is prone to entropy collapse, where the LLM\nquickly converges to a near-deterministic form, hindering exploration and\nprogress during prolonged RL training. In this work, we reveal that the\nclipping mechanism in PPO and GRPO induces biases on entropy. Through\ntheoretical and empirical analyses, we show that clip-low increases entropy,\nwhile clip-high decreases it. Further, under standard clipping parameters, the\neffect of clip-high dominates, resulting in an overall entropy reduction even\nwhen purely random rewards are provided to the RL algorithm. Our findings\nhighlight an overlooked confounding factor in RLVR: independent of the reward\nsignal, the clipping mechanism influences entropy, which in turn affects the\nreasoning behavior. Furthermore, our analysis demonstrates that clipping can be\ndeliberately used to control entropy. Specifically, with a more aggressive\nclip-low value, one can increase entropy, promote exploration, and ultimately\nprevent entropy collapse in RLVR training.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86PPO\u548cGRPO\u4e2d\u7684\u88c1\u526a\u673a\u5236\u4f1a\u5bfc\u81f4\u71b5\u504f\u5dee\uff1aclip-low\u589e\u52a0\u71b5\uff0cclip-high\u51cf\u5c11\u71b5\u3002\u5728\u6807\u51c6\u88c1\u526a\u53c2\u6570\u4e0b\uff0cclip-high\u6548\u5e94\u4e3b\u5bfc\uff0c\u5bfc\u81f4\u6574\u4f53\u71b5\u51cf\u5c11\u3002\u4f5c\u8005\u5efa\u8bae\u4f7f\u7528\u66f4\u6fc0\u8fdb\u7684clip-low\u503c\u6765\u589e\u52a0\u71b5\u3001\u4fc3\u8fdb\u63a2\u7d22\uff0c\u9632\u6b62RLVR\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u3002", "motivation": "RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u662f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u5bb9\u6613\u53d1\u751f\u71b5\u5d29\u6e83\uff0c\u5373\u6a21\u578b\u5feb\u901f\u6536\u655b\u5230\u8fd1\u4e4e\u786e\u5b9a\u6027\u7684\u5f62\u5f0f\uff0c\u963b\u788d\u957f\u671fRL\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u548c\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76PPO\u548cGRPO\u4e2d\u88c1\u526a\u673a\u5236\u5bf9\u71b5\u7684\u5f71\u54cd\uff0c\u63ed\u793aclip-low\u589e\u52a0\u71b5\u3001clip-high\u51cf\u5c11\u71b5\u7684\u89c4\u5f8b\u3002", "result": "\u5728\u6807\u51c6\u88c1\u526a\u53c2\u6570\u4e0b\uff0cclip-high\u6548\u5e94\u4e3b\u5bfc\uff0c\u5373\u4f7f\u63d0\u4f9b\u7eaf\u968f\u673a\u5956\u52b1\uff0cRL\u7b97\u6cd5\u4e5f\u4f1a\u5bfc\u81f4\u6574\u4f53\u71b5\u51cf\u5c11\u3002\u88c1\u526a\u673a\u5236\u72ec\u7acb\u4e8e\u5956\u52b1\u4fe1\u53f7\u5f71\u54cd\u71b5\uff0c\u8fdb\u800c\u5f71\u54cd\u63a8\u7406\u884c\u4e3a\u3002", "conclusion": "\u88c1\u526a\u673a\u5236\u53ef\u4ee5\u6709\u610f\u7528\u4e8e\u63a7\u5236\u71b5\uff0c\u901a\u8fc7\u66f4\u6fc0\u8fdb\u7684clip-low\u503c\u53ef\u4ee5\u589e\u52a0\u71b5\u3001\u4fc3\u8fdb\u63a2\u7d22\uff0c\u6700\u7ec8\u9632\u6b62RLVR\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u3002"}}
{"id": "2509.26522", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26522", "abs": "https://arxiv.org/abs/2509.26522", "authors": ["Xi Wang", "James McInerney", "Lequn Wang", "Nathan Kallus"], "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting", "comment": null, "summary": "Large reasoning models show improved performance with longer chains of\nthought. However, recent work has highlighted (qualitatively) their tendency to\noverthink, continuing to revise answers even after reaching the correct\nsolution. We quantitatively confirm this inefficiency by tracking Pass@1 for\nanswers averaged over a large number of rollouts and find that the model often\nbegins to always produce the correct answer early in the reasoning, making\nextra reasoning a waste of tokens. To detect and prevent overthinking, we\npropose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)\n-- for monitoring and deciding whether to exit reasoning early. By appending a\nstop thinking token (</think>) and monitoring the entropy of the following\ntoken as the model reasons, we obtain a trajectory that decreases and\nstabilizes when Pass@1 plateaus; thresholding its variance under an exponential\nmoving average yields a practical stopping rule. Importantly, our approach\nenables adaptively allocating compute based on the EAT trajectory, allowing us\nto spend compute in a more efficient way compared with fixing the token budget\nfor all questions. Empirically, on MATH500 and AIME2025, EAT reduces token\nusage by 13 - 21% without harming accuracy, and it remains effective in black\nbox settings where logits from the reasoning model are not accessible, and EAT\nis computed with proxy models.", "AI": {"tldr": "\u63d0\u51fa\u4e86EAT\uff08Entropy After </Think>\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u63a8\u7406\u8fc7\u7a0b\u4e2d\u505c\u6b62\u601d\u8003\u6807\u8bb0\u540e\u7684\u71b5\u503c\u53d8\u5316\u6765\u68c0\u6d4b\u548c\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u503e\u5411\uff0c\u5373\u4f7f\u5df2\u7ecf\u5f97\u5230\u6b63\u786e\u7b54\u6848\u4ecd\u4f1a\u7ee7\u7eed\u4fee\u6b63\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u63a8\u7406\u4f55\u65f6\u8fbe\u5230\u7a33\u5b9a\u72b6\u6001\u4ee5\u4fbf\u53ca\u65f6\u505c\u6b62\u3002", "method": "\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6dfb\u52a0\u505c\u6b62\u601d\u8003\u6807\u8bb0</think>\uff0c\u76d1\u63a7\u540e\u7eed\u6807\u8bb0\u7684\u71b5\u503c\u8f68\u8ff9\u3002\u5f53\u71b5\u503c\u4e0b\u964d\u5e76\u7a33\u5b9a\u65f6\uff08Pass@1\u8fbe\u5230\u5e73\u53f0\u671f\uff09\uff0c\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u65b9\u5dee\u9608\u503c\u6765\u786e\u5b9a\u505c\u6b62\u65f6\u673a\u3002", "result": "\u5728MATH500\u548cAIME2025\u6570\u636e\u96c6\u4e0a\uff0cEAT\u65b9\u6cd5\u51cf\u5c11\u4e8613-21%\u7684token\u4f7f\u7528\u91cf\uff0c\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u7387\u3002\u5373\u4f7f\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\uff08\u65e0\u6cd5\u8bbf\u95ee\u63a8\u7406\u6a21\u578b\u7684logits\uff09\uff0c\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u8ba1\u7b97EAT\u4ecd\u7136\u6709\u6548\u3002", "conclusion": "EAT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
