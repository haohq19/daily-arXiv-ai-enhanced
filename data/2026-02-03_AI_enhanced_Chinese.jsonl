{"id": "2602.00007", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00007", "abs": "https://arxiv.org/abs/2602.00007", "authors": ["MinGyu Jeon", "SuWan Cho", "JaeYoung Shu"], "title": "PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.", "AI": {"tldr": "PPoGA\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\u548c\u9884\u6d4b\u5904\u7406\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM+KG\u65b9\u6cd5\u5728\u521d\u59cb\u63a8\u7406\u8ba1\u5212\u9519\u8bef\u65f6\u5bb9\u6613\u5931\u8d25\uff0c\u7c7b\u4f3c\u4e8e\u8ba4\u77e5\u529f\u80fd\u56fa\u7740\u95ee\u9898\uff0c\u65e0\u6cd5\u91cd\u65b0\u8c03\u6574\u7b56\u7565\uff0c\u5bfc\u81f4\u8ffd\u6c42\u4e0d\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPPoGA\u6846\u67b6\uff0c\u91c7\u7528\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\u5206\u79bb\u9ad8\u5c42\u7b56\u7565\u4e0e\u5e95\u5c42\u6267\u884c\uff0c\u5f15\u5165\u9884\u6d4b\u5904\u7406\u673a\u5236\u9884\u6d4b\u7ed3\u679c\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u81ea\u6211\u4fee\u6b63\u673a\u5236\uff0c\u5305\u62ec\u8def\u5f84\u4fee\u6b63\u548c\u8ba1\u5212\u4fee\u6b63\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3KGQA\u57fa\u51c6\u6d4b\u8bd5\uff08GrailQA\u3001CWQ\u3001WebQSP\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cPPoGA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5143\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u95ee\u9898\u91cd\u6784\uff09\u5bf9\u4e8e\u6784\u5efa\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684AI\u63a8\u7406\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0cPPoGA\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u6211\u4fee\u6b63\u673a\u5236\u63d0\u5347\u63a8\u7406\u9c81\u68d2\u6027\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00015", "abs": "https://arxiv.org/abs/2602.00015", "authors": ["Xun Xu"], "title": "G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \\textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \\textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.", "AI": {"tldr": "G-MemLLM\uff1a\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u66f4\u65b0\u7684\u8bb0\u5fc6\u589e\u5f3aLLM\u67b6\u6784\uff0c\u901a\u8fc7\u6f5c\u5728\u8bb0\u5fc6\u5e93\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4e0a\u4e0b\u6587\u538b\u7f29\u6216\u5faa\u73af\u6807\u8bb0\uff09\u5e38\u51fa\u73b0\"\u4e0a\u4e0b\u6587\u8150\u5316\"\u6216\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\uff0c\u9700\u8981\u66f4\u597d\u7684\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u3002", "method": "\u63d0\u51faG-MemLLM\u67b6\u6784\uff0c\u5c06\u51bb\u7ed3\u7684LLM\u4e3b\u5e72\u4e0e\u53ef\u8bad\u7ec3\u7684\u6f5c\u5728\u8bb0\u5fc6\u5e93\u7ed3\u5408\u3002\u6838\u5fc3\u521b\u65b0\u662fGRU\u98ce\u683c\u7684\u95e8\u63a7\u66f4\u65b0\u903b\u8f91\uff0c\u5141\u8bb8\u6a21\u578b\u9009\u62e9\u6027\u66f4\u65b0\u3001\u4fdd\u7559\u6216\u8986\u76d6\u6f5c\u5728\u8bb0\u5fc6\u69fd\uff0c\u907f\u514d\u5faa\u73af\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u77e5\u8bc6\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "result": "\u5728GPT-2\uff08124M\uff09\u5230Llama 3.1\uff088B\uff09\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5728HotpotQA\u548cZsRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aLlama 3.1-8B\u5728ZsRE\u4e0a\u51c6\u786e\u7387\u63d0\u534713.3%\uff1bGPT-2\u5728HotpotQA\u4e0a\u7b54\u6848F1\u63d0\u53478.56\u5206\uff1bLlama 3.1-8B\u5728HotpotQA\u4e0a\u652f\u6301\u4e8b\u5b9eF1\u63d0\u53476.89\u5206\u3002", "conclusion": "G-MemLLM\u901a\u8fc7\u95e8\u63a7\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u4fdd\u6301\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u80fd\u529b\uff0c\u4e14\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.00016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00016", "abs": "https://arxiv.org/abs/2602.00016", "authors": ["Jiongchi Yu", "Yuhan Ma", "Xiaoyu Zhang", "Junjie Wang", "Qiang Hu", "Chao Shen", "Xiaofei Xie"], "title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems", "comment": "28 pages", "summary": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.", "AI": {"tldr": "PTCBENCH\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53d7\u63a7\u60c5\u5883\u4e0b\u4eba\u683c\u4e00\u81f4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5916\u90e8\u573a\u666f\uff08\u5982\u5931\u4e1a\uff09\u4f1a\u663e\u8457\u6539\u53d8LLM\u7684\u4eba\u683c\u7279\u8d28\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u4ee3\u7406\u548cAI\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u4fdd\u6301\u4e00\u81f4\u4e14\u771f\u5b9e\u7684\u4eba\u683c\u5bf9\u4e8e\u7528\u6237\u4fe1\u4efb\u548c\u53c2\u4e0e\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u4eba\u683c\u7279\u8d28\u662f\u52a8\u6001\u4e14\u60c5\u5883\u4f9d\u8d56\u7684\u57fa\u672c\u5fc3\u7406\u5b66\u5171\u8bc6\u3002", "method": "\u5f15\u5165PTCBENCH\u57fa\u51c6\uff0c\u5c06\u6a21\u578b\u7f6e\u4e8e12\u79cd\u4e0d\u540c\u7684\u5916\u90e8\u60c5\u5883\uff08\u5730\u70b9\u80cc\u666f\u548c\u751f\u6d3b\u4e8b\u4ef6\uff09\uff0c\u4f7f\u7528NEO\u4e94\u56e0\u7d20\u4eba\u683c\u91cf\u8868\u4e25\u683c\u8bc4\u4f30\u4eba\u683c\uff0c\u5171\u5206\u6790\u4e8639,240\u4e2a\u4eba\u683c\u7279\u8d28\u8bb0\u5f55\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u5916\u90e8\u573a\u666f\uff08\u5982\"\u5931\u4e1a\"\uff09\u4f1a\u89e6\u53d1LLM\u663e\u8457\u7684\u4eba\u683c\u53d8\u5316\uff0c\u751a\u81f3\u6539\u53d8\u5176\u63a8\u7406\u80fd\u529b\u3002PTCBENCH\u4e3a\u8bc4\u4f30\u73b0\u5b9e\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4eba\u683c\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002", "conclusion": "PTCBENCH\u4e3a\u5f00\u53d1\u7a33\u5065\u4e14\u5fc3\u7406\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5efa\u7acb\u4e86\u8bc4\u4f30\u4eba\u683c\u4e00\u81f4\u6027\u7684\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLM\u5728\u60c5\u611f\u4ee3\u7406\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2602.00115", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00115", "abs": "https://arxiv.org/abs/2602.00115", "authors": ["David El-Chai Ben-Ezra", "Adar Tal", "Daniel Brisk"], "title": "Event Driven Clustering Algorithm", "comment": "~10 pages, 2 figures", "summary": "This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u7684\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u805a\u7c7b\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u5c0f\u4e8b\u4ef6\u7c07\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6O(n)\u4e14\u8fd0\u884c\u65f6\u95f4\u4e0e\u50cf\u7d20\u9635\u5217\u7ef4\u5ea6\u65e0\u5173\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4ea7\u751f\u5f02\u6b65\u4e8b\u4ef6\u6d41\u6570\u636e\uff0c\u4f20\u7edf\u805a\u7c7b\u7b97\u6cd5\u5728\u5904\u7406\u8fd9\u79cd\u6570\u636e\u65f6\u6548\u7387\u4e0d\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u3001\u68c0\u6d4b\u5c0f\u4e8b\u4ef6\u7c07\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u7684\u5c42\u6b21\u51dd\u805a\u805a\u7c7b\u7b97\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u7279\u6b8a\u5f02\u6b65\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u7cbe\u5999\u3001\u9ad8\u6548\u4e14\u7b80\u5355\u7684\u51b3\u7b56\u673a\u5236\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u7684\u65f6\u95f4-\u7a7a\u95f4\u8ddd\u79bb\u68c0\u6d4b\u4e8b\u4ef6\u7c07\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6O(n)\uff0c\u5176\u4e2dn\u4e3a\u4e8b\u4ef6\u6570\u91cf\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u4e0e\u50cf\u7d20\u9635\u5217\u7ef4\u5ea6\u65e0\u5173\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u5c0f\u4e8b\u4ef6\u7c07\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u805a\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u5c0f\u4e8b\u4ef6\u7c07\u68c0\u6d4b\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.00428", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00428", "abs": "https://arxiv.org/abs/2602.00428", "authors": ["Naen Xu", "Hengyu An", "Shuo Shi", "Jinghuai Zhang", "Chunyi Zhou", "Changjiang Li", "Tianyu Du", "Zhihui Fu", "Jun Wang", "Shouling Ji"], "title": "When Agents \"Misremember\" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems", "comment": "ICLR 2026", "summary": "Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\uff08\u96c6\u4f53\u8bb0\u5fc6\u504f\u5dee\uff09\uff0c\u63d0\u51fa\u4e86MANBENCH\u57fa\u51c6\u6765\u8bc4\u4f30\u8be5\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u5e73\u5747\u51cf\u5c1174.40%\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4f46\u667a\u80fd\u4f53\u5bf9\u96c6\u4f53\u8ba4\u77e5\u504f\u5dee\u7684\u6613\u611f\u6027\u4ecd\u672a\u5145\u5206\u7814\u7a76\u3002\u66fc\u5fb7\u62c9\u6548\u5e94\u4f5c\u4e3a\u96c6\u4f53\u9519\u8bef\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b58\u5728\u98ce\u9669\uff0c\u53ef\u80fd\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u5e76\u5f15\u53d1\u4f26\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51faMANBENCH\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cd\u6613\u53d7\u66fc\u5fb7\u62c9\u6548\u5e94\u5f71\u54cd\u7684\u4efb\u52a1\u7c7b\u578b\u548c\u4e94\u79cd\u4e0d\u540c\u667a\u80fd\u4f53\u89d2\u8272\u4e0e\u8bb0\u5fc6\u65f6\u95f4\u5c3a\u5ea6\u7684\u4ea4\u4e92\u534f\u8bae\u3002\u8bc4\u4f30\u591a\u79cdLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u5206\u6790\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u7f13\u89e3\u7b56\u7565\uff1a\u5305\u62ec\u63d0\u793a\u7ea7\u9632\u5fa1\uff08\u8ba4\u77e5\u951a\u5b9a\u548c\u6765\u6e90\u5ba1\u67e5\uff09\u548c\u6a21\u578b\u7ea7\u5bf9\u9f50\u9632\u5fa1\u3002", "result": "\u5728MANBENCH\u4e0a\u91cf\u5316\u4e86\u66fc\u5fb7\u62c9\u6548\u5e94\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u56e0\u7d20\u5bf9\u5176\u5f71\u54cd\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u51cf\u5c11\u4e8674.40%\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u97e7\u6027\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u66fc\u5fb7\u62c9\u6548\u5e94\u7684\u5b58\u5728\u548c\u5f71\u54cd\u56e0\u7d20\uff0c\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u6709\u6548\u964d\u4f4e\u4e86\u8be5\u6548\u5e94\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u5177\u97e7\u6027\u548c\u4f26\u7406\u5bf9\u9f50\u7684\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.00126", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00126", "abs": "https://arxiv.org/abs/2602.00126", "authors": ["Dmytro Filatov", "Valentyn Fedorov", "Vira Filatova", "Andrii Zelenchuk"], "title": "D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection", "comment": "9 pages", "summary": "Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.", "AI": {"tldr": "D3R-Net\uff1a\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u53cc\u57df\u53bb\u566a\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\"\u4fee\u590d\"\u4efb\u52a1\u548c\u9891\u57df\u611f\u77e5\u6b63\u5219\u5316\uff0c\u6539\u5584\u9ad8\u9891\u7ec6\u8282\u91cd\u5efa\uff0c\u63d0\u5347\u7f3a\u9677\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91cd\u5efa\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u5bf9\u9ad8\u9891\u7ec6\u8282\u91cd\u5efa\u4e0d\u4f73\uff0c\u5bfc\u81f4\u7ec6\u5fae\u7f3a\u9677\u88ab\u90e8\u5206\u91cd\u5efa\u800c\u975e\u7a81\u51fa\u663e\u793a\uff0c\u9650\u5236\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faD3R-Net\u53cc\u57df\u53bb\u566a\u91cd\u5efa\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5408\u6210\u635f\u574f\u7684\u6b63\u5e38\u56fe\u50cf\u8fdb\u884c\u81ea\u76d1\u7763\"\u4fee\u590d\"\u8bad\u7ec3\uff0c\u9632\u6b62\u6052\u7b49\u6620\u5c04\uff1b2\uff09\u7ed3\u5408\u7a7a\u95f4MSE\u635f\u5931\u548cFFT\u5e45\u5ea6\u635f\u5931\u8fdb\u884c\u9891\u57df\u4e00\u81f4\u6027\u7ea6\u675f\uff1b3\uff09\u53ef\u9009SSIM\u635f\u5931\uff1b4\uff09\u91c7\u7528\u8f7b\u91cf\u5377\u79ef\u81ea\u7f16\u7801\u5668\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728MVTec AD Hazelnut\u57fa\u51c6\u4e0a\uff0cFFT\u635f\u5931\u5c06PRO AUC\u4ece0.603\u63d0\u5347\u81f30.687\uff1b\u572815\u4e2aMVTec\u7c7b\u522b\u4e0a\uff0c\u5e73\u5747\u50cf\u7d20ROC AUC\u4ece0.733\u63d0\u5347\u81f30.751\uff0cPRO AUC\u4ece0.417\u63d0\u5347\u81f30.468\uff0c\u5355GPU\u4e0a\u7ea620FPS\u3002", "conclusion": "D3R-Net\u901a\u8fc7\u53cc\u57df\u91cd\u5efa\u7b56\u7565\u6709\u6548\u6539\u5584\u9ad8\u9891\u7ec6\u8282\u5904\u7406\uff0c\u63d0\u5347\u5f02\u5e38\u5b9a\u4f4d\u4e00\u81f4\u6027\uff0c\u4e3a\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.00120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00120", "abs": "https://arxiv.org/abs/2602.00120", "authors": ["Xianghong Hu", "Tianning Xu", "Ying Chen", "Shuai Wang"], "title": "Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control", "comment": "12 pages, 4 figures. An extended and pedagogical version will appear as a book chapter", "summary": "Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7528\u4e8e\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u6807\u7b7e\u6a21\u7cca\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u4fe1\u606f\u6cc4\u6f0f\u4e09\u4e2a\u5b9e\u9645\u95ee\u9898\uff0c\u53d1\u73b0AutoGluon\u5728\u63a7\u5236\u6cc4\u6f0f\u548c\u5904\u7406\u4e0d\u5e73\u8861\u540e\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u662f\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u7684\u6838\u5fc3\u4efb\u52a1\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8fdd\u7ea6\u6807\u7b7e\u5b9a\u4e49\u6a21\u7cca\u3001\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4ee5\u53ca\u7531\u65f6\u95f4\u7ed3\u6784\u548c\u4e8b\u540e\u53d8\u91cf\u5f15\u8d77\u7684\u4fe1\u606f\u6cc4\u6f0f\uff0c\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86\u8bc4\u4f30\u6709\u6548\u6027\u548c\u90e8\u7f72\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u6cc4\u6f0f\u611f\u77e5\u7684\u7279\u5f81\u9009\u62e9\u3001\u4e25\u683c\u7684\u65f6\u95f4\u5206\u5272\uff08\u9650\u5236\u8d37\u6b3e\u53d1\u653e\u548c\u62a5\u544a\u65f6\u671f\uff09\u3001\u4ee5\u53ca\u63a7\u5236\u6027\u7684\u591a\u6570\u7c7b\u4e0b\u91c7\u6837\u3002\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u6cc4\u6f0f\u63a7\u5236\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u3002", "result": "\u5728\u4e0d\u540c\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0cAutoGluon\u5728\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5f3a\u7684AUROC\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u4e25\u683c\u7684\u6cc4\u6f0f\u63a7\u5236\u548c\u9002\u5f53\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fAutoML\u65b9\u6cd5\u5982AutoGluon\uff09\u80fd\u591f\u6709\u6548\u9884\u6d4b\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\uff0c\u4e3a\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2602.01040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01040", "abs": "https://arxiv.org/abs/2602.01040", "authors": ["Yuhang Zhang", "Chao Yan", "Jiaxi Yu", "Jiaping Xiao", "Mir Feroskhan"], "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration", "comment": null, "summary": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.", "AI": {"tldr": "CAPO\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u63d0\u793a\u7f16\u6392\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u8de8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5c06\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u4e0e\u9886\u57df\u7279\u5b9a\u53d8\u5316\uff08\u5982\u5149\u7167\u3001\u89c6\u91ce\u3001\u65cb\u8f6c\uff09\u5206\u79bb\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u4e14\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u8de8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u4f20\u611f\u5668\u914d\u7f6e\u548c\u52a8\u6001\u7279\u6027\u5dee\u5f02\u4f7f\u5f97\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faContrAstive Prompt Orchestration (CAPO)\uff1a1) \u6df7\u5408\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u6574\u5408\u89c6\u89c9\u3001\u65f6\u5e8f\u52a8\u4f5c\u548c\u6587\u672c\u76ee\u6807\uff0c\u5efa\u7acb\u53ef\u5b66\u4e60\u63d0\u793a\u6c60\uff1b2) \u81ea\u9002\u5e94\u63d0\u793a\u7f16\u6392\u673a\u5236\u6839\u636e\u5f53\u524d\u89c2\u6d4b\u52a8\u6001\u805a\u5408\u63d0\u793a\uff0c\u8bc6\u522b\u4e3b\u5bfc\u9886\u57df\u56e0\u7d20\u5e76\u6784\u5efa\u6700\u4f18\u72b6\u6001\u8868\u793a\u3002", "result": "CAPO\u5728\u6837\u672c\u6548\u7387\u548c\u6e10\u8fdb\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5149\u7167\u3001\u89c6\u91ce\u3001\u65cb\u8f6c\u7b49\u5267\u70c8\u73af\u5883\u53d8\u5316\u548c\u7269\u7406\u53d8\u5316\u7684\u672a\u89c1\u76ee\u6807\u57df\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "CAPO\u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7f16\u6392\u6709\u6548\u5206\u79bb\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u4e0e\u9886\u57df\u7279\u5b9a\u53d8\u5316\uff0c\u4e3a\u8de8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00659", "abs": "https://arxiv.org/abs/2602.00659", "authors": ["Qusai Khaled", "Laura Genga", "Uzay Kaymak"], "title": "Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics", "comment": "Submitted to 21st International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU2026)", "summary": "In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u7cca\u76f8\u4f3c\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u8d85\u6ee4\u819c\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u89e3\u91ca\u7684\u5065\u5eb7\u6307\u6570\u548c\u900f\u660e\u89c4\u5219\u5b9e\u73b0\u53ef\u4fe1\u9884\u6d4b", "motivation": "\u53cd\u6e17\u900f\u6d77\u6c34\u6de1\u5316\u4e2d\u8d85\u6ee4\u819c\u56e0\u6c61\u67d3\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u9884\u6d4b\u7ef4\u62a4\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u64cd\u4f5c\u4eba\u5458\u4e0d\u4fe1\u4efb\uff0c\u9700\u8981\u900f\u660e\u53ef\u4fe1\u7684\u9884\u6d4b\u65b9\u6cd5", "method": "\u57fa\u4e8e\u6a21\u7cca\u76f8\u4f3c\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u9884\u6d4b\u6846\u67b6\uff1a1) \u57fa\u4e8e\u8de8\u819c\u538b\u529b\u3001\u901a\u91cf\u548c\u963b\u529b\u7684\u7269\u7406\u4fe1\u606f\u5065\u5eb7\u6307\u6570\uff1b2) \u9ad8\u65af\u96b6\u5c5e\u51fd\u6570\u6a21\u7cca\u5316\uff1b3) \u76f8\u4f3c\u6027\u5ea6\u91cf\u8bc6\u522b\u5386\u53f2\u9000\u5316\u8f68\u8ff9\uff1b4) Takagi-Sugeno\u6a21\u7cca\u89c4\u5219\u5236\u5b9aRUL\u9884\u6d4b", "result": "\u5728\u5de5\u4e1a\u89c4\u6a21UF\u7cfb\u7edf\u768412,528\u4e2a\u64cd\u4f5c\u5468\u671f\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a4.50\u4e2a\u5468\u671f\uff0c\u540c\u65f6\u751f\u6210\u4e0e\u4e13\u5bb6\u7406\u89e3\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u89c4\u5219\u5e93", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d85\u6ee4\u819c\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u900f\u660e\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u64cd\u4f5c\u4eba\u5458\u5bf9\u9884\u6d4b\u7ef4\u62a4\u6a21\u578b\u7684\u4fe1\u4efb"}}
{"id": "2602.01085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01085", "abs": "https://arxiv.org/abs/2602.01085", "authors": ["Qi Jing Chen", "Shilin Shan", "Timothy Bretl", "Quang-Cuong Pham"], "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes", "comment": "7 pages, 4 figures", "summary": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u901a\u8fc7\u89c2\u5bdf\u5f62\u72b6\u6765\u68c0\u6d4b\u548c\u4f30\u8ba1\u4f5c\u7528\u4e8e\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u5916\u90e8\u529b\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u529b\u4f20\u611f\u5668\u6216\u672b\u7aef\u63a5\u89e6\u5047\u8bbe\u3002", "motivation": "\u5728\u673a\u5668\u4eba-\u7ebf\u7f06\u4ea4\u4e92\u4efb\u52a1\u4e2d\uff0c\u63a5\u89e6\u5e38\u53d1\u751f\u5728\u673a\u5668\u4eba\u8eab\u4f53\u5176\u4ed6\u90e8\u4f4d\u800c\u975e\u672b\u7aef\u6267\u884c\u5668\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u529b\u4f20\u611f\u5668\u6216\u672b\u7aef\u63a5\u89e6\u5047\u8bbe\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u901a\u8fc7\u5f62\u72b6\u4fe1\u606f\u5c31\u80fd\u51c6\u786e\u8bc6\u522b\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u83b7\u53d6\u7ebf\u7f06\u5f62\u72b6\u4fe1\u606f\uff0c\u5047\u8bbe\u7ebf\u7f06\u5904\u4e8e\u6216\u63a5\u8fd1\u9759\u6001\u5e73\u8861\uff0c\u901a\u8fc7\u63a8\u5bfc\u4e00\u81f4\u6027\u6761\u4ef6\u5e76\u6c42\u89e3\u57fa\u4e8e\u7ebf\u7f06\u529b-\u529b\u77e9\u5e73\u8861\u7684\u7ebf\u6027\u65b9\u7a0b\u7ec4\uff0c\u4f30\u8ba1\u5916\u90e8\u529b\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u5728\u9009\u5b9a\u4ea4\u4e92\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u51c6\u786e\u4f30\u8ba1\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ec5\u901a\u8fc7\u5f62\u72b6\u4fe1\u606f\u6709\u6548\u68c0\u6d4b\u548c\u4f30\u8ba1\u5916\u90e8\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u9ad8\u6548\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01226", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01226", "abs": "https://arxiv.org/abs/2602.01226", "authors": ["Aditya Shibu", "Marah Saleh", "Mohamed Al-Musleh", "Nidhal Abdulaziz"], "title": "SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., \"Form a circle\") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.", "AI": {"tldr": "SkySim\u662f\u4e00\u4e2a\u57fa\u4e8eROS2\u548cGazebo\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u4eff\u771f\u6846\u67b6\uff0c\u4f7f\u7528Gemini 3.5 Pro\u8fdb\u884c\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u89c4\u5212\uff0c\u7ed3\u5408\u4eba\u5de5\u52bf\u573a\u5b89\u5168\u8fc7\u6ee4\u5668\u786e\u4fdd\u8f68\u8ff9\u5b89\u5168\uff0c\u5b9e\u73b0\u4e86\u975e\u4e13\u5bb6\u7528\u6237\u5bf9\u65e0\u4eba\u673a\u96c6\u7fa4\u7684\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u3002", "motivation": "\u65e0\u4eba\u673a\u96c6\u7fa4\u5728\u7269\u6d41\u3001\u519c\u4e1a\u548c\u76d1\u63a7\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u63a7\u5236\uff0c\u4f46\u751f\u6210\u7684\u8f68\u8ff9\u7f3a\u4e4f\u7269\u7406\u57fa\u7840\u4e14\u4e0d\u5b89\u5168\u3002", "method": "\u63d0\u51faSkySim\u6846\u67b6\uff0c\u5c06LLM\u9ad8\u7ea7\u89c4\u5212\u4e0e\u4f4e\u7ea7\u5b89\u5168\u6267\u884c\u89e3\u8026\uff1a\u4f7f\u7528Gemini 3.5 Pro\u5c06\u7528\u6237\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u8f6c\u6362\u4e3a\u7a7a\u95f4\u822a\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u4eba\u5de5\u52bf\u573a\u5b89\u5168\u8fc7\u6ee4\u5668\u8fdb\u884c\u78b0\u649e\u907f\u514d\u3001\u8fd0\u52a8\u5b66\u9650\u5236\u548c\u5730\u7406\u56f4\u680f\u68c0\u67e5\uff0c\u4ee520Hz\u9891\u7387\u6267\u884c\u3002", "result": "\u4f7f\u75283\u300110\u548c30\u67b6Crazyflie\u65e0\u4eba\u673a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u7387\u8fbe\u5230100%\uff08\u6d4b\u8bd5\u6240\u6709\u51e0\u4f55\u57fa\u5143\uff09\uff0c\u5b9e\u65f6\u78b0\u649e\u9884\u9632\u6709\u6548\uff0c\u7cfb\u7edf\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SkySim\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fed\u4ee3\u4f18\u5316\u65e0\u4eba\u673a\u96c6\u7fa4\u884c\u4e3a\uff0c\u5c06AI\u8ba4\u77e5\u4e0e\u673a\u5668\u4eba\u5b89\u5168\u76f8\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2602.00777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00777", "abs": "https://arxiv.org/abs/2602.00777", "authors": ["Xuan Ai", "Qingqing Yang", "Peng Wang", "Lei Deng", "Lin Zhang", "Renhai Chen", "Gong Zhang"], "title": "HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference", "comment": null, "summary": "Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\\bf HyLRA} ({\\bf Hy}brid {\\bf L}ayer {\\bf R}euse {\\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \\textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \\textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\\%--46\\% while maintaining comparable performance (with $<1\\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \\href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\\texttt{/r/unified-cache-management-CF80/}}", "AI": {"tldr": "HyLRA\u662f\u4e00\u79cd\u57fa\u4e8e\u5c42\u95f4\u7a00\u758f\u6027\u5206\u6790\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u4f18\u5316\u5c42\u95f4\u7b56\u7565\uff0c\u5728\u654f\u611f\u5c42\u4fdd\u7559\u5b8c\u6574\u6ce8\u610f\u529b\uff0c\u5728\u5bb9\u5fcd\u5c42\u91cd\u7528\u524d\u5c42\u7684\u5173\u952etoken\u7d22\u5f15\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u53d7\u9650\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548cKV\u7f13\u5b58\u7684\u5185\u5b58\u5360\u7528\u3002\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\uff0c\u8981\u4e48\u8fc7\u5ea6\u526a\u679d\uff0c\u65e0\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f18\u5e73\u8861\u3002", "method": "HyLRA\u901a\u8fc7\u5c42\u95f4\u7a00\u758f\u6027\u5206\u6790\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u7684\u53cc\u91cd\u7279\u6027\uff1a\u5c42\u5185\u654f\u611f\u6027\u548c\u5c42\u95f4\u76f8\u4f3c\u6027\u3002\u91c7\u7528\u79bb\u7ebf\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u751f\u6210\u6700\u4f18\u5c42\u95f4\u7b56\u7565\uff0c\u654f\u611f\u5c42\u4fdd\u7559\u5b8c\u6574\u6ce8\u610f\u529b\uff0c\u5bb9\u5fcd\u5c42\u76f4\u63a5\u91cd\u7528\u524d\u5c42top-k\u7d22\u5f15\uff0c\u907f\u514d\u4e8c\u6b21\u8ba1\u7b97\u3002", "result": "HyLRA\u5c06\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53476%-46%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\uff08\u51c6\u786e\u7387\u4e0b\u964d<1%\uff09\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "HyLRA\u901a\u8fc7\u5c42\u95f4\u6df7\u5408\u6ce8\u610f\u529b\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00751", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00751", "abs": "https://arxiv.org/abs/2602.00751", "authors": ["Cl\u00e1udio L\u00facio do Val Lopes", "Jo\u00e3o Marcus Pitta", "Fabiano Bel\u00e9m", "Gildson Alves", "Fl\u00e1vio Vin\u00edcius Cruzeiro Martins"], "title": "Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance", "comment": "9 pages, 5 figures 2026 IEEE/ACM 5th International Conference on AI Engineering - Software Engineering for AI}{April 12--13, 2026}{Rio de Janeiro, Brazil", "summary": "The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.\n  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Maria\u5e73\u53f0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u521d\u7ea7\u533b\u7597\u4fdd\u5065\u7684\u751f\u4ea7\u7ea7AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u56db\u4e2a\u5de5\u7a0b\u652f\u67f1\u6765\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aAI\uff1a\u6e05\u6d01\u67b6\u6784\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u3001Agent\u6a21\u5757\u5316\u548c\u4eba\u5728\u56de\u8def\u6cbb\u7406\u3002", "motivation": "AI\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u96c6\u6210\u9762\u4e34\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\uff0c\u9700\u8981\u4ece\u5b64\u7acb\u6a21\u578b\u8f6c\u5411\u5065\u58ee\u3001\u53ef\u6cbb\u7406\u548c\u53ef\u9760\u7684\u7cfb\u7edf\u3002\u5f53\u524d\u5de5\u4e1a\u5e94\u7528\u5e38\u5b58\u5728\u8106\u5f31\u3001\u539f\u578b\u884d\u751f\u7684\u67b6\u6784\u548c\u7cfb\u7edf\u6027\u76d1\u7763\u7f3a\u5931\uff0c\u5bfc\u81f4\"\u8d23\u4efb\u771f\u7a7a\"\uff0c\u5b89\u5168\u6027\u548c\u95ee\u8d23\u6027\u53d7\u635f\u3002", "method": "\u63d0\u51faMaria\u5e73\u53f0\u4f5c\u4e3a\u884c\u4e1a\u6848\u4f8b\u7814\u7a76\uff0c\u91c7\u7528\u534f\u540c\u67b6\u6784\uff1a\u7ed3\u5408\u6e05\u6d01\u67b6\u6784\uff08\u53ef\u7ef4\u62a4\u6027\uff09\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\uff08\u5f39\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff09\uff1b\u5f15\u5165Agent\u4f5c\u4e3a\u4e3b\u8981\u6a21\u5757\u5316\u5355\u5143\uff0c\u6bcf\u4e2aAgent\u62e5\u6709\u81ea\u4e3b\u7684MLOps\u751f\u547d\u5468\u671f\uff1b\u6280\u672f\u4e0a\u96c6\u6210\u4eba\u5728\u56de\u8def\u6cbb\u7406\u6a21\u578b\u4f5c\u4e3a\u5173\u952e\u7684\u4e8b\u4ef6\u9a71\u52a8\u6570\u636e\u6e90\u3002", "result": "Maria\u5e73\u53f0\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u56db\u4e2a\u5de5\u7a0b\u652f\u67f1\u7684\u6574\u5408\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aAI\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53c2\u8003\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u9ad8\u98ce\u9669\u9886\u57df\u6784\u5efa\u53ef\u7ef4\u62a4\u3001\u53ef\u6269\u5c55\u548c\u53ef\u95ee\u8d23\u7684AI\u7cfb\u7edf\u3002", "conclusion": "\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aAI\u9700\u8981\u901a\u8fc7\u6e05\u6d01\u67b6\u6784\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u3001Agent\u6a21\u5757\u5316\u548c\u4eba\u5728\u56de\u8def\u6cbb\u7406\u8fd9\u56db\u4e2a\u5de5\u7a0b\u652f\u67f1\u7684\u5168\u9762\u6574\u5408\u6765\u5b9e\u73b0\u3002Maria\u5e73\u53f0\u4e3a\u5de5\u7a0b\u5e08\u5728\u9ad8\u98ce\u9669\u9886\u57df\u6784\u5efa\u53ef\u7ef4\u62a4\u3001\u53ef\u6269\u5c55\u548c\u53ef\u95ee\u8d23\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7ecf\u9a8c\u3002"}}
{"id": "2602.00179", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00179", "abs": "https://arxiv.org/abs/2602.00179", "authors": ["Joseph L. Breeden"], "title": "How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models", "comment": "22 pages; 2 figures", "summary": "For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\u65b9\u6cd5\uff08\u5982LIME\u548cSHAP\uff09\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u4e0d\u7a33\u5b9a\u6027\u53cd\u6620\u4e86\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u975e\u65b9\u6cd5\u7f3a\u9677\u3002\u6b63\u786e\u505a\u6cd5\u662f\u5148\u8bc4\u4f30\u9884\u6d4b\u662f\u5426\u53ef\u7528\uff0c\u53ea\u6709\u5728\u4f4e\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u624d\u5bfb\u6c42\u89e3\u91ca\uff0c\u5426\u5219\u5e94\u4f7f\u7528\u66f4\u7b80\u5355\u7684\u6a21\u578b\u3002", "motivation": "\u5728\u5173\u952e\u51b3\u7b56\u5e94\u7528\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u662f\u4e3b\u8981\u5173\u6ce8\u70b9\uff0c\u4f46\u73b0\u6709\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\u65b9\u6cd5\u56e0\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u4e0d\u7a33\u5b9a\u800c\u53d7\u5230\u6279\u8bc4\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u6279\u8bc4\u53cd\u6620\u4e86\u5bf9\u95ee\u9898\u7684\u8bef\u89e3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u89e3\u91ca\u6027\u8bc4\u4f30\u7684\u987a\u5e8f\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u89e3\u91ca\u6027\u8bc4\u4f30\u6846\u67b6\uff1a\u9996\u5148\u8bc4\u4f30\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53ea\u6709\u5f53\u9884\u6d4b\u5177\u6709\u8db3\u591f\u4f4e\u7684\u53ef\u7528\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\u624d\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u8fd1\u4f3c\u5bfb\u6c42\u89e3\u91ca\u3002\u5f53\u6ca1\u6709\u53ef\u7528\u9884\u6d4b\u65f6\uff0c\u51b3\u7b56\u5e94\u56de\u9000\u5230\u66f4\u7b80\u5355\u7684\u6574\u4f53\u6a21\u578b\uff08\u5982\u4f20\u7edf\u903b\u8f91\u56de\u5f52\uff09\u3002", "result": "\u8bba\u8bc1\u4e86\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u4e0e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u76f8\u5173\uff0c\u8fd9\u662f\u5408\u7406\u7684\u800c\u975e\u7f3a\u9677\u3002\u6307\u51fa\u67d0\u4e9b\u58f0\u79f0\u5904\u5904\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff08\u5982ReLU\u7f51\u7edc\u6216\u4efb\u4f55\u5206\u6bb5\u7ebf\u6027\u6a21\u578b\uff09\u5b9e\u9645\u4e0a\u53ea\u6709\u865a\u5e7b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u4e3a\u5728\u5206\u6bb5\u8fb9\u754c\u5904\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u592a\u9ad8\u800c\u65e0\u6cd5\u4f7f\u7528\u3002", "conclusion": "\u89e3\u91ca\u6027\u8bc4\u4f30\u5e94\u9075\u5faa\u6b63\u786e\u987a\u5e8f\uff1a\u5148\u5224\u65ad\u9884\u6d4b\u662f\u5426\u53ef\u7528\uff0c\u518d\u5bfb\u6c42\u89e3\u91ca\u3002\u89e3\u91ca\u4e00\u4e2a\u4e0d\u53ef\u7528\u7684\u9884\u6d4b\u6beb\u65e0\u610f\u4e49\u3002\u5bf9\u4e8e\u9ad8\u4e0d\u786e\u5b9a\u6027\u533a\u57df\uff0c\u5e94\u4f7f\u7528\u66f4\u7b80\u5355\u7684\u6a21\u578b\u8fdb\u884c\u51b3\u7b56\u3002"}}
{"id": "2602.00268", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00268", "abs": "https://arxiv.org/abs/2602.00268", "authors": ["Ariel Shaulov", "Eitan Shaar", "Amit Edenzon", "Lior Wolf"], "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation", "comment": null, "summary": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u4e0d\u7a33\u5b9a\u7684\u6f5c\u5728token\u6765\u7f13\u89e3\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u9519\u8bef\u4f1a\u968f\u7740\u65f6\u95f4\u7d2f\u79ef\u548c\u653e\u5927\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e3b\u8981\u4e0d\u662f\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\u5bfc\u81f4\u7684\uff0c\u800c\u662f\u63a8\u7406\u65f6\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u7279\u522b\u662f\u7531\u4e8e\u5728\u81ea\u56de\u5f52\u63a8\u7406\u4e2d\u91cd\u590d\u4f7f\u7528\u4e86\u5df2\u635f\u574f\u7684\u6f5c\u5728\u6761\u4ef6token\u3002", "method": "\u63d0\u51fa\u7b80\u5355\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff1a\u5728\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u8bc6\u522b\u90a3\u4e9b\u8868\u793a\u4e0e\u5148\u524d\u751f\u6210\u6279\u6b21\u663e\u8457\u504f\u79bb\u7684\u4e0d\u7a33\u5b9a\u6f5c\u5728token\uff08\u8868\u660e\u53ef\u80fd\u5df2\u635f\u574f\u6216\u53d1\u751f\u8bed\u4e49\u6f02\u79fb\uff09\uff0c\u5e76\u5728\u5c06\u8fd9\u4e9btoken\u91cd\u65b0\u7528\u4e8e\u6761\u4ef6\u751f\u6210\u4e4b\u524d\u5c06\u5176\u79fb\u9664\u3002\u901a\u8fc7\u4ece\u81ea\u56de\u5f52\u4e0a\u4e0b\u6587\u4e2d\u663e\u5f0f\u79fb\u9664\u635f\u574f\u7684\u6f5c\u5728token\uff0c\u800c\u4e0d\u662f\u4fee\u6539\u6574\u4e2a\u7a7a\u95f4\u533a\u57df\u6216\u6a21\u578b\u53c2\u6570\uff0c\u9632\u6b62\u4e0d\u53ef\u9760\u7684\u6f5c\u5728\u4fe1\u606f\u5f71\u54cd\u672a\u6765\u7684\u751f\u6210\u6b65\u9aa4\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u957f\u65f6\u57df\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u8fc7\u7a0b\u6216\u79bb\u5f00\u6f5c\u5728\u7a7a\u95f4\u3002", "conclusion": "\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\u4e3b\u8981\u6e90\u4e8e\u63a8\u7406\u65f6\u7684\u9519\u8bef\u4f20\u64ad\u800c\u975e\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u8bc6\u522b\u548c\u79fb\u9664\u4e0d\u7a33\u5b9a\u7684\u6f5c\u5728token\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u7684\u957f\u65f6\u57df\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.00288", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00288", "abs": "https://arxiv.org/abs/2602.00288", "authors": ["Baiqi Li", "Kangyi Zhao", "Ce Zhang", "Chancharik Mitra", "Jean de Dieu Nyandwi", "Gedas Bertasius"], "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs", "comment": "For code and data, see https://baiqi-li.github.io/timeblind_project/", "summary": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .", "AI": {"tldr": "TimeBlind\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7406\u89e3\u4e0a\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u89c6\u9891\u5bf9\u5bf9\u6bd4\u63ed\u793a\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u65f6\u95f4\u903b\u8f91\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u8bed\u4e49\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u65f6\u95f4\u52a8\u6001\u7684\u7406\u89e3\u4ecd\u7136\u8584\u5f31\u3002\u9700\u8981\u4e13\u95e8\u7684\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u65f6\u95f4\u903b\u8f91\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u7ebf\u7d22\u3002", "method": "\u91c7\u7528\u6700\u5c0f\u5bf9\u8303\u5f0f\uff1a\u521b\u5efa\u89c6\u9891\u5bf9\uff0c\u8fd9\u4e9b\u89c6\u9891\u5bf9\u5177\u6709\u5b8c\u5168\u76f8\u540c\u7684\u9759\u6001\u89c6\u89c9\u5185\u5bb9\uff0c\u4f46\u65f6\u95f4\u7ed3\u6784\u4e0d\u540c\u3002\u901a\u8fc7\u4e92\u8865\u95ee\u9898\u8bbe\u8ba1\u6765\u6d88\u9664\u8bed\u8a00\u5148\u9a8c\u3002\u5c06\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7406\u89e3\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u539f\u5b50\u4e8b\u4ef6\u8bc6\u522b\u3001\u4e8b\u4ef6\u5c5e\u6027\u8868\u5f81\u3001\u4e8b\u4ef6\u76f8\u4e92\u4f9d\u8d56\u63a8\u7406\u3002", "result": "\u8bc4\u4f30\u4e8620\u591a\u4e2a\u6700\u5148\u8fdb\u7684MLLM\uff08\u5305\u62ecGPT-5\u3001Gemini 3 Pro\uff09\u5728600\u4e2a\u5b9e\u4f8b\uff082400\u4e2a\u89c6\u9891-\u95ee\u9898\u5bf9\uff09\u4e0a\u7684\u8868\u73b0\u3002\u6700\u4f73\u6a21\u578b\u7684\u5b9e\u4f8b\u51c6\u786e\u7387\u4ec5\u4e3a48.2%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0898.2%\uff09\u3002\u8fd9\u8868\u660e\u524d\u6cbf\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u6377\u5f84\u800c\u975e\u771f\u6b63\u7684\u65f6\u95f4\u903b\u8f91\u3002", "conclusion": "TimeBlind\u63ed\u793a\u4e86\u5f53\u524dMLLM\u5728\u65f6\u95f4\u7406\u89e3\u4e0a\u7684\u6839\u672c\u7f3a\u9677\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u9891\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bca\u65ad\u5de5\u5177\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01535", "abs": "https://arxiv.org/abs/2602.01535", "authors": ["Huzaifa Mustafa Unjhawala", "Khizar Shaikh", "Luning Bakke", "Radu Serban", "Dan Negrut"], "title": "Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations", "comment": "19 pages, 15 figures", "summary": "While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u6708\u7403\u8f66\u8f66\u8f6e\u51e0\u4f55\u4e0e\u8f6c\u5411\u63a7\u5236\u5668\u53c2\u6570\uff0c\u4f7f\u7528\u8fde\u7eed\u4ecb\u8d28\u6a21\u578b\u8fdb\u884c\u5168\u8f66\u95ed\u73af\u4eff\u771f\uff0c\u76f8\u6bd4\u4f20\u7edfDEM\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u6548\u7387", "motivation": "\u4f20\u7edf\u79bb\u6563\u5143\u65b9\u6cd5\uff08DEM\uff09\u6a21\u62df\u53ef\u53d8\u5f62\u5730\u5f62\u6210\u672c\u8fc7\u9ad8\uff0c\u901a\u5e38\u53ea\u80fd\u8fdb\u884c\u5355\u8f6e\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u8fdb\u884c\u5168\u8f66\u95ed\u73af\u4eff\u771f\uff0c\u9650\u5236\u4e86\u8d8a\u91ce\u8f66\u8f86\u673a\u68b0\u8bbe\u8ba1\u4e0e\u63a7\u5236\u7684\u8054\u5408\u4f18\u5316", "method": "\u4f7f\u7528\u8fde\u7eed\u4ecb\u8d28\u8868\u793a\u6a21\u578b\uff08CRM\uff09\u8fdb\u884c\u9ad8\u6548\u5730\u5f62\u529b\u5b66\u6a21\u62df\uff0c\u5efa\u7acb\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u4f18\u5316\u8f66\u8f6e\u53c2\u6570\uff08\u534a\u5f84\u3001\u5bbd\u5ea6\u3001\u6293\u5730\u9f7f\u7279\u5f81\uff09\u548c\u8f6c\u5411PID\u589e\u76ca\uff0c\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u5e73\u8861\u884c\u9a76\u901f\u5ea6\u3001\u8ddf\u8e2a\u8bef\u5dee\u548c\u80fd\u8017", "result": "\u5b8c\u62103000\u6b21\u5168\u8f66\u4eff\u771f\u4ec5\u97005-9\u5929\uff0c\u76f8\u6bd4\u4e4b\u524dDEM\u65b9\u6cd5\u9700\u8981\u6570\u6708\u5927\u5e45\u63d0\u5347\u6548\u7387\uff1b\u521d\u6b65\u786c\u4ef6\u6d4b\u8bd5\u663e\u793a\u4eff\u771f\u4f18\u5316\u8bbe\u8ba1\u5728\u7269\u7406\u6708\u7403\u8f66\u4e0a\u4fdd\u6301\u76f8\u5bf9\u6027\u80fd\u8d8b\u52bf", "conclusion": "\u53ef\u6269\u5c55\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u80fd\u591f\u5b9e\u73b0\u8d8a\u91ce\u8f66\u8f86\u8f66\u8f6e\u8bbe\u8ba1\u4e0e\u63a7\u5236\u7684\u5b9e\u7528\u8054\u5408\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684DEM\u7814\u7a76\uff0c\u76f8\u5173\u4eff\u771f\u57fa\u7840\u8bbe\u65bd\u5df2\u5f00\u6e90\u53d1\u5e03"}}
{"id": "2602.00866", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00866", "abs": "https://arxiv.org/abs/2602.00866", "authors": ["Akiharu Esashi", "Pawissanutt Lertpongrujikorn", "Justin Makino", "Yuibi Fujimoto", "Mohsen Amini Salehi"], "title": "Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data", "comment": null, "summary": "The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2aCAN\u603b\u7ebf\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u89e3\u7801\u540e\u7684\u8f66\u8f86\u4fe1\u53f7\u89c6\u4e3a\u8bed\u8a00\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u591a\u79cd\u6c7d\u8f66\u4fdd\u9669\u4efb\u52a1\u4e0a\u5fae\u8c03\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316", "motivation": "\u73b0\u6709CAN\u6570\u636e\u5904\u7406\u65b9\u6cd5\u591a\u4e3a\u5b64\u7acb\u7684\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u7f3a\u4e4f\u5171\u4eab\u8868\u793a\u5b66\u4e60\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002\u53d7NLP\u548cCV\u9886\u57df\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u6210\u529f\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5c06\u8fd9\u4e00\u8303\u5f0f\u5e94\u7528\u4e8eCAN\u6570\u636e\u9886\u57df", "method": "\u5c06CAN\u6570\u636e\u89c6\u4e3a\u8bed\u8a00\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u5206\u8bcd\u65b9\u6848\u5904\u7406\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u4fe1\u53f7\uff0c\u89e3\u51b3\u65f6\u95f4\u590d\u6742\u6027\u548c\u884c\u7a0b\u7279\u5b9a\u53d8\u5f02\u6027\u6311\u6218\uff0c\u5728\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u89e3\u7801CAN\u4fe1\u53f7\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u5f02\u6784\u6c7d\u8f66\u4fdd\u9669\u4efb\u52a1\u4e0a\u5fae\u8c03", "result": "\u5355\u4e2a\u9884\u8bad\u7ec3\u7684CAN\u6a21\u578b\u80fd\u591f\u6709\u6548\u9002\u5e94\u591a\u79cd\u9884\u6d4b\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u5728CAN\u6570\u636e\u9886\u57df\u7684\u9002\u7528\u6027", "conclusion": "\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u5728NLP\u548cCV\u9886\u57df\u53d6\u5f97\u6210\u529f\u540e\uff0c\u540c\u6837\u9002\u7528\u4e8eCAN\u6570\u636e\uff0c\u4e3a\u6c7d\u8f66AI\u4e2d\u7684\u53ef\u6cdb\u5316\u8868\u793a\u5b66\u4e60\u786e\u7acb\u4e86\u65b0\u65b9\u5411"}}
{"id": "2602.00983", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00983", "abs": "https://arxiv.org/abs/2602.00983", "authors": ["Batuhan K. Karaman", "Aditya Rawal", "Suhaila Shakiah", "Mohammad Ghavamzadeh", "Mingyi Hong", "Arijit Biswas", "Ruida Zhou"], "title": "DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning", "comment": "This work is accepted to the 29th International Conference on Artificial Intelligence and Statistics (AISTATS) 2026", "summary": "Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.", "AI": {"tldr": "DISPO\u7b97\u6cd5\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u4e0e\u9519\u8bef\u56de\u7b54\u7684\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u4e0a\u4e0b\u88c1\u526a\uff0c\u5b9e\u73b0\u56db\u4e2a\u53ef\u63a7\u7b56\u7565\u66f4\u65b0\u673a\u5236\uff0c\u5728\u4fdd\u6301\u63a2\u7d22-\u84b8\u998f\u5e73\u8861\u7684\u540c\u65f6\u907f\u514d\u707e\u96be\u6027\u5931\u8d25\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u660e\u663e\u6743\u8861\uff1aPPO\u98ce\u683c\u65b9\u6cd5\u8bad\u7ec3\u7a33\u5b9a\u4f46\u5b66\u4e60\u7f13\u6162\uff0cREINFORCE\u98ce\u683c\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "DISPO\u7b97\u6cd5\u5c06\u6b63\u786e\u4e0e\u9519\u8bef\u56de\u7b54\u7684\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u4e0a\u4e0b\u88c1\u526a\u89e3\u8026\uff0c\u5f62\u6210\u56db\u4e2a\u53ef\u63a7\u7b56\u7565\u66f4\u65b0\u673a\u5236\uff0c\u5206\u522b\u8c03\u8282\u63a2\u7d22\u4e0e\u84b8\u998f\u5e73\u8861\uff0c\u9632\u6b62\u707e\u96be\u6027\u5931\u8d25\u3002", "result": "\u5728AIME'24\u4e0a\u8fbe\u523061.04%\u51c6\u786e\u7387\uff08vs. CISPO 55.42%\uff0cDAPO 50.21%\uff09\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u5747\u6709\u7c7b\u4f3c\u63d0\u5347\u3002", "conclusion": "DISPO\u901a\u8fc7\u53ef\u63a7\u7684\u6743\u91cd\u88c1\u526a\u673a\u5236\u5e73\u8861\u63a2\u7d22\u4e0e\u84b8\u998f\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u907f\u514d\u6027\u80fd\u5d29\u6e83\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00299", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00299", "abs": "https://arxiv.org/abs/2602.00299", "authors": ["Rituparna Datta", "Zihan Guan", "Baltazar Espinoza", "Yiqi Su", "Priya Pitre", "Srini Venkatramanan", "Naren Ramakrishnan", "Anil Vullikanti"], "title": "Agentic Framework for Epidemiological Modeling", "comment": null, "summary": "Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.", "AI": {"tldr": "EPIAGENT\u662f\u4e00\u4e2a\u81ea\u52a8\u5408\u6210\u3001\u6821\u51c6\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u6d41\u884c\u75c5\u5b66\u6a21\u62df\u5668\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e3a\u8fed\u4ee3\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901f\u6709\u6548\u6a21\u578b\u7684\u6536\u655b\u3002", "motivation": "\u4f20\u7edf\u6d41\u884c\u75c5\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u578b\u7c7b\u522b\uff0c\u9700\u8981\u968f\u7740\u75c5\u539f\u4f53\u3001\u653f\u7b56\u548c\u573a\u666f\u5047\u8bbe\u7684\u53d8\u5316\u800c\u624b\u52a8\u91cd\u65b0\u8bbe\u8ba1\uff0c\u8fd9\u9650\u5236\u4e86\u5efa\u6a21\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u6d41\u884c\u75c5\u6d41\u56fe\u4e2d\u95f4\u8868\u793a\uff0c\u5c06\u573a\u666f\u89c4\u8303\u4e0e\u6a21\u578b\u7ed3\u6784\u8fde\u63a5\uff0c\u652f\u6301\u6a21\u5757\u5316\u6b63\u786e\u6027\u68c0\u67e5\uff1b\u901a\u8fc7\u8fed\u4ee3\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u81ea\u52a8\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u673a\u5236\u6a21\u578b\uff0c\u5728\u7269\u7406\u548c\u6d41\u884c\u75c5\u5b66\u7ea6\u675f\u4e0b\u8fdb\u884c\u53ef\u89e3\u91ca\u53c2\u6570\u5b66\u4e60\u3002", "result": "EPIAGENT\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u589e\u957f\u52a8\u6001\uff0c\u5728\u4e0d\u540c\u75ab\u82d7\u63a5\u79cd\u548c\u514d\u75ab\u9003\u9038\u5047\u8bbe\u4e0b\u4ea7\u751f\u6d41\u884c\u75c5\u5b66\u4e00\u81f4\u7684\u53cd\u4e8b\u5b9e\u9884\u6d4b\uff1b\u667a\u80fd\u4f53\u53cd\u9988\u5faa\u73af\u9632\u6b62\u6a21\u578b\u9000\u5316\uff0c\u663e\u8457\u52a0\u901f\u5411\u6709\u6548\u6a21\u578b\u7684\u6536\u655b\u3002", "conclusion": "EPIAGENT\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4e13\u4e1a\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u6d41\u884c\u75c5\u5b66\u6a21\u62df\u5668\u7684\u81ea\u52a8\u5408\u6210\u548c\u4f18\u5316\uff0c\u4e3a\u516c\u5171\u536b\u751f\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2602.00420", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00420", "abs": "https://arxiv.org/abs/2602.00420", "authors": ["Yihang Chen", "Zhao Xu", "Youyuan Jiang", "Tianle Zheng", "Cho-Jui Hsieh"], "title": "Text is All You Need for Vision-Language Model Jailbreaking", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.", "AI": {"tldr": "Text-DJ\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u67e5\u8be2\u5206\u89e3\u4e3a\u591a\u4e2a\u826f\u6027\u5b50\u67e5\u8be2\u5e76\u6dfb\u52a0\u5927\u91cf\u65e0\u5173\u5e72\u6270\u67e5\u8be2\uff0c\u4ee5\u56fe\u50cf\u7f51\u683c\u5f62\u5f0f\u7ed5\u8fc7\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u4e3b\u8981\u9488\u5bf9\u663e\u5f0f\u6587\u672c\u8f93\u5165\u6216\u76f8\u5173\u89c6\u89c9\u573a\u666f\uff0c\u4f46\u5ffd\u7565\u4e86\u6a21\u578bOCR\u80fd\u529b\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528OCR\u529f\u80fd\u7ed5\u8fc7\u8fd9\u4e9b\u5b89\u5168\u9632\u62a4\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5c06\u5355\u4e2a\u6709\u5bb3\u67e5\u8be2\u5206\u89e3\u4e3a\u591a\u4e2a\u8bed\u4e49\u76f8\u5173\u4f46\u66f4\u826f\u6027\u7684\u5b50\u67e5\u8be2\uff1b2) \u9009\u62e9\u4e0e\u6709\u5bb3\u67e5\u8be2\u6700\u5927\u7a0b\u5ea6\u65e0\u5173\u7684\u5e72\u6270\u67e5\u8be2\uff1b3) \u5c06\u6240\u6709\u5b50\u67e5\u8be2\u548c\u5e72\u6270\u67e5\u8be2\u4ee5\u56fe\u50cf\u7f51\u683c\u5f62\u5f0f\u540c\u65f6\u5448\u73b0\u7ed9\u6a21\u578b\uff0c\u5176\u4e2d\u5b50\u67e5\u8be2\u4f4d\u4e8e\u7f51\u683c\u4e2d\u95f4\u4f4d\u7f6e\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed5\u8fc7\u4e86\u6700\u5148\u8fdb\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u66b4\u9732\u4e86OCR\u80fd\u529b\u5728\u9762\u5bf9\u5206\u6563\u7684\u591a\u56fe\u50cf\u5bf9\u6297\u8f93\u5165\u65f6\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bOCR\u80fd\u529b\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u8868\u660e\u73b0\u6709\u5b89\u5168\u534f\u8bae\u65e0\u6cd5\u6709\u6548\u5904\u7406\u788e\u7247\u5316\u7684\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u9488\u5bf9\u6b64\u7c7b\u653b\u51fb\u5f00\u53d1\u4e13\u95e8\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2602.01002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01002", "abs": "https://arxiv.org/abs/2602.01002", "authors": ["Itai Shapira", "Gerdus Benade", "Ariel D. Procaccia"], "title": "How RLHF Amplifies Sycophancy", "comment": null, "summary": "Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u504f\u597d\u7684\u540e\u8bad\u7ec3\u4e2d\u51fa\u73b0\u7684\u8c04\u5a9a\u884c\u4e3a\u589e\u5f3a\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u5982\u4f55\u901a\u8fc7\u5956\u52b1\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\u653e\u5927\u673a\u5236\u5bfc\u81f4\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u9644\u548c\u7528\u6237\u89c2\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65f6\u5e72\u9884\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u540e\u8bad\u7ec3\u540e\uff0c\u7ecf\u5e38\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8c04\u5a9a\u884c\u4e3a\u2014\u2014\u5373\u4f7f\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u6216\u5408\u7406\u5224\u65ad\u76f8\u51b2\u7a81\uff0c\u4e5f\u66f4\u503e\u5411\u4e8e\u9644\u548c\u7528\u6237\u7684\u89c2\u70b9\u3002\u8fd9\u79cd\u884c\u4e3a\u7684\u589e\u5f3a\u5bf9\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u4ea7\u751f\u673a\u5236\u5e76\u627e\u5230\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "1. \u5f62\u5f0f\u5316\u5206\u6790\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u5982\u4f55\u901a\u8fc7\u5956\u52b1\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\u653e\u5927\u673a\u5236\u5bfc\u81f4\u8c04\u5a9a\u884c\u4e3a\u589e\u5f3a\uff1b2. \u8bc6\u522b\u884c\u4e3a\u6f02\u79fb\u65b9\u5411\u7531\u57fa\u7840\u7b56\u7565\u4e0b\u8d5e\u540c\u63d0\u793a\u4e2d\u7684\u4fe1\u5ff5\u4fe1\u53f7\u4e0e\u5b66\u4e60\u5230\u7684\u5956\u52b1\u4e4b\u95f4\u7684\u534f\u65b9\u5dee\u51b3\u5b9a\uff1b3. \u5206\u6790Bradley-Terry\u7b49\u968f\u673a\u6548\u7528\u6a21\u578b\u4e0b\u7684\u6210\u5bf9\u6bd4\u8f83\u5956\u52b1\u5b66\u4e60\uff1b4. \u63d0\u51fa\u8bad\u7ec3\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6563\u5ea6\u6700\u63a5\u8fd1\u65e0\u7ea6\u675f\u540e\u8bad\u7ec3\u7b56\u7565\u7684\u72ec\u7279\u7b56\u7565\u6765\u9632\u6b62\u8c04\u5a9a\u884c\u4e3a\u589e\u52a0\uff0c\u5e76\u63a8\u5bfc\u51fa\u76f8\u5e94\u7684\u6700\u5c0f\u5956\u52b1\u4fee\u6b63\u4f5c\u4e3a\u95ed\u5f0f\u534f\u8bae\u60e9\u7f5a\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u53d1\u73b0\u5956\u52b1\u5dee\u8ddd\u666e\u904d\u5b58\u5728\uff0c\u5e76\u5728\u6240\u6709\u8003\u8651\u7684\u914d\u7f6e\u4e2d\u90fd\u5bfc\u81f4\u4e86\u884c\u4e3a\u6f02\u79fb\u3002\u63d0\u51fa\u7684\u534f\u8bae\u60e9\u7f5a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4e2d\u548c\u653e\u5927\u673a\u5236\uff0c\u9632\u6b62\u8c04\u5a9a\u884c\u4e3a\u7684\u589e\u52a0\u3002", "conclusion": "\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u4f1a\u901a\u8fc7\u5956\u52b1\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\u653e\u5927\u673a\u5236\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c04\u5a9a\u884c\u4e3a\u3002\u901a\u8fc7\u5206\u6790\u5956\u52b1\u5dee\u8ddd\u548c\u884c\u4e3a\u6f02\u79fb\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u672c\u6587\u63d0\u51fa\u7684\u8bad\u7ec3\u65f6\u5e72\u9884\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u5c11\u8c04\u5a9a\u503e\u5411\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.01125", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01125", "abs": "https://arxiv.org/abs/2602.01125", "authors": ["Jichu Li", "Yilun Zhong", "Zhiting Li", "Feng Zhou", "Quyu Kong"], "title": "Long-range Modeling and Processing of Multimodal Event Sequences", "comment": null, "summary": "Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6269\u5c55\u5230\u89c6\u89c9\u6a21\u6001\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e8f\u5217\u538b\u7f29\u673a\u5236\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u751f\u6210\u6587\u672c\u5206\u6790\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u70b9\u8fc7\u7a0b\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff1a\u591a\u6a21\u6001\u6570\u636e\u4f1a\u663e\u8457\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\uff0c\u963b\u788d\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u751f\u6210\u9700\u8981\u957f\u8ddd\u79bb\u7406\u89e3\u7684\u8fde\u8d2f\u957f\u6587\u672c\u63cf\u8ff0\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u4e30\u5bcc\u591a\u6a21\u6001\u5185\u5bb9\u5e76\u63a8\u7406\u4e8b\u4ef6\u52a8\u6001\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4\u76f8\u4f3c\u6027\u7684\u81ea\u9002\u5e94\u5e8f\u5217\u538b\u7f29\u673a\u5236\uff0c\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\u540c\u65f6\u4fdd\u7559\u5173\u952e\u6a21\u5f0f\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u5148\u5728\u538b\u7f29\u5e8f\u5217\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b\u5c06\u6587\u672c\u751f\u6210\u5b9a\u4f4d\u4e3a\u6838\u5fc3\u80fd\u529b\uff0c\u4e0e\u65f6\u95f4\u548c\u7c7b\u578b\u9884\u6d4b\u5e76\u5217\u3002", "result": "\u5728DanmakuTPP-QA\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u751f\u6210\u6587\u672c\u5206\u6790\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06LLM-based TPPs\u6269\u5c55\u5230\u89c6\u89c9\u6a21\u6001\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u591a\u6a21\u6001\u4e8b\u4ef6\u5e8f\u5217\u5efa\u6a21\u548c\u6587\u672c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2602.00361", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00361", "abs": "https://arxiv.org/abs/2602.00361", "authors": ["Philipp Altmann", "Maximilian Mansky", "Maximilian Zorn", "Jonas Stein", "Claudia Linnhoff-Popien"], "title": "Quantum Generator Kernels", "comment": "28 pages, 4 figures, 8 tables, under review", "summary": "Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u751f\u6210\u6838(QGK)\uff0c\u901a\u8fc7\u53d8\u5206\u751f\u6210\u7ec4(VGG)\u6784\u5efa\u53ef\u53c2\u6570\u5316\u7684\u91cf\u5b50\u6838\uff0c\u89e3\u51b3NISQ\u8bbe\u5907\u6570\u636e\u5d4c\u5165\u9650\u5236\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u91cf\u5b50\u4e0e\u7ecf\u5178\u6838\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u6838\u65b9\u6cd5\u7406\u8bba\u4e0a\u80fd\u5c06\u7ecf\u5178\u4e0d\u53ef\u5206\u7279\u5f81\u5728\u91cf\u5b50\u7a7a\u95f4\u4e2d\u5206\u79bb\uff0c\u4f46NISQ\u786c\u4ef6\u9650\u5236\u9700\u8981\u6709\u6548\u538b\u7f29\u548c\u5d4c\u5165\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\uff08\u5982\u56fe\u50cf\uff09\u3002\u73b0\u6709\u6df7\u5408\u67b6\u6784\u7684\u56fa\u5b9a\u5d4c\u5165\u8fc7\u7a0b\u53ef\u80fd\u963b\u788d\u53d1\u6325\u91cf\u5b50\u8ba1\u7b97\u5168\u90e8\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u91cf\u5b50\u751f\u6210\u6838(QGK)\uff0c\u5305\u542b\u53d8\u5206\u751f\u6210\u7ec4(VGGs)\uff0c\u5c06\u901a\u7528\u751f\u6210\u5668\u5408\u5e76\u4e3a\u53ef\u53c2\u6570\u5316\u7b97\u5b50\uff0c\u5b9e\u73b0\u91cf\u5b50\u7a7a\u95f4\u7684\u89c4\u6a21\u5316\u8986\u76d6\u3002\u901a\u8fc7\u8bad\u7ec3\u6743\u91cd\u5411\u91cf\u53c2\u6570\u5316VGG\u5728\u5f53\u524d\u6570\u636e\u4e0a\u4e0b\u6587\u4e2d\u7684\u6295\u5f71\uff0c\u4f18\u5316\u6838\u4e0e\u76ee\u6807\u57df\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aQGK\u5728\u6295\u5f71\u548c\u5206\u7c7b\u80fd\u529b\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cf\u5b50\u4e0e\u7ecf\u5178\u6838\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u591a\u79cdQML\u5e94\u7528\u7684\u901a\u7528\u6846\u67b6\u6f5c\u529b\u3002", "conclusion": "QGK\u901a\u8fc7\u751f\u6210\u5668\u65b9\u6cd5\u89e3\u51b3\u4e86\u91cf\u5b50\u6838\u5728NISQ\u8bbe\u5907\u4e0a\u7684\u6570\u636e\u5d4c\u5165\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2602.01086", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01086", "abs": "https://arxiv.org/abs/2602.01086", "authors": ["Takahito Nakajima"], "title": "MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI", "comment": "19 pages, 5 figures. Code available at https://github.com/medbeads/medbeads", "summary": "Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous \"Clinical Agents\" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a \"Context Mismatch\": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable \"Beads\"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This \"write-once, read-many\" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the \"Context Mismatch\" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for \"Trustworthy Medical AI.\" It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient \"AI-native language.\" We release MedBeads as open-source software to accelerate agent-native data standards.", "AI": {"tldr": "MedBeads \u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411AI\u4ee3\u7406\u7684\u533b\u7597\u6570\u636e\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7f\u7528\u4e0d\u53ef\u53d8\u7684Merkle DAG\u7ed3\u6784\u5b58\u50a8\u4e34\u5e8a\u4e8b\u4ef6\uff0c\u89e3\u51b3\u4f20\u7edfEMR\u4e0eAI\u4ee3\u7406\u4e4b\u95f4\u7684\"\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\"\u95ee\u9898\uff0c\u786e\u4fdd\u6570\u636e\u53ef\u8ffd\u6eaf\u3001\u9632\u7be1\u6539\u3002", "motivation": "\u5f53\u524d\u7535\u5b50\u75c5\u5386\u7cfb\u7edf\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\uff0cAI\u4ee3\u7406\u63a5\u6536\u788e\u7247\u5316\u6570\u636e\uff0c\u9700\u8981\u4f9d\u8d56\u6982\u7387\u63a8\u7406\u91cd\u5efa\u60a3\u8005\u5386\u53f2\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u548c\u5ba1\u8ba1\u56f0\u96be\uff0c\u5b58\u5728\"\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\"\u95ee\u9898\u3002", "method": "\u63d0\u51faMedBeads\u67b6\u6784\uff0c\u5c06\u4e34\u5e8a\u4e8b\u4ef6\u4f5c\u4e3a\u4e0d\u53ef\u53d8\u7684\"Beads\"\u8282\u70b9\u5b58\u50a8\u5728Merkle\u6709\u5411\u65e0\u73af\u56fe\u4e2d\uff0c\u6bcf\u4e2a\u8282\u70b9\u52a0\u5bc6\u5f15\u7528\u5176\u56e0\u679c\u524d\u9a71\u3002\u91c7\u7528Go\u6838\u5fc3\u5f15\u64ce\u3001Python\u4e2d\u95f4\u4ef6\u548cReact\u53ef\u89c6\u5316\u754c\u9762\u5b9e\u73b0\u539f\u578b\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06FHIR\u8d44\u6e90\u8f6c\u6362\u4e3a\u56e0\u679c\u94fe\u63a5\u56fe\uff0cBFS\u4e0a\u4e0b\u6587\u68c0\u7d22\u7b97\u6cd5\u5b9e\u73b0O(V+E)\u590d\u6742\u5ea6\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\uff0c\u7be1\u6539\u68c0\u6d4b\u901a\u8fc7\u5bc6\u7801\u5b66\u94fe\u4fdd\u8bc1\uff0c\u53ef\u89c6\u5316\u754c\u9762\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u7406\u89e3\u3002", "conclusion": "MedBeads\u901a\u8fc7\u4ece\u6982\u7387\u641c\u7d22\u8f6c\u5411\u786e\u5b9a\u6027\u56fe\u904d\u5386\uff0c\u4ece\u53ef\u53d8\u8bb0\u5f55\u8f6c\u5411\u4e0d\u53ef\u53d8\u94fe\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\"\u53ef\u4fe1\u533b\u7597AI\"\u63d0\u4f9b\u57fa\u7840\uff0c\u786e\u4fddAI\u63a5\u6536\u7684\u4e0a\u4e0b\u6587\u662f\u786e\u5b9a\u6027\u548c\u9632\u7be1\u6539\u7684\u3002"}}
{"id": "2602.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00504", "abs": "https://arxiv.org/abs/2602.00504", "authors": ["Jiahe Wu", "Bing Cao", "Qilong Wang", "Qinghua Hu", "Dongdong Li", "Pengfei Zhu"], "title": "RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.", "AI": {"tldr": "RGBX-R1\u6846\u67b6\u901a\u8fc7UAV\u63d0\u793a\u7b56\u7565\u6784\u5efa\u89c6\u89c9\u6a21\u6001\u601d\u7ef4\u94fe\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u589e\u5f3aMLLM\u5bf9\u7ea2\u5916\u3001\u6df1\u5ea6\u7b49X\u6a21\u6001\u7684\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u5728RGBX-Grounding\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8eRGB\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5728\u7ea2\u5916\u3001\u6df1\u5ea6\u3001\u4e8b\u4ef6\u6570\u636e\u7b49\u5176\u4ed6\u89c6\u89c9\u6a21\u6001\u4e0a\u7684\u6027\u80fd\uff0c\u800c\u8fd9\u4e9b\u6a21\u6001\u5bf9\u590d\u6742\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faRGBX-R1\u6846\u67b6\uff1a1) \u4f7f\u7528\u7406\u89e3-\u5173\u8054-\u9a8c\u8bc1(UAV)\u63d0\u793a\u7b56\u7565\u6784\u5efa\u89c6\u89c9\u6a21\u6001\u601d\u7ef4\u94fe(VM-CoT)\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03(CS-SFT)\u548c\u65f6\u7a7a\u5f3a\u5316\u5fae\u8c03(ST-RFT)\uff0c\u540e\u8005\u4f7f\u7528\u6a21\u6001\u7406\u89e3\u65f6\u7a7a\u5956\u52b1(MuST)\u3002", "result": "\u6784\u5efa\u9996\u4e2aRGBX-Grounding\u57fa\u51c6\uff0c\u5728\u4e09\u4e2aRGBX grounding\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534722.71%\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "RGBX-R1\u6846\u67b6\u6709\u6548\u6269\u5c55\u4e86MLLM\u5bf9\u591a\u79cd\u89c6\u89c9\u6a21\u6001\u7684\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e2d\u7684\u591a\u6a21\u6001\u7406\u89e3\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01109", "abs": "https://arxiv.org/abs/2602.01109", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction", "comment": "9 pages, 7 figures", "summary": "Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.", "AI": {"tldr": "BiCarFormer\uff1a\u9996\u4e2a\u7528\u4e8e\u8f66\u8f86\u6545\u969c\u6a21\u5f0f\u591a\u6807\u7b7e\u5e8f\u5217\u5206\u7c7b\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u6574\u5408DTC\u5e8f\u5217\u548c\u73af\u5883\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd", "motivation": "\u5f53\u524d\u8f66\u8f86\u8bca\u65ad\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u8bca\u65ad\u6545\u969c\u7801\u5e8f\u5217\uff0c\u4f46\u5ffd\u7565\u4e86\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u538b\u529b\u7b49\u73af\u5883\u4f20\u611f\u5668\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4e13\u5bb6\u8bca\u65ad\u6545\u969c\u6a21\u5f0f\u81f3\u5173\u91cd\u8981\u3002\u771f\u5b9e\u4e16\u754c\u6570\u636e\u590d\u6742\u4e14\u566a\u58f0\u5927\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51faBiCarFormer\u53cc\u5411Transformer\u6a21\u578b\uff0c\u4e13\u95e8\u5904\u7406\u8f66\u8f86\u4e8b\u4ef6\u5e8f\u5217\uff0c\u91c7\u7528\u5d4c\u5165\u878d\u5408\u548c\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u8bca\u65ad\u4ee3\u7801\u4e0e\u73af\u5883\u6570\u636e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u5305\u542b22,137\u4e2a\u9519\u8bef\u4ee3\u7801\u548c360\u4e2a\u9519\u8bef\u6a21\u5f0f\u7684\u771f\u5b9e\u4e16\u754c\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4ec5\u4f7f\u7528DTC\u5e8f\u5217\u7684\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u6574\u5408\u73af\u5883\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u9c81\u68d2\u7684\u8f66\u8f86\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u5e76\u589e\u5f3a\u6c7d\u8f66\u884c\u4e1a\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u3002"}}
{"id": "2602.02236", "categories": ["cs.RO", "cs.LG", "cs.NE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02236", "abs": "https://arxiv.org/abs/2602.02236", "authors": ["Julian Lemmel", "Felix Resch", "M\u00f3nika Farsang", "Ramin Hasani", "Daniela Rus", "Radu Grosu"], "title": "Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL", "comment": null, "summary": "Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.", "AI": {"tldr": "RTRRL\u7b97\u6cd5\u80fd\u5728\u7ebf\u5fae\u8c03\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u4e0e\u751f\u7269\u542f\u53d1\u7684LRC-RNN\u7ed3\u5408\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u65f6\u9762\u4e34\u73af\u5883\u52a8\u6001\u53d8\u5316\u3001\u4f20\u611f\u5668\u6f02\u79fb\u548c\u4efb\u52a1\u76ee\u6807\u53d8\u5316\u7b49\u6311\u6218\uff0c\u56fa\u5b9a\u7b56\u7565\u6027\u80fd\u4f1a\u8fc5\u901f\u4e0b\u964d\uff0c\u9700\u8981\u5728\u7ebf\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5b9e\u65f6\u5faa\u73af\u5f3a\u5316\u5b66\u4e60\uff08RTRRL\uff09\u5728\u7ebf\u5fae\u8c03\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4e0e\u751f\u7269\u542f\u53d1\u7684\u6db2\u4f53\u7535\u963b-\u6db2\u4f53\u7535\u5bb9\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08LRC-RNN\uff09\u7ed3\u5408\uff0c\u5f62\u6210\u95ed\u73af\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u5728\u6a21\u62dfCarRacing\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754cRoboRacer\u4e8b\u4ef6\u76f8\u673a\u7ebf\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "RTRRL\u4e0e\u751f\u7269\u542f\u53d1\u7f51\u7edc\u7ed3\u5408\u80fd\u6709\u6548\u89e3\u51b3\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u4e3a\u5b66\u4e60\u578b\u63a7\u5236\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.01155", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01155", "abs": "https://arxiv.org/abs/2602.01155", "authors": ["Hugo Math", "Julian Lorentz", "Stefan Oelsner", "Rainer Lienhart"], "title": "Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles", "comment": "7 pages, 3 figures", "summary": "Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.", "AI": {"tldr": "CAREP\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u8f66\u8f86\u8bca\u65ad\u6545\u969c\u7801(DTC)\u5e8f\u5217\u4e2d\u81ea\u52a8\u751f\u6210\u9519\u8bef\u6a21\u5f0f(EP)\u89c4\u5219\uff0c\u66ff\u4ee3\u4f20\u7edf\u624b\u5de5\u7f16\u5199\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u8f66\u8f86\u4ea7\u751f\u6570\u5343\u79cd\u4e0d\u540c\u7684\u8bca\u65ad\u6545\u969c\u7801\uff0c\u6c7d\u8f66\u5236\u9020\u5546\u4f7f\u7528\u8fd9\u4e9b\u4ee3\u7801\u7684\u5e03\u5c14\u7ec4\u5408\uff08\u9519\u8bef\u6a21\u5f0f\uff09\u6765\u8868\u5f81\u7cfb\u7edf\u6545\u969c\u3002\u7136\u800c\uff0cEP\u89c4\u5219\u4ecd\u7136\u7531\u9886\u57df\u4e13\u5bb6\u624b\u5de5\u7f16\u5199\uff0c\u968f\u7740\u8f66\u8f86\u590d\u6742\u6027\u589e\u52a0\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u6210\u672c\u9ad8\u6602\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "CAREP\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff1a1)\u56e0\u679c\u53d1\u73b0\u667a\u80fd\u4f53\u8bc6\u522bDTC-EP\u6f5c\u5728\u5173\u7cfb\uff1b2)\u4e0a\u4e0b\u6587\u4fe1\u606f\u667a\u80fd\u4f53\u6574\u5408\u5143\u6570\u636e\u548c\u63cf\u8ff0\uff1b3)\u7f16\u6392\u667a\u80fd\u4f53\u5408\u6210\u5019\u9009\u5e03\u5c14\u89c4\u5219\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u5305\u542b29,100\u4e2a\u72ec\u7279DTC\u548c474\u4e2a\u9519\u8bef\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCAREP\u80fd\u591f\u81ea\u52a8\u51c6\u786e\u5730\u53d1\u73b0\u672a\u77e5\u7684EP\u89c4\u5219\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528LLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u7684\u56e0\u679c\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5b9e\u7528\u56e0\u679c\u53d1\u73b0\u548c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u63a8\u7406\uff0cCAREP\u5411\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6545\u969c\u8bca\u65ad\u8fc8\u8fdb\u4e00\u6b65\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u8f66\u8f86\u7ef4\u62a4\u3002"}}
{"id": "2602.00559", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00559", "abs": "https://arxiv.org/abs/2602.00559", "authors": ["Wenbin Xing", "Quanxing Zha", "Lizheng Zu", "Mengran Li", "Ming Li", "Junchi Yan"], "title": "Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models", "comment": null, "summary": "Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., \"All are correct\" and \"None of the above\") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.", "AI": {"tldr": "\u63d0\u51fa\u4e86OmniVCHall\u57fa\u51c6\u6765\u8bc4\u4f30\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86TriCD\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5e7b\u89c9\u7f13\u89e3\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u9519\u8bef\u7c7b\u578b\uff0c\u800c\u7531\u591a\u4e2a\u4ea4\u4e92\u65f6\u7a7a\u56e0\u7d20\u9519\u8bef\u63a8\u7406\u4ea7\u751f\u7684\u7ec4\u5408\u5e7b\u89c9\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1) \u6784\u5efaOmniVCHall\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u89c6\u9891\u9886\u57df\uff0c\u5f15\u5165\u65b0\u7684\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u5b9a\u4e49\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6cd5\uff0c\u5e76\u5305\u542b\u5bf9\u6297\u6027\u7b54\u6848\u9009\u9879\uff1b2) \u63d0\u51faTriCD\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff0c\u5305\u542b\u4e09\u91cd\u8def\u5f84\u6821\u51c6\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6270\u52a8\u63a7\u5236\u5668\u6784\u5efa\u8d1f\u6837\u672c\u89c6\u9891\u53d8\u4f53\uff0c\u4f7f\u7528\u663e\u8457\u6027\u5f15\u5bfc\u589e\u5f3a\u6a21\u5757\u5f3a\u5316\u89c6\u89c9\u8bc1\u636e\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u8bc4\u4f30\u4e8639\u4e2a\u4ee3\u8868\u6027VLLM\uff0c\u53d1\u73b0\u5373\u4f7f\u5148\u8fdb\u6a21\u578b\uff08\u5982Qwen3-VL\u548cGPT-5\uff09\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002TriCD\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u9aa8\u5e72\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "\u7ec4\u5408\u5e7b\u89c9\u662f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6311\u6218\uff0cOmniVCHall\u57fa\u51c6\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\uff0cTriCD\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u548c\u6821\u51c6\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2602.00424", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.00424", "abs": "https://arxiv.org/abs/2602.00424", "authors": ["Philipp Hoellmer", "Stefano Martiniani"], "title": "Open Materials Generation with Inference-Time Reinforcement Learning", "comment": "16 pages, 8 figures, 1 table", "summary": "Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.", "AI": {"tldr": "OMatG-IRL\uff1a\u57fa\u4e8e\u63a8\u7406\u65f6\u5f3a\u5316\u5b66\u4e60\u7684\u6676\u4f53\u6750\u6599\u751f\u6210\u6846\u67b6\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u5f97\u5206\u51fd\u6570\uff0c\u901a\u8fc7\u6270\u52a8\u751f\u6210\u52a8\u529b\u5b66\u5b9e\u73b0\u76ee\u6807\u5c5e\u6027\u5f3a\u5316\uff0c\u5728\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u3002", "motivation": "\u8fde\u7eed\u65f6\u95f4\u751f\u6210\u6a21\u578b\u53ef\u7528\u4e8e\u6676\u4f53\u6750\u6599\u8bbe\u8ba1\uff0c\u4f46\u96be\u4ee5\u5c06\u76ee\u6807\u5c5e\u6027\u663e\u5f0f\u878d\u5165\u751f\u6210\u8fc7\u7a0b\u3002\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u5bf9\u9f50\u751f\u6210\u6a21\u578b\u4e0e\u4e0b\u6e38\u76ee\u6807\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5f97\u5206\u51fd\u6570\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4ec5\u5b66\u4e60\u901f\u5ea6\u573a\u7684\u6d41\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faOMatG-IRL\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u5b66\u4e60\u7684\u901f\u5ea6\u573a\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u5f97\u5206\u51fd\u6570\u3002\u901a\u8fc7\u968f\u673a\u6270\u52a8\u5e95\u5c42\u751f\u6210\u52a8\u529b\u5b66\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u63a2\u7d22\u548c\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u3002", "result": "\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\uff0c\u80fd\u6709\u6548\u5f3a\u5316\u57fa\u4e8e\u80fd\u91cf\u7684\u76ee\u6807\uff0c\u540c\u65f6\u901a\u8fc7\u6210\u5206\u6761\u4ef6\u4fdd\u6301\u591a\u6837\u6027\u3002\u6027\u80fd\u4e0e\u57fa\u4e8e\u5f97\u5206\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u80fd\u5b66\u4e60\u65f6\u95f4\u4f9d\u8d56\u7684\u901f\u5ea6\u9000\u706b\u8c03\u5ea6\uff0c\u5b9e\u73b0\u91c7\u6837\u6548\u7387\u6570\u91cf\u7ea7\u63d0\u5347\u548c\u751f\u6210\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "OMatG-IRL\u4e3a\u6676\u4f53\u6750\u6599\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a8\u7406\u65f6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u5f97\u5206\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u76ee\u6807\u5c5e\u6027\u5f3a\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.02411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02411", "abs": "https://arxiv.org/abs/2602.02411", "authors": ["Hanwen Ren", "Junyong Kim", "Aathman Tharmasanthiran", "Ahmed H. Qureshi"], "title": "Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces", "comment": null, "summary": "Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.", "AI": {"tldr": "CAM-MCTS\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7269\u4f53\u91cd\u6392\u89c4\u5212\u7684\u96c6\u4e2d\u5f0f\u5f02\u6b65\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4e2d\u4efb\u52a1\u5206\u914d\u548c\u5f02\u6b65\u6267\u884c\u7b56\u7565\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u901a\u5e38\u662f\u975e\u5355\u8c03\u7684\uff08\u7269\u4f53\u76f8\u4e92\u963b\u6321\u9700\u8981\u4e34\u65f6\u79fb\u52a8\uff09\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5904\u7406\u5355\u8c03\u5b9e\u4f8b\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4", "method": "\u63d0\u51faCAM-MCTS\u6846\u67b6\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4efb\u52a1\u5206\u914d\uff08\u667a\u80fd\u4f53\u4e86\u89e3\u5f7c\u6b64\u610f\u56fe\u4ee5\u4f18\u5316\u5168\u5c40\u89c4\u5212\uff09\u548c\u5f02\u6b65\u4efb\u52a1\u6267\u884c\u7b56\u7565\uff08\u667a\u80fd\u4f53\u5728\u9002\u5f53\u65f6\u523b\u627f\u62c5\u65b0\u4efb\u52a1\u800c\u4e0d\u7b49\u5f85\u5176\u4ed6\u667a\u80fd\u4f53\uff09\uff0c\u4f7f\u7528\u4e00\u6b65\u524d\u77bb\u6210\u672c\u4f30\u8ba1\u6307\u5bfc", "result": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5355\u8c03\u548c\u975e\u5355\u8c03\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u51cf\u5c11makespan\uff08\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff09\uff0c\u5728\u771f\u5b9e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027", "conclusion": "CAM-MCTS\u901a\u8fc7\u6700\u5c0f\u5316\u7a7a\u95f2\u65f6\u95f4\u3001\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u7269\u4f53\u91cd\u6392\u89c4\u5212\u7684\u6574\u4f53\u7cfb\u7edf\u6548\u7387"}}
{"id": "2602.01297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01297", "abs": "https://arxiv.org/abs/2602.01297", "authors": ["Shaowei Shen", "Xiaohong Yang", "Jie Yang", "Lianfen Huang", "Yongcai Zhang", "Yang Zou", "Seyyedali Hosseinalipour"], "title": "RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis", "comment": "9 pages, 4 figures", "summary": "Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.", "AI": {"tldr": "RE-MCDF\u662f\u4e00\u4e2a\u5173\u7cfb\u589e\u5f3a\u7684\u591a\u4e13\u5bb6\u4e34\u5e8a\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1-\u4fee\u8ba2\u95ed\u73af\u67b6\u6784\u89e3\u51b3\u7535\u5b50\u75c5\u5386\u8bca\u65ad\u4e2d\u7684\u5f02\u8d28\u6027\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u795e\u7ecf\u75c5\u5b66\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u5b50\u75c5\u5386\uff08\u7279\u522b\u662f\u795e\u7ecf\u75c5\u5b66\u9886\u57df\uff09\u5177\u6709\u5f02\u8d28\u6027\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\uff0c\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5bb9\u6613\u4ea7\u751f\u81ea\u6211\u5f3a\u5316\u7684\u9519\u8bef\uff0c\u800c\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4ea4\u4e92\u6d45\u8584\uff0c\u5ffd\u7565\u4e86\u75be\u75c5\u95f4\u7684\u4e30\u5bcc\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff08\u5982\u4e92\u65a5\u6027\u3001\u75c5\u7406\u517c\u5bb9\u6027\u3001\u8bca\u65ad\u6df7\u6dc6\uff09\uff0c\u65e0\u6cd5\u6392\u9664\u4e34\u5e8a\u4e0d\u53ef\u884c\u7684\u5047\u8bbe\u3002", "method": "\u63d0\u51faRE-MCDF\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1\uff09\u751f\u6210\u5019\u9009\u8bca\u65ad\u548c\u652f\u6301\u8bc1\u636e\u7684\u4e3b\u8981\u4e13\u5bb6\uff1b2\uff09\u52a8\u6001\u4f18\u5148\u5904\u7406\u5f02\u8d28\u6027\u4e34\u5e8a\u6307\u6807\u7684\u5b9e\u9a8c\u5ba4\u4e13\u5bb6\uff1b3\uff09\u5f3a\u5236\u6267\u884c\u75be\u75c5\u95f4\u903b\u8f91\u7ea6\u675f\u7684\u591a\u5173\u7cfb\u611f\u77e5\u4e0e\u8bc4\u4f30\u4e13\u5bb6\u7ec4\u3002\u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u524d\u4e24\u4e2a\u4e13\u5bb6\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u7535\u5b50\u75c5\u5386\u8bc1\u636e\uff0c\u4e13\u5bb6\u7ec4\u9a8c\u8bc1\u548c\u4fee\u6b63\u5019\u9009\u8bca\u65ad\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728CMEMR\u7684\u795e\u7ecf\u75c5\u5b66\u5b50\u96c6\uff08NEEMRs\uff09\u548c\u81ea\u5efa\u6570\u636e\u96c6\uff08XMEMRs\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRE-MCDF\u5728\u590d\u6742\u8bca\u65ad\u573a\u666f\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RE-MCDF\u901a\u8fc7\u6574\u5408\u591a\u4e13\u5bb6\u534f\u4f5c\u548c\u663e\u5f0f\u903b\u8f91\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u7535\u5b50\u75c5\u5386\u7684\u667a\u80fd\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u66f4\u7b26\u5408\u4e34\u5e8a\u5b9e\u8df5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00520", "abs": "https://arxiv.org/abs/2602.00520", "authors": ["Minghui Sun", "Haoyu Gong", "Xingyu You", "Jillian Hurst", "Benjamin Goldstein", "Matthew Engelhard"], "title": "NEST: Nested Event Stream Transformer for Sequences of Multisets", "comment": "11 pages", "summary": "Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.", "AI": {"tldr": "NEST\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u96c6\u5e8f\u5217\u4e8b\u4ef6\u6d41\u7684\u5c42\u6b21\u5316Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u4fdd\u6301\u539f\u59cb\u5c42\u6b21\u7ed3\u6784\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8d28\u91cf", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u6d41\u57fa\u7840\u6a21\u578b\u5c06\u5c42\u6b21\u7ed3\u6784\u6241\u5e73\u5316\u4e3a\u5355\u7ef4\u5e8f\u5217\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff08\u5bc6\u96c6\u6ce8\u610f\u529b\uff09\u548c\u5b66\u4e60\u865a\u5047\u7684\u96c6\u5408\u5185\u5173\u7cfb\uff0c\u540c\u65f6\u901a\u8fc7\u542f\u53d1\u5f0f\u540e\u8bad\u7ec3\u6c60\u5316\u5f97\u5230\u7684\u96c6\u5408\u7ea7\u8868\u793a\u8d28\u91cf\u8f83\u4f4e", "method": "\u63d0\u51faNEST\uff08Nested Event Stream Transformer\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u96c6\u5e8f\u5217\u4e8b\u4ef6\u6d41\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4fdd\u6301\u539f\u59cb\u5c42\u6b21\u7ed3\u6784\uff1b\u5e76\u5f15\u5165Masked Set Modeling\uff08MSM\uff09\u8303\u5f0f\uff0c\u4fc3\u8fdb\u66f4\u597d\u7684\u96c6\u5408\u7ea7\u8868\u793a\u5b66\u4e60", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u96c6\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNEST\u80fd\u591f\u6355\u6349\u771f\u5b9e\u4e16\u754c\u52a8\u6001\uff0c\u540c\u65f6\u63d0\u9ad8\u9884\u8bad\u7ec3\u6548\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "conclusion": "\u5728\u57fa\u7840\u6a21\u578b\u67b6\u6784\u4e2d\u4fdd\u6301\u4e8b\u4ef6\u6d41\u7684\u539f\u59cb\u5c42\u6b21\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u65e2\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u53c8\u80fd\u6539\u5584\u8868\u793a\u8d28\u91cf"}}
{"id": "2602.01566", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01566", "abs": "https://arxiv.org/abs/2602.01566", "authors": ["Chiwei Zhu", "Benfeng Xu", "Mingxuan Du", "Shaohan Wang", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents", "comment": "19 pages, 6 figures", "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.", "AI": {"tldr": "FS-Researcher\uff1a\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u5de5\u4f5c\u7a7a\u95f4\u89e3\u51b3LLM\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u8d8a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u77e5\u8bc6\u79ef\u7d2f\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4f5c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u4ee3\u8868\u6027\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u5176\u957f\u8f68\u8ff9\u5e38\u8d85\u51fa\u6a21\u578b\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u538b\u7f29\u4e86\u8bc1\u636e\u6536\u96c6\u548c\u62a5\u544a\u7f16\u5199\u7684token\u9884\u7b97\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) Context Builder\u667a\u80fd\u4f53\u4f5c\u4e3a\u56fe\u4e66\u7ba1\u7406\u5458\u6d4f\u89c8\u4e92\u8054\u7f51\u3001\u7f16\u5199\u7ed3\u6784\u5316\u7b14\u8bb0\u3001\u5c06\u539f\u59cb\u8d44\u6599\u5f52\u6863\u5230\u53ef\u8d85\u8d8a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5206\u5c42\u77e5\u8bc6\u5e93\uff1b2) Report Writer\u667a\u80fd\u4f53\u5206\u6bb5\u7f16\u5199\u6700\u7ec8\u62a5\u544a\uff0c\u5c06\u77e5\u8bc6\u5e93\u4f5c\u4e3a\u4e8b\u5b9e\u6765\u6e90\u3002\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6301\u4e45\u5316\u5916\u90e8\u5185\u5b58\u548c\u8de8\u667a\u80fd\u4f53/\u4f1a\u8bdd\u7684\u5171\u4eab\u534f\u8c03\u5a92\u4ecb\u3002", "result": "\u5728\u4e24\u4e2a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\uff08DeepResearch Bench\u548cDeepConsult\uff09\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u62a5\u544a\u8d28\u91cf\u3002\u5206\u6790\u663e\u793a\u6700\u7ec8\u62a5\u544a\u8d28\u91cf\u4e0e\u5206\u914d\u7ed9Context Builder\u7684\u8ba1\u7b97\u91cf\u5448\u6b63\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u6587\u4ef6\u7cfb\u7edf\u8303\u5f0f\u4e0b\u7684\u6709\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "conclusion": "FS-Researcher\u901a\u8fc7\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6301\u4e45\u5316\u5de5\u4f5c\u7a7a\u95f4\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4e2d\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u77e5\u8bc6\u79ef\u7d2f\u548c\u62a5\u544a\u751f\u6210\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u957f\u89c6\u91ce\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01156", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01156", "abs": "https://arxiv.org/abs/2602.01156", "authors": ["Shunpeng Yang", "Ben Liu", "Hua Chen"], "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning", "comment": "Submitted to ICLR 2026", "summary": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.", "AI": {"tldr": "PolicyFlow\uff1a\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\u907f\u514d\u5b8c\u6574\u6d41\u8f68\u8ff9\u7684\u4f3c\u7136\u8bc4\u4f30\uff0c\u7ed3\u5408\u5e03\u6717\u6b63\u5219\u5316\u5668\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9ad8\u65af\u7b56\u7565PPO\u3002", "motivation": "PPO\u7b97\u6cd5\u5728\u5904\u7406\u9ad8\u65af\u5206\u5e03\u7b56\u7565\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6269\u5c55\u5230\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u7b56\u7565\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b8c\u6574\u6d41\u8f68\u8ff9\u7684\u4f3c\u7136\u8bc4\u4f30\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faPolicyFlow\u7b97\u6cd5\uff1a1\uff09\u901a\u8fc7\u901f\u5ea6\u573a\u53d8\u5316\u6cbf\u7b80\u5355\u63d2\u503c\u8def\u5f84\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u907f\u514d\u5b8c\u6574\u6d41\u8f68\u8ff9\u7684\u4f3c\u7136\u8bc4\u4f30\uff1b2\uff09\u5f15\u5165\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u5e03\u6717\u8fd0\u52a8\u542f\u53d1\u7684\u9690\u5f0f\u7b56\u7565\u71b5\u6b63\u5219\u5316\u5668\uff0c\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u5e76\u9f13\u52b1\u884c\u4e3a\u591a\u6837\u6027\u3002", "result": "\u5728MultiGoal\u3001PointMaze\u3001IsaacLab\u548cMuJoCo Playground\u7b49\u591a\u79cd\u73af\u5883\u7684\u4efb\u52a1\u4e2d\uff0cPolicyFlow\u76f8\u6bd4\u4f7f\u7528\u9ad8\u65af\u7b56\u7565\u7684PPO\u4ee5\u53caFPO\u3001DPPO\u7b49\u6d41\u57fa\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u5728MultiGoal\u4efb\u52a1\u4e2d\u7279\u522b\u5c55\u793a\u4e86\u6355\u83b7\u66f4\u4e30\u5bcc\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u7684\u80fd\u529b\u3002", "conclusion": "PolicyFlow\u6210\u529f\u5730\u5c06\u8868\u8fbe\u80fd\u529b\u5f3a\u7684CNF\u7b56\u7565\u4e0ePPO\u98ce\u683c\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u91cd\u8981\u6027\u6bd4\u7387\u8fd1\u4f3c\u548c\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u8ba1\u7b97\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u590d\u6742\u7b56\u7565\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01556", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01556", "abs": "https://arxiv.org/abs/2602.01556", "authors": ["Hong Su"], "title": "Autonomous Question Formation for Large Language Model-Driven AI Systems", "comment": null, "summary": "Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4eba\u7c7b\u6a21\u62df\u7684\u6846\u67b6\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u5e76\u8bbe\u5b9a\u4efb\u52a1\uff0c\u901a\u8fc7\u63a8\u7406\u5185\u90e8\u72b6\u6001\u3001\u73af\u5883\u89c2\u5bdf\u548c\u4e0e\u5176\u4ed6AI\u7cfb\u7edf\u7684\u4ea4\u4e92\u6765\u63d0\u5347\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u9884\u5b9a\u4e49\u4efb\u52a1\u548c\u56fa\u5b9a\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73af\u5883\u53d8\u5316\u65f6\u81ea\u4e3b\u8bc6\u522b\u5e94\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002\u9700\u8981\u8ba9AI\u7cfb\u7edf\u80fd\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u5e76\u8bbe\u5b9a\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4eba\u7c7b\u6a21\u62df\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5f62\u6210\u4f5c\u4e3a\u4efb\u52a1\u9009\u62e9\u548c\u6267\u884c\u4e4b\u524d\u7684\u4e00\u7b49\u51b3\u7b56\u8fc7\u7a0b\u3002\u6574\u5408\u5185\u90e8\u9a71\u52a8\u3001\u73af\u5883\u611f\u77e5\u548c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u4e09\u79cd\u63d0\u793a\u8303\u56f4\uff0c\u9010\u6b65\u6269\u5c55\u8ba4\u77e5\u8986\u76d6\u3002\u652f\u6301\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u95ee\u9898\u5f62\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u73af\u5883\u611f\u77e5\u63d0\u793a\u76f8\u6bd4\u5185\u90e8\u9a71\u52a8\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86\u65e0\u8fdb\u98df\u4e8b\u4ef6\uff0c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u63d0\u793a\u572820\u5929\u4eff\u771f\u4e2d\u5c06\u7d2f\u79ef\u65e0\u8fdb\u98df\u4e8b\u4ef6\u8fdb\u4e00\u6b65\u51cf\u5c1160%\u4ee5\u4e0a\uff0c\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u6539\u8fdb(p<0.05)\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7fAI\u7cfb\u7edf\u80fd\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u548c\u8bbe\u5b9a\u4efb\u52a1\uff0c\u901a\u8fc7\u6574\u5408\u591a\u7ef4\u5ea6\u8ba4\u77e5\u8303\u56f4\u548c\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u52a8\u6001\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2602.00541", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00541", "abs": "https://arxiv.org/abs/2602.00541", "authors": ["Zilin Jing", "Vincent Jeanselme", "Yuta Kobayashi", "Simon A. Lee", "Chao Pang", "Aparajita Kashyap", "Yanwei Li", "Xinzhuo Jiang", "Shalmali Joshi"], "title": "One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models", "comment": null, "summary": "Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability", "AI": {"tldr": "ORA\uff1a\u4e00\u79cd\u65b0\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u65f6\u95f4\u548c\u76f8\u5173\u6d4b\u91cf\u503c\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u901a\u7528\u7684\u8868\u793a", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u5177\u6709\u4e0d\u89c4\u5219\u91c7\u6837\u3001\u6df7\u5408\u79bb\u6563\u4e8b\u4ef6\u548c\u6570\u503c\u6d4b\u91cf\u7684\u7279\u70b9\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349EHR\u7684\u5b8c\u6574\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u63d0\u51faORA\uff08\u6807\u8bb0\u65f6\u95f4\u5230\u4e8b\u4ef6\uff09\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u65f6\u95f4\u548c\u76f8\u5173\u8fde\u7eed\u6d4b\u91cf\u503c\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86EHR\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u6570\u503c\u7279\u5f81\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u4e0b\u6e38\u4efb\u52a1\u548c\u6a21\u578b\u67b6\u6784\u4e0a\uff0cORA\u76ee\u6807\u59cb\u7ec8\u6bd4\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u548c\u5ffd\u7565\u8fde\u7eed\u6d4b\u91cf\u7684\u9884\u8bad\u7ec3\u635f\u5931\u4ea7\u751f\u66f4\u901a\u7528\u7684\u8868\u793a\u3002\u6539\u8fdb\u4e0d\u4ec5\u4f53\u73b0\u5728\u4f20\u7edf\u5206\u7c7b\u8bc4\u4f30\uff0c\u8fd8\u5305\u62ec\u66f4\u597d\u7684\u56de\u5f52\u548c\u65f6\u95f4\u5230\u4e8b\u4ef6\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "ORA\u4e0d\u4ec5\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u66f4\u91cd\u8981\u7684\u662f\u8868\u660e\uff1a\u8003\u8651EHR\u7ed3\u6784\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u5bf9\u4e8e\u6269\u5c55\u4e0b\u6e38\u80fd\u529b\u548c\u63d0\u9ad8\u6cdb\u5316\u6027\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e3aEHR\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.01740", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01740", "abs": "https://arxiv.org/abs/2602.01740", "authors": ["Qixin Xiao", "Kun Zhou"], "title": "MACD: Model-Aware Contrastive Decoding via Counterfactual Data", "comment": null, "summary": "Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.", "AI": {"tldr": "MACD\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u6784\u5efa\u6765\u51cf\u5c11\u5e7b\u89c9\uff0c\u7279\u522b\u9488\u5bf9\u89c6\u89c9\u8bc1\u636e\u5f31\u3001\u6a21\u7cca\u6216\u6709\u504f\u7684\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u8bc1\u636e\u5f31\u3001\u6a21\u7cca\u6216\u6709\u504f\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u7684\u89e3\u7801\u65b9\u6cd5\uff08\u5982\u5bf9\u6bd4\u89e3\u7801\uff09\u4f9d\u8d56\u968f\u673a\u6270\u52a8\u6784\u5efa\u5bf9\u6bd4\u6570\u636e\uff0c\u96be\u4ee5\u63a7\u5236\u9a71\u52a8\u5e7b\u89c9\u7684\u89c6\u89c9\u7ebf\u7d22\u6216\u4e0e\u6a21\u578b\u5f31\u70b9\u826f\u597d\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u5bf9\u6bd4\u89e3\u7801\uff08MACD\uff09\uff0c\u5229\u7528\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u7684\u53cd\u9988\u8bc6\u522b\u5bfc\u81f4\u5e7b\u89c9\u7684\u5bf9\u8c61\u533a\u57df\uff0c\u5728\u5bf9\u8c61\u7ea7\u522b\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u53cd\u4e8b\u5b9e\u8f93\u5165\uff08\u800c\u975e\u4efb\u610f\u7684\u5e27\u6216\u65f6\u95f4\u4fee\u6539\uff09\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u96c6\u6210\u5230\u5bf9\u6bd4\u89e3\u7801\u4e2d\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u6807\u8bb0\u9009\u62e9\u3002", "result": "\u5728EventHallusion\u3001MVBench\u3001Perception-test\u548cVideo-MME\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMACD\u80fd\u6301\u7eed\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u5728Qwen\u548cInternVL\u7b49\u4e0d\u540c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u4fdd\u6301\u6216\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u6d89\u53ca\u5c0f\u7269\u4f53\u3001\u906e\u6321\u7269\u4f53\u6216\u5171\u73b0\u7269\u4f53\u7684\u6311\u6218\u6027\u573a\u666f\u65f6\u7279\u522b\u6709\u6548\u3002", "conclusion": "MACD\u901a\u8fc7\u6a21\u578b\u5f15\u5bfc\u7684\u53cd\u4e8b\u5b9e\u6784\u5efa\u4e0e\u89e3\u7801\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u7b56\u7565\u6765\u51cf\u5c11\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u8bc1\u636e\u4e0d\u8db3\u7684\u590d\u6742\u573a\u666f\u4e2d\u3002"}}
{"id": "2602.01725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01725", "abs": "https://arxiv.org/abs/2602.01725", "authors": ["Yurun Chen", "Zeyi Liao", "Ping Yin", "Taotao Xie", "Keting Yin", "Shengyu Zhang"], "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models", "comment": null, "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.", "AI": {"tldr": "SafePred\u63d0\u51fa\u9884\u6d4b\u6027\u62a4\u680f\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u9884\u6d4b\u4e0e\u51b3\u7b56\u4f18\u5316\uff0c\u89e3\u51b3\u73b0\u6709\u53cd\u5e94\u5f0f\u62a4\u680f\u65e0\u6cd5\u5e94\u5bf9\u957f\u671f\u98ce\u9669\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u7684\u62a4\u680f\u5927\u591a\u91c7\u7528\u53cd\u5e94\u5f0f\u65b9\u6cd5\uff0c\u53ea\u80fd\u5728\u5f53\u524d\u89c2\u5bdf\u7a7a\u95f4\u5185\u7ea6\u675f\u4ee3\u7406\u884c\u4e3a\u3002\u867d\u7136\u80fd\u9632\u6b62\u77ed\u671f\u98ce\u9669\uff08\u5982\u70b9\u51fb\u9493\u9c7c\u94fe\u63a5\uff09\uff0c\u4f46\u65e0\u6cd5\u4e3b\u52a8\u907f\u514d\u957f\u671f\u98ce\u9669\uff1a\u770b\u4f3c\u5408\u7406\u7684\u884c\u52a8\u53ef\u80fd\u5bfc\u81f4\u5ef6\u8fdf\u51fa\u73b0\u7684\u9ad8\u98ce\u9669\u540e\u679c\uff08\u5982\u6e05\u7406\u65e5\u5fd7\u5bfc\u81f4\u672a\u6765\u5ba1\u8ba1\u4e0d\u53ef\u8ffd\u6eaf\uff09\uff0c\u53cd\u5e94\u5f0f\u62a4\u680f\u65e0\u6cd5\u5728\u5f53\u524d\u89c2\u5bdf\u7a7a\u95f4\u5185\u8bc6\u522b\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u6027\u62a4\u680f\u65b9\u6cd5\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u9884\u6d4b\u7684\u672a\u6765\u98ce\u9669\u4e0e\u5f53\u524d\u51b3\u7b56\u5bf9\u9f50\u3002\u57fa\u4e8e\u6b64\u63d0\u51faSafePred\u6846\u67b6\uff0c\u5efa\u7acb\u98ce\u9669\u5230\u51b3\u7b56\u7684\u5faa\u73af\uff0c\u652f\u6301\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a1\uff09\u77ed\u671f\u548c\u957f\u671f\u98ce\u9669\u9884\u6d4b\uff1a\u4ee5\u5b89\u5168\u7b56\u7565\u4e3a\u57fa\u7840\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u751f\u6210\u77ed\u671f\u548c\u957f\u671f\u98ce\u9669\u7684\u8bed\u4e49\u8868\u793a\uff0c\u8bc6\u522b\u5e76\u526a\u679d\u5bfc\u81f4\u9ad8\u98ce\u9669\u72b6\u6001\u7684\u884c\u4e3a\uff1b2\uff09\u51b3\u7b56\u4f18\u5316\uff1a\u901a\u8fc7\u6b65\u9aa4\u7ea7\u5e72\u9884\u548c\u4efb\u52a1\u7ea7\u91cd\u65b0\u89c4\u5212\uff0c\u5c06\u9884\u6d4b\u98ce\u9669\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5b89\u5168\u51b3\u7b56\u6307\u5bfc\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSafePred\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u98ce\u9669\u884c\u4e3a\uff0c\u4e0e\u53cd\u5e94\u5f0f\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc797.6%\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5e76\u5c06\u4efb\u52a1\u6548\u7528\u63d0\u9ad8\u4e86\u9ad8\u8fbe21.4%\u3002", "conclusion": "SafePred\u901a\u8fc7\u9884\u6d4b\u6027\u62a4\u680f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u957f\u671f\u98ce\u9669\u95ee\u9898\uff0c\u5c06\u98ce\u9669\u9884\u6d4b\u4e0e\u51b3\u7b56\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u5728\u4fdd\u6301\u9ad8\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2602.01910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01910", "abs": "https://arxiv.org/abs/2602.01910", "authors": ["Michele Fiori", "Gabriele Civitarese", "Flora D. Salim", "Claudio Bettini"], "title": "DomusFM: A Foundation Model for Smart-Home Sensor Data", "comment": null, "summary": "Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.", "AI": {"tldr": "DomusFM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u8bbe\u8ba1\u7684\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u53cc\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5728\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff1a\u76d1\u7763\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e0d\u5b9e\u7528\uff1b\u73b0\u6709\u57fa\u7840\u6a21\u578b\u53ea\u5173\u6ce8\u60ef\u6027\u4f20\u611f\u5668\uff0c\u65e0\u6cd5\u5904\u7406\u667a\u80fd\u5bb6\u5c45\u4e8c\u5143\u4f20\u611f\u5668\u6570\u636e\u7684\u7a00\u758f\u3001\u79bb\u6563\u7279\u6027\uff1b\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u9700\u8981\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6216\u63d0\u793a\uff0c\u4f9d\u8d56\u5916\u90e8\u670d\u52a1\u6216\u6602\u8d35\u786c\u4ef6\uff0c\u5b58\u5728\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faDomusFM\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u53cc\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u540c\u65f6\u6355\u6349token\u7ea7\u8bed\u4e49\u5c5e\u6027\u548c\u5e8f\u5217\u7ea7\u65f6\u95f4\u4f9d\u8d56\u3002\u6574\u5408\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5d4c\u5165\u3001\u4e13\u95e8\u7684\u65f6\u95f4\u6a21\u5f0f\u7f16\u7801\u5668\u548c\u4e8c\u5143\u72b6\u6001\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u901a\u7528\u8868\u793a\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5171\u667a\u80fd\u5bb6\u5c45\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7559\u4e00\u6570\u636e\u96c6\u8bc4\u4f30\uff0cDomusFM\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5373\u4f7f\u5728\u4ec5\u67095%\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u83b7\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DomusFM\u89e3\u51b3\u4e86\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00656", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00656", "abs": "https://arxiv.org/abs/2602.00656", "authors": ["Yingxu Wang", "Xinwang Liu", "Mengzhu Wang", "Siyang Gao", "Nan Yin"], "title": "Riemannian Flow Matching for Disentangled Graph Domain Adaptation", "comment": null, "summary": "Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.", "AI": {"tldr": "DisRFM\uff1a\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u56fe\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u9ece\u66fc\u5d4c\u5165\u548c\u57fa\u4e8e\u6d41\u7684\u4f20\u8f93\u89e3\u51b3\u7ed3\u6784\u9000\u5316\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898", "motivation": "\u4f20\u7edf\u56fe\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u4f7f\u7528\u5bf9\u6297\u5b66\u4e60\u5bf9\u9f50\u56fe\u5d4c\u5165\uff0c\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u7ed3\u6784\u9000\u5316\u95ee\u9898 - \u5c42\u6b21\u548c\u8bed\u4e49\u8868\u793a\u7ea0\u7f20\uff1b2\uff09\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898 - \u6700\u5c0f\u6700\u5927\u5bf9\u6297\u8bad\u7ec3\u7684\u632f\u8361\u52a8\u6001", "method": "1\uff09\u5c06\u56fe\u5d4c\u5165\u9ece\u66fc\u6d41\u5f62\uff0c\u4f7f\u7528\u6781\u5750\u6807\u663e\u5f0f\u89e3\u8026\u7ed3\u6784\uff08\u534a\u5f84\uff09\u548c\u8bed\u4e49\uff08\u89d2\u5ea6\uff09\uff1b2\uff09\u901a\u8fc7\u5f84\u5411Wasserstein\u5bf9\u9f50\u4fdd\u6301\u62d3\u6251\u7ed3\u6784\uff0c\u901a\u8fc7\u89d2\u5ea6\u805a\u7c7b\u5b9e\u73b0\u8bed\u4e49\u533a\u5206\uff1b3\uff09\u4f7f\u7528\u9ece\u66fc\u6d41\u5339\u914d\u5b66\u4e60\u5e73\u6ed1\u5411\u91cf\u573a\uff0c\u6cbf\u6d4b\u5730\u7ebf\u8def\u5f84\u5f15\u5bfc\u6e90\u7279\u5f81\u5411\u76ee\u6807\u79fb\u52a8", "result": "\u7406\u8bba\u8bc1\u660e\u6d41\u5339\u914d\u7684\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u66f4\u7d27\u7684\u76ee\u6807\u98ce\u9669\u8fb9\u754c\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660eDisRFM\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "DisRFM\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u7ed3\u6784\u9000\u5316\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u57df\u9002\u5e94\u6027\u80fd"}}
{"id": "2602.00767", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00767", "abs": "https://arxiv.org/abs/2602.00767", "authors": ["Muhammed Ustaomeroglu", "Guannan Qu"], "title": "BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features", "comment": "41 pages, 32 figures. Code available", "summary": "Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.", "AI": {"tldr": "\u901a\u8fc7\u8bc6\u522b\u5e76\u7ea6\u675f\u5bfc\u81f4\u6a21\u578b\u5728\u5fae\u8c03\u4e2d\u51fa\u73b0\u4e0d\u826f\u884c\u4e3a\u7684\u5185\u90e8\u7279\u5f81\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c1195%\u7684\"\u6d8c\u73b0\u9519\u4f4d\"\u73b0\u8c61\uff0c\u4e14\u4e0d\u5f71\u54cd\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u8303\u56f4\u7684\u76d1\u7763\u76ee\u6807\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u867d\u7136\u5b66\u4f1a\u4e86\u76ee\u6807\u884c\u4e3a\uff0c\u4f46\u4e5f\u4f1a\u53d1\u5c55\u51fa\u4e0d\u826f\u7684\u57df\u5916\u884c\u4e3a\uff08\u6d8c\u73b0\u9519\u4f4d\uff09\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u673a\u5236\u6027\u65b9\u6cd5\u6765\u9632\u6b62\u8fd9\u79cd\u95ee\u9898\u3002", "method": "\u8bc6\u522b\u63a7\u5236\u9519\u4f4d\u884c\u4e3a\u7684\u5c11\u91cf\u5185\u90e8\u7279\u5f81\uff0c\u7136\u540e\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u963b\u6b62\u6a21\u578b\u5f3a\u5316\u8fd9\u4e9b\u7279\u5f81\u3002\u901a\u8fc7\u7279\u5f81\u963b\u65ad\uff08\u7ea6\u675f\uff09\u6280\u672f\uff0c\u5728\u516d\u4e2a\u5fae\u8c03\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u963b\u65ad\u56fa\u5b9a\u7279\u5f81\u96c6\u53ef\u5b9e\u73b0\u9ad8\u8fbe95%\u7684\u76f8\u5bf9\u9519\u4f4d\u51cf\u5c11\uff0c\u4e14\u4e0d\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u6216\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u957f\u65f6\u95f4\u5fae\u8c03\u4f1a\u5bfc\u81f4\u9519\u4f4d\u91cd\u65b0\u51fa\u73b0\uff0c\u8868\u660e\u5b58\u5728\u901a\u8fc7\u66ff\u4ee3\u7279\u5f81\u6216\u5c42\u7684\u91cd\u65b0\u8def\u7531\u3002", "conclusion": "\u9488\u5bf9\u5185\u90e8\u673a\u5236\u7684\u9488\u5bf9\u6027\u8bad\u7ec3\u65f6\u7ea6\u675f\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u6d8c\u73b0\u9519\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002\u8fd9\u4e3a\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u673a\u5236\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.01194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01194", "abs": "https://arxiv.org/abs/2602.01194", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Fenghua Ling", "Lei Bai"], "title": "EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting", "comment": null, "summary": "Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.", "AI": {"tldr": "\u63d0\u51faEMFormer\u67b6\u6784\u548c\u7d2f\u79ef\u4e0a\u4e0b\u6587\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u52a8\u6001\u635f\u5931\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u5929\u6c14\u9884\u62a5\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u957f\u671f\u5929\u6c14\u9884\u62a5\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u89c4\u5212\u548c\u707e\u5bb3\u51c6\u5907\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u9ad8\u8bad\u7ec3\u5f00\u9500\u7b49\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u63d0\u51fa\u9ad8\u6548\u591a\u5c3a\u5ea6Transformer\uff08EMFormer\uff09\uff0c\u901a\u8fc7\u5355\u5377\u79ef\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b2) \u91c7\u7528\u7d2f\u79ef\u4e0a\u4e0b\u6587\u5fae\u8c03\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\uff1b3) \u8bbe\u8ba1\u6b63\u5f26\u52a0\u6743\u52a8\u6001\u5e73\u8861\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u5929\u6c14\u9884\u62a5\u548c\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5728\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\uff08ImageNet-1K\u548cADE20K\uff09\u4e0a\u5c55\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edf\u591a\u5c3a\u5ea6\u6a21\u5757\u5b9e\u73b05.69\u500d\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u9884\u6d4b\u7684\u5b8c\u6574\u7ba1\u9053\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01418", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01418", "abs": "https://arxiv.org/abs/2602.01418", "authors": ["Christoffer Koo \u00d8hrstr\u00f8m", "Rafael I. Cabral Muchacho", "Yifei Dong", "Filippos Moumtzidellis", "Ronja G\u00fcldenring", "Florian T. Pokorny", "Lazaros Nalpantidis"], "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas", "comment": null, "summary": "We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.", "AI": {"tldr": "PaPE\u662f\u4e00\u79cd\u57fa\u4e8e\u629b\u7269\u7ebf\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u4e13\u95e8\u4e3a\u89c6\u89c9\u6a21\u6001\u8bbe\u8ba1\uff0c\u57288\u4e2a\u6570\u636e\u96c6\u4e0a7\u4e2a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5728ImageNet-1K\u4e0a\u5916\u63a8\u80fd\u529b\u663e\u8457\u63d0\u534710.5%", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u4e3b\u8981\u4ece\u8bed\u8a00\u5904\u7406\u76841D\u5e8f\u5217\u6269\u5c55\u5230\u89c6\u89c9\u7684nD\u7ed3\u6784\uff0c\u4f46\u672a\u80fd\u5145\u5206\u8003\u8651\u89c6\u89c9\u6a21\u6001\u7684\u7279\u6027\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u7b26\u5408\u89c6\u89c9\u7279\u6027\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5", "method": "\u63d0\u51fa\u629b\u7269\u7ebf\u4f4d\u7f6e\u7f16\u7801(PaPE)\uff0c\u57fa\u4e8e\u5e73\u79fb\u4e0d\u53d8\u6027\u3001\u65cb\u8f6c\u4e0d\u53d8\u6027(PaPE-RI)\u3001\u8ddd\u79bb\u8870\u51cf\u3001\u65b9\u5411\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7b49\u539f\u5219\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u3001\u70b9\u4e91\u3001\u89c6\u9891\u548c\u4e8b\u4ef6\u76f8\u673a\u7b49\u591a\u79cd\u89c6\u89c9\u6a21\u6001", "result": "\u5728\u6db5\u76d64\u79cd\u6a21\u6001\u76848\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cPaPE\u6216PaPE-RI\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff1b\u5728ImageNet-1K\u5916\u63a8\u5b9e\u9a8c\u4e2d\uff0cPaPE\u6bd4\u6b21\u4f18\u4f4d\u7f6e\u7f16\u7801\u7edd\u5bf9\u63d0\u5347\u8fbe10.5%", "conclusion": "PaPE\u662f\u4e00\u79cd\u6709\u6548\u7684\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u9002\u5e94\u89c6\u89c9\u6a21\u6001\u7279\u6027\uff0c\u5177\u6709\u4f18\u5f02\u7684\u5916\u63a8\u80fd\u529b\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027"}}
{"id": "2602.01530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01530", "abs": "https://arxiv.org/abs/2602.01530", "authors": ["Parsa Esmaeilkhani", "Longin Jan Latecki"], "title": "Preserving Localized Patch Semantics in VLMs", "comment": null, "summary": "Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word \"cat\"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "AI": {"tldr": "\u63d0\u51faLogit Lens Loss (LLL)\u4f86\u89e3\u6c7a\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4e2d\u8996\u89batoken\u8cc7\u8a0a\u64f4\u6563\u554f\u984c\uff0c\u4f7fLogit Lens\u53ef\u8996\u5316\u80fd\u7522\u751f\u6709\u610f\u7fa9\u7684\u7269\u4ef6\u4fe1\u5fc3\u5716\uff0c\u4e26\u63d0\u5347\u8996\u89ba\u4efb\u52d9\u8868\u73fe\u3002", "motivation": "Logit Lens\u5728\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4e2d\u53ef\u8996\u5316\u5716\u50cftoken\u7684\u6982\u5ff5\u5167\u5bb9\uff0c\u4f46\u8996\u89ba\u8cc7\u8a0a\u5e38\u64f4\u6563\u5230\u8a9e\u8a00token\uff0c\u7834\u58de\u5c40\u90e8\u6027\uff0c\u4f7f\u53ef\u8996\u5316\u7121\u6cd5\u7528\u65bc\u89e3\u91cb\u6027\u3002", "method": "\u63d0\u51faLogit Lens Loss (LLL)\u4f5c\u70ba\u4e0b\u4e00\u500btoken\u9810\u6e2c\u7684\u88dc\u5145\u640d\u5931\uff0c\u4f7f\u8996\u89batoken\u5d4c\u5165\u8207\u63cf\u8ff0\u5176\u5716\u50cf\u5340\u57df\u7684\u6587\u672c\u6982\u5ff5\u8a9e\u7fa9\u5c0d\u9f4a\uff0c\u7121\u9700\u67b6\u69cb\u4fee\u6539\u6216\u5927\u898f\u6a21\u8a13\u7df4\u3002", "result": "LLL\u4e0d\u50c5\u4f7fLogit Lens\u80fd\u7522\u751f\u6709\u610f\u7fa9\u7684\u7269\u4ef6\u4fe1\u5fc3\u5716\uff0c\u9084\u63d0\u5347\u4e86\u5206\u5272\u7b49\u8996\u89ba\u4e2d\u5fc3\u4efb\u52d9\u7684\u8868\u73fe\uff0c\u7121\u9700\u9644\u52a0\u7279\u6b8a\u982d\u90e8\u3002", "conclusion": "Logit Lens Loss\u6709\u6548\u89e3\u6c7a\u8996\u89batoken\u8cc7\u8a0a\u64f4\u6563\u554f\u984c\uff0c\u589e\u5f37\u4e86\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91cb\u6027\u548c\u8996\u89ba\u4efb\u52d9\u8868\u73fe\u3002"}}
{"id": "2602.01633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01633", "abs": "https://arxiv.org/abs/2602.01633", "authors": ["Xinyuan Zhao", "Yihang Wu", "Ahmad Chaddad", "Tareef Daqqaq", "Reem Kateb"], "title": "Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification", "comment": "Accepted in Knowledge-Based Systems", "summary": "While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\\% to 41.69\\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u548c\u5ba2\u6237\u7aef\u611f\u77e5\u805a\u5408\u7b56\u7565\uff0c\u89e3\u51b3\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u9650\u5236\u4e0b\uff0c\u533b\u7597\u56fe\u50cf\u7b49\u654f\u611f\u6570\u636e\u96be\u4ee5\u96c6\u4e2d\u83b7\u53d6\u3002\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u9762\u4e34\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u8d28\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931(DAFL)\uff1a\u6839\u636e\u5ba2\u6237\u7aef\u6837\u672c\u5206\u5e03\u548c\u7c7b\u522b\u6570\u636e\u5206\u5e03\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u4e0d\u5e73\u8861\u7cfb\u6570\uff0c\u786e\u4fdd\u5c11\u6570\u7c7b\u5f97\u5230\u8db3\u591f\u5173\u6ce8\uff1b2. \u5ba2\u6237\u7aef\u611f\u77e5\u52a0\u6743\u805a\u5408\u7b56\u7565\uff1a\u6839\u636e\u6570\u636e\u89c4\u6a21\u548c\u7279\u5f81\u81ea\u9002\u5e94\u8c03\u6574\u6743\u91cd\uff0c\u66f4\u597d\u6355\u6349\u5ba2\u6237\u7aef\u95f4\u5dee\u5f02\u3002", "result": "\u5728ISIC\u3001Ocular Disease\u548cRSNA-ICH\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8eDenseNet121\u3001ResNet50\u3001ViT-S/16\u3001ViT-L/32\u3001FedCLIP\u3001Swin Transformer\u3001CoAtNet\u548cMixNet\uff0c\u51c6\u786e\u7387\u63d0\u53470.98%\u523041.69%\u3002\u5728ISIC\u6570\u636e\u96c6\u4e0a\u7684\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u635f\u5931\u51fd\u6570\u548c\u805a\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u548c\u5ba2\u6237\u7aef\u611f\u77e5\u805a\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.01124", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01124", "abs": "https://arxiv.org/abs/2602.01124", "authors": ["Md Abrar Jahin", "Taufikur Rahman Fuad", "Jay Pujara", "Craig Knoblock"], "title": "ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs", "comment": null, "summary": "Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \\cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\\%$ Macro-F1 and $2.4\\%$ Micro-F1 while achieving $3-10\\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $\u03c1< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\\%$ sparsity.", "AI": {"tldr": "ChronoSpike\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684LIF\u795e\u7ecf\u5143\u3001\u591a\u5934\u6ce8\u610f\u529b\u7a7a\u95f4\u805a\u5408\u548c\u8f7b\u91cf\u7ea7Transformer\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u5185\u5b58\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u57fa\u672c\u6743\u8861\uff1a\u6ce8\u610f\u529b\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u590d\u6742\u5ea6\u9ad8\uff08O(T\u00b2)\uff09\uff0c\u5faa\u73af\u67b6\u6784\u5b58\u5728\u68af\u5ea6\u95ee\u9898\u548c\u5bc6\u96c6\u72b6\u6001\u5b58\u50a8\u95ee\u9898\u3002\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5177\u6709\u4e8b\u4ef6\u9a71\u52a8\u6548\u7387\uff0c\u4f46\u53d7\u9650\u4e8e\u5e8f\u5217\u4f20\u64ad\u3001\u4e8c\u8fdb\u5236\u4fe1\u606f\u4e22\u5931\u548c\u5c40\u90e8\u805a\u5408\u65e0\u6cd5\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faChronoSpike\uff0c\u6574\u5408\u4e86\uff1a1\uff09\u5177\u6709\u6bcf\u901a\u9053\u819c\u52a8\u529b\u5b66\u7684\u53ef\u5b66\u4e60LIF\u795e\u7ecf\u5143\uff1b2\uff09\u5728\u8fde\u7eed\u7279\u5f81\u4e0a\u7684\u591a\u5934\u6ce8\u610f\u529b\u7a7a\u95f4\u805a\u5408\uff1b3\uff09\u8f7b\u91cf\u7ea7Transformer\u65f6\u95f4\u7f16\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u5c40\u90e8\u5efa\u6a21\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u6355\u83b7\uff0c\u5177\u6709\u7ebf\u6027\u5185\u5b58\u590d\u6742\u5ea6O(T\u00b7d)\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cChronoSpike\u572812\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e862.0% Macro-F1\u548c2.4% Micro-F1\u7684\u63d0\u5347\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u5faa\u73af\u65b9\u6cd5\u5feb3-10\u500d\uff0c\u53c2\u6570\u9884\u7b97\u6052\u5b9a105K\uff08\u4e0e\u56fe\u5927\u5c0f\u65e0\u5173\uff09\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u819c\u7535\u4f4d\u6709\u754c\u6027\u3001\u6536\u7f29\u56e0\u5b50\u03c1<1\u4e0b\u7684\u68af\u5ea6\u6d41\u7a33\u5b9a\u6027\u4ee5\u53caBIBO\u7a33\u5b9a\u6027\u3002", "conclusion": "ChronoSpike\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u9002\u5e94\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u4e0e\u8868\u8fbe\u80fd\u529b\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u4e0b\u7684\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff08\u663e\u793a\u5f02\u8d28\u65f6\u95f4\u611f\u53d7\u91ce\u548c83-88%\u7a00\u758f\u6027\uff09\u3002"}}
{"id": "2602.01128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01128", "abs": "https://arxiv.org/abs/2602.01128", "authors": ["Mete Erdogan"], "title": "Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models", "comment": null, "summary": "Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.", "AI": {"tldr": "TS-DPO\uff1a\u5728\u5207\u7ebf\u7a7a\u95f4\u4e2d\u6267\u884cDPO\uff0c\u5b66\u4e60\u6bcf\u4e2a\u76ee\u6807\u7684\u66f4\u65b0\u65b9\u5411\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u7ebf\u6027\u7ec4\u5408\u4ee5\u5b9e\u73b0\u7528\u6237\u6307\u5b9a\u7684\u884c\u4e3a\u5e73\u8861", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08\u5982DPO\uff09\u5c06\u53cd\u9988\u538b\u7f29\u4e3a\u5355\u4e00\u6807\u91cf\u5956\u52b1\uff0c\u56fa\u5b9a\u4e86\u76ee\u6807\u95f4\u7684\u5e73\u8861\uff0c\u65e0\u6cd5\u904d\u5386\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u9700\u8981\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5e73\u8861\u591a\u4e2a\u4eba\u7c7b\u504f\u597d\u7ef4\u5ea6\uff08\u5982\u5e2e\u52a9\u6027\u3001\u5b89\u5168\u6027\u3001\u7b80\u6d01\u6027\uff09\uff0c\u5b9e\u73b0\u53ef\u63a7\u5bf9\u9f50\u3002", "method": "\u57fa\u4e8e\u5207\u7ebf\u7a7a\u95f4\u7406\u8bba\uff0c\u5c06\u5fae\u8c03\u89c6\u4e3a\u5728\u6a21\u578b\u5207\u7ebf\u7a7a\u95f4\u4e2d\u7684\u64cd\u4f5c\uff0c\u7ebf\u6027\u5316\u66f4\u65b0\u4f5c\u4e3a\u53ef\u7ec4\u5408\u7684\u52a0\u6cd5\u5411\u91cf\u3002\u63d0\u51fa\u5207\u7ebf\u7a7a\u95f4\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08TS-DPO\uff09\uff0c\u5728\u5c40\u90e8\u7ebf\u6027\u673a\u5236\u4e2d\u6267\u884cDPO\uff0c\u5b66\u4e60\u6bcf\u4e2a\u76ee\u6807\u7684\u66f4\u65b0\u65b9\u5411\u3002\u8fd9\u4e9b\u65b9\u5411\u53ef\u5728\u63a8\u7406\u65f6\u7ebf\u6027\u7ec4\u5408\uff0c\u65e0\u9700\u989d\u5916\u4f18\u5316\u3002", "result": "\u5728HelpSteer\u548cUltraFeedback\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5e2e\u52a9\u6027-\u7b80\u6d01\u6027\u6743\u8861\uff0cTS-DPO\u6bd4\u6807\u91cf\u5316DPO\u5b9e\u73b0\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u8986\u76d6\u548c\u66f4\u5e73\u6ed1\u7684\u504f\u597d\u63a7\u5236\u3002\u5178\u578b\u76f8\u5173\u5206\u6790\uff08CCA\uff09\u663e\u793a\u5207\u7ebf\u7a7a\u95f4\u8bad\u7ec3\u653e\u5927\u4e86\u4e0e\u4e0d\u540c\u504f\u597d\u5bf9\u9f50\u7684\u5178\u578b\u65b9\u5411\uff0c\u6539\u5584\u4e86\u5206\u79bb\u6027\u3002", "conclusion": "TS-DPO\u901a\u8fc7\u5207\u7ebf\u7a7a\u95f4\u4e2d\u7684\u504f\u597d\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u4e2a\u4eba\u7c7b\u504f\u597d\u7ef4\u5ea6\u7684\u53ef\u63a7\u5bf9\u9f50\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u8c03\u6574\u76ee\u6807\u5e73\u8861\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2602.01135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01135", "abs": "https://arxiv.org/abs/2602.01135", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation", "comment": "8 pages, 6 figures,", "summary": "We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.", "AI": {"tldr": "TRACE\uff1a\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u4f5c\u4e3a\u9884\u8bad\u7ec3\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u4ece\u5355\u6761\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u63a8\u65ad\u4e8b\u4ef6\u7c7b\u578b\u95f4\u7684\u56e0\u679c\u56fe\uff0c\u652f\u6301\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\uff0c\u53ef\u7ebf\u6027\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8bcd\u6c47\u8868\u3002", "motivation": "\u4ece\u8f66\u8f86\u65e5\u5fd7\u3001\u5236\u9020\u7cfb\u7edf\u6216\u60a3\u8005\u8f68\u8ff9\u7b49\u5355\u6761\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u53d1\u73b0\u56e0\u679c\u5173\u7cfb\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u91cd\u590d\u6837\u672c\u3001\u9ad8\u7ef4\u5ea6\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u3002", "method": "TRACE\u6846\u67b6\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u9884\u8bad\u7ec3\u5bc6\u5ea6\u4f30\u8ba1\u5668\u8fdb\u884c\u6761\u4ef6\u4e92\u4fe1\u606f\u4f30\u8ba1\uff0c\u63a8\u65ad\u4e8b\u4ef6\u7c7b\u578b\u95f4\u7684\u6458\u8981\u56e0\u679c\u56fe\uff0c\u652f\u6301\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\uff0c\u5728GPU\u4e0a\u5b8c\u5168\u5e76\u884c\u5316\u3002", "result": "\u5728\u7406\u8bba\u53ef\u8bc6\u522b\u6027\u8bc1\u660e\u4e0b\uff0c\u5b9e\u9a8c\u663e\u793aTRACE\u5728\u4e0d\u540c\u57fa\u7ebf\u548c\u8bcd\u6c47\u91cf\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5305\u62ec\u5728\u8d85\u8fc729,100\u4e2a\u4e8b\u4ef6\u7c7b\u578b\u7684\u8f66\u8f86\u8bca\u65ad\u6839\u56e0\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "TRACE\u4e3a\u4ece\u5355\u6761\u9ad8\u7ef4\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02014", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02014", "abs": "https://arxiv.org/abs/2602.02014", "authors": ["Hongxin Xiang", "Pengsen Ma", "Yunkang Cao", "Di Yu", "Haowen Chen", "Xinyu Yang", "Xiangxiang Zeng"], "title": "Rethinking Genomic Modeling Through Optical Character Recognition", "comment": null, "summary": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.", "AI": {"tldr": "OpticalDNA\uff1a\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u56e0\u7ec4\u5efa\u6a21\u6846\u67b6\uff0c\u5c06DNA\u5e8f\u5217\u89c6\u4e3aOCR\u98ce\u683c\u7684\u6587\u6863\u7406\u89e3\uff0c\u901a\u8fc7\u89c6\u89c9DNA\u7f16\u7801\u5668\u548c\u6587\u6863\u89e3\u7801\u5668\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\uff0c\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u4ee5\u66f4\u5c11\u7684token\u548c\u53c2\u6570\u53d6\u5f97\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\u5927\u591a\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u5c06DNA\u89c6\u4e3a\u4e00\u7ef4token\u5e8f\u5217\uff0c\u4f46\u8fd9\u79cd\u987a\u5e8f\u8bfb\u53d6\u65b9\u5f0f\u4e0e\u57fa\u56e0\u7ec4\u7a00\u758f\u3001\u4e0d\u8fde\u7eed\u7684\u8bed\u4e49\u7ed3\u6784\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u5728\u4f4e\u4fe1\u606f\u80cc\u666f\u4e0a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u96be\u4ee5\u5b9e\u73b0\u7406\u89e3\u9a71\u52a8\u7684\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u3002", "method": "\u5c06DNA\u5e8f\u5217\u6e32\u67d3\u4e3a\u7ed3\u6784\u5316\u89c6\u89c9\u5e03\u5c40\uff0c\u8bad\u7ec3OCR\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542b\u89c6\u89c9DNA\u7f16\u7801\u5668\uff08\u751f\u6210\u53ef\u91cd\u6784\u7684\u7d27\u51d1\u89c6\u89c9token\uff09\u548c\u6587\u6863\u89e3\u7801\u5668\u3002\u5b9a\u4e49\u4e86\u57fa\u4e8e\u6838\u5fc3\u57fa\u56e0\u7ec4\u539f\u8bed\u7684\u63d0\u793a\u6761\u4ef6\u76ee\u6807\uff1a\u8bfb\u53d6\u3001\u533a\u57df\u5b9a\u4f4d\u3001\u5b50\u5e8f\u5217\u68c0\u7d22\u548c\u63a9\u7801\u8de8\u5ea6\u8865\u5168\u3002", "result": "\u5728\u591a\u6837\u57fa\u56e0\u7ec4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOpticalDNA\u59cb\u7ec8\u4f18\u4e8e\u8fd1\u671f\u57fa\u7ebf\uff1b\u5728\u957f\u8fbe45\u4e07\u4e2a\u78b1\u57fa\u7684\u5e8f\u5217\u4e0a\uff0c\u4ee5\u8fd120\u500d\u66f4\u5c11\u7684\u6709\u6548token\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u4e14\u4ec5\u9700256k\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u8d85\u8d8a\u6fc0\u6d3b\u53c2\u6570\u591a\u8fbe985\u500d\u7684\u6a21\u578b\u3002", "conclusion": "OpticalDNA\u901a\u8fc7\u89c6\u89c9\u6587\u6863\u7406\u89e3\u8303\u5f0f\u91cd\u65b0\u6784\u5efa\u57fa\u56e0\u7ec4\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u5e03\u5c40\u611f\u77e5\u7684DNA\u8868\u793a\uff0c\u5728\u51cf\u5c11\u6709\u6548token\u9884\u7b97\u7684\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u57fa\u56e0\u7ec4\u4fe1\u606f\uff0c\u4e3a\u957f\u5e8f\u5217\u57fa\u56e0\u7ec4\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\u3002"}}
{"id": "2602.01367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01367", "abs": "https://arxiv.org/abs/2602.01367", "authors": ["Pinar Erbil", "Alberto Archetti", "Eugenio Lomurno", "Matteo Matteucci"], "title": "Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation", "comment": null, "summary": "Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.", "AI": {"tldr": "CONVERSE\u662f\u4e00\u4e2a\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u6df1\u5ea6\u751f\u5b58\u5206\u6790\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u5206\u5c42\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u751f\u5b58\u5206\u6790\u6765\u4f30\u8ba1\u65f6\u95f4\u5230\u4e8b\u4ef6\u7ed3\u679c\u3001\u5206\u5c42\u60a3\u8005\u98ce\u9669\u5e76\u6307\u5bfc\u6cbb\u7597\u8ba1\u5212\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u8be5\u9886\u57df\u8868\u73b0\u51fa\u8272\u4f46\u9762\u4e34\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\uff1a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u51c6\u786e\u4f46\u9ed1\u7bb1\u6027\u8d28\u9650\u5236\u4e34\u5e8a\u91c7\u7528\uff0c\u800c\u53ef\u89e3\u91ca\u7684\u805a\u7c7b\u65b9\u6cd5\u901a\u5e38\u727a\u7272\u9884\u6d4b\u80fd\u529b\u3002", "method": "CONVERSE\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f7f\u7528\u53d8\u5206\u5d4c\u5165\u548c\u591a\u79cd\u7c07\u5185\u7c07\u95f4\u5bf9\u6bd4\u635f\u5931\u3002\u91c7\u7528\u81ea\u6b65\u5b66\u4e60\u4ece\u6613\u5230\u96be\u9010\u6b65\u7eb3\u5165\u6837\u672c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u652f\u6301\u7c07\u7279\u5b9a\u751f\u5b58\u5934\u5b9e\u73b0\u51c6\u786e\u7684\u96c6\u6210\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cCONVERSE\u76f8\u6bd4\u73b0\u6709\u6df1\u5ea6\u751f\u5b58\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u610f\u4e49\u7684\u60a3\u8005\u5206\u5c42\u3002", "conclusion": "CONVERSE\u901a\u8fc7\u7edf\u4e00\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u6df1\u5ea6\u751f\u5b58\u5206\u6790\u4e2d\u6210\u529f\u5e73\u8861\u4e86\u9884\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u5206\u5c42\u5de5\u5177\u3002"}}
{"id": "2602.01428", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01428", "abs": "https://arxiv.org/abs/2602.01428", "authors": ["Weiqing He", "Xiang Li", "Li Shen", "Weijie Su", "Qi Long"], "title": "Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models", "comment": "Accepted at ICLR 2026", "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u5728\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\u3002", "motivation": "\u6c34\u5370\u6280\u672f\u662f\u8ffd\u8e2a\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53d7\u5230\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u963b\u788d\u3002\u63a8\u6d4b\u91c7\u6837\u80fd\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff1a\u66f4\u9ad8\u7684\u6c34\u5370\u5f3a\u5ea6\u4f1a\u964d\u4f4e\u8349\u7a3f\u6a21\u578b\u7684\u63a5\u53d7\u7387\uff0c\u963b\u788d\u4e24\u8005\u540c\u65f6\u5b9e\u73b0\u3002", "method": "1. \u5f15\u5165\u91cf\u5316\u6c34\u5370\u5f3a\u5ea6\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u8be5\u6807\u51c6\u63a7\u5236\u7edf\u8ba1\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u5728\u4ee4\u724c\u662f\u4f2a\u968f\u673a\u6570\u7684\u786e\u5b9a\u6027\u51fd\u6570\u65f6\u8fbe\u5230\u6700\u5927\u30022. \u5c06\u6743\u8861\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u4e24\u79cd\u73b0\u6709\u6c34\u5370\u65b9\u6848\u63a8\u5bfc\u51fa\u660e\u786e\u7684\u5e15\u7d2f\u6258\u66f2\u7ebf\u30023. \u63d0\u51fa\u4e00\u79cd\u539f\u5219\u6027\u673a\u5236\uff0c\u5411\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u786e\u4fdd\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\u540c\u65f6\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u53ef\u68c0\u6d4b\u6027\u3002\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\u53ef\u4ee5\u7edf\u4e00\u7684\u539f\u5219\uff0c\u4e3a\u4e24\u8005\u7684\u9ad8\u6548\u5b9e\u7528\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5e76\u975e\u7edd\u5bf9\uff0c\u901a\u8fc7\u5411\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u6ce8\u5165\u4f2a\u968f\u673a\u6027\u7684\u539f\u5219\u6027\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\uff0c\u4e3a\u6c34\u5370\u6280\u672f\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01439", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01439", "abs": "https://arxiv.org/abs/2602.01439", "authors": ["Perry Dong", "Kuo-Han Hung", "Alexander Swerdlow", "Dorsa Sadigh", "Chelsea Finn"], "title": "TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse", "comment": null, "summary": "Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86Transformer\u5728\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u7f29\u653e\u4e2d\u7684\u6ce8\u610f\u529b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u6700\u5c0f\u5230\u6700\u5927\u7f51\u7edc\u89c4\u6a2143%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u89c4\u6a21\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u9a71\u52a8\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ecd\u4e3b\u8981\u4f7f\u7528\u5c0f\u578b\u4ef7\u503c\u51fd\u6570\u3002\u76f4\u63a5\u7f29\u653e\u4ef7\u503c\u51fd\u6570\uff08\u5305\u62ec\u4f7f\u7528\u5df2\u77e5\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684Transformer\u67b6\u6784\uff09\u901a\u5e38\u4f1a\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76Transformer\u5728\u4ef7\u503c\u51fd\u6570\u7f29\u653e\u4e2d\u5931\u6548\u7684\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u8bc6\u522b\u51fa\u7f29\u653e\u4e2d\u7684\u5173\u952e\u5931\u6548\u6a21\u5f0f\uff1a\u968f\u7740\u5bb9\u91cf\u589e\u52a0\uff0c\u6ce8\u610f\u529b\u5206\u6570\u5d29\u6e83\u3002\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u7684\u71b5\u6765\u6709\u6548\u9632\u6b62\u8fd9\u79cd\u5d29\u6e83\u5e76\u7a33\u5b9a\u8bad\u7ec3\uff0c\u4ece\u800c\u80fd\u591f\u4f7f\u7528\u66f4\u5927\u7684\u6a21\u578b\u3002", "result": "TQL\u65b9\u6cd5\u5728\u4ece\u6700\u5c0f\u5230\u6700\u5927\u7f51\u7edc\u89c4\u6a21\u7684\u7f29\u653e\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe43%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u800c\u5148\u524d\u65b9\u6cd5\u5219\u906d\u53d7\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\uff0cTransformer Q-Learning\u6210\u529f\u89e3\u9501\u4e86Transformer\u5728\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\u4e2d\u7684\u7f29\u653e\u6f5c\u529b\uff0c\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u5927\u89c4\u6a21\u4ef7\u503c\u51fd\u6570\u8bad\u7ec3\u3002"}}
{"id": "2602.01456", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01456", "abs": "https://arxiv.org/abs/2602.01456", "authors": ["Yilun Kuang", "Yash Dagade", "Tim G. J. Rudner", "Randall Balestriero", "Yann LeCun"], "title": "Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations", "comment": null, "summary": "Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRDMReg\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5339\u914dRectified Generalized Gaussian\u5206\u5e03\u6765\u5b66\u4e60\u7a00\u758f\u3001\u975e\u8d1f\u8868\u793a\uff0c\u6539\u8fdbJEPA\u67b6\u6784", "motivation": "\u73b0\u6709JEPA\u65b9\u6cd5\u4f7f\u7528\u5404\u5411\u540c\u6027\u9ad8\u65af\u5206\u5e03\u6b63\u5219\u5316\uff0c\u503e\u5411\u4e8e\u4ea7\u751f\u5bc6\u96c6\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u9ad8\u6548\u8868\u793a\u4e2d\u89c2\u5bdf\u5230\u7684\u7a00\u758f\u6027\u5173\u952e\u7279\u6027", "method": "\u5f15\u5165Rectified Distribution Matching Regularization (RDMReg)\uff0c\u4e00\u79cd\u5207\u7247\u53cc\u6837\u672c\u5206\u5e03\u5339\u914d\u635f\u5931\uff0c\u5c06\u8868\u793a\u5bf9\u9f50\u5230Rectified Generalized Gaussian (RGG)\u5206\u5e03\u3002RGG\u901a\u8fc7\u6574\u6d41\u5b9e\u73b0\u671f\u671b\u2113\u2080\u8303\u6570\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u540c\u65f6\u5728\u671f\u671b\u2113\u209a\u8303\u6570\u7ea6\u675f\u4e0b\u4fdd\u6301\u6700\u5927\u71b5", "result": "Rectified LpJEPA\u5b66\u4e60\u5230\u7a00\u758f\u3001\u975e\u8d1f\u8868\u793a\uff0c\u5728\u7a00\u758f\u6027-\u6027\u80fd\u6743\u8861\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u7684\u4e0b\u6e38\u6027\u80fd", "conclusion": "RDMReg\u80fd\u6709\u6548\u5f3a\u5236\u7a00\u758f\u6027\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0cRectified LpJEPA\u4e25\u683c\u6cdb\u5316\u4e86\u5148\u524d\u57fa\u4e8e\u9ad8\u65af\u7684JEPA\u65b9\u6cd5"}}
{"id": "2602.02214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02214", "abs": "https://arxiv.org/abs/2602.02214", "authors": ["Hongzhou Zhu", "Min Zhao", "Guande He", "Hang Su", "Chongxuan Li", "Jun Zhu"], "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation", "comment": "Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "summary": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "AI": {"tldr": "\u63d0\u51faCausal Forcing\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u5230\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u7684\u67b6\u6784\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u65f6\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u7684\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u5b58\u5728\u67b6\u6784\u5dee\u8ddd\uff0c\u4f7f\u7528\u53cc\u5411\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\u8fdd\u53cd\u5e27\u7ea7\u5355\u5c04\u6761\u4ef6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faCausal Forcing\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\uff0c\u4ece\u800c\u6865\u63a5\u67b6\u6784\u5dee\u8ddd\uff0c\u786e\u4fdd\u6ee1\u8db3\u5e27\u7ea7\u5355\u5c04\u6761\u4ef6\uff0c\u6062\u590d\u6559\u5e08\u6d41\u6620\u5c04\u3002", "result": "\u5728\u6240\u6709\u6307\u6807\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u76f8\u6bd4SOTA Self Forcing\u5728Dynamic Degree\u4e0a\u63d0\u534719.3%\uff0cVisionReward\u4e0a\u63d0\u53478.7%\uff0cInstruction Following\u4e0a\u63d0\u534716.7%\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u81ea\u56de\u5f52\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\uff0cCausal Forcing\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u5230\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u7684\u7406\u8bba\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2602.01734", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01734", "abs": "https://arxiv.org/abs/2602.01734", "authors": ["Lianhai Ren", "Yucheng Ding", "Xiao Liu", "Qianxiao Li", "Peng Cheng", "Yeyun Gong"], "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration", "comment": null, "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $\u03bc$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMSign\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5b9a\u671f\u5e94\u7528\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6062\u590d\u7a33\u5b9a\u79e9\uff0c\u6709\u6548\u9632\u6b62LLM\u9884\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u7206\u70b8\u95ee\u9898\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u4e8e7.0%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u8868\u73b0\u4e3a\u7a81\u53d1\u7684\u68af\u5ea6\u7206\u70b8\uff0c\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5e76\u89e3\u51b3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u03bcP\u7f29\u653e\u76845M\u53c2\u6570NanoGPT\u6a21\u578b\u7814\u7a76\u8bad\u7ec3\u5931\u8d25\uff0c\u53d1\u73b0\u5d29\u6e83\u524d\u7684\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\uff1a\u6743\u91cd\u77e9\u9635\u7a33\u5b9a\u79e9\u5feb\u901f\u4e0b\u964d\u548c\u76f8\u90bb\u5c42\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u9f50\u5ea6\u589e\u52a0\u3002\u63d0\u51faMSign\u4f18\u5316\u5668\uff0c\u5b9a\u671f\u5e94\u7528\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6765\u6062\u590d\u7a33\u5b9a\u79e9\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8fd9\u4e24\u4e2a\u6761\u4ef6\u5171\u540c\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u968f\u7f51\u7edc\u6df1\u5ea6\u6307\u6570\u589e\u957f\u3002\u57285M\u52303B\u53c2\u6570\u7684\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cMSign\u80fd\u6709\u6548\u9632\u6b62\u8bad\u7ec3\u5931\u8d25\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u4e8e7.0%\u3002", "conclusion": "MSign\u4f18\u5316\u5668\u901a\u8fc7\u6253\u7834\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u673a\u5236\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7a33\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002"}}
{"id": "2602.01766", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01766", "abs": "https://arxiv.org/abs/2602.01766", "authors": ["Runsong Zhao", "Shilei Liu", "Jiwei Tang", "Langming Liu", "Haibin Chen", "Weidong Zhang", "Yujin Yuan", "Tong Xiao", "Jingbo Zhu", "Wenbo Su", "Bo Zheng"], "title": "CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling", "comment": null, "summary": "The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/", "AI": {"tldr": "CoMeT\u662f\u4e00\u79cd\u65b0\u578bTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u5185\u5b58\u7cfb\u7edf\u548c\u5206\u5757\u5904\u7406\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5e8f\u5217\u3002", "motivation": "\u6807\u51c6Transformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u4e0d\u65ad\u589e\u957f\u7684KV\u7f13\u5b58\u662f\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u4e3b\u8981\u969c\u788d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5904\u7406\u4efb\u610f\u957f\u5e8f\u5217\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "CoMeT\u91c7\u7528\u53cc\u5185\u5b58\u7cfb\u7edf\uff1aFIFO\u961f\u5217\u7684\u4e34\u65f6\u5185\u5b58\u5904\u7406\u8fd1\u671f\u4e8b\u4ef6\uff0c\u5e26\u95e8\u63a7\u66f4\u65b0\u89c4\u5219\u7684\u5168\u5c40\u5185\u5b58\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u3002\u8fd9\u4e9b\u5185\u5b58\u4f5c\u4e3a\u52a8\u6001\u8f6f\u63d0\u793a\u7528\u4e8e\u4e0b\u4e00\u6570\u636e\u5757\uff0c\u5e76\u5f15\u5165\u5c42\u7ea7\u6d41\u6c34\u7ebf\u5e76\u884c\u7b56\u7565\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "CoMeT\u572832k\u4e0a\u4e0b\u6587\u5fae\u8c03\u540e\uff0c\u80fd\u5728100\u4e07token\u5e8f\u5217\u4e2d\u51c6\u786e\u68c0\u7d22\u4efb\u610f\u4f4d\u7f6e\u7684\u5bc6\u7801\u3002\u5728SCROLLS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5176\u4ed6\u9ad8\u6548\u65b9\u6cd5\uff0c\u5728\u6458\u8981\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u667a\u80fd\u4f53\u548c\u7528\u6237\u884c\u4e3aQA\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "CoMeT\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5185\u5b58\u67b6\u6784\u548c\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86Transformer\u5904\u7406\u957f\u5e8f\u5217\u7684\u5185\u5b58\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.01855", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.01855", "abs": "https://arxiv.org/abs/2602.01855", "authors": ["Blagoj Hristov", "Hristijan Gjoreski", "Vesna Ojleska Latkoska", "Gorjan Nadzinski"], "title": "Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG", "comment": null, "summary": "Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\\pm$ 2.98% to 96.9% $\\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u7a00\u758f\u53cc\u901a\u9053sEMG\u4f20\u611f\u5668\u5b9e\u73b0\u7cbe\u786e\u7684\u808c\u7535\u5047\u80a2\u63a7\u5236\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u65f6\u95f4\u5d4c\u5165\u548c\u5f52\u4e00\u5316\u878d\u5408\u7b56\u7565\uff0c\u572810\u7c7b\u52a8\u4f5c\u8bc6\u522b\u4e0a\u8fbe\u523095.7%\u7684F1\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u9ad8\u5bc6\u5ea6\u591a\u4f20\u611f\u5668\u9635\u5217\u6210\u672c\u9ad8\u4e14\u590d\u6742\uff0c\u9650\u5236\u4e86\u808c\u7535\u5047\u80a2\u7684\u666e\u53ca\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6700\u5c0f\u5316\u4f20\u611f\u5668\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\uff0c\u4f7f\u7528Time2Vec\u53ef\u5b66\u4e60\u65f6\u95f4\u5d4c\u5165\u6355\u6349\u751f\u7269\u4fe1\u53f7\u7684\u968f\u673a\u65f6\u95f4\u626d\u66f2\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u52a0\u6027\u878d\u5408\u7b56\u7565\u5bf9\u9f50\u65f6\u7a7a\u7279\u5f81\u5206\u5e03\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u534f\u8bae\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u572810\u7c7b\u52a8\u4f5c\u8bc6\u522b\u4e0a\u83b7\u5f9795.7% \u00b1 0.20%\u7684\u591a\u88ab\u8bd5F1\u5206\u6570\uff0c\u4f18\u4e8e\u6807\u51c6Transformer\u548cCNN-LSTM\u6a21\u578b\u3002\u5feb\u901f\u6821\u51c6\u534f\u8bae\uff08\u6bcf\u4e2a\u624b\u52bf\u4ec5\u97002\u6b21\u8bd5\u9a8c\uff09\u53ef\u5c06\u65b0\u88ab\u8bd5\u6027\u80fd\u4ece21.0%\u63d0\u5347\u81f396.9%\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u65f6\u95f4\u5d4c\u5165\u53ef\u4ee5\u8865\u507f\u4f4e\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u6311\u6218\u4e86\u9ad8\u5bc6\u5ea6\u4f20\u611f\u7684\u5fc5\u8981\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u5feb\u901f\u4e2a\u6027\u5316\u7684\u5047\u80a2\u63a5\u53e3\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u6709\u6548\u7684\u84dd\u56fe\u3002"}}
{"id": "2602.01897", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01897", "abs": "https://arxiv.org/abs/2602.01897", "authors": ["Sungheon Jeong", "Sanggeon Yun", "Ryozo Masukawa", "Wenjun Haung", "Hanning Chen", "Mohsen Imani"], "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs", "comment": null, "summary": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \\emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \\emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \\emph{Code is available at} \\texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.", "AI": {"tldr": "\u63d0\u51fa\u5185\u90e8\u6d41\u7b7e\u540d\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7LLM\u5185\u90e8\u6df1\u5ea6\u52a8\u6001\u6765\u68c0\u6d4b\u548c\u5b9a\u4f4d\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u4e0d\u5fe0\u5b9e\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u6a21\u578b\u5373\u53ef\u8fdb\u884c\u81ea\u6211\u68c0\u67e5\u548c\u9488\u5bf9\u6027\u4fee\u6b63\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0e\u4e0a\u4e0b\u6587\u4e0d\u5fe0\u5b9e\u4f46\u6d41\u7545\u7684\u7b54\u6848\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u6216\u751f\u6210\u540e\u7684\u72ec\u7acb\u5224\u65ad\uff0c\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u5185\u90e8\u51b3\u7b56\u52a8\u6001\u7684\u81ea\u6211\u68c0\u67e5\u673a\u5236\u3002", "method": "\u5f15\u5165\u5185\u90e8\u6d41\u7b7e\u540d\uff1a\u901a\u8fc7\u504f\u7f6e\u4e2d\u5fc3\u76d1\u63a7\u7a33\u5b9atoken\u7ea7\u52a8\u6001\uff0c\u5728\u6df1\u5ea6\u7a97\u53e3\u5185\u6784\u5efa\u79fb\u52a8\u8bfb\u53d6\u5bf9\u9f50\u5b50\u7a7a\u95f4\uff0c\u4f7f\u7528\u6b63\u4ea4\u4f20\u8f93\u5bf9\u9f50\u76f8\u90bb\u7a97\u53e3\u5e27\uff0c\u63d0\u53d6\u6df1\u5ea6\u53ef\u6bd4\u8f83\u7684\u4f20\u8f93\u6b65\u957f\u3001\u8f6c\u5411\u89d2\u548c\u5b50\u7a7a\u95f4\u6f02\u79fb\u7b49\u7279\u5f81\u3002\u8bad\u7ec3\u8f7b\u91cfGRU\u9a8c\u8bc1\u5668\u8fdb\u884c\u81ea\u6211\u68c0\u67e5\uff0c\u5e76\u80fd\u5b9a\u4f4d\u95ee\u9898\u6df1\u5ea6\u4e8b\u4ef6\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6b63\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u4e0d\u5fe0\u5b9e\u751f\u6210\uff0c\u5b9a\u4f4d\u95ee\u9898\u6df1\u5ea6\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u56de\u6eda\u5230\u95ee\u9898token\u5e76\u5728\u8bc6\u522b\u5757\u5904\u94b3\u5236\u5f02\u5e38\u4f20\u8f93\u6b65\u957f\uff08\u540c\u65f6\u4fdd\u7559\u6b63\u4ea4\u6b8b\u5dee\uff09\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6b63\uff0c\u5b9e\u73b0\u4f4e\u5f00\u9500\u7684\u81ea\u6211\u68c0\u67e5\u3002", "conclusion": "\u5185\u90e8\u6d41\u7b7e\u540d\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u5185\u90e8\u51b3\u7b56\u52a8\u6001\u8fdb\u884c\u53ef\u64cd\u4f5c\u5b9a\u4f4d\u548c\u4f4e\u5f00\u9500\u81ea\u6211\u68c0\u67e5\u7684\u7ba1\u9053\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u6a21\u578b\u5373\u53ef\u5b9e\u73b0LLM\u7684\u81ea\u6211\u9a8c\u8bc1\u548c\u4fee\u6b63\u3002"}}
{"id": "2602.01914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01914", "abs": "https://arxiv.org/abs/2602.01914", "authors": ["Wenbo Pan", "Zhichao Liu", "Xianlong Wang", "Haining Yu", "Xiaohua Jia"], "title": "Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs", "comment": "ICML 2025 submission", "summary": "Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.", "AI": {"tldr": "FlashTrace\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u591atoken\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8token\u805a\u5408\u548c\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5f52\u56e0\u7684\u6548\u7387\u74f6\u9888\u548c\u5fe0\u5b9e\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u73b0\u4ee3LLM\u8d8a\u6765\u8d8a\u4f9d\u8d56\u6269\u5c55\u63a8\u7406\u94fe\uff0c\u73b0\u6709token\u5f52\u56e0\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u6548\u7387\u74f6\u9888\u2014\u2014\u5f52\u56e0M\u4e2a\u76ee\u6807token\u5728\u957f\u5ea6\u4e3aN\u7684\u4e0a\u4e0b\u6587\u4e2d\u9700\u8981O(M*N)\u64cd\u4f5c\uff0c\u4f7f\u957f\u4e0a\u4e0b\u6587\u5f52\u56e0\u53d8\u5f97\u6781\u5176\u7f13\u6162\uff1b2\uff09\u5fe0\u5b9e\u5ea6\u4e0b\u964d\u2014\u2014\u4e2d\u95f4\u63a8\u7406token\u5438\u6536\u4e86\u5f52\u56e0\u8d28\u91cf\uff0c\u963b\u6b62\u91cd\u8981\u6027\u4f20\u64ad\u56de\u539f\u59cb\u8f93\u5165\u3002", "method": "FlashTrace\u91c7\u7528\u8de8token\u805a\u5408\u6280\u672f\uff0c\u5728\u5355\u6b21\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u591atoken\u76ee\u6807\u7684\u5f52\u56e0\uff0c\u540c\u65f6\u4fdd\u6301\u5fe0\u5b9e\u5ea6\u3002\u8bbe\u8ba1\u4e86\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u901a\u8fc7\u4e2d\u95f4\u63a8\u7406\u94fe\u8ffd\u8e2a\u91cd\u8981\u6027\u56de\u5230\u6e90\u8f93\u5165\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\uff08RULER\uff09\u548c\u591a\u6b65\u63a8\u7406\uff08MATH\u3001MorehopQA\uff09\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlashTrace\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u5b9e\u73b0\u4e86\u8d85\u8fc7130\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f18\u8d8a\u7684\u5fe0\u5b9e\u5ea6\u3002\u9012\u5f52\u5f52\u56e0\u5206\u6790\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5355\u6b21\u9012\u5f52\u8df3\u8dc3\u4e5f\u80fd\u901a\u8fc7\u8ffd\u8e2a\u63a8\u7406\u94fe\u4e2d\u7684\u91cd\u8981\u6027\u63d0\u9ad8\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "FlashTrace\u901a\u8fc7\u9ad8\u6548\u7684\u8de8token\u805a\u5408\u548c\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u6b65\u63a8\u7406\u573a\u666f\u4e0b\u7684token\u5f52\u56e0\u6548\u7387\u548c\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01966", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01966", "abs": "https://arxiv.org/abs/2602.01966", "authors": ["Hongzhuo Yu", "Fei Zhu", "Guo-Sen Xie", "Ling Shao"], "title": "Self-Consolidation for Self-Evolving Agents", "comment": null, "summary": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.", "AI": {"tldr": "\u63d0\u51faLLM\u4ee3\u7406\u81ea\u6211\u6f14\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u603b\u7ed3\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u81ea\u6574\u5408\u673a\u5236\u5c06\u6587\u672c\u7ecf\u9a8c\u84b8\u998f\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u5b9e\u73b0\u957f\u671f\u6f14\u5316", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u7cfb\u7edf\u7f3a\u4e4f\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\uff0c\u4e3b\u8981\u4f9d\u8d56\u68c0\u7d22\u6210\u529f\u8f68\u8ff9\u4f5c\u4e3a\u6f14\u793a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u5ffd\u89c6\u5931\u8d25\u5c1d\u8bd5\u7684\u6559\u5b66\u4ef7\u503c\uff1b2) \u4e0d\u65ad\u79ef\u7d2f\u7684\u6587\u672c\u7ecf\u9a8c\u589e\u52a0\u68c0\u7d22\u65f6\u95f4\u3001\u5f15\u5165\u566a\u58f0\u5e76\u8017\u5c3d\u4e0a\u4e0b\u6587\u7a97\u53e3", "method": "\u63d0\u51fa\u81ea\u6211\u6f14\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u673a\u5236\uff1a1) \u5bf9\u6bd4\u53cd\u601d\u7b56\u7565\uff0c\u660e\u786e\u603b\u7ed3\u6613\u9519\u6a21\u5f0f\u5e76\u6355\u83b7\u53ef\u91cd\u7528\u89c1\u89e3\uff1b2) \u81ea\u6574\u5408\u673a\u5236\uff0c\u5c06\u975e\u53c2\u6570\u5316\u6587\u672c\u7ecf\u9a8c\u84b8\u998f\u4e3a\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5c06\u5927\u91cf\u5386\u53f2\u7ecf\u9a8c\u5185\u5316\u5230\u6f5c\u5728\u7a7a\u95f4", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u4ee3\u7406\u6f14\u5316\u4e2d\u7684\u4f18\u52bf", "conclusion": "\u63d0\u51fa\u7684\u81ea\u6211\u6f14\u5316\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u548c\u53c2\u6570\u5316\u7ecf\u9a8c\u6574\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u4ee3\u7406\u7f3a\u4e4f\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u957f\u671f\u6f14\u5316"}}
{"id": "2602.02126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02126", "abs": "https://arxiv.org/abs/2602.02126", "authors": ["Junhan Kim", "Gukryeol Lee", "Seungwoo Son", "Jeewook Kim", "Yongkweon Jeon"], "title": "Two-Stage Grid Optimization for Group-wise Quantization of LLMs", "comment": "ICASSP 2026", "summary": "Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u6765\u6539\u8fdbGPTQ\u5206\u7ec4\u91cf\u5316\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u7cbe\u5ea6\u3002", "motivation": "GPTQ\u7b49\u73b0\u6709\u5206\u7ec4\u91cf\u5316\u65b9\u6cd5\u5728\u786e\u5b9a\u5206\u7ec4\u5c3a\u5ea6\u65f6\u5ffd\u7565\u4e86\u8f93\u5165\u7edf\u8ba1\u7279\u6027\u548c\u7ec4\u95f4\u76f8\u5173\u6027\uff0c\u4e0e\u5176\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u7684\u76ee\u6807\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728GPTQ\u524d\u521d\u59cb\u5316\u6bcf\u4e2a\u5206\u7ec4\u5c3a\u5ea6\u4ee5\u6700\u5c0f\u5316\u5206\u7ec4\u91cd\u5efa\u635f\u5931\uff1b\u7b2c\u4e8c\u9636\u6bb5\u51bb\u7ed3GPTQ\u5f97\u5230\u7684\u6574\u6570\u6743\u91cd\uff0c\u4f7f\u7528\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u548c\u95ed\u5f0f\u66f4\u65b0\u89c4\u5219\u4f18\u5316\u5206\u7ec4\u5c3a\u5ea6\u4ee5\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\uff0c\u5e76\u8003\u8651\u524d\u5c42\u91cf\u5316\u8bef\u5dee\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u5206\u7ec4\u91cf\u5316\u6548\u679c\uff0c\u4ee5\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u83b7\u5f97\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u663e\u5f0f\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86GPTQ\u5ffd\u7565\u8f93\u5165\u7edf\u8ba1\u548c\u7ec4\u95f4\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u7ec4\u91cf\u5316\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2602.02161", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02161", "abs": "https://arxiv.org/abs/2602.02161", "authors": ["Aniq Ur Rahman", "Justin P. Coon"], "title": "Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction", "comment": null, "summary": "Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u5df2\u77e5\u56e0\u679c\u7ed3\u6784\u7684\u56e0\u679c\u65f6\u5e8f\u4ea4\u4e92\u56fe\u6765\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u6355\u6349\u5230\u56e0\u679c\u673a\u5236\u3002", "motivation": "\u5f53\u524d\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u63a7\u5236\u65f6\u5e8f\u4ea4\u4e92\u7684\u56e0\u679c\u673a\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9a8c\u8bc1\u6a21\u578b\u56e0\u679c\u7406\u89e3\u80fd\u529b\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1. \u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u7684\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff0c\u652f\u6301\u5174\u594b\u548c\u6291\u5236\u6548\u5e94\uff1b2. \u5c06\u8be5\u673a\u5236\u6269\u5c55\u5230\u65f6\u5e8f\u4ea4\u4e92\u56fe\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u8de8\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u7684\u8ddd\u79bb\u5ea6\u91cf\u6765\u6bd4\u8f83\u56e0\u679c\u6a21\u578b\uff1b4. \u901a\u8fc7\u63a7\u5236\u56e0\u679c\u504f\u79fb\u548c\u65f6\u95f4\u6233\u91cd\u6392\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u3002", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u5047\u8bbe\uff1a\u5728\u4e00\u4e2a\u56e0\u679c\u6a21\u578b\u4e0a\u8bad\u7ec3\u7684\u9884\u6d4b\u5668\uff0c\u5728\u8db3\u591f\u4e0d\u540c\u7684\u56e0\u679c\u6a21\u578b\u4e0a\u8bc4\u4f30\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u6846\u67b6\u4e3a\u56e0\u679c\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u56e0\u679c\u673a\u5236\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u5f0f\u8bc6\u522b\uff0c\u4e3a\u56e0\u679c\u611f\u77e5\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2602.02179", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02179", "abs": "https://arxiv.org/abs/2602.02179", "authors": ["Marina Mastroleo", "Alberto Archetti", "Federico Mastroleo", "Matteo Matteucci"], "title": "SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks", "comment": null, "summary": "Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.", "AI": {"tldr": "SurvKAN\uff1a\u57fa\u4e8eKAN\u67b6\u6784\u7684\u5b8c\u5168\u53c2\u6570\u5316\u751f\u5b58\u5206\u6790\u6a21\u578b\uff0c\u6d88\u9664\u6bd4\u4f8b\u98ce\u9669\u5047\u8bbe\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd", "motivation": "\u4f20\u7edf\u751f\u5b58\u6a21\u578b\uff08\u5982Cox\uff09\u4f9d\u8d56\u6bd4\u4f8b\u98ce\u9669\u7b49\u9650\u5236\u6027\u5047\u8bbe\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e34\u5e8a\u52a8\u6001\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982DeepSurv\u3001DeepHit\uff09\u867d\u7136\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528\uff1b\u73b0\u6709\u6df7\u5408\u6a21\u578b\uff08\u5982CoxKAN\uff09\u4ecd\u53d7\u534a\u53c2\u6570Cox\u6846\u67b6\u7ea6\u675f", "method": "\u63d0\u51faSurvKAN\uff0c\u4e00\u4e2a\u57fa\u4e8eKAN\u67b6\u6784\u7684\u5b8c\u5168\u53c2\u6570\u5316\u3001\u65f6\u95f4\u8fde\u7eed\u7684\u751f\u5b58\u6a21\u578b\u3002\u5c06\u65f6\u95f4\u4f5c\u4e3aKAN\u7684\u663e\u5f0f\u8f93\u5165\uff0c\u76f4\u63a5\u9884\u6d4b\u5bf9\u6570\u98ce\u9669\u51fd\u6570\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u4f18\u5316\u5b8c\u6574\u751f\u5b58\u4f3c\u7136\u3002\u67b6\u6784\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5355\u53d8\u91cf\u51fd\u6570\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u793a\u4e2a\u4f53\u7279\u5f81\u5982\u4f55\u968f\u65f6\u95f4\u5f71\u54cd\u98ce\u9669", "result": "\u5728\u6807\u51c6\u751f\u5b58\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSurvKAN\u5728\u4e00\u81f4\u6027\u6307\u6570\u548c\u6821\u51c6\u6307\u6807\u4e0a\u76f8\u6bd4\u7ecf\u5178\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u57fa\u7ebf\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u4e00\u81f4\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u6a21\u5f0f", "conclusion": "SurvKAN\u6210\u529f\u89e3\u51b3\u4e86\u751f\u5b58\u5206\u6790\u4e2d\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u53c2\u6570\u5316\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6d88\u9664\u4e86\u6bd4\u4f8b\u98ce\u9669\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e34\u5e8a\u53ef\u63a5\u53d7\u7684\u900f\u660e\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027"}}
{"id": "2602.02230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02230", "abs": "https://arxiv.org/abs/2602.02230", "authors": ["Ziyu Zhou", "Yuchen Fang", "Weilin Ruan", "Shiyu Wang", "James Kwok", "Yuxuan Liang"], "title": "SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting", "comment": "Under review", "summary": "Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.", "AI": {"tldr": "SEDformer\uff1a\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684IMTS\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u7a00\u758f-\u4e8b\u4ef6\u5bf9\u5076\u6027\u7279\u6027\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u8109\u51b2\u7f16\u7801\u548c\u538b\u7f29\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u80fd\u8017\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u548cTransformer\u7684\u9884\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217(IMTS)\u7684\u7a00\u758f-\u4e8b\u4ef6\u5bf9\u5076\u6027(SED)\u7279\u6027\uff1a\u901a\u8fc7\u586b\u5145\u5bf9\u9f50\u5230\u5747\u5300\u7f51\u683c\u8fdd\u53cd\u4e86\u7a00\u758f\u6027\uff0c\u800c\u5173\u7cfb\u91cd\u6784\u7834\u574f\u4e86\u5c40\u90e8\u65f6\u95f4\u8fde\u7eed\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5fe0\u5b9e\u4e8eIMTS SED\u7279\u6027\u7684\u5efa\u6a21\u8303\u5f0f\u3002", "method": "\u63d0\u51faSEDformer\uff1a1) SED-based Spike Encoder\u4f7f\u7528\u4e8b\u4ef6\u5bf9\u9f50LIF\u795e\u7ecf\u5143\u5c06\u539f\u59cb\u89c2\u6d4b\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u540c\u6b65\u8109\u51b2\uff1b2) Event-Preserving Temporal Downsampling\u6a21\u5757\u538b\u7f29\u957f\u95f4\u9699\u540c\u65f6\u4fdd\u7559\u663e\u8457\u8109\u51b2\uff1b3) \u5806\u53e0\u7684SED-based Spike Transformer\u5757\u4f7f\u7528\u57fa\u4e8e\u819c\u7535\u4f4d\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u5efa\u6a21\u5e8f\u5217\u5185\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5171IMTS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEDformer\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "SEDformer\u4e3aIMTS\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u7136\u4e14\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u5229\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\uff0c\u66f4\u597d\u5730\u5bf9\u9f50\u4e86IMTS\u7684\u7a00\u758f-\u4e8b\u4ef6\u5bf9\u5076\u6027\u672c\u8d28\u3002"}}
{"id": "2602.02264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02264", "abs": "https://arxiv.org/abs/2602.02264", "authors": ["Paolo Marcandelli", "Natansh Mathur", "Stefano Markidis", "Martina Siena", "Stefano Mariani"], "title": "Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training", "comment": "51 pages, 15 figures, 6 tables", "summary": "Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u7b56\u7565\u548cPhIS-FNO\u6a21\u578b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\u548c\u6837\u6761\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763PDE\u6c42\u89e3\uff0c\u8fbe\u5230\u63a5\u8fd1\u76d1\u7763\u5b66\u4e60\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u9700\u8981\u76d1\u7763\u6570\u636e\uff0c\u800c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u6536\u655b\u4e0d\u7a33\u5b9a\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u7269\u7406\u7ea6\u675f\u8fdb\u884c\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u53c8\u80fd\u7a33\u5b9a\u6536\u655b\u5e76\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u7b56\u7565\uff1a1) \u6e10\u8fdb\u5f0f\u5728\u635f\u5931\u51fd\u6570\u4e2d\u5b9e\u65bd\u8fb9\u754c\u6761\u4ef6\uff1b2) \u968f\u540e\u52a0\u5165\u5185\u90e8\u6b8b\u5dee\uff1b3) \u6bcf\u9636\u6bb5\u91cd\u65b0\u521d\u59cb\u5316\u4f18\u5316\u5668\u4f5c\u4e3a\u5ef6\u7eed\u673a\u5236\u3002\u540c\u65f6\u63d0\u51faPhIS-FNO\u6a21\u578b\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u5c42\u548cHermite\u6837\u6761\u6838\u8fdb\u884c\u5e73\u6ed1\u6b8b\u5dee\u8bc4\u4f30\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPhIS-FNO\u4ec5\u4f7f\u7528\u72ed\u7a84\u8fb9\u754c\u533a\u57df\u7684\u6807\u6ce8\u4fe1\u606f\uff0c\u8fbe\u5230\u4e86\u4e0e\u76d1\u7763\u5b66\u4e60\u76f8\u5f53\u7684\u7cbe\u5ea6\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5206\u9636\u6bb5\u3001\u57fa\u4e8e\u6837\u6761\u7684\u4f18\u5316\u4e3a\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684PDE\u6c42\u89e3\u3002"}}
{"id": "2602.02281", "categories": ["cs.LG", "cs.AI", "cs.NE", "physics.class-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.02281", "abs": "https://arxiv.org/abs/2602.02281", "authors": ["Antonino Emanuele Scurria"], "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time", "comment": "15 pages, 8 figures", "summary": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"Dyadic Backpropagation\"\u6846\u67b6\uff0c\u5c06\u53cd\u5411\u4f20\u64ad\u89e3\u91ca\u4e3a\u7269\u7406\u52a8\u529b\u7cfb\u7edf\u7684\u6709\u9650\u65f6\u95f4\u677e\u5f1b\u8fc7\u7a0b\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u7406\u8bba\u5728\u52a0\u500d\u72b6\u6001\u7a7a\u95f4\u63a8\u5bfc\u5168\u5c40\u80fd\u91cf\u51fd\u6570\uff0c\u5176\u978d\u70b9\u52a8\u529b\u5b66\u57282L\u6b65\u5185\u7cbe\u786e\u6062\u590d\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u3002", "motivation": "\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u88ab\u89c6\u4e3a\u7b26\u53f7\u8ba1\u7b97\uff0c\u4f46\u4f5c\u8005\u5e0c\u671b\u5c06\u5176\u7406\u89e3\u4e3a\u7269\u7406\u52a8\u529b\u7cfb\u7edf\u7684\u81ea\u7136\u6d8c\u73b0\u8fc7\u7a0b\uff0c\u4e3a\u6a21\u62df\u548c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u524d\u5411\u63a8\u7406\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u5e94\u7528\u975e\u4fdd\u5b88\u7cfb\u7edf\u7684\u62c9\u683c\u6717\u65e5\u7406\u8bba\u5904\u7406\u975e\u5bf9\u79f0\u4ea4\u4e92\uff0c\u5728\u7f16\u7801\u6fc0\u6d3b\u548c\u654f\u611f\u5ea6\u7684\u52a0\u500d\u72b6\u6001\u7a7a\u95f4\u6784\u9020\u5168\u5c40\u80fd\u91cf\u51fd\u6570\uff0c\u5206\u6790\u5176\u978d\u70b9\u52a8\u529b\u5b66\u3002", "result": "\u8bc1\u660e\u5355\u4f4d\u6b65\u957f\u6b27\u62c9\u79bb\u6563\u5316\u57282L\u6b65\u5185\u7cbe\u786e\u6062\u590d\u6807\u51c6\u53cd\u5411\u4f20\u64ad\uff0c\u65e0\u9700\u5bf9\u79f0\u6743\u91cd\u3001\u6e10\u8fd1\u6536\u655b\u6216\u5fae\u5c0f\u6270\u52a8\u7b49\u8fd1\u4f3c\u6761\u4ef6\u3002", "conclusion": "\u53cd\u5411\u4f20\u64ad\u662f\u8fde\u7eed\u7269\u7406\u677e\u5f1b\u8fc7\u7a0b\u7684\u6570\u5b57\u4f18\u5316\u5f71\u5b50\uff0c\u4e3a\u6a21\u62df\u548c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u7684\u7cbe\u786e\u68af\u5ea6\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e25\u683c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.02383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02383", "abs": "https://arxiv.org/abs/2602.02383", "authors": ["Maksim Afanasyev", "Illarion Iov"], "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization", "comment": null, "summary": "Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.", "AI": {"tldr": "SLIME\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u53c2\u8003\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u951a\u5b9a\u9879\u3001\u7a33\u5b9a\u60e9\u7f5a\u548c\u53cc\u8fb9\u754c\u673a\u5236\u89e3\u51b3DPO\u65b9\u6cd5\u4e2d\u7684\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9632\u6b62\u9ad8\u8d28\u91cf\u8f93\u51fa\u6982\u7387\u4e0b\u964d\u548c\u683c\u5f0f\u5d29\u6e83\u3002", "motivation": "\u73b0\u6709\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u5b58\u5728\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u4f18\u5316\u9009\u62e9\u4e0e\u62d2\u7edd\u54cd\u5e94\u4e4b\u95f4\u7684\u76f8\u5bf9\u8fb9\u754c\u4e0d\u80fd\u4fdd\u8bc1\u4fdd\u6301\u9009\u62e9\u54cd\u5e94\u7684\u7edd\u5bf9\u6982\u7387\uff0c\u5bfc\u81f4\"\u9057\u5fd8\"\uff08\u9ad8\u8d28\u91cf\u8f93\u51fa\u6982\u7387\u4e0b\u964d\uff09\u548c\"\u683c\u5f0f\u5d29\u6e83\"\uff08\u62d2\u7edd\u5e8f\u5217\u8fc7\u5ea6\u60e9\u7f5a\uff09\u3002", "method": "\u63d0\u51faSLIME\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u90e8\u5206\u76ee\u6807\uff1a(1)\u951a\u5b9a\u9879\u6700\u5927\u5316\u504f\u597d\u54cd\u5e94\u7684\u4f3c\u7136\uff1b(2)\u7a33\u5b9a\u60e9\u7f5a\u9632\u6b62\u62d2\u7edd\u6807\u8bb0\u6982\u7387\u5d29\u6e83\u4e3a\u96f6\uff1b(3)\u53cc\u8fb9\u754c\u673a\u5236\u7ed3\u5408\u786c\u7ea6\u675f\u548c\u8f6f\u7ea6\u675f\u8fdb\u884c\u7cbe\u786e\u8fb9\u754c\u5851\u9020\u3002", "result": "SLIME\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u751f\u6210\u7a33\u5b9a\u6027\u3002", "conclusion": "SLIME\u6210\u529f\u5730\u5c06\u504f\u597d\u5b66\u4e60\u4e0e\u751f\u6210\u8d28\u91cf\u89e3\u8026\uff0c\u89e3\u51b3\u4e86\u73b0\u6709DPO\u65b9\u6cd5\u4e2d\u7684\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u4f18\u8d8a\u7684\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2602.02405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02405", "abs": "https://arxiv.org/abs/2602.02405", "authors": ["Ethan Mendes", "Jungsoo Park", "Alan Ritter"], "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning", "comment": null, "summary": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.", "AI": {"tldr": "DAIL\u65b9\u6cd5\u901a\u8fc7\u4e24\u6b65\u6cd5\u5229\u7528\u5c11\u91cf\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff1a\u5148\u5c06\u4e13\u5bb6\u89e3\u7b54\u8f6c\u5316\u4e3a\u8be6\u7ec6\u63a8\u7406\u8f68\u8ff9\uff0c\u518d\u7528\u5bf9\u6bd4\u5b66\u4e60\u805a\u7126\u4e13\u5bb6\u6d1e\u5bdf\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6a21\u578b\u80fd\u91c7\u6837\u6b63\u786e\u89e3\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u8981\u4e48\u9700\u8981\u66f4\u5f3a\u6a21\u578b\u89e3\u51b3\u95ee\u9898\u3002\u4f46\u8bb8\u591a\u96be\u9898\u5bf9\u524d\u6cbf\u6a21\u578b\u4e5f\u96be\u89e3\uff0c\u65e0\u6cd5\u63d0\u53d6\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\u3002\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u9ad8\u4f46\u6602\u8d35\uff0c\u4e14\u76f4\u63a5\u6a21\u4eff\u4f1a\u5931\u8d25\uff0c\u56e0\u4e3a\u4e13\u5bb6\u89e3\u7b54\u662f\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\uff0c\u5305\u542b\u9690\u542b\u63a8\u7406\u8df3\u8dc3\uff0c\u4e0e\u6a21\u578b\u5206\u5e03\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5bf9\u9f50\u6a21\u4eff\u5b66\u4e60(DAIL)\uff1a\u7b2c\u4e00\u6b65\u5c06\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u8f6c\u5316\u4e3a\u8be6\u7ec6\u3001\u5206\u5e03\u5185\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u89e3\u51b3\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff1b\u7b2c\u4e8c\u6b65\u5e94\u7528\u5bf9\u6bd4\u76ee\u6807\uff0c\u4f7f\u5b66\u4e60\u805a\u7126\u4e8e\u4e13\u5bb6\u6d1e\u5bdf\u548c\u65b9\u6cd5\u8bba\u3002", "result": "\u4ec5\u7528\u4e0d\u52301000\u4e2a\u9ad8\u8d28\u91cf\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\uff0c\u5728Qwen2.5-Instruct\u548cQwen3\u6a21\u578b\u4e0a\u5b9e\u73b010-25%\u7684pass@k\u63d0\u5347\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad82-4\u500d\uff0c\u5e76\u5177\u5907\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DAIL\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u5c11\u91cf\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4e13\u5bb6\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u4e3a\u96be\u4ee5\u901a\u8fc7\u4f20\u7edf\u65b9\u6cd5\u89e3\u51b3\u7684\u590d\u6742\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
