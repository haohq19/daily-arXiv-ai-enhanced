<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes](https://arxiv.org/abs/2512.05136)
*Yujie Xiao,Gongzhen Tang,Deyun Zhang,Jun Li,Guangkun Nie,Haoyu Wang,Shun Huang,Tong Liu,Qinghao Zhao,Kangyin Chen,Shenda Hong*

Main category: cs.CV

TL;DR: 开发可解释的AI-ECG模型，通过心电图预测冠状动脉严重或完全狭窄，在内外验证集上表现良好，性能稳定且具有临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病是全球主要健康负担，需要准确识别罪犯血管和评估狭窄程度。虽然冠脉CTA是首选无创诊断方法，但其依赖高端设备、有辐射暴露且需要患者严格配合，限制了大规模应用。随着AI发展和心电图广泛可用，AI-ECG为CAD筛查提供了有前景的替代方案。

Method: 开发可解释的AI-ECG模型，预测四大主要冠状动脉（RCA、LM、LAD、LCX）在冠脉CTA上的严重或完全狭窄。在内部和外部验证集上评估模型性能，并在临床正常心电图亚组、不同人口统计学和采集时间亚组中进行分析。通过风险分层和可解释性分析揭示模型决策的关键电生理区域。

Result: 内部验证集AUC：RCA 0.794、LM 0.818、LAD 0.744、LCX 0.755；外部验证集AUC：RCA 0.749、LM 0.971、LAD 0.667、LCX 0.727。在临床正常心电图亚组中性能保持稳定。亚组分析确认模型在不同人群中的稳定性。风险分层显示校准和累积事件曲线一致分离。可解释性分析揭示高低风险组间波形差异，识别出影响模型决策的关键电生理区域。

Conclusion: 可解释的AI-ECG模型能够有效预测冠状动脉严重狭窄，在内外验证集上表现良好且稳定。该模型不仅提供可靠的预测性能，还通过可解释性分析揭示了冠状动脉狭窄的心电图相关特征，为CAD筛查提供了新的见解和潜在临床应用价值。

Abstract: Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.

</details>


### [2] [IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction](https://arxiv.org/abs/2512.05240)
*Dmitrii Torbunov,Onur Okuducu,Yi Huang,Odera Dim,Rebecca Coles,Yonggang Cui,Yihui Ren*

Main category: cs.CV

TL;DR: 提出混合捕获范式：结合稀疏RGB关键帧与连续事件流，离线重建完整RGB视频，降低功耗同时保持标准视频输出


<details>
  <summary>Details</summary>
Motivation: 传统RGB摄像头能耗高，事件相机功耗低但输出异步事件流而非标准视频。需要一种既能降低功耗又能生成标准视频的解决方案

Method: 提出IE2Video任务：从单个初始帧和后续事件数据重建RGB视频。研究两种架构：1）自回归模型（HyperE2VID）适配RGB生成；2）通过学习编码器和低秩适配将事件表示注入预训练文本到视频扩散模型（LTX）

Result: 基于扩散的方法比自回归基线感知质量提升33%（LPIPS 0.283 vs 0.422）。在三个事件相机数据集（BS-ERGB, HS-ERGB far/close）上验证，支持32-128帧序列长度，展示强大的跨数据集泛化能力

Conclusion: 混合捕获范式有效降低功耗同时保持视频质量，扩散方法优于自回归方法，具有实际应用潜力

Abstract: Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.

</details>


### [3] [HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models](https://arxiv.org/abs/2512.05746)
*Shizhuo Mao,Hongtao Zou,Qihu Xie,Song Chen,Yi Kang*

Main category: cs.CV

TL;DR: 提出HQ-DM量化感知训练框架，通过单哈达玛变换减少扩散模型激活矩阵中的异常值，在低比特量化下保持性能，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中应用广泛，但计算和内存成本高。现有量化方法难以处理推理过程中激活矩阵的异常值，导致低比特量化下性能显著下降

Method: 提出HQ-DM量化感知训练框架，采用单哈达玛变换处理激活矩阵，有效减少激活异常值，同时支持INT卷积操作并防止权重异常值放大

Result: 在ImageNet 256x256数据集上使用LDM-4模型，W4A4和W4A3量化方案分别将Inception Score提升12.8%和467.73%，显著优于现有最佳方法

Conclusion: HQ-DM框架通过单哈达玛变换有效解决了扩散模型量化中的激活异常值问题，在保持性能的同时显著降低了计算和存储开销

Abstract: Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.

</details>


### [4] [Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning](https://arxiv.org/abs/2512.05830)
*Muhammet Cagri Yeke,Samil Sirin,Kivilcim Yuksel,Abdurrahman Gumus*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的Phase-OTDR光纤事件检测方法，通过将一维数据转换为灰度图像（使用GADF、GASF和RP技术），组合成多通道RGB表示，并应用迁移学习模型，实现了对六种光纤事件的高精度分类。


<details>
  <summary>Details</summary>
Motivation: 传统Phase-OTDR数据分析方法在处理复杂光纤传感数据时存在局限性，需要更鲁棒和高效的分析技术来准确检测和分类光纤中的各种事件。

Method: 1. 将一维Phase-OTDR数据转换为灰度图像：使用Gramian Angular Difference Field (GADF)、Gramian Angular Summation Field (GASF)和Recurrence Plot (RP)三种技术；2. 将三种灰度图像组合成多通道RGB表示；3. 应用迁移学习模型（EfficientNetB0和DenseNet121）进行分类；4. 采用5折交叉验证评估模型性能。

Result: EfficientNetB0模型达到98.84%的分类准确率，DenseNet121达到98.24%的准确率。5折交叉验证测试准确率分别为99.07%和98.68%。该方法在减少数据集大小的同时提高了分析效率。

Conclusion: 图像化分析方法在解释复杂光纤传感数据方面具有变革性潜力，显著提高了光纤监测系统的准确性和可靠性。研究代码和图像化数据集已在GitHub公开，支持进一步研究。

Abstract: This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)
*Bruno Jacob,Amanda A. Howard,Panos Stinis*

Main category: cs.LG

TL;DR: 提出多保真度学习框架，用量子求解器生成低保真解，再用稀疏经典数据训练神经网络进行校正，克服量子硬件限制，实现高精度PDE求解。


<details>
  <summary>Details</summary>
Motivation: 量子PDE求解器面临近量子硬件的严重限制：量子比特数限制空间分辨率，电路深度限制长时间积分精度，这些硬件瓶颈使量子求解器只能提供低保真解，尽管理论上有计算加速潜力。

Method: 引入多保真度学习框架：首先用量子求解器生成丰富的低保真解作为训练数据，然后使用稀疏的高保真经典数据训练多保真度神经网络架构，该架构平衡线性和非线性变换来学习校正映射。

Result: 在粘性Burgers方程和不可压缩Navier-Stokes流动等非线性PDE基准测试中，该框架成功校正了粗网格量子预测，并实现了超出经典训练窗口的时间外推，预测精度可与经典方法竞争。

Conclusion: 通过弥合硬件受限的量子模拟与应用需求之间的差距，该工作为从当前量子设备中提取计算价值建立了途径，推进了近量子计算在计算物理中的算法开发和实际部署。

Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.

</details>


### [6] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试AI天气预报模型FourCastNetv2对输入噪声的鲁棒性，通过注入高斯噪声和完全随机初始条件来评估其对飓风预测的稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预报模型在输入噪声和不确定性下的鲁棒性对于极端天气事件（如飓风）的可靠性预测至关重要，需要了解模型对初始条件扰动的敏感度。

Method: 进行两个实验：1) 在飓风Florence的ERA5初始条件中注入不同水平的高斯噪声，分析对预测轨迹和强度的影响；2) 使用完全随机初始条件启动模型，观察模型对无意义输入的反应。

Result: FCNv2在低到中等噪声下能准确保持飓风特征；即使在高噪声下也能维持基本风暴轨迹和结构，但位置精度下降；模型在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预测。

Conclusion: FCNv2表现出对输入噪声的良好鲁棒性，倾向于生成稳定平滑的输出，但存在低估风暴强度的系统性偏差；该评估方法简单且可移植到其他数据驱动的AI天气预报模型。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [7] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD：一种反射式进化编排器，将LLM推理与反馈对齐的架构搜索相结合，通过多轮多专家共识、自适应反射探索和帕累托进化选择，实现高性能神经架构设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的神经架构设计系统存在挑战：令牌级设计循环是离散且不可微分的，反馈无法平滑指导架构改进，导致模式崩溃为冗余结构或漂移到不可行设计。

Method: 1. 多轮多专家共识：将孤立设计规则转化为有意义的架构线索；2. 自适应反射探索：利用奖励方差调整探索程度，不确定性时探索，稳定性达到时细化；3. 帕累托引导的进化选择：联合优化准确性、效率、延迟、置信度和结构多样性。

Result: 在CIFAR10、CIFAR100、ImageNet16-120、COCO-5K和Cityscape数据集上实现最先进性能，消融和迁移研究验证了RevoNAD在实际可靠和可部署神经架构设计中的有效性。

Conclusion: RevoNAD有效桥接LLM推理与反馈对齐的架构搜索，解决了现有LLM驱动生成方法的局限性，实现了高性能、可靠且可部署的神经架构设计。

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [8] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: IdealTSF框架利用非理想负样本增强时间序列预测，通过预训练、训练和优化三阶段，在噪声样本或低质量数据场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据中的缺失值和异常值阻碍深度学习预测性能，现有方法主要关注特征提取或将次优数据作为正样本进行知识迁移，而利用非理想负样本来增强事件预测是更有效的方法。

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据中提取知识；2) 训练阶段将序列数据转化为理想正样本；3) 应用带有对抗扰动的负优化机制。

Result: 大量实验表明，负样本数据在基本注意力架构中释放了时间序列预测的显著潜力，特别是在噪声样本或低质量数据场景中。

Conclusion: IdealTSF框架特别适合具有噪声样本或低质量数据的应用，通过整合理想正负样本，有效提升了时间序列预测性能。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [9] [Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks](https://arxiv.org/abs/2512.05868)
*Brian Ezinwoke,Oliver Rhodes*

Main category: cs.LG

TL;DR: 该研究将脉冲神经网络应用于高频交易中的价格尖峰预测，通过贝叶斯优化和新的惩罚性尖峰准确率目标函数，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统金融模型难以捕捉高频交易环境中的微秒级时间结构，而脉冲神经网络具有处理离散事件和保持毫秒级时序的自然优势，适合用于价格尖峰预测。

Method: 将高频股票数据转换为脉冲序列，评估三种架构：无监督STDP训练的SNN、具有显式抑制竞争的新型SNN、监督反向传播网络。使用贝叶斯优化和新的惩罚性尖峰准确率目标函数进行超参数调优。

Result: 使用PSA优化的模型在模拟交易中始终优于使用SA优化的模型和基线。扩展的SNN模型在简单回测中获得最高累计回报（76.8%），显著优于监督替代方案（42.54%回报）。

Conclusion: 研究验证了脉冲神经网络在通过任务特定目标函数进行鲁棒调优后，在高频交易价格尖峰预测中的有效潜力。

Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.

</details>


### [10] [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)
*Neha Gupta,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 提出基于LSTM的循环神经网络框架，用于估计分数泊松过程的参数，相比传统矩估计方法减少约55.3%的均方误差，并在实际高频数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 分数泊松过程能够建模具有记忆性和长程依赖的事件到达，但传统参数估计方法（如矩估计）可能不够准确。需要开发更精确的参数估计方法来处理具有复杂时间依赖性的实际数据。

Method: 使用基于长短期记忆网络的循环神经网络框架，从到达时间间隔序列中估计分数泊松过程的两个关键参数μ和β。LSTM能够有效建模时间依赖性，通过序列数据学习参数估计。

Result: 在合成数据上，相比传统矩估计方法，提出的LSTM方法减少了约55.3%的均方误差。在实际数据测试中，该方法能够有效跟踪蒙哥马利县紧急呼叫记录和AAPL股票交易数据的每日模式和参数变化。

Conclusion: 基于LSTM的神经网络框架能够有效估计分数泊松过程的参数，在合成和实际数据上都表现出优越性能，特别适用于具有复杂时间依赖性的高频数据。

Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $μ>0$ and $β\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN结合变分量子电路与Rainbow DQN，在人力资源分配问题上比经典方法提升4.9-13.4%性能


<details>
  <summary>Details</summary>
Motivation: 资源分配是NP难问题，经典深度强化学习方法受限于函数逼近器的表示能力，需要利用量子计算的叠加和纠缠特性来提升性能

Method: 提出变分量子Rainbow DQN(VQR-DQN)，将环拓扑变分量子电路集成到Rainbow DQN中，将人力资源分配问题建模为基于人员能力、事件调度和转移时间的MDP

Result: 在四个HRAP基准测试中，VQR-DQN相比随机基线减少26.8%归一化makespan，比Double DQN和经典Rainbow DQN提升4.9-13.4%性能

Conclusion: 电路表达能力、纠缠与策略质量的理论联系验证了量子增强DRL在大规模资源分配中的潜力

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>
