{"id": "2512.24077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24077", "abs": "https://arxiv.org/abs/2512.24077", "authors": ["Chunhui Wan", "Xunan Dai", "Zhuo Wang", "Minglei Li", "Yanpeng Wang", "Yinan Mao", "Yu Lan", "Zhiwen Xiao"], "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm", "comment": null, "summary": "The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike \"blind\" mutation operators, LoongFlow integrates LLMs into a cognitive \"Plan-Execute-Summarize\" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.", "AI": {"tldr": "LoongFlow\u662f\u4e00\u4e2a\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u96c6\u6210\u5230\"\u8ba1\u5212-\u6267\u884c-\u603b\u7ed3\"\u8ba4\u77e5\u8303\u5f0f\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fdb\u5316\u6548\u7387\uff0c\u5728\u7b97\u6cd5\u53d1\u73b0\u548c\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u4f18\u5316\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf60%\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u65b9\u6cd5\u5728\u4ece\u9759\u6001LLM\u5411\u81ea\u6211\u6539\u8fdb\u4ee3\u7406\u8fc7\u6e21\u65f6\u5b58\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u4ee3\u7801\u7a7a\u95f4\u4e2d\u5bb9\u6613\u8fc7\u65e9\u6536\u655b\u548c\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faLoongFlow\u6846\u67b6\uff0c\u5c06LLM\u96c6\u6210\u5230\"\u8ba1\u5212-\u6267\u884c-\u603b\u7ed3\"\u8ba4\u77e5\u8303\u5f0f\u4e2d\uff0c\u91c7\u7528\u6df7\u5408\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\u7ed3\u5408\u591a\u5c9b\u6a21\u578b\u3001MAP-Elites\u548c\u81ea\u9002\u5e94\u73bb\u5c14\u5179\u66fc\u9009\u62e9\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728AlphaEvolve\u57fa\u51c6\u6d4b\u8bd5\u548cKaggle\u7ade\u8d5b\u4e2d\uff0cLoongFlow\u6bd4OpenEvolve\u3001ShinkaEvolve\u7b49\u9886\u5148\u57fa\u7ebf\u5728\u8fdb\u5316\u6548\u7387\u4e0a\u63d0\u5347\u9ad8\u8fbe60%\uff0c\u540c\u65f6\u53d1\u73b0\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LoongFlow\u5728\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u751f\u6210\u4e13\u5bb6\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u6807\u5fd7\u7740\u4ece\u9759\u6001LLM\u5411\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2512.23755", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23755", "abs": "https://arxiv.org/abs/2512.23755", "authors": ["Sheo Yon Jhin", "Noseong Park"], "title": "HINTS: Extraction of Human Insights from Time-Series Without External Sources", "comment": "AAAI 2026 AI4TS Workshop paper", "summary": "Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.", "AI": {"tldr": "HINTS\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u65f6\u95f4\u5e8f\u5217\u6b8b\u5dee\u4e2d\u5185\u751f\u63d0\u53d6\u4eba\u7c7b\u56e0\u7d20\uff08\u793e\u4f1a\u5f71\u54cd\u3001\u8bb0\u5fc6\u548c\u504f\u89c1\uff09\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4eba\u7c7b\u51b3\u7b56\u3001\u60c5\u611f\u548c\u96c6\u4f53\u5fc3\u7406\u662f\u5f71\u54cd\u91d1\u878d\u7ecf\u6d4e\u7cfb\u7edf\u65f6\u95f4\u52a8\u6001\u7684\u590d\u6742\u56e0\u7d20\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6570\u636e\uff08\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\uff09\u6765\u6355\u6349\u8fd9\u4e9b\u56e0\u7d20\uff0c\u4f46\u5b58\u5728\u6570\u636e\u4f9d\u8d56\u6210\u672c\u9ad8\uff08\u8d22\u52a1\u3001\u8ba1\u7b97\u3001\u5b9e\u8df5\uff09\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHINTS\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528Friedkin-Johnsen\u610f\u89c1\u52a8\u529b\u5b66\u6a21\u578b\u4f5c\u4e3a\u7ed3\u6784\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ece\u65f6\u95f4\u5e8f\u5217\u6b8b\u5dee\u4e2d\u5185\u751f\u63d0\u53d6\u6f5c\u5728\u4eba\u7c7b\u56e0\u7d20\uff08\u793e\u4f1a\u5f71\u54cd\u3001\u8bb0\u5fc6\u3001\u504f\u89c1\u6a21\u5f0f\uff09\uff0c\u5e76\u5c06\u63d0\u53d6\u7684\u56e0\u7d20\u4f5c\u4e3a\u6ce8\u610f\u529b\u56fe\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u9aa8\u5e72\u6a21\u578b\u4e2d\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHINTS\u80fd\u6301\u7eed\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6848\u4f8b\u7814\u7a76\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86HINTS\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u793a\u63d0\u53d6\u7684\u56e0\u7d20\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u6709\u5f3a\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "HINTS\u65e0\u9700\u5916\u90e8\u6570\u636e\u5c31\u80fd\u6709\u6548\u63d0\u53d6\u4eba\u7c7b\u56e0\u7d20\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.23765", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23765", "abs": "https://arxiv.org/abs/2512.23765", "authors": ["Tiancheng Su", "Meicong Zhang", "Guoxiu He"], "title": "Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning", "comment": null, "summary": "Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.", "AI": {"tldr": "EASD\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u6d4b\u89e3\u7801\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u71b5\u60e9\u7f5a\u673a\u5236\uff0c\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u9ad8\u65f6\u62d2\u7edd\u5019\u9009token\uff0c\u8ba9\u76ee\u6807\u6a21\u578b\u91cd\u65b0\u91c7\u6837\uff0c\u4ece\u800c\u53ef\u80fd\u8d85\u8d8a\u76ee\u6807\u6a21\u578b\u81ea\u8eab\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u4e2d\uff0c\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u8fc7\u5ea6\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u6027\u80fd\u53ea\u80fd\u8fbe\u5230\u76ee\u6807\u6a21\u578b\u7684\u6c34\u5e73\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u8ba9\u63a8\u6d4b\u89e3\u7801\u6709\u53ef\u80fd\u8d85\u8d8a\u76ee\u6807\u6a21\u578b\u672c\u8eab\u7684\u6027\u80fd\u3002", "method": "\u5728\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u52a8\u6001\u71b5\u60e9\u7f5a\u673a\u5236\uff1a\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\uff0c\u5229\u7528\u91c7\u6837\u5206\u5e03\u7684\u71b5\u91cf\u5316\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002\u5f53\u4e24\u4e2a\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u9ad8\u71b5\u4e14\u5b83\u4eec\u7684top-N\u9884\u6d4b\u6709\u663e\u8457\u91cd\u53e0\u65f6\uff0c\u62d2\u7edd\u5bf9\u5e94token\uff0c\u8ba9\u76ee\u6807LLM\u91cd\u65b0\u91c7\u6837\uff0c\u9632\u6b62\u4f4e\u7f6e\u4fe1\u5ea6\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEASD\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u76ee\u6807LLM\u672c\u8eab\u7684\u6027\u80fd\u3002\u540c\u65f6\u8bc1\u660e\u4e86EASD\u7684\u6548\u7387\u4e0e\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u76f8\u5f53\u3002", "conclusion": "EASD\u901a\u8fc7\u71b5\u611f\u77e5\u7684\u52a8\u6001\u60e9\u7f5a\u673a\u5236\uff0c\u4e0d\u4ec5\u4fdd\u6301\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u6548\u7387\u4f18\u52bf\uff0c\u8fd8\u7a81\u7834\u4e86\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5b9e\u73b0\u4e86\u53ef\u80fd\u8d85\u8d8a\u76ee\u6807\u6a21\u578b\u6027\u80fd\u7684\u63a8\u7406\u52a0\u901f\u3002"}}
{"id": "2512.23773", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23773", "abs": "https://arxiv.org/abs/2512.23773", "authors": ["Molei Qin", "Xinyu Cai", "Yewen Li", "Haochong Xia", "Chuqiao Zong", "Shuo Sun", "Xinrun Wang", "Bo An"], "title": "FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading", "comment": null, "summary": "Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.", "AI": {"tldr": "FineFT\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u9ad8\u6760\u6746\u671f\u8d27\u4ea4\u6613\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u3001\u76c8\u5229\u80fd\u529b\u7b5b\u9009\u548cVAE\u5f15\u5bfc\u7684\u98ce\u9669\u7ba1\u7406\uff0c\u5728\u4fdd\u6301\u9ad8\u6536\u76ca\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u73b0\u8d27\u5e02\u573a\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u9ad8\u6760\u6746\u671f\u8d27\u4ea4\u6613\uff0c\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u9ad8\u6760\u6746\u653e\u5927\u5956\u52b1\u6ce2\u52a8\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u96be\u4ee5\u6536\u655b\uff1b2\uff09\u7f3a\u4e4f\u5bf9\u80fd\u529b\u8fb9\u754c\u7684\u81ea\u6211\u8ba4\u77e5\uff0c\u9047\u5230\u65b0\u5e02\u573a\u72b6\u6001\uff08\u5982\u9ed1\u5929\u9e45\u4e8b\u4ef6\uff09\u65f6\u9762\u4e34\u91cd\u5927\u635f\u5931\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u96c6\u6210RL\u6846\u67b6\uff1a\u9636\u6bb5I\u901a\u8fc7\u96c6\u6210TD\u8bef\u5dee\u9009\u62e9\u6027\u66f4\u65b0Q\u5b66\u4e60\u5668\u4ee5\u63d0\u9ad8\u6536\u655b\u6027\uff1b\u9636\u6bb5II\u57fa\u4e8e\u76c8\u5229\u80fd\u529b\u7b5b\u9009Q\u5b66\u4e60\u5668\uff0c\u5e76\u8bad\u7ec3VAE\u8bc6\u522b\u5b66\u4e60\u5668\u7684\u80fd\u529b\u8fb9\u754c\uff1b\u9636\u6bb5III\u6839\u636e\u8bad\u7ec3\u597d\u7684VAE\u6307\u5bfc\uff0c\u4ece\u7b5b\u9009\u540e\u7684\u96c6\u6210\u7b56\u7565\u548c\u4fdd\u5b88\u7b56\u7565\u4e2d\u9009\u62e9\uff0c\u4ee5\u7ef4\u6301\u76c8\u5229\u80fd\u529b\u5e76\u964d\u4f4e\u65b0\u5e02\u573a\u72b6\u6001\u98ce\u9669\u3002", "result": "\u5728\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u4e0b\u5bf9\u52a0\u5bc6\u8d27\u5e01\u671f\u8d27\u8fdb\u884c5\u500d\u6760\u6746\u5b9e\u9a8c\uff0cFineFT\u57286\u4e2a\u91d1\u878d\u6307\u6807\u4e0a\u4f18\u4e8e12\u4e2aSOTA\u57fa\u7ebf\uff0c\u98ce\u9669\u964d\u4f4e\u8d85\u8fc740%\u7684\u540c\u65f6\u83b7\u5f97\u4f18\u4e8e\u7b2c\u4e8c\u540d\u7684\u76c8\u5229\u80fd\u529b\u3002\u53ef\u89c6\u5316\u663e\u793a\u4e0d\u540c\u667a\u80fd\u4f53\u4e13\u6ce8\u4e8e\u4e0d\u540c\u5e02\u573a\u52a8\u6001\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9eVAE\u8def\u7531\u6709\u6548\u964d\u4f4e\u6700\u5927\u56de\u64a4\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u6539\u5584\u6536\u655b\u548c\u6027\u80fd\u3002", "conclusion": "FineFT\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u6760\u6746\u671f\u8d27\u4ea4\u6613\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u98ce\u9669\u7ba1\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u3001\u9009\u62e9\u6027\u66f4\u65b0\u548cVAE\u5f15\u5bfc\u7684\u80fd\u529b\u8fb9\u754c\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u9ad8\u6536\u76ca\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u98ce\u9669\u7684\u5e73\u8861\u3002"}}
{"id": "2512.24097", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.24097", "abs": "https://arxiv.org/abs/2512.24097", "authors": ["Wenzheng Zeng", "Difei Gao", "Mike Zheng Shou", "Hwee Tou Ng"], "title": "Factorized Learning for Temporally Grounded Video-Language Models", "comment": "ICCV 2025 paper. This arXiv version updates Figure 1 to include the concurrent work Qwen2.5-VL to ensure consistency with Table 1", "summary": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.", "AI": {"tldr": "D\u00b2VLM\uff1a\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u548c\u56e0\u5b50\u5316\u504f\u597d\u4f18\u5316\uff0c\u63d0\u5347\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u54cd\u5e94\u65b9\u9762\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u4ef6\u7ea7\u611f\u77e5\u7684\u65f6\u95f4\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u65f6\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u54cd\u5e94\u4e24\u4e2a\u4efb\u52a1\u901a\u5e38\u8026\u5408\u5904\u7406\uff0c\u7f3a\u4e4f\u6e05\u6670\u7684\u903b\u8f91\u5c42\u6b21\u7ed3\u6784\uff0c\u5bfc\u81f4\u6b21\u4f18\u76ee\u6807", "method": "\u63d0\u51faD\u00b2VLM\u6846\u67b6\uff0c\u89e3\u8026\u65f6\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u54cd\u5e94\u5b66\u4e60\uff0c\u91c7\u7528\"\u5148\u5b9a\u4f4d\u540e\u57fa\u4e8e\u8bc1\u636e\u56de\u7b54\"\u8303\u5f0f\uff0c\u5f15\u5165\u8bc1\u636e\u4ee4\u724c\u8fdb\u884c\u8bc1\u636e\u5b9a\u4f4d\uff1b\u63d0\u51fa\u56e0\u5b50\u5316\u504f\u597d\u4f18\u5316(FPO)\u7b97\u6cd5\uff0c\u5c06\u6982\u7387\u65f6\u95f4\u5b9a\u4f4d\u5efa\u6a21\u7eb3\u5165\u4f18\u5316\u76ee\u6807\uff1b\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u652f\u6301\u56e0\u5b50\u5316\u504f\u597d\u5b66\u4e60", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf", "conclusion": "\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u548c\u56e0\u5b50\u5316\u504f\u597d\u4f18\u5316\uff0cD\u00b2VLM\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u65f6\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u54cd\u5e94\u4efb\u52a1\uff0c\u63d0\u5347\u4e8b\u4ef6\u7ea7\u611f\u77e5\u80fd\u529b"}}
{"id": "2512.24120", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24120", "abs": "https://arxiv.org/abs/2512.24120", "authors": ["Chandini Vysyaraju", "Raghuvir Duvvuri", "Avi Goyal", "Dmitry Ignatov", "Radu Timofte"], "title": "Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design", "comment": null, "summary": "Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFSAP\uff08Few-Shot Architecture Prompting\uff09\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7814\u7a76LLM\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u67b6\u6784\u751f\u6210\u4e2d\u7684\u793a\u4f8b\u6570\u91cf\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u53bb\u91cd\u9a8c\u8bc1\u6280\u672f\uff0c\u57287\u4e2a\u89c6\u89c9\u57fa\u51c6\u4e0a\u751f\u62101900\u4e2a\u72ec\u7279\u67b6\u6784\u3002", "motivation": "\u81ea\u52a8\u5316\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u9762\u4e34\u4efb\u52a1\u591a\u6837\u6027\u548c\u8ba1\u7b97\u7ea6\u675f\u7684\u6311\u6218\u3002LLM\u4e3a\u66ff\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u7684NAS\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u4f46\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u67b6\u6784\u751f\u6210\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u63d0\u793a\u5de5\u7a0b\u548c\u9a8c\u8bc1\u7b56\u7565\u65b9\u9762\u3002", "method": "1. \u63d0\u51faFew-Shot Architecture Prompting (FSAP)\uff1a\u7cfb\u7edf\u7814\u7a76\u652f\u6301\u793a\u4f8b\u6570\u91cf\uff08n=1-6\uff09\u5bf9LLM\u751f\u6210\u89c6\u89c9\u67b6\u6784\u7684\u5f71\u54cd\uff1b2. \u5f15\u5165Whitespace-Normalized Hash Validation\uff1a\u8f7b\u91cf\u7ea7\u53bb\u91cd\u65b9\u6cd5\uff08<1ms\uff09\uff0c\u6bd4AST\u89e3\u6790\u5feb100\u500d\uff1b3. \u57fa\u4e8e\u4efb\u52a1\u65e0\u5173\u7684NNGPT/LEMUR\u6846\u67b6\uff1b4. \u57287\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0n=3\u4e2a\u793a\u4f8b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u6700\u4f73\u5e73\u8861\u67b6\u6784\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u805a\u7126\uff1b\u751f\u62101900\u4e2a\u72ec\u7279\u67b6\u6784\uff1b\u63d0\u51fa\u6570\u636e\u96c6\u5e73\u8861\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u89e3\u51b3\u5f02\u6784\u89c6\u89c9\u4efb\u52a1\u95f4\u7684\u67b6\u6784\u6bd4\u8f83\u6311\u6218\uff1b\u9a8c\u8bc1\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "conclusion": "\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u57fa\u4e8eLLM\u7684\u67b6\u6784\u641c\u7d22\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u6307\u5357\uff0c\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u5b9e\u8df5\uff0c\u4f7f\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u8005\u66f4\u5bb9\u6613\u8fdb\u884c\u81ea\u52a8\u5316\u8bbe\u8ba1\u3002"}}
{"id": "2512.24146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24146", "abs": "https://arxiv.org/abs/2512.24146", "authors": ["Chubin Chen", "Sujie Hu", "Jiashu Zhu", "Meiqi Wu", "Jintao Chen", "Yanxun Li", "Nisha Huang", "Chengyu Fang", "Jiahong Wu", "Xiangxiang Chu", "Xiu Li"], "title": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning", "comment": null, "summary": "Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.", "AI": {"tldr": "\u63d0\u51faD\u00b2-Align\u6846\u67b6\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u4e2d\u51fa\u73b0\u7684\u504f\u597d\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u65b9\u5411\u6027\u89e3\u8026\u5956\u52b1\u4fe1\u53f7\u6765\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u5728\u81ea\u52a8\u5316\u5956\u52b1\u6307\u6807\u4e0a\u83b7\u5f97\u9ad8\u5206\uff0c\u4f46\u4f1a\u5bfc\u81f4\u504f\u597d\u6a21\u5f0f\u5d29\u6e83\u2014\u2014\u6a21\u578b\u6536\u655b\u5230\u72ed\u7a84\u7684\u9ad8\u5206\u8f93\u51fa\uff08\u5982\u5355\u4e00\u98ce\u683c\u6216\u666e\u904d\u8fc7\u66dd\uff09\uff0c\u4e25\u91cd\u964d\u4f4e\u751f\u6210\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u65b9\u5411\u6027\u89e3\u8026\u5bf9\u9f50\u6846\u67b6\uff1a1\uff09\u5728\u5956\u52b1\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u65b9\u5411\u6027\u6821\u6b63\uff08\u4fdd\u6301\u6a21\u578b\u51bb\u7ed3\uff09\uff1b2\uff09\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5c06\u6821\u6b63\u5e94\u7528\u4e8e\u5956\u52b1\u4fe1\u53f7\uff0c\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u5230\u7279\u5b9a\u6a21\u5f0f\u3002", "result": "D\u00b2-Align\u5728\u8d28\u91cf\u548c\u591a\u6837\u6027\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u66f4\u597d\u5bf9\u9f50\uff0c\u540c\u65f6\u907f\u514d\u4e86\u504f\u597d\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u65b9\u5411\u6027\u6821\u6b63\u5956\u52b1\u4fe1\u53f7\uff0cD\u00b2-Align\u6709\u6548\u7f13\u89e3\u4e86\u504f\u597d\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u66f4\u597d\u5bf9\u9f50\u3002"}}
{"id": "2512.24138", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24138", "abs": "https://arxiv.org/abs/2512.24138", "authors": ["Haoran He", "Yuxiao Ye", "Jie Liu", "Jiajun Liang", "Zhiyong Wang", "Ziyang Yuan", "Xintao Wang", "Hangyu Mao", "Pengfei Wan", "Ling Pan"], "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking", "comment": "17 pages. Project: https://tinnerhrhe.github.io/gardo_project", "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.", "AI": {"tldr": "GARDO\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u6b63\u5219\u5316\u3001\u81ea\u9002\u5e94\u53c2\u8003\u6a21\u578b\u66f4\u65b0\u548c\u591a\u6837\u6027\u5956\u52b1\u589e\u5f3a\uff0c\u89e3\u51b3\u6269\u6563\u6a21\u578bRL\u5fae\u8c03\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u3001\u63a2\u7d22\u4e0d\u8db3\u548c\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898", "motivation": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u53ef\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\uff0c\u4f46\u4ee3\u7406\u5956\u52b1\u4e0e\u771f\u5b9e\u76ee\u6807\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff08\u4ee3\u7406\u5206\u6570\u4e0a\u5347\u4f46\u771f\u5b9e\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3001\u591a\u6837\u6027\u5d29\u6e83\uff09\u3002\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u4f7f\u7528\u6b21\u4f18\u53c2\u8003\u7b56\u7565\uff0c\u4f1a\u635f\u5bb3\u6837\u672c\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faGARDO\u6846\u67b6\uff1a1\uff09\u9009\u62e9\u6027\u6b63\u5219\u5316\uff1a\u4ec5\u5bf9\u9ad8\u4e0d\u786e\u5b9a\u6027\u6837\u672c\u65bd\u52a0\u60e9\u7f5a\uff1b2\uff09\u81ea\u9002\u5e94\u6b63\u5219\u5316\uff1a\u5b9a\u671f\u66f4\u65b0\u53c2\u8003\u6a21\u578b\u4ee5\u5339\u914d\u5728\u7ebf\u7b56\u7565\u80fd\u529b\uff1b3\uff09\u591a\u6837\u6027\u611f\u77e5\u4f18\u5316\uff1a\u5bf9\u9ad8\u8d28\u91cf\u4e14\u9ad8\u591a\u6837\u6027\u7684\u6837\u672c\u589e\u5f3a\u5956\u52b1\uff0c\u9f13\u52b1\u6a21\u5f0f\u8986\u76d6\u3002", "result": "\u5728\u591a\u79cd\u4ee3\u7406\u5956\u52b1\u548c\u672a\u89c1\u6307\u6807\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGARDO\u80fd\u6709\u6548\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u3001\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6837\u672c\u6548\u7387\u6216\u63a2\u7d22\u80fd\u529b\uff0c\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GARDO\u901a\u8fc7\u9009\u62e9\u6027\u6b63\u5219\u5316\u3001\u81ea\u9002\u5e94\u53c2\u8003\u6a21\u578b\u66f4\u65b0\u548c\u591a\u6837\u6027\u5956\u52b1\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578bRL\u5fae\u8c03\u4e2d\u5956\u52b1\u9ed1\u5ba2\u3001\u63a2\u7d22\u4e0d\u8db3\u548c\u6a21\u5f0f\u5d29\u6e83\u7684\u7ade\u4e89\u9700\u6c42\uff0c\u4e3a\u89c6\u89c9\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2512.24243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24243", "abs": "https://arxiv.org/abs/2512.24243", "authors": ["Fuqiang Gu", "Yuanke Li", "Xianlei Long", "Kangping Ji", "Chao Chen", "Qingyi Gu", "Zhenliang Ni"], "title": "MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation", "comment": "Accepted by AAAI 2026", "summary": "Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.", "AI": {"tldr": "MambaSeg\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u4f7f\u7528\u5e76\u884cMamba\u7f16\u7801\u5668\u5904\u7406RGB\u56fe\u50cf\u548c\u4e8b\u4ef6\u6d41\uff0c\u901a\u8fc7\u53cc\u7ef4\u4ea4\u4e92\u6a21\u5757\uff08DDIM\uff09\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u878d\u5408\uff0c\u5728DDD17\u548cDSEC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "RGB\u65b9\u6cd5\u5728\u5feb\u901f\u8fd0\u52a8\u3001\u4f4e\u5149\u7167\u6216\u9ad8\u52a8\u6001\u8303\u56f4\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u5ef6\u8fdf\u4f46\u7f3a\u4e4f\u989c\u8272\u548c\u7eb9\u7406\u3002\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u4e14\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u878d\u5408\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u6d41\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027\u3002", "method": "\u63d0\u51faMambaSeg\u53cc\u5206\u652f\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u4f7f\u7528\u5e76\u884cMamba\u7f16\u7801\u5668\u5206\u522b\u5904\u7406RGB\u56fe\u50cf\u548c\u4e8b\u4ef6\u6d41\u3002\u5f15\u5165\u53cc\u7ef4\u4ea4\u4e92\u6a21\u5757\uff08DDIM\uff09\uff0c\u5305\u542b\u8de8\u7a7a\u95f4\u4ea4\u4e92\u6a21\u5757\uff08CSIM\uff09\u548c\u8de8\u65f6\u95f4\u4ea4\u4e92\u6a21\u5757\uff08CTIM\uff09\uff0c\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u878d\u5408\uff0c\u6539\u5584\u8de8\u6a21\u6001\u5bf9\u9f50\u5e76\u51cf\u5c11\u6b67\u4e49\u3002", "result": "\u5728DDD17\u548cDSEC\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMambaSeg\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u591a\u6a21\u6001\u611f\u77e5\u6f5c\u529b\u3002", "conclusion": "MambaSeg\u901a\u8fc7\u5e76\u884cMamba\u7f16\u7801\u5668\u548c\u53cc\u7ef4\u4ea4\u4e92\u6a21\u5757\uff0c\u6709\u6548\u878d\u5408RGB\u548c\u4e8b\u4ef6\u6570\u636e\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u9c81\u68d2\u591a\u6a21\u6001\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.24404", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24404", "abs": "https://arxiv.org/abs/2512.24404", "authors": ["Soham Pahari", "M. Srinivas"], "title": "Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning", "comment": null, "summary": "Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.", "AI": {"tldr": "ViReLoc\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u63a8\u7406\u8fdb\u884c\u7a7a\u95f4\u89c4\u5212\u548c\u5b9a\u4f4d\uff0c\u65e0\u9700GPS\u6570\u636e\uff0c\u5728\u5bfc\u822a\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\uff0c\u9650\u5236\u4e86\u5176\u5728\u7a7a\u95f4\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u5bfc\u822a\u548c\u5730\u7406\u5b9a\u4f4d\uff09\u4e2d\u7684\u6709\u6548\u6027\u3002\u6587\u672c\u63a8\u7406\u96be\u4ee5\u7406\u89e3\u7a7a\u95f4\u4f9d\u8d56\u548c\u51e0\u4f55\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u63a8\u7406\u8303\u5f0f\u3002", "method": "\u63d0\u51faGeo-Consistent Visual Planning\u8303\u5f0f\uff0c\u5f00\u53d1ViReLoc\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8868\u793a\u8fdb\u884c\u89c4\u5212\u548c\u5b9a\u4f4d\u3002\u65b9\u6cd5\u5305\u62ec\uff1a\u5b66\u4e60\u7a7a\u95f4\u4f9d\u8d56\u548c\u51e0\u4f55\u5173\u7cfb\uff0c\u5728\u89c6\u89c9\u57df\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u4f18\u5316\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7279\u5f81\u4ea4\u4e92\u6765\u5bf9\u9f50\u8de8\u89c6\u89d2\u5e76\u51cf\u5c11\u89c6\u89d2\u5dee\u5f02\u3002", "result": "\u5728\u591a\u79cd\u5bfc\u822a\u548c\u5b9a\u4f4d\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u663e\u793a\uff0cViReLoc\u5728\u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u6027\u548c\u8de8\u89c6\u89d2\u68c0\u7d22\u6027\u80fd\u65b9\u9762\u6301\u7eed\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9\u63a8\u7406\u5728\u5bfc\u822a\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u89c6\u89c9\u63a8\u7406\u662f\u5bfc\u822a\u548c\u5b9a\u4f4d\u7684\u5f3a\u5927\u8865\u5145\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u5b9e\u65f6GPS\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\uff0c\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7a7a\u95f4\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.24408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24408", "abs": "https://arxiv.org/abs/2512.24408", "authors": ["Bohong Chen", "Haiyang Liu"], "title": "DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model", "comment": "Project Page: https://robinwitch.github.io/DyStream-Page", "summary": "Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stream-friendly autoregressive framework with flow-matching heads for probabilistic modeling, and (2) We propose a causal encoder enhanced by a lookahead module to incorporate short future context (e.g., 60 ms) to improve quality while maintaining low latency. Our analysis shows this simple-and-effective method significantly surpass alternative causal strategies, including distillation and generative encoder. Extensive experiments show that DyStream could generate video within 34 ms per frame, guaranteeing the entire system latency remains under 100 ms. Besides, it achieves state-of-the-art lip-sync quality, with offline and online LipSync Confidence scores of 8.13 and 7.61 on HDTF, respectively. The model, weights and codes are available.", "AI": {"tldr": "DyStream\uff1a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5b9e\u65f6\u751f\u6210\u8bf4\u8bdd\u4eba-\u542c\u8005\u53cc\u5411\u5bf9\u8bdd\u89c6\u9891\uff0c\u5ef6\u8fdf\u4f4e\u4e8e100ms\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5507\u90e8\u540c\u6b65", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u5757\u7684\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u7684\u975e\u56e0\u679c\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5bfc\u81f4\u663e\u8457\u5ef6\u8fdf\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5bf9\u8bdd\u4e2d\u542c\u8005\u5373\u65f6\u975e\u8bed\u8a00\u53cd\u9988\u7684\u9700\u6c42", "method": "\u91c7\u7528\u6d41\u53cb\u597d\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u7ed3\u5408\u6d41\u5339\u914d\u5934\u90e8\u8fdb\u884c\u6982\u7387\u5efa\u6a21\uff1b\u63d0\u51fa\u56e0\u679c\u7f16\u7801\u5668\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u77bb\u6a21\u5757\u5f15\u5165\u77ed\u672a\u6765\u4e0a\u4e0b\u6587\uff0860ms\uff09\u4ee5\u63d0\u5347\u8d28\u91cf\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf", "result": "\u6bcf\u5e27\u751f\u6210\u65f6\u95f434ms\uff0c\u7cfb\u7edf\u603b\u5ef6\u8fdf\u4f4e\u4e8e100ms\uff1b\u5728HDTF\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u79bb\u7ebf8.13\u548c\u5728\u7ebf7.61\u7684LipSync Confidence\u5206\u6570\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5507\u90e8\u540c\u6b65\u8d28\u91cf", "conclusion": "DyStream\u901a\u8fc7\u6d41\u5339\u914d\u81ea\u56de\u5f52\u6846\u67b6\u548c\u56e0\u679c\u7f16\u7801\u5668\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u8d28\u91cf\u7684\u8bf4\u8bdd\u4eba-\u542c\u8005\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u56e0\u679c\u7b56\u7565"}}
{"id": "2512.25026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.25026", "abs": "https://arxiv.org/abs/2512.25026", "authors": ["Nasim Borazjanizadeh", "James McClelland"], "title": "Modeling Language as a Sequence of Thoughts", "comment": null, "summary": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level \"thought\" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.", "AI": {"tldr": "\u63d0\u51faThought Gestalt\u6a21\u578b\uff0c\u4e00\u79cd\u53cc\u5c42\u6b21\u62bd\u8c61\uff08token\u548c\u53e5\u5b50\u7ea7\"\u601d\u60f3\"\u72b6\u6001\uff09\u7684\u5faa\u73afTransformer\uff0c\u901a\u8fc7\u8bb0\u5fc6\u673a\u5236\u63d0\u5347\u8bed\u8a00\u5efa\u6a21\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u6570\u636e\u6548\u7387", "motivation": "\u4f20\u7edfTransformer\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8868\u5c42\u5171\u73b0\u7edf\u8ba1\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u4f53\u548c\u4e8b\u4ef6\u7684\u5168\u5c40\u4e00\u81f4\u6f5c\u5728\u8868\u793a\uff0c\u5bfc\u81f4\u5173\u7cfb\u65b9\u5411\u8106\u5f31\uff08\u5982\u9006\u8f6c\u8bc5\u5492\uff09\u3001\u4e0a\u4e0b\u6587\u9519\u8bef\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u3002\u800c\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u8868\u660e\uff0c\u4eba\u7c7b\u7406\u89e3\u8bed\u8a00\u65f6\u4f1a\u5c06\u5176\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u4e8b\u4ef6\u5f0f\u8868\u793a\u5e76\u6301\u4e45\u8bb0\u5fc6", "method": "\u63d0\u51faThought Gestalt\u6a21\u578b\uff1a1\uff09\u53cc\u5c42\u6b21\u62bd\u8c61\u5efa\u6a21\uff08token\u548c\u53e5\u5b50\u7ea7\"\u601d\u60f3\"\u72b6\u6001\uff09\uff1b2\uff09\u5faa\u73afTransformer\u7ed3\u6784\uff0c\u9010\u53e5\u751f\u6210token\u65f6\u4ea4\u53c9\u5173\u6ce8\u5148\u524d\u53e5\u5b50\u8868\u793a\u7684\u8bb0\u5fc6\uff1b3\uff09\u4f7f\u7528\u76f8\u540c\u53c2\u6570\u96c6\u751f\u6210token\u548c\u53e5\u5b50\u8868\u793a\uff0c\u901a\u8fc7\u5355\u4e00\u76ee\u6807\uff08\u4e0b\u4e00\u4e2atoken\u4ea4\u53c9\u71b5\uff09\u8bad\u7ec3\uff1b4\uff09\u4fdd\u7559\u5199\u5165\u8bb0\u5fc6\u7684\u53e5\u5b50\u8868\u793a\u8ba1\u7b97\u56fe\uff0c\u4f7f\u672a\u6765token\u635f\u5931\u7684\u68af\u5ea6\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u53cd\u5411\u4f20\u64ad\u4f18\u5316\u65e9\u671f\u53e5\u5b50\u5411\u91cf\u751f\u6210\u53c2\u6570", "result": "1\uff09\u5728\u6269\u5c55\u5b9e\u9a8c\u4e2d\uff0cTG\u76f8\u6bd4\u5339\u914d\u7684GPT-2\u548c\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u6301\u7eed\u63d0\u5347\u6548\u7387\uff0c\u7f29\u653e\u62df\u5408\u8868\u660eGPT-2\u9700\u8981\u591a5-8%\u7684\u6570\u636e\u548c33-42%\u7684\u53c2\u6570\u624d\u80fd\u8fbe\u5230TG\u7684\u635f\u5931\u6c34\u5e73\uff1b2\uff09\u5728\u7236\u5b50\u5173\u7cfb\u9006\u8f6c\u8bc5\u5492\u63a2\u6d4b\u4efb\u52a1\u4e0a\uff0cTG\u51cf\u5c11\u4e86\u5173\u7cfb\u65b9\u5411\u6cdb\u5316\u9519\u8bef", "conclusion": "Thought Gestalt\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u7684\u53e5\u5b50\u7ea7\"\u601d\u60f3\"\u72b6\u6001\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u5173\u7cfb\u65b9\u5411\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2512.25070", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.25070", "abs": "https://arxiv.org/abs/2512.25070", "authors": ["Nikhil Chandak", "Shashwat Goel", "Ameya Prabhu", "Moritz Hardt", "Jonas Geiping"], "title": "Scaling Open-Ended Reasoning to Predict the Future", "comment": "45 pages", "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.", "AI": {"tldr": "\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u5f0f\u9884\u6d4b\uff0c\u4f7f\u7528\u81ea\u52a8\u5316\u65b9\u6cd5\u4ece\u65b0\u95fb\u751f\u6210\u9884\u6d4b\u95ee\u9898\uff0c\u5f00\u53d1OpenForecaster 8B\u6a21\u578b\uff0c\u5728\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u66f4\u5927\u89c4\u6a21\u7684\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u9ad8\u98ce\u9669\u51b3\u7b56\u9700\u8981\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u672a\u6765\u8fdb\u884c\u63a8\u7406\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u5f00\u653e\u5f0f\u9884\u6d4b\u7684\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u5316\u65b9\u6cd5\u4ece\u65e5\u5e38\u65b0\u95fb\u4e2d\u5408\u6210\u9884\u6d4b\u95ee\u9898\uff0c\u521b\u5efaOpenForesight\u6570\u636e\u96c6\uff0c\u8bad\u7ec3Qwen3\u601d\u7ef4\u6a21\u578b\uff0c\u91c7\u7528\u79bb\u7ebf\u65b0\u95fb\u8bed\u6599\u9632\u6b62\u4fe1\u606f\u6cc4\u9732\uff0c\u7ed3\u5408\u68c0\u7d22\u548c\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u3002", "result": "OpenForecaster 8B\u6a21\u578b\u57282025\u5e745-8\u6708\u7684\u6d4b\u8bd5\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u66f4\u5927\u89c4\u6a21\u7684\u4e13\u6709\u6a21\u578b\uff0c\u4e14\u6821\u51c6\u6539\u8fdb\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u548c\u4e13\u95e8\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u9ad8\u6027\u80fd\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5f00\u6e90\u6240\u6709\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7814\u7a76\u3002"}}
{"id": "2512.24639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24639", "abs": "https://arxiv.org/abs/2512.24639", "authors": ["Siyang Wang", "Hanting Li", "Wei Li", "Jie Hu", "Xinghao Chen", "Feng Zhao"], "title": "From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation", "comment": null, "summary": "Inspired by the remarkable success of autoregressive models in language modeling, this paradigm has been widely adopted in visual generation. However, the sequential token-by-token decoding mechanism inherent in traditional autoregressive models leads to low inference efficiency.In this paper, we propose RadAR, an efficient and parallelizable framework designed to accelerate autoregressive visual generation while preserving its representational capacity. Our approach is motivated by the observation that visual tokens exhibit strong local dependencies and spatial correlations with their neighbors--a property not fully exploited in standard raster-scan decoding orders. Specifically, we organize the generation process around a radial topology: an initial token is selected as the starting point, and all other tokens are systematically grouped into multiple concentric rings according to their spatial distances from this center. Generation then proceeds in a ring-wise manner, from inner to outer regions, enabling the parallel prediction of all tokens within the same ring. This design not only preserves the structural locality and spatial coherence of visual scenes but also substantially increases parallelization. Furthermore, to address the risk of inconsistent predictions arising from simultaneous token generation with limited context, we introduce a nested attention mechanism. This mechanism dynamically refines implausible outputs during the forward pass, thereby mitigating error accumulation and preventing model collapse. By integrating radial parallel prediction with dynamic output correction, RadAR significantly improves generation efficiency.", "AI": {"tldr": "RadAR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f84\u5411\u62d3\u6251\u7684\u5e76\u884c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u72b6\u5e76\u884c\u9884\u6d4b\u548c\u5d4c\u5957\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u91c7\u7528\u987a\u5e8f\u89e3\u7801\u673a\u5236\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u89c6\u89c9token\u5177\u6709\u5f3a\u70c8\u7684\u5c40\u90e8\u4f9d\u8d56\u6027\u548c\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u4f46\u6807\u51c6\u5149\u6805\u626b\u63cf\u89e3\u7801\u987a\u5e8f\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e00\u7279\u6027\u3002", "method": "\u91c7\u7528\u5f84\u5411\u62d3\u6251\u7ec4\u7ec7\u751f\u6210\u8fc7\u7a0b\uff1a\u9009\u62e9\u521d\u59cbtoken\u4f5c\u4e3a\u8d77\u70b9\uff0c\u5c06\u6240\u6709\u5176\u4ed6token\u6309\u7a7a\u95f4\u8ddd\u79bb\u5206\u7ec4\u5230\u591a\u4e2a\u540c\u5fc3\u73af\u4e2d\u3002\u4ece\u5185\u5230\u5916\u8fdb\u884c\u73af\u72b6\u751f\u6210\uff0c\u5b9e\u73b0\u540c\u4e00\u73af\u5185\u6240\u6709token\u7684\u5e76\u884c\u9884\u6d4b\u3002\u5f15\u5165\u5d4c\u5957\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u4fee\u6b63\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u8f93\u51fa\u3002", "result": "RadAR\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002\u901a\u8fc7\u5e76\u884c\u9884\u6d4b\u548c\u52a8\u6001\u8f93\u51fa\u4fee\u6b63\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u9519\u8bef\u7d2f\u79ef\u548c\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "RadAR\u901a\u8fc7\u521b\u65b0\u7684\u5f84\u5411\u5e76\u884c\u9884\u6d4b\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.24731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24731", "abs": "https://arxiv.org/abs/2512.24731", "authors": ["Bingxuan Li", "Yiming Cui", "Yicheng He", "Yiwei Wang", "Shu Zhang", "Longyin Wen", "Yulei Niu"], "title": "EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation", "comment": null, "summary": "Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.", "AI": {"tldr": "EchoFoley\u63d0\u51fa\u89c6\u9891\u58f0\u97f3\u751f\u6210\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u4e8b\u4ef6\u8868\u793a\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6162\u5feb\u601d\u8003\u7b56\u7565\u7684\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u53ef\u63a7\u6027\u548c\u97f3\u8d28\u3002", "motivation": "\u5f53\u524d\u89c6\u9891-\u6587\u672c\u5230\u97f3\u9891\uff08VT2A\uff09\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09\u89c6\u89c9\u4e0e\u6587\u672c\u6761\u4ef6\u4e0d\u5e73\u8861\u5bfc\u81f4\u89c6\u89c9\u4e3b\u5bfc\uff1b2\uff09\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\u7684\u5177\u4f53\u5b9a\u4e49\uff1b3\uff09\u6307\u4ee4\u7406\u89e3\u80fd\u529b\u5f31\uff0c\u73b0\u6709\u6570\u636e\u96c6\u4f9d\u8d56\u7b80\u77ed\u5206\u7c7b\u6807\u7b7e\u3002", "method": "\u63d0\u51faEchoFoley\u4efb\u52a1\uff0c\u4f7f\u7528\u7b26\u53f7\u5316\u58f0\u97f3\u4e8b\u4ef6\u8868\u793a\uff08\u6307\u5b9a\u4f55\u65f6\u3001\u4ec0\u4e48\u3001\u5982\u4f55\u4ea7\u751f\u58f0\u97f3\uff09\uff0c\u6784\u5efaEchoFoley-6k\u5927\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5f00\u53d1EchoVidia\u6846\u67b6\u91c7\u7528\u6162\u5feb\u601d\u8003\u7b56\u7565\u8fdb\u884c\u4ee5\u58f0\u97f3\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u667a\u80fd\u751f\u6210\u3002", "result": "EchoVidia\u5728\u53ef\u63a7\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709VT2A\u6a21\u578b40.7%\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u63d0\u534712.5%\u3002", "conclusion": "EchoFoley\u4efb\u52a1\u548cEchoVidia\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VT2A\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7b26\u53f7\u5316\u63a7\u5236\u548c\u667a\u80fd\u751f\u6210\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u58f0\u97f3\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2512.24922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24922", "abs": "https://arxiv.org/abs/2512.24922", "authors": ["Bart\u0142omiej Olber", "Jakub Winter", "Pawe\u0142 Wawrzy\u0144ski", "Andrii Gamalii", "Daniel G\u00f3rniak", "Marcin \u0141ojek", "Robert Nowak", "Krystian Radlak"], "title": "Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection", "comment": null, "summary": "3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u7684LiDAR\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4ec5\u9700\u6807\u6ce8\u76ee\u6807\u57df\u4e2d\u5c11\u91cf\u4ee3\u8868\u6027\u6837\u672c\u5373\u53ef\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u6280\u672f\u9632\u6b62\u6743\u91cd\u6f02\u79fb\u3002", "motivation": "3D\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\uff08\u5982\u7f8e\u56fd\u3001\u4e9a\u6d32\u3001\u6b27\u6d32\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u9009\u62e9\u76ee\u6807\u57df\u4e2d\u5c11\u91cf\u4ee3\u8868\u6027\u4e14\u591a\u6837\u5316\u7684\u6837\u672c\u8fdb\u884c\u6807\u6ce8\uff0c\u7ed3\u5408\u53d7\u6301\u7eed\u5b66\u4e60\u542f\u53d1\u7684\u540e\u8bad\u7ec3\u6280\u672f\u9632\u6b62\u539f\u59cb\u6a21\u578b\u6743\u91cd\u6f02\u79fb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57df\u81ea\u9002\u5e94\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u7ebf\u6027\u63a2\u6d4b\u548c\u73b0\u6709SOTA\u57df\u81ea\u9002\u5e94\u6280\u672f\uff0c\u4e14\u4ec5\u9700\u6781\u5c0f\u7684\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u9009\u62e9\u5173\u952e\u6837\u672c\u5e76\u914d\u5408\u6301\u7eed\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u6807\u6ce8\u6210\u672c\u5b9e\u73b0\u9ad8\u6548\u7684LiDAR\u57df\u81ea\u9002\u5e94\uff0c\u63d0\u53473D\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
