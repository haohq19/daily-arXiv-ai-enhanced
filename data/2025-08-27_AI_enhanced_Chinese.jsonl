{"id": "2508.18397", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18397", "abs": "https://arxiv.org/abs/2508.18397", "authors": ["Antonio Guillen-Perez"], "title": "Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning", "comment": null, "summary": "Offline Reinforcement Learning (RL) presents a promising paradigm for\ntraining autonomous vehicle (AV) planning policies from large-scale, real-world\ndriving logs. However, the extreme data imbalance in these logs, where mundane\nscenarios vastly outnumber rare \"long-tail\" events, leads to brittle and unsafe\npolicies when using standard uniform data sampling. In this work, we address\nthis challenge through a systematic, large-scale comparative study of data\ncuration strategies designed to focus the learning process on information-rich\nsamples. We investigate six distinct criticality weighting schemes which are\ncategorized into three families: heuristic-based, uncertainty-based, and\nbehavior-based. These are evaluated at two temporal scales, the individual\ntimestep and the complete scenario. We train seven goal-conditioned\nConservative Q-Learning (CQL) agents with a state-of-the-art, attention-based\narchitecture and evaluate them in the high-fidelity Waymax simulator. Our\nresults demonstrate that all data curation methods significantly outperform the\nbaseline. Notably, data-driven curation using model uncertainty as a signal\nachieves the most significant safety improvements, reducing the collision rate\nby nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear\ntrade-off where timestep-level weighting excels at reactive safety while\nscenario-level weighting improves long-horizon planning. Our work provides a\ncomprehensive framework for data curation in Offline RL and underscores that\nintelligent, non-uniform sampling is a critical component for building safe and\nreliable autonomous agents.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u6bd4\u8f83\u7814\u7a76\uff0c\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u516d\u79cd\u5173\u952e\u6027\u52a0\u6743\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u80fd\uff0c\u5176\u4e2d\u57fa\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u5c06\u78b0\u649e\u7387\u4ece16.0%\u964d\u81f35.5%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u6781\u7aef\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u666e\u901a\u573a\u666f\u8fdc\u591a\u4e8e\u7f55\u89c1\"\u957f\u5c3e\"\u4e8b\u4ef6\uff0c\u5bfc\u81f4\u6807\u51c6\u5747\u5300\u6570\u636e\u91c7\u6837\u4ea7\u751f\u8106\u5f31\u4e14\u4e0d\u5b89\u5168\u7684\u7b56\u7565\u3002", "method": "\u7814\u7a76\u4e86\u516d\u79cd\u5173\u952e\u6027\u52a0\u6743\u65b9\u6848\uff0c\u5206\u4e3a\u4e09\u7c7b\uff1a\u542f\u53d1\u5f0f\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u884c\u4e3a\u57fa\u7840\u65b9\u6cd5\uff0c\u5728\u4e24\u4e2a\u65f6\u95f4\u5c3a\u5ea6\uff08\u5355\u65f6\u95f4\u6b65\u548c\u5b8c\u6574\u573a\u666f\uff09\u8bc4\u4f30\uff0c\u8bad\u7ec3\u4e86\u4e03\u4e2a\u76ee\u6807\u6761\u4ef6\u4fdd\u5b88Q\u5b66\u4e60\u667a\u80fd\u4f53\u3002", "result": "\u6240\u6709\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u57fa\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u5b9e\u73b0\u6700\u5927\u5b89\u5168\u6539\u8fdb\uff0c\u78b0\u649e\u7387\u964d\u4f4e\u8fd1\u4e09\u500d\uff08\u4ece16.0%\u964d\u81f35.5%\uff09\u3002\u65f6\u95f4\u6b65\u7ea7\u52a0\u6743\u64c5\u957f\u53cd\u5e94\u6027\u5b89\u5168\uff0c\u573a\u666f\u7ea7\u52a0\u6743\u6539\u5584\u957f\u671f\u89c4\u5212\u3002", "conclusion": "\u667a\u80fd\u975e\u5747\u5300\u91c7\u6837\u662f\u6784\u5efa\u5b89\u5168\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7b5b\u9009\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002"}}
{"id": "2508.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18460", "abs": "https://arxiv.org/abs/2508.18460", "authors": ["Tianze Liu", "Md Abu Bakr Siddique", "Hongyu An"], "title": "Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models", "comment": null, "summary": "Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable\nprowess across various cognitive tasks using extensive training data. However,\nthe reliance on large datasets and neural networks presents challenges such as\nhighpower consumption and limited adaptability, particularly in\nSWaP-constrained applications like planetary exploration. To address these\nissues, we propose enhancing the autonomous capabilities of intelligent robots\nby emulating the associative learning observed in animals. Associative learning\nenables animals to adapt to their environment by memorizing concurrent events.\nBy replicating this mechanism, neuromorphic robots can navigate dynamic\nenvironments autonomously, learning from interactions to optimize performance.\nThis paper explores the emulation of associative learning in rodents using\nneuromorphic robots within open-field maze environments, leveraging insights\nfrom spatial cells such as place and grid cells. By integrating these models,\nwe aim to enable online associative learning for spatial tasks in real-time\nscenarios, bridging the gap between biological spatial cognition and robotics\nfor advancements in autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u52a8\u7269\u8054\u60f3\u5b66\u4e60\u673a\u5236\u6765\u589e\u5f3a\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edfAI\u65b9\u6cd5\u5728\u529f\u8017\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u884c\u661f\u63a2\u7d22\u7b49SWaP\u53d7\u9650\u5e94\u7528\u4e2d\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u7684AI\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u5b58\u5728\u9ad8\u529f\u8017\u548c\u6709\u9650\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u592a\u7a7a\u63a2\u7d22\u7b49\u5e94\u7528\u4e2d\u3002\u52a8\u7269\u901a\u8fc7\u8054\u60f3\u5b66\u4e60\u80fd\u591f\u6709\u6548\u9002\u5e94\u73af\u5883\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7075\u611f\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u556e\u9f7f\u7c7b\u52a8\u7269\u7684\u8054\u60f3\u5b66\u4e60\u673a\u5236\uff0c\u5229\u7528\u7a7a\u95f4\u7ec6\u80de\uff08\u4f4d\u7f6e\u7ec6\u80de\u548c\u7f51\u683c\u7ec6\u80de\uff09\u7684\u795e\u7ecf\u6a21\u578b\uff0c\u5728\u5f00\u653e\u573a\u5730\u8ff7\u5bab\u73af\u5883\u4e2d\u5b9e\u73b0\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u7684\u5728\u7ebf\u8054\u60f3\u5b66\u4e60\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u548c\u4f18\u5316\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u80fd\u591f\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u8fdb\u884c\u7a7a\u95f4\u4efb\u52a1\u7684\u5728\u7ebf\u8054\u60f3\u5b66\u4e60\uff0c\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u6765\u81ea\u4e3b\u9002\u5e94\u548c\u4f18\u5316\u884c\u4e3a\uff0c\u964d\u4f4e\u4e86\u529f\u8017\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u3002", "conclusion": "\u901a\u8fc7\u4eff\u751f\u5b66\u65b9\u6cd5\u6a21\u62df\u52a8\u7269\u8054\u60f3\u5b66\u4e60\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u5728\u7a7a\u95f4\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u751f\u7269\u7a7a\u95f4\u8ba4\u77e5\u4e0e\u673a\u5668\u4eba\u6280\u672f\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.18289", "categories": ["cs.LG", "I.2; J.2"], "pdf": "https://arxiv.org/pdf/2508.18289", "abs": "https://arxiv.org/abs/2508.18289", "authors": ["Mateus A. Fernandes", "Michael M. Furlanetti", "Eduardo Gildin", "Marcio A. Sampaio"], "title": "Data-driven models for production forecasting and decision supporting in petroleum reservoirs", "comment": "Manuscript as submitted to Journal of Petroleum Exploration and\n  Production Technology", "summary": "Forecasting production reliably and anticipating changes in the behavior of\nrock-fluid systems are the main challenges in petroleum reservoir engineering.\nThis project proposes to deal with this problem through a data-driven approach\nand using machine learning methods. The objective is to develop a methodology\nto forecast production parameters based on simple data as produced and injected\nvolumes and, eventually, gauges located in wells, without depending on\ninformation from geological models, fluid properties or details of well\ncompletions and flow systems. Initially, we performed relevance analyses of the\nproduction and injection variables, as well as conditioning the data to suit\nthe problem. As reservoir conditions change over time, concept drift is a\npriority concern and require special attention to those observation windows and\nthe periodicity of retraining, which are also objects of study. For the\nproduction forecasts, we study supervised learning methods, such as those based\non regressions and Neural Networks, to define the most suitable for our\napplication in terms of performance and complexity. In a first step, we\nevaluate the methodology using synthetic data generated from the UNISIM III\ncompositional simulation model. Next, we applied it to cases of real plays in\nthe Brazilian pre-salt. The expected result is the design of a reliable\npredictor for reproducing reservoir dynamics, with rapid response, capability\nof dealing with practical difficulties such as restrictions in wells and\nprocessing units, and that can be used in actions to support reservoir\nmanagement, including the anticipation of deleterious behaviors, optimization\nof production and injection parameters and the analysis of the effects of\nprobabilistic events, aiming to maximize oil recovery.", "AI": {"tldr": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u5730\u8d28\u6a21\u578b\u548c\u6d41\u4f53\u6027\u8d28\uff0c\u901a\u8fc7\u751f\u4ea7\u6ce8\u5165\u6570\u636e\u9884\u6d4b\u6cb9\u6c14\u7530\u4ea7\u91cf", "motivation": "\u89e3\u51b3\u6cb9\u6c14\u853d\u5c42\u5de5\u7a0b\u4e2d\u751f\u4ea7\u9884\u6d4b\u7684\u4e3b\u8981\u6311\u6218\uff0c\u9884\u5149\u5ca9\u77f3-\u6d41\u4f53\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\uff0c\u51cf\u5c11\u5bf9\u5730\u8d28\u6a21\u578b\u548c\u6d41\u4f53\u6027\u8d28\u4fe1\u606f\u7684\u4f9d\u8d56", "method": "\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u56de\u5f52\u548c\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u901a\u8fc7\u751f\u4ea7\u6ce8\u5165\u53d8\u91cf\u76f8\u5173\u6027\u5206\u6790\u548c\u6570\u636e\u5904\u7406\uff0c\u5904\u7406\u6982\u5ff5\u6f0f\u6dc0\u95ee\u9898\uff0c\u5148\u7528\u7ec4\u5408\u6a21\u578b\u9a8c\u8bc1\u540e\u5e94\u7528\u4e8e\u5df4\u897f\u76d6\u76d0\u5c42\u5b9e\u9645\u6848\u4f8b", "result": "\u5f00\u53d1\u51fa\u53ef\u9760\u7684\u9884\u6d4b\u5668\uff0c\u80fd\u591f\u5feb\u901f\u54cd\u5e94\u3001\u5904\u7406\u5b9e\u9645\u56f0\u96be\uff08\u5982\u4e95\u5c04\u548c\u5904\u7406\u8bbe\u5907\u9650\u5236\uff09\uff0c\u91cd\u73b0\u853d\u5c42\u52a8\u6001", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u652f\u6301\u853d\u5c42\u7ba1\u7406\uff0c\u9884\u963b\u574f\u884c\u4e3a\u3001\u4f18\u5316\u751f\u4ea7\u6ce8\u5165\u53c2\u6570\u3001\u5206\u6790\u6982\u7387\u4e8b\u4ef6\u5f71\u54cd\uff0c\u6700\u5927\u5316\u6cb9\u6c14\u91c7\u53d6\u7387\uff0c\u4e3a\u6cb9\u6c14\u7530\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.18463", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18463", "abs": "https://arxiv.org/abs/2508.18463", "authors": ["Md. Rashid Shahriar Khan", "Md. Abrar Hasan", "Mohammod Tareq Aziz Justice"], "title": "Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling", "comment": "11 pages, 7 figures, 4 tables", "summary": "Detecting anomalies in surveillance footage is inherently challenging due to\ntheir unpredictable and context-dependent nature. This work introduces a novel\ncontext-aware zero-shot anomaly detection framework that identifies abnormal\nevents without exposure to anomaly examples during training. The proposed\nhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal\ndynamics and semantic context. TimeSformer serves as the vision backbone to\nextract rich spatial-temporal features, while DPC forecasts future\nrepresentations to identify temporal deviations. Furthermore, a CLIP-based\nsemantic stream enables concept-level anomaly detection through\ncontext-specific text prompts. These components are jointly trained using\nInfoNCE and CPC losses, aligning visual inputs with their temporal and semantic\nrepresentations. A context-gating mechanism further enhances decision-making by\nmodulating predictions with scene-aware cues or global video features. By\nintegrating predictive modeling with vision-language understanding, the system\ncan generalize to previously unseen behaviors in complex environments. This\nframework bridges the gap between temporal reasoning and semantic context in\nzero-shot anomaly detection for surveillance. The code for this research has\nbeen made available at\nhttps://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408TimeSformer\u3001DPC\u548cCLIP\u6a21\u578b\u6765\u68c0\u6d4b\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u5f02\u5e38\u4e8b\u4ef6\uff0c\u65e0\u9700\u5728\u8bad\u7ec3\u65f6\u63a5\u89e6\u5f02\u5e38\u6837\u672c\u3002", "motivation": "\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5f02\u5e38\u4e8b\u4ef6\u4e0d\u53ef\u9884\u6d4b\u4e14\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5f02\u5e38\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u96f6\u6837\u672c\u65b9\u6cd5\u53ef\u4ee5\u68c0\u6d4b\u672a\u89c1\u8fc7\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff1aTimeSformer\u4f5c\u4e3a\u89c6\u89c9\u9aa8\u5e72\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0cDPC\u9884\u6d4b\u672a\u6765\u8868\u793a\u68c0\u6d4b\u65f6\u95f4\u504f\u5dee\uff0cCLIP\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5b9e\u73b0\u6982\u5ff5\u7ea7\u5f02\u5e38\u68c0\u6d4b\u3002\u4f7f\u7528InfoNCE\u548cCPC\u635f\u5931\u8054\u5408\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u95e8\u63a7\u673a\u5236\u589e\u5f3a\u51b3\u7b56\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6574\u5408\u9884\u6d4b\u5efa\u6a21\u548c\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u884c\u4e3a\uff0c\u5f25\u5408\u4e86\u65f6\u95f4\u63a8\u7406\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76d1\u63a7\u89c6\u9891\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2508.18651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18651", "abs": "https://arxiv.org/abs/2508.18651", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "\u63d0\u51faCollaborative Decoding (CoDe)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6574\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u7684\u8f93\u51fa\u6982\u7387\uff0c\u6253\u7834\u5fe0\u5b9e\u6027\u548c\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u6574\u5408\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65f6\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u5fe0\u5b9e\u6027\u548c\u8868\u8fbe\u6027\uff0c\u8981\u4e48\u7f3a\u4e4f\u77e5\u8bc6\u652f\u6301\u800c\u635f\u5bb3\u5fe0\u5b9e\u6027\uff0c\u8981\u4e48\u8fc7\u4e8e\u5197\u957f\u800c\u727a\u7272\u8868\u8fbe\u6027\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u89e3\u7801(CoDe)\u6846\u67b6\uff0c\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u52a8\u6001\u6574\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u7684\u8f93\u51fa\u6982\u7387\uff0c\u5e76\u5f15\u5165\u77e5\u8bc6\u611f\u77e5\u91cd\u6392\u5e8f\u673a\u5236\u9632\u6b62\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoDe\u6846\u67b6\u5728\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u80fd\u663e\u8457\u63d0\u5347\u5fe0\u5b9e\u6027\u800c\u4e0d\u635f\u5bb3\u8868\u8fbe\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoDe\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u6574\u5408\u4e2d\u5fe0\u5b9e\u6027\u4e0e\u8868\u8fbe\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2508.18748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18748", "abs": "https://arxiv.org/abs/2508.18748", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "ChronoRAG\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u53d9\u4e8b\u6587\u672c\u7684\u65b0\u578bRAG\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u8fde\u8d2f\u6bb5\u843d\u548c\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u6765\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u6027\u80fd", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u53d9\u4e8b\u6587\u672c\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u53d9\u4e8b\u7406\u89e3\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u548c\u6bb5\u843d\u95f4\u7684\u65f6\u5e8f\u5173\u7cfb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b64\u7acb\u7247\u6bb5", "method": "\u63d0\u51faChronoRAG\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u5c06\u5206\u6563\u6587\u6863\u4fe1\u606f\u91cd\u6784\u4e3a\u8fde\u8d2f\u7ed3\u6784\u5316\u6bb5\u843d\uff1b\u901a\u8fc7\u663e\u5f0f\u6355\u83b7\u548c\u7ef4\u62a4\u68c0\u7d22\u6bb5\u843d\u7684\u65f6\u95f4\u987a\u5e8f\u6765\u4fdd\u6301\u53d9\u4e8b\u6d41", "result": "\u5728NarrativeQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cChronoRAG\u5728\u9700\u8981\u4e8b\u5b9e\u8bc6\u522b\u548c\u590d\u6742\u65f6\u5e8f\u5173\u7cfb\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb", "conclusion": "\u65f6\u5e8f\u987a\u5e8f\u63a8\u7406\u5bf9\u4e8e\u89e3\u51b3\u53d9\u4e8b\u95ee\u7b54\u81f3\u5173\u91cd\u8981\uff0cChronoRAG\u6709\u6548\u89e3\u51b3\u4e86\u53d9\u4e8b\u6587\u672c\u7279\u6709\u7684\u6311\u6218"}}
{"id": "2508.18723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18723", "abs": "https://arxiv.org/abs/2508.18723", "authors": ["Hiroaki Aizawa", "Yuta Naito", "Kohei Fukuda"], "title": "Class-wise Flooding Regularization for Imbalanced Image Classification", "comment": "Accepted to ACPR2025", "summary": "The purpose of training neural networks is to achieve high generalization\nperformance on unseen inputs. However, when trained on imbalanced datasets, a\nmodel's prediction tends to favor majority classes over minority classes,\nleading to significant degradation in the recognition performance of minority\nclasses. To address this issue, we propose class-wise flooding regularization,\nan extension of flooding regularization applied at the class level. Flooding is\na regularization technique that mitigates overfitting by preventing the\ntraining loss from falling below a predefined threshold, known as the flooding\nlevel, thereby discouraging memorization. Our proposed method assigns a\nclass-specific flooding level based on class frequencies. By doing so, it\nsuppresses overfitting in majority classes while allowing sufficient learning\nfor minority classes. We validate our approach on imbalanced image\nclassification. Compared to conventional flooding regularizations, our method\nimproves the classification performance of minority classes and achieves better\noverall generalization.", "AI": {"tldr": "\u7c7b\u522b\u6d4b\u6d17\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u7c7b\u522b\u9891\u7387\u8bbe\u7f6e\u4e0d\u540c\u7684\u6d4b\u6d17\u6c34\u5e73\uff0c\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u5c11\u6570\u7c7b\u7684\u8bc6\u522b\u6027\u80fd\u548c\u6574\u4f53\u6cdb\u5316\u80fd\u529b", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u5bf9\u591a\u6570\u7c7b\u504f\u89c1\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5c11\u6570\u7c7b\u7684\u8bc6\u522b\u6027\u80fd", "method": "\u63d0\u51fa\u7c7b\u522b\u6d4b\u6d17\u6b63\u5219\u5316\uff0c\u6839\u636e\u7c7b\u522b\u9891\u7387\u8bbe\u7f6e\u4e0d\u540c\u7684\u6d4b\u6d17\u6c34\u5e73\uff0c\u538b\u5236\u591a\u6570\u7c7b\u8fc7\u62df\u5408\u540c\u65f6\u5145\u5206\u5b66\u4e60\u5c11\u6570\u7c7b", "result": "\u5728\u4e0d\u5e73\u8861\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u65b9\u6cd5\u5728\u5c11\u6570\u7c7b\u5206\u7c7b\u6027\u80fd\u548c\u6574\u4f53\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u8d85\u8fc7\u4f20\u7edf\u6d4b\u6d17\u6b63\u5219\u5316\u65b9\u6cd5", "conclusion": "\u7c7b\u522b\u6d4b\u6d17\u6b63\u5219\u5316\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u540c\u65f6\u63d0\u9ad8\u5c11\u6570\u7c7b\u8bc6\u522b\u6027\u80fd\u548c\u6574\u4f53\u6cdb\u5316\u6027\u80fd"}}
{"id": "2508.19094", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19094", "abs": "https://arxiv.org/abs/2508.19094", "authors": ["Vincenzo Polizzi", "Stephen Yang", "Quentin Clark", "Jonathan Kelly", "Igor Gilitschenski", "David B. Lindell"], "title": "VibES: Induced Vibration for Persistent Event-Based Sensing", "comment": null, "summary": "Event cameras are a bio-inspired class of sensors that asynchronously measure\nper-pixel intensity changes. Under fixed illumination conditions in static or\nlow-motion scenes, rigidly mounted event cameras are unable to generate any\nevents, becoming unsuitable for most computer vision tasks. To address this\nlimitation, recent work has investigated motion-induced event stimulation that\noften requires complex hardware or additional optical components. In contrast,\nwe introduce a lightweight approach to sustain persistent event generation by\nemploying a simple rotating unbalanced mass to induce periodic vibrational\nmotion. This is combined with a motion-compensation pipeline that removes the\ninjected motion and yields clean, motion-corrected events for downstream\nperception tasks. We demonstrate our approach with a hardware prototype and\nevaluate it on real-world captured datasets. Our method reliably recovers\nmotion parameters and improves both image reconstruction and edge detection\nover event-based sensing without motion induction.", "AI": {"tldr": "\u901a\u8fc7\u91c7\u7528\u7b80\u5355\u7684\u65cb\u8f6c\u4e0d\u5e73\u8861\u8d28\u91cf\u4ea7\u751f\u5468\u671f\u6027\u9707\u52a8\uff0c\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u5728\u9759\u6001\u573a\u666f\u4e2d\u65e0\u6cd5\u4ea7\u751f\u4e8b\u4ef6\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u8fd0\u52a8\u8865\u507f\u7ba1\u9053\u83b7\u5f97\u6e05\u6d01\u7684\u4e8b\u4ef6\u6570\u636e\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u56fa\u5b9a\u7167\u660e\u548c\u9759\u6001/\u4f4e\u8fd0\u52a8\u573a\u666f\u4e2d\u65e0\u6cd5\u4ea7\u751f\u4e8b\u4ef6\uff0c\u5f71\u54cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6267\u884c\u3002\u73b0\u6709\u65b9\u6848\u9700\u8981\u590d\u6742\u786c\u4ef6\u6216\u989d\u5916\u5149\u5b66\u7ec4\u4ef6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u65cb\u8f6c\u4e0d\u5e73\u8861\u8d28\u91cf\u4ea7\u751f\u5468\u671f\u6027\u9707\u52a8\u6fc0\u53d1\u4e8b\u4ef6\uff0c\u7ed3\u5408\u8fd0\u52a8\u8865\u507f\u7ba1\u9053\u53bb\u9664\u6ce8\u5165\u8fd0\u52a8\uff0c\u4ea7\u751f\u6e05\u6d01\u7684\u7ecf\u8fd0\u52a8\u6821\u6b63\u7684\u4e8b\u4ef6\u6570\u636e\u3002", "result": "\u5728\u5b9e\u9645\u91c7\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u53ef\u9760\u5730\u6062\u590d\u8fd0\u52a8\u53c2\u6570\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u548c\u8fb9\u7f18\u68c0\u6d4b\u4efb\u52a1\u4e0a\u90fd\u8d85\u8d8a\u4e86\u65e0\u8fd0\u52a8\u6fc0\u53d1\u7684\u4e8b\u4ef6\u611f\u77e5\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u901a\u8fc7\u4eba\u5de5\u9707\u52a8\u6fc0\u53d1\u548c\u8fd0\u52a8\u8865\u507f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u9759\u6001\u573a\u666f\u4e2d\u7684\u95ee\u9898\uff0c\u4e3a\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4e8b\u4ef6\u6570\u636e\u3002"}}
{"id": "2508.18785", "categories": ["eess.SP", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18785", "abs": "https://arxiv.org/abs/2508.18785", "authors": ["Luqing Luo", "Wenjin Gui", "Yunfei Liu", "Ziyue Zhang", "Yunxi Zhang", "Fengxiang Wang", "Zonghao Guo", "Zizhi Ma", "Xinzhu Liu", "Hanxiang He", "Jinhai Li", "Xin Qiu", "Wupeng Xie", "Yangang Sun"], "title": "EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding", "comment": null, "summary": "Deep understanding of electromagnetic signals is fundamental to dynamic\nspectrum management, intelligent transportation, autonomous driving and\nunmanned vehicle perception. The field faces challenges because electromagnetic\nsignals differ greatly from text and images, showing high heterogeneity, strong\nbackground noise and complex joint time frequency structure, which prevents\nexisting general models from direct use. Electromagnetic communication and\nsensing tasks are diverse, current methods lack cross task generalization and\ntransfer efficiency, and the scarcity of large high quality datasets blocks the\ncreation of a truly general multitask learning framework. To overcome these\nissue, we introduce EMind, an electromagnetic signals foundation model that\nbridges large scale pretraining and the unique nature of this modality. We\nbuild the first unified and largest standardized electromagnetic signal dataset\ncovering multiple signal types and tasks. By exploiting the physical properties\nof electromagnetic signals, we devise a length adaptive multi-signal packing\nmethod and a hardware-aware training strategy that enable efficient use and\nrepresentation learning from heterogeneous multi-source signals. Experiments\nshow that EMind achieves strong performance and broad generalization across\nmany downstream tasks, moving decisively from task specific models to a unified\nframework for electromagnetic intelligence. The code is available at:\nhttps://github.com/GabrielleTse/EMind.", "AI": {"tldr": "EMind\u662f\u4e00\u4e2a\u7535\u78c1\u4fe1\u53f7\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u7269\u7406\u7279\u6027\u5229\u7528\uff0c\u89e3\u51b3\u4e86\u7535\u78c1\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5f02\u6784\u6027\u3001\u566a\u58f0\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u7535\u78c1\u4fe1\u53f7\u4e0e\u6587\u672c\u56fe\u50cf\u5dee\u5f02\u5927\uff0c\u5b58\u5728\u9ad8\u5f02\u6784\u6027\u3001\u5f3a\u80cc\u666f\u566a\u58f0\u548c\u590d\u6742\u65f6\u9891\u7ed3\u6784\uff0c\u73b0\u6709\u901a\u7528\u6a21\u578b\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\uff0c\u4e14\u7f3a\u4e4f\u8de8\u4efb\u52a1\u6cdb\u5316\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u9996\u4e2a\u7edf\u4e00\u6807\u51c6\u5316\u7535\u78c1\u4fe1\u53f7\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u957f\u5ea6\u81ea\u9002\u5e94\u591a\u4fe1\u53f7\u6253\u5305\u65b9\u6cd5\u548c\u786c\u4ef6\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u7535\u78c1\u4fe1\u53f7\u7269\u7406\u7279\u6027\u8fdb\u884c\u8868\u793a\u5b66\u4e60\u3002", "result": "EMind\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u548c\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4ece\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u5230\u7edf\u4e00\u7535\u78c1\u667a\u80fd\u6846\u67b6\u7684\u8f6c\u53d8\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7535\u78c1\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u7535\u78c1\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.18787", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18787", "abs": "https://arxiv.org/abs/2508.18787", "authors": ["Constantino \u00c1lvarez Casado", "Sasan Sharifipour", "Manuel Lage Ca\u00f1ellas", "Nhi Nguyen", "Le Nguyen", "Miguel Bordallo L\u00f3pez"], "title": "Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring", "comment": "23 pages, 2 figures, 10 formulas, 3 tables", "summary": "The growing integration of smart environments and low-power computing\ndevices, coupled with mass-market sensor technologies, is driving advancements\nin remote and non-contact physiological monitoring. However, deploying these\nsystems in real-time on resource-constrained platforms introduces significant\nchallenges related to scalability, interoperability, and performance. This\npaper presents a real-time remote photoplethysmography (rPPG) system optimized\nfor low-power devices, designed to extract physiological signals, such as heart\nrate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial\nvideo streams. The system is built on the Face2PPG pipeline, which processes\nvideo frames sequentially for rPPG signal extraction and analysis, while\nleveraging a multithreaded architecture to manage video capture, real-time\nprocessing, network communication, and graphical user interface (GUI) updates\nconcurrently. This design ensures continuous, reliable operation at 30 frames\nper second (fps), with adaptive feedback through a collaborative user interface\nto guide optimal signal capture conditions. The network interface includes both\nan HTTP server for continuous video streaming and a RESTful API for on-demand\nvital sign retrieval. To ensure accurate performance despite the limitations of\nlow-power devices, we use a hybrid programming model combining Functional\nReactive Programming (FRP) and the Actor Model, allowing event-driven\nprocessing and efficient task parallelization. The system is evaluated under\nreal-time constraints, demonstrating robustness while minimizing computational\noverhead. Our work addresses key challenges in real-time biosignal monitoring,\noffering practical solutions for optimizing performance in modern healthcare\nand human-computer interaction applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4f4e\u529f\u8017\u8bbe\u5907\u4f18\u5316\u7684\u5b9e\u65f6\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2(rPPG)\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u9762\u90e8\u89c6\u9891\u6d41\u4e2d\u63d0\u53d6\u5fc3\u7387\u3001\u547c\u5438\u9891\u7387\u548c\u8840\u6c27\u9971\u548c\u5ea6\u7b49\u751f\u7406\u4fe1\u53f7\uff0c\u91c7\u7528\u591a\u7ebf\u7a0b\u67b6\u6784\u548c\u6df7\u5408\u7f16\u7a0b\u6a21\u578b\u786e\u4fdd30fps\u7684\u8fde\u7eed\u53ef\u9760\u8fd0\u884c\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u73af\u5883\u548c\u4f4e\u529f\u8017\u8ba1\u7b97\u8bbe\u5907\u7684\u65e5\u76ca\u96c6\u6210\uff0c\u4ee5\u53ca\u5927\u4f17\u5e02\u573a\u4f20\u611f\u5668\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8fdc\u7a0b\u975e\u63a5\u89e6\u5f0f\u751f\u7406\u76d1\u6d4b\u6b63\u5728\u8fdb\u6b65\u3002\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u65f6\u90e8\u7f72\u8fd9\u4e9b\u7cfb\u7edf\u9762\u4e34\u7740\u53ef\u6269\u5c55\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u6027\u80fd\u65b9\u9762\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u57fa\u4e8eFace2PPG\u6d41\u6c34\u7ebf\u5904\u7406\u89c6\u9891\u5e27\u8fdb\u884crPPG\u4fe1\u53f7\u63d0\u53d6\u548c\u5206\u6790\uff0c\u91c7\u7528\u591a\u7ebf\u7a0b\u67b6\u6784\u7ba1\u7406\u89c6\u9891\u6355\u83b7\u3001\u5b9e\u65f6\u5904\u7406\u3001\u7f51\u7edc\u901a\u4fe1\u548cGUI\u66f4\u65b0\u3002\u4f7f\u7528\u51fd\u6570\u5f0f\u54cd\u5e94\u5f0f\u7f16\u7a0b(FRP)\u548cActor\u6a21\u578b\u7684\u6df7\u5408\u7f16\u7a0b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u548c\u9ad8\u6548\u4efb\u52a1\u5e76\u884c\u5316\u3002", "result": "\u7cfb\u7edf\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u540c\u65f6\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\uff0c\u80fd\u591f\u4ee530fps\u8fde\u7eed\u53ef\u9760\u8fd0\u884c\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u53cd\u9988\u6307\u5bfc\u6700\u4f73\u4fe1\u53f7\u6355\u83b7\u6761\u4ef6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u89e3\u51b3\u4e86\u5b9e\u65f6\u751f\u7269\u4fe1\u53f7\u76d1\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u73b0\u4ee3\u533b\u7597\u4fdd\u5065\u548c\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18904", "abs": "https://arxiv.org/abs/2508.18904", "authors": ["Thien-Phuc Tran", "Minh-Quang Nguyen", "Minh-Triet Tran", "Tam V. Nguyen", "Trong-Le Do", "Duy-Nam Ly", "Viet-Tham Huynh", "Khanh-Duy Le", "Mai-Khiem Tran", "Trung-Nghia Le"], "title": "Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025", "comment": "ACM Multimedia 2025", "summary": "The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM\nMultimedia 2025, introduces the first large-scale benchmark for event-level\nmultimodal understanding. Traditional captioning and retrieval tasks largely\nfocus on surface-level recognition of people, objects, and scenes, often\noverlooking the contextual and semantic dimensions that define real-world\nevents. EVENTA addresses this gap by integrating contextual, temporal, and\nsemantic information to capture the who, when, where, what, and why behind an\nimage. Built upon the OpenEvents V1 dataset, the challenge features two tracks:\nEvent-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval.\nA total of 45 teams from six countries participated, with evaluation conducted\nthrough Public and Private Test phases to ensure fairness and reproducibility.\nThe top three teams were invited to present their solutions at ACM Multimedia\n2025. EVENTA establishes a foundation for context-aware, narrative-driven\nmultimedia AI, with applications in journalism, media analysis, cultural\narchiving, and accessibility. Further details about the challenge are available\nat the official homepage: https://ltnghia.github.io/eventa/eventa-2025.", "AI": {"tldr": "EVENTA Grand Challenge 2025\u63a8\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u4e8b\u4ef6\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u57fa\u51c6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u589e\u5f3a\u56fe\u50cf\u68c0\u7d22\u548c\u5b57\u5e55\u751f\u6210\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u7ef4\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5b57\u5e55\u548c\u68c0\u7d22\u4efb\u52a1\u4e3b\u8981\u5173\u6ce8\u4eba\u7269\u3001\u7269\u4f53\u548c\u573a\u666f\u7684\u8868\u9762\u8bc6\u522b\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u5b9a\u4e49\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u7684\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u7ef4\u5ea6\u3002", "method": "\u57fa\u4e8eOpenEvents V1\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u8d5b\u9053\uff1a\u4e8b\u4ef6\u589e\u5f3a\u56fe\u50cf\u68c0\u7d22\u4e0e\u5b57\u5e55\u751f\u6210\u3001\u57fa\u4e8e\u4e8b\u4ef6\u7684\u56fe\u50cf\u68c0\u7d22\uff0c\u901a\u8fc7\u516c\u5f00\u548c\u79c1\u6709\u6d4b\u8bd5\u9636\u6bb5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6765\u81ea6\u4e2a\u56fd\u5bb6\u768445\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u524d\u4e09\u540d\u56e2\u961f\u5728ACM Multimedia 2025\u4e0a\u5c55\u793a\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u7acb\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u53d9\u4e8b\u9a71\u52a8\u7684\u591a\u5a92\u4f53AI\u57fa\u7840\u3002", "conclusion": "EVENTA\u4e3a\u65b0\u95fb\u3001\u5a92\u4f53\u5206\u6790\u3001\u6587\u5316\u5b58\u6863\u548c\u53ef\u8bbf\u95ee\u6027\u7b49\u5e94\u7528\u9886\u57df\u5960\u5b9a\u4e86\u6280\u672f\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4e8b\u4ef6\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.18873", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18873", "abs": "https://arxiv.org/abs/2508.18873", "authors": ["Yunyang Cao", "Juekai Lin", "Wenhao Li", "Bo Jin"], "title": "MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes", "comment": null, "summary": "Discovering complex causal dependencies in temporal point processes (TPPs) is\ncritical for modeling real-world event sequences. Existing methods typically\nrely on static or first-order causal structures, overlooking the multi-order\nand time-varying nature of causal relationships. In this paper, we propose\nMOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.\nMOCHA characterizes multi-order influences as multi-hop causal paths over a\nlatent time-evolving graph. To model such dynamics, we introduce a time-varying\ndirected acyclic graph (DAG) with learnable structural weights, where\nacyclicity and sparsity constraints are enforced to ensure structural validity.\nWe design an end-to-end differentiable framework that jointly models causal\ndiscovery and TPP dynamics, enabling accurate event prediction and revealing\ninterpretable structures. Extensive experiments on real-world datasets\ndemonstrate that MOCHA not only achieves state-of-the-art performance in event\nprediction, but also reveals meaningful and interpretable causal structures.", "AI": {"tldr": "MOCHA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u591a\u9636\u52a8\u6001\u56e0\u679c\u5173\u7cfb\uff0c\u901a\u8fc7\u5efa\u6a21\u65f6\u53d8\u6709\u5411\u65e0\u73af\u56fe\u548c\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u4e8b\u4ef6\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u6216\u4e00\u9636\u56e0\u679c\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u56e0\u679c\u5173\u7cfb\u7684\u591a\u9636\u6027\u548c\u65f6\u53d8\u7279\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u590d\u6742\u56e0\u679c\u4f9d\u8d56\u3002", "method": "\u63d0\u51faMOCHA\u6846\u67b6\uff0c\u5c06\u591a\u9636\u5f71\u54cd\u5efa\u6a21\u4e3a\u6f5c\u5728\u65f6\u53d8\u56fe\u4e0a\u7684\u591a\u8df3\u56e0\u679c\u8def\u5f84\uff1b\u4f7f\u7528\u65f6\u53d8\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u548c\u53ef\u5b66\u4e60\u7ed3\u6784\u6743\u91cd\uff0c\u65bd\u52a0\u65e0\u73af\u6027\u548c\u7a00\u758f\u6027\u7ea6\u675f\uff1b\u8bbe\u8ba1\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u56e0\u679c\u53d1\u73b0\u548c\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u52a8\u6001\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMOCHA\u4e0d\u4ec5\u5728\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u4e14\u63ed\u793a\u4e86\u6709\u610f\u4e49\u4e14\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u7ed3\u6784\u3002", "conclusion": "MOCHA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u4e2d\u591a\u9636\u52a8\u6001\u56e0\u679c\u5173\u7cfb\u7684\u53d1\u73b0\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u4e8b\u4ef6\u5e8f\u5217\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.18884", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18884", "abs": "https://arxiv.org/abs/2508.18884", "authors": ["Gaurish Trivedi", "Alakh Sharma", "Kartikey Singh Bhandari", "Dhruv Kumar", "Pratik Narang", "Jagat Sesh Challa"], "title": "HAEPO: History-Aggregated Exploratory Policy Optimization", "comment": "Under review", "summary": "Exploration is essential in modern learning, from reinforcement learning\nenvironments with small neural policies to large language models (LLMs).\nExisting work, such as DPO, leverages full sequence log-likelihoods to capture\nan entire trajectory of the model's decisions, while methods like GRPO\naggregate per-token ratios into a trajectory-level update. However, both often\nlimit exploration on long-horizon tasks. We introduce History-Aggregated\nExploratory Policy Optimization (HAEPO), a history-aware exploratory loss to\ncombat these shortcomings. HAEPO compresses each trajectory into the sum of its\nlogarithmic probabilities (a cumulative logarithmic likelihood), and applies a\nPlackett-Luce softmax across trajectories to obtain normalized weights\nproportional to their returns, thus encouraging broader exploration. We add\nentropy regularization to stabilize the aggressive updates to prevent premature\ncollapse and a soft KL penalty relative to a frozen copy of the previous\n(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,\naligns closely with true rewards, and demonstrates robust learning behavior\nbetter or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO\nprovides a stable and interpretable framework by explicitly leveraging\nfull-trajectory history while balancing exploration and stability.", "AI": {"tldr": "HAEPO\u662f\u4e00\u79cd\u65b0\u7684\u63a2\u7d22\u6027\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5386\u53f2\u805a\u5408\u548c\u7d2f\u79ef\u5bf9\u6570\u4f3c\u7136\u6765\u589e\u5f3a\u957f\u65f6\u57df\u4efb\u52a1\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u5728\u6536\u655b\u901f\u5ea6\u3001\u63a2\u7d22\u5e7f\u5ea6\u548c\u5956\u52b1\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982DPO\u548cGRPO\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u5f80\u5f80\u9650\u5236\u63a2\u7d22\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5229\u7528\u5b8c\u6574\u8f68\u8ff9\u5386\u53f2\u5e76\u4fc3\u8fdb\u5e7f\u6cdb\u63a2\u7d22\u7684\u65b0\u65b9\u6cd5\u3002", "method": "HAEPO\u5c06\u8f68\u8ff9\u538b\u7f29\u4e3a\u7d2f\u79ef\u5bf9\u6570\u4f3c\u7136\uff0c\u4f7f\u7528Plackett-Luce softmax\u83b7\u5f97\u4e0e\u56de\u62a5\u6210\u6b63\u6bd4\u7684\u5f52\u4e00\u5316\u6743\u91cd\uff0c\u5e76\u52a0\u5165\u71b5\u6b63\u5219\u5316\u548c\u8f6fKL\u60e9\u7f5a\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHAEPO\u6536\u655b\u5feb\u3001\u63a2\u7d22\u5f7b\u5e95\u3001\u4e0e\u771f\u5b9e\u5956\u52b1\u5bf9\u9f50\u7d27\u5bc6\uff0c\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8ePPO\u3001GRPO\u548cDPO\u3002", "conclusion": "HAEPO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u5b8c\u6574\u8f68\u8ff9\u5386\u53f2\u6765\u5e73\u8861\u63a2\u7d22\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.19024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19024", "abs": "https://arxiv.org/abs/2508.19024", "authors": ["Yi Pan", "Yujia Zhang", "Michael Kampffmeyer", "Xiaoguang Zhao"], "title": "ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task\nthat involves retrieving videos based on queries relevant to only specific\nsegments. While existing works follow the paradigm of developing models to\nprocess unimodal features, powerful pretrained vision-language models like CLIP\nremain underexplored in this field. To bridge this gap, we propose ProPy, a\nmodel with systematic architectural adaption of CLIP specifically designed for\nPRVR. Drawing insights from the semantic relevance of multi-granularity events,\nProPy introduces two key innovations: (1) A Prompt Pyramid structure that\norganizes event prompts to capture semantics at multiple granularity levels,\nand (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that\nenables dynamic semantic interaction among events. With these designs, ProPy\nachieves SOTA performance on three public datasets, outperforming previous\nmodels by significant margins. Code is available at\nhttps://github.com/BUAAPY/ProPy.", "AI": {"tldr": "ProPy\u662f\u4e00\u4e2a\u57fa\u4e8eCLIP\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u793a\u91d1\u5b57\u5854\u7ed3\u6784\u548c\u7956\u5148-\u540e\u4ee3\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684PRVR\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u5355\u6a21\u6001\u7279\u5f81\uff0c\u800c\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5982CLIP\u5728\u8be5\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faProPy\u6a21\u578b\uff0c\u91c7\u7528\u63d0\u793a\u91d1\u5b57\u5854\u7ed3\u6784\u7ec4\u7ec7\u591a\u7c92\u5ea6\u4e8b\u4ef6\u63d0\u793a\uff0c\u5e76\u6784\u5efa\u7956\u5148-\u540e\u4ee3\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\u4e8b\u4ef6\u95f4\u7684\u52a8\u6001\u8bed\u4e49\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d\u6a21\u578b\u3002", "conclusion": "ProPy\u901a\u8fc7\u7cfb\u7edf\u6027\u7684CLIP\u67b6\u6784\u9002\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19016", "abs": "https://arxiv.org/abs/2508.19016", "authors": ["Kelly Kurowski", "Xixi Lu", "Hajo A Reijers"], "title": "Working My Way Back to You: Resource-Centric Next-Activity Prediction", "comment": null, "summary": "Predictive Process Monitoring (PPM) aims to train models that forecast\nupcoming events in process executions. These predictions support early\nbottleneck detection, improved scheduling, proactive interventions, and timely\ncommunication with stakeholders. While existing research adopts a control-flow\nperspective, we investigate next-activity prediction from a resource-centric\nviewpoint, which offers additional benefits such as improved work organization,\nworkload balancing, and capacity forecasting. Although resource information has\nbeen shown to enhance tasks such as process performance analysis, its role in\nnext-activity prediction remains unexplored. In this study, we evaluate four\nprediction models and three encoding strategies across four real-life datasets.\nCompared to the baseline, our results show that LightGBM and Transformer models\nperform best with an encoding based on 2-gram activity transitions, while\nRandom Forest benefits most from an encoding that combines 2-gram transitions\nand activity repetition features. This combined encoding also achieves the\nhighest average accuracy. This resource-centric approach could enable smarter\nresource allocation, strategic workforce planning, and personalized employee\nsupport by analyzing individual behavior rather than case-level progression.\nThe findings underscore the potential of resource-centric next-activity\nprediction, opening up new venues for research on PPM.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u8d44\u6e90\u89d2\u5ea6\u51fa\u53d1\u7684\u4e0b\u4e00\u6d3b\u52a8\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u56db\u79cd\u9884\u6d4b\u6a21\u578b\u548c\u4e09\u79cd\u7f16\u7801\u7b56\u7565\uff0c\u53d1\u73b0LightGBM\u548cTransformer\u6a21\u578b\u57282-gram\u6d3b\u52a8\u8fc1\u79fb\u7f16\u7801\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u800c\u968f\u673a\u68ee\u6797\u5728\u7ed32-gram\u8fc1\u79fb\u548c\u6d3b\u52a8\u91cd\u590d\u7279\u5f81\u7684\u7f16\u7801\u4e0b\u8868\u73b0\u6700\u597d\u3002", "motivation": "\u5f53\u524d\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u7814\u7a76\u4e3b\u8981\u4ece\u63a7\u5236\u6d41\u89d2\u5ea6\u51fa\u53d1\uff0c\u672c\u6587\u91c7\u7528\u8d44\u6e90\u4e2d\u5fc3\u89c6\u89d2\u6765\u7814\u7a76\u4e0b\u4e00\u6d3b\u52a8\u9884\u6d4b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9\u6539\u5584\u5de5\u4f5c\u7ec4\u7ec7\u3001\u5e73\u8861\u5de5\u4f5c\u8d1f\u8377\u548c\u9884\u6d4b\u5bb9\u91cf\u3002", "method": "\u5728\u56db\u4e2a\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08LightGBM\u3001Transformer\u3001\u968f\u673a\u68ee\u6797\u7b49\uff09\u548c\u4e09\u79cd\u7f16\u7801\u7b56\u7565\uff0c\u5305\u62ec2-gram\u6d3b\u52a8\u8fc1\u79fb\u7f16\u7801\u4ee5\u53ca\u7ed3\u54082-gram\u8fc1\u79fb\u548c\u6d3b\u52a8\u91cd\u590d\u7279\u5f81\u7684\u7f16\u7801\u65b9\u6cd5\u3002", "result": "\u7ed32-gram\u8fc1\u79fb\u548c\u6d3b\u52a8\u91cd\u590d\u7279\u5f81\u7684\u7f16\u7801\u65b9\u6cd5\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002LightGBM\u548cTransformer\u6a21\u578b\u57282-gram\u6d3b\u52a8\u8fc1\u79fb\u7f16\u7801\u4e0b\u8868\u73b0\u6700\u597d\uff0c\u800c\u968f\u673a\u68ee\u6797\u5728\u7ed3\u5408\u7f16\u7801\u4e0b\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8d44\u6e90\u4e2d\u5fc3\u7684\u4e0b\u4e00\u6d3b\u52a8\u9884\u6d4b\u65b9\u6cd5\u4e3a\u66f4\u667a\u80fd\u7684\u8d44\u6e90\u5206\u914d\u3001\u6218\u7565\u6027\u52b3\u52a1\u89c4\u5212\u548c\u4e2a\u6027\u5316\u5458\u5de5\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4e3a\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u9886\u57df\u5f00\u542f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.19031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19031", "abs": "https://arxiv.org/abs/2508.19031", "authors": ["Vemula Sreenath", "Filippo Gatti", "Pierre Jehel"], "title": "Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data", "comment": "19 pages, 9 Figures and 2 Tables", "summary": "Ground motion models (GMMs) predict how strongly the ground will shake during\nan earthquake. They are essential for structural analysis, seismic design, and\nseismic risk assessment studies. Traditional machine learning (ML) approaches\nare popular to develop GMMs, due to large earthquake databases worldwide.\nHowever, they operate as \"black boxes,\" which are hard to interpret and trust,\nlimiting their use in high-stake decisions. Additionally, these databases\nsuffer from significant data imbalances: fewer large, critically damaging\nrecords near the fault compared to abundant, less severely damaging distant\nrecords. These two limitations are addressed in this work by developing a\ntransparent ML architecture using the HazBinLoss function. Each input (e.g.,\nmagnitude, distance, their interaction term, etc.) is processed separately and\nadded linearly to obtain the output, resulting in exact contribution of each\nterm. The HazBinLoss function assigns higher weights to critical near-field\nlarge magnitude records and lower weights to less-critical far-field smaller\nmagnitude records, during training to prevent underprediction of the most\ndamaging scenarios. Our model captures known seismological principles and\nachieves comparable performance with established GMMs while maintaining\ntransparency. This framework enables broader adoption of ML-based approaches\nfor risk assessment studies and disaster planning.", "AI": {"tldr": "\u901a\u8fc7\u900f\u660e\u7684\u7ebf\u6027\u673a\u5668\u5b66\u4e60\u67b6\u6784\u548cHazBinLoss\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u5730\u9707\u5730\u9762\u8fd0\u52a8\u6a21\u578b\u7684\u9ed1\u76d2\u95ee\u9898\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8fbe\u5230\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u5730\u9707\u5730\u9762\u8fd0\u52a8\u6a21\u578b\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a1)\u9ed1\u76d2\u6a21\u578b\u96be\u4ee5\u89e3\u91ca\u548c\u4fe1\u4efb\uff0c\u9650\u5236\u5728\u91cd\u8981\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff1b2)\u6570\u636e\u5e73\u8861\u95ee\u9898\uff0c\u8fd1\u65ad\u5c42\u5927\u9707\u7ea7\u8bb0\u5f55\u5c11\u4e8e\u8fdc\u573a\u5c0f\u9707\u7ea7\u8bb0\u5f55\u3002", "method": "\u8bbe\u8ba1\u4e86\u900f\u660e\u7684ML\u67b6\u6784\uff0c\u6bcf\u4e2a\u8f93\u5165\u53d8\u91cf\uff08\u9707\u7ea7\u3001\u8ddd\u79bb\u7b49\uff09\u72ec\u7acb\u5904\u7406\u5e76\u7ebf\u6027\u76f8\u52a0\u5f97\u5230\u8f93\u51fa\uff0c\u4f7f\u6bcf\u4e2a\u53d8\u91cf\u7684\u8d21\u732e\u53ef\u89e3\u91ca\u3002\u4f7f\u7528HazBinLoss\u51fd\u6570\uff0c\u5728\u8bad\u7ec3\u4e2d\u7ed9\u8fc7\u5c11\u7684\u5173\u952e\u8fd1\u573a\u5927\u9707\u8bb0\u5f55\u8d4b\u4e88\u66f4\u9ad8\u6743\u91cd\uff0c\u4ee5\u907f\u514d\u5bf9\u6700\u5177\u7834\u574f\u6027\u573a\u666f\u7684\u9884\u6d4b\u4e0d\u8db3\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u6350\u63cf\u5df2\u77e5\u7684\u5730\u9707\u5b66\u539f\u7406\uff0c\u4e0e\u73b0\u6709\u7684\u5730\u9707\u5730\u9762\u8fd0\u52a8\u6a21\u578b\u8fbe\u5230\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u98ce\u9669\u8bc4\u4f30\u548c\u707e\u5bb3\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u91c7\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u53ef\u80fd\u6027\uff0c\u89e3\u51b3\u4e86\u9ed1\u76d2\u6a21\u578b\u548c\u6570\u636e\u4e0d\u5e73\u8861\u8fd9\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u3002"}}
