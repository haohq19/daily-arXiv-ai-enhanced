{"id": "2507.19733", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.19733", "abs": "https://arxiv.org/abs/2507.19733", "authors": ["Alec Scully", "Cameron Stockton", "Forrest Hare"], "title": "Integrating Activity Predictions in Knowledge Graphs", "comment": "7 pages. 18 figures. Semantic Technology for Intelligence, Defense,\n  and Security (STIDS 2024)", "summary": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL."}
{"id": "2507.19766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19766", "abs": "https://arxiv.org/abs/2507.19766", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity."}
{"id": "2507.19883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19883", "abs": "https://arxiv.org/abs/2507.19883", "authors": ["Ahmed Abouelazm", "Mohammad Mahmoud", "Conrad Walter", "Oleksandr Shchetsura", "Erne Hussong", "Helen Gremmelmaier", "J. Marius ZÃ¶llner"], "title": "Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA", "comment": "Paper is accepted in IEEE International Automated Vehicle Validation\n  Conference (IAVVC 2025)", "summary": "Autonomous driving promises safer roads, reduced congestion, and improved\nmobility, yet validating these systems across diverse conditions remains a\nmajor challenge. Real-world testing is expensive, time-consuming, and sometimes\nunsafe, making large-scale validation impractical. In contrast, simulation\nenvironments offer a scalable and cost-effective alternative for rigorous\nverification and validation. A critical component of the validation process is\nscenario generation, which involves designing and configuring traffic scenarios\nto evaluate autonomous systems' responses to various events and uncertainties.\nHowever, existing scenario generation tools often require programming\nknowledge, limiting accessibility for non-technical users. To address this\nlimitation, we present an interactive, no-code framework for scenario\ngeneration. Our framework features a graphical interface that enables users to\ncreate, modify, save, load, and execute scenarios without needing coding\nexpertise or detailed simulation knowledge. Unlike script-based tools such as\nScenic or ScenarioRunner, our approach lowers the barrier to entry and supports\na broader user base. Central to our framework is a graph-based scenario\nrepresentation that facilitates structured management, supports both manual and\nautomated generation, and enables integration with deep learning-based scenario\nand behavior generation methods. In automated mode, the framework can randomly\nsample parameters such as actor types, behaviors, and environmental conditions,\nallowing the generation of diverse and realistic test datasets. By simplifying\nthe scenario generation process, this framework supports more efficient testing\nworkflows and increases the accessibility of simulation-based validation for\nresearchers, engineers, and policymakers."}
{"id": "2507.19914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19914", "abs": "https://arxiv.org/abs/2507.19914", "authors": ["Akram Khairi", "Hussain Sajwani", "Abdallah Mohammad Alkilany", "Laith AbuAssi", "Mohamad Halwani", "Islam Mohamed Zaid", "Ahmed Awadalla", "Dewald Swart", "Abdulla Ayyad", "Yahya Zweiri"], "title": "High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements", "comment": "14 pages, 11 figures", "summary": "Inspecting large-scale industrial surfaces like aircraft fuselages for\nquality control requires capturing their precise 3D surface geometry at high\nresolution. Vision-based tactile sensors (VBTSs) offer high local resolution\nbut require slow 'press-and-lift' measurements stitched for large areas.\nApproaches with sliding or roller/belt VBTS designs provide measurements\ncontinuity. However, they face significant challenges respectively: sliding\nstruggles with friction/wear and both approaches are speed-limited by\nconventional camera frame rates and motion blur, making large-area scanning\ntime consuming. Thus, a rapid, continuous, high-resolution method is needed. We\nintroduce a novel tactile sensor integrating a neuromorphic camera in a rolling\nmechanism to achieve this. Leveraging its high temporal resolution and\nrobustness to motion blur, our system uses a modified event-based multi-view\nstereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning\nspeeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11\ntimes faster than prior continuous tactile sensing methods. A multi-reference\nBayesian fusion strategy enhances accuracy (reducing MAE by 25.2\\% compared to\nEMVS) and mitigates curvature errors. We also validate high-speed feature\nrecognition via Braille reading 2.6 times faster than previous approaches."}
{"id": "2507.19530", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19530", "abs": "https://arxiv.org/abs/2507.19530", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation", "comment": null, "summary": "Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)\nwhere hemodynamic instability can\n  rapidly progress to cardiovascular collapse. Current machine\n  learning (ML) approaches suffer from three limitations: lack of\n  external validation, absence of uncertainty quantification, and\n  inadequate data leakage prevention. This study presents the\n  first comprehensive framework with novel algorithmic leakage\n  prevention, uncertainty quantification, and cross-institutional\n  validation for electronic health records (EHRs) based BP pre dictions. Our\nmethodology implemented systematic data leakage\n  prevention, uncertainty quantification through quantile regres sion, and\nexternal validation between the MIMIC-III and eICU\n  databases. An ensemble framework combines Gradient Boosting,\n  Random Forest, and XGBoost with 74 features across five\n  physiological domains. Internal validation achieved a clinically\n  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03\n  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI\n  standards. External validation showed 30% degradation with\n  critical limitations in patients with hypotensive. Uncertainty\n  quantification generated valid prediction intervals (80.3% SBP\n  and 79.9% DBP coverage), enabling risk-stratified protocols\n  with narrow intervals (< 15 mmHg) for standard monitoring\n  and wide intervals (> 30 mmHg) for manual verification. This\n  framework provides realistic deployment expectations for cross institutional\nAI-assisted BP monitoring in critical care settings.\n  The source code is publicly available at https://github.com/\n  mdbasit897/clinical-bp-prediction-ehr."}
{"id": "2507.19975", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs."}
{"id": "2507.19818", "categories": ["cs.CV", "86A32, 62H35", "I.4.8; I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.19818", "abs": "https://arxiv.org/abs/2507.19818", "authors": ["Xin Hong", "Longchao Da", "Hua Wei"], "title": "FM-LC: A Hierarchical Framework for Urban Flood Mapping by Land Cover Identification Models", "comment": "5 pages and 4 figures. Submitted to the IEEE for possible publication", "summary": "Urban flooding in arid regions poses severe risks to infrastructure and\ncommunities. Accurate, fine-scale mapping of flood extents and recovery\ntrajectories is therefore essential for improving emergency response and\nresilience planning. However, arid environments often exhibit limited spectral\ncontrast between water and adjacent surfaces, rapid hydrological dynamics, and\nhighly heterogeneous urban land covers, which challenge traditional\nflood-mapping approaches. High-resolution, daily PlanetScope imagery provides\nthe temporal and spatial detail needed. In this work, we introduce FM-LC, a\nhierarchical framework for Flood Mapping by Land Cover identification, for this\nchallenging task. Through a three-stage process, it first uses an initial\nmulti-class U-Net to segment imagery into water, vegetation, built area, and\nbare ground classes. We identify that this method has confusion between\nspectrally similar categories (e.g., water vs. vegetation). Second, by early\nchecking, the class with the major misclassified area is flagged, and a\nlightweight binary expert segmentation model is trained to distinguish the\nflagged class from the rest. Third, a Bayesian smoothing step refines\nboundaries and removes spurious noise by leveraging nearby pixel information.\nWe validate the framework on the April 2024 Dubai storm event, using pre- and\npost-rainfall PlanetScope composites. Experimental results demonstrate average\nF1-score improvements of up to 29% across all land-cover classes and notably\nsharper flood delineations, significantly outperforming conventional\nsingle-stage U-Net baselines."}
{"id": "2507.20784", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20784", "abs": "https://arxiv.org/abs/2507.20784", "authors": ["Mohamed Sorour", "Mohamed Heshmat", "Khaled Elgeneidy", "PÃ¥l Johan From"], "title": "A Strawberry Harvesting Tool with Minimal Footprint", "comment": null, "summary": "In this paper, a novel prototype for harvesting table-top grown strawberries\nis presented, that is minimalist in its footprint interacting with the fruit.\nIn our methodology, a smooth trapper manipulates the stem into a precise groove\nlocation at which a distant laser beam is focused. The tool reaches\ntemperatures as high as 188{\\deg} Celsius and as such killing germs and\npreventing the spread of local plant diseases. The burnt stem wound preserves\nwater content and in turn the fruit shelf life. Cycle and cut times achieved\nare 5.56 and 2.88 seconds respectively in successful in-door harvesting\ndemonstration. Extensive experiments are performed to optimize the laser spot\ndiameter and lateral speed against the cutting time."}
{"id": "2507.19839", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19839", "abs": "https://arxiv.org/abs/2507.19839", "authors": ["Tiantian Peng", "Yuyang Liu", "Shuo Yang", "Qiuhe Hong", "YongHong Tian"], "title": "GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning", "comment": null, "summary": "Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot\ngeneralization by aligning visual and textual modalities in a shared embedding\nspace. However, when continuously fine-tuned on diverse tasks, CLIP suffers\nfrom catastrophic forgetting and degradation of its embedding alignment,\nundermining its zero-shot capabilities. In this work, we propose Gradient Null\nSpace Projection (GNSP), an efficient continual learning method that projects\ntask-specific gradients onto the null space of previously learned knowledge.\nThis orthogonal projection mathematically prevents interference with previous\ntasks without relying on rehearsal or architectural modification. Furthermore,\nto preserve the inherent generalization property of CLIP, we introduce\nknowledge distillation and combine it with a modality alignment preservation\nloss inspired by CLIP pre-training to stabilize the structure of the multimodal\nembedding space during fine-tuning. On the MTIL benchmark consisting of 11\ntasks, our method achieved SOTA performance on both the Average and Last key\nmetrics. More importantly, experiments show that our method successfully\nmaintains the original modality gap and cross-modal retrieval performance of\nCLIP, confirming its effectiveness in maintaining a robust visual-language\nspace throughout the continual learning process."}
{"id": "2507.20870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20870", "abs": "https://arxiv.org/abs/2507.20870", "authors": ["Elena Merlo", "Marta Lagomarsino", "Arash Ajoudani"], "title": "A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning", "comment": null, "summary": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness."}
{"id": "2507.20755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20755", "abs": "https://arxiv.org/abs/2507.20755", "authors": ["Arpan Dasgupta", "Sarvesh Gharat", "Neha Madhiwalla", "Aparna Hegde", "Milind Tambe", "Aparna Taneja"], "title": "Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours", "comment": null, "summary": "Automated voice calls with health information are a proven method for\ndisseminating maternal and child health information among beneficiaries and are\ndeployed in several programs around the world. However, these programs often\nsuffer from beneficiary dropoffs and poor engagement. In previous work, through\nreal-world trials, we showed that an AI model, specifically a restless bandit\nmodel, could identify beneficiaries who would benefit most from live service\ncall interventions, preventing dropoffs and boosting engagement. However, one\nkey question has remained open so far: does such improved listenership via\nAI-targeted interventions translate into beneficiaries' improved knowledge and\nhealth behaviors? We present a first study that shows not only listenership\nimprovements due to AI interventions, but also simultaneously links these\nimprovements to health behavior changes. Specifically, we demonstrate that\nAI-scheduled interventions, which enhance listenership, lead to statistically\nsignificant improvements in beneficiaries' health behaviors such as taking iron\nor calcium supplements in the postnatal period, as well as understanding of\ncritical health topics during pregnancy and infancy. This underscores the\npotential of AI to drive meaningful improvements in maternal and child health."}
{"id": "2507.19948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19948", "abs": "https://arxiv.org/abs/2507.19948", "authors": ["Luoxi Jing", "Dianxi Shi", "Zhe Liu", "Songchang Jin", "Chunping Qiu", "Ziteng Qiao", "Yuxian Li", "Jianqiang Xia"], "title": "UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with Convolution-Compensated ViT Dual SA Block", "comment": "Accepted by IJCAI 2025 (International Joint Conference on Artificial\n  Intelligence)", "summary": "Depth estimation plays a crucial role in 3D scene understanding and is\nextensively used in a wide range of vision tasks. Image-based methods struggle\nin challenging scenarios, while event cameras offer high dynamic range and\ntemporal resolution but face difficulties with sparse data. Combining event and\nimage data provides significant advantages, yet effective integration remains\nchallenging. Existing CNN-based fusion methods struggle with occlusions and\ndepth disparities due to limited receptive fields, while Transformer-based\nfusion methods often lack deep modality interaction. To address these issues,\nwe propose UniCT Depth, an event-image fusion method that unifies CNNs and\nTransformers to model local and global features. We propose the\nConvolution-compensated ViT Dual SA (CcViT-DA) Block, designed for the encoder,\nwhich integrates Context Modeling Self-Attention (CMSA) to capture spatial\ndependencies and Modal Fusion Self-Attention (MFSA) for effective cross-modal\nfusion. Furthermore, we design the tailored Detail Compensation Convolution\n(DCC) Block to improve texture details and enhances edge representations.\nExperiments show that UniCT Depth outperforms existing image, event, and\nfusion-based monocular depth estimation methods across key metrics."}
{"id": "2507.20564", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.20564", "abs": "https://arxiv.org/abs/2507.20564", "authors": ["Duc-Tai Dinh", "Duc Anh Khoa Dinh"], "title": "ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning", "comment": null, "summary": "We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system\nin Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image\nretrieval and captioning. Our zero-shot approach requires no finetuning on the\ncompetition's data. For retrieval, we ensemble similarity scores from CLIP,\nSigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt\nto guide the Gemma 3 model, enabling it to link high-level events from the\narticle to the visual content in the image. Our system achieved a final score\nof 0.42002, securing a top-4 position on the private test set, demonstrating\nthe effectiveness of combining foundation models through ensembling and\nprompting. Our code is available at https://github.com/ductai05/ZSE-Cap."}
{"id": "2507.19530", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19530", "abs": "https://arxiv.org/abs/2507.19530", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation", "comment": null, "summary": "Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)\nwhere hemodynamic instability can\n  rapidly progress to cardiovascular collapse. Current machine\n  learning (ML) approaches suffer from three limitations: lack of\n  external validation, absence of uncertainty quantification, and\n  inadequate data leakage prevention. This study presents the\n  first comprehensive framework with novel algorithmic leakage\n  prevention, uncertainty quantification, and cross-institutional\n  validation for electronic health records (EHRs) based BP pre dictions. Our\nmethodology implemented systematic data leakage\n  prevention, uncertainty quantification through quantile regres sion, and\nexternal validation between the MIMIC-III and eICU\n  databases. An ensemble framework combines Gradient Boosting,\n  Random Forest, and XGBoost with 74 features across five\n  physiological domains. Internal validation achieved a clinically\n  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03\n  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI\n  standards. External validation showed 30% degradation with\n  critical limitations in patients with hypotensive. Uncertainty\n  quantification generated valid prediction intervals (80.3% SBP\n  and 79.9% DBP coverage), enabling risk-stratified protocols\n  with narrow intervals (< 15 mmHg) for standard monitoring\n  and wide intervals (> 30 mmHg) for manual verification. This\n  framework provides realistic deployment expectations for cross institutional\nAI-assisted BP monitoring in critical care settings.\n  The source code is publicly available at https://github.com/\n  mdbasit897/clinical-bp-prediction-ehr."}
{"id": "2507.20786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.20786", "abs": "https://arxiv.org/abs/2507.20786", "authors": ["Sam Osian", "Arpan Dutta", "Sahil Bhandari", "Iain E. Buchan", "Dan W. Joyce"], "title": "Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models", "comment": "8 pages, 1 figure", "summary": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research."}
{"id": "2507.20163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20163", "abs": "https://arxiv.org/abs/2507.20163", "authors": ["Zeyu Xi", "Haoying Sun", "Yaofei Wu", "Junchi Yan", "Haoran Zhang", "Lifang Wu", "Liang Wang", "Changwen Chen"], "title": "Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning", "comment": "Accepted by ICCV 2025 (Poster)", "summary": "Existing sports video captioning methods often focus on the action yet\noverlook player identities, limiting their applicability. Although some methods\nintegrate extra information to generate identity-aware descriptions, the player\nidentities are sometimes incorrect because the extra information is independent\nof the video content. This paper proposes a player-centric multimodal prompt\ngeneration network for identity-aware sports video captioning (LLM-IAVC), which\nfocuses on recognizing player identities from a visual perspective.\nSpecifically, an identity-related information extraction module (IRIEM) is\ndesigned to extract player-related multimodal embeddings. IRIEM includes a\nplayer identification network (PIN) for extracting visual features and player\nnames, and a bidirectional semantic interaction module (BSIM) to link player\nfeatures with video content for mutual enhancement. Additionally, a visual\ncontext learning module (VCLM) is designed to capture the key video context\ninformation. Finally, by integrating the outputs of the above modules as the\nmultimodal prompt for the large language model (LLM), it facilitates the\ngeneration of descriptions with player identities. To support this work, we\nconstruct a new benchmark called NBA-Identity, a large identity-aware\nbasketball video captioning dataset with 9,726 videos covering 9 major event\ntypes. The experimental results on NBA-Identity and VC-NBA-2022 demonstrate\nthat our proposed model achieves advanced performance. Code and dataset are\npublicly available at https://github.com/Zeyu1226-mt/LLM-IAVC."}
{"id": "2507.20174", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20174", "abs": "https://arxiv.org/abs/2507.20174", "authors": ["Fei Kong", "Jinhao Duan", "Kaidi Xu", "Zhenhua Guo", "Xiaofeng Zhu", "Xiaoshuang Shi"], "title": "LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks", "comment": null, "summary": "Real-world applications, such as autonomous driving and humanoid robot\nmanipulation, require precise spatial perception. However, it remains\nunderexplored how Vision-Language Models (VLMs) recognize spatial relationships\nand perceive spatial movement. In this work, we introduce a spatial evaluation\npipeline and construct a corresponding benchmark. Specifically, we categorize\nspatial understanding into two main types: absolute spatial understanding,\nwhich involves querying the absolute spatial position (e.g., left, right) of an\nobject within an image, and 3D spatial understanding, which includes movement\nand rotation. Notably, our dataset is entirely synthetic, enabling the\ngeneration of test samples at a low cost while also preventing dataset\ncontamination. We conduct experiments on multiple state-of-the-art VLMs and\nobserve that there is significant room for improvement in their spatial\nunderstanding abilities. Explicitly, in our experiments, humans achieve\nnear-perfect performance on all tasks, whereas current VLMs attain human-level\nperformance only on the two simplest tasks. For the remaining tasks, the\nperformance of VLMs is distinctly lower than that of humans. In fact, the\nbest-performing Vision-Language Models even achieve near-zero scores on\nmultiple tasks. The dataset and code are available on\nhttps://github.com/kong13661/LRR-Bench."}
{"id": "2507.20177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20177", "abs": "https://arxiv.org/abs/2507.20177", "authors": ["Yaozong Zheng", "Bineng Zhong", "Qihua Liang", "Shengping Zhang", "Guorong Li", "Xianxian Li", "Rongrong Ji"], "title": "Towards Universal Modal Tracking with Online Dense Temporal Token Learning", "comment": "arXiv admin note: text overlap with arXiv:2401.01686", "summary": "We propose a universal video-level modality-awareness tracking model with\nonline dense temporal token learning (called {\\modaltracker}). It is designed\nto support various tracking tasks, including RGB, RGB+Thermal, RGB+Depth, and\nRGB+Event, utilizing the same model architecture and parameters. Specifically,\nour model is designed with three core goals: \\textbf{Video-level Sampling}. We\nexpand the model's inputs to a video sequence level, aiming to see a richer\nvideo context from an near-global perspective. \\textbf{Video-level\nAssociation}. Furthermore, we introduce two simple yet effective online dense\ntemporal token association mechanisms to propagate the appearance and motion\ntrajectory information of target via a video stream manner. \\textbf{Modality\nScalable}. We propose two novel gated perceivers that adaptively learn\ncross-modal representations via a gated attention mechanism, and subsequently\ncompress them into the same set of model parameters via a one-shot training\nmanner for multi-task inference. This new solution brings the following\nbenefits: (i) The purified token sequences can serve as temporal prompts for\nthe inference in the next video frames, whereby previous information is\nleveraged to guide future inference. (ii) Unlike multi-modal trackers that\nrequire independent training, our one-shot training scheme not only alleviates\nthe training burden, but also improves model representation. Extensive\nexperiments on visible and multi-modal benchmarks show that our {\\modaltracker}\nachieves a new \\textit{SOTA} performance. The code will be available at\nhttps://github.com/GXNU-ZhongLab/ODTrack."}
{"id": "2507.21009", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21009", "abs": "https://arxiv.org/abs/2507.21009", "authors": ["Danil Savine", "Muni Sreenivas Pydi", "Jamal Atif", "Olivier CappÃ©"], "title": "Memorization in Fine-Tuned Large Language Models", "comment": null, "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."}
{"id": "2507.19766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19766", "abs": "https://arxiv.org/abs/2507.19766", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity."}
{"id": "2507.20286", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.20286", "abs": "https://arxiv.org/abs/2507.20286", "authors": ["Liyuan Zhang", "Zeyun Cheng", "Yan Yang", "Yong Liu", "Jinke Ma"], "title": "T$^\\text{3}$SVFND: Towards an Evolving Fake News Detector for Emergencies with Test-time Training on Short Video Platforms", "comment": "16 pages, 3 figures, published to DASFAA 2025", "summary": "The existing methods for fake news videos detection may not be generalized,\nbecause there is a distribution shift between short video news of different\nevents, and the performance of such techniques greatly drops if news records\nare coming from emergencies. We propose a new fake news videos detection\nframework (T$^3$SVFND) using Test-Time Training (TTT) to alleviate this\nlimitation, enhancing the robustness of fake news videos detection.\nSpecifically, we design a self-supervised auxiliary task based on Mask Language\nModeling (MLM) that masks a certain percentage of words in text and predicts\nthese masked words by combining contextual information from different\nmodalities (audio and video). In the test-time training phase, the model adapts\nto the distribution of test data through auxiliary tasks. Extensive experiments\non the public benchmark demonstrate the effectiveness of the proposed model,\nespecially for the detection of emergency news."}
{"id": "2507.19975", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs."}
{"id": "2507.20542", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20542", "abs": "https://arxiv.org/abs/2507.20542", "authors": ["Dawon Ahn", "Jun-Gi Jang", "Evangelos E. Papalexakis"], "title": "Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation", "comment": null, "summary": "Group fairness is important to consider in tensor decomposition to prevent\ndiscrimination based on social grounds such as gender or age. Although few\nworks have studied group fairness in tensor decomposition, they suffer from\nperformance degradation. To address this, we propose STAFF(Sparse Tensor\nAugmentation For Fairness) to improve group fairness by minimizing the gap in\ncompletion errors of different groups while reducing the overall tensor\ncompletion error. Our main idea is to augment a tensor with augmented entities\nincluding sufficient observed entries to mitigate imbalance and group bias in\nthe sparse tensor. We evaluate \\method on tensor completion with various\ndatasets under conventional and deep learning-based tensor models. STAFF\nconsistently shows the best trade-off between completion error and group\nfairness; at most, it yields 36% lower MSE and 59% lower MADE than the\nsecond-best baseline."}
{"id": "2507.20573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20573", "abs": "https://arxiv.org/abs/2507.20573", "authors": ["Yaxin Xiao", "Qingqing Ye", "Li Hu", "Huadi Zheng", "Haibo Hu", "Zi Liang", "Haoyang Li", "Yijie Jiao"], "title": "Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy", "comment": "Accepted by ICCV 2025", "summary": "Machine unlearning enables the removal of specific data from ML models to\nuphold the right to be forgotten. While approximate unlearning algorithms offer\nefficient alternatives to full retraining, this work reveals that they fail to\nadequately protect the privacy of unlearned data. In particular, these\nalgorithms introduce implicit residuals which facilitate privacy attacks\ntargeting at unlearned data. We observe that these residuals persist regardless\nof model architectures, parameters, and unlearning algorithms, exposing a new\nattack surface beyond conventional output-based leakage. Based on this insight,\nwe propose the Reminiscence Attack (ReA), which amplifies the correlation\nbetween residuals and membership privacy through targeted fine-tuning\nprocesses. ReA achieves up to 1.90x and 1.12x higher accuracy than prior\nattacks when inferring class-wise and sample-wise membership, respectively. To\nmitigate such residual-induced privacy risk, we develop a dual-phase\napproximate unlearning framework that first eliminates deep-layer unlearned\ndata traces and then enforces convergence stability to prevent models from\n\"pseudo-convergence\", where their outputs are similar to retrained models but\nstill preserve unlearned residuals. Our framework works for both classification\nand generation tasks. Experimental evaluations confirm that our approach\nmaintains high unlearning efficacy, while reducing the adaptive privacy attack\naccuracy to nearly random guess, at the computational cost of 2-12% of full\nretraining from scratch."}
{"id": "2507.20174", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20174", "abs": "https://arxiv.org/abs/2507.20174", "authors": ["Fei Kong", "Jinhao Duan", "Kaidi Xu", "Zhenhua Guo", "Xiaofeng Zhu", "Xiaoshuang Shi"], "title": "LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks", "comment": null, "summary": "Real-world applications, such as autonomous driving and humanoid robot\nmanipulation, require precise spatial perception. However, it remains\nunderexplored how Vision-Language Models (VLMs) recognize spatial relationships\nand perceive spatial movement. In this work, we introduce a spatial evaluation\npipeline and construct a corresponding benchmark. Specifically, we categorize\nspatial understanding into two main types: absolute spatial understanding,\nwhich involves querying the absolute spatial position (e.g., left, right) of an\nobject within an image, and 3D spatial understanding, which includes movement\nand rotation. Notably, our dataset is entirely synthetic, enabling the\ngeneration of test samples at a low cost while also preventing dataset\ncontamination. We conduct experiments on multiple state-of-the-art VLMs and\nobserve that there is significant room for improvement in their spatial\nunderstanding abilities. Explicitly, in our experiments, humans achieve\nnear-perfect performance on all tasks, whereas current VLMs attain human-level\nperformance only on the two simplest tasks. For the remaining tasks, the\nperformance of VLMs is distinctly lower than that of humans. In fact, the\nbest-performing Vision-Language Models even achieve near-zero scores on\nmultiple tasks. The dataset and code are available on\nhttps://github.com/kong13661/LRR-Bench."}
{"id": "2507.20629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20629", "abs": "https://arxiv.org/abs/2507.20629", "authors": ["Dezhi An", "Wenqiang Liu", "Kefan Wang", "Zening Chen", "Jun Lu", "Shengcai Zhang"], "title": "DAMS:Dual-Branch Adaptive Multiscale Spatiotemporal Framework for Video Anomaly Detection", "comment": "13 pages,7 figures", "summary": "The goal of video anomaly detection is tantamount to performing\nspatio-temporal localization of abnormal events in the video. The multiscale\ntemporal dependencies, visual-semantic heterogeneity, and the scarcity of\nlabeled data exhibited by video anomalies collectively present a challenging\nresearch problem in computer vision. This study offers a dual-path architecture\ncalled the Dual-Branch Adaptive Multiscale Spatiotemporal Framework (DAMS),\nwhich is based on multilevel feature decoupling and fusion, enabling efficient\nanomaly detection modeling by integrating hierarchical feature learning and\ncomplementary information. The main processing path of this framework\nintegrates the Adaptive Multiscale Time Pyramid Network (AMTPN) with the\nConvolutional Block Attention Mechanism (CBAM). AMTPN enables multigrained\nrepresentation and dynamically weighted reconstruction of temporal features\nthrough a three-level cascade structure (time pyramid pooling, adaptive feature\nfusion, and temporal context enhancement). CBAM maximizes the entropy\ndistribution of feature channels and spatial dimensions through dual attention\nmapping. Simultaneously, the parallel path driven by CLIP introduces a\ncontrastive language-visual pre-training paradigm. Cross-modal semantic\nalignment and a multiscale instance selection mechanism provide high-order\nsemantic guidance for spatio-temporal features. This creates a complete\ninference chain from the underlying spatio-temporal features to high-level\nsemantic concepts. The orthogonal complementarity of the two paths and the\ninformation fusion mechanism jointly construct a comprehensive representation\nand identification capability for anomalous events. Extensive experimental\nresults on the UCF-Crime and XD-Violence benchmarks establish the effectiveness\nof the DAMS framework."}
{"id": "2507.20757", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20757", "abs": "https://arxiv.org/abs/2507.20757", "authors": ["Matan Kichler", "Shai Bagon", "Mark Sheinin"], "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry", "comment": "ICCV 2025", "summary": "Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers."}
{"id": "2507.19975", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs."}
{"id": "2507.20901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20901", "abs": "https://arxiv.org/abs/2507.20901", "authors": ["Manasi Muglikar", "Nico Messikommer", "Marco Cannici", "Davide Scaramuzza"], "title": "Event-Based De-Snowing for Autonomous Driving", "comment": null, "summary": "Adverse weather conditions, particularly heavy snowfall, pose significant\nchallenges to both human drivers and autonomous vehicles. Traditional\nimage-based de-snowing methods often introduce hallucination artifacts as they\nrely solely on spatial information, while video-based approaches require high\nframe rates and suffer from alignment artifacts at lower frame rates. Camera\nparameters, such as exposure time, also influence the appearance of snowflakes,\nmaking the problem difficult to solve and heavily dependent on network\ngeneralization. In this paper, we propose to address the challenge of desnowing\nby using event cameras, which offer compressed visual information with\nsubmillisecond latency, making them ideal for de-snowing images, even in the\npresence of ego-motion. Our method leverages the fact that snowflake occlusions\nappear with a very distinctive streak signature in the spatio-temporal\nrepresentation of event data. We design an attention-based module that focuses\non events along these streaks to determine when a background point was occluded\nand use this information to recover its original intensity. We benchmark our\nmethod on DSEC-Snow, a new dataset created using a green-screen technique that\noverlays pre-recorded snowfall data onto the existing DSEC driving dataset,\nresulting in precise ground truth and synchronized image and event streams. Our\napproach outperforms state-of-the-art de-snowing methods by 3 dB in PSNR for\nimage reconstruction. Moreover, we show that off-the-shelf computer vision\nalgorithms can be applied to our reconstructions for tasks such as depth\nestimation and optical flow, achieving a $20\\%$ performance improvement over\nother de-snowing methods. Our work represents a crucial step towards enhancing\nthe reliability and safety of vision systems in challenging winter conditions,\npaving the way for more robust, all-weather-capable applications."}
{"id": "2507.20757", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20757", "abs": "https://arxiv.org/abs/2507.20757", "authors": ["Matan Kichler", "Shai Bagon", "Mark Sheinin"], "title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry", "comment": "ICCV 2025", "summary": "Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers."}
{"id": "2507.19839", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19839", "abs": "https://arxiv.org/abs/2507.19839", "authors": ["Tiantian Peng", "Yuyang Liu", "Shuo Yang", "Qiuhe Hong", "YongHong Tian"], "title": "GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning", "comment": null, "summary": "Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot\ngeneralization by aligning visual and textual modalities in a shared embedding\nspace. However, when continuously fine-tuned on diverse tasks, CLIP suffers\nfrom catastrophic forgetting and degradation of its embedding alignment,\nundermining its zero-shot capabilities. In this work, we propose Gradient Null\nSpace Projection (GNSP), an efficient continual learning method that projects\ntask-specific gradients onto the null space of previously learned knowledge.\nThis orthogonal projection mathematically prevents interference with previous\ntasks without relying on rehearsal or architectural modification. Furthermore,\nto preserve the inherent generalization property of CLIP, we introduce\nknowledge distillation and combine it with a modality alignment preservation\nloss inspired by CLIP pre-training to stabilize the structure of the multimodal\nembedding space during fine-tuning. On the MTIL benchmark consisting of 11\ntasks, our method achieved SOTA performance on both the Average and Last key\nmetrics. More importantly, experiments show that our method successfully\nmaintains the original modality gap and cross-modal retrieval performance of\nCLIP, confirming its effectiveness in maintaining a robust visual-language\nspace throughout the continual learning process."}
{"id": "2507.21009", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21009", "abs": "https://arxiv.org/abs/2507.21009", "authors": ["Danil Savine", "Muni Sreenivas Pydi", "Jamal Atif", "Olivier CappÃ©"], "title": "Memorization in Fine-Tuned Large Language Models", "comment": null, "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."}
