<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 提出了EgoExo-Con基准来评估视频LLM在多视角视频中的时间理解一致性，发现现有模型在跨视角一致性方面表现不佳，并提出了View-GRPO强化学习框架来改进跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 研究视频LLM在不同视角视频中是否能保持时间理解的一致性，因为现实世界中同一事件往往从多个视角被记录。

Method: 引入EgoExo-Con基准，包含同步的自我中心和他者视角视频对，评估时间验证和时间定位任务。提出View-GRPO强化学习框架来增强视角特定的时间推理和跨视角一致性。

Result: 现有视频LLM在跨视角一致性方面表现很差，比单视角性能差很多。简单微调会改善一致性但可能降低单视角性能。View-GRPO方法在改善跨视角一致性方面优于简单SFT和GRPO。

Conclusion: 跨视角时间理解一致性是视频LLM的重要挑战，View-GRPO框架能有效提升模型在这方面的表现。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [2] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出了一种专门为事件相机设计的事件标记化方法Spiking Patches，能够保持事件的异步性和空间稀疏性，同时实现比体素快3.4倍、比帧快10.4倍的推理速度，并在精度上达到或超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将事件表示为帧或体素，虽然精度高但失去了事件相机的异步性和空间稀疏性特性，需要一种能保持这些独特性质的事件表示方法。

Method: 提出Spiking Patches标记化方法，将异步稀疏事件流转换为标记表示，并在手势识别和物体检测任务中使用GNN、PCN和Transformer进行评估。

Result: Spiking Patches标记的推理时间比体素标记快3.4倍，比帧标记快10.4倍，在手势识别上精度提升达3.8%，在物体检测上提升达1.4%，同时保持或超过现有方法的精度。

Conclusion: 事件标记化为事件视觉提供了新方向，是朝着保持事件相机特性方法的重要一步。

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [3] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 提出了LoCoT2V-Bench基准，专门用于评估复杂提示下的长视频生成，包含多维度评估框架和新指标，发现现有模型在事件间一致性、细粒度对齐和高级主题遵循方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成评估主要关注简化的提示和低级指标，忽视了与提示的细粒度对齐以及叙事连贯性、主题表达等抽象维度，特别是在处理复杂提示和长视频生成方面存在评估空白。

Method: 基于真实世界视频构建包含场景转换和事件动态等元素的复杂提示集，建立多维度评估框架，包括事件级对齐、细粒度时间一致性、内容清晰度和关注叙事流、情感响应、角色发展等抽象属性的人类期望实现度(HERD)指标。

Result: 对9个代表性长视频生成模型的评估显示，现有方法在基础视觉和时间方面表现良好，但在事件间一致性、细粒度对齐和高级主题遵循等方面存在困难。

Conclusion: LoCoT2V-Bench为长形式复杂文本到视频生成提供了全面可靠的评估平台，并指明了未来方法改进的关键方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出PCN-TA方法，通过时间摊销减少预测编码的计算开销，在机器人感知任务中比反向传播减少10%权重更新，比基线PC网络减少50%推理步骤


<details>
  <summary>Details</summary>
Motivation: 边缘机器人系统需要高效的在线学习算法，传统反向传播在生物合理性和连续适应方面存在不足，预测编码框架虽具生物合理性但计算开销大

Method: PCN-TA方法保留跨时间帧的潜在状态，利用时间相关性来减少计算需求，同时保持学习性能

Result: 在COIL-20机器人感知数据集上，PCN-TA比反向传播减少10%权重更新，比基线PC网络减少50%推理步骤

Conclusion: PCN-TA显著降低计算开销，为边缘部署和实时适应提供支持，其生物启发特性也适合未来神经形态硬件实现

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [5] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出了一个双混合专家框架用于离散时间生存分析，结合特征编码器MoE和风险MoE来建模患者异质性和时间动态。


<details>
  <summary>Details</summary>
Motivation: 生存分析需要同时建模患者异质性和时间动态，现有方法难以灵活整合这两个方面。

Method: 使用双MoE框架：特征编码器MoE用于亚组感知表示学习，风险MoE利用患者特征和时间嵌入捕捉时间动态。

Result: 在METABRIC和GBSG乳腺癌数据集上，时间依赖性C指数在测试集上提升高达0.04，在Consurv框架中集成时获得进一步增益。

Conclusion: 双MoE框架能有效提升生存分析性能，灵活整合现有深度学习生存分析流程。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [6] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种超越传统参数优化的机器学习新范式，将模型视为可塑的几何实体，通过优化度量张量场来动态塑造模型空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在固定几何空间中搜索最优参数存在局限性，需要一种能够动态调整模型几何结构的新方法，以获得更强的表达能力和防止过拟合。

Method: 构建变分框架，损失函数平衡数据保真度和流形固有几何复杂性；将连续流形离散化为三角网格，用边长参数化度量张量，利用自动微分工具进行高效优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用的深刻类比，为"数据驱动几何"概念提供了优雅物理解释；即使固定拓扑，度量优化也比固定几何模型具有更强的表达能力。

Conclusion: 这项工作为构建能够自主演化几何和拓扑的完全动态"元学习器"奠定了坚实基础，在科学模型发现和鲁棒表示学习等领域具有广阔应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [7] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 使用机器学习方法预测医院再入院风险，随机森林模型表现最佳，可识别关键因素帮助降低再入院率和医疗成本。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是医疗支付方、提供者和政策制定者的优先事项，再入院率被用作衡量医院医疗质量的基准。

Method: 使用逻辑回归、随机森林和支持向量机等机器学习技术分析健康索赔数据，采用主成分分析进行降维处理。

Result: 随机森林模型表现最佳，其次是逻辑回归和支持向量机，基于AUC指标评估。

Conclusion: 这些模型可用于识别导致再入院的关键因素，帮助识别需要重点关注的患者，从而降低再入院率、减少成本并提高医疗质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [8] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 本文通过改进经典强化学习方法，使其能够在稀疏、随机和非平稳环境中高效运行，特别是在水下污染云搜索等应用中。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在随机和非平稳环境中表现有限，特别是在奖励稀疏的场景下（如自主水下车辆搜索污染云），需要改进以适应这些复杂环境。

Method: 系统研究了多种改进方法，包括分层算法变更、多目标学习，以及集成位置记忆作为外部输出过滤器以防止状态重复访问，重点采用改进的蒙特卡洛方法。

Result: 改进的蒙特卡洛方法在性能上显著优于传统Q学习和两种穷举搜索模式，证明了其在复杂环境中的适应性。

Conclusion: 强化学习方法可以通过适当改进，有效应用于随机、非平稳和奖励稀疏的环境中。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [9] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: 开发基于Flow Matching的生成式AI模型FM-Cast，用于高效、准确的平流层环流时空演变的概率预报，在SSW事件预测中表现优异且计算效率极高。


<details>
  <summary>Details</summary>
Motivation: 平流层突然增温(SSW)是次季节可预报性的关键来源和极端冬季天气的主要驱动因素，但其准确高效预报仍是数值天气预报系统的持续挑战，数据驱动方法在SSW复杂三维动力学中的应用仍待探索。

Method: 采用基于Flow Matching的生成式AI模型FM-Cast，构建平流层环流时空演变的概率预报系统，通过理想化实验区分对流层强迫和内部平流层动力学驱动的SSW事件。

Result: 在18个主要SSW事件(1998-2024)评估中，FM-Cast能提前20天准确预测10个事件的爆发、强度和形态，集合准确率超过50%，性能媲美或优于领先NWP系统，在消费级GPU上仅需2分钟完成50成员、30天预报。

Conclusion: 建立了计算高效的平流层异常概率预报范式，展示了生成式AI在加深大气-气候动力学物理理解方面的潜力，揭示了SSW可预报性与其物理驱动因素的根本联系。

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [10] [Aeolus: A Multi-structural Flight Delay Dataset](https://arxiv.org/abs/2510.26616)
*Lin Xu,Xinyun Yuan,Yuxuan Liang,Suwan Yin,Yuankai Wu*

Main category: cs.LG

TL;DR: Aeolus是一个大规模多模态航班延误数据集，包含表格数据、航班链模块和航班网络图，旨在支持航班延误预测和表格数据基础模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有航班延误数据集通常局限于平面表格结构，无法捕捉延误传播的时空动态特性。Aeolus旨在解决这一局限性。

Method: 提供三种对齐模态：包含丰富特征的表格数据集（5000万+航班）、建模延误传播的航班链模块、编码共享资源连接的航班网络图。

Result: 构建了具有时间分割、全面特征和严格防泄漏的数据集，支持回归、分类、时序结构建模和图学习等多种任务。

Conclusion: Aeolus填补了领域特定建模和通用结构化数据研究的关键空白，为航班延误预测和表格数据基础模型提供了统一基准。

Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 提出利用自主AI代理实现FinOps自动化，通过模拟端到端行业流程，从多源数据获取到分析生成优化建议，评估显示AI代理能达到实际FinOps从业者的能力水平。


<details>
  <summary>Details</summary>
Motivation: FinOps从业者面临来自多个云提供商和内部系统的异构账单数据格式、分类和指标，导致难以合成可操作见解和做出及时决策。

Method: 构建FinOps代理系统，模拟从多源数据检索到数据整合分析再到生成优化建议的端到端行业流程，使用开源和闭源语言模型评估代理性能。

Result: 代理能够理解、规划和执行任务，其能力与实际的FinOps从业者相当。

Conclusion: 自主目标驱动AI代理可以有效解决FinOps中的数据异构挑战，实现自动化成本优化。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [12] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance是一个用于网络规模合规性评估的框架，通过将监管文本表示为策略图、运行时上下文表示为上下文图并进行对齐，来改进基于LLM的监管推理。


<details>
  <summary>Details</summary>
Motivation: 网络规模的合规性面临实际挑战：每个请求都需要监管评估。监管文本具有交叉引用和规范性，而运行时上下文是非结构化的自然语言，需要将语义信息与结构化监管元素对齐。

Method: 引入GraphCompliance框架，将监管文本表示为策略图（编码规范结构和交叉引用），运行时上下文表示为上下文图（将事件形式化为SAO和实体关系三元组），然后对齐这两个图，为法官LLM提供结构化信息基础。

Result: 在300个GDPR衍生真实场景的五个评估任务中，GraphCompliance比纯LLM和RAG基线获得4.1-7.2个百分点的micro-F1提升，具有更少的欠预测和过预测，召回率更高且假阳性率更低。

Conclusion: 消融研究表明每个图组件都有贡献，表明结构化表示和法官LLM对于规范性推理是互补的。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [13] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种实用框架，将人格视为社会赋予实体的义务束（权利与责任），而非形而上学属性，以应对AI代理多样化带来的新型人格涌现。


<details>
  <summary>Details</summary>
Motivation: AI代理的出现将引发新型人格的"寒武纪大爆发"，需要实用方法来处理这种多样化，而不陷入关于AI意识或理性的无解争论。

Method: 采用义务束解绑方法，将传统人格概念分解为可定制的权利和责任组合，利用去中心化数字身份技术，探索人格作为问题和解决方案的双重角色。

Result: 开发了实用工具（如创建可制裁的AI"个体"以促进AI合同），提供了更灵活的社会整合方式，避免了形而上学争论。

Conclusion: 通过拒绝寻求单一本质定义的基础主义方法，本文为AI代理融入社会提供了更实用和灵活的思路，强调人格作为社会建构的治理工具。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 提出了一种基于模糊逻辑和句法依赖解析的细粒度情感分析方法，能够对实体评论进行强度分级（非常弱、弱、中等、强、非常强）的排名。


<details>
  <summary>Details</summary>
Motivation: 传统的基于词典的情感分析方法没有考虑意见的强度差异，无法区分不同程度的积极或消极情感。

Method: 结合模糊逻辑算法对意见词进行分类，使用句法依赖解析找到与目标方面相关的意见词，通过意见词与特定产品方面的关联来计算实体得分。

Result: 能够对实体评论进行细粒度的强度分级和排名，考虑了副词、形容词、名词和动词等多种意见词类型。

Conclusion: 该方法实现了对意见强度和方向的更精确分析，为基于用户查询的实体排名提供了更细致的解决方案。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [15] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER通过自动生成教科书式课程和基于布鲁姆分类法的问答对，将通用大语言模型转化为领域专家，在保持广泛能力的同时显著提升专业领域性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用任务上表现出色，但在需要深入原理理解的专业领域（如经济学、心理学）表现不佳，存在关键领域差距。

Method: ACER首先为特定学科生成目录，然后基于布鲁姆分类法创建问答对，构建综合性课程。使用交错课程安排进行持续预训练，确保内容和认知维度的对齐学习。

Result: 在Llama 3.2模型上，ACER在专业MMLU子集上显著提升，微观经济学准确率提高5个百分点，所有目标领域平均提升3个百分点。还促进跨领域知识迁移，非目标领域性能提升0.7点，在ARC和GPQA基准上提升超过2点。

Conclusion: ACER提供了一个可扩展且有效的方法，能够显著缩小大语言模型在关键专业领域的性能差距，同时保持通用推理能力。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [16] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个包含50道国际数学奥林匹克竞赛难度数学题的基准测试，旨在评估大型语言模型的数学推理能力，现有模型在该测试上表现不佳（最高准确率仅52.4%）。


<details>
  <summary>Details</summary>
Motivation: 现有数学竞赛基准测试对顶级大型语言模型评估效果有限，因为出现了性能饱和现象（如AIME24/25），需要更具挑战性的测试来推动模型数学推理能力的进一步发展。

Method: 创建包含50道人工设计的数学问题，所有问题都经过专家交叉验证达到IMO难度标准，且均为原创问题以避免数据记忆导致的性能泄漏。每个问题只需最终答案，便于自动评分。

Result: 在26个大型语言模型上的实验结果显示，最佳模型在AMO-Bench上仅达到52.4%的准确率，大多数模型低于40%。进一步分析显示随着测试时计算量的增加存在有希望的扩展趋势。

Conclusion: 当前大型语言模型在数学推理方面仍有很大改进空间，AMO-Bench的发布将促进语言模型推理能力的进一步研究。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration](https://arxiv.org/abs/2510.26536)
*Huajie Tan,Cheng Chi,Xiansheng Chen,Yuheng Ji,Zhongxia Zhao,Xiaoshuai Hao,Yaoxu Lyu,Mingyu Cao,Junkai Zhao,Huaihai Lyu,Enshen Zhou,Ning Chen,Yankai Fu,Cheng Peng,Wei Guo,Dong Liang,Zhuo Chen,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了RoboOS-NeXT框架，通过统一的时空-具身记忆(STEM)实现多机器人系统的终身适应、可扩展协调和鲁棒调度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的个体记忆，无法实现长期学习、扩展到异构团队或从故障中恢复，需要统一的内存表示。

Method: 引入STEM记忆，整合空间场景几何、时间事件历史和具身配置文件，采用大脑-小脑框架，高层大脑模型进行全局规划，低层控制器本地执行。

Result: 在餐厅、超市和家庭等复杂协调任务中表现出优越性能，验证了在异构具身系统中的有效性。

Conclusion: RoboOS-NeXT通过统一记忆中心设计实现了终身、可扩展和鲁棒的多机器人协作。

Abstract: The proliferation of collaborative robots across diverse tasks and
embodiments presents a central challenge: achieving lifelong adaptability,
scalable coordination, and robust scheduling in multi-agent systems. Existing
approaches, from vision-language-action (VLA) models to hierarchical
frameworks, fall short due to their reliance on limited or dividual-agent
memory. This fundamentally constrains their ability to learn over long
horizons, scale to heterogeneous teams, or recover from failures, highlighting
the need for a unified memory representation. To address these limitations, we
introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,
and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel
Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene
geometry, temporal event history, and embodiment profiles into a shared
representation. This memory-centric design is integrated into a
brain-cerebellum framework, where a high-level brain model performs global
planning by retrieving and updating STEM, while low-level controllers execute
actions locally. This closed loop between cognition, memory, and execution
enables dynamic task allocation, fault-tolerant collaboration, and consistent
state synchronization. We conduct extensive experiments spanning complex
coordination tasks in restaurants, supermarkets, and households. Our results
demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous
embodiments, validating its effectiveness in enabling lifelong, scalable, and
robust multi-robot collaboration. Project website:
https://flagopen.github.io/RoboOS/

</details>


### [18] [Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments](https://arxiv.org/abs/2510.26646)
*Xiaoyi He,Danggui Chen,Zhenshuo Zhang,Zimeng Bai*

Main category: cs.RO

TL;DR: 提出分层路径规划与控制框架，结合高层DQN选择离散子目标与低层TD3控制器执行连续动作，在动态和部分可观测环境中实现更好的导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决单一算法在复杂环境中路径规划的局限性，结合离散决策和连续控制的优势，提高导航系统的成功率和样本效率。

Method: 使用分层架构：高层DQN选择行为和子目标，低层TD3执行平滑速度控制；设计实用奖励机制（方向、距离、避障等）和LiDAR安全门防止不安全动作。

Result: 在动态和部分可观测环境中，相比单一算法基线和基于规则的规划器，提高了成功率、样本效率，对未见障碍配置有更好泛化能力，减少了控制突变。

Conclusion: 分层框架有效结合离散决策和连续控制，在复杂导航任务中优于单一方法，具有更好的泛化性和控制平滑性。

Abstract: This paper presents a hierarchical path-planning and control framework that
combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with
a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller
for continuous actuation. The high-level module selects behaviors and
sub-goals; the low-level module executes smooth velocity commands. We design a
practical reward shaping scheme (direction, distance, obstacle avoidance,
action smoothness, collision penalty, time penalty, and progress), together
with a LiDAR-based safety gate that prevents unsafe motions. The system is
implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,
including success rate, collision rate, path efficiency, and re-planning
efficiency, in dynamic and partially observable environments. Experiments show
improved success rate and sample efficiency over single-algorithm baselines
(DQN or TD3 alone) and rule-based planners, with better generalization to
unseen obstacle configurations and reduced abrupt control changes. Code and
evaluation scripts are available at the project repository.

</details>
