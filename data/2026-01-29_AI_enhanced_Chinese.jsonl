{"id": "2601.19955", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.19955", "abs": "https://arxiv.org/abs/2601.19955", "authors": ["Jean-Marc Fellous", "Gert Cauwenberghs", "Cornelia Ferm\u00fcller", "Yulia Sandamisrkaya", "Terrence Sejnowski"], "title": "NeuroAI and Beyond", "comment": "53 pages, 5 figures, extended appendix", "summary": "Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u57fa\u4e8e2025\u5e748\u6708\u7814\u8ba8\u4f1a\uff0c\u63a2\u8ba8\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u4ea4\u53c9\u9886\u57df\uff0c\u63d0\u51faNeuroAI\u6982\u5ff5\uff0c\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u6539\u8fdbAI\u7b97\u6cd5\uff0c\u540c\u65f6\u6df1\u5316\u5bf9\u751f\u7269\u795e\u7ecf\u8ba1\u7b97\u7684\u7406\u89e3\u3002", "motivation": "\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u8fd1\u5e74\u6765\u5404\u81ea\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e24\u8005\u4e4b\u95f4\u7684\u8fde\u63a5\u4ecd\u7136\u677e\u6563\u3002\u8bba\u6587\u65e8\u5728\u8bc6\u522b\u8fd9\u4e24\u4e2a\u9886\u57df\u5f53\u524d\u548c\u672a\u6765\u7684\u534f\u540c\u673a\u4f1a\uff0c\u4fc3\u8fdb\u66f4\u7d27\u5bc6\u7684\u4ea4\u53c9\u878d\u5408\u3002", "method": "\u57fa\u4e8e2025\u5e748\u6708\u4e3e\u884c\u7684\u7814\u8ba8\u4f1a\uff0c\u805a\u7126\u4e8e\u5177\u8eab\u6027\u3001\u8bed\u8a00\u4e0e\u901a\u4fe1\u3001\u673a\u5668\u4eba\u5b66\u3001\u4eba\u7c7b\u4e0e\u673a\u5668\u5b66\u4e60\u3001\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u7b49\u5b50\u9886\u57df\uff0c\u6536\u96c6\u591a\u4f4d\u9886\u5148\u7814\u7a76\u4eba\u5458\u7684\u4e2a\u4eba\u89c2\u70b9\uff0c\u5e76\u9644\u4e0a\u7814\u7a76\u4eba\u5458\u548c\u5b66\u5458\u7684SWOT\u5206\u6790\u3002", "result": "\u8bc6\u522b\u4e86\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u4e4b\u95f4\u7684\u534f\u540c\u9886\u57df\uff0c\u63d0\u51fa\u4e86NeuroAI\u6982\u5ff5\u2014\u2014\u4e00\u79cd\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u8ba4\u4e3a\u8fd9\u79cd\u4ea4\u53c9\u878d\u5408\u65e2\u80fd\u663e\u8457\u63d0\u5347AI\u7b97\u6cd5\u7684\u8303\u56f4\u548c\u6548\u7387\uff0c\u53c8\u80fd\u6539\u53d8\u5bf9\u751f\u7269\u795e\u7ecf\u8ba1\u7b97\u7684\u7406\u89e3\u65b9\u5f0f\u3002", "conclusion": "\u5021\u5bfc\u53d1\u5c55NeuroAI\uff0c\u8fd9\u79cd\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4eba\u5de5\u667a\u80fd\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u9700\u8981\u7ee7\u7eed\u63a2\u7d22\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u540c\u65f6\u901a\u8fc7SWOT\u5206\u6790\u8bc4\u4f30\u4e86NeuroAI\u7684\u76ca\u5904\u548c\u98ce\u9669\u3002"}}
{"id": "2601.19939", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19939", "abs": "https://arxiv.org/abs/2601.19939", "authors": ["Hyunmin Kim", "Yukun Zhou", "Rahul A. Jonas", "Lie Ju", "Sunjin Hwang", "Pearse A. Keane", "Siegfried K. Wagner"], "title": "oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction", "comment": "Accepted to ISBI 2026", "summary": "Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.", "AI": {"tldr": "\u63d0\u51faOculomix\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u56fe\u50cf\u6df7\u5408\u6837\u672c\u589e\u5f3a\uff0c\u901a\u8fc7\u7ea6\u675f\u6df7\u5408\u7a7a\u95f4\u5230\u60a3\u8005\u548c\u68c0\u67e5\u5c42\u7ea7\uff0c\u66f4\u597d\u5730\u4fdd\u7559\u60a3\u8005\u7279\u5f02\u6027\u7279\u5f81\uff0c\u5728\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u7ea7\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7ea7\u6df7\u5408\u6837\u672c\u589e\u5f3a\u65b9\u6cd5\uff08\u5982CutMix\u3001MixUp\uff09\u4f1a\u6270\u52a8\u60a3\u8005\u7279\u5f02\u6027\u5c5e\u6027\uff08\u5982\u533b\u7597\u5171\u75c5\u548c\u4e34\u5e8a\u56e0\u7d20\uff09\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ea\u8003\u8651\u56fe\u50cf\u548c\u6807\u7b7e\uff0c\u4e0d\u9002\u5408\u9700\u8981\u4fdd\u7559\u60a3\u8005\u7279\u5b9a\u7279\u5f81\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u3002", "method": "\u63d0\u51faOculomix\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u57fa\u4e8e\u4e24\u4e2a\u4e34\u5e8a\u5148\u9a8c\uff1a1\uff09\u540c\u4e00\u60a3\u8005\u540c\u4e00\u65f6\u95f4\u70b9\u91c7\u96c6\u7684\u56fe\u50cf\u5171\u4eab\u76f8\u540c\u5c5e\u6027\uff1b2\uff09\u540c\u4e00\u60a3\u8005\u4e0d\u540c\u65f6\u95f4\u70b9\u91c7\u96c6\u7684\u56fe\u50cf\u5177\u6709\u8f6f\u65f6\u95f4\u8d8b\u52bf\uff08\u53d1\u75c5\u7387\u968f\u65f6\u95f4\u589e\u52a0\uff09\u3002\u8be5\u65b9\u6cd5\u5c06\u6df7\u5408\u7a7a\u95f4\u7ea6\u675f\u5230\u60a3\u8005\u548c\u68c0\u67e5\u5c42\u7ea7\uff0c\u5229\u7528\u5206\u5c42\u5173\u7cfb\u4fdd\u7559\u60a3\u8005\u7279\u5f02\u6027\u7279\u5f81\u3002", "result": "\u5728\u5927\u578b\u591a\u79cd\u65cf\u4eba\u7fa4\uff08Alzeye\uff09\u4e2d\u9a8c\u8bc1\uff0c\u7528\u4e8e\u9884\u6d4b5\u5e74\u4e3b\u8981\u4e0d\u826f\u5fc3\u8840\u7ba1\u4e8b\u4ef6\uff08MACE\uff09\u3002Oculomix\u5728AUROC\u4e0a\u6bd4\u56fe\u50cf\u7ea7CutMix\u548cMixUp\u63d0\u5347\u9ad8\u8fbe3%\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u773c\u7ec4\u5b66\u4e2d\u7684\u5fc5\u8981\u6027\u548c\u4ef7\u503c\u3002", "conclusion": "Oculomix\u901a\u8fc7\u5206\u5c42\u91c7\u6837\u7b56\u7565\u89e3\u51b3\u4e86\u4f20\u7edf\u6df7\u5408\u6837\u672c\u589e\u5f3a\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u66f4\u597d\u5730\u4fdd\u7559\u60a3\u8005\u7279\u5f02\u6027\u7279\u5f81\uff0c\u5728\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u773c\u7ec4\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2601.20381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20381", "abs": "https://arxiv.org/abs/2601.20381", "authors": ["Alexandre Chapin", "Emmanuel Dellandr\u00e9a", "Liming Chen"], "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation", "comment": null, "summary": "Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.", "AI": {"tldr": "STORM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5bf9\u8c61\u4e2d\u5fc3\u9002\u914d\u6a21\u5757\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u69fd\u4f4d\u589e\u5f3a\u51bb\u7ed3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u5f3a\u5927\u7684\u611f\u77e5\u7279\u5f81\uff0c\u4f46\u5176\u5bc6\u96c6\u8868\u793a\u7f3a\u4e4f\u660e\u786e\u7684\u5bf9\u8c61\u7ea7\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6536\u7f29\u6027\u3002\u9700\u8981\u5c06\u901a\u7528\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8f6c\u5316\u4e3a\u4efb\u52a1\u611f\u77e5\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u3002", "method": "\u63d0\u51faSTORM\u6a21\u5757\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1) \u4f7f\u7528\u8bed\u8a00\u5d4c\u5165\u8fdb\u884c\u89c6\u89c9-\u8bed\u4e49\u9884\u8bad\u7ec3\u4ee5\u7a33\u5b9a\u5bf9\u8c61\u4e2d\u5fc3\u69fd\u4f4d\uff1b2) \u4e0e\u4e0b\u6e38\u64cd\u4f5c\u7b56\u7565\u8054\u5408\u9002\u914d\u3002\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\uff0c\u9632\u6b62\u69fd\u4f4d\u9000\u5316\u5e76\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5bf9\u8c61\u53d1\u73b0\u57fa\u51c6\u548c\u6a21\u62df\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTORM\u76f8\u6bd4\u76f4\u63a5\u4f7f\u7528\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u7279\u5f81\u6216\u7aef\u5230\u7aef\u8bad\u7ec3\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff0c\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u89c6\u89c9\u5e72\u6270\u7269\uff0c\u5e76\u63d0\u5347\u63a7\u5236\u6027\u80fd\u3002", "conclusion": "\u591a\u9636\u6bb5\u9002\u914d\u662f\u9ad8\u6548\u5c06\u901a\u7528\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8f6c\u5316\u4e3a\u4efb\u52a1\u611f\u77e5\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u7684\u6709\u6548\u673a\u5236\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u611f\u77e5-\u4efb\u52a1\u5bf9\u9f50\u3002"}}
{"id": "2601.19967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19967", "abs": "https://arxiv.org/abs/2601.19967", "authors": ["Jinlin Liu", "Wei Chen", "Xiaojin Zhang"], "title": "Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers", "comment": "This paper has been accepted to ICLR 2026", "summary": "Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.", "AI": {"tldr": "\u63d0\u51faPIL\u65b9\u6cd5\uff0c\u4f7f\u7528\u7ebf\u6027\u4ee3\u7406\u6a21\u578b\u751f\u6210\u6270\u52a8\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u4fdd\u62a4\uff0c\u540c\u65f6\u63ed\u793a\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u901a\u8fc7\u8bf1\u5bfc\u6a21\u578b\u7ebf\u6027\u5316\u53d1\u6325\u4f5c\u7528", "motivation": "\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u65b9\u6cd5\u4f9d\u8d56\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\u751f\u6210\u6270\u52a8\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5", "method": "\u63d0\u51faPerturbation-Induced Linearization (PIL)\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u7ebf\u6027\u4ee3\u7406\u6a21\u578b\u751f\u6210\u6270\u52a8\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "result": "PIL\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u63ed\u793a\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u901a\u8fc7\u8bf1\u5bfc\u6df1\u5ea6\u6a21\u578b\u7ebf\u6027\u5316\u53d1\u6325\u4f5c\u7528", "conclusion": "PIL\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6570\u636e\u4fdd\u62a4\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u6709\u6548\u6027\u7684\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u6570\u636e\u4fdd\u62a4\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2601.19931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19931", "abs": "https://arxiv.org/abs/2601.19931", "authors": ["Sebastien Kawada", "Dylan Holyoak"], "title": "CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity", "comment": "6 pages (including references), 2 figures, 2 tables. System description paper for SemEval-2026 Task 4 (Narrative Story Similarity)", "summary": "We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7528\u4e8e\u53d9\u4e8b\u6545\u4e8b\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u7ed3\u5408\u795e\u7ecf\u81ea\u4e00\u81f4\u6027\u6295\u7968\u4e0e\u591a\u5c3a\u5ea6\u53d9\u4e8b\u5206\u6790\u96c6\u6210\u4f5c\u4e3a\u5e73\u5c40\u51b3\u80dc\u5668", "motivation": "\u89e3\u51b3\u53d9\u4e8b\u6545\u4e8b\u76f8\u4f3c\u6027\u8bc4\u4f30\u4e2d\u7684\u6a21\u7cca\u6848\u4f8b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5ef6\u8fdf\u5230\u7b26\u53f7\u65b9\u6cd5\u6765\u589e\u5f3a\u795e\u7ecf\u9884\u6d4b\u80fd\u529b", "method": "\u7ea7\u8054\u67b6\u6784\uff1a\u795e\u7ecf\u7ec4\u4ef6\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u8f6e\u81ea\u4e00\u81f4\u6027\u6295\u7968\uff0c\u8fbe\u5230\u8d85\u591a\u6570\u9608\u503c\u65f6\u505a\u51fa\u51b3\u7b56\uff1b\u5b8c\u7f8e\u5e73\u5c40\u65f6\uff0c\u7b26\u53f7\u96c6\u6210\u7ed3\u5408\u4e94\u79cd\u53d9\u4e8b\u76f8\u4f3c\u6027\u4fe1\u53f7\uff08\u8bcd\u6c47\u91cd\u53e0\u3001\u8bed\u4e49\u5d4c\u5165\u3001\u6545\u4e8b\u8bed\u6cd5\u7ed3\u6784\u3001\u4e8b\u4ef6\u94fe\u5bf9\u9f50\u3001\u53d9\u4e8b\u5f20\u529b\u66f2\u7ebf\uff09\u4f5c\u4e3a\u6700\u7ec8\u51b3\u7b56", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\u8fbe\u523081%\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u9009\u62e9\u6027\u5ef6\u8fdf\u5230\u7b26\u53f7\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3a\u795e\u7ecf\u9884\u6d4b\u5728\u771f\u6b63\u6a21\u7cca\u7684\u53d9\u4e8b\u6bd4\u8f83\u4e2d\u7684\u8868\u73b0", "conclusion": "\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u6295\u7968\u548c\u7b26\u53f7\u5206\u6790\u96c6\u6210\uff0c\u6709\u6548\u5904\u7406\u53d9\u4e8b\u76f8\u4f3c\u6027\u8bc4\u4f30\u4e2d\u7684\u6a21\u7cca\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u9009\u62e9\u6027\u5ef6\u8fdf\u7b56\u7565\u7684\u4ef7\u503c"}}
{"id": "2601.20279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20279", "abs": "https://arxiv.org/abs/2601.20279", "authors": ["Xiaofeng Zhang", "Yuanchao Zhu", "Chaochen Gu", "Xiaosong Yuan", "Qiyan Zhao", "Jiawei Cao", "Feilong Tang", "Sinan Fan", "Yaomin Shen", "Chen Shen", "Hao Tang"], "title": "Hallucination Begins Where Saliency Drops", "comment": "Accepted in ICLR 2026", "summary": "Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency", "AI": {"tldr": "\u63d0\u51faLVLMs-Saliency\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u6ce8\u610f\u529b\u6743\u91cd\u548c\u8f93\u5165\u68af\u5ea6\u6765\u91cf\u5316\u89c6\u89c9\u57fa\u7840\u5f3a\u5ea6\uff0c\u53d1\u73b0\u5e7b\u89c9\u5e38\u51fa\u73b0\u5728\u524d\u5e8ftoken\u5bf9\u4e0b\u4e00token\u9884\u6d4b\u7684\u663e\u8457\u6027\u8f83\u4f4e\u65f6\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u63a8\u7406\u65f6\u673a\u5236\u6765\u7f13\u89e3\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u524d\u5411\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5ffd\u7565\u4e86\u68af\u5ea6\u4fe1\u53f7\uff0c\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u5e7b\u89c9\u548c\u4e8b\u5b9e\u57fa\u7840\u8f93\u51fa\u3002\u9700\u8981\u7ed3\u5408\u68af\u5ea6\u4fe1\u606f\u6765\u66f4\u597d\u7406\u89e3token\u5728\u7f51\u7edc\u4e2d\u7684\u5f71\u54cd\u4f20\u64ad\u3002", "method": "\u63d0\u51faLVLMs-Saliency\u68af\u5ea6\u611f\u77e5\u8bca\u65ad\u6846\u67b6\uff0c\u878d\u5408\u6ce8\u610f\u529b\u6743\u91cd\u548c\u8f93\u5165\u68af\u5ea6\u91cf\u5316\u89c6\u89c9\u57fa\u7840\u5f3a\u5ea6\u3002\u57fa\u4e8e\u5206\u6790\u53d1\u73b0\uff0c\u63d0\u51fa\u4e24\u79cd\u63a8\u7406\u65f6\u673a\u5236\uff1a\u663e\u8457\u6027\u5f15\u5bfc\u62d2\u7edd\u91c7\u6837\uff08SGRS\uff09\u52a8\u6001\u8fc7\u6ee4\u5019\u9009token\uff1b\u5c40\u90e8\u4e00\u81f4\u6027\u589e\u5f3a\uff08LocoRE\uff09\u6a21\u5757\u52a0\u5f3a\u5f53\u524dtoken\u5bf9\u6700\u8fd1\u524d\u5e8ftoken\u7684\u6ce8\u610f\u529b\u3002", "result": "\u5728\u591a\u4e2aLVLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u7545\u6027\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u6a21\u578b\u53ef\u9760\u6027\u7684\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LVLMs-Saliency\u6846\u67b6\u901a\u8fc7\u68af\u5ea6\u611f\u77e5\u5206\u6790\u63ed\u793a\u4e86\u5e7b\u89c9\u4ea7\u751f\u7684\u5173\u952e\u6a21\u5f0f\uff0c\u63d0\u51fa\u7684\u63a8\u7406\u65f6\u673a\u5236\u80fd\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.20701", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20701", "abs": "https://arxiv.org/abs/2601.20701", "authors": ["Guowei Zou", "Haitao Wang", "Hejun Wu", "Yukun Qian", "Yuhang Wang", "Weibing Li"], "title": "One Step Is Enough: Dispersive MeanFlow Policy Optimization", "comment": "Code and project page: https://guowei-zou.github.io/dmpo-page/", "summary": "Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step\n  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that\n  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,\n  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments\n  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With\n  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with\n  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world\n  applicability.", "AI": {"tldr": "DMPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u6b65\u751f\u6210\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7MeanFlow\u3001\u5206\u6563\u6b63\u5219\u5316\u548cRL\u5fae\u8c03\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\uff0c\u76f8\u6bd4\u591a\u6b65\u91c7\u6837\u65b9\u6cd5\u67095-20\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u751f\u6210\u7b56\u7565\u9700\u8981\u591a\u6b65\u91c7\u6837\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u7684\u65f6\u95f4\u5173\u952e\u573a\u666f\u9700\u6c42\u3002", "method": "\u63d0\u51faDMPO\u6846\u67b6\uff1a1) MeanFlow\u5b9e\u73b0\u6570\u5b66\u63a8\u5bfc\u7684\u5355\u6b65\u63a8\u7406\uff0c\u65e0\u9700\u77e5\u8bc6\u84b8\u998f\uff1b2) \u5206\u6563\u6b63\u5219\u5316\u9632\u6b62\u8868\u793a\u5d29\u6e83\uff1b3) RL\u5fae\u8c03\u8d85\u8d8a\u4e13\u5bb6\u6f14\u793a\u3002", "result": "\u5728RoboMimic\u64cd\u4f5c\u548cOpenAI Gym\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u591a\u6b65\u57fa\u7ebf\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53475-20\u500d\uff0c\u8fbe\u5230>120Hz\u5b9e\u65f6\u63a7\u5236\u8981\u6c42\uff0c\u5728Franka\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "DMPO\u901a\u8fc7\u5355\u6b65\u751f\u6210\u5b9e\u73b0\u4e86\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u7b56\u7565\u7684\u65f6\u95f4\u9650\u5236\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2601.20720", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20720", "abs": "https://arxiv.org/abs/2601.20720", "authors": ["Matej Halinkovic", "Nina Masarykova", "Alexey Vinel", "Marek Galinski"], "title": "Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.", "AI": {"tldr": "Li-ViP3D++\uff1a\u57fa\u4e8e\u67e5\u8be2\u7684\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u95e8\u63a7\u53ef\u53d8\u5f62\u878d\u5408\uff08QGDF\uff09\u5728\u67e5\u8be2\u7a7a\u95f4\u96c6\u6210\u591a\u89c6\u89d2RGB\u548cLiDAR\u6570\u636e\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u6a21\u5757\u5316\u611f\u77e5\u9884\u6d4b\u6d41\u7a0b\u5b58\u5728\u4fe1\u606f\u6d41\u53d7\u9650\u548c\u8bef\u5dee\u653e\u5927\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u67e5\u8be2\u7684\u7aef\u5230\u7aef\u6a21\u578b\u5728\u76f8\u673a\u548cLiDAR\u878d\u5408\u65b9\u9762\u63a2\u7d22\u4e0d\u8db3\uff0c\u901a\u5e38\u4f7f\u7528\u542f\u53d1\u5f0f\u5bf9\u9f50\u548c\u79bb\u6563\u9009\u62e9\u6b65\u9aa4\uff0c\u5bfc\u81f4\u4fe1\u606f\u5229\u7528\u4e0d\u5145\u5206\u548c\u504f\u5dee\u5f15\u5165\u3002", "method": "\u63d0\u51fa\u67e5\u8be2\u95e8\u63a7\u53ef\u53d8\u5f62\u878d\u5408\uff08QGDF\uff09\uff1a1\uff09\u901a\u8fc7\u63a9\u7801\u6ce8\u610f\u529b\u8de8\u76f8\u673a\u548c\u7279\u5f81\u5c42\u805a\u5408\u56fe\u50cf\u8bc1\u636e\uff1b2\uff09\u901a\u8fc7\u5e26\u5b66\u4e60\u504f\u79fb\u91cf\u7684\u5b8c\u5168\u53ef\u5fae\u5206BEV\u91c7\u6837\u63d0\u53d6LiDAR\u4e0a\u4e0b\u6587\uff1b3\uff09\u5e94\u7528\u67e5\u8be2\u6761\u4ef6\u95e8\u63a7\u81ea\u9002\u5e94\u52a0\u6743\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u7ebf\u7d22\u3002\u67b6\u6784\u8054\u5408\u4f18\u5316\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u591a\u5047\u8bbe\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cLi-ViP3D++\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u884c\u4e3a\u8d28\u91cf\u548c\u68c0\u6d4b\u8d28\u91cf\uff1aEPA\u8fbe\u52300.335\uff0cmAP\u8fbe\u52300.502\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\uff08FP ratio 0.147\uff09\uff0c\u4e14\u6bd4\u524d\u4ee3Li-ViP3D\u66f4\u5feb\uff08139.82 ms vs. 145.91 ms\uff09\u3002", "conclusion": "\u67e5\u8be2\u7a7a\u95f4\u4e2d\u7684\u5b8c\u5168\u53ef\u5fae\u5206\u76f8\u673a-LiDAR\u878d\u5408\u80fd\u591f\u5728\u4e0d\u727a\u7272\u90e8\u7f72\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u7aef\u5230\u7aef\u611f\u77e5\u9884\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u67e5\u8be2\u7a7a\u95f4\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.20069", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20069", "abs": "https://arxiv.org/abs/2601.20069", "authors": ["Chi-Yao Huang", "Khoa Vo", "Aayush Atul Verma", "Duo Lu", "Yezhou Yang"], "title": "Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning", "comment": "Accepted to ICLR 2026", "summary": "Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.", "AI": {"tldr": "\u63d0\u51faDomain Expansion\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u6c60\u5316\u673a\u5236\u6784\u5efa\u591a\u4efb\u52a1\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u89e3\u51b3\u591a\u76ee\u6807\u8bad\u7ec3\u4e2d\u7684\u6f5c\u5728\u8868\u793a\u5d29\u6e83\u95ee\u9898", "motivation": "\u591a\u76ee\u6807\u8bad\u7ec3\u4e2d\uff0c\u4e0d\u540c\u4efb\u52a1\u7684\u68af\u5ea6\u51b2\u7a81\u4f1a\u5bfc\u81f4\u5171\u4eab\u8868\u793a\u9000\u5316\uff0c\u9677\u5165\u5bf9\u6240\u6709\u4efb\u52a1\u90fd\u6b21\u4f18\u7684\u59a5\u534f\u72b6\u6001\uff0c\u5373\u6f5c\u5728\u8868\u793a\u5d29\u6e83\u95ee\u9898", "method": "\u63d0\u51faDomain Expansion\u6846\u67b6\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u6b63\u4ea4\u6c60\u5316\u673a\u5236\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3a\u6bcf\u4e2a\u76ee\u6807\u5206\u914d\u76f8\u4e92\u6b63\u4ea4\u7684\u5b50\u7a7a\u95f4", "result": "\u5728ShapeNet\u3001MPIIGaze\u548cRotated MNIST\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u9632\u6b62\u8868\u793a\u5d29\u6e83\uff0c\u8fd8\u4ea7\u751f\u53ef\u89e3\u91ca\u3001\u53ef\u7ec4\u5408\u7684\u6f5c\u5728\u7a7a\u95f4", "conclusion": "\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7ed3\u6784\u6709\u6548\u89e3\u51b3\u591a\u76ee\u6807\u8bad\u7ec3\u4e2d\u7684\u8868\u793a\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u53ef\u64cd\u4f5c\u7684\u6f5c\u5728\u8868\u793a"}}
{"id": "2601.20367", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20367", "abs": "https://arxiv.org/abs/2601.20367", "authors": ["Qing Lyu", "Zhe Fu", "Alexandre Bayen"], "title": "Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models", "comment": null, "summary": "Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u65bc\u591a\u667a\u80fd\u9ad4Transformer\u7684\u7121\u76e3\u7763\u7570\u5e38\u6aa2\u6e2c\u6846\u67b6\uff0c\u7528\u65bc\u8b58\u5225\u81ea\u52d5\u99d5\u99db\u4e2d\u7684\u5b89\u5168\u95dc\u9375\u5834\u666f\uff0c\u901a\u904e\u9810\u6e2c\u6b98\u5dee\u5efa\u6a21\u6b63\u5e38\u99d5\u99db\u4e26\u6aa2\u6e2c\u504f\u5dee\uff0c\u5728NGSIM\u6578\u64da\u96c6\u4e0a\u9a57\u8b49\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52d5\u99d5\u99db\u4e2d\u5b89\u5168\u95dc\u9375\u5834\u666f\u7a00\u5c11\uff0c\u76e3\u7763\u6a19\u8a3b\u4e0d\u73fe\u5be6\uff1b\u50b3\u7d71\u57fa\u65bc\u898f\u5247\u7684\u6307\u6a19\u904e\u65bc\u7c21\u55ae\uff0c\u7121\u6cd5\u6355\u6349\u8907\u96dc\u4ea4\u4e92\u98a8\u96aa\uff1b\u73fe\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7d71\u6027\u9a57\u8b49\u7d71\u8a08\u7570\u5e38\u662f\u5426\u771f\u6b63\u53cd\u6620\u7269\u7406\u5371\u96aa\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u9ad4Transformer\u5efa\u6a21\u6b63\u5e38\u99d5\u99db\u884c\u70ba\uff0c\u901a\u904e\u9810\u6e2c\u6b98\u5dee\u6e2c\u91cf\u504f\u5dee\uff1b\u63d0\u51fa\u96d9\u91cd\u8a55\u4f30\u65b9\u6848\uff1a\u7a69\u5b9a\u6027\uff08Kendall\u79e9\u76f8\u95dc\u4fc2\u6578\u548cJaccard\u6307\u6578\uff09\u548c\u7269\u7406\u5c0d\u9f4a\u6027\uff08\u8207\u66ff\u4ee3\u5b89\u5168\u6307\u6a19\u7684\u76f8\u95dc\u6027\uff09\u3002", "result": "\u5728NGSIM\u6578\u64da\u96c6\u4e0a\uff0c\u6700\u5927\u6b98\u5dee\u805a\u5408\u5668\u5be6\u73fe\u6700\u9ad8\u7269\u7406\u5c0d\u9f4a\u6027\u540c\u6642\u4fdd\u6301\u7a69\u5b9a\u6027\uff1b\u8b58\u5225\u51fa388\u500b\u88ab\u50b3\u7d71\u65b9\u6cd5\u907a\u6f0f\u7684\u7368\u7279\u7570\u5e38\uff0c\u6355\u6349\u5230\u5982\u6a6b\u5411\u6f02\u79fb\u4e0b\u7684\u53cd\u61c9\u5236\u52d5\u7b49\u7d30\u5fae\u591a\u667a\u80fd\u9ad4\u98a8\u96aa\uff1b\u7570\u5e38\u805a\u985e\u70ba\u56db\u7a2e\u53ef\u89e3\u91cb\u98a8\u96aa\u985e\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7121\u76e3\u7763\u7570\u5e38\u6aa2\u6e2c\u6846\u67b6\u80fd\u6709\u6548\u8b58\u5225\u81ea\u52d5\u99d5\u99db\u4e2d\u7684\u5b89\u5168\u95dc\u9375\u5834\u666f\uff0c\u5f4c\u88dc\u4e86\u50b3\u7d71\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u70ba\u4eff\u771f\u548c\u6e2c\u8a66\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u898b\u89e3\u3002"}}
{"id": "2601.20789", "categories": ["cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20789", "abs": "https://arxiv.org/abs/2601.20789", "authors": ["Ethan Shen", "Danny Tormoen", "Saurabh Shah", "Ali Farhadi", "Tim Dettmers"], "title": "SERA: Soft-Verified Efficient Repository Agents", "comment": "21 main pages, 7 pages appendix", "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "AI": {"tldr": "SERA\u662f\u4e00\u79cd\u9ad8\u6548\u8bad\u7ec3\u4ee3\u7801\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd\uff0c\u6210\u672c\u6bd4\u5f3a\u5316\u5b66\u4e60\u4f4e26\u500d\uff0c\u6bd4\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4f4e57\u500d\uff0c\u53ef\u9488\u5bf9\u79c1\u6709\u4ee3\u7801\u5e93\u8fdb\u884c\u4e13\u4e1a\u5316\u8bad\u7ec3\u3002", "motivation": "\u5f00\u6e90\u6743\u91cd\u4ee3\u7801\u4ee3\u7406\u76f8\u5bf9\u4e8e\u95ed\u6e90\u7cfb\u7edf\u5e94\u5177\u6709\u6839\u672c\u4f18\u52bf\uff1a\u80fd\u591f\u9488\u5bf9\u79c1\u6709\u4ee3\u7801\u5e93\u8fdb\u884c\u4e13\u4e1a\u5316\u8bad\u7ec3\uff0c\u5c06\u4ed3\u5e93\u7279\u5b9a\u4fe1\u606f\u76f4\u63a5\u7f16\u7801\u5230\u6743\u91cd\u4e2d\u3002\u4f46\u7531\u4e8e\u8bad\u7ec3\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u8fd9\u4e00\u4f18\u52bf\u4e00\u76f4\u505c\u7559\u5728\u7406\u8bba\u5c42\u9762\u3002", "method": "\u63d0\u51faSoft-Verified Efficient Repository Agents (SERA)\u65b9\u6cd5\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u548cSoft Verified Generation (SVG)\u6280\u672f\u3002SVG\u80fd\u4ece\u5355\u4e2a\u4ee3\u7801\u4ed3\u5e93\u751f\u6210\u6570\u5343\u6761\u8f68\u8ff9\uff0c\u7ed3\u5408\u6210\u672c\u6548\u76ca\u5b9e\u73b0\u79c1\u6709\u4ee3\u7801\u5e93\u4e13\u4e1a\u5316\u3002", "result": "SERA\u5728\u5b8c\u5168\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5339\u914dDevstral-Small-2\u7b49\u524d\u6cbf\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u3002\u8bad\u7ec3\u6210\u672c\u6bd4\u5f3a\u5316\u5b66\u4e60\u4f4e26\u500d\uff0c\u6bd4\u5148\u524d\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4f4e57\u500d\u3002\u751f\u6210\u8d85\u8fc720\u4e07\u6761\u5408\u6210\u8f68\u8ff9\u7528\u4e8e\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "SERA\u4f7f\u9488\u5bf9\u79c1\u6709\u4ee3\u7801\u5e93\u7684\u4e13\u4e1a\u5316\u8bad\u7ec3\u53d8\u5f97\u5b9e\u7528\uff0c\u5c06\u52a0\u901f\u5f00\u6e90\u4ee3\u7801\u4ee3\u7406\u7814\u7a76\uff0c\u5c55\u793a\u5f00\u6e90\u6a21\u578b\u5728\u79c1\u6709\u4ee3\u7801\u5e93\u4e13\u4e1a\u5316\u65b9\u9762\u7684\u4f18\u52bf\u3002\u53d1\u5e03SERA\u4f5c\u4e3aAi2\u5f00\u6e90\u4ee3\u7801\u4ee3\u7406\u7cfb\u5217\u7684\u9996\u4e2a\u6a21\u578b\u3002"}}
{"id": "2601.20585", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20585", "abs": "https://arxiv.org/abs/2601.20585", "authors": ["Aiming Hao", "Chen Zhu", "Jiashu Zhu", "Jiahong Wu", "Xiangxiang Chu"], "title": "Ranking-aware Reinforcement Learning for Ordinal Ranking", "comment": "Accepted to ICASSP2026", "summary": "Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.", "AI": {"tldr": "\u63d0\u51faRARL\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u663e\u5f0f\u5b66\u4e60\u5e8f\u6570\u4f9d\u8d56\u5173\u7cfb\uff0c\u7ed3\u5408\u56de\u5f52\u548c\u6392\u5e8f\u4efb\u52a1\uff0c\u4f7f\u7528\u6392\u5e8f\u611f\u77e5\u5956\u52b1\u548c\u54cd\u5e94\u53d8\u5f02\u64cd\u4f5c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u5e8f\u6570\u56de\u5f52\u548c\u6392\u5e8f\u4e2d\u7684\u5e8f\u6570\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u5b66\u4e60\u8fd9\u4e9b\u5173\u7cfb\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faRARL\u6846\u67b6\uff1a1) \u7edf\u4e00\u76ee\u6807\u51fd\u6570\u534f\u540c\u6574\u5408\u56de\u5f52\u548c\u6392\u5e8f\u5b66\u4e60\uff1b2) \u6392\u5e8f\u611f\u77e5\u53ef\u9a8c\u8bc1\u5956\u52b1\u8054\u5408\u8bc4\u4f30\u56de\u5f52\u7cbe\u5ea6\u548c\u6392\u5e8f\u51c6\u786e\u6027\uff1b3) \u54cd\u5e94\u53d8\u5f02\u64cd\u4f5c\u6ce8\u5165\u53d7\u63a7\u566a\u58f0\u6539\u5584\u63a2\u7d22\uff1b4) \u901a\u8fc7\u7b56\u7565\u4f18\u5316\u76f4\u63a5\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RARL\u7684\u6709\u6548\u6027\u3002", "conclusion": "RARL\u6210\u529f\u89e3\u51b3\u4e86\u5e8f\u6570\u4f9d\u8d56\u5efa\u6a21\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u56de\u5f52\u548c\u6392\u5e8f\u4efb\u52a1\u7684\u534f\u540c\u6539\u8fdb\u3002"}}
{"id": "2601.20714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20714", "abs": "https://arxiv.org/abs/2601.20714", "authors": ["Raul de la Rosa", "Ivana Dusparic", "Nicolas Cardozo"], "title": "Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions", "comment": null, "summary": "Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.", "AI": {"tldr": "MORPHIN\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684Q\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u53c2\u6570\uff0c\u9002\u5e94\u5956\u52b1\u51fd\u6570\u53d8\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u975e\u5e73\u7a33\u73af\u5883\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u5956\u52b1\u51fd\u6570\u53d8\u5316\u6216\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "MORPHIN\u6846\u67b6\u96c6\u6210\u4e86\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u548c\u63a2\u7d22\u8d85\u53c2\u6570\uff0c\u9002\u5e94\u5956\u52b1\u51fd\u6570\u53d8\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u7559\u5148\u524d\u7684\u7b56\u7565\u77e5\u8bc6\u3002", "result": "\u5728Gridworld\u57fa\u51c6\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6a21\u62df\u4e2d\uff0cMORPHIN\u76f8\u6bd4\u6807\u51c6Q\u5b66\u4e60\u57fa\u7ebf\u5b9e\u73b0\u4e861.7\u500d\u7684\u5b66\u4e60\u6548\u7387\u63d0\u5347\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u6301\u7eed\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "MORPHIN\u6846\u67b6\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u52a8\u6001\u53d8\u5316\u4e2d\u4fdd\u6301\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.20729", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20729", "abs": "https://arxiv.org/abs/2601.20729", "authors": ["Anchen Sun", "Zhibin Chen", "Xiaodong Cai"], "title": "Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis", "comment": null, "summary": "The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.", "AI": {"tldr": "\u63d0\u51faCox-MT\u6a21\u578b\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u7684Mean Teacher\u6846\u67b6\uff0c\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u8bad\u7ec3ANN-based Cox\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u764c\u75c7\u9884\u540e\u9884\u6d4b\u6027\u80fd", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6837\u672c\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u6807\u8bb0\u6570\u636e\u6709\u9650\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002\u9700\u8981\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u57fa\u4e8eMean Teacher\u6846\u67b6\u5f00\u53d1\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001ANN-based Cox\u6a21\u578b\uff08Cox-MT\uff09\uff0c\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728TCGA\u6570\u636e\u96c6\u4e0a\uff0c\u5355\u6a21\u6001Cox-MT\u6a21\u578b\uff08\u4f7f\u7528RNA-seq\u6216\u5168\u5207\u7247\u56fe\u50cf\uff09\u5728\u56db\u79cd\u764c\u75c7\u7c7b\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709Cox-nnet\u6a21\u578b\uff1b\u968f\u7740\u672a\u6807\u8bb0\u6837\u672c\u589e\u52a0\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1b\u591a\u6a21\u6001Cox-MT\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "Cox-MT\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6807\u8bb0\u6570\u636e\u7684\u73b0\u6709ANN-based Cox\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
