{"id": "2510.12843", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12843", "abs": "https://arxiv.org/abs/2510.12843", "authors": ["Ansh Tiwari", "Ayush Chauhan"], "title": "Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks", "comment": null, "summary": "Spiking neural networks (SNNs) promise energy-efficient artificial\nintelligence on neuromorphic hardware but struggle with tasks requiring both\nfast adaptation and long-term memory, especially in continual learning. We\npropose Local Timescale Gating (LT-Gate), a neuron model that combines dual\ntime-constant dynamics with an adaptive gating mechanism. Each spiking neuron\ntracks information on a fast and a slow timescale in parallel, and a learned\ngate locally adjusts their influence. This design enables individual neurons to\npreserve slow contextual information while responding to fast signals,\naddressing the stability-plasticity dilemma. We further introduce a\nvariance-tracking regularization that stabilizes firing activity, inspired by\nbiological homeostasis. Empirically, LT-Gate yields significantly improved\naccuracy and retention in sequential learning tasks: on a challenging temporal\nclassification benchmark it achieves about 51 percent final accuracy, compared\nto about 46 percent for a recent Hebbian continual-learning baseline and lower\nfor prior SNN methods. Unlike approaches that require external replay or\nexpensive orthogonalizations, LT-Gate operates with local updates and is fully\ncompatible with neuromorphic hardware. In particular, it leverages features of\nIntel's Loihi chip (multiple synaptic traces with different decay rates) for\non-chip learning. Our results demonstrate that multi-timescale gating can\nsubstantially enhance continual learning in SNNs, narrowing the gap between\nspiking and conventional deep networks on lifelong-learning tasks.", "AI": {"tldr": "\u63d0\u51faLT-Gate\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u65f6\u95f4\u5c3a\u5ea6\u52a8\u6001\u548c\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u89e3\u51b3SNN\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u5728\u65f6\u5e8f\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548c\u8bb0\u5fc6\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u5177\u6709\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5728\u9700\u8981\u5feb\u901f\u9002\u5e94\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002", "method": "LT-Gate\u6a21\u578b\u8ba9\u6bcf\u4e2a\u8109\u51b2\u795e\u7ecf\u5143\u5e76\u884c\u8ddf\u8e2a\u5feb\u901f\u548c\u6162\u901f\u65f6\u95f4\u5c3a\u5ea6\u4fe1\u606f\uff0c\u901a\u8fc7\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u5c40\u90e8\u8c03\u6574\u5176\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u65b9\u5dee\u8ddf\u8e2a\u6b63\u5219\u5316\u6765\u7a33\u5b9a\u53d1\u653e\u6d3b\u52a8\u3002", "result": "\u5728\u6311\u6218\u6027\u65f6\u5e8f\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u7ea651%\u7684\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u4f18\u4e8eHebbian\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u7684\u7ea646%\u548c\u5148\u524dSNN\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u91cd\u653e\u6216\u6602\u8d35\u6b63\u4ea4\u5316\u3002", "conclusion": "\u591a\u65f6\u95f4\u5c3a\u5ea6\u95e8\u63a7\u53ef\u663e\u8457\u589e\u5f3aSNN\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u7f29\u5c0f\u8109\u51b2\u7f51\u7edc\u4e0e\u4f20\u7edf\u6df1\u5ea6\u7f51\u7edc\u5728\u7ec8\u8eab\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u5dee\u8ddd\uff0c\u5e76\u5b8c\u5168\u517c\u5bb9\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u3002"}}
{"id": "2510.13029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13029", "abs": "https://arxiv.org/abs/2510.13029", "authors": ["Xinlei Wang", "Mingtian Tan", "Jing Qiu", "Junhua Zhao", "Jinjin Gu"], "title": "Toward Reasoning-Centric Time-Series Analysis", "comment": null, "summary": "Traditional time series analysis has long relied on pattern recognition,\ntrained on static and well-established benchmarks. However, in real-world\nsettings -- where policies shift, human behavior adapts, and unexpected events\nunfold -- effective analysis must go beyond surface-level trends to uncover the\nactual forces driving them. The recent rise of Large Language Models (LLMs)\npresents new opportunities for rethinking time series analysis by integrating\nmultimodal inputs. However, as the use of LLMs becomes popular, we must remain\ncautious, asking why we use LLMs and how to exploit them effectively. Most\nexisting LLM-based methods still employ their numerical regression ability and\nignore their deeper reasoning potential. This paper argues for rethinking time\nseries with LLMs as a reasoning task that prioritizes causal structure and\nexplainability. This shift brings time series analysis closer to human-aligned\nunderstanding, enabling transparent and context-aware insights in complex\nreal-world environments.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u91cd\u65b0\u6784\u60f3\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u5229\u7528LLMs\u7684\u6df1\u5c42\u63a8\u7406\u6f5c\u529b\u800c\u975e\u6570\u503c\u56de\u5f52\u80fd\u529b\uff0c\u5f3a\u8c03\u56e0\u679c\u7ed3\u6784\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5206\u6790\u66f4\u8d34\u8fd1\u4eba\u7c7b\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u653f\u7b56\u53d8\u5316\u3001\u4eba\u7c7b\u884c\u4e3a\u9002\u5e94\u548c\u610f\u5916\u4e8b\u4ef6\u9891\u53d1\uff0c\u9700\u8981\u8d85\u8d8a\u8868\u9762\u8d8b\u52bf\u5206\u6790\u5b9e\u9645\u9a71\u52a8\u56e0\u7d20\u3002\u73b0\u6709LLM\u65b9\u6cd5\u591a\u4f7f\u7528\u6570\u503c\u56de\u5f52\u80fd\u529b\u800c\u5ffd\u89c6\u5176\u6df1\u5c42\u63a8\u7406\u6f5c\u529b\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u91cd\u65b0\u5b9a\u4e49\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u5229\u7528LLMs\u7684\u591a\u6a21\u6001\u8f93\u5165\u6574\u5408\u80fd\u529b\uff0c\u4f18\u5148\u8003\u8651\u56e0\u679c\u7ed3\u6784\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u66f4\u8d34\u8fd1\u4eba\u7c7b\u5bf9\u9f50\u7684\u7406\u89e3\uff0c\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u63d0\u4f9b\u900f\u660e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u89d2\u8272\uff0c\u5c06\u5176\u4f5c\u4e3a\u63a8\u7406\u5de5\u5177\u800c\u975e\u5355\u7eaf\u56de\u5f52\u6a21\u578b\uff0c\u5f3a\u8c03\u56e0\u679c\u89e3\u91ca\u548c\u900f\u660e\u6027\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2510.12904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12904", "abs": "https://arxiv.org/abs/2510.12904", "authors": ["Saurav Sharma", "Chinedu Innocent Nwoye", "Didier Mutter", "Nicolas Padoy"], "title": "State-Change Learning for Prediction of Future Events in Endoscopic Videos", "comment": "24 pages, 13 figures", "summary": "Surgical future prediction, driven by real-time AI analysis of surgical\nvideo, is critical for operating room safety and efficiency. It provides\nactionable insights into upcoming events, their timing, and risks-enabling\nbetter resource allocation, timely instrument readiness, and early warnings for\ncomplications (e.g., bleeding, bile duct injury). Despite this need, current\nsurgical AI research focuses on understanding what is happening rather than\npredicting future events. Existing methods target specific tasks in isolation,\nlacking unified approaches that span both short-term (action triplets, events)\nand long-term horizons (remaining surgery duration, phase transitions). These\nmethods rely on coarse-grained supervision while fine-grained surgical action\ntriplets and steps remain underexplored. Furthermore, methods based only on\nfuture feature prediction struggle to generalize across different surgical\ncontexts and procedures. We address these limits by reframing surgical future\nprediction as state-change learning. Rather than forecasting raw observations,\nour approach classifies state transitions between current and future timesteps.\nWe introduce SurgFUTR, implementing this through a teacher-student\narchitecture. Video clips are compressed into state representations via\nSinkhorn-Knopp clustering; the teacher network learns from both current and\nfuture clips, while the student network predicts future states from current\nvideos alone, guided by our Action Dynamics (ActDyn) module. We establish\nSFPBench with five prediction tasks spanning short-term (triplets, events) and\nlong-term (remaining surgery duration, phase and step transitions) horizons.\nExperiments across four datasets and three procedures show consistent\nimprovements. Cross-procedure transfer validates generalizability.", "AI": {"tldr": "\u63d0\u51faSurgFUTR\u65b9\u6cd5\uff0c\u5c06\u624b\u672f\u672a\u6765\u9884\u6d4b\u91cd\u6784\u4e3a\u72b6\u6001\u53d8\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u548cAction Dynamics\u6a21\u5757\u5b9e\u73b0\u8de8\u624b\u672f\u7a0b\u5e8f\u7684\u901a\u7528\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u624b\u672fAI\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7406\u89e3\u5f53\u524d\u4e8b\u4ef6\u800c\u975e\u9884\u6d4b\u672a\u6765\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u540c\u624b\u672f\u60c5\u5883\u95f4\u6cdb\u5316\u3002", "method": "\u91c7\u7528\u5e08\u751f\u67b6\u6784\uff0c\u901a\u8fc7Sinkhorn-Knopp\u805a\u7c7b\u5c06\u89c6\u9891\u7247\u6bb5\u538b\u7f29\u4e3a\u72b6\u6001\u8868\u793a\uff0c\u6559\u5e08\u7f51\u7edc\u5b66\u4e60\u5f53\u524d\u548c\u672a\u6765\u7247\u6bb5\uff0c\u5b66\u751f\u7f51\u7edc\u4ec5\u4ece\u5f53\u524d\u89c6\u9891\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5e76\u7531Action Dynamics\u6a21\u5757\u6307\u5bfc\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u624b\u672f\u7a0b\u5e8f\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u4e00\u81f4\u6539\u8fdb\uff0c\u8de8\u7a0b\u5e8f\u8fc1\u79fb\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SurgFUTR\u901a\u8fc7\u72b6\u6001\u53d8\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u624b\u672f\u672a\u6765\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u826f\u597d\u7684\u8de8\u7a0b\u5e8f\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.12934", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12934", "abs": "https://arxiv.org/abs/2510.12934", "authors": ["Alex Gower"], "title": "Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines", "comment": "4 pages, 2 figures, NeurIPS 2025 Machine Learning and the Physical\n  Sciences (ML4PS)", "summary": "Physical systems that naturally perform energy descent offer a direct route\nto accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify\nthis idea: their GHz-frequency dynamics mirror both the optimization of\nenergy-based models (EBMs) and gradient descent on loss landscapes, while\nintrinsic noise corresponds to Langevin dynamics - supporting sampling as well\nas optimization. Equilibrium Propagation (EP) unifies these processes into\ndescent on a single total energy landscape, enabling local learning rules\nwithout global backpropagation. We show that EP on OIMs achieves competitive\naccuracy ($\\sim 97.2 \\pm 0.1 \\%$ on MNIST, $\\sim 88.0 \\pm 0.1 \\%$ on\nFashion-MNIST), while maintaining robustness under realistic hardware\nconstraints such as parameter quantization and phase noise. These results\nestablish OIMs as a fast, energy-efficient substrate for neuromorphic learning,\nand suggest that EBMs - often bottlenecked by conventional processors - may\nfind practical realization on physical hardware whose dynamics directly perform\ntheir optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5728\u632f\u8361\u5668\u4f0a\u8f9b\u673a(OIMs)\u4e0a\u5b9e\u73b0\u5e73\u8861\u4f20\u64ad(EP)\u7684\u65b9\u6cd5\uff0c\u5c06\u4f18\u5316\u548c\u91c7\u6837\u7edf\u4e00\u5230\u5355\u4e00\u80fd\u91cf\u666f\u89c2\u4e2d\uff0c\u5728MNIST\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u786c\u4ef6\u7ea6\u675f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5229\u7528\u7269\u7406\u7cfb\u7edf\u81ea\u7136\u6267\u884c\u80fd\u91cf\u4e0b\u964d\u7684\u7279\u6027\u6765\u52a0\u901f\u673a\u5668\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5c06\u80fd\u91cf\u57fa\u6a21\u578b(EBMs)\u7684\u4f18\u5316\u76f4\u63a5\u6620\u5c04\u5230\u7269\u7406\u786c\u4ef6\u52a8\u529b\u5b66\u4e0a\uff0c\u907f\u514d\u4f20\u7edf\u5904\u7406\u5668\u7684\u74f6\u9888\u3002", "method": "\u5728\u632f\u8361\u5668\u4f0a\u8f9b\u673a(OIMs)\u4e0a\u5b9e\u73b0\u5e73\u8861\u4f20\u64ad(EP)\uff0c\u5229\u7528GHz\u9891\u7387\u52a8\u529b\u5b66\u6a21\u62df\u80fd\u91cf\u57fa\u6a21\u578b\u4f18\u5316\u548c\u68af\u5ea6\u4e0b\u964d\uff0c\u5185\u5728\u566a\u58f0\u5bf9\u5e94\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff0c\u652f\u6301\u91c7\u6837\u548c\u4f18\u5316\u3002", "result": "\u5728MNIST\u4e0a\u8fbe\u523097.2\u00b10.1%\u51c6\u786e\u7387\uff0c\u5728Fashion-MNIST\u4e0a\u8fbe\u523088.0\u00b10.1%\u51c6\u786e\u7387\uff0c\u5728\u53c2\u6570\u91cf\u5316\u548c\u76f8\u4f4d\u566a\u58f0\u7b49\u73b0\u5b9e\u786c\u4ef6\u7ea6\u675f\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "OIMs\u4e3a\u795e\u7ecf\u5f62\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u8282\u80fd\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u8868\u660e\u80fd\u91cf\u57fa\u6a21\u578b\u53ef\u4ee5\u5728\u76f4\u63a5\u6267\u884c\u5176\u4f18\u5316\u7684\u7269\u7406\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.12992", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.12992", "abs": "https://arxiv.org/abs/2510.12992", "authors": ["Neel P. Bhatt", "Po-han Li", "Kushagra Gupta", "Rohan Siva", "Daniel Milan", "Alexander T. Hogue", "Sandeep P. Chinchali", "David Fridovich-Keil", "Zhangyang Wang", "Ufuk Topcu"], "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles", "comment": null, "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/", "AI": {"tldr": "UNCAP\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u8ba9\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u7136\u8bed\u8a00\u6d88\u606f\u8fdb\u884c\u901a\u4fe1\uff0c\u540c\u65f6\u660e\u786e\u8003\u8651\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e86\u534f\u540c\u89c4\u5212\u7684\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4f20\u8f93\u9ad8\u5e26\u5bbd\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u6d41\uff0c\u8981\u4e48\u5ffd\u7565\u5171\u4eab\u6570\u636e\u4e2d\u7684\u611f\u77e5\u548c\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u65e2\u4e0d\u53ef\u6269\u5c55\u4e5f\u4e0d\u5b89\u5168\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u901a\u4fe1\u534f\u8bae\uff1a\u9996\u5148\u8bc6\u522b\u6700\u76f8\u5173\u7684\u8f66\u8f86\u5b50\u96c6\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\uff0c\u7136\u540e\u9009\u5b9a\u7684\u8f66\u8f86\u4f20\u8f93\u5b9a\u91cf\u8868\u8fbe\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u6d88\u606f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u878d\u5408\u6700\u5927\u5316\u4e92\u4fe1\u606f\u7684\u6d88\u606f\u6765\u6574\u5408\u6700\u76f8\u5173\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u901a\u4fe1\u5e26\u5bbd\u51cf\u5c1163%\uff0c\u9a7e\u9a76\u5b89\u5168\u8bc4\u5206\u63d0\u9ad831%\uff0c\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e61%\uff0c\u8fd1\u78b0\u649e\u4e8b\u4ef6\u4e2d\u7684\u78b0\u649e\u8ddd\u79bb\u88d5\u5ea6\u589e\u52a0\u56db\u500d\u3002", "conclusion": "UNCAP\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u548c\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8f66\u534f\u540c\u89c4\u5212\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.13042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13042", "abs": "https://arxiv.org/abs/2510.13042", "authors": ["Zhengxu Tang", "Zizheng Wang", "Luning Wang", "Zitao Shuai", "Chenhao Zhang", "Siyu Qian", "Yirui Wu", "Bohao Wang", "Haosong Rao", "Zhenyu Yang", "Chenwei Wu"], "title": "SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models", "comment": null, "summary": "Text-to-video (T2V) generation models have made significant progress in\ncreating visually appealing videos. However, they struggle with generating\ncoherent sequential narratives that require logical progression through\nmultiple events. Existing T2V benchmarks primarily focus on visual quality\nmetrics but fail to evaluate narrative coherence over extended sequences. To\nbridge this gap, we present SeqBench, a comprehensive benchmark for evaluating\nsequential narrative coherence in T2V generation. SeqBench includes a carefully\ndesigned dataset of 320 prompts spanning various narrative complexities, with\n2,560 human-annotated videos generated from 8 state-of-the-art T2V models.\nAdditionally, we design a Dynamic Temporal Graphs (DTG)-based automatic\nevaluation metric, which can efficiently capture long-range dependencies and\ntemporal ordering while maintaining computational efficiency. Our DTG-based\nmetric demonstrates a strong correlation with human annotations. Through\nsystematic evaluation using SeqBench, we reveal critical limitations in current\nT2V models: failure to maintain consistent object states across multi-action\nsequences, physically implausible results in multi-object scenarios, and\ndifficulties in preserving realistic timing and ordering relationships between\nsequential actions. SeqBench provides the first systematic framework for\nevaluating narrative coherence in T2V generation and offers concrete insights\nfor improving sequential reasoning capabilities in future models. Please refer\nto https://videobench.github.io/SeqBench.github.io/ for more details.", "AI": {"tldr": "SeqBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u987a\u5e8f\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b320\u4e2a\u63d0\u793a\u7684\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u52a8\u6001\u65f6\u5e8f\u56fe\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u5f53\u524dT2V\u6a21\u578b\u5728\u591a\u52a8\u4f5c\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709T2V\u751f\u6210\u6a21\u578b\u5728\u521b\u5efa\u89c6\u89c9\u5438\u5f15\u529b\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u751f\u6210\u9700\u8981\u903b\u8f91\u9012\u8fdb\u7684\u591a\u4e8b\u4ef6\u8fde\u8d2f\u987a\u5e8f\u53d9\u4e8b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6269\u5c55\u5e8f\u5217\u4e2d\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86SeqBench\u57fa\u51c6\uff0c\u5305\u62ec\u7cbe\u5fc3\u8bbe\u8ba1\u7684320\u4e2a\u63d0\u793a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5404\u79cd\u53d9\u4e8b\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u52a8\u6001\u65f6\u5e8f\u56fe\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u9ad8\u6548\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u65f6\u95f4\u987a\u5e8f\u5173\u7cfb\u3002", "result": "\u57fa\u4e8eDTG\u7684\u6307\u6807\u4e0e\u4eba\u7c7b\u6807\u6ce8\u663e\u793a\u51fa\u5f3a\u76f8\u5173\u6027\u3002\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524dT2V\u6a21\u578b\u7684\u5173\u952e\u5c40\u9650\uff1a\u5728\u591a\u52a8\u4f5c\u5e8f\u5217\u4e2d\u65e0\u6cd5\u7ef4\u6301\u4e00\u81f4\u7684\u5bf9\u8c61\u72b6\u6001\u3001\u591a\u5bf9\u8c61\u573a\u666f\u4e2d\u4ea7\u751f\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u7ed3\u679c\u3001\u96be\u4ee5\u4fdd\u6301\u987a\u5e8f\u52a8\u4f5c\u4e4b\u95f4\u7684\u73b0\u5b9e\u65f6\u95f4\u987a\u5e8f\u5173\u7cfb\u3002", "conclusion": "SeqBench\u4e3a\u8bc4\u4f30T2V\u751f\u6210\u4e2d\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u5e76\u4e3a\u6539\u8fdb\u672a\u6765\u6a21\u578b\u7684\u987a\u5e8f\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u89c1\u89e3\u3002"}}
{"id": "2510.12838", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12838", "abs": "https://arxiv.org/abs/2510.12838", "authors": ["Qianben Chen", "Jingyi Cao", "Jiayu Zhang", "Tianrui Qin", "Xiaowan Li", "King Zhu", "Dingfeng Shi", "He Zhu", "Minghao Liu", "Xiaobo Liang", "Ge Zhang", "Jian Yang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "A\\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning", "comment": "9 pages, 5 figures, submitted to ICLR 2026", "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A\\textsuperscript{2}FM),\na unified framework that follows a route-then-align principle: the model first\nlearns task-aware routing and then aligns mode-specific trajectories under a\nshared backbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A\\textsuperscript{2}FM achieves\n13.4\\% on BrowseComp, 70.4\\% on AIME25, and 16.7\\% on HLE, setting new SOTA\namong comparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only \\$0.00487 per correct answer-cutting cost by\n45.2\\% relative to reasoning and 33.5\\% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.", "AI": {"tldr": "A\u00b2FM\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u8def\u7531\u548c\u6a21\u5f0f\u5bf9\u9f50\uff0c\u7ed3\u5408\u63a8\u7406\u3001\u5de5\u5177\u8c03\u7528\u548c\u5373\u65f6\u5904\u7406\u4e09\u79cd\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6210\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u63a8\u7406\u578b\u548c\u4ee3\u7406\u578b\uff0c\u5404\u81ea\u6709\u4e0d\u540c\u4f18\u52bf\u4f46\u5b58\u5728\u6548\u7387\u95ee\u9898\u3002\u63a8\u7406\u578b\u6a21\u578b\u64c5\u957f\u5185\u90e8\u601d\u7ef4\u94fe\u4f46\u65e0\u6cd5\u8c03\u7528\u5916\u90e8\u5de5\u5177\uff0c\u4ee3\u7406\u578b\u6a21\u578b\u80fd\u4e0e\u73af\u5883\u4ea4\u4e92\u4f46\u63a8\u7406\u80fd\u529b\u8f83\u5f31\uff0c\u4e24\u8005\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u90fd\u4f1a\u8fc7\u5ea6\u601d\u8003\u6216\u8fc7\u5ea6\u8c03\u7528\u5de5\u5177\u3002", "method": "\u91c7\u7528\u8def\u7531-\u5bf9\u9f50\u539f\u5219\uff1a\u5148\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u8def\u7531\uff0c\u7136\u540e\u5728\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u4e0b\u5bf9\u9f50\u6a21\u5f0f\u7279\u5b9a\u8f68\u8ff9\u3002\u5f15\u5165\u5373\u65f6\u6a21\u5f0f\u5904\u7406\u7b80\u5355\u67e5\u8be2\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6216\u5de5\u5177\u8c03\u7528\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316(APO)\uff0c\u5f3a\u5236\u8de8\u6a21\u5f0f\u81ea\u9002\u5e94\u91c7\u6837\u5e76\u5e94\u7528\u6210\u672c\u6b63\u5219\u5316\u5956\u52b1\u3002", "result": "\u572832B\u89c4\u6a21\u4e0a\uff0cA\u00b2FM\u5728BrowseComp\u3001AIME25\u548cHLE\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523013.4%\u300170.4%\u548c16.7%\uff0c\u521b\u4e0b\u53ef\u6bd4\u6a21\u578b\u7684\u65b0SOTA\uff0c\u5728\u4ee3\u7406\u3001\u63a8\u7406\u548c\u901a\u7528\u57fa\u51c6\u4e0a\u4e0e\u524d\u6cbfLLM\u7ade\u4e89\u3002\u81ea\u9002\u5e94\u6267\u884c\u6bcf\u4e2a\u6b63\u786e\u7b54\u6848\u6210\u672c\u4ec50.00487\u7f8e\u5143\uff0c\u76f8\u6bd4\u63a8\u7406\u6a21\u5f0f\u964d\u4f4e\u6210\u672c45.2%\uff0c\u76f8\u6bd4\u4ee3\u7406\u6a21\u5f0f\u964d\u4f4e\u6210\u672c33.5%\u3002", "conclusion": "A\u00b2FM\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u578b\u548c\u4ee3\u7406\u578bLLM\u7684\u5206\u88c2\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6210\u672c\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u591a\u529f\u80fd\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.13025", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.13025", "abs": "https://arxiv.org/abs/2510.13025", "authors": ["Xiaoyuan Cheng", "Wenxuan Yuan", "Yiming Yang", "Yuanzhao Zhang", "Sibo Cheng", "Yi He", "Zhuo Sun"], "title": "Information Shapes Koopman Representation", "comment": null, "summary": "The Koopman operator provides a powerful framework for modeling dynamical\nsystems and has attracted growing interest from the machine learning community.\nHowever, its infinite-dimensional nature makes identifying suitable\nfinite-dimensional subspaces challenging, especially for deep architectures. We\nargue that these difficulties come from suboptimal representation learning,\nwhere latent variables fail to balance expressivity and simplicity. This\ntension is closely related to the information bottleneck (IB) dilemma:\nconstructing compressed representations that are both compact and predictive.\nRethinking Koopman learning through this lens, we demonstrate that latent\nmutual information promotes simplicity, yet an overemphasis on simplicity may\ncause latent space to collapse onto a few dominant modes. In contrast,\nexpressiveness is sustained by the von Neumann entropy, which prevents such\ncollapse and encourages mode diversity. This insight leads us to propose an\ninformation-theoretic Lagrangian formulation that explicitly balances this\ntradeoff. Furthermore, we propose a new algorithm based on the Lagrangian\nformulation that encourages both simplicity and expressiveness, leading to a\nstable and interpretable Koopman representation. Beyond quantitative\nevaluations, we further visualize the learned manifolds under our\nrepresentations, observing empirical results consistent with our theoretical\npredictions. Finally, we validate our approach across a diverse range of\ndynamical systems, demonstrating improved performance over existing Koopman\nlearning methods. The implementation is publicly available at\nhttps://github.com/Wenxuan52/InformationKoopman.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684Koopman\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u8868\u793a\u7b80\u6d01\u6027\u548c\u8868\u8fbe\u80fd\u529b\u6765\u89e3\u51b3\u6df1\u5ea6\u67b6\u6784\u4e2dKoopman\u7b97\u5b50\u5b66\u4e60\u7684\u6311\u6218\u3002", "motivation": "Koopman\u7b97\u5b50\u7684\u65e0\u9650\u7ef4\u7279\u6027\u4f7f\u5f97\u5bfb\u627e\u5408\u9002\u7684\u6709\u9650\u7ef4\u5b50\u7a7a\u95f4\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u6df1\u5ea6\u67b6\u6784\u4e2d\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u56f0\u96be\u6e90\u4e8e\u6b21\u4f18\u7684\u8868\u793a\u5b66\u4e60\uff0c\u5176\u4e2d\u6f5c\u5728\u53d8\u91cf\u672a\u80fd\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u548c\u7b80\u6d01\u6027\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u8bba\u62c9\u683c\u6717\u65e5\u516c\u5f0f\uff0c\u660e\u786e\u5e73\u8861\u7b80\u6d01\u6027\uff08\u901a\u8fc7\u6f5c\u5728\u4e92\u4fe1\u606f\uff09\u548c\u8868\u8fbe\u80fd\u529b\uff08\u901a\u8fc7\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5\uff09\u4e4b\u95f4\u7684\u6743\u8861\u3002\u57fa\u4e8e\u6b64\u516c\u5f0f\u5f00\u53d1\u65b0\u7b97\u6cd5\uff0c\u4fc3\u8fdb\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684Koopman\u8868\u793a\u3002", "result": "\u5728\u591a\u79cd\u52a8\u529b\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709Koopman\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u53ef\u89c6\u5316\u5b66\u4e60\u5230\u7684\u6d41\u5f62\uff0c\u89c2\u5bdf\u7ed3\u679c\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\u3002", "conclusion": "\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\u91cd\u65b0\u601d\u8003Koopman\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u8861\u7b80\u6d01\u6027\u548c\u8868\u8fbe\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4ea7\u751f\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684Koopman\u8868\u793a\uff0c\u5728\u591a\u4e2a\u52a8\u529b\u7cfb\u7edf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.13030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13030", "abs": "https://arxiv.org/abs/2510.13030", "authors": ["Pouria Behnoudfar", "Charlotte Moser", "Marc Bocquet", "Sibo Cheng", "Nan Chen"], "title": "Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators", "comment": null, "summary": "Computer models are indispensable tools for understanding the Earth system.\nWhile high-resolution operational models have achieved many successes, they\nexhibit persistent biases, particularly in simulating extreme events and\nstatistical distributions. In contrast, coarse-grained idealized models isolate\nfundamental processes and can be precisely calibrated to excel in\ncharacterizing specific dynamical and statistical features. However, different\nmodels remain siloed by disciplinary boundaries. By leveraging the\ncomplementary strengths of models of varying complexity, we develop an\nexplainable AI framework for Earth system emulators. It bridges the model\nhierarchy through a reconfigured latent data assimilation technique, uniquely\nsuited to exploit the sparse output from the idealized models. The resulting\nbridging model inherits the high resolution and comprehensive variables of\noperational models while achieving global accuracy enhancements through\ntargeted improvements from idealized models. Crucially, the mechanism of AI\nprovides a clear rationale for these advancements, moving beyond black-box\ncorrection to physically insightful understanding in a computationally\nefficient framework that enables effective physics-assisted digital twins and\nuncertainty quantification. We demonstrate its power by significantly\ncorrecting biases in CMIP6 simulations of El Ni\\~no spatiotemporal patterns,\nleveraging statistically accurate idealized models. This work also highlights\nthe importance of pushing idealized model development and advancing\ncommunication between modeling communities.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u4e1a\u52a1\u6a21\u578b\u548c\u7406\u60f3\u5316\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u6784\u5efa\u5730\u7403\u7cfb\u7edf\u6a21\u62df\u5668\uff0c\u663e\u8457\u6539\u5584\u4e86CMIP6\u6a21\u62df\u4e2d\u5384\u5c14\u5c3c\u8bfa\u65f6\u7a7a\u6a21\u5f0f\u7684\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u9ad8\u5206\u8fa8\u7387\u4e1a\u52a1\u6a21\u578b\u5b58\u5728\u6301\u7eed\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u6781\u7aef\u4e8b\u4ef6\u548c\u7edf\u8ba1\u5206\u5e03\u65b9\u9762\uff0c\u800c\u7406\u60f3\u5316\u6a21\u578b\u867d\u7136\u80fd\u7cbe\u786e\u6821\u51c6\u7279\u5b9a\u7279\u5f81\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5b66\u79d1\u58c1\u5792\u3002", "method": "\u5229\u7528\u91cd\u6784\u7684\u6f5c\u5728\u6570\u636e\u540c\u5316\u6280\u672f\uff0c\u5c06\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u6a21\u578b\u5c42\u6b21\u8fde\u63a5\u8d77\u6765\uff0c\u7279\u522b\u9002\u5408\u5229\u7528\u7406\u60f3\u5316\u6a21\u578b\u7684\u7a00\u758f\u8f93\u51fa\uff0c\u6784\u5efa\u6865\u63a5\u6a21\u578b\u3002", "result": "\u6865\u63a5\u6a21\u578b\u7ee7\u627f\u4e86\u4e1a\u52a1\u6a21\u578b\u7684\u9ad8\u5206\u8fa8\u7387\u548c\u5168\u9762\u53d8\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u7406\u60f3\u5316\u6a21\u578b\u7684\u9488\u5bf9\u6027\u6539\u8fdb\u5b9e\u73b0\u4e86\u5168\u5c40\u7cbe\u5ea6\u63d0\u5347\uff0c\u663e\u8457\u7ea0\u6b63\u4e86CMIP6\u6a21\u62df\u4e2d\u5384\u5c14\u5c3c\u8bfa\u65f6\u7a7a\u6a21\u5f0f\u7684\u504f\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u8d85\u8d8a\u9ed1\u76d2\u6821\u6b63\u7684\u7269\u7406\u89e3\u91ca\uff0c\u8fd8\u5f3a\u8c03\u4e86\u63a8\u52a8\u7406\u60f3\u5316\u6a21\u578b\u53d1\u5c55\u548c\u4fc3\u8fdb\u5efa\u6a21\u793e\u533a\u95f4\u4ea4\u6d41\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6709\u6548\u7684\u7269\u7406\u8f85\u52a9\u6570\u5b57\u5b6a\u751f\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2510.13003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13003", "abs": "https://arxiv.org/abs/2510.13003", "authors": ["Yifeng Xiong", "Xiaohui Xie"], "title": "OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language\nmodels but suffers from catastrophic forgetting when learned updates interfere\nwith the dominant singular directions that encode essential pre-trained\nknowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically\ngrounded approach that prevents this interference through double-sided\northogonal projections. By decomposing frozen weights via SVD, OPLoRA\nconstrains LoRA updates to lie entirely within the orthogonal complement of the\ntop-$k$ singular subspace using projections $P_L = I - U_k U_k^\\top$ and $P_R =\nI - V_k V_k^\\top$. We prove that this construction exactly preserves the\ntop-$k$ singular triples, providing mathematical guarantees for knowledge\nretention. To quantify subspace interference, we introduce $\\rho_k$, a metric\nmeasuring update alignment with dominant directions. Extensive experiments\nacross commonsense reasoning, mathematics, and code generation demonstrate that\nOPLoRA significantly reduces forgetting while maintaining competitive\ntask-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal\nprojection as an effective mechanism for knowledge preservation in\nparameter-efficient fine-tuning.", "AI": {"tldr": "OPLoRA\u901a\u8fc7\u53cc\u9762\u6b63\u4ea4\u6295\u5f71\u9632\u6b62LoRA\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u7ea6\u675f\u66f4\u65b0\u4f4d\u4e8e\u524dk\u4e2a\u5947\u5f02\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e2d\uff0c\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "motivation": "LoRA\u5728\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b66\u4e60\u66f4\u65b0\u4f1a\u5e72\u6270\u7f16\u7801\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u4e3b\u5bfc\u5947\u5f02\u65b9\u5411\u3002", "method": "\u901a\u8fc7SVD\u5206\u89e3\u51bb\u7ed3\u6743\u91cd\uff0c\u4f7f\u7528\u6295\u5f71\u77e9\u9635P_L = I - U_k U_k^T\u548cP_R = I - V_k V_k^T\u7ea6\u675fLoRA\u66f4\u65b0\u5b8c\u5168\u4f4d\u4e8e\u524dk\u4e2a\u5947\u5f02\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eOPLoRA\u663e\u8457\u51cf\u5c11\u9057\u5fd8\uff0c\u540c\u65f6\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6b63\u4ea4\u6295\u5f71\u662f\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u77e5\u8bc6\u4fdd\u7559\u7684\u6709\u6548\u673a\u5236\uff0c\u4e3a\u77e5\u8bc6\u4fdd\u7559\u63d0\u4f9b\u6570\u5b66\u4fdd\u8bc1\u3002"}}
{"id": "2510.13079", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13079", "abs": "https://arxiv.org/abs/2510.13079", "authors": ["Chen Zheng", "Yuhang Cai", "Deyi Liu", "Jin Ma", "Yiyuan Ma", "Yuan Yang", "Jing Liu", "Yutao Zeng", "Xun Zhou", "Siyuan Qiao"], "title": "GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models", "comment": null, "summary": "Modern large language models leverage Mixture-of-Experts (MoE) architectures\nfor efficient scaling, but face a critical challenge: functionally similar\nexperts are often selected simultaneously, creating redundant computation and\nlimiting effective model capacity. Existing auxiliary balance loss methods\nimprove token distribution but fail to address the underlying expert diversity\nproblem. We introduce GatePro, a novel parameter-free method that directly\npromotes expert selection diversity. GatePro identifies the most similar expert\npairs and introduces localized competition mechanisms, preventing redundant\nexpert co-activation while maintaining natural expert specialization. Our\ncomprehensive evaluation demonstrates GatePro's effectiveness across model\nscales and benchmarks. Analysis demonstrates GatePro's ability to achieve\nenhanced expert diversity, where experts develop more distinct and\ncomplementary capabilities, avoiding functional redundancy. This approach can\nbe deployed hot-swappable during any training phase without additional\nlearnable parameters, offering a practical solution for improving MoE\neffectiveness.", "AI": {"tldr": "GatePro\u662f\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6700\u76f8\u4f3c\u7684\u4e13\u5bb6\u5bf9\u5e76\u5f15\u5165\u5c40\u90e8\u7ade\u4e89\u673a\u5236\uff0c\u76f4\u63a5\u63d0\u5347MoE\u67b6\u6784\u4e2d\u4e13\u5bb6\u9009\u62e9\u7684\u591a\u6837\u6027\uff0c\u907f\u514d\u529f\u80fd\u5197\u4f59\u3002", "motivation": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528MoE\u67b6\u6784\u8fdb\u884c\u9ad8\u6548\u6269\u5c55\uff0c\u4f46\u9762\u4e34\u529f\u80fd\u76f8\u4f3c\u4e13\u5bb6\u540c\u65f6\u88ab\u9009\u62e9\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6709\u6548\u6a21\u578b\u5bb9\u91cf\u53d7\u9650\u3002\u73b0\u6709\u5e73\u8861\u635f\u5931\u65b9\u6cd5\u6539\u5584\u4e86token\u5206\u5e03\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u6839\u672c\u7684\u4e13\u5bb6\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "GatePro\u8bc6\u522b\u6700\u76f8\u4f3c\u7684\u4e13\u5bb6\u5bf9\uff0c\u5f15\u5165\u5c40\u90e8\u7ade\u4e89\u673a\u5236\uff0c\u9632\u6b62\u5197\u4f59\u4e13\u5bb6\u540c\u65f6\u6fc0\u6d3b\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u53ef\u5728\u4efb\u4f55\u8bad\u7ec3\u9636\u6bb5\u70ed\u63d2\u62d4\u90e8\u7f72\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660eGatePro\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u6709\u6548\u3002\u5206\u6790\u663e\u793aGatePro\u80fd\u591f\u5b9e\u73b0\u589e\u5f3a\u7684\u4e13\u5bb6\u591a\u6837\u6027\uff0c\u4e13\u5bb6\u53d1\u5c55\u51fa\u66f4\u72ec\u7279\u548c\u4e92\u8865\u7684\u80fd\u529b\uff0c\u907f\u514d\u529f\u80fd\u5197\u4f59\u3002", "conclusion": "GatePro\u4e3a\u6539\u8fdbMoE\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u76f4\u63a5\u4fc3\u8fdb\u4e13\u5bb6\u9009\u62e9\u591a\u6837\u6027\u6765\u89e3\u51b3\u529f\u80fd\u5197\u4f59\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u5728\u4efb\u4f55\u8bad\u7ec3\u9636\u6bb5\u90e8\u7f72\u3002"}}
{"id": "2510.13115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13115", "abs": "https://arxiv.org/abs/2510.13115", "authors": ["Surya Tejaswi Yerramsetty", "Almas Fathimah"], "title": "Multi-Label Clinical Text Eligibility Classification and Summarization System", "comment": null, "summary": "Clinical trials are central to medical progress because they help improve\nunderstanding of human health and the healthcare system. They play a key role\nin discovering new ways to detect, prevent, or treat diseases, and it is\nessential that clinical trials include participants with appropriate and\ndiverse medical backgrounds. In this paper, we propose a system that leverages\nNatural Language Processing (NLP) and Large Language Models (LLMs) to automate\nmulti-label clinical text eligibility classification and summarization. The\nsystem combines feature extraction methods such as word embeddings (Word2Vec)\nand named entity recognition to identify relevant medical concepts, along with\ntraditional vectorization techniques such as count vectorization and TF-IDF\n(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF\nword embeddings that integrate both count-based and embedding-based strengths\nto capture term importance effectively. Multi-label classification using Random\nForest and SVM models is applied to categorize documents based on eligibility\ncriteria. Summarization techniques including TextRank, Luhn, and GPT-3 are\nevaluated to concisely summarize eligibility requirements. Evaluation with\nROUGE scores demonstrates the effectiveness of the proposed methods. This\nsystem shows potential for automating clinical trial eligibility assessment\nusing data-driven approaches, thereby improving research efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408NLP\u548cLLM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u591a\u6807\u7b7e\u4e34\u5e8a\u6587\u672c\u8d44\u683c\u5206\u7c7b\u548c\u6458\u8981\u751f\u6210\uff0c\u65e8\u5728\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u7684\u6548\u7387\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u5bf9\u533b\u5b66\u8fdb\u6b65\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u8981\u5305\u542b\u9002\u5f53\u548c\u591a\u6837\u5316\u7684\u53c2\u4e0e\u8005\u3002\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u6539\u8fdb\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u8bcd\u5d4c\u5165(Word2Vec)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u8ba1\u6570\u5411\u91cf\u5316\u548cTF-IDF\u7b49\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548cSVM\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u8bc4\u4f30TextRank\u3001Luhn\u548cGPT-3\u7b49\u6458\u8981\u6280\u672f\u3002", "result": "\u901a\u8fc7ROUGE\u5206\u6570\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u8bc4\u4f30\u7684\u53ef\u884c\u6027\uff0c\u6709\u671b\u63d0\u9ad8\u533b\u5b66\u7814\u7a76\u6548\u7387\u3002"}}
{"id": "2510.13232", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13232", "abs": "https://arxiv.org/abs/2510.13232", "authors": ["Inha Kang", "Youngsun Lim", "Seonho Lee", "Jiho Choi", "Junsuk Choe", "Hyunjung Shim"], "title": "What \"Not\" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging", "comment": "38 pages", "summary": "State-of-the-art vision-language models (VLMs) suffer from a critical failure\nin understanding negation, often referred to as affirmative bias. This\nlimitation is particularly severe in described object detection (DOD) tasks. To\naddress this, we propose two primary contributions: (1) a new dataset pipeline\nand (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a\ndataset constructed with a systematic chain-of-thought (CoT) and VQA-based\npipeline to generate high-quality, instance-grounded negation data. Second, we\npropose NegToMe, a novel text token merging module that directly tackles the\narchitectural cause of affirmative bias. NegToMe fundamentally addresses the\nstructural loss of negation cues in tokenization, grouping them with attributes\ninto coherent semantic phrases. It maintains correct polarity at the input\nlevel, enabling robust negation understanding even with limited data. For\ninstance, to prevent a model from treating the fragmented tokens \"not\" and\n\"girl\" as simply \"girl\", NegToMe binds them into a single token whose meaning\nis correctly distinguished from that of \"girl\" alone. This module is integrated\nwith a parameter-efficient and strategic LoRA fine-tuning approach. Our method\nsignificantly improves performance on challenging negation benchmarks with a\nlowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval\nand demonstrating generalization to SoTA VLMs. This work marks a crucial step\nforward in addressing negation understanding for real-world detection\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86CoVAND\u6570\u636e\u96c6\u548cNegToMe\u6a21\u5757\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5426\u5b9a\u7406\u89e3\u4e0a\u7684\u7f3a\u9677\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63cf\u8ff0\u6027\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5426\u5b9a\u7406\u89e3\u7684\u4e25\u91cd\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u63cf\u8ff0\u6027\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u80af\u5b9a\u6027\u504f\u89c1\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u68c0\u6d4b\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "1) \u5f00\u53d1CoVAND\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u548cVQA\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b9e\u4f8b\u63a5\u5730\u5426\u5b9a\u6570\u636e\uff1b2) \u63d0\u51faNegToMe\u6587\u672c\u6807\u8bb0\u5408\u5e76\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u5426\u5b9a\u8bcd\u4e0e\u5c5e\u6027\u8bcd\u7ec4\u5408\u6210\u8fde\u8d2f\u7684\u8bed\u4e49\u77ed\u8bed\u6765\u89e3\u51b3\u6807\u8bb0\u5316\u8fc7\u7a0b\u4e2d\u7684\u5426\u5b9a\u7ebf\u7d22\u4e22\u5931\u95ee\u9898\uff1b3) \u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684LoRA\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5728\u6311\u6218\u6027\u5426\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c06NMS-AP\u5728OVDEval\u4e0a\u63d0\u5347\u4e86\u9ad8\u8fbe+10.8\u5206\uff0c\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\uff0c\u5e76\u5728\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5c55\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5728\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u68c0\u6d4b\u5e94\u7528\u4e2d\u7684\u5426\u5b9a\u7406\u89e3\u95ee\u9898\u4e0a\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13361", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.13361", "abs": "https://arxiv.org/abs/2510.13361", "authors": ["Yisen Wang", "Yichuan Mo", "Hongjun Wang", "Junyi Li", "Zhouchen Lin"], "title": "Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training", "comment": null, "summary": "Despite the rapid progress of neural networks, they remain highly vulnerable\nto adversarial examples, for which adversarial training (AT) is currently the\nmost effective defense. While AT has been extensively studied, its practical\napplications expose two major limitations: natural accuracy tends to degrade\nsignificantly compared with standard training, and robustness does not transfer\nwell across attacks crafted under different norm constraints. Unlike prior\nworks that attempt to address only one issue within a single network, we\npropose to partition the overall generalization goal into multiple sub-tasks,\neach assigned to a dedicated base learner. By specializing in its designated\nobjective, each base learner quickly becomes an expert in its field. In the\nlater stages of training, we interpolate their parameters to form a\nknowledgeable global learner, while periodically redistributing the global\nparameters back to the base learners to prevent their optimization trajectories\nfrom drifting too far from the shared target. We term this framework Generalist\nand introduce three variants tailored to different application scenarios. Both\ntheoretical analysis and extensive experiments demonstrate that Generalist\nachieves lower generalization error and significantly alleviates the trade-off\nproblems compared with baseline methods. Our results suggest that Generalist\nprovides a promising step toward developing fully robust classifiers in the\nfuture.", "AI": {"tldr": "\u63d0\u51faGeneralist\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6cdb\u5316\u76ee\u6807\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u4e13\u95e8\u4f18\u5316\u7279\u5b9a\u76ee\u6807\uff0c\u7136\u540e\u53c2\u6570\u63d2\u503c\u5f62\u6210\u5168\u5c40\u5b66\u4e60\u5668\uff0c\u7f13\u89e3\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u81ea\u7136\u7cbe\u5ea6\u4e0b\u964d\u548c\u8de8\u653b\u51fb\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u81ea\u7136\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u8303\u6570\u7ea6\u675f\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u8fc1\u79fb\u80fd\u529b\u5dee\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u89e3\u51b3\u5176\u4e2d\u4e00\u4e2a\u95ee\u9898\uff0c\u4e14\u5c40\u9650\u4e8e\u5355\u4e00\u7f51\u7edc\u3002", "method": "\u5c06\u6574\u4f53\u6cdb\u5316\u76ee\u6807\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u4e13\u95e8\u4f18\u5316\u7279\u5b9a\u76ee\u6807\u3002\u8bad\u7ec3\u540e\u671f\u901a\u8fc7\u53c2\u6570\u63d2\u503c\u5f62\u6210\u5168\u5c40\u5b66\u4e60\u5668\uff0c\u5e76\u5468\u671f\u6027\u5730\u5c06\u5168\u5c40\u53c2\u6570\u91cd\u65b0\u5206\u914d\u7ed9\u57fa\u7840\u5b66\u4e60\u5668\u4ee5\u9632\u6b62\u4f18\u5316\u8f68\u8ff9\u504f\u79bb\u5171\u4eab\u76ee\u6807\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeneralist\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u6cdb\u5316\u8bef\u5dee\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u6743\u8861\u95ee\u9898\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Generalist\u4e3a\u5f00\u53d1\u5b8c\u5168\u9c81\u68d2\u7684\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u901a\u8fc7\u4e13\u4e1a\u5316\u5206\u5de5\u548c\u77e5\u8bc6\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u6297\u8bad\u7ec3\u7684\u5173\u952e\u5c40\u9650\u3002"}}
{"id": "2510.13385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13385", "abs": "https://arxiv.org/abs/2510.13385", "authors": ["Michael Vitali", "Pierre Pinson"], "title": "Prediction Markets with Intermittent Contributions", "comment": "Submitted to PSCC 2026", "summary": "Although both data availability and the demand for accurate forecasts are\nincreasing, collaboration between stakeholders is often constrained by data\nownership and competitive interests. In contrast to recent proposals within\ncooperative game-theoretical frameworks, we place ourselves in a more general\nframework, based on prediction markets. There, independent agents trade\nforecasts of uncertain future events in exchange for rewards. We introduce and\nanalyse a prediction market that (i) accounts for the historical performance of\nthe agents, (ii) adapts to time-varying conditions, while (iii) permitting\nagents to enter and exit the market at will. The proposed design employs robust\nregression models to learn the optimal forecasts' combination whilst handling\nmissing submissions. Moreover, we introduce a pay-off allocation mechanism that\nconsiders both in-sample and out-of-sample performance while satisfying several\ndesirable economic properties. Case-studies using simulated and real-world data\nallow demonstrating the effectiveness and adaptability of the proposed market\ndesign.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u5e02\u573a\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u5386\u53f2\u8868\u73b0\u3001\u9002\u5e94\u65f6\u53d8\u6761\u4ef6\u5e76\u5141\u8bb8\u81ea\u7531\u8fdb\u51fa\u7684\u65b9\u5f0f\uff0c\u89e3\u51b3\u6570\u636e\u6240\u6709\u6743\u548c\u7ade\u4e89\u5229\u76ca\u9650\u5236\u4e0b\u7684\u9884\u6d4b\u534f\u4f5c\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u53ef\u7528\u6027\u548c\u51c6\u786e\u9884\u6d4b\u9700\u6c42\u90fd\u5728\u589e\u957f\uff0c\u4f46\u6570\u636e\u6240\u6709\u6743\u548c\u7ade\u4e89\u5229\u76ca\u9650\u5236\u4e86\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u7684\u534f\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u534f\u4f5c\u673a\u5236\u3002", "method": "\u91c7\u7528\u9884\u6d4b\u5e02\u573a\u6846\u67b6\uff0c\u4f7f\u7528\u7a33\u5065\u56de\u5f52\u6a21\u578b\u5b66\u4e60\u6700\u4f18\u9884\u6d4b\u7ec4\u5408\u5e76\u5904\u7406\u7f3a\u5931\u63d0\u4ea4\uff0c\u5f15\u5165\u8003\u8651\u6837\u672c\u5185\u5916\u8868\u73b0\u7684\u6536\u76ca\u5206\u914d\u673a\u5236\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5e02\u573a\u8bbe\u8ba1\u5177\u6709\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u9884\u6d4b\u5e02\u573a\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u72ec\u7acb\u4ee3\u7406\u4eba\u5728\u6570\u636e\u6240\u6709\u6743\u548c\u7ade\u4e89\u5229\u76ca\u7ea6\u675f\u4e0b\u7684\u9884\u6d4b\u534f\u4f5c\uff0c\u540c\u65f6\u6ee1\u8db3\u91cd\u8981\u7684\u7ecf\u6d4e\u7279\u6027\u3002"}}
{"id": "2510.13499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13499", "abs": "https://arxiv.org/abs/2510.13499", "authors": ["Xiaozhe Li", "TianYi Lyu", "Siyi Yang", "Yuxi Gong", "Yizhao Yang", "Jinxuan Huang", "Ligao Zhang", "Zhuoyi Huang", "Qingwen Liu"], "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding", "comment": null, "summary": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u52a8\u6001\u5b9e\u65f6\u8bc4\u4f30\u57fa\u51c6\\bench\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u8005\u9886\u57df\u7684\u4eba\u7c7b\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u516c\u5171\u8ba8\u8bba\u5177\u6709\u4ea4\u7ec7\u51b2\u7a81\u7684\u89c2\u70b9\u3001\u4e0d\u540c\u5173\u5207\u3001\u76ee\u6807\u548c\u60c5\u611f\u503e\u5411\u7b49\u7279\u70b9\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30LLM\u5728\u8fd9\u79cd\u590d\u6742\u73af\u5883\u4e0b\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\\bench\u57fa\u51c6\uff0c\u652f\u6301\u5b9e\u65f6\u66f4\u65b0\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7b5b\u9009\u6d41\u7a0b\u9632\u6b62\u6570\u636e\u6c61\u67d3\uff0c\u662f\u76ee\u524d\u540c\u7c7b\u57fa\u51c6\u4e2d\u89c4\u6a21\u6700\u5927\u3001\u591a\u6837\u6027\u6700\u4e30\u5bcc\u7684\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u610f\u56fe\u7406\u89e3\u7684\u5927\u89c4\u6a21\u52a8\u6001\u8bc4\u4f30\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\\bench\u57fa\u51c6\u4e3a\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u8ba8\u8bba\u4e2d\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.13614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13614", "abs": "https://arxiv.org/abs/2510.13614", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo.", "AI": {"tldr": "MemoTime\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u57fa\u7840\u3001\u9012\u5f52\u63a8\u7406\u548c\u6301\u7eed\u7ecf\u9a8c\u5b66\u4e60\u6765\u589e\u5f3aLLM\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u65f6\u5e8f\u5fe0\u5b9e\u6027\u3001\u591a\u5b9e\u4f53\u65f6\u5e8f\u540c\u6b65\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u7684LLM\u63a8\u7406\u65b9\u6cd5\u5728\u4fdd\u6301\u591a\u8df3\u63a8\u7406\u7684\u65f6\u5e8f\u5fe0\u5b9e\u6027\u3001\u5b9e\u73b0\u591a\u5b9e\u4f53\u65f6\u5e8f\u540c\u6b65\u3001\u9002\u5e94\u591a\u6837\u5316\u65f6\u5e8f\u64cd\u4f5c\u7b26\u4ee5\u53ca\u91cd\u7528\u5148\u524d\u63a8\u7406\u7ecf\u9a8c\u7b49\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMemoTime\u6846\u67b6\uff0c\u5c06\u590d\u6742\u65f6\u5e8f\u95ee\u9898\u5206\u89e3\u4e3a\u65f6\u95f4\u6811\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9e\u73b0\u64cd\u4f5c\u7b26\u611f\u77e5\u63a8\u7406\uff0c\u91c7\u7528\u52a8\u6001\u8bc1\u636e\u68c0\u7d22\u5c42\u9009\u62e9\u64cd\u4f5c\u7b26\u7279\u5b9a\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u5e76\u5efa\u7acb\u81ea\u6f14\u8fdb\u7ecf\u9a8c\u8bb0\u5fc6\u5b58\u50a8\u5df2\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u5e8fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemoTime\u5b9e\u73b0\u4e86\u6574\u4f53\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe24.0%\uff0c\u5e76\u4f7f\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u4e0eGPT-4-Turbo\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "MemoTime\u901a\u8fc7\u7ed3\u6784\u5316\u65f6\u5e8f\u63a8\u7406\u548c\u6301\u7eed\u7ecf\u9a8c\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u65f6\u5e8f\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13567", "abs": "https://arxiv.org/abs/2510.13567", "authors": ["Omayma Moussadek", "Riccardo Salami", "Simone Calderara"], "title": "DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning", "comment": null, "summary": "Federated continual learning (FCL) enables models to learn new tasks across\nmultiple distributed clients, protecting privacy and without forgetting\npreviously acquired knowledge. However, current methods face challenges\nbalancing performance, privacy preservation, and communication efficiency. We\nintroduce a Distributed Online LoRA for Federated INcremental learning method\nDOLFIN, a novel approach combining Vision Transformers with low-rank adapters\ndesigned to efficiently and stably learn new tasks in federated environments.\nOur method leverages LoRA for minimal communication overhead and incorporates\nDualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on\nCIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet\nheterogeneity settings, DOLFIN consistently surpasses six strong baselines in\nfinal average accuracy while matching their memory footprint. Orthogonal\nlow-rank adapters offer an effective and scalable solution for\nprivacy-preserving continual learning in federated settings.", "AI": {"tldr": "\u63d0\u51faDOLFIN\u65b9\u6cd5\uff0c\u7ed3\u5408Vision Transformers\u548c\u4f4e\u79e9\u9002\u914d\u5668\uff0c\u5728\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u65b0\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u901a\u4fe1\u5f00\u9500\u548c\u9632\u6b62\u9057\u5fd8\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u5e73\u8861\u6027\u80fd\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668(LoRA)\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u7ed3\u5408DualGradient Projection Memory(DualGPM)\u9632\u6b62\u9057\u5fd8\uff0c\u5728\u8054\u90a6\u73af\u5883\u4e2d\u5b9e\u73b0\u5206\u5e03\u5f0f\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u5728CIFAR-100\u3001ImageNet-R\u3001ImageNet-A\u548cCUB-200\u6570\u636e\u96c6\u4e0a\uff0cDOLFIN\u5728\u4e24\u79cdDirichlet\u5f02\u6784\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u516d\u4e2a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6700\u7ec8\u5e73\u5747\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u6b63\u4ea4\u4f4e\u79e9\u9002\u914d\u5668\u4e3a\u8054\u90a6\u73af\u5883\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13721", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13721", "abs": "https://arxiv.org/abs/2510.13721", "authors": ["Run Luo", "Xiaobo Xia", "Lu Wang", "Longze Chen", "Renke Shan", "Jing Luo", "Min Yang", "Tat-Seng Chua"], "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching", "comment": null, "summary": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal\nretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal\nfoundation model that achieves unified modeling through discrete flow\nparadigms. By leveraging metric-induced probability paths and kinetic optimal\nvelocities, NExT-OMNI natively supports any-to-any understanding and generation\nwith enhanced response efficiency, while enabling broader application scenarios\nthrough concise unified representations rather than task-decoupled designs.\nTrained on large-scale interleaved text, image, video, and audio data,\nNExT-OMNI delivers competitive performance on multimodal generation and\nunderstanding benchmarks, while outperforming prior unified models in\nmulti-turn multimodal interaction and cross-modal retrieval, highlighting its\narchitectural advantages as a next-generation multimodal foundation model. To\nadvance further research, we release training details, data protocols, and\nopen-source both the code and model checkpoints.", "AI": {"tldr": "NExT-OMNI\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u6d41\u8303\u5f0f\u5b9e\u73b0\u7edf\u4e00\u5efa\u6a21\uff0c\u652f\u6301\u4efb\u610f\u6a21\u6001\u95f4\u7684\u7406\u89e3\u548c\u751f\u6210\uff0c\u5728\u591a\u8f6e\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u53d7\u81ea\u56de\u5f52\u67b6\u6784\u9650\u5236\uff0c\u65e0\u6cd5\u5e73\u8861\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u4e14\u6df7\u5408\u548c\u89e3\u8026\u7b56\u7565\u7684\u8bbe\u8ba1\u5197\u4f59\uff0c\u9650\u5236\u4e86\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u5ea6\u91cf\u8bf1\u5bfc\u6982\u7387\u8def\u5f84\u548c\u52a8\u529b\u5b66\u6700\u4f18\u901f\u5ea6\u7684\u79bb\u6563\u6d41\u8303\u5f0f\uff0c\u901a\u8fc7\u7b80\u6d01\u7edf\u4e00\u8868\u793a\u800c\u975e\u4efb\u52a1\u89e3\u8026\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4efb\u610f\u6a21\u6001\u95f4\u7684\u7406\u89e3\u548c\u751f\u6210\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4ea4\u9519\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u591a\u6a21\u6001\u751f\u6210\u548c\u7406\u89e3\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u8f6e\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7edf\u4e00\u6a21\u578b\u3002", "conclusion": "NExT-OMNI\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5177\u6709\u67b6\u6784\u4f18\u52bf\uff0c\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.13654", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13654", "abs": "https://arxiv.org/abs/2510.13654", "authors": ["Marcel Meyer", "Sascha Kaltenpoth", "Kevin Zalipski", "Oliver M\u00fcller"], "title": "Time Series Foundation Models: Benchmarking Challenges and Requirements", "comment": null, "summary": "Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TSFMs)\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u591a\u91cd\u6311\u6218\uff0c\u5305\u62ec\u57fa\u51c6\u6570\u636e\u96c6\u4ee3\u8868\u6027\u4e0d\u8db3\u3001\u7f3a\u4e4f\u65f6\u7a7a\u8bc4\u4f30\u3001\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u4ee5\u53ca\u5916\u90e8\u51b2\u51fb\u5bfc\u81f4\u7684\u6a21\u5f0f\u8bb0\u5fc6\u95ee\u9898\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u9700\u8981\u786e\u4fdd\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\uff0c\u907f\u514d\u91cd\u8e48\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5df2\u89c2\u5bdf\u5230\u7684\u9677\u9631\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u73b0\u6709TSFM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa\u6570\u636e\u5206\u533a\u6df7\u4e71\u3001\u57fa\u51c6\u6570\u636e\u96c6\u4ee3\u8868\u6027\u4e0d\u8db3\u3001\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u7b49\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u5e7f\u6cdb\u7684\u6570\u636e\u5206\u533a\u6df7\u6dc6\uff0c\u5bfc\u81f4\u6027\u80fd\u4f30\u8ba1\u81a8\u80c0\u548c\u5168\u5c40\u77e5\u8bc6\u5411\u5c40\u90e8\u65f6\u95f4\u5e8f\u5217\u7684\u9519\u8bef\u8f6c\u79fb\u3002", "conclusion": "\u547c\u5401\u7814\u7a76\u793e\u533a\u8bbe\u8ba1\u65b0\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5982\u57fa\u4e8e\u771f\u6b63\u6837\u672c\u5916\u672a\u6765\u6570\u636e\u7684\u8bc4\u4f30\uff0c\u4ee5\u4fdd\u969cTSFM\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.13656", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13656", "abs": "https://arxiv.org/abs/2510.13656", "authors": ["Priyobrata Mondal", "Faizanuddin Ansari", "Swagatam Das"], "title": "Rebalancing with Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced Classification", "comment": null, "summary": "The class imbalance problem refers to the insufficiency of data in certain\nclasses, which causes a classifier to be biased toward the majority class.\nDistribution calibration is a technique that seeks to estimate a more accurate\nclass distribution based on an observed or estimated one. To address this\nissue, we propose a distribution calibration-based method-Rebalancing with\nCalibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced\nClassification, which estimates the distribution parameters of the minority\nclasses using weighted parameters derived from a mixture of Gaussian components\nfrom both the majority and intermediate classes. An encoder-decoder network is\ntrained to preserve the structure of the imbalanced data and prevent\ndisentanglement. After training, feature vectors extracted from the encoder are\nused to generate synthetic samples through our distribution calibration\nstrategy. This approach effectively mitigates the overgeneralization problem\nthat arises when only the distribution of the majority class is used to\napproximate the minority class statistics. Instead, our method calibrates the\nparameters by leveraging the distribution of data points in neighboring\nregions. Experimental results demonstrate that the proposed method achieves\nsuperior classification performance compared to several baseline and\nstate-of-the-art techniques across a diverse range of image, text, and tabular\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u6821\u51c6\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u7c7b\u65b9\u6cd5RCS\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6570\u7c7b\u548c\u4e2d\u95f4\u7c7b\u7684\u9ad8\u65af\u6df7\u5408\u6210\u5206\u52a0\u6743\u53c2\u6570\u6765\u4f30\u8ba1\u5c11\u6570\u7c7b\u7684\u5206\u5e03\u53c2\u6570\uff0c\u6709\u6548\u7f13\u89e3\u8fc7\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u5bfc\u81f4\u5206\u7c7b\u5668\u504f\u5411\u591a\u6570\u7c7b\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f7f\u7528\u591a\u6570\u7c7b\u5206\u5e03\u6765\u8fd1\u4f3c\u5c11\u6570\u7c7b\u7edf\u8ba1\u4f1a\u4ea7\u751f\u8fc7\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u4fdd\u6301\u4e0d\u5e73\u8861\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u5e03\u6821\u51c6\u7b56\u7565\u4ece\u7f16\u7801\u5668\u63d0\u53d6\u7684\u7279\u5f81\u5411\u91cf\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u5229\u7528\u90bb\u8fd1\u533a\u57df\u6570\u636e\u70b9\u5206\u5e03\u6765\u6821\u51c6\u53c2\u6570\u3002", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u3001\u6587\u672c\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u591a\u4e2a\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u6280\u672f\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "RCS\u65b9\u6cd5\u901a\u8fc7\u5206\u5e03\u6821\u51c6\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.13762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13762", "abs": "https://arxiv.org/abs/2510.13762", "authors": ["Paolo Conti", "Mengwu Guo", "Attilio Frangi", "Andrea Manzoni"], "title": "Progressive multi-fidelity learning for physical system predictions", "comment": null, "summary": "Highly accurate datasets from numerical or physical experiments are often\nexpensive and time-consuming to acquire, posing a significant challenge for\napplications that require precise evaluations, potentially across multiple\nscenarios and in real-time. Even building sufficiently accurate surrogate\nmodels can be extremely challenging with limited high-fidelity data.\nConversely, less expensive, low-fidelity data can be computed more easily and\nencompass a broader range of scenarios. By leveraging multi-fidelity\ninformation, prediction capabilities of surrogates can be improved. However, in\npractical situations, data may be different in types, come from sources of\ndifferent modalities, and not be concurrently available, further complicating\nthe modeling process. To address these challenges, we introduce a progressive\nmulti-fidelity surrogate model. This model can sequentially incorporate diverse\ndata types using tailored encoders. Multi-fidelity regression from the encoded\ninputs to the target quantities of interest is then performed using neural\nnetworks. Input information progressively flows from lower to higher fidelity\nlevels through two sets of connections: concatenations among all the encoded\ninputs, and additive connections among the final outputs. This dual connection\nsystem enables the model to exploit correlations among different datasets while\nensuring that each level makes an additive correction to the previous level\nwithout altering it. This approach prevents performance degradation as new\ninput data are integrated into the model and automatically adapts predictions\nbased on the available inputs. We demonstrate the effectiveness of the approach\non numerical benchmarks and a real-world case study, showing that it reliably\nintegrates multi-modal data and provides accurate predictions, maintaining\nperformance when generalizing across time and parameter variations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6a21\u578b\uff0c\u80fd\u591f\u987a\u5e8f\u6574\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\uff0c\u901a\u8fc7\u5b9a\u5236\u7f16\u7801\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u591a\u4fdd\u771f\u5ea6\u56de\u5f52\uff0c\u5229\u7528\u53cc\u91cd\u8fde\u63a5\u7cfb\u7edf\u786e\u4fdd\u6027\u80fd\u4e0d\u9000\u5316\u3002", "motivation": "\u9ad8\u4fdd\u771f\u5ea6\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800c\u4f4e\u4fdd\u771f\u5ea6\u6570\u636e\u66f4\u5bb9\u6613\u83b7\u5f97\u4f46\u7cbe\u5ea6\u8f83\u4f4e\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u53ef\u80fd\u7c7b\u578b\u4e0d\u540c\u3001\u6765\u6e90\u6a21\u6001\u5404\u5f02\u4e14\u975e\u540c\u65f6\u53ef\u7528\uff0c\u8fd9\u7ed9\u5efa\u6a21\u5e26\u6765\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6e10\u8fdb\u5f0f\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5b9a\u5236\u7f16\u7801\u5668\u987a\u5e8f\u6574\u5408\u4e0d\u540c\u7c7b\u578b\u6570\u636e\uff0c\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u591a\u4fdd\u771f\u5ea6\u56de\u5f52\u3002\u8f93\u5165\u4fe1\u606f\u901a\u8fc7\u4e24\u79cd\u8fde\u63a5\u65b9\u5f0f\u4ece\u4f4e\u5230\u9ad8\u4fdd\u771f\u5ea6\u6d41\u52a8\uff1a\u7f16\u7801\u8f93\u5165\u95f4\u7684\u62fc\u63a5\u8fde\u63a5\u548c\u6700\u7ec8\u8f93\u51fa\u95f4\u7684\u52a0\u6cd5\u8fde\u63a5\u3002", "result": "\u5728\u6570\u503c\u57fa\u51c6\u548c\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u4e2d\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u53ef\u9760\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\uff0c\u5728\u65f6\u95f4\u548c\u53c2\u6570\u53d8\u5316\u6cdb\u5316\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u591a\u4fdd\u771f\u5ea6\u4fe1\u606f\uff0c\u9632\u6b62\u65b0\u6570\u636e\u96c6\u6210\u65f6\u6027\u80fd\u9000\u5316\uff0c\u5e76\u57fa\u4e8e\u53ef\u7528\u8f93\u5165\u81ea\u52a8\u8c03\u6574\u9884\u6d4b\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
