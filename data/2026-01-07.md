<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis](https://arxiv.org/abs/2601.02447)
*Bennet Kahrs,Julia Andresen,Fenja Falta,Monty Santarossa,Heinz Handels,Timo Kepp*

Main category: cs.CV

TL;DR: 提出两种基于隐式神经表示（INR）的框架，用于处理各向异性的视网膜OCT图像，实现密集3D分析和分辨率无关的视网膜图谱


<details>
  <summary>Details</summary>
Motivation: 临床视网膜OCT成像通常采用大切片间距，导致高度各向异性的图像和稀疏扫描。现有2D方法存在相邻B扫描结果不一致的问题，且卷积神经网络受限于训练数据分辨率，无法适应不同成像协议

Method: 提出两种基于INR的框架：1）通过整合en-face模态信息进行B扫描间插值，保留结构信息；2）创建分辨率无关的视网膜图谱，实现通用分析。两种方法都利用可泛化的INR，通过群体训练改进视网膜形状表示

Result: 开发出分辨率独立的框架，能够处理大B扫描间距的OCT图像，为视网膜结构和病理的容积评估开辟可能性

Conclusion: 基于INR的方法能够有效处理各向异性OCT数据，实现密集3D分析，为临床稀疏扫描视网膜图像的容积评估提供了新工具

Abstract: Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.

</details>


### [2] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

TL;DR: 本文提出了MovieRecapsQA，一个基于电影解说视频的开源多模态视频问答基准，包含约8.2K个问题-答案对，支持无参考评估，用于测试模型整合视觉和对话线索的能力。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA基准难以捕捉真实视频（如电影）所需的多模态推理能力，且大多不是开放式问答。需要一个新的基准来评估模型整合视觉和文本线索回答复杂问题的能力。

Method: 利用YouTube电影解说视频创建基准，通过解说摘要生成约8.2K个与电影字幕对齐的QA对，提供验证答案所需的"事实"信息，支持无参考评估。基准包含不同长度视频片段和按模态/类型分类的问题。

Result: 评估了7个最先进的多模态大模型，发现：1）纯视觉问题最具挑战性；2）模型倾向于依赖文本输入；3）从视频内容提取准确事实信息仍然困难；4）专有和开源模型在视频依赖问题上表现相当。

Conclusion: MovieRecapsQA是首个提供显式文本上下文的开源VideoQA基准，揭示了当前多模态模型在整合视觉和文本线索方面的局限性，为未来研究提供了细粒度分析工具。

Abstract: Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


### [3] [TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors](https://arxiv.org/abs/2601.02908)
*Wei-Yuan Cheng,Kai-Po Chang,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出TA-Prompting方法，通过时间锚点增强VideoLLMs，提高密集视频描述中事件边界定位的准确性，并引入事件连贯性采样策略优化多事件描述生成。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs在未修剪视频中难以准确定位事件边界，导致生成的描述缺乏时间基础。需要改进视频语言模型在密集视频描述任务中的时间感知能力。

Method: 提出TA-Prompting方法：1) 时间锚点学习精确定位事件边界；2) 提示VideoLLMs进行时间感知的视频事件理解；3) 推理时引入事件连贯性采样策略，选择跨时间事件连贯且与视频跨模态相似的事件描述。

Result: 在基准数据集上的实验表明，TA-Prompting优于现有最先进的VideoLLMs，在密集视频描述、时刻检索和TemporalQA等时间理解任务上取得优越性能。

Conclusion: TA-Prompting通过时间锚点和事件连贯性采样策略有效提升了VideoLLMs在密集视频描述中的时间定位能力和描述质量，为视频事件理解提供了新方法。

Abstract: Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.

</details>


### [4] [Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning](https://arxiv.org/abs/2601.02918)
*Guoqiang Liang,Jianyi Wang,Zhonghua Wu,Shangchen Zhou*

Main category: cs.CV

TL;DR: Zoom-IQA是一个基于视觉语言模型的图像质量评估方法，通过模拟人类认知行为（不确定性感知、区域推理和迭代优化）来同时生成质量描述和分数，相比现有方法具有更好的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估方法要么只预测数值分数而不提供解释，要么提供低层次描述但缺乏精确分数。虽然基于推理的视觉语言模型有潜力同时生成描述和分数，但现有VLM方法在整合视觉和文本线索方面能力有限，导致推理不可靠。

Method: 提出Zoom-IQA模型，模拟三个关键认知行为：不确定性感知、区域推理和迭代优化。采用两阶段训练流程：1) 在Grounded-Rationale-IQA数据集上进行监督微调，让模型将评估基于关键区域；2) 强化学习进行动态策略探索，使用KL-Coverage正则化防止推理和评分多样性崩溃，并通过渐进重采样策略缓解标注偏差。

Result: 大量实验表明Zoom-IQA在鲁棒性、可解释性和泛化能力方面都有提升。在图像修复等下游任务中的应用进一步证明了Zoom-IQA的有效性。

Conclusion: Zoom-IQA通过模拟人类认知过程，显著提升了基于视觉语言模型的图像质量评估性能，实现了更可靠的推理和更全面的质量分析。

Abstract: Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth](https://arxiv.org/abs/2601.02609)
*Arjun S. Nair*

Main category: cs.LG

TL;DR: Chronicals是一个开源训练框架，通过四种优化技术实现3.51倍加速：融合Triton内核减少75%内存流量、Cut Cross-Entropy将logit内存从5GB降至135MB、LoRA+使用16倍差异学习率、序列打包恢复60-75%计算浪费。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调受限于内存瓶颈：7B参数模型需要84GB内存（权重14GB、梯度14GB、优化器状态56GB），超过A100-40GB的容量。现有训练框架如Unsloth存在效率问题，甚至存在梯度为零的虚假训练情况。

Method: 1) 融合Triton内核：通过RMSNorm(7x)、SwiGLU(5x)、QK-RoPE(2.3x)融合减少75%内存流量；2) Cut Cross-Entropy：通过在线softmax计算将logit内存从5GB降至135MB；3) LoRA+：理论推导出适配器矩阵间16倍差异学习率；4) Best-Fit Decreasing序列打包：恢复60-75%因填充浪费的计算。

Result: 在Qwen2.5-0.5B和A100-40GB上，Chronicals全微调达到41,184 tokens/秒，比Unsloth的11,736 tokens/秒快3.51倍；LoRA rank 32达到11,699 tokens/秒，比Unsloth MAX的2,857 tokens/秒快4.10倍。发现Unsloth报告的46,000 tokens/秒基准测试梯度为零，模型未真正训练。

Conclusion: Chronicals通过四种协同优化显著提升大语言模型微调效率，提供完整的数学基础证明，包括在线softmax正确性证明、FlashAttention IO复杂度边界、LoRA+学习率推导和装箱问题近似保证。所有实现、基准测试和证明均已开源。

Abstract: Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding.
  On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training.
  We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.

</details>


### [6] [Stratified Hazard Sampling: Minimal-Variance Event Scheduling for CTMC/DTMC Discrete Diffusion and Flow Models](https://arxiv.org/abs/2601.02799)
*Seunghwan Jang,SooJean Han*

Main category: cs.LG

TL;DR: 提出分层危险采样(SHS)，一种用于离散生成模型的推理方法，通过分层累积危险值来减少编辑方差，解决传统独立伯努利采样导致的欠编辑和过编辑问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于CTMC/DTMC的离散生成模型在推理时使用独立伯努利采样决定token是否跳转，导致编辑次数和时间方差过大，产生欠编辑（残留噪声）和过编辑（不必要替换）等失败模式，降低可重复性。

Method: 提出分层危险采样(SHS)：将每个token的编辑建模为累积危险（CTMC）或累积跳转质量（DTMC）驱动的事件，通过分层累积量放置编辑事件——每个位置使用单一随机相位，当累积危险跨越单位间隔阈值时token跳转。还提出相位分配变体用于黑名单式词汇约束，优先在高风险位置早期编辑以减轻后期掩码伪影。

Result: SHS在保持期望跳转次数的同时，达到无偏整数估计器的最小可能方差（有界于1/4），不改变每次跳转的目标采样从而保留多模态性。该方法可作为即插即用、无需超参数的推理原则应用于任何具有停留vs替换分解的采样器。

Conclusion: 分层危险采样(SHS)通过减少编辑方差显著改善了离散生成模型的推理质量，解决了传统独立采样方法的固有问题，同时保持多模态性和期望编辑次数，为约束生成提供了有效变体。

Abstract: CTMC/DTMC-based discrete generative models, including uniform-noise discrete diffusion (e.g., D3PM/CTDD) and discrete flow matching, enable non-autoregressive sequence generation by repeatedly replacing tokens through a time-inhomogeneous Markov process. Inference is typically implemented with step-based simulation: each token decides to jump via independent Bernoulli (or categorical) draws at every discretization step. Under uniform-noise initialization, where self-correction requires multiple edits per position, these independent decisions induce substantial variance in both the number and timing of edits, leading to characteristic failure modes such as under-editing (residual noise) or over-editing (cascading unnecessary substitutions), decreasing reproducibility.
  We propose Stratified Hazard Sampling (SHS), a drop-in and hyperparameter-free inference principle for any sampler that admits a stay-vs.-replace decomposition. SHS models per-token edits as events driven by cumulative hazard (CTMC) or cumulative jump mass (DTMC) and places events by stratifying this cumulative quantity: with a single random phase per position, a token jumps whenever its accumulated hazard crosses unit-spaced thresholds. This preserves the expected number of jumps while achieving the minimum possible variance among unbiased integer estimators (bounded by 1/4), without altering per-jump destination sampling and thus retaining multimodality. We also introduce a phase-allocation variant for blacklist-style lexical constraints that prioritizes early edits at high-risk positions to mitigate late-masking artifacts.

</details>


### [7] [Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction](https://arxiv.org/abs/2601.02884)
*Hana Yahia,Bruno Figliuzzi,Florent Di Meglio,Laurent Gerbaud,Stephane Menand,Mohamed Mahjoub*

Main category: cs.LG

TL;DR: 该研究比较了钻井时间序列数据中的领域泛化技术，用于预测连续的粘滑指数（SSI）。ADG和IRM模型分别比基线模型性能提升10%和8%，其中ADG模型表现最佳，严重事件检测率从20%提升至60%。


<details>
  <summary>Details</summary>
Motivation: 开发能够在不同钻井井场间泛化的鲁棒回归模型，预测关键的粘滑指数（SSI），以评估钻头的扭转振动，解决传统模型在新井场性能下降的问题。

Method: 使用60秒标记的1Hz地面钻井数据序列训练回归模型，采用网格搜索优化超参数。比较了对抗领域泛化（ADG）、不变风险最小化（IRM）和基线模型，并评估了迁移学习（TL）的效果。模型在训练井场以外的井场进行测试。

Result: ADG和IRM模型分别比基线模型性能提升10%和8%。严重事件检测率从基线模型的20%大幅提升至60%。ADG模型略优于IRM模型，迁移学习进一步提升了预训练模型的性能。

Conclusion: 领域泛化方法在钻井应用中具有显著潜力，ADG是最有效的方法，能够显著提升模型在新井场的泛化能力和严重事件检测能力，迁移学习可进一步优化性能。

Abstract: This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.

</details>


### [8] [From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures](https://arxiv.org/abs/2601.02997)
*Waleed Khalid,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: LLM通过闭环合成框架自主设计神经网络架构，经过22轮微调后，模型能够生成高性能、结构新颖的卷积网络，验证了LLM可以通过执行反馈学习经验性架构先验。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在程序合成方面表现出色，但其在神经网络架构设计方面的自主能力——平衡语法可靠性、性能和结构新颖性——尚未得到充分探索。研究者希望探索LLM是否能够通过执行反馈学习经验性架构先验，超越训练数据限制。

Method: 将代码导向的LLM置于闭环合成框架中，经过22轮监督微调循环。模型生成PyTorch卷积网络，通过验证、低保真性能信号（单轮准确率）评估，并使用MinHash-Jaccard准则过滤结构冗余。高性能新颖架构被转换为提示-代码对，通过参数高效的LoRA适配进行迭代微调，从LEMUR数据集初始化。

Result: 模型内化了经验性架构先验，成为强大的生成器。有效生成率稳定在50.6%（峰值74.5%），平均首轮准确率从28.06%提升至50.99%，超过40%准确率的候选比例从2.04%增长到96.81%。模型生成了455个原始语料中不存在的高性能架构。

Conclusion: 通过将代码合成与执行反馈相结合，这项工作为将随机生成器转化为自主、性能驱动的神经网络设计器提供了可扩展的蓝图，证明LLM能够内化经验性、非文本奖励来超越其训练数据。

Abstract: Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.

</details>


### [9] [Real-Time Adaptive Anomaly Detection in Industrial IoT Environments](https://arxiv.org/abs/2601.03085)
*Mahsa Raeiszadeh,Amin Ebrahimzadeh,Roch H. Glitho,Johan Eker,Raquel A. F. Mini*

Main category: cs.LG

TL;DR: 提出一种用于工业物联网流数据异常检测的自适应方法，结合多源预测模型和概念漂移适应技术，在准确性和可扩展性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 下一代网络需要自动化异常检测系统来处理多维异构数据，特别是在工业物联网环境中，实时异常检测对于预防故障至关重要。现有方法在处理工业物联网多维数据流的复杂性和动态性方面存在不足。

Method: 提出自适应异常检测算法，将预测模型与新颖的概念漂移适应方法相结合，用于处理工业物联网流数据。该方法利用多源预测模型来应对数据复杂性，并通过漂移适应机制处理数据动态变化。

Result: 通过轨迹驱动评估表明，该方法在AUC指标上达到89.71%的准确率，优于现有最先进的异常检测方法，同时满足效率和可扩展性要求。

Conclusion: 该方法为工业物联网流数据提供了一种准确、高效且可扩展的异常检测解决方案，能够有效处理多维异构数据的复杂性和动态变化。

Abstract: To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today's industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.

</details>


### [10] [PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback](https://arxiv.org/abs/2601.03149)
*Dehao Yuan,Tyler Farnan,Stefan Tesliuc,Doron L Bergman,Yulun Wu,Xiaoyu Liu,Minghui Liu,James Montgomery,Nam H Nguyen,C. Bayan Bruss,Furong Huang*

Main category: cs.LG

TL;DR: PersonaLedger：基于LLM和规则引擎的合成金融交易数据生成系统，在保护隐私的同时实现行为多样性和逻辑正确性


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规限制了真实交易数据的获取，阻碍了金融AI的开放研究。现有合成数据生成方法无法同时实现行为多样性和逻辑正确性：基于规则的方法缺乏人类行为的丰富性，基于学习的方法（如GAN）经常违反金融约束且仍需私有数据训练。

Method: 提出PersonaLedger生成引擎：1）使用基于丰富用户角色的大语言模型生成多样化交易流；2）结合专家可配置的程序化引擎确保正确性。LLM和引擎形成闭环交互：每次事件后，引擎更新用户状态、强制执行金融规则，并返回上下文感知的"nextprompt"引导LLM生成可行的下一步行动。

Result: 创建了包含23,000个用户的3,000万笔交易的公开数据集，以及包含流动性不足分类和身份盗窃分割两个任务的基准测试套件。PersonaLedger提供了现实、隐私保护的资源，支持预测和异常检测模型的严格评估。

Conclusion: PersonaLedger为社区提供了丰富、现实且隐私保护的资源（包含代码、规则和生成日志），加速金融AI创新并支持严格、可复现的评估。

Abstract: Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.

</details>


### [11] [Dynamic Hyperparameter Importance for Efficient Multi-Objective Optimization](https://arxiv.org/abs/2601.03166)
*Daphne Theodorakopoulos,Marcel Wever,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出一种动态优化方法，根据目标权衡动态调整超参数重要性，加速收敛并提升帕累托前沿质量


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化方法通常将所有超参数视为同等重要，忽略了超参数重要性会随目标权衡变化而显著不同，这限制了优化效率

Method: 集成HyperSHAP计算超参数重要性，利用ParEGO算法产生的目标权重动态调整配置空间，固定不重要超参数以聚焦重要超参数搜索

Result: 在PyMOO和YAHPO-Gym的多样化任务上验证，相比基线方法在收敛速度和帕累托前沿质量方面均有改进

Conclusion: 动态考虑超参数重要性的优化方法能有效加速多目标优化收敛并产生更好的解决方案

Abstract: Choosing a suitable ML model is a complex task that can depend on several objectives, e.g., accuracy, model size, fairness, inference time, or energy consumption. In practice, this requires trading off multiple, often competing, objectives through multi-objective optimization (MOO). However, existing MOO methods typically treat all hyperparameters as equally important, overlooking that hyperparameter importance (HPI) can vary significantly depending on the trade-off between objectives. We propose a novel dynamic optimization approach that prioritizes the most influential hyperparameters based on varying objective trade-offs during the search process, which accelerates empirical convergence and leads to better solutions. Building on prior work on HPI for MOO post-analysis, we now integrate HPI, calculated with HyperSHAP, into the optimization. For this, we leverage the objective weightings naturally produced by the MOO algorithm ParEGO and adapt the configuration space by fixing the unimportant hyperparameters, allowing the search to focus on the important ones. Eventually, we validate our method with diverse tasks from PyMOO and YAHPO-Gym. Empirical results demonstrate improvements in convergence speed and Pareto front quality compared to baselines.

</details>


### [12] [PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters](https://arxiv.org/abs/2601.03237)
*Javier Salazar Cavazos*

Main category: cs.LG

TL;DR: PET-TURTLE改进了TURTLE深度聚类算法，通过引入幂律先验处理不平衡数据分布，并加入稀疏logits简化搜索空间，提高了不平衡和平衡数据集的聚类精度。


<details>
  <summary>Details</summary>
Motivation: TURTLE算法假设数据簇是平衡的，但在实际不平衡数据分布下会产生不理想的超平面，导致更高的聚类错误。需要改进算法以处理不平衡数据。

Method: 1) 通过幂律先验推广成本函数以处理不平衡数据分布；2) 在标签过程中引入稀疏logits，优化更简单的搜索空间。

Result: 在合成和真实数据上的实验表明，PET-TURTLE提高了不平衡数据源的准确性，防止对少数簇的过度预测，并增强了整体聚类性能。

Conclusion: PET-TURTLE成功解决了TURTLE在处理不平衡数据时的局限性，通过幂律先验和稀疏logits改进，在平衡和不平衡数据集上都获得了更好的聚类性能。

Abstract: Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs](https://arxiv.org/abs/2601.02818)
*Muzhen Zhang,Yujie Cheng,Zhanxiang Lei*

Main category: cs.AI

TL;DR: 提出量子增强的长短期记忆注意力模型(QLSTMA)，首次将变分量子电路融入循环单元，显著提升储层渗透率预测精度，8-qubit独立门结构比传统LSTMA降低MAE 19%、RMSE 20%。


<details>
  <summary>Details</summary>
Motivation: 储层参数（特别是渗透率）的空间预测对油气勘探开发至关重要，但渗透率变化范围大、变异性高，现有方法难以提供可靠预测。量子计算中的纠缠和叠加原理有望提升复杂地质参数的预测能力。

Method: 提出QLSTMA模型，将变分量子电路(VQC)集成到循环单元中。设计了两种量子化结构：共享门结构(QLSTMA-SG)和独立门结构(QLSTMA-IG)，研究量子结构配置和量子比特数对模型性能的影响。

Result: 8-qubit QLSTMA-IG模型显著优于传统LSTMA，MAE降低19%，RMSE降低20%，在复杂测井数据区域表现尤为突出。增加量子比特数可进一步提高精度，尽管目前依赖经典模拟。

Conclusion: 量子-经典混合神经网络在储层预测中具有巨大潜力，为未来在真实量子硬件上部署此类模型建立了基础框架，可扩展到石油工程和地球科学的更广泛应用。

Abstract: Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference](https://arxiv.org/abs/2601.02569)
*Hossein Rajabzadeh,Maryam Dialameh,Chul B. Park,Il-Min Kim,Hyock Ju Kwon*

Main category: cs.CL

TL;DR: LoRA-Drop：通过时间计算调度加速LLM解码，在大多数解码步骤中重用前一个token的隐藏状态并应用低秩LoRA校正，定期刷新执行完整模型，无需路由网络，兼容标准KV缓存。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型受限于顺序解码，每个新token通常需要执行所有transformer层。现有动态深度和层跳过方法虽然能减少成本，但依赖辅助路由机制或在跳过层未补偿时导致精度下降。

Method: 提出LoRA-Drop框架，对固定中间层子集应用时间计算调度：大多数解码步骤重用前一个token隐藏状态并应用低秩LoRA校正，定期刷新步骤执行完整模型防止漂移。无需路由网络，兼容标准KV缓存，可在LoRA步骤中跳过KV更新减少KV缓存占用。

Result: 在LLaMA2-7B、LLaMA3-8B、Qwen2.5-7B和Qwen2.5-14B上，实现高达2.6倍解码加速和45-55% KV缓存减少，同时保持与基线精度相差在0.5个百分点内。在推理、代码生成和长上下文/多语言基准测试中识别出保持质量同时提供显著效率提升的调度配置安全区。

Conclusion: LoRA-Drop为LLM中的自适应容量推理提供了一条简单路径，通过时间计算调度实现高效解码，无需复杂路由机制，在保持精度的同时显著提升效率。

Abstract: Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \textbf{LLaMA2-7B}, \textbf{LLaMA3-8B}, \textbf{Qwen2.5-7B}, and \textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \textbf{2.6$\times$ faster decoding} and \textbf{45--55\% KV-cache reduction} while staying within \textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.

</details>


### [15] [DataParasite Enables Scalable and Repurposable Online Data Curation](https://arxiv.org/abs/2601.02578)
*Mengyi Sun*

Main category: cs.CL

TL;DR: DataParasite是一个开源、模块化的在线数据收集管道，可将表格数据整理任务分解为独立的实体级搜索，通过轻量级配置文件定义，使用共享的Python脚本执行，显著降低计算社会科学中的数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 计算社会科学中的许多研究依赖于从异构在线来源组装的数据库，这一过程通常劳动密集、成本高昂且难以复现。现有的大语言模型代理搜索系统往往不透明、不灵活或不适合科学数据整理。

Method: DataParasite将表格整理任务分解为独立的实体级搜索，通过轻量级配置文件定义，使用共享的任务无关Python脚本执行。管道可通过自然语言指令重新用于新任务，包括没有预定义实体列表的任务。

Result: 在多个计算社会科学经典任务（教师招聘历史、精英死亡事件、政治生涯轨迹）上评估，DataParasite实现了高准确率，同时将数据收集成本相对于人工整理降低了一个数量级。

Conclusion: DataParasite通过降低在线数据组装的技术和劳动力障碍，为计算社会科学及其他领域提供了可扩展、透明和可重复使用的数据整理实用基础。

Abstract: Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.

</details>


### [16] [When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark](https://arxiv.org/abs/2601.02663)
*Subha Ghoshal,Ali Al-Bustami*

Main category: cs.CL

TL;DR: 研究比较了大型语言模型在推理时使用规划与外部工具的效果，发现在事件问答任务中工具增强能提升准确性但大幅增加延迟，而在说服性回复生成中单次提示效果最好，强调需要根据任务特性权衡模型大小与工具复杂度。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型越来越依赖推理时规划和外部工具来改进推理能力，但缺乏对这些方法在实际任务中效果的系统评估，特别是在准确性、延迟和成本方面的权衡。

Method: 使用LangChain和LangGraph框架，在两个真实世界任务上进行比较：事件中心问答（Event-QA）和Reddit ChangeMyView说服性回复生成（CMV）。对比单次提示基线与配备任务特定工具（DBpedia SPARQL/查找/模式探索、维基百科检索、主题网络搜索）的规划-执行-重新规划智能体。评估GPT-4o和GPT-4o-mini在相同工作流程下的表现，报告准确性和端到端延迟。

Result: 在Event-QA任务中，最佳工具增强配置显著提升准确性（GPT-4o从47.5%提升到67.5%），但延迟大幅增加（从约8秒增加到约317秒）。在CMV任务中，单次提示效果最好（GPT-4o-mini达到75%准确率，约6秒延迟），而规划+搜索大幅增加延迟但未带来一致的准确性提升。复杂的多工具编排暴露了较小模型的退化问题。

Conclusion: 研究结果表明，需要根据具体任务特性和成本考虑，谨慎选择模型大小和智能体/工具复杂度。工具增强在某些任务中能提升准确性但代价高昂，而在其他任务中简单方法可能更优，强调了任务特定优化的重要性。

Abstract: Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan-execute-replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\% $\rightarrow$ 67.5\% for GPT-4o) while increasing latency by orders of magnitude ($\sim$8s $\rightarrow$ $\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\% at $\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.

</details>


### [17] [Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking](https://arxiv.org/abs/2601.02669)
*Hongzhan Lin,Zixin Chen,Zhiqi Shen,Ziyang Luo,Zhen Ye,Jing Ma,Tat-Seng Chua,Guandong Xu*

Main category: cs.CL

TL;DR: FactArena：一个自动化、竞技场式的评估框架，用于全面评估LLM在完整事实核查流程中的表现，包括声明提取、证据检索和验证，超越了传统的静态声明验证评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注声明验证，忽略了完整的事实核查工作流程（包括声明提取和证据检索）。这种狭窄的焦点使得当前基准无法揭示现代LLM的系统性推理失败、事实盲点和鲁棒性限制。

Method: FactArena包含三个关键组件：(1) LLM驱动的事实核查流程，标准化声明分解、工具增强的证据检索和基于理由的裁决预测；(2) 竞技场式判断机制，基于统一参考指南进行无偏见的成对比较；(3) 竞技场驱动的声明演化模块，自适应生成更具挑战性的声明以测试LLM的事实鲁棒性。

Result: 在16个最先进的LLM（涵盖7个模型系列）上，FactArena产生了稳定且可解释的排名。分析显示静态声明验证准确性与端到端事实核查能力之间存在显著差异，突显了全面评估的必要性。

Conclusion: FactArena提供了一个可扩展且可信赖的范式，用于诊断LLM的事实推理能力，指导未来模型开发，并推进LLM在安全关键的事实核查应用中的可靠部署。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.

</details>


### [18] [Extracting books from production language models](https://arxiv.org/abs/2601.02671)
*Ahmed Ahmed,A. Feder Cooper,Sanmi Koyejo,Percy Liang*

Main category: cs.CL

TL;DR: 研究发现生产级LLMs（如Claude、GPT-4、Gemini、Grok）仍存在训练数据提取风险，即使有安全防护措施，部分模型可提取大量受版权保护的文本内容。


<details>
  <summary>Details</summary>
Motivation: 解决关于LLMs是否记忆训练数据以及能否提取受版权内容的法律争议，特别关注生产级LLMs在安全措施下的数据提取可行性。

Method: 采用两阶段方法：1) 初始探测（有时使用Best-of-N越狱）测试提取可行性；2) 迭代连续提示尝试提取书籍内容。使用基于块的最长公共子串近似（nv-recall）评估提取成功率。

Result: 不同LLMs表现各异：Gemini 2.5 Pro和Grok 3无需越狱即可提取大量文本（nv-recall达76.8%和70.3%）；Claude 3.7 Sonnet越狱后可近乎逐字输出整本书（nv-recall=95.8%）；GPT-4.1需要更多越狱尝试且最终拒绝继续（nv-recall=4.0%）。

Conclusion: 即使有模型和系统级安全防护，生产级LLMs仍存在训练数据提取风险，这对版权保护具有重要意义。

Abstract: Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.

</details>


### [19] [LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation](https://arxiv.org/abs/2601.02957)
*Fabian Lukassen,Christoph Weisser,Michael Schlee,Manish Kumar,Anton Thielmann,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 提出结合集成统计方法与大型语言模型的时间序列变点检测框架，提升检测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决两个关键问题：1) 单个检测方法在不同数据特征下表现互补，方法选择困难且容易得到次优结果；2) 缺乏对检测到的变化的自动化、上下文相关的解释

Method: 集成十种不同的变点检测算法，结合LLM驱动的解释管道自动生成上下文叙事，对于私有或领域特定数据采用检索增强生成(RAG)方案

Result: 相比单个方法，集成方法实现了更优的性能和鲁棒性，能够将检测到的变点与潜在的现实世界历史事件联系起来

Conclusion: 开源Python框架在金融、政治科学和环境科学等多个领域展示了实用价值，将原始统计输出转化为分析师和决策者可操作的见解

Abstract: This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.

</details>


### [20] [Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation](https://arxiv.org/abs/2601.03051)
*Vidhi Rathore,Sambu Aneesh,Himanshu Singh*

Main category: cs.CL

TL;DR: 提出基于图神经网络的对话级幻觉检测方法，将对话建模为时序图，通过消息传递和注意力池化识别幻觉


<details>
  <summary>Details</summary>
Motivation: 对话AI系统在多轮对话中容易产生幻觉，特别是在上下文变化和出现矛盾时。现有方法难以有效检测对话级别的幻觉，需要新的框架来建模对话结构和时序关系。

Method: 将整个对话表示为时序图，每个对话轮次作为节点，使用句子编码器编码。构建两种边连接：1) 共享实体边 - 连接引用相同实体的轮次；2) 时序边 - 连接连续对话轮次。通过消息传递更新节点嵌入，使用注意力池化将上下文感知的节点嵌入组合为单个向量，最后通过分类器检测幻觉类型。

Result: 该方法相比现有方法有轻微性能提升，注意力机制可用于解释决策过程。代码和模型权重已开源。

Conclusion: 图神经网络方法能有效建模对话结构和时序关系，为对话级幻觉检测提供了新思路，且具有可解释性。

Abstract: Hallucinations can be produced by conversational AI systems, particularly in multi-turn conversations where context changes and contradictions may eventually surface. By representing the entire conversation as a temporal graph, we present a novel graph-based method for detecting dialogue-level hallucinations. Our framework models each dialogue as a node, encoding it using a sentence transformer. We explore two different ways of connectivity: i) shared-entity edges, which connect turns that refer to the same entities; ii) temporal edges, which connect contiguous turns in the conversation. Message-passing is used to update the node embeddings, allowing flow of information between related nodes. The context-aware node embeddings are then combined using attention pooling into a single vector, which is then passed on to a classifier to determine the presence and type of hallucinations. We demonstrate that our method offers slightly improved performance over existing methods. Further, we show the attention mechanism can be used to justify the decision making process. The code and model weights are made available at: https://github.com/sambuaneesh/anlp-project.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [21] [M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination](https://arxiv.org/abs/2601.02777)
*Jingcheng Cao,Chaoran Xiong,Jianmin Song,Shang Yan,Jiachen Liu,Ling Pei*

Main category: cs.RO

TL;DR: M-SEVIQ是一个多波段立体事件视觉和惯性四足机器人数据集，用于解决敏捷运动中的视觉感知挑战


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机在四足机器人敏捷运动中容易产生模糊图像，特别是在低光条件下。事件相机具有低延迟、高时间分辨率和宽动态范围的优势，但现有事件相机数据集在立体配置和多波段感知方面存在局限。

Method: 使用Unitree Go2四足机器人配备立体事件相机、帧式相机、IMU和关节编码器，收集了超过30个真实世界序列，涵盖不同速度水平、照明波长和光照条件，并提供完整的校准数据。

Result: 创建了M-SEVIQ数据集，包含多波段立体事件视觉和惯性数据，支持传感器融合和基准测试，为敏捷机器人感知研究提供资源。

Conclusion: M-SEVIQ数据集能够支持敏捷机器人感知、传感器融合、语义分割和多模态视觉研究，特别是在挑战性环境中的应用。

Abstract: Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.

</details>


### [22] [A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints](https://arxiv.org/abs/2601.03055)
*Shiying Dong,Zhipeng Shen,Rudolf Reiter,Hailong Huang,Bingzhao Gao,Hong Chen,Wen-Hua Chen*

Main category: cs.RO

TL;DR: 提出一种时间缩放直接多重射击方案和快速半定规划凸松弛方法，用于解决具有时空约束的最优控制问题，显著改善数值特性并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 自主智能体在时空约束下的最优控制问题（如自动驾驶节能驾驶、四旋翼导航）通常通过非线性规划近似，但由于动力学与事件时序的耦合导致非凸性，求解困难。现有方法通常预设航点时间或使用非凸轨迹优化，这简化了问题但往往得到次优解。

Method: 1. 提出时间缩放直接多重射击方案，将预测时域划分为与特征时间约束对齐的段；2. 开发基于半定规划的快速凸松弛方法，利用提升公式的稀疏模式。

Result: 综合仿真研究表明该方法在解的最优性和计算效率方面表现优异。在四旋翼航点飞行任务（具有约束开放时间窗口）的真实世界实验中，验证了该方法在复杂环境中的实际适用性。

Conclusion: 所提出的时间缩放直接多重射击方案和半定规划凸松弛方法能够有效解决具有时空约束的最优控制问题，在保持最优性的同时显著提高计算效率，适用于实际复杂环境中的自主智能体控制。

Abstract: Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments.

</details>
