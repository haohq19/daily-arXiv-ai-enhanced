<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: MirrorTalk是一个基于条件扩散模型的生成框架，通过语义解耦风格编码器从参考视频中提取纯风格表示，采用分层调制策略平衡音频和风格特征，实现个性化说话人脸合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成个性化说话人脸时，难以同时保持说话者独特风格和唇部同步准确性，主要问题在于面部动作中说话者特定风格和语义内容的固有混淆，这阻碍了将说话者独特个性忠实迁移到任意语音中。

Method: 提出MirrorTalk框架：1）基于条件扩散模型的生成框架；2）语义解耦风格编码器（SDSE）从简短参考视频中提取纯风格表示；3）在扩散过程中引入分层调制策略，动态平衡不同面部区域的音频和风格特征贡献。

Result: 大量实验表明，MirrorTalk在唇部同步准确性和个性化保持方面相比最先进方法取得了显著改进。

Conclusion: MirrorTalk通过解耦风格表示和分层调制策略，有效解决了说话风格与语义内容混淆的问题，实现了既保持说话者独特风格又确保唇部同步准确性的个性化说话人脸合成。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [2] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 本研究比较了深度图像和点云数据在奶牛体况评分预测中的表现，发现深度图像在未分割和全身分割数据上表现更优，点云对噪声更敏感，三维点云在当前评估条件下未显示出明显优势。


<details>
  <summary>Details</summary>
Motivation: 传统视觉体况评分主观且劳动密集，计算机视觉方法如深度图像已被应用，但三维点云数据因能表示更丰富的几何特征而受到关注，然而深度图像与点云方法的直接比较研究有限。

Method: 在四个设置下比较顶视深度图像和点云数据：1)未分割原始数据，2)分割全身数据，3)分割后躯数据，4)手工特征数据。使用来自商业农场的1,020头奶牛数据，采用奶牛级交叉验证防止数据泄漏。

Result: 深度图像模型在使用未分割原始数据和分割全身数据时始终比点云模型准确率更高；使用分割后躯数据时两者性能相当；两种方法在使用手工特征数据时准确率均下降；点云预测对噪声和模型架构更敏感。

Conclusion: 在评估条件下，三维点云并未显示出相对于深度图像的一致优势，深度图像在奶牛体况评分预测中表现更稳定可靠。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [3] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: SEAL是首个面向开放词汇事件实例分割（OV-EIS）的语义感知事件分割框架，支持多粒度的事件分割和开放词汇掩码分类，无需视觉提示即可实现通用时空OV-EIS。


<details>
  <summary>Details</summary>
Motivation: 现有事件传感器研究主要局限于语义级理解，缺乏针对开放词汇事件实例分割的全面解决方案。需要支持多粒度（实例级和部件级）的事件分割和分类能力。

Method: 提出SEAL框架，统一支持事件分割和开放词汇掩码分类。采用参数高效架构，并开发了无需用户提供视觉提示的通用时空OV-EIS变体。

Result: 在四个从粗到细粒度配置的基准测试中，SEAL在性能和推理速度上大幅超越基线方法，参数效率高。

Conclusion: SEAL是首个解决开放词汇事件实例分割的框架，支持多粒度理解，为事件传感器场景理解提供了新的解决方案。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 首个临床EEG到语言基础模型CELM，能够总结长时间EEG记录并生成多尺度临床报告，性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 从长时间EEG记录中生成总结异常模式、诊断发现和临床解释的报告目前仍然非常耗时费力，需要自动化解决方案。

Method: 开发CELM模型，整合预训练的EEG基础模型和语言模型，支持多尺度报告生成（记录描述、背景活动、癫痫样异常、事件/癫痫发作、印象）。

Result: 在有患者病史监督的情况下，生成指标（如ROUGE-1和METEOR）相对改进70%-95%；在零样本设置下，CELM得分为0.43-0.52，显著优于基线的0.17-0.26。

Conclusion: CELM是首个能够总结长时间EEG记录并生成多尺度临床报告的基础模型，为临床EEG报告自动化提供了有效解决方案。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [5] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析重新表述为一系列二分类问题，通过离散化事件时间，使现成的表格基础模型能够通过上下文学习进行生存分析，无需显式训练。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务上取得了显著成功，但由于右删失（数据观测可能在事件发生前结束），将其适应于建模生存分析的时间到事件结果具有挑战性。

Method: 开发了一个基于分类的框架，通过离散化事件时间，将静态和动态生存分析重新表述为一系列二分类问题。删失观测被自然地处理为在某些时间点缺少标签的示例。

Result: 在标准删失假设下，最小化二分类损失可以恢复真实的生存概率。在53个真实世界数据集上的评估表明，使用此分类公式的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线。

Conclusion: 该分类框架使表格基础模型能够通过上下文学习进行生存分析，无需显式训练，并在多个数据集上展现出优于传统方法的性能。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [6] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL是一个安全的去中心化联邦学习框架，结合DAG账本、侧链和零知识证明，实现隐私保护的模型验证和对抗行为检测。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时实现协作模型训练，但集中式和去中心化方法都面临可扩展性、安全性和更新验证的挑战。

Method: 提出ZK-HybridFL框架，集成有向无环图(DAG)账本、专用侧链和零知识证明(ZKPs)，使用事件驱动智能合约和预言机辅助侧链验证本地模型更新，内置挑战机制检测对抗行为。

Result: 在图像分类和语言建模任务中，ZK-HybridFL相比Blade-FL和ChainFL实现了更快的收敛速度、更高的准确率、更低的困惑度和更低的延迟，能抵抗大量对抗节点和空闲节点，支持亚秒级链上验证和高效gas使用。

Conclusion: ZK-HybridFL是一个可扩展且安全的去中心化联邦学习解决方案，适用于多样化环境，能防止无效更新和孤儿攻击。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [7] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: 论文提出机器学习遗忘中存在"残留知识"风险：即使模型已遗忘特定样本，其对抗扰动版本仍能被识别，揭示了新的隐私漏洞。作者提出RURK微调策略来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法通过统计不可区分性保证遗忘效果，但这些保证不能自然扩展到对抗扰动输入的情况。遗忘样本的轻微扰动仍可能被遗忘模型正确识别，而重新训练的模型却无法识别，这揭示了新的隐私风险：遗忘样本的信息可能在其局部邻域中持续存在。

Method: 提出RURK（Residual Knowledge Reduction）微调策略，通过惩罚模型重新识别扰动遗忘样本的能力来减少残留知识。该方法在高维环境中是必要的，因为残留知识在这种情况下是不可避免的。

Result: 在视觉基准测试和深度神经网络上的实验表明，残留知识在现有遗忘方法中普遍存在，而RURK方法能有效防止残留知识。

Conclusion: 机器学习遗忘中存在残留知识这一新的隐私风险，特别是在对抗扰动场景下。提出的RURK方法能有效缓解这一问题，为机器学习遗忘提供了更强的隐私保护。

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [8] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 开发了一个基于LLM的自动化系统，用于大规模生成和解决高质量预测问题，性能优于人工平台


<details>
  <summary>Details</summary>
Motivation: 预测未来事件对决策至关重要，是衡量通用智能的指标。现有自动化方法依赖重复数据源（如天气、股票），限制了问题多样性和实用性，需要更自动化的系统来生成和解决大量多样化预测问题。

Method: 使用LLM驱动的网络研究代理自动生成和解决预测问题。系统生成1499个多样化、真实的预测问题，并在数月后自动解决这些问题。

Result: 系统生成可验证、无歧义问题的准确率约96%，超过领先的人工平台Metaculus。问题解决准确率约95%。更智能的LLM（Gemini 3 Pro Brier分数0.134，GPT-5 0.149，Gemini 2.5 Flash 0.179）表现更好。问题分解策略显著改善Brier分数（0.132 vs 0.141）。

Conclusion: 基于LLM的自动化系统能够大规模生成和解决高质量预测问题，性能优于人工平台，并能直接用于改进预测能力，为AI预测评估提供了有效工具。

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [9] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，包含模型基础、模型自由和情景控制三个模块，用于大规模统计机器学习中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 人类通过多个相互作用的神经系统（模型基础规划、模型自由习惯响应和情景记忆学习）实现高效学习。本文旨在将这些生物学效率原理转化为可扩展贝叶斯推理的采样算法，解决大规模统计机器学习中的计算挑战。

Method: 提出三组件算法：1) 模型基础模块：使用目标分布进行引导但计算缓慢的采样；2) 模型自由模块：利用先前样本学习参数空间模式，实现快速反射式采样而无需直接评估昂贵的目标分布；3) 情景控制模块：通过回忆特定过去事件（样本）支持快速采样。

Result: 该方法推进了贝叶斯方法，促进其在大规模统计机器学习问题中的应用，特别是在贝叶斯深度学习中进行适当和原则性的不确定性量化。

Conclusion: 受人类多系统学习启发的三模块采样算法为大规模贝叶斯推理提供了计算高效的解决方案，特别适用于需要不确定性量化的贝叶斯深度学习应用。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [10] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug是一个针对电子健康记录中患者冷启动问题的元学习框架，通过两级元适应机制和不确定性量化模块，显著提升了新患者的药物推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法面临患者冷启动问题，即新患者因缺乏足够的处方历史而难以获得可靠的个性化推荐。虽然已有研究利用医学知识图谱缓解项目冷启动，但未能充分解决患者个性化适应问题。元学习在推荐系统中处理稀疏交互用户方面有潜力，但在电子健康记录这种具有独特时序结构的数据中应用不足。

Method: 提出MetaDrug框架，包含：1）两级元适应机制：自我适应（利用患者自身医疗事件作为支持集捕捉时序依赖）和同伴适应（利用相似患者的就诊记录丰富新患者表示）；2）不确定性量化模块：对支持集就诊进行排序并过滤无关信息以保证适应一致性。

Result: 在MIMIC-III和急性肾损伤（AKI）数据集上的实验结果表明，MetaDrug在冷启动患者上持续优于最先进的药物推荐方法。

Conclusion: MetaDrug通过创新的元学习框架有效解决了电子健康记录中的患者冷启动问题，为临床决策提供了更可靠的数据驱动药物推荐。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [11] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL：一种多任务强化学习方法，通过将命题视为参数化谓词而非离散符号，实现对新命题和任务的零样本泛化


<details>
  <summary>Details</summary>
Motivation: 现有LTL引导的多任务强化学习方法虽然能在LTL规范结构上成功泛化，但无法泛化到未见过的命题词汇表（描述LTL中高层事件的符号）。这限制了策略对全新命题和任务的适应能力。

Method: 将命题视为参数化谓词的实例而非离散符号，提出新颖的架构来嵌入和组合谓词以表示LTL规范，使策略能够学习相关命题间的共享结构。

Result: 在具有挑战性的环境中，实现了对新命题和任务的零样本泛化，不仅能在LTL公式结构上组合泛化，还能在命题上参数化泛化。

Conclusion: PlatoLTL通过参数化谓词表示方法，解决了多任务强化学习中命题泛化的关键挑战，为构建更具通用性的智能体提供了有效途径。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [12] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: STGAT框架通过时空图注意力网络检测能源物联网设备中的时间异常（时钟漂移、同步偏移、Y2K38溢出），结合漂移感知时间嵌入和图注意力，在受控时间扰动实验中达到95.7%准确率。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的时间完整性对智能电网等能源网络至关重要，但存在时钟漂移、时间同步操纵和Y2K38时间戳溢出等漏洞，传统基于可靠时间戳的异常检测方法无法处理这些时间不一致问题。

Method: 提出STGAT框架：1) 使用漂移感知时间嵌入和时间自注意力捕捉单个设备的时间演化异常；2) 利用图注意力建模时间误差的空间传播；3) 通过曲率正则化潜在表示在几何上分离正常时钟演化与异常模式。

Result: 在受控时间扰动的能源物联网遥测数据实验中，STGAT达到95.7%准确率，显著优于循环、Transformer和图基线模型（d > 1.8, p < 0.001），检测延迟降低26%（仅2.3个时间步延迟），在溢出、漂移和物理不一致情况下保持稳定性能。

Conclusion: STGAT能有效检测能源物联网中的时间异常，通过建模时间扭曲和设备间一致性，为智能电网等关键基础设施提供可靠的时间完整性保障，显著优于现有方法。

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [13] [SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training](https://arxiv.org/abs/2601.23155)
*Powei Chang,Jinpeng Zhang,Bowen Chen,Chenyu Wang,Chenlu Guo,Yixing Zhang,Yukang Gao,JianXiang Xiang,Yue Gao,Chaoqun Sun,Yiyi Chen,Dongying Kong*

Main category: cs.LG

TL;DR: SPICE：一种基于信息的数据选择方法，通过惩罚梯度冲突来最大化Fisher信息，仅用10%数据就能达到或超过全数据调优的性能。


<details>
  <summary>Details</summary>
Motivation: 基于Fisher信息的数据选择虽然理论上具有单调子模性，但在实践中梯度冲突会减缓边际信息增益的衰减，导致信息损失。需要量化冲突对子模性的影响并设计冲突感知的选择器。

Method: 提出ε-分解理论量化冲突对子模性的偏离，基于此设计SPICE选择器：最大化信息同时惩罚梯度错位，支持早停和代理模型以提高效率。

Result: SPICE选择的子集比原始标准具有更高的对数行列式信息，这些信息增益转化为性能提升：在8个基准测试中，仅用10%数据就能匹配或超过包括全数据调优在内的6种方法。

Conclusion: SPICE通过冲突感知的信息最大化选择，实现了显著降低训练成本的同时获得性能提升，为高效指令调优提供了有效的数据选择方法。

Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: GGMS是一个用于分布式协议设计的强化学习框架，结合了蒙特卡洛树搜索、Transformer编码器和模型检查器，能够自动搜索并验证正确的分布式协议。


<details>
  <summary>Details</summary>
Motivation: 设计可证明正确的分布式协议极其困难且耗时，传统方法难以处理多智能体不完全信息博弈场景，需要自动化方法来加速协议设计过程。

Method: 将协议设计建模为不完全信息博弈中的策略搜索问题，使用GGMS框架：集成蒙特卡洛树搜索变体、Transformer动作编码器、全局深度优先搜索来跳出局部最优，并通过模型检查器提供重复反馈。

Result: GGMS能够学习比现有方法更大规模设置下的正确协议，输出协议经过有界执行下的穷举模型检查验证，并在温和假设下证明搜索过程的完备性。

Conclusion: GGMS为分布式协议设计提供了一种自动化、可验证的解决方案，能够显著减少人工设计工作量，并在理论上保证找到正确协议的可能性。

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [15] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net是一个用于极端天气下城市时空预测的双分支Transformer架构，通过自注意力与交叉注意力分离内在和天气诱导的交通模式，并采用因果数据增强提升罕见场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端天气（如暴雨）下的城市时空预测存在挑战：1）事件罕见且动态变化；2）天气作为辅助输入时通常使用粗粒度描述符；3）缺乏捕捉细粒度时空效应的专门机制；4）现有因果方法忽视时间动态或依赖固定混杂因子分层。

Method: 提出WED-Net（Weather-Effect Disentanglement Network）：1）双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式；2）使用记忆库增强表示；3）通过自适应门控融合；4）引入判别器明确区分天气条件；5）设计因果数据增强策略，扰动非因果部分同时保留因果结构。

Result: 在三个城市的出租车流量数据集上实验表明，WED-Net在极端天气条件下表现出稳健性能，支持更安全的移动性、灾害准备和城市韧性。代码已公开。

Conclusion: WED-Net通过解耦天气效应和因果增强，有效解决了极端天气下城市时空预测的挑战，为实际应用中的安全移动、灾害准备和城市韧性提供了有前景的解决方案。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [16] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双形式经验模式，包括用于复杂子任务的专门子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体往往无法从经验中积累知识，将每个任务视为独立挑战。现有方法将经验提取为扁平化文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致经验库随着积累而退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取和维护双形式经验模式：1) 为程序性子任务提取具有独立推理和记忆的专门子智能体；2) 为静态知识提取技能模式（指南或代码片段）；3) 通过持续维护机制对模式进行评分、剪枝和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个任务上分别达到98.4%、70.4%和27.1%的成功率，步骤减少20-73%。在TravelPlanner上，自动提取的系统性能超过手动设计的系统（27.1% vs 12.1%），展示了其捕捉程序协调的能力。

Conclusion: AutoRefine框架通过提取双形式经验模式和持续维护机制，有效解决了智能体经验积累和知识库退化问题，显著提升了任务执行效率和成功率，特别是在捕捉复杂程序协调方面表现出色。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [17] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：基于轨迹驱动的抽象机制，自动从执行日志构建状态，支持在线构建智能体行为MDP，通过谓词树学习和反例精化实现自动化验证


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过工具行动，在长期随机交互轨迹中演化行为，这使得保证变得复杂。现有动态概率保证方法需要手动定义状态抽象，增加了采用摩擦。需要自动化状态构建来降低验证门槛

Method: 提出TriCEGAR方法：1）从执行日志自动构建状态抽象；2）使用谓词树表示抽象并从轨迹学习；3）利用反例进行精化；4）在线构建智能体行为MDP；5）实现框架原生支持，捕获类型化智能体生命周期事件

Result: 实现了自动化状态抽象构建，支持概率模型检查计算边界（如Pmax(成功)和Pmin(失败)），并展示了运行似然如何支持异常检测作为护栏信号

Conclusion: TriCEGAR通过自动化状态抽象构建解决了智能体AI系统运行时验证的采用障碍，提供了可扩展的验证框架，降低了手动定义状态抽象的负担

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [18] [Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking](https://arxiv.org/abs/2601.22410)
*Imene Kolli,Kai-Robin Lange,Jonas Rieger,Carsten Jentsch*

Main category: cs.CL

TL;DR: 提出基于图的可解释框架，通过构建词中心语义网络来分析历时语料库中的语义演变，结合分布相似性和词汇可替换性，通过聚类和对齐追踪语义变化轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有语义演变分析方法通常依赖预定义的义项清单，缺乏透明度和灵活性。需要一种紧凑且可解释的表示方法，能够捕捉语义网络结构变化，而不受限于固定义项分类。

Method: 为每个目标词和时间切片构建词中心语义网络，整合历时Skip-gram嵌入的分布相似性和特定时间掩码语言模型的词汇可替换性。通过聚类外围图识别义项相关结构，通过节点重叠对齐跨时间聚类，通过聚类组成和归一化聚类质量追踪变化。

Result: 在《纽约时报杂志》文章语料库（1980-2017）的应用研究中，显示图连接性反映多义性动态，诱导的社区捕捉了对比轨迹：事件驱动的义项替换（trump）、语义稳定性与聚类过分割效应（god）、以及与数字通信相关的渐进关联变化（post）。

Conclusion: 词中心语义图为探索义项演变提供了紧凑且透明的表示方法，无需依赖预定义的义项清单，能够有效捕捉不同类型的语义变化模式。

Abstract: We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories.

</details>


### [19] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: 提出阈值技术减少罕见token的边缘化影响，改善低资源语言模型性能


<details>
  <summary>Details</summary>
Motivation: 神经语言模型在低资源语言上表现不佳，因为训练数据有限，罕见token在训练集中出现频率低，导致它们在学习过程中被过度边缘化，无法有效学习

Method: 提出阈值技术来减少边缘化的负面影响，通过限制过度边缘化的有害影响，让罕见token能够获得更有意义的对齐。这是首次展示如何应用负采样来改善罕见token的表征

Result: 通过字符级语言模型的实验证明，该方法显著提高了在低资源语言验证数据上的性能表现

Conclusion: 该方法为改善低资源语言模型性能提供了新途径，通过减少罕见token的边缘化影响，使它们能够更有效地学习，从而提升语言模型在低资源语言上的表现

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [20] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm是一种新的Transformer归一化方法，通过跨整个Transformer块的残差连接稳定训练，同时采用PostNorm风格计算提升性能，解决了PreNorm和PostNorm之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer架构中归一化层的位置选择存在根本性权衡：PreNorm架构确保训练稳定性但可能导致深层模型性能下降，PostNorm架构提供强大性能但存在严重训练不稳定性。需要一种方法同时获得两者的优势。

Method: 提出SpanNorm技术：1) 建立跨越整个Transformer块的干净残差连接以稳定信号传播；2) 采用PostNorm风格计算，对聚合输出进行归一化以提升模型性能；3) 结合有原则的缩放策略，确保网络信号方差有界。

Result: 理论分析表明SpanNorm能保持网络信号方差有界，防止PostNorm模型的梯度问题，同时缓解PreNorm的表示崩溃。实证结果显示SpanNorm在密集模型和MoE场景中均优于标准归一化方案。

Conclusion: SpanNorm成功解决了Transformer架构中归一化位置的权衡问题，为更强大和稳定的Transformer架构铺平了道路，在保持训练稳定性的同时提升了模型性能。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [21] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: TSLM通过树状结构语言建模，让语言模型能在单次生成过程中编码分支结构，实现系统性探索，提升推理效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型按顺序生成推理，无法解耦无关探索路径，导致搜索效率低下，需要多次独立前向计算。

Method: 引入树状结构语言建模(TSLM)，使用特殊标记编码分支结构，在完整搜索树（包括成功和失败尝试）上进行训练，让模型内化系统性探索能力。

Result: TSLM实现了鲁棒性能和优越的推理效率，避免了外部搜索方法需要的多次独立前向计算。

Conclusion: 基于完整树状结构轨迹的监督学习为开发语言模型的系统性探索能力提供了高效替代方案，展示了推理时扩展的新范式。

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [22] [FNF: Functional Network Fingerprint for Large Language Models](https://arxiv.org/abs/2601.22692)
*Yiheng Liu,Junhao Ning,Sichen Xia,Haiyang Sun,Yang Yang,Hanyang Chi,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 提出Functional Network Fingerprint (FNF)方法，通过分析功能网络活动一致性来检测LLM是否源自受害模型，无需训练、样本高效，对常见模型修改具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开发成本高昂且有重要商业价值，防止开源LLM被未经授权盗用、保护开发者知识产权成为关键挑战。需要一种有效的方法来检测模型是否源自特定原始模型。

Method: 提出Functional Network Fingerprint (FNF)方法，基于功能网络活动一致性检测模型来源。该方法无需训练，仅需少量样本，通过分析神经元活动模式的一致性来判断两个模型是否具有共同起源。

Result: 实验表明，具有共同起源的模型（即使规模或架构不同）在多样化输入样本下表现出高度一致的功能网络活动模式。而独立训练的模型则无法保持这种活动对齐。该方法对微调、剪枝、参数置换等常见修改具有鲁棒性。

Conclusion: FNF为模型所有者和第三方提供了一种简单、非侵入性且有效的工具来保护LLM知识产权，仅需少量样本即可验证，同时保持模型实用性。

Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [23] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文提出了跨语言对齐引导（CLAS）框架，用于在多语言大语言模型中减少政治偏见，确保跨语言一致性，同时保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型日益影响全球话语，公平性和意识形态中立性对负责任的AI部署至关重要。现有研究主要关注高资源西方语言或狭窄的多语言设置，跨语言一致性和安全的后处理缓解方法研究不足。

Method: 提出跨语言对齐引导（CLAS）框架，通过将政治提示诱导的潜在意识形态表示对齐到共享的意识形态子空间来增强现有引导方法，确保跨语言一致性，并使用自适应机制防止过度校正和保持连贯性。

Result: 实验表明，该方法在经济和社会两个轴向上显著减少了偏见，同时响应质量下降最小。框架为公平感知的多语言LLM治理建立了可扩展和可解释的范式。

Conclusion: CLAS框架在平衡意识形态中立性与语言文化多样性方面，为多语言大语言模型的公平治理提供了有效的解决方案。

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>
