{"id": "2506.19999", "categories": ["cs.LG", "cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.19999", "abs": "https://arxiv.org/abs/2506.19999", "authors": ["Francesco Ignazio Re", "Andreas Opedal", "Glib Manaiev", "Mario Giulianelli", "Ryan Cotterell"], "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior", "comment": "ACL 2025", "summary": "Reading is a process that unfolds across space and time, alternating between\nfixations where a reader focuses on a specific point in space, and saccades\nwhere a reader rapidly shifts their focus to a new point. An ansatz of\npsycholinguistics is that modeling a reader's fixations and saccades yields\ninsight into their online sentence processing. However, standard approaches to\nsuch modeling rely on aggregated eye-tracking measurements and models that\nimpose strong assumptions, ignoring much of the spatio-temporal dynamics that\noccur during reading. In this paper, we propose a more general probabilistic\nmodel of reading behavior, based on a marked spatio-temporal point process,\nthat captures not only how long fixations last, but also where they land in\nspace and when they take place in time. The saccades are modeled using a Hawkes\nprocess, which captures how each fixation excites the probability of a new\nfixation occurring near it in time and space. The duration time of fixation\nevents is modeled as a function of fixation-specific predictors convolved\nacross time, thus capturing spillover effects. Empirically, our Hawkes process\nmodel exhibits a better fit to human saccades than baselines. With respect to\nfixation durations, we observe that incorporating contextual surprisal as a\npredictor results in only a marginal improvement in the model's predictive\naccuracy. This finding suggests that surprisal theory struggles to explain\nfine-grained eye movements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u70b9\u8fc7\u7a0b\u7684\u6982\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u6a21\u62df\u9605\u8bfb\u884c\u4e3a\uff0c\u5305\u62ec\u6ce8\u89c6\u70b9\u7684\u4f4d\u7f6e\u3001\u65f6\u95f4\u548c\u6301\u7eed\u65f6\u95f4\uff0c\u4ee5\u53ca\u626b\u89c6\u7684\u52a8\u6001\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u805a\u5408\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u548c\u5f3a\u5047\u8bbe\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u9605\u8bfb\u4e2d\u7684\u65f6\u7a7a\u52a8\u6001\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u901a\u7528\u7684\u6a21\u578b\u6355\u6349\u8fd9\u4e9b\u7ec6\u8282\u3002", "method": "\u4f7f\u7528\u6807\u8bb0\u7684\u65f6\u7a7a\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u626b\u89c6\u901a\u8fc7Hawkes\u8fc7\u7a0b\u5efa\u6a21\uff0c\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7684\u9884\u6d4b\u53d8\u91cf\u5efa\u6a21\u3002", "result": "Hawkes\u8fc7\u7a0b\u6a21\u578b\u5728\u62df\u5408\u4eba\u7c7b\u626b\u89c6\u884c\u4e3a\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u4e0a\u4e0b\u6587\u60ca\u5947\u5ea6\u5bf9\u6ce8\u89c6\u6301\u7eed\u65f6\u95f4\u7684\u9884\u6d4b\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u60ca\u5947\u5ea6\u7406\u8bba\u5728\u89e3\u91ca\u7cbe\u7ec6\u773c\u52a8\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2506.20373", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20373", "abs": "https://arxiv.org/abs/2506.20373", "authors": ["Joerg Deigmoeller", "Stephan Hasler", "Nakul Agarwal", "Daniel Tanneberg", "Anna Belardinelli", "Reza Ghoddoosian", "Chao Wang", "Felix Ocker", "Fan Zhang", "Behzad Dariush", "Michael Gienger"], "title": "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition", "comment": null, "summary": "We introduce CARMA, a system for situational grounding in human-robot group\ninteractions. Effective collaboration in such group settings requires\nsituational awareness based on a consistent representation of present persons\nand objects coupled with an episodic abstraction of events regarding actors and\nmanipulated objects. This calls for a clear and consistent assignment of\ninstances, ensuring that robots correctly recognize and track actors, objects,\nand their interactions over time. To achieve this, CARMA uniquely identifies\nphysical instances of such entities in the real world and organizes them into\ngrounded triplets of actors, objects, and actions.\n  To validate our approach, we conducted three experiments, where multiple\nhumans and a robot interact: collaborative pouring, handovers, and sorting.\nThese scenarios allow the assessment of the system's capabilities as to role\ndistinction, multi-actor awareness, and consistent instance identification. Our\nexperiments demonstrate that the system can reliably generate accurate\nactor-action-object triplets, providing a structured and robust foundation for\napplications requiring spatiotemporal reasoning and situated decision-making in\ncollaborative settings.", "AI": {"tldr": "CARMA\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u673a\u7fa4\u4f53\u4ea4\u4e92\u4e2d\u60c5\u5883\u611f\u77e5\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u8ddf\u8e2a\u53c2\u4e0e\u8005\u3001\u7269\u4f53\u53ca\u5176\u4ea4\u4e92\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7684\u4e09\u5143\u7ec4\uff08\u53c2\u4e0e\u8005-\u52a8\u4f5c-\u7269\u4f53\uff09\uff0c\u4e3a\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u5728\u7fa4\u4f53\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u60c5\u5883\u611f\u77e5\u80fd\u529b\u4ee5\u6709\u6548\u534f\u4f5c\uff0c\u8fd9\u8981\u6c42\u5bf9\u53c2\u4e0e\u8005\u3001\u7269\u4f53\u53ca\u5176\u4ea4\u4e92\u8fdb\u884c\u4e00\u81f4\u7684\u8868\u5f81\u548c\u8ddf\u8e2a\u3002", "method": "CARMA\u901a\u8fc7\u552f\u4e00\u6807\u8bc6\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b9e\u4f53\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u4e3a\u53c2\u4e0e\u8005-\u52a8\u4f5c-\u7269\u4f53\u7684\u4e09\u5143\u7ec4\uff0c\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CARMA\u5728\u89d2\u8272\u533a\u5206\u3001\u591a\u53c2\u4e0e\u8005\u611f\u77e5\u548c\u5b9e\u4f8b\u8bc6\u522b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u80fd\u591f\u53ef\u9760\u751f\u6210\u51c6\u786e\u7684\u4e09\u5143\u7ec4\u3002", "conclusion": "CARMA\u4e3a\u9700\u8981\u65f6\u7a7a\u63a8\u7406\u548c\u60c5\u5883\u51b3\u7b56\u7684\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u4e14\u9c81\u68d2\u7684\u57fa\u7840\u3002"}}
{"id": "2506.20222", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20222", "abs": "https://arxiv.org/abs/2506.20222", "authors": ["Pujing Yang", "Guangyi Zhang", "Yunlong Cai", "Lei Yu", "Guanding Yu"], "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission", "comment": null, "summary": "Event cameras asynchronously capture pixel-level intensity changes with\nextremely low latency. They are increasingly used in conjunction with RGB\ncameras for a wide range of vision-related applications. However, a major\nchallenge in these hybrid systems lies in the transmission of the large volume\nof triggered events and RGB images. To address this, we propose a transmission\nscheme that retains efficient reconstruction performance of both sources while\naccomplishing real-time deblurring in parallel. Conventional RGB cameras and\nevent cameras typically capture the same scene in different ways, often\nresulting in significant redundant information across their outputs. To address\nthis, we develop a joint event and image (E-I) transmission framework to\neliminate redundancy and thereby optimize channel bandwidth utilization. Our\napproach employs Bayesian modeling and the information bottleneck method to\ndisentangle the shared and domain-specific information within the E-I inputs.\nThis disentangled information bottleneck framework ensures both the compactness\nand informativeness of extracted shared and domain-specific information.\nMoreover, it adaptively allocates transmission bandwidth based on scene\ndynamics, i.e., more symbols are allocated to events for dynamic details or to\nimages for static information. Simulation results demonstrate that the proposed\nscheme not only achieves superior reconstruction quality compared to\nconventional systems but also delivers enhanced deblurring performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4e8b\u4ef6\u548c\u56fe\u50cf\uff08E-I\uff09\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u6d88\u9664\u5197\u4f59\uff0c\u4f18\u5316\u5e26\u5bbd\u5229\u7528\uff0c\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u53bb\u6a21\u7cca\u3002", "motivation": "\u6df7\u5408\u7cfb\u7edf\u4e2d\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\u4f20\u8f93\u5927\u91cf\u6570\u636e\u5b58\u5728\u6311\u6218\uff0c\u4e14\u4e24\u8005\u8f93\u51fa\u5b58\u5728\u5197\u4f59\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\uff0c\u5206\u79bb\u5171\u4eab\u548c\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u5206\u914d\u4f20\u8f93\u5e26\u5bbd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u53bb\u6a21\u7cca\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5197\u4f59\u548c\u5e26\u5bbd\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2506.20401", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20401", "abs": "https://arxiv.org/abs/2506.20401", "authors": ["Jinchun Du", "Bojie Shen", "Muhammad Aamir Cheema", "Adel N. Toosi"], "title": "Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation", "comment": null, "summary": "With the rising popularity of electric vehicles (EVs), modern service\nsystems, such as ride-hailing delivery services, are increasingly integrating\nEVs into their operations. Unlike conventional vehicles, EVs often have a\nshorter driving range, necessitating careful consideration of charging when\nfulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -\nallowing EVs to also discharge energy back to the grid - new opportunities and\ncomplexities emerge. We introduce the Electric Vehicle Orienteering Problem\nwith V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select\ncustomer requests or orders while managing when and where to charge or\ndischarge. This involves navigating dynamic electricity prices, charging\nstation selection, and route constraints. We formulate the problem as a Mixed\nInteger Programming (MIP) model and propose two near-optimal metaheuristic\nalgorithms: one evolutionary (EA) and the other based on large neighborhood\nsearch (LNS). Experiments on real-world data show our methods can double driver\nprofits compared to baselines, while maintaining near-optimal performance on\nsmall instances and excellent scalability on larger ones. Our work highlights a\npromising path toward smarter, more profitable EV-based mobility systems that\nactively support the energy grid.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7535\u52a8\u6c7d\u8f66\u5bfc\u5411\u95ee\u9898\uff08EVOP-V2G\uff09\uff0c\u7ed3\u5408V2G\u6280\u672f\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u7684\u5145\u653e\u7535\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u5229\u6da6\u3002\u901a\u8fc7MIP\u6a21\u578b\u548c\u4e24\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08EA\u548cLNS\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5229\u6da6\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u666e\u53ca\u548cV2G\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u5728\u670d\u52a1\u7cfb\u7edf\u4e2d\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u7684\u5145\u653e\u7535\u7b56\u7565\u4ee5\u63d0\u5347\u5229\u6da6\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff1a\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\u548c\u5927\u90bb\u57df\u641c\u7d22\uff08LNS\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u80fd\u5c06\u53f8\u673a\u5229\u6da6\u7ffb\u500d\uff0c\u5e76\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u63a5\u8fd1\u6700\u4f18\uff0c\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7535\u52a8\u6c7d\u8f66\u79fb\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u66f4\u76c8\u5229\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u652f\u6301\u7535\u7f51\u7684\u80fd\u6e90\u7ba1\u7406\u3002"}}
{"id": "2506.20015", "categories": ["cs.LG", "cs.IT", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.20015", "abs": "https://arxiv.org/abs/2506.20015", "authors": ["Dengyu Wu", "Jiechen Chen", "H. Vincent Poor", "Bipin Rajendran", "Osvaldo Simeone"], "title": "Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons", "comment": null, "summary": "Neuromorphic computing offers an energy-efficient alternative to conventional\ndeep learning accelerators for real-time time-series processing. However, many\nedge applications, such as wireless sensing and audio recognition, generate\nstreaming signals with rich spectral features that are not effectively captured\nby conventional leaky integrate-and-fire (LIF) spiking neurons. This paper\ninvestigates a wireless split computing architecture that employs\nresonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain\nsignals directly, eliminating the need for costly spectral pre-processing. By\nresonating at tunable frequencies, RF neurons extract time-localized spectral\nfeatures while maintaining low spiking activity. This temporal sparsity\ntranslates into significant savings in both computation and transmission\nenergy. Assuming an OFDM-based analog wireless interface for spike\ntransmission, we present a complete system design and evaluate its performance\non audio classification and modulation classification tasks. Experimental\nresults show that the proposed RF-SNN architecture achieves comparable accuracy\nto conventional LIF-SNNs and ANNs, while substantially reducing spike rates and\ntotal energy consumption during inference and communication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u632f\u653e\u7535\u795e\u7ecf\u5143\uff08RF\uff09\u7684\u65e0\u7ebf\u5206\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u65f6\u57df\u4fe1\u53f7\uff0c\u51cf\u5c11\u8ba1\u7b97\u548c\u4f20\u8f93\u80fd\u8017\u3002", "motivation": "\u4f20\u7edfLIF\u795e\u7ecf\u5143\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8fb9\u7f18\u5e94\u7528\u4e2d\u4e30\u5bcc\u7684\u9891\u8c31\u7279\u5f81\uff0c\u9700\u6602\u8d35\u9884\u5904\u7406\u3002", "method": "\u91c7\u7528RF\u795e\u7ecf\u5143\u76f4\u63a5\u5904\u7406\u65f6\u57df\u4fe1\u53f7\uff0c\u5229\u7528\u53ef\u8c03\u8c10\u9891\u7387\u63d0\u53d6\u5c40\u90e8\u9891\u8c31\u7279\u5f81\uff0c\u7ed3\u5408OFDM\u65e0\u7ebf\u63a5\u53e3\u4f20\u8f93\u3002", "result": "RF-SNN\u67b6\u6784\u5728\u97f3\u9891\u548c\u8c03\u5236\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e0eLIF-SNN\u548cANN\u7cbe\u5ea6\u76f8\u5f53\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u8109\u51b2\u7387\u548c\u80fd\u8017\u3002", "conclusion": "RF\u795e\u7ecf\u5143\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5177\u6709\u9ad8\u6548\u80fd\u4f18\u52bf\uff0c\u9002\u5408\u5b9e\u65f6\u65f6\u57df\u4fe1\u53f7\u5904\u7406\u3002"}}
{"id": "2506.20464", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2506.20464", "abs": "https://arxiv.org/abs/2506.20464", "authors": ["Dibyayan Patra", "Pasindu Ranasinghe", "Bikram Banerjee", "Simit Raval"], "title": "A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners", "comment": null, "summary": "Rock bolts are crucial components of the subterranean support systems in\nunderground mines that provide adequate structural reinforcement to the rock\nmass to prevent unforeseen hazards like rockfalls. This makes frequent\nassessments of such bolts critical for maintaining rock mass stability and\nminimising risks in underground mining operations. Where manual surveying of\nrock bolts is challenging due to the low light conditions in the underground\nmines and the time-intensive nature of the process, automated detection of rock\nbolts serves as a plausible solution. To that end, this study focuses on the\nautomatic identification of rock bolts within medium to large-scale 3D point\nclouds obtained from underground mines using mobile laser scanners. Existing\ntechniques for automated rock bolt identification primarily rely on feature\nengineering and traditional machine learning approaches. However, such\ntechniques lack robustness as these point clouds present several challenges due\nto data noise, varying environments, and complex surrounding structures.\nMoreover, the target rock bolts are extremely small objects within large-scale\npoint clouds and are often partially obscured due to the application of\nreinforcement shotcrete. Addressing these challenges, this paper proposes an\napproach termed DeepBolt, which employs a novel two-stage deep learning\narchitecture specifically designed for handling severe class imbalance for the\nautomatic and efficient identification of rock bolts in complex 3D point\nclouds. The proposed method surpasses state-of-the-art semantic segmentation\nmodels by up to 42.5% in Intersection over Union (IoU) for rock bolt points.\nAdditionally, it outperforms existing rock bolt identification techniques,\nachieving a 96.41% precision and 96.96% recall in classifying rock bolts,\ndemonstrating its robustness and effectiveness in complex underground\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepBolt\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u590d\u67423D\u70b9\u4e91\u4e2d\u81ea\u52a8\u9ad8\u6548\u8bc6\u522b\u5ca9\u77f3\u951a\u6746\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u566a\u58f0\u3001\u73af\u5883\u53d8\u5316\u548c\u76ee\u6807\u906e\u6321\u7b49\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5730\u4e0b\u77ff\u5c71\u4e2d\u5ca9\u77f3\u951a\u6746\u7684\u9891\u7e41\u8bc4\u4f30\u5bf9\u7ef4\u6301\u5ca9\u4f53\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u68c0\u6d4b\u56e0\u73af\u5883\u6076\u52a3\u4e14\u8017\u65f6\uff0c\u81ea\u52a8\u5316\u68c0\u6d4b\u6210\u4e3a\u5fc5\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u70b9\u4e91\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDeepBolt\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4e13\u95e8\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u5ca9\u77f3\u951a\u6746\u7684\u81ea\u52a8\u8bc6\u522b\u3002", "result": "DeepBolt\u5728\u5ca9\u77f3\u951a\u6746\u70b9\u7684IoU\u4e0a\u6bd4\u73b0\u6709\u8bed\u4e49\u5206\u5272\u6a21\u578b\u63d0\u534742.5%\uff0c\u5206\u7c7b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5206\u522b\u8fbe\u523096.41%\u548c96.96%\u3002", "conclusion": "DeepBolt\u5728\u590d\u6742\u5730\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5ca9\u77f3\u951a\u6746\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20204", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20204", "abs": "https://arxiv.org/abs/2506.20204", "authors": ["Eduardo Gutierrez Maestro", "Hadi Banaee", "Amy Loutfi"], "title": "Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets", "comment": null, "summary": "Affective priming exemplifies the challenge of ambiguity in affective\ncomputing. While the community has largely addressed this issue from a\nlabel-based perspective, identifying data points in the sequence affected by\nthe priming effect, the impact of priming on data itself, particularly in\nphysiological signals, remains underexplored. Data affected by priming can lead\nto misclassifications when used in learning models. This study proposes the\nAffective Priming Score (APS), a data-driven method to detect data points\ninfluenced by the priming effect. The APS assigns a score to each data point,\nquantifying the extent to which it is affected by priming. To validate this\nmethod, we apply it to the SEED and SEED-VII datasets, which contain sufficient\ntransitions between emotional events to exhibit priming effects. We train\nmodels with the same configuration using both the original data and\npriming-free sequences. The misclassification rate is significantly reduced\nwhen using priming-free sequences compared to the original data. This work\ncontributes to the broader challenge of ambiguity by identifying and mitigating\npriming effects at the data level, enhancing model robustness, and offering\nvaluable insights for the design and collection of affective computing\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAffective Priming Score (APS)\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u60c5\u611f\u8ba1\u7b97\u4e2d\u53d7\u542f\u52a8\u6548\u5e94\u5f71\u54cd\u7684\u6570\u636e\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u4e2d\u542f\u52a8\u6548\u5e94\u7684\u6a21\u7cca\u6027\u5bf9\u6570\u636e\u5206\u7c7b\u9020\u6210\u6311\u6218\uff0c\u76ee\u524d\u7814\u7a76\u591a\u4ece\u6807\u7b7e\u89d2\u5ea6\u89e3\u51b3\uff0c\u4f46\u5bf9\u6570\u636e\u672c\u8eab\uff08\u5c24\u5176\u662f\u751f\u7406\u4fe1\u53f7\uff09\u7684\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAPS\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u6253\u5206\u4ee5\u91cf\u5316\u542f\u52a8\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u5728SEED\u548cSEED-VII\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u4f7f\u7528\u53bb\u542f\u52a8\u6548\u5e94\u6570\u636e\u540e\uff0c\u6a21\u578b\u7684\u8bef\u5206\u7c7b\u7387\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "APS\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u5c42\u9762\u8bc6\u522b\u548c\u7f13\u89e3\u542f\u52a8\u6548\u5e94\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u548c\u6536\u96c6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.20567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20567", "abs": "https://arxiv.org/abs/2506.20567", "authors": ["Zhiwang Zhang", "Dong Xu", "Wanli Ouyang", "Chuanqi Tan"], "title": "Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization", "comment": "10 pages", "summary": "In this work, we propose a division-and-summarization (DaS) framework for\ndense video captioning. After partitioning each untrimmed long video as\nmultiple event proposals, where each event proposal consists of a set of short\nvideo segments, we extract visual feature (e.g., C3D feature) from each segment\nand use the existing image/video captioning approach to generate one sentence\ndescription for this segment. Considering that the generated sentences contain\nrich semantic descriptions about the whole event proposal, we formulate the\ndense video captioning task as a visual cue aided sentence summarization\nproblem and propose a new two stage Long Short Term Memory (LSTM) approach\nequipped with a new hierarchical attention mechanism to summarize all generated\nsentences as one descriptive sentence with the aid of visual features.\nSpecifically, the first-stage LSTM network takes all semantic words from the\ngenerated sentences and the visual features from all segments within one event\nproposal as the input, and acts as the encoder to effectively summarize both\nsemantic and visual information related to this event proposal. The\nsecond-stage LSTM network takes the output from the first-stage LSTM network\nand the visual features from all video segments within one event proposal as\nthe input, and acts as the decoder to generate one descriptive sentence for\nthis event proposal. Our comprehensive experiments on the ActivityNet Captions\ndataset demonstrate the effectiveness of our newly proposed DaS framework for\ndense video captioning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u4e0e\u603b\u7ed3\uff08DaS\uff09\u7684\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5LSTM\u7f51\u7edc\u548c\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u63cf\u8ff0\u6027\u53e5\u5b50\u3002", "motivation": "\u89e3\u51b3\u672a\u4fee\u526a\u957f\u89c6\u9891\u7684\u5bc6\u96c6\u5b57\u5e55\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5272\u89c6\u9891\u4e3a\u4e8b\u4ef6\u63d0\u6848\u5e76\u5229\u7528\u89c6\u89c9\u7279\u5f81\u8f85\u52a9\u53e5\u5b50\u603b\u7ed3\u3002", "method": "1. \u5206\u5272\u89c6\u9891\u4e3a\u4e8b\u4ef6\u63d0\u6848\uff0c\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\u5e76\u751f\u6210\u53e5\u5b50\u63cf\u8ff0\uff1b2. \u4f7f\u7528\u4e24\u9636\u6bb5LSTM\u7f51\u7edc\uff08\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff09\u548c\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u603b\u7ed3\u53e5\u5b50\u3002", "result": "\u5728ActivityNet Captions\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86DaS\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "DaS\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u8f85\u52a9\u53e5\u5b50\u603b\u7ed3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u751f\u6210\u7684\u6548\u679c\u3002"}}
{"id": "2506.20583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20583", "abs": "https://arxiv.org/abs/2506.20583", "authors": ["Zhiwang Zhang", "Dong Xu", "Wanli Ouyang", "Luping Zhou"], "title": "Dense Video Captioning using Graph-based Sentence Summarization", "comment": "12 pages", "summary": "Recently, dense video captioning has made attractive progress in detecting\nand captioning all events in a long untrimmed video. Despite promising results\nwere achieved, most existing methods do not sufficiently explore the scene\nevolution within an event temporal proposal for captioning, and therefore\nperform less satisfactorily when the scenes and objects change over a\nrelatively long proposal. To address this problem, we propose a graph-based\npartition-and-summarization (GPaS) framework for dense video captioning within\ntwo stages. For the ``partition\" stage, a whole event proposal is split into\nshort video segments for captioning at a finer level. For the ``summarization\"\nstage, the generated sentences carrying rich description information for each\nsegment are summarized into one sentence to describe the whole event. We\nparticularly focus on the ``summarization\" stage, and propose a framework that\neffectively exploits the relationship between semantic words for summarization.\nWe achieve this goal by treating semantic words as nodes in a graph and\nlearning their interactions by coupling Graph Convolutional Network (GCN) and\nLong Short Term Memory (LSTM), with the aid of visual cues. Two schemes of\nGCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN\nand LSTM. The effectiveness of our approach is demonstrated via an extensive\ncomparison with the state-of-the-arts methods on the two benchmarks ActivityNet\nCaptions dataset and YouCook II dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5206\u533a\u4e0e\u603b\u7ed3\uff08GPaS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u533a\u548c\u603b\u7ed3\u4e24\u9636\u6bb5\u6539\u8fdb\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\uff0c\u5229\u7528GCN\u548cLSTM\u7ed3\u5408\u7684\u65b9\u6cd5\u4f18\u5316\u8bed\u4e49\u603b\u7ed3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u63a2\u7d22\u4e8b\u4ef6\u65f6\u95f4\u63d0\u6848\u5185\u7684\u573a\u666f\u6f14\u53d8\uff0c\u5bfc\u81f4\u5728\u573a\u666f\u548c\u5bf9\u8c61\u53d8\u5316\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5206\u533a\u9636\u6bb5\u5c06\u4e8b\u4ef6\u63d0\u6848\u62c6\u5206\u4e3a\u66f4\u77ed\u7684\u89c6\u9891\u7247\u6bb5\u8fdb\u884c\u63cf\u8ff0\uff1b\u603b\u7ed3\u9636\u6bb5\u901a\u8fc7GCN-LSTM\u4ea4\u4e92\u6a21\u5757\uff08GLI\uff09\u5c06\u7247\u6bb5\u63cf\u8ff0\u603b\u7ed3\u4e3a\u5b8c\u6574\u4e8b\u4ef6\u63cf\u8ff0\u3002", "result": "\u5728ActivityNet Captions\u548cYouCook II\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPaS\u6846\u67b6\u901a\u8fc7\u5206\u533a\u548c\u603b\u7ed3\u4e24\u9636\u6bb5\u6709\u6548\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u573a\u666f\u53d8\u5316\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.20638", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20638", "abs": "https://arxiv.org/abs/2506.20638", "authors": ["Cl\u00e9ment Forray", "Pauline Delporte", "Nicolas Delaygue", "Florence Genin", "Dawa Derksen"], "title": "Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects", "comment": "accepted for CVPR 2025 NFBCC workshop", "summary": "Obtaining a better knowledge of the current state and behavior of objects\norbiting Earth has proven to be essential for a range of applications such as\nactive debris removal, in-orbit maintenance, or anomaly detection. 3D models\nrepresent a valuable source of information in the field of Space Situational\nAwareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to\nperform 3D reconstruction of non-cooperative space objects from simulated\nimages. This scenario is challenging for NeRF models due to unusual camera\ncharacteristics and environmental conditions : mono-chromatic images, unknown\nobject orientation, limited viewing angles, absence of diffuse lighting etc. In\nthis work we focus primarly on the joint optimization of camera poses alongside\nthe NeRF. Our experimental results show that the most accurate 3D\nreconstruction is achieved when training with successive images one-by-one. We\nestimate camera poses by optimizing an uniform rotation and use regularization\nto prevent successive poses from being too far apart.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528NeRF\u6280\u672f\u4ece\u6a21\u62df\u56fe\u50cf\u4e2d\u91cd\u5efa\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u76843D\u6a21\u578b\uff0c\u91cd\u70b9\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4e0eNeRF\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8868\u660e\u9010\u5e27\u8bad\u7ec3\u80fd\u83b7\u5f97\u6700\u51c6\u786e\u76843D\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u4e86\u89e3\u5730\u7403\u8f68\u9053\u7269\u4f53\u7684\u72b6\u6001\u548c\u884c\u4e3a\u5bf9\u592a\u7a7a\u788e\u7247\u6e05\u9664\u3001\u5728\u8f68\u7ef4\u62a4\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c3D\u6a21\u578b\u662f\u592a\u7a7a\u6001\u52bf\u611f\u77e5\uff08SSA\uff09\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\u3002", "method": "\u4f7f\u7528NeRF\u6280\u672f\u4ece\u6a21\u62df\u56fe\u50cf\u4e2d\u91cd\u5efa3D\u6a21\u578b\uff0c\u91cd\u70b9\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4e0eNeRF\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u91c7\u7528\u9010\u5e27\u8bad\u7ec3\u548c\u6b63\u5219\u5316\u9632\u6b62\u76f8\u673a\u59ff\u6001\u53d8\u5316\u8fc7\u5927\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9010\u5e27\u8bad\u7ec3\u80fd\u83b7\u5f97\u6700\u51c6\u786e\u76843D\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4e0eNeRF\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u76843D\u91cd\u5efa\u7cbe\u5ea6\u3002"}}
{"id": "2506.20623", "categories": ["cs.LG", "cond-mat.dis-nn", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.20623", "abs": "https://arxiv.org/abs/2506.20623", "authors": ["Fariba Jangjoo", "Matteo Marsili", "Yasser Roudi"], "title": "Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning", "comment": "13 pages, 2 figures", "summary": "Closed-loop learning is the process of repeatedly estimating a model from\ndata generated from the model itself. It is receiving great attention due to\nthe possibility that large neural network models may, in the future, be\nprimarily trained with data generated by artificial neural networks themselves.\nWe study this process for models that belong to exponential families, deriving\nequations of motions that govern the dynamics of the parameters. We show that\nmaximum likelihood estimation of the parameters endows sufficient statistics\nwith the martingale property and that as a result the process converges to\nabsorbing states that amplify initial biases present in the data. However, we\nshow that this outcome may be prevented by polluting the data with an\ninfinitesimal fraction of data points generated from a fixed model, by relying\non maximum a posteriori estimation or by introducing regularisation.\nFurthermore, we show that the asymptotic behavior of the dynamics is not\nreparametrisation invariant.", "AI": {"tldr": "\u7814\u7a76\u4e86\u95ed\u73af\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u53c2\u6570\u52a8\u6001\u7684\u6536\u655b\u6027\u53ca\u5176\u5bf9\u521d\u59cb\u504f\u89c1\u7684\u653e\u5927\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u6b62\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u5927\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u672a\u6765\u53ef\u80fd\u4e3b\u8981\u4f9d\u8d56\u81ea\u8eab\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u95ed\u73af\u5b66\u4e60\u5bf9\u6a21\u578b\u53c2\u6570\u7684\u5f71\u54cd\u3002", "method": "\u9488\u5bf9\u6307\u6570\u65cf\u6a21\u578b\uff0c\u63a8\u5bfc\u53c2\u6570\u52a8\u6001\u7684\u8fd0\u52a8\u65b9\u7a0b\uff0c\u5206\u6790\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u6027\u8d28\u3002", "result": "\u53d1\u73b0\u95ed\u73af\u5b66\u4e60\u4f1a\u653e\u5927\u521d\u59cb\u504f\u89c1\uff0c\u4f46\u53ef\u901a\u8fc7\u6570\u636e\u6c61\u67d3\u3001\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\u6216\u6b63\u5219\u5316\u907f\u514d\u3002", "conclusion": "\u95ed\u73af\u5b66\u4e60\u7684\u6e10\u8fdb\u884c\u4e3a\u4e0d\u6ee1\u8db3\u53c2\u6570\u5316\u4e0d\u53d8\u6027\uff0c\u9700\u8c28\u614e\u8bbe\u8ba1\u8bad\u7ec3\u7b56\u7565\u3002"}}
