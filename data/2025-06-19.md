<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation](https://arxiv.org/abs/2506.14835)
*Kiet Dang Vu,Trung Thai Tran,Duc Dung Nguyen*

Main category: cs.CV

TL;DR: MonoVQD是一种新型框架，通过改进DETR架构，解决了单目3D检测中的关键问题，包括稳定性、梯度消失和查询质量优化。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测在直接应用DETR架构时存在性能限制，MonoVQD旨在解决这些问题。

Method: 提出Mask Separated Self-Attention机制、Variational Query Denoising技术和自蒸馏策略。

Result: 在KITTI和nuScenes数据集上表现优异，展示了强大的泛化能力。

Conclusion: MonoVQD通过创新方法显著提升了单目和多视角3D检测的性能。

Abstract: Precisely localizing 3D objects from a single image constitutes a central
challenge in monocular 3D detection. While DETR-like architectures offer a
powerful paradigm, their direct application in this domain encounters inherent
limitations, preventing optimal performance. Our work addresses these
challenges by introducing MonoVQD, a novel framework designed to fundamentally
advance DETR-based monocular 3D detection. We propose three main contributions.
First, we propose the Mask Separated Self-Attention mechanism that enables the
integration of the denoising process into a DETR architecture. This improves
the stability of Hungarian matching to achieve a consistent optimization
objective. Second, we present the Variational Query Denoising technique to
address the gradient vanishing problem of conventional denoising methods, which
severely restricts the efficiency of the denoising process. This explicitly
introduces stochastic properties to mitigate this fundamental limitation and
unlock substantial performance gains. Finally, we introduce a sophisticated
self-distillation strategy, leveraging insights from later decoder layers to
synergistically improve query quality in earlier layers, thereby amplifying the
iterative refinement process. Rigorous experimentation demonstrates that
MonoVQD achieves superior performance on the challenging KITTI monocular
benchmark. Highlighting its broad applicability, MonoVQD's core components
seamlessly integrate into other architectures, delivering significant
performance gains even in multi-view 3D detection scenarios on the nuScenes
dataset and underscoring its robust generalization capabilities.

</details>


### [2] [Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models](https://arxiv.org/abs/2506.15201)
*Xuelin Shen,Jiayin Xu,Kangsheng Yin,Wenhan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PSIC的图像压缩方法，通过多解码选项保护用户隐私，同时保留图像压缩功能。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言预训练模型（VLP）的语义理解能力提升，公开图像容易被搜索引擎等工具利用，因此需要保护用户隐私。

Method: 提出PSIC方法，结合Conditional Latent Trigger Generation（CLTG）模块和Uncertainty-Aware Encryption-Oriented（UAEO）优化函数，实现多解码选项和隐私保护。

Result: 实验证明PSIC在多种下游任务中有效，既能保护隐私，又能保持图像质量。

Conclusion: PSIC是一种即插即用的方法，可集成到现有图像压缩模型中，同时实现隐私保护和图像压缩功能。

Abstract: The improved semantic understanding of vision-language pretrained (VLP)
models has made it increasingly difficult to protect publicly posted images
from being exploited by search engines and other similar tools. In this
context, this paper seeks to protect users' privacy by implementing defenses at
the image compression stage to prevent exploitation. Specifically, we propose a
flexible coding method, termed Privacy-Shielded Image Compression (PSIC), that
can produce bitstreams with multiple decoding options. By default, the
bitstream is decoded to preserve satisfactory perceptual quality while
preventing interpretation by VLP models. Our method also retains the original
image compression functionality. With a customizable input condition, the
proposed scheme can reconstruct the image that preserves its full semantic
information. A Conditional Latent Trigger Generation (CLTG) module is proposed
to produce bias information based on customizable conditions to guide the
decoding process into different reconstructed versions, and an
Uncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed
to leverage the soft labels inferred from the target VLP model's uncertainty on
the training data. This paper further incorporates an adaptive multi-objective
optimization strategy to obtain improved encrypting performance and perceptual
quality simultaneously within a unified training process. The proposed scheme
is plug-and-play and can be seamlessly integrated into most existing Learned
Image Compression (LIC) models. Extensive experiments across multiple
downstream tasks have demonstrated the effectiveness of our design.

</details>


### [3] [AI-driven visual monitoring of industrial assembly tasks](https://arxiv.org/abs/2506.15285)
*Mattia Nardon,Stefano Messelodi,Antonio Granata,Fabio Poiesi,Alberto Danese,Davide Boscaini*

Main category: cs.CV

TL;DR: ViMAT是一种无需视觉标记或固定工作空间设置的AI驱动系统，用于实时监控工业装配任务，结合感知和推理模块，在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业装配任务的视觉监控对防止设备损坏和确保工人安全至关重要，但现有商业解决方案通常需要固定设置或视觉标记。

Method: ViMAT结合多视角视频流的感知模块和基于观察状态与任务知识的推理模块，实时推断装配动作。

Result: 在LEGO组件更换和液压模具重构任务中，ViMAT通过定量和定性分析验证了其有效性，尤其在部分和不确定视觉观察的复杂场景中。

Conclusion: ViMAT在无需额外约束条件下，成功实现了工业装配任务的实时监控，展示了其在实际应用中的潜力。

Abstract: Visual monitoring of industrial assembly tasks is critical for preventing
equipment damage due to procedural errors and ensuring worker safety. Although
commercial solutions exist, they typically require rigid workspace setups or
the application of visual markers to simplify the problem. We introduce ViMAT,
a novel AI-driven system for real-time visual monitoring of assembly tasks that
operates without these constraints. ViMAT combines a perception module that
extracts visual observations from multi-view video streams with a reasoning
module that infers the most likely action being performed based on the observed
assembly state and prior task knowledge. We validate ViMAT on two assembly
tasks, involving the replacement of LEGO components and the reconfiguration of
hydraulic press molds, demonstrating its effectiveness through quantitative and
qualitative analysis in challenging real-world scenarios characterized by
partial and uncertain visual observations. Project page:
https://tev-fbk.github.io/ViMAT

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen,Ruoxin Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 提出了一种结合BiGRU和多头注意力机制的混合模型（BiGRU-MHA），用于SSD健康状态预测，显著提高了分类准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: SSD健康状态预测对数据可靠性至关重要，但传统模型存在泛化能力不足的问题，需要一种更准确且稳定的方法。

Method: 模型结合BiGRU的双向时序建模和多头注意力机制，动态分配特征权重，提升对关键健康指标的敏感性。

Result: 在训练集和测试集上分别达到92.70%和92.44%的分类准确率，AUC为0.94，泛化能力优异。

Conclusion: 该模型为SSD健康预测提供了新技术方案，解决了传统模型的泛化瓶颈，具有实际应用价值，可降低数据丢失风险并优化维护成本。

Abstract: Aiming at the critical role of SSD health state prediction in data
reliability assurance, this study proposes a hybrid BiGRU-MHA model that
incorporates a multi-head attention mechanism to enhance the accuracy and
stability of storage device health classification. The model innovatively
integrates temporal feature extraction and key information focusing
capabilities. Specifically, it leverages the bidirectional timing modeling
advantages of the BiGRU network to capture both forward and backward
dependencies of SSD degradation features. Simultaneously, the multi-head
attention mechanism dynamically assigns feature weights, improving the model's
sensitivity to critical health indicators. Experimental results show that the
proposed model achieves classification accuracies of 92.70% on the training set
and 92.44% on the test set, with a minimal performance gap of only 0.26%,
demonstrating excellent generalization ability. Further analysis using the
receiver operating characteristic (ROC) curve shows an area under the curve
(AUC) of 0.94 on the test set, confirming the model's robust binary
classification performance. This work not only presents a new technical
approach for SSD health prediction but also addresses the generalization
bottleneck of traditional models, offering a verifiable method with practical
value for preventive maintenance of industrial-grade storage systems. The
results show the model can significantly reduce data loss risks by providing
early failure warnings and help optimize maintenance costs, supporting
intelligent decision-making in building reliable storage systems for cloud
computing data centers and edge storage environments.

</details>


### [5] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang,Boyu Wang,Bin Gu,Charles Ling*

Main category: cs.LG

TL;DR: 论文提出了一种事件驱动的在线垂直联邦学习（VFL）框架，解决了数据异步到达和非凸模型下的挑战，并通过动态局部遗憾（DLR）优化性能。


<details>
  <summary>Details</summary>
Motivation: 在线学习在VFL中更具适应性，但由于数据异步到达和非凸模型的复杂性，现有研究未能解决这些问题。

Method: 提出事件驱动的在线VFL框架，仅激活部分客户端，引入动态局部遗憾（DLR）处理非凸和非平稳环境。

Result: 实验表明，该框架在非平稳数据下更稳定，显著降低了通信和计算成本。

Conclusion: 该框架为在线VFL提供了高效且稳定的解决方案，填补了研究空白。

Abstract: Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [6] [Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings](https://arxiv.org/abs/2506.14900)
*Imane Guellil,Salomé Andres,Atul Anand,Bruce Guthrie,Huayu Zhang,Abul Hasan,Honghan Wu,Beatrice Alex*

Main category: cs.CL

TL;DR: 本文介绍了一个手动标注的老年患者出院摘要中不良事件（AE）提取的语料库，解决了临床NLP资源中老年人群代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 老年患者在临床NLP资源中代表性不足，需要专门的数据集来支持不良事件提取研究。

Method: 构建了一个包含14种临床显著不良事件及其上下文属性的数据集，支持不连续和重叠实体标注。使用FlairNLP评估了多种模型在不同标注粒度下的性能。

Result: 基于Transformer的模型在文档级粗粒度提取上表现优异（F1=0.943），但在细粒度实体级任务上表现显著下降（F1=0.675），尤其是罕见事件和复杂属性。

Conclusion: 尽管整体性能较高，但在检测代表性不足的不良事件和捕捉临床语言细微差别方面仍存在挑战。数据集可通过DataLoch请求获取，为未来研究提供了基准。

Abstract: In this work, we present a manually annotated corpus for Adverse Event (AE)
extraction from discharge summaries of elderly patients, a population often
underrepresented in clinical NLP resources. The dataset includes 14 clinically
significant AEs-such as falls, delirium, and intracranial haemorrhage, along
with contextual attributes like negation, diagnosis type, and in-hospital
occurrence. Uniquely, the annotation schema supports both discontinuous and
overlapping entities, addressing challenges rarely tackled in prior work. We
evaluate multiple models using FlairNLP across three annotation granularities:
fine-grained, coarse-grained, and coarse-grained with negation. While
transformer-based models (e.g., BERT-cased) achieve strong performance on
document-level coarse-grained extraction (F1 = 0.943), performance drops
notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly
for rare events and complex attributes. These results demonstrate that despite
high-level scores, significant challenges remain in detecting underrepresented
AEs and capturing nuanced clinical language. Developed within a Trusted
Research Environment (TRE), the dataset is available upon request via DataLoch
and serves as a robust benchmark for evaluating AE extraction methods and
supporting future cross-dataset generalisation.

</details>


### [7] [Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods](https://arxiv.org/abs/2506.15030)
*Drew Walker,Swati Rajwal,Sudeshna Das,Snigdha Peddireddy,Abeed Sarker*

Main category: cs.CL

TL;DR: 论文通过自然语言处理技术分析执法和法医叙述，识别社会孤立和孤独感，并开发高质量分类器，发现特定人群（如男性、同性恋、离婚者）更易受社会孤立影响。


<details>
  <summary>Details</summary>
Motivation: 近年来社会孤立和孤独感增加，与自杀率上升密切相关，但美国国家暴力死亡报告系统（NVDRS）未记录这些因素，因此需通过NLP技术从文本中识别。

Method: 使用主题建模生成词典，并结合监督学习分类器开发高质量分类器（平均F1：0.86，准确率：0.82），分析2002至2020年的30万例自杀案例。

Result: 识别出1,198例提及长期社会孤立的自杀案例，男性、同性恋和离婚者更易被分类为长期社会孤立。其他显著预测因素包括离婚、失去子女监护权、搬迁或分手。

Conclusion: 该方法可改进美国对社会孤立和孤独感的监测与预防。

Abstract: Social isolation and loneliness, which have been increasing in recent years
strongly contribute toward suicide rates. Although social isolation and
loneliness are not currently recorded within the US National Violent Death
Reporting System's (NVDRS) structured variables, natural language processing
(NLP) techniques can be used to identify these constructs in law enforcement
and coroner medical examiner narratives. Using topic modeling to generate
lexicon development and supervised learning classifiers, we developed
high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over
300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic
social isolation. Decedents had higher odds of chronic social isolation
classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =
3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).
We found significant predictors for other social isolation topics of recent or
impending divorce, child custody loss, eviction or recent move, and break-up.
Our methods can improve surveillance and prevention of social isolation and
loneliness in the United States.

</details>


### [8] [GenRecal: Generation after Recalibration from Large to Small Vision-Language Models](https://arxiv.org/abs/2506.15681)
*Byung-Kwan Lee,Ryo Hachiuma,Yong Man Ro,Yu-Chiang Frank Wang,Yueh-Hua Wu*

Main category: cs.CL

TL;DR: GenRecal是一种新型通用蒸馏框架，用于解决异构视觉语言模型（VLM）之间的知识转移问题，通过特征对齐和适配实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 由于大型VLM在资源受限设备上部署困难，且异构VLM架构多样，需要一种通用方法实现知识蒸馏。

Method: 提出GenRecal框架，包含Recalibrator模块，用于对齐和适配异构VLM的特征表示。

Result: 在多个基准测试中，GenRecal显著提升性能，优于大规模开源和闭源VLM。

Conclusion: GenRecal为异构VLM的知识蒸馏提供了一种通用且高效的解决方案。

Abstract: Recent advancements in vision-language models (VLMs) have leveraged large
language models (LLMs) to achieve performance on par with closed-source systems
like GPT-4V. However, deploying these models in real-world scenarios,
particularly on resource-constrained devices, remains challenging due to their
substantial computational demands. This has spurred interest in distilling
knowledge from large VLMs into smaller, more efficient counterparts. A key
challenge arises here from the diversity of VLM architectures, which are built
on different LLMs and employ varying token types-differing in vocabulary size,
token splits, and token index ordering. To address this challenge of limitation
to a specific VLM type, we present Generation after Recalibration (GenRecal), a
novel, general-purpose distillation framework for VLMs. GenRecal incorporates a
Recalibrator that aligns and adapts feature representations between
heterogeneous VLMs, enabling effective knowledge transfer across different
types of VLMs. Through extensive experiments on multiple challenging
benchmarks, we demonstrate that GenRecal significantly improves baseline
performances, eventually outperforming large-scale open- and closed-source
VLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [9] [Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation](https://arxiv.org/abs/2506.15150)
*Yuanlong Ji,Xingbang Yang,Ruoqi Zhao,Qihan Ye,Quan Zheng,Yubo Fan*

Main category: cs.RO

TL;DR: 提出了一种基于IMU信号的步态相位估计神经网络，结合时间卷积和Transformer层，通过通道掩码重建预训练策略提升模型泛化能力，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决步态相位估计在高精度和鲁棒性方面的挑战，特别是在地形变化时。

Method: 开发了一种基于隐式建模的神经网络，结合时间卷积和Transformer层，并提出通道掩码重建预训练策略。

Result: 在稳定地形和地形过渡条件下，步态相位RMSE和相位率MAE均优于基线方法，硬件验证进一步证实其可靠性。

Conclusion: 该方法为更智能和自适应的外骨骼系统铺平了道路，提升了人机交互的安全性和效率。

Abstract: Gait phase estimation based on inertial measurement unit (IMU) signals
facilitates precise adaptation of exoskeletons to individual gait variations.
However, challenges remain in achieving high accuracy and robustness,
particularly during periods of terrain changes. To address this, we develop a
gait phase estimation neural network based on implicit modeling of human
locomotion, which combines temporal convolution for feature extraction with
transformer layers for multi-channel information fusion. A channel-wise masked
reconstruction pre-training strategy is proposed, which first treats gait phase
state vectors and IMU signals as joint observations of human locomotion, thus
enhancing model generalization. Experimental results demonstrate that the
proposed method outperforms existing baseline approaches, achieving a gait
phase RMSE of $2.729 \pm 1.071%$ and phase rate MAE of $0.037 \pm 0.016%$ under
stable terrain conditions with a look-back window of 2 seconds, and a phase
RMSE of $3.215 \pm 1.303%$ and rate MAE of $0.050 \pm 0.023%$ under terrain
transitions. Hardware validation on a hip exoskeleton further confirms that the
algorithm can reliably identify gait cycles and key events, adapting to various
continuous motion scenarios. This research paves the way for more intelligent
and adaptive exoskeleton systems, enabling safer and more efficient human-robot
interaction across diverse real-world environments.

</details>
