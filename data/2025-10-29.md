<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo是一个长期、流式、全模态的基准测试，用于在现实日常场景中评估以自我为中心的AI助手，包含超过14小时/参与者的同步自我中心视频、音频和文本数据，涵盖12个诊断子任务和3,291个人工验证的QA项目。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常孤立评估AI助手的能力，缺乏真实的流式场景或仅支持短期任务，无法满足现实世界中对多模态输入、实时响应和长期记忆保持的需求。

Method: 构建包含工作与学习、生活方式与日常、社交活动、外出与文化四个领域的同步多模态数据集，所有数据在统一全局时间线上对齐，并通过人工精炼获得高质量视觉叙述和语音转录。定义了12个诊断子任务，涵盖记忆、理解和跨记忆推理三大核心能力。

Result: TeleEgo提供了3,291个人工验证的QA项目，涵盖单选、二元、多选和开放式多种问题格式，并在严格的流式设置下进行评估。提出了实时准确率和记忆持久时间两个关键指标。

Conclusion: TeleEgo为实用AI助手的发展提供了现实且全面的评估框架，能够同时评估正确性、时间响应性和长期记忆保持能力。

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [2] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA是一种单阶段的参数高效微调方法，通过非线性核函数扩展低秩分解和自适应双层稀疏分配机制，在降低内存使用的同时提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏调优方法存在两个问题：1）两阶段范式忽略了微调过程中的参数调整，限制了性能；2）由于需要存储所有权重矩阵，内存使用率高。

Method: 提出SNELLA单阶段方法：1）通过两个低秩可学习矩阵的合并来选择性更新权重矩阵，并引入非线性核函数防止权重更新间的相互依赖；2）提出自适应双层稀疏分配机制，基于重要性分数端到端地促进权重在层间和层内竞争。

Result: 在分类、分割和生成任务上的实验表明，SNELLA在低内存使用下达到SOTA性能：在FGVC基准上比SPT-LoRA提高1.8%的Top-1准确率；在86M到632M参数规模的模型上实现31.1%-39.9%的内存减少。

Conclusion: SNELLA通过单阶段设计和创新的稀疏分配机制，在保持高性能的同时显著降低了内存使用，为参数高效微调提供了有效解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [3] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出了首个基于事件相机的微眼动数据集，使用脉冲神经网络成功分类不同角位移的微眼动，准确率约90%。


<details>
  <summary>Details</summary>
Motivation: 传统微眼动研究方法成本高、可扩展性差，事件相机提供了高速、低延迟的替代方案，需要建立相关数据集支持认知计算研究。

Method: 使用Blender渲染高保真眼动场景，模拟0.5-2.0度角位移的微眼动，通过v2e转换为事件流，使用Spiking-VGG系列网络和光学流增强的Spiking-VGG16Flow进行分类。

Result: 模型平均准确率达到约90%，能够独立于事件数量或持续时间对微眼动进行角位移分类。

Conclusion: 证明了脉冲神经网络在精细运动识别中的潜力，为基于事件的视觉研究建立了基准，数据集和代码将公开。

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [4] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出了一种新型切换式双学生架构，通过选择最可靠的学生来增强协作，并引入损失感知指数移动平均策略来提升伪标签质量，在3D医学图像分割中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统师生框架在医学图像分割中受到师生网络强相关性和不可靠知识传递的限制，需要改进学习效果。

Method: 采用切换式双学生架构，迭代选择最可靠学生；引入损失感知指数移动平均策略，动态确保教师从学生吸收有意义信息。

Result: 在3D医学图像分割数据集上广泛评估，优于当前最先进的半监督方法，在有限监督下显著提升分割精度。

Conclusion: 该即插即用框架通过增强师生协作和提升伪标签质量，有效解决了半监督医学图像分割中的知识传递不可靠问题。

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [5] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: PathoGaze1.0是一个全面的行为数据集，记录了病理学家在癌症诊断过程中完整的视觉搜索和决策过程，包括眼动追踪、鼠标交互和诊断决策数据。


<details>
  <summary>Details</summary>
Motivation: 病理学家对千兆像素全切片图像的解释准确率平均只有70%，且增加第二位病理学家并不能显著改善决策一致性。该领域缺乏足够的行为数据来解释诊断错误和不一致性。

Method: 使用PTAH测试平台收集了19位病理学家解读397张WSIs的数据，包括18.69小时的眼动追踪、鼠标交互、刺激追踪、视口导航和诊断决策数据。

Result: 总共记录了171,909次注视、263,320次扫视和1,867,362次鼠标交互事件，创建了一个全面的行为数据集。

Conclusion: 这些数据可用于改善病理学家和AI系统的训练，支持人类专家。所有实验已预注册，完整数据集和分析代码已公开。

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [NUM2EVENT: Interpretable Event Reasoning from Numerical time-series](https://arxiv.org/abs/2510.23630)
*Ninghui Feng,Yiyan Qi*

Main category: cs.LG

TL;DR: 该论文提出了数字到事件推理任务，通过多阶段微调框架从数值时间序列中推断结构化事件，显著优于现有LLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在数值时间序列理解方面有限，主要关注预测或趋势描述，未能揭示驱动数值变化的潜在事件或解释推理过程。

Method: 提出推理感知框架，包含代理引导事件提取器、基于标记多元Hawkes的合成生成器，以及结合时间序列编码器和结构化解码器的两阶段微调流程。

Result: 在多领域数据集上的实验表明，该方法在事件级精度和召回率方面显著优于强LLM基线。

Conclusion: 为连接定量推理和语义理解开辟了新方向，使LLM能够直接从数值动态中解释和预测事件。

Abstract: Large language models (LLMs) have recently demonstrated impressive multimodal
reasoning capabilities, yet their understanding of purely numerical time-series
signals remains limited. Existing approaches mainly focus on forecasting or
trend description, without uncovering the latent events that drive numerical
changes or explaining the reasoning process behind them. In this work, we
introduce the task of number-to-event reasoning and decoding, which aims to
infer interpretable structured events from numerical inputs, even when current
text is unavailable. To address the data scarcity and semantic alignment
challenges, we propose a reasoning-aware framework that integrates an
agent-guided event extractor (AGE), a marked multivariate Hawkes-based
synthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a
time-series encoder with a structured decoder. Our model explicitly reasons
over numerical changes, generates intermediate explanations, and outputs
structured event hypotheses. Experiments on multi-domain datasets show that our
method substantially outperforms strong LLM baselines in event-level precision
and recall. These results suggest a new direction for bridging quantitative
reasoning and semantic understanding, enabling LLMs to explain and predict
events directly from numerical dynamics.

</details>


### [7] [Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization](https://arxiv.org/abs/2510.23667)
*Amin Heyrani Nobari,Lyle Regenwetter,Cyril Picard,Ligong Han,Faez Ahmed*

Main category: cs.LG

TL;DR: OAT是一个基于基础模型的拓扑优化框架，能够直接预测任意长宽比、分辨率、体积分数、载荷和固定条件下的最小柔度布局，相比现有方法在多个基准测试中显著降低柔度，并在单GPU上实现亚秒级推理。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习拓扑优化方法局限于固定方形网格、少量手动编码的边界条件和后处理优化，无法实现通用部署。

Method: OAT结合了分辨率无关和形状无关的自编码器、隐式神经场解码器以及条件潜在扩散模型，在包含220万个优化结构的OpenTO数据集上训练。

Result: 在四个公共基准和两个未见测试中，OAT相比最佳现有模型将平均柔度降低高达90%，在64×64到256×256分辨率和高达10:1的长宽比下实现亚秒级推理。

Conclusion: OAT为物理感知拓扑优化提供了一个通用、快速且分辨率无关的框架，并为逆向设计中的生成建模研究提供了大规模数据集。

Abstract: Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.

</details>


### [8] [Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents](https://arxiv.org/abs/2510.23682)
*Gokturk Aytug Akarlar*

Main category: cs.LG

TL;DR: Chimera是一个神经-符号-因果架构，集成了LLM策略师、形式化验证的符号约束引擎和因果推理模块，在电子商务环境中实现了稳定可靠的自决策，显著优于仅使用LLM或LLM加符号约束的基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自主决策代理存在灾难性脆弱性，相同能力仅因提示框架不同就会产生截然不同的结果，这在高风险领域部署时存在严重风险。

Method: 提出Chimera架构，整合三个互补组件：LLM策略师用于决策制定，形式化验证的符号约束引擎确保安全约束，因果推理模块进行反事实推理。在包含价格弹性、信任动态和季节性需求的电子商务环境中进行52周模拟测试。

Result: 在偏向销量或利润优化的组织偏见下，仅使用LLM的代理会灾难性失败（销量场景损失99K美元）或破坏品牌信任（利润场景下降48.6%）。添加符号约束可防止灾难但仅达到Chimera利润的43-87%。Chimera始终提供最高回报（152万美元和196万美元，某些情况达220万美元），同时提升品牌信任（+1.8%和+10.8%，某些情况+20.86%）。

Conclusion: 架构设计而非提示工程决定了自主代理在生产环境中的可靠性。TLA+形式化验证证明所有场景下零约束违规。

Abstract: Large language models show promise as autonomous decision-making agents, yet
their deployment in high-stakes domains remains fraught with risk. Without
architectural safeguards, LLM agents exhibit catastrophic brittleness:
identical capabilities produce wildly different outcomes depending solely on
prompt framing. We present Chimera, a neuro-symbolic-causal architecture that
integrates three complementary components - an LLM strategist, a formally
verified symbolic constraint engine, and a causal inference module for
counterfactual reasoning. We benchmark Chimera against baseline architectures
(LLM-only, LLM with symbolic constraints) across 52-week simulations in a
realistic e-commerce environment featuring price elasticity, trust dynamics,
and seasonal demand. Under organizational biases toward either volume or margin
optimization, LLM-only agents fail catastrophically (total loss of \$99K in
volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding
symbolic constraints prevents disasters but achieves only 43-87% of Chimera's
profit. Chimera consistently delivers the highest returns (\$1.52M and \$1.96M
respectively, some cases +\$2.2M) while improving brand trust (+1.8% and
+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+
formal verification proves zero constraint violations across all scenarios.
These results establish that architectural design not prompt engineering
determines the reliability of autonomous agents in production environments. We
provide open-source implementations and interactive demonstrations for
reproducibility.

</details>


### [9] [Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction](https://arxiv.org/abs/2510.23794)
*Jun Liu,Tao Zhou,Jiarui Li,Xiaohui Zhong,Peng Zhang,Jie Feng,Lei Chen,Hao Li*

Main category: cs.LG

TL;DR: FuXi-ENS是一种基于AI的台风集合预报系统，相比传统ECMWF-ENS在台风路径预报和物理变量预测方面表现更优，但在强度预报上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 传统台风集合预报系统计算成本高且难以完全表征大气非线性特征，需要开发更高效的AI替代方案。

Method: FuXi-ENS采用可学习扰动方案生成集合预报，并与ECMWF-ENS在2018年全球90个台风上进行系统性比较。

Result: FuXi-ENS在台风相关物理变量预测和路径预报方面优势明显，集合离散度更小，但强度预报仍偏低；能更好地捕捉大尺度环流和暖核周围的湿度湍流能量分布。

Conclusion: 可学习扰动方案有潜力提升台风预报技能，为基于AI的极端天气集合预报提供了重要见解。

Abstract: Tropical cyclones (TCs) are highly destructive and inherently uncertain
weather systems. Ensemble forecasting helps quantify these uncertainties, yet
traditional systems are constrained by high computational costs and limited
capability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a
learnable perturbation scheme for ensemble generation, representing a novel
AI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with
ECMWF-ENS using all 90 global TCs in 2018, examining their performance in
TC-related physical variables, track and intensity forecasts, and the
associated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear
advantages in predicting TC-related physical variables, and achieves more
accurate track forecasts with reduced ensemble spread, though it still
underestimates intensity relative to observations. Further dynamical and
thermodynamical analyses reveal that FuXi-ENS better captures large-scale
circulation, with moisture turbulent energy more tightly concentrated around
the TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.
These findings highlight the potential of learnable perturbations to improve TC
forecasting skill and provide valuable insights for advancing AI-based ensemble
prediction of extreme weather events that have significant societal impacts.

</details>


### [10] [GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA](https://arxiv.org/abs/2510.23868)
*Zhichao Wang*

Main category: cs.LG

TL;DR: GIFT是一种新颖的强化学习框架，通过最小化隐式和显式奖励模型之间的差异来对齐LLMs，将复杂的奖励最大化问题转化为简单的MSE损失，具有更快的收敛速度和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法如PPO、DPO等存在各种限制：PPO直接最大化累积奖励但训练不稳定，DPO使用离线数据缺乏探索能力，GRPO需要较多超参数且容易过拟合。需要一种既能保持在线探索能力，又具有稳定训练和良好泛化性能的方法。

Method: 结合了GRPO的多响应生成和归一化、DPO的隐式奖励公式化、UNA的隐式-显式奖励对齐原则，通过联合归一化隐式和显式奖励，将优化问题转化为凸的MSE损失函数。

Result: 在数学基准测试中实现了优越的推理和对齐性能，计算效率高，收敛更快，泛化能力更强，训练过拟合显著减少。

Conclusion: GIFT通过创新的隐式-显式奖励对齐框架，成功解决了现有方法在稳定性、探索能力和泛化性能方面的局限性，为LLM对齐提供了一种高效可靠的解决方案。

Abstract: I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine
\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning
LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT
minimizes the discrepancy between implicit and explicit reward models. It
combines three key ideas: (1) the online multi-response generation and
normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the
implicit-explicit reward alignment principle of UNA. By jointly normalizing the
implicit and explicit rewards, GIFT eliminates an otherwise intractable term
that prevents effective use of implicit rewards. This normalization transforms
the complex reward maximization objective into a simple mean squared error
(MSE) loss between the normalized reward functions, converting a non-convex
optimization problem into a convex, stable, and analytically differentiable
formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy
and thus retains exploration capability. Compared to GRPO, it requires fewer
hyperparameters, converges faster, and generalizes better with significantly
reduced training overfitting. Empirically, GIFT achieves superior reasoning and
alignment performance on mathematical benchmarks while remaining
computationally efficient.

</details>


### [11] [Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling](https://arxiv.org/abs/2510.23977)
*Yohan Abeysinghe,Muhammad Akhtar Munir,Sanoojan Baliah,Ron Sarafian,Fahad Shahbaz Khan,Yinon Rudich,Salman Khan*

Main category: cs.LG

TL;DR: SynCast是一个高分辨率神经预测模型，通过整合气象和空气成分数据来改进颗粒物浓度的预测，特别是在极端污染事件中表现优异。


<details>
  <summary>Details</summary>
Motivation: 空气污染是全球健康和环境的主要风险，现有模型往往低估罕见但危险的污染事件，需要更准确预测颗粒物浓度以支持及时的公共卫生预警和干预。

Method: 基于区域适应的transformer架构，结合扩散基随机精炼模块，利用ERA5和CAMS数据集，采用领域感知目标和极值理论指导的损失函数。

Result: 在多个颗粒物变量（PM1、PM2.5、PM10）预测上表现出显著改进，特别是在极端条件下，在不影响全局准确性的前提下显著提高了高影响区域的性能。

Conclusion: SynCast为下一代空气质量早期预警系统提供了可扩展的基础，支持脆弱区域的气候健康风险缓解。

Abstract: Air pollution remains a leading global health and environmental risk,
particularly in regions vulnerable to episodic air pollution spikes due to
wildfires, urban haze and dust storms. Accurate forecasting of particulate
matter (PM) concentrations is essential to enable timely public health warnings
and interventions, yet existing models often underestimate rare but hazardous
pollution events. Here, we present SynCast, a high-resolution neural
forecasting model that integrates meteorological and air composition data to
improve predictions of both average and extreme pollution levels. Built on a
regionally adapted transformer backbone and enhanced with a diffusion-based
stochastic refinement module, SynCast captures the nonlinear dynamics driving
PM spikes more accurately than existing approaches. Leveraging on harmonized
ERA5 and CAMS datasets, our model shows substantial gains in forecasting
fidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),
especially under extreme conditions. We demonstrate that conventional loss
functions underrepresent distributional tails (rare pollution events) and show
that SynCast, guided by domain-aware objectives and extreme value theory,
significantly enhances performance in highly impacted regions without
compromising global accuracy. This approach provides a scalable foundation for
next-generation air quality early warning systems and supports climate-health
risk mitigation in vulnerable regions.

</details>


### [12] [HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing](https://arxiv.org/abs/2510.23980)
*Guojing Cong,Tom Potok,Hamed Poursiami,Maryam Parsa*

Main category: cs.LG

TL;DR: HDGC算法将图卷积与超维计算中的绑定和捆绑操作相结合，在转导图学习中表现出色，在准确性和速度方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 结合图神经网络和超维计算的优势，开发一种既准确又高效的图学习方法，特别是在能效方面具有潜力。

Method: 将图卷积与超维计算中的绑定和捆绑操作相结合，主要操作在二值向量上进行。

Result: 在准确率上优于主流图神经网络和超维计算方法；在相同GPU平台上，比GCNII快9561倍，比HDGL快144.5倍。

Conclusion: HDGC在准确性和效率方面都表现出色，特别适合在神经形态和内存计算设备上实现优异的能效表现。

Abstract: We present a novel algorithm, \hdgc, that marries graph convolution with
binding and bundling operations in hyperdimensional computing for transductive
graph learning. For prediction accuracy \hdgc outperforms major and popular
graph neural network implementations as well as state-of-the-art
hyperdimensional computing implementations for a collection of homophilic
graphs and heterophilic graphs. Compared with the most accurate learning
methodologies we have tested, on the same target GPU platform, \hdgc is on
average 9561.0 and 144.5 times faster than \gcnii, a graph neural network
implementation and HDGL, a hyperdimensional computing implementation,
respectively. As the majority of the learning operates on binary vectors, we
expect outstanding energy performance of \hdgc on neuromorphic and emerging
process-in-memory devices.

</details>


### [13] [Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction](https://arxiv.org/abs/2510.24240)
*Edward Markai,Sina Molavipour*

Main category: cs.LG

TL;DR: 本文扩展了基于规则的TLogic框架，通过引入实体类别来限制规则应用范围，提高预测准确性和可解释性。当类别未知时，使用LLM方法生成类别，并研究了类别预测中的聚合方法选择。


<details>
  <summary>Details</summary>
Motivation: 现有的时序知识图谱预测方法多为基于嵌入的黑盒模型，缺乏可解释性。本文旨在结合高准确率和可解释预测，提供透明性让用户能够评估预测阶段应用的规则。

Method: 扩展TLogic规则框架，新规则格式将实体类别作为关键组件，限制规则仅应用于相关实体。当类别未知时，采用基于LLM的数据驱动方法生成类别，并研究类别预测中的实体得分聚合方法选择。

Result: 该方法在保持高准确率的同时提供了可解释的预测，通过实体类别限制提高了规则应用的针对性。

Conclusion: 提出的扩展TLogic框架成功结合了预测准确性和可解释性，实体类别的引入增强了规则应用的精确性，LLM方法有效解决了类别未知的问题，为时序知识图谱预测提供了透明且有效的解决方案。

Abstract: Temporal Knowledge Graphs have emerged as a powerful way of not only modeling
static relationships between entities but also the dynamics of how relations
evolve over time. As these informational structures can be used to store
information from a real-world setting, such as a news flow, predicting future
graph components to a certain extent equates predicting real-world events. Most
of the research in this field focuses on embedding-based methods, often
leveraging convolutional neural net architectures. These solutions act as black
boxes, limiting insight. In this paper, we explore an extension to an
established rule-based framework, TLogic, that yields a high accuracy in
combination with explainable predictions. This offers transparency and allows
the end-user to critically evaluate the rules applied at the end of the
prediction stage. The new rule format incorporates entity category as a key
component with the purpose of limiting rule application only to relevant
entities. When categories are unknown for building the graph, we propose a
data-driven method to generate them with an LLM-based approach. Additionally,
we investigate the choice of aggregation method for scores of retrieved
entities when performing category prediction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 该研究提出了一种条件深度学习模型，通过结合个体的社会基础设施韧性(SIR)和空间上下文，来预测破坏性事件后个体移动模式的变化。


<details>
  <summary>Details</summary>
Motivation: 预测破坏性事件前个体移动模式的变化具有挑战性，因为缺乏衡量个体异质性社会基础设施韧性的指标，常用特征在规模上受限，个体移动与空间环境的复杂交互未被充分捕捉，以及个体级移动数据稀疏且不适合传统预测方法。

Method: 开发了一个条件深度学习模型，将个体的社会基础设施韧性(SIR)纳入模型，利用大规模稀疏个体级数据捕捉个体移动模式与局部空间环境之间的复杂关系。

Result: 实验表明，结合个体的SIR和空间上下文可以增强模型预测事件后个体移动模式的能力。条件模型能够捕捉到具有相似事件前移动模式但SIR不同的个体在移动模式上的差异变化。

Conclusion: 该研究证明了将个体社会基础设施韧性纳入深度学习模型对于预测破坏性事件后个体移动模式变化的重要性，为理解个体在危机中的行为变化提供了新视角。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [15] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: LLMLogAnalyzer是一个基于聚类的日志分析聊天机器人，结合大语言模型和机器学习算法，简化日志分析流程，在多个任务上比现有LLM聊天机器人性能提升39%-68%。


<details>
  <summary>Details</summary>
Motivation: 系统日志是网络安全的核心，但分析大量多样化日志数据面临高成本、缺乏专业知识和时间限制等挑战，使得许多组织难以进行基本分析。

Method: 采用模块化架构，包括路由器、日志识别器、日志解析器和搜索工具，通过聚类方法解决LLM的上下文窗口限制和结构化文本处理能力差的问题。

Result: 在四个不同领域日志和各种任务上的评估显示，相比ChatGPT、ChatPDF和NotebookLM等最先进的LLM聊天机器人，性能显著提升39%-68%，鲁棒性增强，使用ROUGE-1分数时四分位距减少93%。

Conclusion: 该框架通过增强LLM在结构化文本分析方面的能力，提高了准确性和鲁棒性，为网络安全专家和非技术用户提供了有价值的资源。

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [16] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 比较经典模型和机器学习模型在电动汽车跟车行为预测中的表现，发现随机森林模型在所有场景下都优于物理模型。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，需要理解其驾驶行为以提高交通安全和开发智能驾驶系统。

Method: 使用经典模型（IDM、OVM、OVRV、CACC）和随机森林回归器，基于真实世界数据集进行校准和预测。

Result: 随机森林模型表现最佳，RMSE分别为0.0046（中等间距）、0.0016（长间距）和0.0025（超长间距）；经典模型中CACC表现最好，长间距RMSE为2.67。

Conclusion: 机器学习模型在预测电动汽车跟车行为方面优于物理模型，对模拟电动汽车行为和混合自动驾驶交通分析具有重要价值。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [17] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: 提出了UniPlanner，首个用于自动驾驶决策的多数据集集成规划框架，通过三个创新技术实现跨数据集统一学习，解决了现有方法局限于单数据集训练的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法局限于单数据集训练，限制了规划鲁棒性。研究发现不同数据集中的车辆轨迹分布和历史-未来相关性具有显著一致性，这为多数据集集成提供了基础。

Method: 1. 历史-未来轨迹字典网络(HFTDN)：聚合多数据集的历史-未来轨迹对，基于历史轨迹相似性检索相关未来轨迹生成跨数据集规划指导
2. 无梯度轨迹映射器(GFTM)：从多数据集学习鲁棒的历史-未来相关性，将历史轨迹转换为通用规划先验
3. 稀疏到密集(S2D)范式：训练时自适应dropout选择性抑制规划先验，推理时充分利用先验最大化规划性能

Result: UniPlanner实现了跨数据集统一学习，通过多数据集集成显著提升了规划鲁棒性，同时防止了捷径学习，确保规划知识的安全迁移。

Conclusion: 该研究证明了多数据集集成在自动驾驶规划中的可行性和有效性，UniPlanner框架为构建更鲁棒的自动驾驶决策系统提供了新思路。

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [18] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一个用于判断性预测的多智能体框架，通过不同智能体对声明真实性进行辩论并生成证据，使用定量双极论证框架(QBAFs)表示，结合多种基于LLM的智能体方法，实验表明多智能体组合能提高预测准确性并提供可解释的证据组合。


<details>
  <summary>Details</summary>
Motivation: 将判断性预测视为声明验证任务，需要评估未来事件的可能性。现有方法在证据收集和验证方面存在局限，需要更全面和可解释的框架来整合不同来源的证据和观点。

Method: 提出多智能体声明验证框架，包含三种基于LLM的智能体：(1)ArgLLM智能体生成和评估QBAFs；(2)RbAM智能体从外部源进行关系型论证挖掘；(3)RAG-ArgLLM智能体结合检索增强生成技术。使用2-3个智能体组合，基于6种不同基础LLM进行实验。

Result: 在标准判断性预测数据集上的实验表明，智能体组合（特别是三个智能体）能够提高预测准确性，同时提供可解释的证据组合用于声明验证。

Conclusion: 多智能体框架通过整合不同来源的证据和观点，有效提高了判断性预测的准确性，并为声明验证提供了可解释的论证过程。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [19] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 该论文提出了改进脉冲神经网络在强化学习中训练性能的方法，通过分析替代梯度斜率设置和引入特权引导策略，在真实世界无人机控制任务中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在复杂控制任务中面临的两个关键挑战：非可微脉冲神经元需要替代梯度但优化特性不明确，以及SNN状态动态需要序列训练但在强化学习中早期训练序列长度有限。

Method: 系统分析替代梯度斜率设置，提出使用特权引导策略来引导学习过程，同时利用脉冲策略的在线环境交互，并结合自适应斜率调度。

Result: 在强化学习设置中，较浅斜率或调度斜率使训练和最终部署性能提高2.1倍。在真实世界无人机位置控制任务中，平均回报达到400分，显著优于行为克隆和TD3BC等现有技术（最多-200分）。

Conclusion: 这项工作推进了对SNN中替代梯度学习的理论理解，并为神经形态控制器的实际训练方法提供了创新，在真实世界机器人系统中得到验证。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [20] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 提出两种架构模式：DOM转换模式处理网页复杂性，将冗长DOM转换为紧凑的任务相关表示；超媒体功能识别模式使代理能动态发现和集成未知Web服务能力。


<details>
  <summary>Details</summary>
Motivation: 解决软件代理从结构化数据构建可操作内部世界模型的两个关键挑战：原始HTML的冗长性使基础模型难以直接处理，以及硬编码API集成的静态性阻碍代理适应不断演化的服务。

Method: 引入结构化数据世界建模的模式语言，包含DOM转换模式和超媒体功能识别模式。DOM转换模式将原始DOM提炼为紧凑的世界模型；超媒体功能识别模式通过解析标准化语义描述来动态发现和集成Web服务能力。

Result: 这两种模式共同提供了一个稳健的框架，使代理能够高效构建和维护准确的世界模型。

Conclusion: 该模式语言为工程化代理提供了可扩展、自适应和可互操作的自动化框架，能够在Web及其扩展资源上实现高效的世界建模。

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages](https://arxiv.org/abs/2510.23896)
*Kosei Uemura,Miaoran Zhang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: AfriMTEB是一个针对非洲语言的多语言文本嵌入基准测试，包含59种语言、14个任务和38个数据集，填补了非洲语言在文本嵌入评估中的空白。同时提出了AfriE5模型，通过跨语言对比蒸馏方法在非洲语言上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言文本嵌入基准测试（如MMTEB）中非洲语言代表性不足，现有任务大多是从翻译基准（如FLORES聚类或SIB-200）重新调整用途而来，需要专门针对非洲语言的评估基准。

Method: 1. 构建AfriMTEB基准：覆盖59种非洲语言、14个任务和38个数据集，包括6个新数据集，涵盖14-56种语言，引入仇恨言论检测、意图检测和情感分类等新任务。2. 开发AfriE5模型：通过跨语言对比蒸馏方法，将指令调优的mE5模型适配到非洲语言。

Result: AfriE5模型在评估中表现出最先进的性能，超越了Gemini-Embeddings和mE5等强基线模型。

Conclusion: AfriMTEB为非洲语言文本嵌入提供了全面的评估基准，AfriE5模型通过跨语言对比蒸馏在非洲语言上取得了优异表现，推动了非洲语言NLP技术的发展。

Abstract: Text embeddings are an essential building component of several NLP tasks such
as retrieval-augmented generation which is crucial for preventing
hallucinations in LLMs. Despite the recent release of massively multilingual
MTEB (MMTEB), African languages remain underrepresented, with existing tasks
often repurposed from translation benchmarks such as FLORES clustering or
SIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB
covering 59 languages, 14 tasks, and 38 datasets, including six newly added
datasets. Unlike many MMTEB datasets that include fewer than five languages,
the new additions span 14 to 56 African languages and introduce entirely new
tasks, such as hate speech detection, intent detection, and emotion
classification, which were not previously covered. Complementing this, we
present AfriE5, an adaptation of the instruction-tuned mE5 model to African
languages through cross-lingual contrastive distillation. Our evaluation shows
that AfriE5 achieves state-of-the-art performance, outperforming strong
baselines such as Gemini-Embeddings and mE5.

</details>


### [22] [ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?](https://arxiv.org/abs/2510.24591)
*Christine Ye,Sihan Yuan,Suchetha Cooray,Steven Dillmann,Ian L. V. Roque,Dalya Baron,Philipp Frank,Sergio Martin-Alvarez,Nolan Koblischke,Frank J Qu,Diyi Yang,Risa Wechsler,Ioana Ciuca*

Main category: cs.CL

TL;DR: ReplicationBench是一个评估AI代理作为科研助手能力的框架，通过让AI复制天体物理学论文来测试其工作忠实性和正确性。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI代理在科研助手中的潜力增加，需要评估其在开放研究工作中的基础忠实性和正确性。

Method: 将天体物理学论文分解为任务，要求AI代理复制论文的核心贡献，包括实验设置、推导、数据分析和代码库，每个任务都与原论文作者共同开发。

Result: ReplicationBench对当前前沿语言模型极具挑战性：即使表现最好的语言模型得分也低于20%。

Conclusion: ReplicationBench建立了首个论文规模、专家验证的天体物理学研究任务基准，揭示了AI代理在科学研究中的多种失败模式，并为衡量AI代理在科学研究中的可靠性提供了可扩展框架。

Abstract: Frontier AI agents show increasing promise as scientific research assistants,
and may eventually be useful for extended, open-ended research workflows.
However, in order to use agents for novel research, we must first assess the
underlying faithfulness and correctness of their work. To evaluate agents as
research assistants, we introduce ReplicationBench, an evaluation framework
that tests whether agents can replicate entire research papers drawn from the
astrophysics literature. Astrophysics, where research relies heavily on
archival data and computational study while requiring little real-world
experimentation, is a particularly useful testbed for AI agents in scientific
research. We split each paper into tasks which require agents to replicate the
paper's core contributions, including the experimental setup, derivations, data
analysis, and codebase. Each task is co-developed with the original paper
authors and targets a key scientific result, enabling objective evaluation of
both faithfulness (adherence to original methods) and correctness (technical
accuracy of results). ReplicationBench is extremely challenging for current
frontier language models: even the best-performing language models score under
20%. We analyze ReplicationBench trajectories in collaboration with domain
experts and find a rich, diverse set of failure modes for agents in scientific
research. ReplicationBench establishes the first benchmark of paper-scale,
expert-validated astrophysics research tasks, reveals insights about agent
performance generalizable to other domains of data-driven science, and provides
a scalable framework for measuring AI agents' reliability in scientific
research.

</details>


### [23] [ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization](https://arxiv.org/abs/2510.24592)
*Guoxin Chen,Jing Wu,Xinjie Chen,Wayne Xin Zhao,Ruihua Song,Chengxi Li,Kai Fan,Dayiheng Liu,Minpeng Liao*

Main category: cs.CL

TL;DR: ReForm是一种反射式自动形式化方法，通过将语义一致性评估整合到形式化过程中，使模型能够迭代生成形式化语句、评估语义保真度并通过渐进优化自我纠正错误。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽然能生成语法正确的形式化语句，但往往无法保持原始问题的语义意图，这源于将自动形式化视为简单翻译任务而缺乏人类专家自然使用的自我反思和迭代优化机制。

Method: 提出ReForm反射式自动形式化方法，结合前瞻有界序列优化(PBSO)训练，在不同序列位置使用不同奖励来确保模型既发展准确的自动形式化能力，又进行正确的语义验证。

Result: 在四个自动形式化基准测试中，ReForm相比最强基线平均提升了17.2个百分点。还引入了包含859个专家标注项的ConsistencyCheck基准，验证了LLM作为评判者的可靠性。

Conclusion: 自动形式化本质上具有挑战性，即使人类专家也会在高达38.5%的情况下产生语义错误，而ReForm通过反射式方法显著提升了自动形式化的性能。

Abstract: Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 17.2 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.

</details>


### [24] [ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking](https://arxiv.org/abs/2510.24698)
*Baixuan Li,Dingchu Zhang,Jialong Wu,Wenbiao Yin,Zhengwei Tao,Yida Zhao,Liwen Zhang,Haiyang Shen,Runnan Fang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: ParallelMuse是一个两阶段范式，通过功能性指定部分展开和压缩推理聚合来解决并行思维在信息搜索代理中的效率低下和长时程推理整合困难问题。


<details>
  <summary>Details</summary>
Motivation: 传统并行思维在信息搜索代理中面临两个关键挑战：从头开始重复展开的低效率，以及由于上下文容量限制而难以在答案生成过程中整合长时程推理轨迹。

Method: 提出两阶段方法：第一阶段是功能性指定部分展开，将生成的序列划分为功能区域，进行不确定性引导的路径重用和分支以提升探索效率；第二阶段是压缩推理聚合，利用推理冗余无损压缩与答案推导相关的信息，并合成连贯的最终答案。

Result: 在多个开源代理和基准测试上的实验表明，性能提升高达62%，同时探索性token消耗减少10-30%。

Conclusion: ParallelMuse通过高效的探索策略和信息压缩机制，显著提升了信息搜索代理的问题解决能力。

Abstract: Parallel thinking expands exploration breadth, complementing the deep
exploration of information-seeking (IS) agents to further enhance
problem-solving capability. However, conventional parallel thinking faces two
key challenges in this setting: inefficiency from repeatedly rolling out from
scratch, and difficulty in integrating long-horizon reasoning trajectories
during answer generation, as limited context capacity prevents full
consideration of the reasoning process. To address these issues, we propose
ParallelMuse, a two-stage paradigm designed for deep IS agents. The first
stage, Functionality-Specified Partial Rollout, partitions generated sequences
into functional regions and performs uncertainty-guided path reuse and
branching to enhance exploration efficiency. The second stage, Compressed
Reasoning Aggregation, exploits reasoning redundancy to losslessly compress
information relevant to answer derivation and synthesize a coherent final
answer. Experiments across multiple open-source agents and benchmarks
demonstrate up to 62% performance improvement with a 10--30% reduction in
exploratory token consumption.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [RoboOmni: Proactive Robot Manipulation in Omni-modal Context](https://arxiv.org/abs/2510.23763)
*Siyin Wang,Jinlan Fu,Feihong Liu,Xinzhe He,Huangxuan Wu,Junhao Shi,Kexin Huang,Zhaoye Fei,Jingjing Gong,Zuxuan Wu,Yugang Jiang,See-Kiong Ng,Tat-Seng Chua,Xipeng Qiu*

Main category: cs.RO

TL;DR: 提出了RoboOmni框架，基于全模态LLM，通过融合听觉和视觉信号来推断用户意图，实现主动的机器人操作协作。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖显式指令，而真实人类交互中很少直接发出指令。需要机器人能够主动推断用户意图以实现有效协作。

Method: 提出Perceiver-Thinker-Talker-Executor框架，基于端到端全模态LLM，统一意图识别、交互确认和动作执行。通过时空融合听觉和视觉信号进行鲁棒的意图识别，支持直接语音交互。

Result: 在仿真和真实环境实验中，RoboOmni在成功率、推理速度、意图识别和主动协助方面超越了基于文本和ASR的基线方法。

Conclusion: RoboOmni框架通过跨模态上下文指令实现了主动的机器人意图识别和协作，为真实世界的人机交互提供了有效解决方案。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.

</details>
