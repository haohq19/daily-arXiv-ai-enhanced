<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 该研究提出了360度视频显著性预测的新方法，包括基于视觉Transformer的SalViT360模型和融合空间音频的SalViT360-AV模型，并创建了YT360-EyeTracking数据集，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 360度视频在VR中提供了全视场体验，但缺乏包含空间音频的综合性数据集，现有方法难以处理球面畸变和音频-视觉整合的复杂性。

Method: 创建了YT360-EyeTracking数据集（81个ODV视频），提出了SalViT360（基于视觉Transformer，具有球面几何感知的时空注意力层）和SalViT360-AV（进一步整合音频输入的Transformer适配器）两个新颖的显著性预测模型。

Result: 在多个基准数据集（包括自建的YT360-EyeTracking）上的实验结果表明，SalViT360和SalViT360-AV在预测360度场景中的观看者注意力方面显著优于现有方法。

Conclusion: 在模型架构中整合空间音频线索对于准确预测全向视频中的显著性至关重要，音频-视觉融合能有效提升显著性预测性能。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [2] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型的pipeline，能够在样本和数据集两个层面解释视觉模型的行为，帮助发现失败案例并理解模型趋势。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型开发主要关注性能指标（如准确率、IoU、mAP），而忽视了可解释性。现有xAI方法多为样本级解释，缺乏对模型在大型数据集上整体行为的理解，这对于防止偏见判断和识别模型模式至关重要。

Method: 利用视觉语言模型构建pipeline，在样本级和数据集级两个层面提供视觉模型的解释。该pipeline能够以最小工作量发现失败案例并获得对视觉模型的深入理解。

Result: 提出的pipeline能够有效解释视觉模型的行为，发现模型失败情况，并提供对模型趋势和模式的洞察。

Conclusion: 该方法将视觉模型开发与xAI分析相结合，推进了图像分析领域的发展，为解决视觉模型可解释性不足的问题提供了有效途径。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [3] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 医疗视觉-语言模型存在严重安全风险，为解决此问题研发了MedFoundationHub工具包，通过GUI界面和Docker部署确保隐私安全，并经过专家评估发现当前模型在病理诊断中存在偏离目标、推理模糊等限制


<details>
  <summary>Details</summary>
Motivation: 医疗视觉-语言模型在临床应用中带来严重安全风险，包括病人健康信息泄露、数据泄漏和网络安全脏点，尤其在医院环境中风险更高

Method: 研发MedFoundationHub GUI工具包：1)支持医生手动选择模型无需编程；2)提供插件式部署支持Hugging Face模型；3)通过Docker化部署确保隐私安全，仅需单卡A6000 GPU即可运行

Result: 评估5个先进VLM模型(Google-MedGemma3-4B、Qwen2-VL-7B-Instruct等)，通过负责病理学家进行1015次临床评分，测试治疗结肠病例和肾脏病例

Conclusion: 当前医疗VLM模型在病理诊断中存在显著限制：答案偏离目标、推理过于模糊、病理术语不一致，需要进一步改进；MedFoundationHub提供了安全可靠的部署方案

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [4] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出了一种延迟脉冲方法和时间依赖的积分发放(tdIF)神经元架构，解决了SNN在视觉检测任务中性能不佳的问题，在超低时间步长(5步内)下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前ANN-SNN转换方法在分类任务中表现优异，但在视觉检测任务中性能仍然不理想，主要由于异质脉冲模式导致的残留膜电位问题。

Method: 采用延迟脉冲方法缓解异质脉冲模式问题，并提出时间依赖的积分发放(tdIF)神经元架构，使IF神经元能够根据时间步长的时序动态调整积累和发放行为。

Result: 在目标检测和车道线检测两个关键视觉任务上进行了广泛评估，结果表明该方法超越了当前ANN-SNN转换方法，在超低延迟(5个时间步内)下实现了最先进的性能。

Conclusion: 该方法使脉冲能够表现出不同的时间特性，而不仅仅依赖基于频率的表示，在更低的时间步长下实现了更精确的特征表示，同时保持了与传统IF神经元相当的能量消耗。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [5] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: Video-MTR是一个强化多轮推理框架，通过迭代选择关键视频片段和问题理解来解决长视频理解问题，采用门控双级奖励系统实现端到端训练，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 长视频理解存在长程时间依赖和多个事件的挑战，现有方法依赖静态推理或外部视觉语言模型，存在复杂度高和端到端训练不足导致的性能次优问题。

Method: 提出强化多轮推理框架Video-MTR，通过迭代选择视频片段进行渐进式推理，引入门控双级奖励系统（轨迹级奖励和轮级奖励）来优化视频片段选择和问题理解。

Result: 在VideoMME、MLVU和EgoSchema等基准测试中，Video-MTR在准确性和效率方面均优于现有方法，实现了最先进的性能。

Conclusion: Video-MTR通过多轮迭代推理和门控双级奖励系统，有效解决了长视频理解问题，无需外部视觉语言模型，实现了端到端训练，推动了长视频理解技术的发展。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [6] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出UTA-Sign方法，通过无监督热成像-事件相机融合增强低光照环境下的交通标志感知，解决热成像盲点和事件相机非均匀采样问题


<details>
  <summary>Details</summary>
Motivation: 热成像相机在低光照条件下表现优异，但在识别相似材料制作的标志时存在盲点；事件相机擅长捕捉动态变化但采样不均匀。两种模态具有互补特性，可提升自动驾驶系统在夜间对交通标志的语义理解安全性

Method: 开发双增强机制，融合热成像帧和事件信号：利用热成像帧提供精确运动线索作为时间参考来对齐不均匀事件信号，同时事件信号为原始热成像帧补充细微标志内容

Result: 在真实场景数据集上验证，在交通标志轮廓提取和质量方面表现优越，在感知层面提高了检测精度

Conclusion: UTA-Sign方法有效解决了低光照环境下交通标志感知的挑战，通过热成像和事件相机的互补融合提升了自动驾驶系统的环境理解能力

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [7] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 提出基于低频感知扰动的主动防御方法，通过结合频域和空间域特征来破坏深度伪造的人脸交换生成过程，在保持视觉质量的同时显著降低伪造效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法多为被动的事后分析，无法预防攻击。需要开发主动防御技术来直接干扰伪造内容的生成过程。

Method: 设计包含编码器、扰动生成器和解码器的完整架构，利用离散小波变换提取低频分量并生成扰动，结合频域和空间域特征来破坏人脸交换模型。

Result: 在CelebA-HQ和LFW数据集上的实验表明，该方法显著降低了人脸交换的有效性，提高了防御成功率，同时保持了视觉质量。

Conclusion: 提出的主动防御方法通过低频扰动有效破坏了深度伪造生成过程，为对抗深度伪造技术提供了新的解决方案。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [8] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 这篇论文探讨了视频内容中抽象概念识别的挑战，认为基础模型的发展为解决这一难题提供了理想条件，并综述了相关任务、数据集和研究进展。


<details>
  <summary>Details</summary>
Motivation: 虽然机器在视频内容理解方面取得了显著进展，能够识别具体可见的物体、动作和场景，但在识别抽象概念（如正义、自由、团结）方面仍远不及人类。抽象概念识别是视频理解领域的关键开放挑战。

Method: 本文采用综述研究方法，系统分析了用于理解视频中抽象概念的不同任务和数据集，回顾了研究人员多年来利用当时可用工具解决这些问题的尝试。

Result: 研究发现，研究社区在抽象概念理解方面已经积累了数十年的经验，这些经验对于在当前多模态基础模型时代重新审视这一挑战具有重要意义。

Conclusion: 利用基础模型的近期进展为解决视频中的抽象概念理解提供了理想条件，借鉴社区长期积累的经验将有助于避免重复造轮子，推动这一重要开放挑战的解决。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [9] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: DVCTNet是一个基于双视图协同训练的网络，通过结合全景X射线图像的全局视图和裁剪牙齿图像的局部视图，使用门控跨视图注意力模块动态融合特征，显著提高了牙科龋齿检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前牙科龋齿检测方法由于对比度变化细微和病变形态多样，检测精度不理想。受牙医临床工作流程启发，需要结合整体图像筛查和详细牙齿级检查来提高检测准确性。

Method: 使用自动牙齿检测建立全局和局部两个互补视图，分别预训练两个视觉基础模型。全局视图模型作为检测骨干生成区域建议和全局特征，局部视图模型提取裁剪牙齿块的详细特征。通过门控跨视图注意力模块动态融合双视图特征。

Result: 在公共数据集和新构建的高精度数据集上，DVCTNet均表现出优于现有最先进方法的性能，证明了其临床适用性。

Conclusion: DVCTNet通过双视图协同训练和动态特征融合，有效提高了牙科龋齿检测的准确性，为临床诊断提供了可靠的工具。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 本文揭示了RL微调比SFT对LLM安全对齐构成更大威胁，并提出了首个针对RL有害微调的有效防御方法TokenBuncher，通过抑制模型响应不确定性来阻止有害行为学习。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，恶意微调风险增加。现有研究主要关注监督微调(SFT)的滥用风险，但研究发现强化学习(RL)微调能更有效地破坏安全对齐并协助有害任务。

Method: 提出TokenBuncher防御方法，通过熵作为奖励的RL和Token Noiser机制来抑制模型响应不确定性，阻止RL利用奖励信号驱动模型学习有害行为。

Result: 在多个模型和RL算法上的实验表明，TokenBuncher能有效缓解有害RL微调，同时保持良性任务效用和微调能力。

Conclusion: RL有害微调比SFT构成更大的系统性风险，TokenBuncher提供了有效且通用的防御解决方案。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [11] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 该论文提出了cMALC-D框架，利用大语言模型生成语义丰富的课程并通过多样性上下文混合机制提升多自然增强学习的格局性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文多自然增强学习方法依赖于噪音大且不稳定的代理信号，如价值估计或优势估计，在多自然环境中因互动和部分可观性而受限。

Method: 提出cMALC-D框架，使用LLM生成语义丰富的课程并提供更稳健的评估信号。重点是多样性上下文混合机制，通过结合历史上下文特征创建新训练场景，防止模式崩溃并鼓励探索。

Result: 在交通信号控制领域的实验表明，与现有课程学习基线相比，cMALC-D显著提高了格局性和样本效率。

Conclusion: 该方法通过LLM指导的课程生成和多样性上下文混合，有效解决了多自然增强学习中的课程生成和评估挑战，提升了策略的应用潜力。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 提出了GDLLM方法，通过距离感知图结构和软推理时序特征学习，增强大语言模型对事件时序关系的长距离依赖和少数类关系的识别能力，在两个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在类别不平衡数据集中处理少数类关系能力有限，大语言模型的手动提示设计可能引入噪声并干扰长距离依赖判断。

Method: 使用图注意力网络构建距离感知图结构辅助LLMs捕获长距离依赖特征，设计基于软推理的时序特征学习范式增强短距离邻近带的关系识别。

Result: 在两个公开数据集TB-Dense和MATRES上实现了最先进的性能表现。

Conclusion: GDLLM框架通过全局特征有效捕获，显著提升了少数关系类的性能并改善了整体学习能力。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>
