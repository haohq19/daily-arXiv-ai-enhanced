<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文探讨了当前视频理解模型的局限性，提出了‘无限视频理解’作为未来研究方向，旨在处理任意时长的视频内容。


<details>
  <summary>Details</summary>
Motivation: 当前模型在处理长视频时面临计算、内存和时序一致性等挑战，需要突破性解决方案。

Method: 提出‘无限视频理解’概念，建议研究流式架构、持久内存机制、分层表示等方法。

Result: 尚未实现，但明确了核心挑战和研究方向。

Conclusion: ‘无限视频理解’是多媒体和AI研究的重要目标，将推动技术创新。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [2] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），解决了传统方法的误差累积问题，并提高了实时性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法在推理阶段因缺乏真实数据导致误差累积，而扩散模型因迭代特性难以实时应用。

Method: 采用混合模型，结合自回归和扩散模型的优势，设计多尺度姿态表示模块和置信感知因果注意力机制。

Result: 在PHOENIX14T和How2Sign数据集上验证了生成质量和实时效率的提升。

Conclusion: 混合方法有效解决了误差累积和实时性问题，提升了手语生成的准确性和鲁棒性。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [3] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 论文提出跨知识蒸馏（CKD）方法，通过语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题，提升SNN在DVS数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在计算机视觉领域潜力巨大，但受限于标注数据和架构不成熟，性能不如人工神经网络（ANNs）。为提升SNNs在DVS数据上的表现，探索利用RGB数据和ANNs进行知识蒸馏。

Method: 提出跨知识蒸馏（CKD），结合语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题。

Result: 在主流神经形态数据集（如N-Caltech101和CEP-DVS）上验证，性能优于当前最优方法。

Conclusion: CKD方法有效解决了跨模态和跨架构挑战，显著提升了SNNs的性能。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [4] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 论文提出了一种动态调整类间混淆损失的音频-视频预训练编码器DICCAE，通过细粒度对齐和认知对比增强模型对相似活动的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视频预训练方法仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。

Method: 提出DICCAE编码器，动态调整类间混淆损失，并引入多模态融合训练框架和聚类引导的自监督预训练策略。

Result: 在VGGSound数据集上达到65.5%的top-1准确率，接近当前最优性能。

Conclusion: DICCAE通过细粒度对齐和动态混淆损失显著提升了模型对相似活动的区分能力，并通过消融实验验证了各模块的必要性。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [5] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 论文提出CrisisLandMark数据集和CLOSP框架，通过文本对齐光学与SAR图像，提升检索性能54%，并引入GeoCLOSP结合地理坐标优化特定任务。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索系统多限于RGB数据，未能充分利用多传感器（如SAR和多光谱）的独特物理信息。

Method: 构建CrisisLandMark数据集，提出CLOSP框架通过对比学习对齐光学与SAR图像，并扩展为GeoCLOSP整合地理坐标。

Result: CLOSP提升检索性能54%，GeoCLOSP在特定任务中表现更优。

Conclusion: 多传感器数据与地理背景的整合对遥感档案的潜力开发至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [6] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: 该论文提出了VRU-Accident基准，用于评估多模态大语言模型（MLLMs）在涉及弱势道路使用者（VRUs）的高风险交通场景中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于涉及VRUs的事故后果严重，目前缺乏标准化基准来定量评估MLLMs在复杂安全关键场景中的表现。

Method: 构建了包含1K真实事故视频、6K多选问答对和1K密集场景描述的VRU-Accident基准，并评估了17种先进MLLMs。

Result: MLLMs在视觉属性上表现良好，但在事故原因、类型和可预防性推理方面存在显著挑战。

Conclusion: VRU-Accident填补了评估MLLMs在VRU安全场景中能力的空白，揭示了现有模型的局限性。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [7] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD是一种基于自然语言指令快速生成可编辑多模态层的图形设计方法，结合参数化渲染和图像资产生成，解决了现有方法缺乏创造力和可编辑性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图形设计方法缺乏创造力和可编辑性，导致自动化设计效果不佳。

Method: IGD利用多模态理解和推理能力预测属性、排序和布局，并采用扩散模型生成图像资产。

Result: 实验结果表明IGD在图形设计中表现优越。

Conclusion: IGD为图形设计提供了新的解决方案，支持复杂任务的可扩展性。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [8] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 情绪普遍性假设认为情绪表达在不同文化和社会背景下相同，但研究指出这种假设可能过度概括，种族背景可能对情绪表达有影响。

Method: 构建跨文化微表情数据库，算法标注种族标签，进行单一种族与混合种族的对比研究，并提出整合种族背景的情绪特征学习框架。

Result: 实验揭示了种族偏见对微表情识别的影响，提出的种族感知框架在识别中考虑了种族差异。

Conclusion: 种族背景对情绪表达有显著影响，未来的情绪识别研究应纳入种族因素以提高准确性。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD是一种通过结合Transformer时间点过程和推测解码技术来加速采样的新方法，保持输出分布不变的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer时间点过程模型在实际应用中采样速度慢的问题。

Method: 利用小型草稿模型生成候选事件，由大型目标模型并行验证，结合了时间点过程的细化算法和语言模型的推测解码技术。

Result: 在合成和真实数据集上，TPP-SD实现了与标准方法相同的输出分布，但速度提升了2-6倍。

Conclusion: TPP-SD在保持采样质量的同时显著提升了速度，填补了强大模型与实际快速采样需求之间的差距。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [10] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 研究了多头潜在注意力（MLA）对预训练中Transformer内部容量的影响，发现旋转嵌入的应用方式对防止容量瓶颈和谱分裂至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨MLA压缩键/值记忆时如何影响Transformer的预训练容量，特别是旋转嵌入的应用方式。

Method: 使用Marchenko-Pastur诊断分析$W_{Q}W_{K}^\top$矩阵谱，比较标准多头注意力（MHA）、MLA-PreRoPE和MLA-Decoupled三种变体。

Result: 发现MHA和MLA-PreRoPE在特定层出现容量瓶颈和秩崩溃，而MLA-Decoupled能维持谱支持并抑制异常值。

Conclusion: 旋转嵌入的应用方式与压缩位置同样重要，跨头共享旋转组件可避免谱分裂并保持容量。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [11] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 论文提出了一种结合CNN和热力学模型的方法，用于预测核电站燃料棒的温度、应力和应变，以支持预测性维护。


<details>
  <summary>Details</summary>
Motivation: 核电站的预测性维护（PdM）需要实时监测燃料棒状态，但现有方法依赖有限温度测量数据，难以全面评估。

Method: 使用CNN架构结合热力学模型，基于燃料棒外表面温度数据预测内部状态。数据集通过BISON和MOOSE-THM仿真生成。

Result: CNN训练1000次未过拟合，能高精度预测温度分布，进而计算燃料棒的应力和应变分布。

Conclusion: 该方法为核电站预测性维护提供了实时监测工具，具有潜在应用价值。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [12] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 该研究探讨了在数据不平衡情况下，可解释人工智能（XAI）方法的鲁棒性，并提出了一种针对少数类的评估方法。


<details>
  <summary>Details</summary>
Motivation: 由于AI模型的广泛部署和立法要求，XAI方法的鲁棒性成为关键，尤其是在数据不平衡的高风险场景中。

Method: 提出了一种基于流形邻域生成、解释聚合和一致性度量的评估方法，专注于少数类。

Result: 通过一个基于数值特征的表格数据集（霜冻事件）的案例研究，验证了方法的有效性。

Conclusion: 研究为评估XAI解释的鲁棒性提供了初步见解，尤其是在数据不平衡的情况下。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [13] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: scSGC是一种基于软图的单细胞RNA测序聚类方法，通过非二值边权重更准确地表征细胞间连续相似性，解决了硬图构建中的信息丢失和聚类混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的单细胞RNA测序聚类方法（如GNN）依赖硬图构建，导致信息丢失和聚类偏差。scSGC旨在通过软图解决这些问题。

Method: scSGC包含三个核心模块：基于ZINB的特征自编码器、双通道切割感知软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型注释和计算效率上优于13种最先进的聚类模型。

Conclusion: scSGC在单细胞RNA测序数据分析中具有显著潜力，能更深入地理解细胞异质性。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [14] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 本研究开发了首个用于预测亚利桑那州山谷热发病率的图神经网络（GNN）模型，整合了环境因素和病例数据，揭示了疾病传播的关键环境驱动因素。


<details>
  <summary>Details</summary>
Motivation: 山谷热在西南美国流行地区仍是重大公共卫生问题，需要更有效的预测方法来指导防控。

Method: 利用图神经网络整合病例数据和环境预测因子（如土壤条件、大气变量等），探索变量间的相关性，并引入滞后效应捕捉疾病进展的延迟。

Result: GNN模型能有效预测山谷热趋势，并识别关键环境驱动因素。

Conclusion: 该模型可为早期预警系统和资源分配提供科学依据，助力高风险地区的疾病预防。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [15] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal-Aligned Transformer (TAT)的模型，用于多时间范围需求预测，特别是在销售高峰期，通过结合已知上下文变量（如节假日和促销信息）提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 需求预测对供应链管理至关重要，尤其是在销售高峰期，准确预测需求峰值尤为困难。

Method: TAT模型包含编码器和解码器，均嵌入了一种新颖的Temporal Alignment Attention (TAA)机制，用于学习上下文依赖的对齐方式。

Result: 在两个大型电商数据集上，TAT在需求峰值预测上实现了高达30%的准确率提升，同时整体性能与其他先进方法相当。

Conclusion: TAT通过结合上下文信息，显著提升了峰值需求预测的准确性，为供应链管理提供了更可靠的决策支持。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 本文提出了一种关于AI可能导致全人类灭绝事件的分类和示例，旨在通过公开讨论支持预防措施。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可能带来的灾难性风险，以促进公众支持预防行动。

Method: 提出分类法并列举潜在的全人类灭绝场景。

Result: 展示了多种可能的AI灾难性风险，强调其可避免性。

Conclusion: 通过公开讨论这些可能性，可以推动预防措施的实施。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [17] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 本文提出了一种基于Metropolis-Hastings采样的框架，用于训练脉冲神经网络（SNNs）在强化学习任务中，避免了梯度方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在实时控制系统中具有生物启发性和高能效性，但其训练因脉冲通信的不可微分性而面临挑战。

Method: 采用Metropolis-Hastings采样，一种贝叶斯推断技术，通过迭代提出并概率接受网络参数更新，基于累积奖励信号进行训练。

Result: 在AcroBot和CartPole两个标准控制基准测试中，该方法在最大化累积奖励、减少网络资源和训练周期方面优于传统深度Q学习和现有SNN方法。

Conclusion: 该框架为SNN在强化学习中的应用提供了一种有效的无梯度训练方法，展示了其在动态代理控制中的潜力。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [18] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: 论文提出了一种名为mmE-Loc的高精度、低延迟地面定位系统，结合事件相机和毫米波雷达，用于无人机精确降落。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机与毫米波雷达采样频率不匹配，限制了系统吞吐量，因此需要一种更高效的传感器融合方法。

Method: 采用事件相机替代传统帧相机，提出一致性协作跟踪模块和图自适应联合优化模块，充分利用时空信息。

Result: 实验表明，mmE-Loc在精度和延迟上显著优于现有方法。

Conclusion: mmE-Loc为无人机精确降落提供了一种高效、可靠的解决方案。

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [19] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 该论文研究了在机器人导航和避障中使用SNN处理LIDAR数据，发现通过调整LIF神经元的膜电位泄漏常数，可以达到与CNN相当的精度，并开源了数据集。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在神经形态硬件中具有高精度、低内存和计算复杂度的优势，适合用于资源有限的自主机器人应用。

Method: 搭建了一个配备LIDAR的机器人平台，收集带标签的LIDAR数据及人工操作的控制命令，研究了SNN中神经元膜泄漏对LIDAR数据处理精度的影响。

Result: 通过调整LIF神经元的膜电位泄漏常数，SNN在机器人控制精度上与非脉冲CNN相当。

Conclusion: 该研究展示了SNN在机器人导航中的潜力，并开源了数据集以促进未来研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [20] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: REACT框架通过实时几何模型和路径规划，解决了水下机器人因缆绳缠绕导致的任务受限问题，实现了高效、安全的全覆盖检测。


<details>
  <summary>Details</summary>
Motivation: 传统水下机器人检测复杂结构时，缆绳缠绕风险限制了任务效率和安全。

Method: REACT使用基于几何的实时缆绳模型（SDF）和路径规划策略，主动避免缠绕。

Result: 仿真和实验显示，REACT在保证全覆盖的同时，任务完成时间缩短20%，且无缠绕问题。

Conclusion: REACT显著提升了水下机器人检测的安全性和效率，适用于复杂环境。

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>
