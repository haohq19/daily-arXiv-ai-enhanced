{"id": "2510.18892", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.18892", "abs": "https://arxiv.org/abs/2510.18892", "authors": ["Richard J. Young", "Brandon Gillins", "Alice M. Matthews"], "title": "When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs", "comment": "21 pages, 3 figures, 5 tables. Comprehensive evaluation of 256 LLMs\n  on instruction-following tasks", "summary": "Despite widespread deployment of Large Language Models, systematic evaluation\nof instruction-following capabilities remains challenging. While comprehensive\nbenchmarks exist, focused assessments that quickly diagnose specific\ninstruction adherence patterns are valuable. As newer models may be trained on\nexisting benchmarks, novel evaluation approaches are needed to assess genuine\ncapabilities rather than memorized performance. This paper presents a\nstreamlined evaluation framework using twenty carefully designed prompts to\nassess LLM instruction-following across diverse task categories. We demonstrate\nthis framework through a large-scale empirical study conducted on October 14,\n2025, testing 256 verified working models from 331 available via OpenRouter. To\nensure methodological rigor and prevent selection bias, we first verified each\nmodel's basic functionality before inclusion. Unlike large-scale benchmarks\nrequiring extensive computational resources, our approach offers a practical\ndiagnostic tool researchers and practitioners can readily apply. Our\nmethodology builds upon verifiable instructions while introducing a compact\ntest suite balancing comprehensiveness with efficiency. Each prompt targets\ndistinct aspects of instruction following, including format compliance, content\nconstraints, logical sequencing, and multi-step task execution. We evaluate\nmodels from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and\nemerging implementations (Qwen, DeepSeek, community models), providing\ncomparative performance analysis. Our findings reveal consistent failure modes\nand identify specific instruction types posing particular challenges. This work\ncontributes both a practical evaluation tool and one of the most comprehensive\nempirical analyses of instruction-following capabilities across the\ncontemporary LLM landscape.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u752820\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u6765\u8bc4\u4f30LLM\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u6d4b\u8bd5\u4e86256\u4e2a\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u5728\u5df2\u6709\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8bc4\u4f30\u771f\u5b9e\u80fd\u529b\u800c\u975e\u8bb0\u5fc6\u6027\u80fd\uff1b\u540c\u65f6\u9700\u8981\u5b9e\u7528\u7684\u8bca\u65ad\u5de5\u5177\u6765\u5feb\u901f\u8bc4\u4f30\u7279\u5b9a\u6307\u4ee4\u9075\u5faa\u6a21\u5f0f\u3002", "method": "\u6784\u5efa\u5305\u542b20\u4e2a\u63d0\u793a\u7684\u7d27\u51d1\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6db5\u76d6\u683c\u5f0f\u5408\u89c4\u3001\u5185\u5bb9\u7ea6\u675f\u3001\u903b\u8f91\u6392\u5e8f\u548c\u591a\u6b65\u4efb\u52a1\u6267\u884c\u7b49\u4e0d\u540c\u65b9\u9762\uff1b\u901a\u8fc7OpenRouter\u6d4b\u8bd5256\u4e2a\u9a8c\u8bc1\u53ef\u7528\u7684\u6a21\u578b\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6301\u7eed\u5b58\u5728\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u8bc6\u522b\u51fa\u7279\u5b9a\u7c7b\u578b\u7684\u6307\u4ee4\u7279\u522b\u5177\u6709\u6311\u6218\u6027\uff1b\u63d0\u4f9b\u4e86\u4e3b\u8981\u63d0\u4f9b\u5546\u548c\u65b0\u5174\u5b9e\u73b0\u6a21\u578b\u7684\u6bd4\u8f83\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u65e2\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u53c8\u5bf9\u5f53\u4ee3LLM\u666f\u89c2\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u8fdb\u884c\u4e86\u6700\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\u4e4b\u4e00\u3002"}}
{"id": "2510.19054", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19054", "abs": "https://arxiv.org/abs/2510.19054", "authors": ["Shiyu Liu", "Ilija Hadzic", "Akshay Gupta", "Aliasghar Arab"], "title": "Motion Planning and Control of an Overactuated 4-Wheel Drive with Constrained Independent Steering", "comment": "7 pages, 5 figures, 3 tables, video available at\n  https://youtu.be/8l9s2Wb_vec, To appear at IEEE 2025 International Conference\n  on Advanced Robotics", "summary": "This paper addresses motion planning and con- trol of an overactuated 4-wheel\ndrive train with independent steering (4WIS) where mechanical constraints\nprevent the wheels from executing full 360-degree rotations (swerve). The\nconfiguration space of such a robot is constrained and contains discontinuities\nthat affect the smoothness of the robot motion. We introduce a mathematical\nformulation of the steering constraints and derive discontinuity planes that\npartition the velocity space into regions of smooth and efficient motion. We\nfurther design the motion planner for path tracking and ob- stacle avoidance\nthat explicitly accounts for swerve constraints and the velocity transition\nsmoothness. The motion controller uses local feedback to generate actuation\nfrom the desired velocity, while properly handling the discontinuity crossing\nby temporarily stopping the motion and repositioning the wheels. We implement\nthe proposed motion planner as an extension to ROS Navigation package and\nevaluate the system in simulation and on a physical robot.", "AI": {"tldr": "\u4e3a\u5177\u6709\u72ec\u7acb\u8f6c\u5411\u7684\u8fc7\u9a71\u52a84\u8f6e\u9a71\u52a8\u7cfb\u7edf\uff084WIS\uff09\u5f00\u53d1\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u5904\u7406\u673a\u68b0\u7ea6\u675f\u5bfc\u81f4\u7684\u8f6c\u5411\u4e0d\u8fde\u7eed\u6027\uff0c\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u548c\u901f\u5ea6\u7a7a\u95f4\u5206\u533a\u5b9e\u73b0\u5e73\u6ed1\u8fd0\u52a8\u3002", "motivation": "4WIS\u673a\u5668\u4eba\u7684\u673a\u68b0\u7ea6\u675f\u9650\u5236\u4e86\u8f66\u8f6e\u7684360\u5ea6\u65cb\u8f6c\u80fd\u529b\uff0c\u5bfc\u81f4\u914d\u7f6e\u7a7a\u95f4\u5b58\u5728\u4e0d\u8fde\u7eed\u6027\uff0c\u5f71\u54cd\u8fd0\u52a8\u5e73\u6ed1\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u8f6c\u5411\u7ea6\u675f\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u63a8\u5bfc\u901f\u5ea6\u7a7a\u95f4\u7684\u4e0d\u8fde\u7eed\u5e73\u9762\u5206\u533a\uff0c\u8bbe\u8ba1\u8003\u8651\u8f6c\u5411\u7ea6\u675f\u548c\u901f\u5ea6\u8fc7\u6e21\u5e73\u6ed1\u6027\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u5c40\u90e8\u53cd\u9988\u63a7\u5236\u5668\u5904\u7406\u4e0d\u8fde\u7eed\u7a7f\u8d8a\u3002", "result": "\u5b9e\u73b0\u4e86ROS\u5bfc\u822a\u5305\u7684\u6269\u5c55\uff0c\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e864WIS\u673a\u5668\u4eba\u5728\u8f6c\u5411\u7ea6\u675f\u4e0b\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5e73\u6ed1\u9ad8\u6548\u7684\u8fd0\u52a8\u3002"}}
{"id": "2510.19058", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19058", "abs": "https://arxiv.org/abs/2510.19058", "authors": ["Fausto Vega", "Jon Arrizabalaga", "Ryan Watson", "Zachary Manchester"], "title": "Convex Maneuver Planning for Spacecraft Collision Avoidance", "comment": "8 pages, 6 figures, Accepted to International Space Robotics\n  Conference", "summary": "Conjunction analysis and maneuver planning for spacecraft collision avoidance\nremains a manual and time-consuming process, typically involving repeated\nforward simulations of hand-designed maneuvers. With the growing density of\nsatellites in low-Earth orbit (LEO), autonomy is becoming essential for\nefficiently evaluating and mitigating collisions. In this work, we present an\nalgorithm to design low-thrust collision-avoidance maneuvers for short-term\nconjunction events. We first formulate the problem as a nonconvex\nquadratically-constrained quadratic program (QCQP), which we then relax into a\nconvex semidefinite program (SDP) using Shor's relaxation. We demonstrate\nempirically that the relaxation is tight, which enables the recovery of\nglobally optimal solutions to the original nonconvex problem. Our formulation\nproduces a minimum-energy solution while ensuring a desired probability of\ncollision at the time of closest approach. Finally, if the desired probability\nof collision cannot be satisfied, we relax this constraint into a penalty,\nyielding a minimum-risk solution. We validate our algorithm with a\nhigh-fidelity simulation of a satellite conjunction in low-Earth orbit with a\nsimulated conjunction data message (CDM), demonstrating its effectiveness in\nreducing collision risk.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u77ed\u671f\u4ea4\u4f1a\u4e8b\u4ef6\u7684\u4f4e\u63a8\u529b\u78b0\u649e\u89c4\u907f\u673a\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u65b9\u6cd5\u6c42\u89e3\u975e\u51f8\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u6700\u8fd1\u63a5\u8fd1\u70b9\u8fbe\u5230\u671f\u671b\u7684\u78b0\u649e\u6982\u7387\u3002", "motivation": "\u968f\u7740\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u5bc6\u5ea6\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u624b\u52a8\u78b0\u649e\u89c4\u907f\u89c4\u5212\u8fc7\u7a0b\u8017\u65f6\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u81ea\u4e3b\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u8bc4\u4f30\u548c\u7f13\u89e3\u78b0\u649e\u7684\u6548\u7387\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u975e\u51f8\u4e8c\u6b21\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff0c\u7136\u540e\u4f7f\u7528Shor\u677e\u5f1b\u8f6c\u5316\u4e3a\u51f8\u534a\u5b9a\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u677e\u5f1b\u7ea6\u675f\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u89e3\u3002", "result": "\u7ecf\u9a8c\u8bc1\u8be5\u677e\u5f1b\u65b9\u6cd5\u662f\u7d27\u81f4\u7684\uff0c\u80fd\u591f\u6062\u590d\u539f\u59cb\u975e\u51f8\u95ee\u9898\u7684\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u6709\u6548\u964d\u4f4e\u4e86\u78b0\u649e\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6700\u5c0f\u80fd\u91cf\u89e3\uff0c\u540c\u65f6\u786e\u4fdd\u671f\u671b\u7684\u78b0\u649e\u6982\u7387\uff0c\u5728\u65e0\u6cd5\u6ee1\u8db3\u7ea6\u675f\u65f6\u63d0\u4f9b\u6700\u5c0f\u98ce\u9669\u89e3\uff0c\u4e3a\u81ea\u4e3b\u78b0\u649e\u89c4\u907f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.19183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19183", "abs": "https://arxiv.org/abs/2510.19183", "authors": ["Fengyuan Sun", "Hui Chen", "Xinhao Xu", "Dandan Zheng", "Jingdong Chen", "Jun Zhou", "Jungong Han", "Guiguang Ding"], "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning", "comment": null, "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.", "AI": {"tldr": "PruneHal\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94KV\u7f13\u5b58\u526a\u679d\u6765\u589e\u5f3aMLLMs\u5bf9\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u5f15\u5165\u989d\u5916\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8981\u4e48\u5728\u63a8\u7406\u65f6\u52a0\u5165\u5916\u90e8\u6216\u5185\u90e8\u4fe1\u606f\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u4f1a\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002\u7814\u7a76\u53d1\u73b0MLLMs\u4e2d\u7684\u5e7b\u89c9\u4e0e\u89c6\u89c9token\u6ce8\u610f\u529b\u4e0d\u8db3\u5bc6\u5207\u76f8\u5173\u3002", "method": "\u63d0\u51faPruneHal\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u9002\u5e94KV\u7f13\u5b58\u526a\u679d\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u7684\u5173\u6ce8\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u4e14\u4e0e\u4e0d\u540c\u89e3\u7801\u7b56\u7565\u517c\u5bb9\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u5e7b\u89c9\u8bc4\u4f30\u57fa\u51c6\u4e0a\u4f7f\u7528\u56db\u79cd\u4e3b\u6d41MLLMs\u8fdb\u884c\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u7a33\u5065\u4e14\u51fa\u8272\u7684\u7ed3\u679c\u3002", "conclusion": "PruneHal\u662f\u9996\u4e2a\u5c06token\u526a\u679d\u5e94\u7528\u4e8eMLLMs\u5e7b\u89c9\u7f13\u89e3\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u4e0d\u540c\u89e3\u7801\u7b56\u7565\u4e2d\uff0c\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2510.19353", "categories": ["cs.CV", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.19353", "abs": "https://arxiv.org/abs/2510.19353", "authors": ["Ahsan Raza Siyal", "Markus Haltmeier", "Ruth Steiger", "Malik Galijasevic", "Elke Ruth Gizewski", "Astrid Ellen Grams"], "title": "DARE: A Deformable Adaptive Regularization Estimator for Learning-Based Medical Image Registration", "comment": null, "summary": "Deformable medical image registration is a fundamental task in medical image\nanalysis. While deep learning-based methods have demonstrated superior accuracy\nand computational efficiency compared to traditional techniques, they often\noverlook the critical role of regularization in ensuring robustness and\nanatomical plausibility. We propose DARE (Deformable Adaptive Regularization\nEstimator), a novel registration framework that dynamically adjusts elastic\nregularization based on the gradient norm of the deformation field. Our\napproach integrates strain and shear energy terms, which are adaptively\nmodulated to balance stability and flexibility. To ensure physically realistic\ntransformations, DARE includes a folding-prevention mechanism that penalizes\nregions with negative deformation Jacobian. This strategy mitigates\nnon-physical artifacts such as folding, avoids over-smoothing, and improves\nboth registration accuracy and anatomical plausibility", "AI": {"tldr": "DARE\u662f\u4e00\u4e2a\u53ef\u53d8\u5f62\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5f39\u6027\u6b63\u5219\u5316\u6765\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u5305\u542b\u6298\u53e0\u9884\u9632\u673a\u5236\u4ee5\u786e\u4fdd\u7269\u7406\u5408\u7406\u7684\u53d8\u6362\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4e2d\u5f80\u5f80\u5ffd\u89c6\u6b63\u5219\u5316\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u89e3\u5256\u5b66\u5408\u7406\u6027\u3002", "method": "\u57fa\u4e8e\u53d8\u5f62\u573a\u68af\u5ea6\u8303\u6570\u52a8\u6001\u8c03\u6574\u5f39\u6027\u6b63\u5219\u5316\uff0c\u6574\u5408\u5e94\u53d8\u548c\u526a\u5207\u80fd\u91cf\u9879\uff0c\u5e76\u5305\u542b\u6298\u53e0\u9884\u9632\u673a\u5236\u60e9\u7f5a\u8d1f\u53d8\u5f62\u96c5\u53ef\u6bd4\u533a\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u8f7b\u4e86\u975e\u7269\u7406\u4f2a\u5f71\u5982\u6298\u53e0\uff0c\u907f\u514d\u4e86\u8fc7\u5ea6\u5e73\u6ed1\uff0c\u63d0\u9ad8\u4e86\u914d\u51c6\u7cbe\u5ea6\u548c\u89e3\u5256\u5b66\u5408\u7406\u6027\u3002", "conclusion": "DARE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u5219\u5316\u5728\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u89e3\u5256\u5b66\u5408\u7406\u6027\u3002"}}
{"id": "2510.19296", "categories": ["cs.LG", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19296", "abs": "https://arxiv.org/abs/2510.19296", "authors": ["Yang Zhang", "Rui Zhang", "Jiaming Guo", "Lei Huang", "Di Huang", "Yunpu Zhao", "Shuyao Cheng", "Pengwei Jin", "Chongxiao Li", "Zidong Du", "Xing Hu", "Qi Guo", "Yunji Chen"], "title": "QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation", "comment": "Accepted to NeurIPS 2025", "summary": "The remarkable progress of Large Language Models (LLMs) presents promising\nopportunities for Verilog code generation which is significantly important for\nautomated circuit design. The lacking of meaningful functional rewards hinders\nthe preference optimization based on Reinforcement Learning (RL) for producing\nfunctionally correct Verilog code. In this paper, we propose Signal-Aware\nLearning for Verilog code generation (QiMeng-SALV) by leveraging code segments\nof functionally correct output signal to optimize RL training. Considering\nVerilog code specifies the structural interconnection of hardware gates and\nwires so that different output signals are independent, the key insight of\nQiMeng-SALV is to extract verified signal-aware implementations in partially\nincorrect modules, so as to enhance the extraction of meaningful functional\nrewards. Roughly, we verify the functional correctness of signals in generated\nmodule by comparing with that of reference module in the training data. Then\nabstract syntax tree (AST) is employed to identify signal-aware code segments\nwhich can provide meaningful functional rewards from erroneous modules.\nFinally, we introduce signal-aware DPO which is optimized on the correct\nsignal-level code segments, thereby preventing noise and interference from\nincorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from\nconventional module-level to fine-grained signal-level optimization in Verilog\ncode generation, addressing the issue of insufficient functional rewards.\nExperiments demonstrate that our method achieves state-of-the-art performance\non VerilogEval and RTLLM, with a 7B parameter model matching the performance of\nthe DeepSeek v3 671B model and significantly outperforming the leading\nopen-source model CodeV trained on the same dataset. Our code is available at\nhttps://github.com/zy1xxx/SALV.", "AI": {"tldr": "\u63d0\u51fa\u4e86QiMeng-SALV\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u529f\u80fd\u6b63\u786e\u7684\u8f93\u51fa\u4fe1\u53f7\u7684\u4ee3\u7801\u6bb5\u6765\u4f18\u5316Verilog\u4ee3\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u5757\u7ea7\u4f18\u5316\u4e2d\u529f\u80fd\u6027\u5956\u52b1\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Verilog\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u504f\u597d\u4f18\u5316\u7f3a\u4e4f\u6709\u610f\u4e49\u7684\u51fd\u6570\u5956\u52b1\uff0c\u963b\u788d\u4e86\u751f\u6210\u529f\u80fd\u6b63\u786e\u4ee3\u7801\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u6a21\u5757\u4e0e\u53c2\u8003\u6a21\u5757\u7684\u4fe1\u53f7\u529f\u80fd\u6b63\u786e\u6027\uff0c\u63d0\u53d6\u5df2\u9a8c\u8bc1\u7684\u4fe1\u53f7\u611f\u77e5\u5b9e\u73b0\uff1b\u4f7f\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u8bc6\u522b\u4fe1\u53f7\u611f\u77e5\u4ee3\u7801\u6bb5\uff1b\u5f15\u5165\u4fe1\u53f7\u611f\u77e5DPO\u5728\u6b63\u786e\u7684\u4fe1\u53f7\u7ea7\u4ee3\u7801\u6bb5\u4e0a\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728VerilogEval\u548cRTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c7B\u53c2\u6570\u6a21\u578b\u6027\u80fd\u4e0eDeepSeek v3 671B\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5728\u540c\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5f00\u6e90\u6a21\u578bCodeV\u3002", "conclusion": "QiMeng-SALV\u6807\u5fd7\u7740Verilog\u4ee3\u7801\u751f\u6210\u4ece\u4f20\u7edf\u6a21\u5757\u7ea7\u4f18\u5316\u5411\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u7ea7\u4f18\u5316\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u529f\u80fd\u6027\u5956\u52b1\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2510.19366", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19366", "abs": "https://arxiv.org/abs/2510.19366", "authors": ["Xinfeng Xia", "Jiacheng Liu", "Xiaofeng Hou", "Peng Tang", "Mingxuan Zhang", "Wenfeng Wang", "Chao Li"], "title": "MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs", "comment": null, "summary": "Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,\nachieve high quality by sparsely activating parameters. However, their reliance\non routing between a few monolithic experts via a top-k mechanism creates a\n\"quality cliff\", offering only a few coarse-grained operating points. This\ninflexibility forces a difficult trade-off between cost and quality, preventing\nadaptation to diverse Service Level Objectives (SLOs) and leading to\nsignificant resource over-provisioning.\n  This paper introduces MoE-Prism, a model-system co-design that transforms\nrigid MoE models into elastic services. Our methodology is divided into two\nphases. First, an \\emph{Offline Refactoring Engine} systematically deconstructs\nmonolithic experts into fine-grained \"sub-experts.\" This engine employs a\npartitioning optimization solver that uses a metaheuristic-based approach to\ngroup neurons, preserving functional locality without requiring retraining.\nSecond, an \\emph{Online Scheduling Engine} leverages this new elasticity\nthrough QoS-aware scheduling. It implements specialized policies to solve\ncomplex system problems, including maximizing throughput in cloud deployments\nand managing latency-optimized offloading for memory-constrained devices. Our\nevaluation across three different MoE models shows that MoE-Prismprovides over\n4 times more distinct, stable operating points than the baseline. This allows\nan AI service to dynamically improve throughput by up to 19.9\\% under a strict\nlatency budget or reduce latency by up to 10.36\\% under limited resources.\nMoE-Prism provides the critical \"control knob\" to bridge the model-system gap,\nenabling the next generation of adaptive, efficient, and QoS-aware AI services.", "AI": {"tldr": "MoE-Prism\u901a\u8fc7\u5c06\u4f20\u7edfMoE\u6a21\u578b\u4e2d\u7684\u5355\u4f53\u4e13\u5bb6\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u5b50\u4e13\u5bb6\uff0c\u63d0\u4f9b\u4e86\u5f39\u6027\u670d\u52a1\u80fd\u529b\uff0c\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u6210\u672c\u548c\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u6743\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMoE\u6a21\u578b\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u53c2\u6570\u5b9e\u73b0\u9ad8\u8d28\u91cf\uff0c\u4f46\u91c7\u7528top-k\u8def\u7531\u673a\u5236\u5bfc\u81f4\u53ea\u80fd\u63d0\u4f9b\u5c11\u91cf\u7c97\u7c92\u5ea6\u64cd\u4f5c\u70b9\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u670d\u52a1\u7ea7\u522b\u76ee\u6807(SLOs)\uff0c\u9020\u6210\u8d44\u6e90\u8fc7\u5ea6\u914d\u7f6e\u3002", "method": "\u91c7\u7528\u6a21\u578b-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff1a1) \u79bb\u7ebf\u91cd\u6784\u5f15\u64ce\u4f7f\u7528\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5c06\u5355\u4f53\u4e13\u5bb6\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u5b50\u4e13\u5bb6\uff1b2) \u5728\u7ebf\u8c03\u5ea6\u5f15\u64ce\u5b9e\u73b0QoS\u611f\u77e5\u8c03\u5ea6\uff0c\u652f\u6301\u4e91\u90e8\u7f72\u7684\u6700\u5927\u541e\u5410\u91cf\u548c\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u7684\u5ef6\u8fdf\u4f18\u5316\u5378\u8f7d\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540cMoE\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMoE-Prism\u6bd4\u57fa\u7ebf\u63d0\u4f9b\u8d85\u8fc74\u500d\u7684\u7a33\u5b9a\u64cd\u4f5c\u70b9\uff0c\u5728\u4e25\u683c\u5ef6\u8fdf\u9884\u7b97\u4e0b\u53ef\u5c06\u541e\u5410\u91cf\u63d0\u9ad819.9%\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u53ef\u5c06\u5ef6\u8fdf\u964d\u4f4e10.36%\u3002", "conclusion": "MoE-Prism\u63d0\u4f9b\u4e86\u5173\u952e\u7684\"\u63a7\u5236\u65cb\u94ae\"\u6765\u5f25\u5408\u6a21\u578b-\u7cfb\u7edf\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u548cQoS\u611f\u77e5\u7684AI\u670d\u52a1\u3002"}}
{"id": "2510.19345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19345", "abs": "https://arxiv.org/abs/2510.19345", "authors": ["Alvaro Perez-Diaz", "James C. Loach", "Danielle E. Toutoungi", "Lee Middleton"], "title": "Foundation Model Forecasts: Form and Function", "comment": "28 pages, 3 figures", "summary": "Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet\naccuracy alone does not determine practical value. The form of a forecast --\npoint, quantile, parametric, or trajectory ensemble -- fundamentally constrains\nwhich operational tasks it can support. We survey recent TSFMs and find that\ntwo-thirds produce only point or parametric forecasts, while many operational\ntasks require trajectory ensembles that preserve temporal dependence. We\nestablish when forecast types can be converted and when they cannot: trajectory\nensembles convert to simpler forms via marginalization without additional\nassumptions, but the reverse requires imposing temporal dependence through\ncopulas or conformal methods. We prove that marginals cannot determine\npath-dependent event probabilities -- infinitely many joint distributions share\nidentical marginals but yield different answers to operational questions. We\nmap six fundamental forecasting tasks to minimal sufficient forecast types and\nprovide a task-aligned evaluation framework. Our analysis clarifies when\nforecast type, not accuracy, differentiates practical utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TSFMs)\u7684\u9884\u6d4b\u5f62\u5f0f(\u70b9\u9884\u6d4b\u3001\u5206\u4f4d\u6570\u9884\u6d4b\u3001\u53c2\u6570\u9884\u6d4b\u6216\u8f68\u8ff9\u96c6\u6210)\u51b3\u5b9a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u8bb8\u591a\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u8f68\u8ff9\u96c6\u6210\u6765\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u800c\u591a\u6570TSFM\u4ec5\u4ea7\u751f\u70b9\u6216\u53c2\u6570\u9884\u6d4b\u3002", "motivation": "\u5f53\u524dTSFMs\u867d\u7136\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u4ec5\u5173\u6ce8\u7cbe\u5ea6\u4e0d\u8db3\u4ee5\u786e\u5b9a\u5b9e\u9645\u4ef7\u503c\u3002\u9884\u6d4b\u5f62\u5f0f\u9650\u5236\u4e86\u5176\u652f\u6301\u7684\u64cd\u4f5c\u4efb\u52a1\u7c7b\u578b\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u9884\u6d4b\u5f62\u5f0f\u4e4b\u95f4\u7684\u8f6c\u6362\u53ef\u80fd\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u8c03\u67e5\u8fd1\u671fTSFMs\u7684\u9884\u6d4b\u5f62\u5f0f\uff0c\u5efa\u7acb\u9884\u6d4b\u7c7b\u578b\u8f6c\u6362\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u8fb9\u9645\u5206\u5e03\u65e0\u6cd5\u786e\u5b9a\u8def\u5f84\u4f9d\u8d56\u4e8b\u4ef6\u6982\u7387\uff0c\u6620\u5c04\u516d\u4e2a\u57fa\u672c\u9884\u6d4b\u4efb\u52a1\u5230\u6700\u5c0f\u5145\u5206\u9884\u6d4b\u7c7b\u578b\uff0c\u63d0\u4f9b\u4efb\u52a1\u5bf9\u9f50\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u4e09\u5206\u4e4b\u4e8c\u7684TSFMs\u4ec5\u4ea7\u751f\u70b9\u6216\u53c2\u6570\u9884\u6d4b\uff0c\u800c\u8bb8\u591a\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u8f68\u8ff9\u96c6\u6210\u3002\u8bc1\u660e\u8f68\u8ff9\u96c6\u6210\u53ef\u4ee5\u901a\u8fc7\u8fb9\u9645\u5316\u8f6c\u6362\u4e3a\u7b80\u5355\u5f62\u5f0f\uff0c\u4f46\u53cd\u5411\u8f6c\u6362\u9700\u8981\u65bd\u52a0\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "conclusion": "\u9884\u6d4b\u7c7b\u578b\u800c\u975e\u7cbe\u5ea6\u51b3\u5b9a\u4e86\u5b9e\u9645\u6548\u7528\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9009\u62e9\u5408\u9002\u7684\u9884\u6d4b\u5f62\u5f0f\uff0c\u8f68\u8ff9\u96c6\u6210\u5bf9\u4e8e\u9700\u8981\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u64cd\u4f5c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.19560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19560", "abs": "https://arxiv.org/abs/2510.19560", "authors": ["Yao Deng", "Xian Zhong", "Wenxuan Liu", "Zhaofei Yu", "Jingling Yuan", "Tiejun Huang"], "title": "HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking", "comment": null, "summary": "RGB cameras excel at capturing rich texture details with high spatial\nresolution, whereas event cameras offer exceptional temporal resolution and a\nhigh dynamic range (HDR). Leveraging their complementary strengths can\nsubstantially enhance object tracking under challenging conditions, such as\nhigh-speed motion, HDR environments, and dynamic background interference.\nHowever, a significant spatio-temporal asymmetry exists between these two\nmodalities due to their fundamentally different imaging mechanisms, hindering\neffective multi-modal integration. To address this issue, we propose\n{Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge\ndistillation framework that explicitly models and mitigates spatio-temporal\nasymmetries. Specifically, HAD proposes a hierarchical alignment strategy that\nminimizes information loss while maintaining the student network's\ncomputational efficiency and parameter compactness. Extensive experiments\ndemonstrate that HAD consistently outperforms state-of-the-art methods, and\ncomprehensive ablation studies further validate the effectiveness and necessity\nof each designed component. The code will be released soon.", "AI": {"tldr": "\u63d0\u51faHAD\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u975e\u5bf9\u79f0\u84b8\u998f\u89e3\u51b3RGB\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u4e4b\u95f4\u7684\u65f6\u7a7a\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd", "motivation": "RGB\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4e92\u8865\u4f18\u52bf\uff0c\u4f46\u4e24\u8005\u5b58\u5728\u663e\u8457\u7684\u65f6\u7a7a\u4e0d\u5bf9\u79f0\u6027\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408", "method": "\u63d0\u51fa\u5206\u5c42\u975e\u5bf9\u79f0\u84b8\u998f\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u5bf9\u9f50\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5b66\u751f\u7f51\u7edc\u8ba1\u7b97\u6548\u7387\u548c\u53c2\u6570\u7d27\u51d1\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u4fe1\u606f\u635f\u5931", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u4e00\u81f4\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u8bbe\u8ba1\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u548c\u5fc5\u8981\u6027", "conclusion": "HAD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u65f6\u7a7a\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd"}}
{"id": "2510.19622", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19622", "abs": "https://arxiv.org/abs/2510.19622", "authors": ["Zhengxuan Wei", "Jiajin Tang", "Sibei Yang"], "title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning", "comment": "This work is accepted by ICCV 2025", "summary": "Existing Moment Retrieval methods face three critical bottlenecks: (1) data\nscarcity forces models into shallow keyword-feature associations; (2) boundary\nambiguity in transition regions between adjacent events; (3) insufficient\ndiscrimination of fine-grained semantics (e.g., distinguishing ``kicking\" vs.\n``throwing\" a ball). In this paper, we propose a zero-external-dependency\nAugmented Moment Retrieval framework, AMR, designed to overcome local optima\ncaused by insufficient data annotations and the lack of robust boundary and\nsemantic discrimination capabilities. AMR is built upon two key insights: (1)\nit resolves ambiguous boundary information and semantic confusion in existing\nannotations without additional data (avoiding costly manual labeling), and (2)\nit preserves boundary and semantic discriminative capabilities enhanced by\ntraining while generalizing to real-world scenarios, significantly improving\nperformance. Furthermore, we propose a two-stage training framework with\ncold-start and distillation adaptation. The cold-start stage employs curriculum\nlearning on augmented data to build foundational boundary/semantic awareness.\nThe distillation stage introduces dual query sets: Original Queries maintain\nDETR-based localization using frozen Base Queries from the cold-start model,\nwhile Active Queries dynamically adapt to real-data distributions. A\ncross-stage distillation loss enforces consistency between Original and Base\nQueries, preventing knowledge forgetting while enabling real-world\ngeneralization. Experiments on multiple benchmarks show that AMR achieves\nimproved performance over prior state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51faAMR\u6846\u67b6\u89e3\u51b3\u65f6\u523b\u68c0\u7d22\u7684\u4e09\u4e2a\u5173\u952e\u74f6\u9888\uff1a\u6570\u636e\u7a00\u7f3a\u3001\u8fb9\u754c\u6a21\u7cca\u548c\u8bed\u4e49\u533a\u5206\u4e0d\u8db3\uff0c\u901a\u8fc7\u96f6\u5916\u90e8\u4f9d\u8d56\u7684\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u9677\u5165\u6d45\u5c42\u5173\u952e\u8bcd\u7279\u5f81\u5173\u8054\u3001\u76f8\u90bb\u4e8b\u4ef6\u95f4\u8fb9\u754c\u6a21\u7cca\u3001\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u8bed\u4e49\u533a\u5206\u80fd\u529b\u4e0d\u8db3", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u51b7\u542f\u52a8\u9636\u6bb5\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u5728\u589e\u5f3a\u6570\u636e\u4e0a\u5efa\u7acb\u57fa\u7840\u8fb9\u754c/\u8bed\u4e49\u611f\u77e5\uff1b\u84b8\u998f\u9636\u6bb5\u5f15\u5165\u53cc\u67e5\u8be2\u96c6\uff08\u539f\u59cb\u67e5\u8be2\u548c\u52a8\u6001\u67e5\u8be2\uff09\uff0c\u901a\u8fc7\u8de8\u9636\u6bb5\u84b8\u998f\u635f\u5931\u4fdd\u6301\u77e5\u8bc6\u4e00\u81f4\u6027", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAMR\u76f8\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347", "conclusion": "AMR\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u65f6\u523b\u68c0\u7d22\u4e2d\u7684\u8fb9\u754c\u6a21\u7cca\u548c\u8bed\u4e49\u6df7\u6dc6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
