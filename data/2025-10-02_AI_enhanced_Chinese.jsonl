{"id": "2510.00188", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00188", "abs": "https://arxiv.org/abs/2510.00188", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements", "comment": null, "summary": "Nonlinear Model Predictive Control (NMPC) is a precise controller, but its\nheavy computational load often prevents application in robotic systems. Some\nstudies have attempted to approximate NMPC using deep neural networks\n(NMPC-DNN). However, in the presence of unexpected disturbances or when\noperating conditions differ from training data, this approach lacks robustness,\nleading to large tracking errors. To address this issue, for the first time,\nthe NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The\nproposed controller is validated by applying it to an exoskeleton robot during\nsquat movement, which has a complex dynamic model and has received limited\nattention regarding robust nonlinear control design. A human-robot dynamic\nmodel with three active joints (ankle, knee, hip) is developed, and more than\n5.3 million training samples are used to train the DNN. The results show that,\nunder unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI\nis significantly lower compared to NMPC-DNN. Moreover, human joint torques are\ngreatly reduced with the use of the exoskeleton, with RMS values for the\nstudied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip,\nrespectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is\n99.93% lower than that of NMPC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e0ePI\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNMPC\u8ba1\u7b97\u91cf\u5927\u548cNMPC-DNN\u5728\u672a\u77e5\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfNMPC\u8ba1\u7b97\u8d1f\u8f7d\u91cd\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\uff1b\u800c\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684NMPC\u8fd1\u4f3c\u65b9\u6cd5\u5728\u9047\u5230\u610f\u5916\u5e72\u6270\u6216\u4e0e\u8bad\u7ec3\u6570\u636e\u4e0d\u540c\u7684\u5de5\u51b5\u65f6\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u8bef\u5dee\u5927\u3002", "method": "\u9996\u6b21\u5c06NMPC-DNN\u8f93\u51fa\u4e0ePI\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u5f62\u6210\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\u3002\u5f00\u53d1\u4e86\u5177\u6709\u4e09\u4e2a\u4e3b\u52a8\u5173\u8282\uff08\u8e1d\u3001\u819d\u3001\u9acb\uff09\u7684\u4eba\u673a\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f7f\u7528\u8d85\u8fc7530\u4e07\u4e2a\u8bad\u7ec3\u6837\u672c\u8bad\u7ec3DNN\u3002", "result": "\u5728DNN\u672a\u89c1\u8fc7\u7684\u6761\u4ef6\u4e0b\uff0c\u6df7\u5408NMPC-DNN-PI\u7684\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u4f4e\u4e8eNMPC-DNN\u3002\u4f7f\u7528\u5916\u9aa8\u9abc\u540e\u4eba\u4f53\u5173\u8282\u626d\u77e9\u5927\u5e45\u964d\u4f4e\uff0c\u8e1d\u3001\u819d\u3001\u9acb\u5173\u8282\u7684RMS\u503c\u5206\u522b\u51cf\u5c11\u4e8630.9%\u300141.8%\u548c29.7%\u3002\u6df7\u5408\u63a7\u5236\u5668\u7684\u8ba1\u7b97\u6210\u672c\u6bd4NMPC\u964d\u4f4e\u4e8699.93%\u3002", "conclusion": "\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4eba\u4f53\u5173\u8282\u8d1f\u8377\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u9c81\u68d2\u975e\u7ebf\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.00045", "categories": ["cs.CV", "cs.AI", "I.2 ARTIFICIAL INTELLIGENCE"], "pdf": "https://arxiv.org/pdf/2510.00045", "abs": "https://arxiv.org/abs/2510.00045", "authors": ["Franck Vandewiele", "Remi Synave", "Samuel Delepoulle", "Remi Cozot"], "title": "Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions", "comment": null, "summary": "Text-to-image (TTI) models are increasingly used in professional,\neducational, and creative contexts, yet their outputs often embed and amplify\nsocial biases. This paper investigates gender representation in six\nstate-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,\nQwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.\nUsing carefully designed prompts, we generated 100 images for each combination\nof five hospital-related professions (cardiologist, hospital director, nurse,\nparamedic, surgeon) and five portrait qualifiers (\"\", corporate, neutral,\naesthetic, beautiful).\n  Our analysis reveals systematic occupational stereotypes: all models produced\nnurses exclusively as women and surgeons predominantly as men. However,\ndifferences emerge across models: Qwen-Image and SDXL enforce rigid male\ndominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in\nmost roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce\ngender stereotypes but with varying degrees of sensitivity to prompt\nformulation. Portrait qualifiers further modulate gender balance, with terms\nlike corporate reinforcing male depictions and beautiful favoring female ones.\nSensitivity varies widely: Qwen-Image remains nearly unaffected, while\nFLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.\n  These findings demonstrate that gender bias in TTI models is both systematic\nand model-specific. Beyond documenting disparities, we argue that prompt\nwording plays a critical role in shaping demographic outcomes. The results\nunderscore the need for bias-aware design, balanced defaults, and user guidance\nto prevent the reinforcement of occupational stereotypes in generative AI.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e866\u79cd\u5148\u8fdb\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u804c\u4e1a\u523b\u677f\u5370\u8c61\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd\u7684\u654f\u611f\u5ea6\u5dee\u5f02\u5f88\u5927\u3002", "motivation": "\u7814\u7a76\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u4e13\u4e1a\u3001\u6559\u80b2\u548c\u521b\u610f\u5e94\u7528\u4e2d\u5982\u4f55\u5d4c\u5165\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\uff0c\u7279\u522b\u662f\u533b\u7597\u804c\u4e1a\u4e2d\u7684\u6027\u522b\u8868\u5f81\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\uff0c\u4e3a5\u79cd\u533b\u9662\u76f8\u5173\u804c\u4e1a\u548c5\u79cd\u8096\u50cf\u4fee\u9970\u8bcd\u7ec4\u5408\u751f\u6210100\u5f20\u56fe\u50cf\uff0c\u5206\u67906\u79cd\u5f00\u6e90\u6a21\u578b\u7684\u6027\u522b\u8868\u5f81\u6a21\u5f0f\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u663e\u793a\u7cfb\u7edf\u6027\u804c\u4e1a\u523b\u677f\u5370\u8c61\uff1a\u62a4\u58eb\u5168\u4e3a\u5973\u6027\uff0c\u5916\u79d1\u533b\u751f\u4e3b\u8981\u4e3a\u7537\u6027\u3002\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5404\u5f02\uff0cQwen-Image\u548cSDXL\u5f3a\u5236\u7537\u6027\u4e3b\u5bfc\uff0cFLUX.1-dev\u504f\u5411\u5973\u6027\uff0c\u63d0\u793a\u8bcd\u4fee\u9970\u8bed\u663e\u8457\u5f71\u54cd\u6027\u522b\u5e73\u8861\u3002", "conclusion": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\u65e2\u7cfb\u7edf\u53c8\u6a21\u578b\u7279\u5b9a\uff0c\u63d0\u793a\u8bcd\u63aa\u8f9e\u5bf9\u4eba\u53e3\u7edf\u8ba1\u7ed3\u679c\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u9700\u8981\u504f\u5dee\u611f\u77e5\u8bbe\u8ba1\u3001\u5e73\u8861\u9ed8\u8ba4\u503c\u548c\u7528\u6237\u6307\u5bfc\u6765\u9632\u6b62\u804c\u4e1a\u523b\u677f\u5370\u8c61\u7684\u5f3a\u5316\u3002"}}
{"id": "2510.00065", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00065", "abs": "https://arxiv.org/abs/2510.00065", "authors": ["Abdelrhman Gaber", "Hassan Abd-Eltawab", "Youssif Abuzied", "Muhammad ElMahdy", "Tamer ElBatt"], "title": "Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients", "comment": null, "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data, making it attractive for privacy-sensitive domains such as\nhealthcare, finance, and IoT. A major obstacle, however, is the heterogeneity\nof tabular data across clients, where divergent schemas and incompatible\nfeature spaces prevent straightforward aggregation. To address this challenge,\nwe propose FedLLM-Align, a federated framework that leverages pre-trained large\nlanguage models (LLMs) as universal feature extractors. Tabular records are\nserialized into text, and embeddings from models such as DistilBERT, ALBERT,\nRoBERTa, and ClinicalBERT provide semantically aligned representations that\nsupport lightweight local classifiers under the standard FedAvg protocol. This\napproach removes the need for manual schema harmonization while preserving\nprivacy, since raw data remain strictly local. We evaluate FedLLM-Align on\ncoronary heart disease prediction using partitioned Framingham datasets with\nsimulated schema divergence. Across all client settings and LLM backbones, our\nmethod consistently outperforms state-of-the-art baselines, achieving up to\n+0.25 improvement in F1-score and a 65% reduction in communication cost. Stress\ntesting under extreme schema divergence further demonstrates graceful\ndegradation, unlike traditional methods that collapse entirely. These results\nestablish FedLLM-Align as a robust, privacy-preserving, and\ncommunication-efficient solution for federated learning in heterogeneous\nenvironments.", "AI": {"tldr": "FedLLM-Align\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u89e3\u51b3\u8868\u683c\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u65e0\u9700\u624b\u52a8\u6a21\u5f0f\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u654f\u611f\u9886\u57df\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u8868\u683c\u6570\u636e\u7684\u5f02\u6784\u6027\uff08\u4e0d\u540c\u6a21\u5f0f\u548c\u7279\u5f81\u7a7a\u95f4\uff09\u963b\u788d\u4e86\u76f4\u63a5\u805a\u5408\uff0c\u9700\u8981\u89e3\u51b3\u6a21\u5f0f\u5dee\u5f02\u95ee\u9898\u3002", "method": "\u5c06\u8868\u683c\u8bb0\u5f55\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u4f7f\u7528DistilBERT\u3001ALBERT\u3001RoBERTa\u3001ClinicalBERT\u7b49LLM\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u5bf9\u9f50\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5728\u6807\u51c6FedAvg\u534f\u8bae\u4e0b\u652f\u6301\u8f7b\u91cf\u7ea7\u672c\u5730\u5206\u7c7b\u5668\u3002", "result": "\u5728\u51a0\u5fc3\u75c5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6240\u6709\u5ba2\u6237\u7aef\u8bbe\u7f6e\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cF1\u5206\u6570\u63d0\u5347\u6700\u9ad8\u8fbe+0.25\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e65%\uff0c\u5728\u6781\u7aef\u6a21\u5f0f\u5dee\u5f02\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "FedLLM-Align\u4e3a\u5f02\u6784\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00167", "categories": ["cs.AI", "cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00167", "abs": "https://arxiv.org/abs/2510.00167", "authors": ["Diego Ortiz Barbosa", "Mohit Agrawal", "Yash Malegaonkar", "Luis Burbano", "Axel Andersson", "Gy\u00f6rgy D\u00e1n", "Henrik Sandberg", "Alvaro A. Cardenas"], "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI", "comment": null, "summary": "Autonomous drones must often respond to sudden events, such as alarms,\nfaults, or unexpected changes in their environment, that require immediate and\nadaptive decision-making. Traditional approaches rely on safety engineers\nhand-coding large sets of recovery rules, but this strategy cannot anticipate\nthe vast range of real-world contingencies and quickly becomes incomplete.\nRecent advances in embodied AI, powered by large visual language models,\nprovide commonsense reasoning to assess context and generate appropriate\nactions in real time. We demonstrate this capability in a simulated urban\nbenchmark in the Unreal Engine, where drones dynamically interpret their\nsurroundings and decide on sudden maneuvers for safe landings. Our results show\nthat embodied AI makes possible a new class of adaptive recovery and\ndecision-making pipelines that were previously infeasible to design by hand,\nadvancing resilience and safety in autonomous aerial systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5177\u8eabAI\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u81ea\u4e3b\u65e0\u4eba\u673a\u63d0\u4f9b\u5b9e\u65f6\u60c5\u5883\u63a8\u7406\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u7a81\u53d1\u4e8b\u4ef6\u7684\u7d27\u6025\u964d\u843d\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b89\u5168\u5de5\u7a0b\u5e08\u624b\u52a8\u7f16\u5199\u5927\u91cf\u6062\u590d\u89c4\u5219\uff0c\u65e0\u6cd5\u9884\u89c1\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5404\u79cd\u610f\u5916\u60c5\u51b5\u4e14\u5bb9\u6613\u53d8\u5f97\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u65b9\u6848\u3002", "method": "\u5229\u7528\u5177\u8eabAI\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728Unreal Engine\u6a21\u62df\u7684\u57ce\u5e02\u573a\u666f\u4e2d\uff0c\u8ba9\u65e0\u4eba\u673a\u52a8\u6001\u89e3\u8bfb\u5468\u56f4\u73af\u5883\u5e76\u51b3\u5b9a\u7d27\u6025\u673a\u52a8\u7b56\u7565\u4ee5\u5b9e\u73b0\u5b89\u5168\u964d\u843d\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5177\u8eabAI\u80fd\u591f\u5b9e\u73b0\u4e00\u7c7b\u4ee5\u524d\u65e0\u6cd5\u624b\u52a8\u8bbe\u8ba1\u7684\u81ea\u9002\u5e94\u6062\u590d\u548c\u51b3\u7b56\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u81ea\u4e3b\u7a7a\u4e2d\u7cfb\u7edf\u7684\u97e7\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2510.00276", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00276", "abs": "https://arxiv.org/abs/2510.00276", "authors": ["Joe Barrow", "Raj Patel", "Misha Kharkovski", "Ben Davies", "Ryan Schmitt"], "title": "SafePassage: High-Fidelity Information Extraction with Black Box LLMs", "comment": null, "summary": "Black box large language models (LLMs) make information extraction (IE) easy\nto configure, but hard to trust. Unlike traditional information extraction\npipelines, the information \"extracted\" is not guaranteed to be grounded in the\ndocument. To prevent this, this paper introduces the notion of a \"safe\npassage\": context generated by the LLM that is both grounded in the document\nand consistent with the extracted information. This is operationalized via a\nthree-step pipeline, SafePassage, which consists of: (1) an LLM extractor that\ngenerates structured entities and their contexts from a document, (2) a\nstring-based global aligner, and (3) a scoring model. Results show that using\nthese three parts in conjunction reduces hallucinations by up to 85% on\ninformation extraction tasks with minimal risk of flagging non-hallucinations.\nHigh agreement between the SafePassage pipeline and human judgments of\nextraction quality mean that the pipeline can be dually used to evaluate LLMs.\nSurprisingly, results also show that using a transformer encoder fine-tuned on\na small number of task-specific examples can outperform an LLM scoring model at\nflagging unsafe passages. These annotations can be collected in as little as\n1-2 hours.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.00646", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00646", "abs": "https://arxiv.org/abs/2510.00646", "authors": ["Haoyang Wang", "Xinyu Luo", "Wenhua Ding", "Jingao Xu", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Haitao Zhang", "Yunhao Liu", "Xinlei Chen"], "title": "Enabling High-Frequency Cross-Modality Visual Positioning Service for Accurate Drone Landing", "comment": "15 pages, 23 figures", "summary": "After years of growth, drone-based delivery is transforming logistics. At its\ncore, real-time 6-DoF drone pose tracking enables precise flight control and\naccurate drone landing. With the widespread availability of urban 3D maps, the\nVisual Positioning Service (VPS), a mobile pose estimation system, has been\nadapted to enhance drone pose tracking during the landing phase, as\nconventional systems like GPS are unreliable in urban environments due to\nsignal attenuation and multi-path propagation. However, deploying the current\nVPS on drones faces limitations in both estimation accuracy and efficiency. In\nthis work, we redesign drone-oriented VPS with the event camera and introduce\nEV-Pose to enable accurate, high-frequency 6-DoF pose tracking for accurate\ndrone landing. EV-Pose introduces a spatio-temporal feature-instructed pose\nestimation module that extracts a temporal distance field to enable 3D point\nmap matching for pose estimation; and a motion-aware hierarchical fusion and\noptimization scheme to enhance the above estimation in accuracy and efficiency,\nby utilizing drone motion in the \\textit{early stage} of event filtering and\nthe \\textit{later stage} of pose optimization. Evaluation shows that EV-Pose\nachieves a rotation accuracy of 1.34$\\degree$ and a translation accuracy of\n6.9$mm$ with a tracking latency of 10.08$ms$, outperforming baselines by\n$>$50\\%, \\tmcrevise{thus enabling accurate drone landings.} Demo:\nhttps://ev-pose.github.io/", "AI": {"tldr": "EV-Pose\u91cd\u65b0\u8bbe\u8ba1\u4e86\u9762\u5411\u65e0\u4eba\u673a\u7684\u89c6\u89c9\u5b9a\u4f4d\u670d\u52a1\uff0c\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u5b9e\u73b0\u51c6\u786e\u3001\u9ad8\u9891\u76846\u81ea\u7531\u5ea6\u59ff\u6001\u8ddf\u8e2a\uff0c\u4ee5\u652f\u6301\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u7740\u9646\u3002", "motivation": "\u4f20\u7edfGPS\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\uff0c\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u670d\u52a1\u5728\u65e0\u4eba\u673a\u4e0a\u90e8\u7f72\u65f6\u5b58\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u9650\u5236\uff0c\u9700\u8981\u6539\u8fdb\u65e0\u4eba\u673a\u7740\u9646\u9636\u6bb5\u7684\u59ff\u6001\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "\u5f15\u5165\u65f6\u7a7a\u7279\u5f81\u6307\u5bfc\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\uff0c\u63d0\u53d6\u65f6\u95f4\u8ddd\u79bb\u573a\u8fdb\u884c3D\u70b9\u4e91\u5339\u914d\uff1b\u91c7\u7528\u8fd0\u52a8\u611f\u77e5\u7684\u5206\u5c42\u878d\u5408\u548c\u4f18\u5316\u65b9\u6848\uff0c\u5728\u4e8b\u4ef6\u8fc7\u6ee4\u548c\u59ff\u6001\u4f18\u5316\u9636\u6bb5\u5229\u7528\u65e0\u4eba\u673a\u8fd0\u52a8\u4fe1\u606f\u3002", "result": "EV-Pose\u5b9e\u73b0\u4e861.34\u5ea6\u7684\u65cb\u8f6c\u7cbe\u5ea6\u548c6.9\u6beb\u7c73\u7684\u5e73\u79fb\u7cbe\u5ea6\uff0c\u8ddf\u8e2a\u5ef6\u8fdf\u4e3a10.08\u6beb\u79d2\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd550%\u4ee5\u4e0a\u3002", "conclusion": "EV-Pose\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7684\u65e0\u4eba\u673a\u7740\u9646\uff0c\u4e3a\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u7269\u6d41\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u59ff\u6001\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00192", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00192", "abs": "https://arxiv.org/abs/2510.00192", "authors": ["Xin Yu", "Cong Xie", "Ziyu Zhao", "Tiantian Fan", "Lingzhou Xue", "Zhi Zhang"], "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning", "comment": null, "summary": "Low-rank adaptation (LoRA) has become a widely used paradigm for\nparameter-efficient fine-tuning of large language models, yet its\nrepresentational capacity often lags behind full fine-tuning. Within the\ncontext of LoRA, a key open question is how to obtain expressive low-rank\nadapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new\nframework that leverages structured pruning to obtain highly representative\nlow-rank adapters from an over-parameterized initialization. Unlike prior\napproaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes\nless important components during fine-tuning and prevents their reactivation,\nenabling flexible and adaptive rank allocation. For structured pruning, by\nminimizing the pruning error for overall loss, we provide fine-grained pruning\nand recovery updates in a gradient-based pruning strategy with grounded\ninterpretation. We provide the first theoretical analysis of the robustness of\nstructured pruning and provably show that under the impact of weight\nperturbation, gradient-based pruning is more robust than activation-based\npruning with respect to overall loss. Empirically, PrunedLoRA consistently\noutperforms LoRA and its variants across supervised fine-tuning tasks in\nmathematical reasoning, code generation, and natural language understanding,\nand it also demonstrates advantages over existing structured pruning methods\nacross diverse sparsity levels.", "AI": {"tldr": "PrunedLoRA\uff1a\u5229\u7528\u7ed3\u6784\u5316\u526a\u679d\u4ece\u8fc7\u53c2\u6570\u5316\u521d\u59cb\u5316\u4e2d\u83b7\u5f97\u9ad8\u4ee3\u8868\u6027\u4f4e\u79e9\u9002\u914d\u5668\u7684\u65b0\u6846\u67b6\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u4f18\u4e8eLoRA\u53ca\u5176\u53d8\u4f53", "motivation": "LoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u80fd\u529b\u901a\u5e38\u843d\u540e\u4e8e\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u5173\u952e\u95ee\u9898\u662f\u5982\u4f55\u4ece\u8fc7\u53c2\u6570\u5316\u7a7a\u95f4\u4e2d\u83b7\u5f97\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u4f4e\u79e9\u9002\u914d\u5668", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u52a8\u6001\u4fee\u526a\u4e0d\u91cd\u8981\u7ec4\u4ef6\u5e76\u9632\u6b62\u5176\u91cd\u65b0\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u81ea\u9002\u5e94\u79e9\u5206\u914d\u3002\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u526a\u679d\u7b56\u7565\uff0c\u6700\u5c0f\u5316\u6574\u4f53\u635f\u5931\u7684\u526a\u679d\u8bef\u5dee", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8eLoRA\u53ca\u5176\u53d8\u4f53\uff0c\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u6c34\u5e73\u4e0b\u4e5f\u4f18\u4e8e\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5", "conclusion": "PrunedLoRA\u63d0\u4f9b\u4e86\u4ece\u8fc7\u53c2\u6570\u5316\u7a7a\u95f4\u83b7\u5f97\u8868\u8fbe\u6027\u4f4e\u79e9\u9002\u914d\u5668\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027"}}
{"id": "2510.00500", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00500", "abs": "https://arxiv.org/abs/2510.00500", "authors": ["Kaiqi Zhang", "Mingguan Yang", "Dali Chang", "Chun Chen", "Yuxiang Zhang", "Kexun He", "Jing Zhao"], "title": "Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems", "comment": null, "summary": "Iterative method selection is crucial for solving sparse linear systems\nbecause these methods inherently lack robustness. Though image-based selection\napproaches have shown promise, their feature extraction techniques might encode\ndistinct matrices into identical image representations, leading to the same\nselection and suboptimal method. In this paper, we introduce RAF\n(Relative-Absolute Fusion), an efficient feature extraction technique to\nenhance image-based selection approaches. By simultaneously extracting and\nfusing image representations as relative features with corresponding numerical\nvalues as absolute features, RAF achieves comprehensive matrix representations\nthat prevent feature ambiguity across distinct matrices, thus improving\nselection accuracy and unlocking the potential of image-based selection\napproaches. We conducted comprehensive evaluations of RAF on SuiteSparse and\nour developed BMCMat (Balanced Multi-Classification Matrix dataset),\ndemonstrating solution time reductions of 0.08s-0.29s for sparse linear\nsystems, which is 5.86%-11.50% faster than conventional image-based selection\napproaches and achieves state-of-the-art (SOTA) performance. BMCMat is\navailable at https://github.com/zkqq/BMCMat.", "AI": {"tldr": "\u63d0\u51fa\u4e86RAF\uff08\u76f8\u5bf9-\u7edd\u5bf9\u878d\u5408\uff09\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u901a\u8fc7\u540c\u65f6\u63d0\u53d6\u548c\u878d\u5408\u56fe\u50cf\u8868\u793a\u4f5c\u4e3a\u76f8\u5bf9\u7279\u5f81\u4e0e\u5bf9\u5e94\u6570\u503c\u4f5c\u4e3a\u7edd\u5bf9\u7279\u5f81\uff0c\u589e\u5f3a\u57fa\u4e8e\u56fe\u50cf\u7684\u9009\u62e9\u65b9\u6cd5\uff0c\u89e3\u51b3\u7279\u5f81\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u9ad8\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u65b9\u6cd5\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u57fa\u4e8e\u56fe\u50cf\u7684\u9009\u62e9\u65b9\u6cd5\u867d\u7136\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u7279\u5f81\u63d0\u53d6\u6280\u672f\u53ef\u80fd\u5c06\u4e0d\u540c\u77e9\u9635\u7f16\u7801\u4e3a\u76f8\u540c\u7684\u56fe\u50cf\u8868\u793a\uff0c\u5bfc\u81f4\u76f8\u540c\u9009\u62e9\u548c\u6b21\u4f18\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u4ee5\u907f\u514d\u7279\u5f81\u6a21\u7cca\u3002", "method": "\u5f15\u5165RAF\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u540c\u65f6\u63d0\u53d6\u548c\u878d\u5408\u56fe\u50cf\u8868\u793a\uff08\u76f8\u5bf9\u7279\u5f81\uff09\u4e0e\u5bf9\u5e94\u6570\u503c\uff08\u7edd\u5bf9\u7279\u5f81\uff09\uff0c\u521b\u5efa\u5168\u9762\u7684\u77e9\u9635\u8868\u793a\uff0c\u9632\u6b62\u4e0d\u540c\u77e9\u9635\u95f4\u7684\u7279\u5f81\u6a21\u7cca\u3002", "result": "\u5728SuiteSparse\u548c\u81ea\u5efaBMCMat\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u65f6\u95f4\u51cf\u5c110.08s-0.29s\uff0c\u6bd4\u4f20\u7edf\u57fa\u4e8e\u56fe\u50cf\u7684\u9009\u62e9\u65b9\u6cd5\u5feb5.86%-11.50%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "RAF\u901a\u8fc7\u76f8\u5bf9-\u7edd\u5bf9\u7279\u5f81\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u56fe\u50cf\u9009\u62e9\u65b9\u6cd5\u4e2d\u7684\u7279\u5f81\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u65b9\u6cd5\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.00319", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00319", "abs": "https://arxiv.org/abs/2510.00319", "authors": ["Wei Shen", "Han Wang", "Haoyu Li", "Huan Zhang"], "title": "DecepChain: Inducing Deceptive Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have been demonstrating increasingly strong\nreasoning capability with their chain-of-thoughts (CoT), which are routinely\nused by humans to judge answer quality. This reliance creates a powerful yet\nfragile basis for trust. In this work, we present an urgent but underexplored\nrisk: attackers could induce LLMs to generate incorrect yet coherent CoTs that\nlook plausible at first glance, while leaving no obvious manipulated traces,\nclosely resembling the reasoning exhibited in benign scenarios. In particular,\nwe introduce DecepChain, a novel backdoor attack paradigm that steers models to\ngenerate reasoning that appears benign while yielding incorrect conclusions\neventually. At a high level, DecepChain exploits LLMs' own hallucination and\namplifies it by fine-tuning on naturally erroneous rollouts generated by the\nmodel itself and then reinforces it via Group Relative Policy Optimization\n(GRPO) with a flipped reward on triggered inputs, plus a plausibility\nregularizer to preserve fluent, benign-looking reasoning. Across multiple\nbenchmarks and models, DecepChain achieves high attack success rates with\nminimal performance degradation on benign scenarios. Moreover, a careful human\nevaluation showed that the human raters struggle to distinguish our manipulated\nreasoning processes from benign ones, underscoring our attack's stealthiness.\nLeft unaddressed, this stealthy failure mode can quietly corrupt LLM answers\nand undermine human trust for LLM reasoning, emphasizing the urgency for future\nresearch into this alarming risk. Project page: https://decepchain.github.io/.", "AI": {"tldr": "DecepChain\u662f\u4e00\u79cd\u65b0\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bf1\u5bfcLLMs\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u6700\u7ec8\u5f97\u51fa\u9519\u8bef\u7ed3\u8bba\u7684\u63a8\u7406\u94fe\uff0c\u8fd9\u79cd\u653b\u51fb\u96be\u4ee5\u88ab\u4eba\u7c7b\u5bdf\u89c9\u4e14\u80fd\u4fdd\u6301\u826f\u6027\u573a\u666f\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLMs\u4f9d\u8d56\u601d\u7ef4\u94fe\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u8fd9\u79cd\u4f9d\u8d56\u4e3a\u653b\u51fb\u8005\u521b\u9020\u4e86\u8106\u5f31\u7684\u57fa\u7840\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u6700\u7ec8\u9519\u8bef\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u79cd\u9690\u853d\u7684\u653b\u51fb\u65b9\u5f0f\u4f1a\u7834\u574f\u4eba\u7c7b\u5bf9LLM\u63a8\u7406\u7684\u4fe1\u4efb\u3002", "method": "\u5229\u7528LLMs\u81ea\u8eab\u7684\u5e7b\u89c9\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5728\u81ea\u7136\u9519\u8bef\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u7136\u540e\u4f7f\u7528GRPO\uff08\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u914d\u5408\u7ffb\u8f6c\u5956\u52b1\u548c\u5408\u7406\u6027\u6b63\u5219\u5316\u5668\u6765\u5f3a\u5316\u653b\u51fb\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8fc7\u7a0b\u7684\u6d41\u7545\u6027\u548c\u5408\u7406\u6027\u5916\u89c2\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\uff0cDecepChain\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u5728\u826f\u6027\u573a\u666f\u4e0a\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u53c2\u4e0e\u8005\u96be\u4ee5\u533a\u5206\u88ab\u64cd\u7eb5\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u826f\u6027\u63a8\u7406\u3002", "conclusion": "\u8fd9\u79cd\u9690\u853d\u7684\u5931\u8d25\u6a21\u5f0f\u4f1a\u6084\u6084\u7834\u574fLLM\u7b54\u6848\u5e76\u524a\u5f31\u4eba\u7c7b\u5bf9LLM\u63a8\u7406\u7684\u4fe1\u4efb\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u8fd9\u4e00\u8b66\u793a\u6027\u98ce\u9669\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2510.00857", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.00857", "abs": "https://arxiv.org/abs/2510.00857", "authors": ["Adi Simhi", "Jonathan Herzig", "Martin Tutek", "Itay Itzhak", "Idan Szpektor", "Yonatan Belinkov"], "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs", "comment": null, "summary": "As large language models (LLMs) evolve from conversational assistants into\nautonomous agents, evaluating the safety of their actions becomes critical.\nPrior safety benchmarks have primarily focused on preventing generation of\nharmful content, such as toxic text. However, they overlook the challenge of\nagents taking harmful actions when the most effective path to an operational\ngoal conflicts with human safety. To address this gap, we introduce\nManagerBench, a benchmark that evaluates LLM decision-making in realistic,\nhuman-validated managerial scenarios. Each scenario forces a choice between a\npragmatic but harmful action that achieves an operational goal, and a safe\naction that leads to worse operational performance. A parallel control set,\nwhere potential harm is directed only at inanimate objects, measures a model's\npragmatism and identifies its tendency to be overly safe. Our findings indicate\nthat the frontier LLMs perform poorly when navigating this safety-pragmatism\ntrade-off. Many consistently choose harmful options to advance their\noperational goals, while others avoid harm only to become overly safe and\nineffective. Critically, we find this misalignment does not stem from an\ninability to perceive harm, as models' harm assessments align with human\njudgments, but from flawed prioritization. ManagerBench is a challenging\nbenchmark for a core component of agentic behavior: making safe choices when\noperational goals and alignment values incentivize conflicting actions.\nBenchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86ManagerBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u6743\u8861\uff0c\u53d1\u73b0\u524d\u6cbfLLM\u5728\u5b89\u5168-\u5b9e\u7528\u6743\u8861\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9632\u6b62\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u4f46\u5ffd\u89c6\u4e86\u5f53\u64cd\u4f5c\u76ee\u6807\u4e0e\u4eba\u7c7b\u5b89\u5168\u51b2\u7a81\u65f6\uff0c\u4ee3\u7406\u91c7\u53d6\u6709\u5bb3\u884c\u52a8\u7684\u98ce\u9669\u3002", "method": "\u521b\u5efa\u5305\u542b\u4eba\u7c7b\u9a8c\u8bc1\u7684\u7ba1\u7406\u573a\u666f\uff0c\u6bcf\u4e2a\u573a\u666f\u90fd\u8981\u6c42\u5728\u5b9e\u7528\u4f46\u6709\u5bb3\u7684\u884c\u52a8\u4e0e\u5b89\u5168\u4f46\u64cd\u4f5c\u6027\u80fd\u8f83\u5dee\u7684\u884c\u52a8\u4e4b\u95f4\u505a\u51fa\u9009\u62e9\uff0c\u5e76\u8bbe\u7f6e\u5e73\u884c\u5bf9\u7167\u7ec4\u6765\u6d4b\u91cf\u6a21\u578b\u7684\u5b9e\u7528\u4e3b\u4e49\u503e\u5411\u3002", "result": "\u524d\u6cbfLLM\u5728\u5b89\u5168-\u5b9e\u7528\u6027\u6743\u8861\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u8bb8\u591a\u6a21\u578b\u4e3a\u63a8\u8fdb\u64cd\u4f5c\u76ee\u6807\u800c\u6301\u7eed\u9009\u62e9\u6709\u5bb3\u9009\u9879\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5219\u56e0\u907f\u514d\u4f24\u5bb3\u800c\u53d8\u5f97\u8fc7\u4e8e\u5b89\u5168\u4e14\u65e0\u6548\u3002", "conclusion": "LLM\u5728\u64cd\u4f5c\u76ee\u6807\u4e0e\u5bf9\u9f50\u4ef7\u503c\u6fc0\u52b1\u51b2\u7a81\u884c\u52a8\u65f6\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b\u5b58\u5728\u6311\u6218\uff0c\u8fd9\u79cd\u9519\u4f4d\u6e90\u4e8e\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u7f3a\u9677\u800c\u975e\u4f24\u5bb3\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2510.00861", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.00861", "abs": "https://arxiv.org/abs/2510.00861", "authors": ["Ziliang Wang", "Kang An", "Xuhui Zheng", "Faqiang Qian", "Weikun Zhang", "Cijun Ouyang", "Jialu Cai", "Yuhang Wang", "Yichao Wu"], "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs", "comment": "10 pages, 4 figures", "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u64e6\u9664\u5f3a\u5316\u5b66\u4e60(ERL)\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u3001\u64e6\u9664\u548c\u91cd\u65b0\u751f\u6210\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u641c\u7d22\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u8df3\u63a8\u7406\u4e2d\u53ef\u9760\u6027\u6709\u9650\uff0c\u4e3b\u8981\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u5206\u89e3\u9519\u8bef\u3001\u68c0\u7d22\u7f3a\u5931\u548c\u63a8\u7406\u9519\u8bef\uff0c\u5176\u4e2d\u4efb\u4e00\u9636\u6bb5\u7684\u5931\u8d25\u90fd\u4f1a\u5f71\u54cd\u6700\u7ec8\u7b54\u6848\u3002", "method": "ERL\u6846\u67b6\u660e\u786e\u8bc6\u522b\u9519\u8bef\u6b65\u9aa4\uff0c\u64e6\u9664\u5b83\u4eec\u5e76\u5728\u539f\u5904\u91cd\u65b0\u751f\u6210\u63a8\u7406\uff0c\u9632\u6b62\u6709\u7f3a\u9677\u7684\u903b\u8f91\u5728\u63a8\u7406\u94fe\u4e2d\u4f20\u64ad\u3002", "result": "\u4f7f\u7528ERL\u8bad\u7ec3\u7684ESearch\u6a21\u578b\u5728HotpotQA\u3001MuSiQue\u30012Wiki\u548cBamboogle\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c3B\u6a21\u578b\u6bd4\u4e4b\u524dSOTA\u63d0\u53478.48% EM\u548c11.56% F1\uff0c7B\u6a21\u578b\u63d0\u53475.38% EM\u548c7.22% F1\u3002", "conclusion": "\u53ef\u64e6\u9664\u5f3a\u5316\u5b66\u4e60\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9c81\u68d2\u591a\u6b65\u63a8\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2510.00836", "categories": ["cs.AI", "cs.CE", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.00836", "abs": "https://arxiv.org/abs/2510.00836", "authors": ["Jieun Yu", "Minjung Park", "Sangmi Chai"], "title": "Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques", "comment": null, "summary": "This study aims to detect pump and dump (P&D) manipulation in cryptocurrency\nmarkets, where the scarcity of such events causes severe class imbalance and\nhinders accurate detection. To address this issue, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, and advanced ensemble learning\nmodels were evaluated to distinguish manipulative trading behavior from normal\nmarket activity. The experimental results show that applying SMOTE greatly\nenhanced the ability of all models to detect P&D events by increasing recall\nand improving the overall balance between precision and recall. In particular,\nXGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,\nrespectively) with strong F1-scores and demonstrated fast computational\nperformance, making them suitable for near real time surveillance. These\nfindings indicate that integrating data balancing techniques with ensemble\nmethods significantly improves the early detection of manipulative activities,\ncontributing to a fairer, more transparent, and more stable cryptocurrency\nmarket.", "AI": {"tldr": "\u4f7f\u7528SMOTE\u6280\u672f\u89e3\u51b3\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u4e2dPump and Dump\u64cd\u7eb5\u68c0\u6d4b\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7ed3\u5408\u96c6\u6210\u5b66\u4e60\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u4e2dPump and Dump\u64cd\u7eb5\u4e8b\u4ef6\u7684\u7a00\u7f3a\u6027\u5bfc\u81f4\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u963b\u788d\u4e86\u51c6\u786e\u7684\u68c0\u6d4b\u3002", "method": "\u5e94\u7528\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u6280\u672f(SMOTE)\u5e76\u8bc4\u4f30\u5148\u8fdb\u7684\u96c6\u6210\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u533a\u5206\u64cd\u7eb5\u6027\u4ea4\u6613\u884c\u4e3a\u4e0e\u6b63\u5e38\u5e02\u573a\u6d3b\u52a8\u3002", "result": "SMOTE\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u6a21\u578b\u68c0\u6d4bP&D\u4e8b\u4ef6\u7684\u80fd\u529b\uff0cXGBoost\u548cLightGBM\u5206\u522b\u8fbe\u523094.87%\u548c93.59%\u7684\u9ad8\u53ec\u56de\u7387\uff0c\u5177\u6709\u5f3a\u5927\u7684F1\u5206\u6570\u548c\u5feb\u901f\u8ba1\u7b97\u6027\u80fd\u3002", "conclusion": "\u5c06\u6570\u636e\u5e73\u8861\u6280\u672f\u4e0e\u96c6\u6210\u65b9\u6cd5\u7ed3\u5408\u53ef\u663e\u8457\u6539\u5584\u64cd\u7eb5\u6d3b\u52a8\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u6709\u52a9\u4e8e\u5efa\u7acb\u66f4\u516c\u5e73\u3001\u900f\u660e\u548c\u7a33\u5b9a\u7684\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u3002"}}
{"id": "2510.00681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.00681", "abs": "https://arxiv.org/abs/2510.00681", "authors": ["Jinchang Zhang", "Zijun Li", "Jiakai Lin", "Guoyu Lu"], "title": "Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation", "comment": null, "summary": "Event cameras offer advantages in object detection tasks due to high-speed\nresponse, low latency, and robustness to motion blur. However, event cameras\nlack texture and color information, making open-vocabulary detection\nparticularly challenging. Current event-based detection methods are typically\ntrained on predefined categories, limiting their ability to generalize to novel\nobjects, where encountering previously unseen objects is common.\nVision-language models (VLMs) have enabled open-vocabulary object detection in\nRGB images. However, the modality gap between images and event streams makes it\nineffective to directly transfer CLIP to event data, as CLIP was not designed\nfor event streams. To bridge this gap, we propose an event-image knowledge\ndistillation framework that leverages CLIP's semantic understanding to achieve\nopen-vocabulary object detection on event data. Instead of training CLIP\ndirectly on event streams, we use image frames as inputs to a teacher model,\nguiding the event-based student model to learn CLIP's rich visual\nrepresentations. Through spatial attention-based distillation, the student\nnetwork learns meaningful visual features directly from raw event inputs while\ninheriting CLIP's broad visual knowledge. Furthermore, to prevent information\nloss due to event data segmentation, we design a hybrid spiking neural network\n(SNN) and convolutional neural network (CNN) framework. Unlike fixed-group\nevent segmentation methods, which often discard crucial temporal information,\nour SNN adaptively determines the optimal event segmentation moments, ensuring\nthat key temporal features are extracted. The extracted event features are then\nprocessed by CNNs for object detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6-\u56fe\u50cf\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7CLIP\u6559\u5e08\u6a21\u578b\u6307\u5bfc\u4e8b\u4ef6\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u6570\u636e\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff0c\u7ed3\u5408SNN\u548cCNN\u7684\u6df7\u5408\u6846\u67b6\u81ea\u9002\u5e94\u63d0\u53d6\u4e8b\u4ef6\u7279\u5f81\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7f3a\u4e4f\u7eb9\u7406\u548c\u989c\u8272\u4fe1\u606f\uff0c\u73b0\u6709\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u3002CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728RGB\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u4f46\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e8b\u4ef6\u6570\u636e\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u5e27\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8f93\u5165\uff0c\u901a\u8fc7\u7a7a\u95f4\u6ce8\u610f\u529b\u84b8\u998f\u6307\u5bfc\u4e8b\u4ef6\u5b66\u751f\u6a21\u578b\u5b66\u4e60CLIP\u7684\u89c6\u89c9\u8868\u793a\u3002\u8bbe\u8ba1SNN-CNN\u6df7\u5408\u6846\u67b6\uff0cSNN\u81ea\u9002\u5e94\u786e\u5b9a\u4e8b\u4ef6\u5206\u5272\u65f6\u523b\uff0cCNN\u5904\u7406\u63d0\u53d6\u7684\u7279\u5f81\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06CLIP\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u8fc1\u79fb\u5230\u4e8b\u4ef6\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff0c\u514b\u670d\u4e86\u4e8b\u4ef6\u6570\u636e\u7f3a\u4e4f\u7eb9\u7406\u4fe1\u606f\u7684\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u56fe\u50cf\u548c\u4e8b\u4ef6\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\uff0cSNN-CNN\u6df7\u5408\u8bbe\u8ba1\u4fdd\u7559\u4e86\u5173\u952e\u65f6\u95f4\u4fe1\u606f\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.00404", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00404", "abs": "https://arxiv.org/abs/2510.00404", "authors": ["Xudong Zhu", "Mohammad Mahdi Khalili", "Zhihui Zhu"], "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features", "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u5b57\u5178\u5b66\u4e60\u63a8\u5bfc\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709SAE\u53d8\u4f53\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86AbsTopK SAE\u6765\u652f\u6301\u53cc\u5411\u6982\u5ff5\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7f3a\u4e4f\u7406\u8bba\u63a8\u5bfc\u6846\u67b6\uff0c\u4e14\u5176\u6b63\u5219\u5316\u5668\u5f3a\u5236\u975e\u8d1f\u6027\uff0c\u65e0\u6cd5\u8868\u793a\u53cc\u5411\u6982\u5ff5\uff08\u5982\u7537\u6027vs\u5973\u6027\uff09\uff0c\u5bfc\u81f4\u8bed\u4e49\u8f74\u5206\u88c2\u4e3a\u5197\u4f59\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u5c55\u5f00\u7a00\u758f\u7f16\u7801\u7684\u8fd1\u7aef\u68af\u5ea6\u65b9\u6cd5\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51faAbsTopK SAE\u53d8\u4f53\uff0c\u57fa\u4e8e\u2113\u2080\u7a00\u758f\u7ea6\u675f\u5bf9\u6700\u5927\u5e45\u5ea6\u6fc0\u6d3b\u8fdb\u884c\u786c\u9608\u503c\u5904\u7406\uff0c\u4fdd\u7559\u6b63\u8d1f\u6fc0\u6d3b\u3002", "result": "\u57284\u4e2aLLM\u548c7\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAbsTopK\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u589e\u5f3a\u4e86\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u7528\u5355\u4e2a\u7279\u5f81\u7f16\u7801\u5bf9\u6bd4\u6982\u5ff5\uff0c\u6027\u80fd\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u9700\u8981\u6807\u6ce8\u6570\u636e\u7684\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "AbsTopK SAE\u901a\u8fc7\u652f\u6301\u53cc\u5411\u6fc0\u6d3b\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u3001\u66f4\u5b8c\u6574\u7684\u8bed\u4e49\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709SAE\u7684\u7ed3\u6784\u6027\u9650\u5236\u3002"}}
{"id": "2510.01114", "categories": ["cs.AI", "I.2.1; I.2.4; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.01114", "abs": "https://arxiv.org/abs/2510.01114", "authors": ["Lionel Levine", "John Santerre", "Alexander S. Young", "T. Barry Levine", "Francis Campion", "Majid Sarrafzadeh"], "title": "PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis", "comment": "8 pages, 6 figures", "summary": "We present PRISM-Consult, a clinician-aligned panel-of-experts architecture\nthat extends the compact PRISM sequence model into a routed family of domain\nspecialists. Episodes are tokenized as structured clinical events; a\nlight-weight router reads the first few tokens and dispatches to specialist\nmodels (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,\nPsychogenic). Each specialist inherits PRISM's small transformer backbone and\ntoken template, enabling parameter efficiency and interpretability. On\nreal-world Emergency Department cohorts, specialists exhibit smooth convergence\nwith low development perplexities across domains, while the router achieves\nhigh routing quality and large compute savings versus consult-all under a\nsafety-first policy. We detail the data methodology (initial vs. conclusive\nICD-9 families), routing thresholds and calibration, and report per-domain\nresults to avoid dominance by common events. The framework provides a practical\npath to safe, auditable, and low-latency consult at scale, and we outline\nvalidation steps-external/temporal replication, asymmetric life-threat\nthresholds, and multi-label arbitration-to meet prospective clinical deployment\nstandards.", "AI": {"tldr": "PRISM-Consult\u662f\u4e00\u4e2a\u4e34\u5e8a\u533b\u751f\u5bf9\u9f50\u7684\u4e13\u5bb6\u7ec4\u5408\u67b6\u6784\uff0c\u5c06\u7d27\u51d1\u7684PRISM\u5e8f\u5217\u6a21\u578b\u6269\u5c55\u4e3a\u8def\u7531\u5f0f\u9886\u57df\u4e13\u5bb6\u5bb6\u65cf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5c06\u6025\u8bca\u75c5\u4f8b\u5206\u53d1\u5230\u4e0d\u540c\u4e13\u79d1\u6a21\u578b\uff0c\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u3001\u4f4e\u5ef6\u8fdf\u7684\u4e34\u5e8a\u54a8\u8be2\u7cfb\u7edf\uff0c\u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u6a21\u578b\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u4e34\u5e8a\u4e8b\u4ef6\u6807\u8bb0\u5316\uff0c\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u8bfb\u53d6\u524d\u51e0\u4e2a\u6807\u8bb0\u5e76\u5206\u53d1\u5230\u4e13\u79d1\u6a21\u578b\uff08\u5fc3\u810f\u8840\u7ba1\u3001\u80ba\u90e8\u3001\u80c3\u80a0\u3001\u808c\u8089\u9aa8\u9abc\u3001\u5fc3\u7406\u6e90\u6027\uff09\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u7ee7\u627fPRISM\u7684\u5c0f\u578btransformer\u9aa8\u5e72\u548c\u6807\u8bb0\u6a21\u677f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6025\u8bca\u79d1\u961f\u5217\u4e2d\uff0c\u4e13\u5bb6\u6a21\u578b\u5728\u5404\u9886\u57df\u8868\u73b0\u51fa\u5e73\u6ed1\u6536\u655b\u548c\u4f4e\u5f00\u53d1\u56f0\u60d1\u5ea6\uff0c\u8def\u7531\u5668\u5728\u5b89\u5168\u4f18\u5148\u7b56\u7565\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8def\u7531\u548c\u5927\u91cf\u8ba1\u7b97\u8282\u7701\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u3001\u4f4e\u5ef6\u8fdf\u7684\u4e34\u5e8a\u54a8\u8be2\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5e76\u6982\u8ff0\u4e86\u5916\u90e8/\u65f6\u95f4\u590d\u5236\u3001\u975e\u5bf9\u79f0\u751f\u547d\u5a01\u80c1\u9608\u503c\u548c\u591a\u6807\u7b7e\u4ef2\u88c1\u7b49\u9a8c\u8bc1\u6b65\u9aa4\u4ee5\u6ee1\u8db3\u524d\u77bb\u6027\u4e34\u5e8a\u90e8\u7f72\u6807\u51c6\u3002"}}
{"id": "2510.00733", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.00733", "abs": "https://arxiv.org/abs/2510.00733", "authors": ["Alessio Cristofoletto", "Cesare Rollo", "Giovanni Birolo", "Piero Fariselli"], "title": "Neural Diffusion Processes for Physically Interpretable Survival Prediction", "comment": "11 pages, 6 figures", "summary": "We introduce DeepFHT, a survival-analysis framework that couples deep neural\nnetworks with first hitting time (FHT) distributions from stochastic process\ntheory. Time to event is represented as the first passage of a latent diffusion\nprocess to an absorbing boundary. A neural network maps input variables to\nphysically meaningful parameters including initial condition, drift, and\ndiffusion, within a chosen FHT process such as Brownian motion, both with drift\nand driftless. This yields closed-form survival and hazard functions and\ncaptures time-varying risk without assuming proportional-hazards.\n  We compare DeepFHT with Cox regression and other existing parametric survival\nmodels, using synthetic and real-world datasets. The method achieves predictive\naccuracy on par with state-of-the-art approaches, while maintaining a\nphysics-based interpretable parameterization that elucidates the relation\nbetween input features and risk. This combination of stochastic process theory\nand deep learning provides a principled avenue for modeling survival phenomena\nin complex systems.", "AI": {"tldr": "DeepFHT\u662f\u4e00\u4e2a\u751f\u5b58\u5206\u6790\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u968f\u673a\u8fc7\u7a0b\u7406\u8bba\u4e2d\u7684\u9996\u6b21\u547d\u4e2d\u65f6\u95f4\u5206\u5e03\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u4e8b\u4ef6\u65f6\u95f4\uff0c\u63d0\u4f9b\u5c01\u95ed\u5f62\u5f0f\u7684\u751f\u5b58\u548c\u98ce\u9669\u51fd\u6570\u3002", "motivation": "\u4f20\u7edf\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u5982Cox\u56de\u5f52\u5047\u8bbe\u6bd4\u4f8b\u98ce\u9669\uff0c\u65e0\u6cd5\u6355\u6349\u65f6\u53d8\u98ce\u9669\u3002\u9700\u8981\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u968f\u673a\u8fc7\u7a0b\u7406\u8bba\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027\u6765\u5efa\u6a21\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u751f\u5b58\u73b0\u8c61\u3002", "method": "\u5c06\u4e8b\u4ef6\u65f6\u95f4\u8868\u793a\u4e3a\u6f5c\u5728\u6269\u6563\u8fc7\u7a0b\u9996\u6b21\u5230\u8fbe\u5438\u6536\u8fb9\u754c\u7684\u65f6\u95f4\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5c06\u8f93\u5165\u53d8\u91cf\u6620\u5c04\u5230FHT\u8fc7\u7a0b\u7684\u7269\u7406\u53c2\u6570\uff08\u521d\u59cb\u6761\u4ef6\u3001\u6f02\u79fb\u3001\u6269\u6563\uff09\uff0c\u57fa\u4e8e\u5e03\u6717\u8fd0\u52a8\u7b49\u968f\u673a\u8fc7\u7a0b\u6784\u5efa\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeepFHT\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u89e3\u91ca\u53c2\u6570\u5316\uff0c\u80fd\u591f\u9610\u660e\u8f93\u5165\u7279\u5f81\u4e0e\u98ce\u9669\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u968f\u673a\u8fc7\u7a0b\u7406\u8bba\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u590d\u6742\u7cfb\u7edf\u4e2d\u751f\u5b58\u73b0\u8c61\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\uff0c\u5b9e\u73b0\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u7269\u7406\u53ef\u89e3\u91ca\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2510.00873", "categories": ["cs.LG", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.00873", "abs": "https://arxiv.org/abs/2510.00873", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n", "Dar\u00edo Fernando Mendieta"], "title": "Reducci\u00f3n de ruido por medio de autoencoders: caso de estudio con la se\u00f1al GW150914", "comment": "in Spanish language, Presented at the RPIC 2023 (Information\n  Processing and Control work Reunion)", "summary": "This brief study focuses on the application of autoencoders to improve the\nquality of low-amplitude signals, such as gravitational events. A pre-existing\nautoencoder was trained using cosmic event data, optimizing its architecture\nand parameters. The results show a significant increase in the signal-to-noise\nratio of the processed signals, demonstrating the potential of autoencoders in\nthe analysis of small signals with multiple sources of interference.", "AI": {"tldr": "\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u63d0\u5347\u4f4e\u632f\u5e45\u4fe1\u53f7\u8d28\u91cf\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5f15\u529b\u4e8b\u4ef6\u7b49\u5fae\u5f31\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u566a\u6bd4\u3002", "motivation": "\u6539\u5584\u4f4e\u632f\u5e45\u4fe1\u53f7\uff08\u5982\u5f15\u529b\u4e8b\u4ef6\uff09\u7684\u8d28\u91cf\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u901a\u5e38\u53d7\u5230\u591a\u79cd\u5e72\u6270\u6e90\u7684\u5f71\u54cd\uff0c\u96be\u4ee5\u5206\u6790\u3002", "method": "\u4f7f\u7528\u9884\u5148\u5b58\u5728\u7684\u81ea\u7f16\u7801\u5668\uff0c\u5229\u7528\u5b87\u5b99\u4e8b\u4ef6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f18\u5316\u5176\u67b6\u6784\u548c\u53c2\u6570\u3002", "result": "\u5904\u7406\u540e\u7684\u4fe1\u53f7\u4fe1\u566a\u6bd4\u663e\u8457\u589e\u52a0\uff0c\u8bc1\u660e\u4e86\u81ea\u7f16\u7801\u5668\u5728\u5206\u6790\u5177\u6709\u591a\u91cd\u5e72\u6270\u6e90\u7684\u5fae\u5f31\u4fe1\u53f7\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u81ea\u7f16\u7801\u5668\u5728\u63d0\u5347\u4f4e\u632f\u5e45\u4fe1\u53f7\u8d28\u91cf\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u5206\u6790\u5fae\u5f31\u4fe1\u53f7\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.00911", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00911", "abs": "https://arxiv.org/abs/2510.00911", "authors": ["Tao Ren", "Jinyang Jiang", "Hui Yang", "Wan Tian", "Minhao Zou", "Guanghao Li", "Zishi Zhang", "Qinghao Wang", "Shentao Qin", "Yanjun Zhao", "Rui Tao", "Hui Shao", "Yijie Peng"], "title": "RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training", "comment": null, "summary": "Reinforcement learning with verifiable reward has recently emerged as a\ncentral paradigm for post-training large language models (LLMs); however,\nprevailing mean-based methods, such as Group Relative Policy Optimization\n(GRPO), suffer from entropy collapse and limited reasoning gains. We argue that\nthese issues stem from overemphasizing high-probability output sequences while\nneglecting rare but informative reasoning paths. To address these challenges,\nwe propose Risk-based Policy Optimization (RiskPO), which substitutes classical\nmean-based objectives with principled risk measures. Specifically, we introduce\na Mixed Value-at-Risk objective that integrates weighted attention over\nmultiple regions of the reward distribution, thereby amplifying gradient\nsignals on challenging instances and preventing overconfident convergence. We\nfurther design a bundling scheme that aggregates multiple questions into\nbundles, thus enriching the feedback signal and yielding more stable and\ninformative training dynamics. Theoretically, we prove that the risk-averse\nupdate alleviates entropy collapse and promotes exploration. Numerically,\nRiskPO achieves consistent and significant improvements in mathematical\nreasoning, multi-modal reasoning, and code generation benchmarks, surpassing\nGRPO and its variants on both Pass@1 and Pass@k metrics. Our results\ndemonstrate that risk-based optimization provides a rigorous and effective\nparadigm for enhancing LLM reasoning capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86RiskPO\u65b9\u6cd5\uff0c\u7528\u98ce\u9669\u5ea6\u91cf\u66ff\u4ee3\u4f20\u7edf\u5747\u503c\u76ee\u6807\uff0c\u901a\u8fc7\u6df7\u5408\u98ce\u9669\u4ef7\u503c\u76ee\u6807\u589e\u5f3a\u56f0\u96be\u5b9e\u4f8b\u7684\u68af\u5ea6\u4fe1\u53f7\uff0c\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u6536\u655b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGRPO\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5747\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5b58\u5728\u71b5\u5d29\u6e83\u548c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6709\u9650\u7684\u95ee\u9898\uff0c\u539f\u56e0\u662f\u8fc7\u5ea6\u5f3a\u8c03\u9ad8\u6982\u7387\u8f93\u51fa\u5e8f\u5217\u800c\u5ffd\u7565\u4e86\u7a00\u6709\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51faRiskPO\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6df7\u5408\u98ce\u9669\u4ef7\u503c\u76ee\u6807\uff0c\u6574\u5408\u5956\u52b1\u5206\u5e03\u7684\u591a\u4e2a\u533a\u57df\uff1b2\uff09\u8bbe\u8ba1\u6346\u7ed1\u65b9\u6848\uff0c\u5c06\u591a\u4e2a\u95ee\u9898\u805a\u5408\u4e3a\u6346\u7ed1\u5305\u4ee5\u4e30\u5bcc\u53cd\u9988\u4fe1\u53f7\uff1b3\uff09\u7406\u8bba\u4e0a\u8bc1\u660e\u98ce\u9669\u89c4\u907f\u66f4\u65b0\u80fd\u7f13\u89e3\u71b5\u5d29\u6e83\u5e76\u4fc3\u8fdb\u63a2\u7d22\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRiskPO\u5728Pass@1\u548cPass@k\u6307\u6807\u4e0a\u5747\u4e00\u81f4\u4e14\u663e\u8457\u4f18\u4e8eGRPO\u53ca\u5176\u53d8\u4f53\u3002", "conclusion": "\u57fa\u4e8e\u98ce\u9669\u7684\u4f18\u5316\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u6709\u6548\u7684\u8303\u5f0f\u3002"}}
