<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Filter-Based Reconstruction of Images from Events](https://arxiv.org/abs/2510.20071)
*Bernd Pfrommer*

Main category: cs.CV

TL;DR: 提出FIBAR方法，一种基于滤波器的异步重建方法，用于从移动事件相机的事件中重建强度图像。该方法使用IIR滤波器整合事件强度变化，通过新颖算法检测陈旧像素并进行高斯滤波降噪。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的强度图像重建方法通常需要GPU部署且复杂度高，需要一种更简单、异步且能在CPU上高效运行的重建方法。

Method: 使用时间数字IIR滤波器整合事件强度变化；通过新颖算法检测陈旧像素并维护最近更新像素窗口；对陈旧像素应用高斯滤波；支持任意时间点的异步图像读取。

Result: 在笔记本电脑CPU上运行速度约42-140百万事件/秒；重建图像比神经网络方法噪声更大且存在重影；但足以完成某些任务如基准标记检测。

Conclusion: FIBAR提供了一种简单有效的异步重建方法，虽然重建质量不如神经网络方法，但在特定应用场景下足够使用，且计算效率高。

Abstract: Reconstructing an intensity image from the events of a moving event camera is
a challenging task that is typically approached with neural networks deployed
on graphics processing units. This paper presents a much simpler, FIlter Based
Asynchronous Reconstruction method (FIBAR). First, intensity changes signaled
by events are integrated with a temporal digital IIR filter. To reduce
reconstruction noise, stale pixels are detected by a novel algorithm that
regulates a window of recently updated pixels. Arguing that for a moving
camera, the absence of events at a pixel location likely implies a low image
gradient, stale pixels are then blurred with a Gaussian filter. In contrast to
most existing methods, FIBAR is asynchronous and permits image read-out at an
arbitrary time. It runs on a modern laptop CPU at about 42(140) million
events/s with (without) spatial filtering enabled. A few simple qualitative
experiments are presented that show the difference in image reconstruction
between FIBAR and a neural network-based approach (FireNet). FIBAR's
reconstruction is noisier than neural network-based methods and suffers from
ghost images. However, it is sufficient for certain tasks such as the detection
of fiducial markers. Code is available at
https://github.com/ros-event-camera/event_image_reconstruction_fibar

</details>


### [2] [DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering](https://arxiv.org/abs/2510.20285)
*Jiayi Zou,Chaofan Chen,Bing-Kun Bao,Changsheng Xu*

Main category: cs.CV

TL;DR: 提出DMC³框架，通过反事实样本构建和对比优化解决第一人称视频问答中的多事件理解和手物交互识别挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了第一人称视角带来的独特挑战，如理解多个事件和识别手物交互。

Method: 包含反事实样本构建模块（通过事件描述转述和核心交互挖掘生成正负样本）和对比优化模块（使用对比损失优化特征距离）。

Result: 在EgoTaskQA的normal和indirect分割上分别达到52.51%和46.04%，在QAEGO4D上达到13.2%，均达到最先进性能。

Conclusion: DMC³框架有效解决了第一人称视频问答中的独特挑战，取得了state-of-the-art的性能。

Abstract: Egocentric Video Question Answering (Egocentric VideoQA) plays an important
role in egocentric video understanding, which refers to answering questions
based on first-person videos. Although existing methods have made progress
through the paradigm of pre-training and fine-tuning, they ignore the unique
challenges posed by the first-person perspective, such as understanding
multiple events and recognizing hand-object interactions. To deal with these
challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
(DMC$^3$) framework, which contains an egocentric videoqa baseline, a
counterfactual sample construction module and a counterfactual sample-involved
contrastive optimization. Specifically, We first develop a counterfactual
sample construction module to generate positive and negative samples for
textual and visual modalities through event description paraphrasing and core
interaction mining, respectively. Then, We feed these samples together with the
original samples into the baseline. Finally, in the counterfactual
sample-involved contrastive optimization module, we apply contrastive loss to
minimize the distance between the original sample features and the positive
sample features, while maximizing the distance from the negative samples.
Experiments show that our method achieve 52.51\% and 46.04\% on the
\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
QAEGO4D, both reaching the state-of-the-art performance.

</details>


### [3] [Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation](https://arxiv.org/abs/2510.20596)
*Ziyu Ye,Chen Ju,Chaofan Ma,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 提出基于相似性原型的跨模态分割框架，通过类别原型学习和相似性约束来解决领域自适应问题，在嵌入空间中学习代表性且可分离的类别原型。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在面对未见数据时性能会急剧下降，对领域偏移敏感。无监督领域自适应旨在减少领域差距，避免对未见领域进行昂贵标注。

Method: 在嵌入空间中学习类别原型，引入相似性约束使原型对每个语义类别具有代表性且不同类别间可分离。使用字典存储来自不同图像的原型，防止类别缺失问题并支持原型对比学习。

Result: 大量实验表明，该方法比其他最先进方法取得了更好的结果。

Conclusion: 基于相似性原型的跨模态分割框架能有效解决领域自适应问题，通过原型学习和对比学习提升性能。

Abstract: Deep learning models have achieved great success on various vision
challenges, but a well-trained model would face drastic performance degradation
when applied to unseen data. Since the model is sensitive to domain shift,
unsupervised domain adaptation attempts to reduce the domain gap and avoid
costly annotation of unseen domains. This paper proposes a novel framework for
cross-modality segmentation via similarity-based prototypes. In specific, we
learn class-wise prototypes within an embedding space, then introduce a
similarity constraint to make these prototypes representative for each semantic
class while separable from different classes. Moreover, we use dictionaries to
store prototypes extracted from different images, which prevents the
class-missing problem and enables the contrastive learning of prototypes, and
further improves performance. Extensive experiments show that our method
achieves better results than other state-of-the-art methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: 使用监督学习和决策树分类器在CAD建模的平面图中对RFID信号进行位置推断，以解决国防资产存储中的安全监控问题。


<details>
  <summary>Details</summary>
Motivation: RFID跟踪在国防资产存储中存在传感器特异性差的问题（如远距离检测、欺骗和伪造漏洞），可能导致错误检测和操作安全事件。

Method: 采用监督学习模拟，使用真实的接收信号强度指示器(RSSI)数据和决策树分类，在CAD建模的平面图中对12个实验室区域进行分类。

Result: 模型在5000个平衡观测值上训练，总体准确率为34.2%，多个区域(F、G、H等)的F1分数超过0.40，但稀有类别(特别是LabZoneC)经常被错误分类。

Conclusion: 基于RSSI的决策树可以在实际模拟中应用，实现区域级异常检测或误放监控，通过改进天线布局或添加传感器融合可提高低覆盖区域的分类性能。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [5] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 提出一种基于拒绝采样的并行采样算法，可在不改变模型结构或重新训练的情况下，对时间点过程模型进行精确的并行采样。


<details>
  <summary>Details</summary>
Motivation: 传统时间点过程模型采用自回归方式采样，效率低下且无法并行化，限制了在大规模应用中的使用。

Method: 基于拒绝采样的并行采样算法，能够从现有TPP模型中并行采样多个未来事件值。

Result: 方法在真实数据集上实现了经验性加速，提供了理论保证。

Conclusion: 该方法在保持表达性建模的同时，实现了高效并行生成，为大规模TPP应用提供了解决方案。

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [6] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 提出了一种完全解耦的训练策略来解决原型自监督学习中的原型崩溃问题，通过分离原型和编码器的优化目标，使用在线EM风格过程更新原型，无需显式正则化即可获得多样化的原型和更好的下游性能。


<details>
  <summary>Details</summary>
Motivation: 原型自监督学习方法普遍存在部分原型崩溃问题，即多个原型收敛到几乎相同的表示，这破坏了提供多样化目标来指导编码器学习丰富表示的核心目的。现有方法通过过度参数化原型集或添加临时正则化来缓解症状而非解决根本原因。

Method: 引入完全解耦的训练策略，将原型和编码器在不同目标下分别学习。具体将原型建模为高斯混合模型，使用在线EM风格过程独立于编码器损失进行更新。

Result: 这种简单而原则性的解耦消除了原型崩溃，无需显式正则化即可获得持续多样化的原型和更强的下游性能。

Conclusion: 通过打破原型和编码器的联合优化，解决了原型崩溃的根本原因，提供了一种更有效的原型自监督学习方法。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [7] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 提出了一种基于优化确定性等价的风险感知约束强化学习框架，通过强拉格朗日对偶性确保与原始约束问题的等价性，并可与标准RL求解器结合使用。


<details>
  <summary>Details</summary>
Motivation: 传统约束优化RL只关注期望累积奖励，忽略了奖励分布尾部的风险事件，无法满足高风险应用中对于异常值风险的关键需求。

Method: 使用优化确定性等价在奖励值和时间上联合实现每阶段鲁棒性，基于参数化强拉格朗日对偶框架确保与原始约束问题的等价性。

Result: 在适当约束条件下建立了与原始问题的等价性，开发了可包装标准RL求解器的简单算法，并通过数值实验验证了风险感知特性。

Conclusion: 该框架为高风险RL应用提供了有效的风险感知约束优化方法，具有良好的收敛性和实际可行性。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [8] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 该研究评估了使用金毛猎犬寿命研究队列的常规实验室数据进行癌症风险分类的可行性，发现虽然存在可检测的癌症信号，但性能不足以用于可靠的临床筛查。


<details>
  <summary>Details</summary>
Motivation: 开发犬类早期癌症检测的无创筛查工具是兽医医学的重要挑战，常规实验室数据提供了低成本来源，但受到生物标志物非特异性和类别不平衡的限制。

Method: 在金毛猎犬寿命研究队列上进行了全面基准评估，系统比较了126个分析流程，包括多种机器学习模型、特征选择方法和数据平衡技术，采用患者级数据分区防止泄漏。

Result: 最优模型（逻辑回归分类器）显示出中等排序能力（AUROC=0.815）但临床分类性能较差（F1-score=0.25），SHAP分析显示预测主要依赖年龄、炎症和贫血等非特异性特征。

Conclusion: 常规实验室数据中存在统计可检测的癌症信号，但信号太弱且与正常衰老或其他炎症状况混淆，无法实现可靠的临床区分，需要整合多模态数据源才能取得有意义的进展。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [9] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: 提出了一种针对标记时间点过程中事件标记分布不平衡问题的阈值方法，通过调整标记概率来优化预测性能，特别针对稀有标记事件。


<details>
  <summary>Details</summary>
Motivation: 现实应用中事件标记分布高度不平衡，现有方法难以准确预测稀有标记事件，这限制了MTPP模型在下一事件预测中的性能。

Method: 采用阈值方法学习阈值来调整标记概率，先预测标记再预测时间；开发了新的神经MTPP模型，支持有效时间采样和标记概率估计，避免计算昂贵的数值积分。

Result: 在真实世界数据集上的广泛实验表明，该方法在下一事件标记和时间预测方面优于各种基线方法。

Conclusion: 提出的阈值方法和神经MTPP模型有效解决了事件标记分布不平衡问题，显著提升了稀有标记事件的预测性能。

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [10] [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
*Xuan Lin,Aocheng Ding,Tengfei Ma,Hua Liang,Zhe Quan*

Main category: cs.LG

TL;DR: MolBridge是一个基于原子级联合图优化的DDI事件预测框架，通过构建药物对的联合图来直接建模分子间关联，解决了现有方法无法显式建模跨分子原子级交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DDI预测方法依赖孤立的药物表示，无法显式建模原子级的跨分子交互，限制了在不同分子复杂度和DDI类型分布下的有效性。

Method: 构建药物对的联合图整合原子结构，引入结构一致性模块迭代优化节点特征同时保持全局结构上下文，有效学习局部和全局交互模式。

Result: 在两个基准数据集上的广泛实验表明，MolBridge在长尾和归纳场景下均优于现有最优基线，实现了优越性能。

Conclusion: 细粒度图优化显著提高了DDI事件预测的准确性、鲁棒性和机制可解释性，为药物-药物相互作用网络的挖掘和分析提供了有效的图基方法。

Abstract: Drug combinations offer therapeutic benefits but also carry the risk of
adverse drug-drug interactions (DDIs), especially under complex molecular
structures. Accurate DDI event prediction requires capturing fine-grained
inter-drug relationships, which are critical for modeling metabolic mechanisms
such as enzyme-mediated competition. However, existing approaches typically
rely on isolated drug representations and fail to explicitly model atom-level
cross-molecular interactions, limiting their effectiveness across diverse
molecular complexities and DDI type distributions. To address these
limitations, we propose MolBridge, a novel atom-level joint graph refinement
framework for robust DDI event prediction. MolBridge constructs a joint graph
that integrates atomic structures of drug pairs, enabling direct modeling of
inter-drug associations. A central challenge in such joint graph settings is
the potential loss of information caused by over-smoothing when modeling
long-range atomic dependencies. To overcome this, we introduce a structure
consistency module that iteratively refines node features while preserving the
global structural context. This joint design allows MolBridge to effectively
learn both local and global interaction outperforms state-of-the-art baselines,
achieving superior performance across long-tail and inductive scenarios.
patterns, yielding robust representations across both frequent and rare DDI
types. Extensive experiments on two benchmark datasets show that MolBridge
consistently. These results demonstrate the advantages of fine-grained graph
refinement in improving the accuracy, robustness, and mechanistic
interpretability of DDI event prediction.This work contributes to Web Mining
and Content Analysis by developing graph-based methods for mining and analyzing
drug-drug interaction networks.

</details>


### [11] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 提出Hurdle-IMDL框架解决遥感降水反演中的标签不平衡问题，通过分解为零膨胀和长尾分布两部分，分别用hurdle模型和逆模型去偏学习处理，显著改善了强降水的反演性能。


<details>
  <summary>Details</summary>
Motivation: 人工智能在定量遥感中受限于不平衡的标签分布，传统训练模型偏向常见样本，导致对罕见样本（如强降水）的检索性能下降。

Method: 采用分治策略：1）用hurdle模型处理零膨胀（非降水样本占主导）；2）提出IMDL方法处理长尾分布（轻降水样本过多），将学习目标转换为无偏的理想逆模型。

Result: 通过统计指标和案例研究验证，Hurdle-IMDL优于传统、代价敏感、生成和多任务学习方法，有效缓解系统低估，显著改善强到极端降水的反演。

Conclusion: IMDL为处理环境变量分布不平衡提供了通用方法，能够增强对罕见但高影响事件的检索能力。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [12] [Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach](https://arxiv.org/abs/2510.20629)
*Mingxuan Liu,Yilin Ning,Haoyuan Wang,Chuan Hong,Matthew Engelhard,Danielle S. Bitterman,William G. La Cava,Nan Liu*

Main category: cs.LG

TL;DR: 提出了一种公平感知生存建模方法FASM，用于缓解生存分析中的算法偏见，特别关注组内和跨组风险排序的公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在医疗保健中的应用日益增多，但临床数据中的结构性不平等和社会偏见可能被数据驱动模型延续甚至放大。生存分析中的删失和时间动态进一步增加了公平模型开发的复杂性。现有算法公平方法往往忽视跨组排名差异，这可能导致高风险黑人患者排在低风险白人患者之后，强化生物本质主义并损害公平护理。

Method: 提出Fairness-Aware Survival Modeling (FASM)方法，旨在缓解关于组内和跨组风险排序随时间变化的算法偏见。使用乳腺癌预后作为代表性案例，并将FASM应用于SEER乳腺癌数据。

Result: FASM显著提高了公平性，同时保持了与不考虑公平性的生存模型相当的判别性能。时间分层评估显示FASM在10年时间范围内保持稳定的公平性，在随访中期观察到最大的改进。

Conclusion: 该方法使得开发既重视准确性又重视公平性的生存模型成为可能，将公平性作为临床护理的核心原则推进。

Abstract: As machine learning models become increasingly integrated into healthcare,
structural inequities and social biases embedded in clinical data can be
perpetuated or even amplified by data-driven models. In survival analysis,
censoring and time dynamics can further add complexity to fair model
development. Additionally, algorithmic fairness approaches often overlook
disparities in cross-group rankings, e.g., high-risk Black patients may be
ranked below lower-risk White patients who do not experience the event of
mortality. Such misranking can reinforce biological essentialism and undermine
equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed
to mitigate algorithmic bias regarding both intra-group and cross-group risk
rankings over time. Using breast cancer prognosis as a representative case and
applying FASM to SEER breast cancer data, we show that FASM substantially
improves fairness while preserving discrimination performance comparable to
fairness-unaware survival models. Time-stratified evaluations show that FASM
maintains stable fairness over a 10-year horizon, with the greatest
improvements observed during the mid-term of follow-up. Our approach enables
the development of survival models that prioritize both accuracy and equity in
clinical decision-making, advancing fairness as a core principle in clinical
care.

</details>


### [13] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: xTime是一个用于时间序列极端事件预测的新框架，通过知识蒸馏和专家混合机制，显著提升了极端事件的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列中的极端事件（如洪水、热浪、医疗紧急事件）具有重要实际意义，但现有模型因数据不平衡和忽略中间事件信息而难以准确预测。

Method: 使用知识蒸馏从低频事件模型向高频事件模型传递信息，并引入专家混合机制动态选择和融合不同频率级别的专家模型输出。

Result: 在多个数据集上的实验表明，xTime在极端事件上的预测准确率从3%提升到78%。

Conclusion: xTime框架通过知识蒸馏和专家混合机制有效解决了极端事件预测中的数据不平衡问题，显著提升了预测性能。

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [14] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 通过约束评分优化模型改进约翰霍普金斯跌倒风险评估工具，结合临床知识和电子健康记录数据，显著提升跌倒风险预测性能，同时保持模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动方法更好地将JHFRAT跌倒风险预测与其他临床有意义指标对齐，改进住院患者跌倒预防方案和患者安全。

Method: 对54,209例住院患者进行回顾性分析，使用约束评分优化模型结合JHFRAT评估数据和额外EHR变量，并与当前JHFRAT和XGBoost基准模型比较。

Result: CSO模型预测性能显著优于当前JHFRAT（AUC-ROC=0.91 vs 0.86），与EHR变量结合后性能相似，虽略低于XGBoost（AUC-ROC=0.94），但对风险标签变化更具鲁棒性。

Conclusion: 这种基于证据的方法为医疗系统使用数据驱动优化技术系统性地增强住院患者跌倒预防方案和患者安全提供了坚实基础，有助于改进风险评估和资源分配。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [15] [Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2510.20718)
*Daniel Sorensen,Bappaditya Dey,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 提出了两种新颖的异常预测方法：基于N-BEATS的单变量预测和基于图神经网络的多变量关系建模，用于半导体制造中的实时故障预测。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程复杂且精度要求高，传统异常检测方法面临高维数据、类别不平衡和变量间复杂依赖关系的挑战，需要从异常检测向异常预测发展以实现实时过程校正。

Method: 采用两阶段框架：首先在无异常数据集上训练预测模型，然后在未见数据上进行预测并与训练信号预测比较，超出阈值的偏差标记为异常。使用N-BEATS（单变量）和GNN（多变量）两种预测模型。

Result: 两种模型在20个时间点内表现出强预测性能，异常预测稳定性可达50个时间点。GNN模型在性能上持续优于N-BEATS，且需要更少的可训练参数和计算成本。

Conclusion: GNN模型是制造环境中在线异常预测的有前景解决方案，能够有效捕获变量间关系并实现稳定预测。

Abstract: Semiconductor manufacturing is an extremely complex and precision-driven
process, characterized by thousands of interdependent parameters collected
across diverse tools and process steps. Multi-variate time-series analysis has
emerged as a critical field for real-time monitoring and fault detection in
such environments. However, anomaly prediction in semiconductor fabrication
presents several critical challenges, including high dimensionality of sensor
data and severe class imbalance due to the rarity of true faults. Furthermore,
the complex interdependencies between variables complicate both anomaly
prediction and root-cause-analysis. This paper proposes two novel approaches to
advance the field from anomaly detection to anomaly prediction, an essential
step toward enabling real-time process correction and proactive fault
prevention. The proposed anomaly prediction framework contains two main stages:
(a) training a forecasting model on a dataset assumed to contain no anomalies,
and (b) performing forecast on unseen time series data. The forecast is
compared with the forecast of the trained signal. Deviations beyond a
predefined threshold are flagged as anomalies. The two approaches differ in the
forecasting model employed. The first assumes independence between variables by
utilizing the N-BEATS model for univariate time series forecasting. The second
lifts this assumption by utilizing a Graph Neural Network (GNN) to capture
inter-variable relationships. Both models demonstrate strong forecasting
performance up to a horizon of 20 time points and maintain stable anomaly
prediction up to 50 time points. The GNN consistently outperforms the N-BEATS
model while requiring significantly fewer trainable parameters and lower
computational cost. These results position the GNN as promising solution for
online anomaly forecasting to be deployed in manufacturing environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: 本研究首次对大型语言模型在生命周期评估中的表现进行了专家基准测试，评估了11个通用LLM在22个LCA相关任务上的表现，发现37%的回复包含不准确或误导性信息，幻觉率高达40%，但同时也显示了在解释质量和简化简单任务方面的优势。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和大型语言模型在生命周期评估中的应用日益增多，但缺乏对其可靠性、鲁棒性和可用性的系统评估。本研究旨在填补这一空白，为LCA领域提供首个基于专家评估的LLM基准测试。

Method: 评估了11个通用LLM（包括商业和开源模型）在22个LCA相关任务上的表现，由17位经验丰富的从业者根据科学准确性、解释质量、鲁棒性、可验证性和指令遵循等标准对模型输出进行评审，共收集了168份专家评审。

Result: 专家认为37%的回复包含不准确或误导信息；准确性和解释质量评分普遍为中等或良好；幻觉率差异显著，某些模型的引用幻觉率高达40%；开源模型在准确性和解释质量等标准上与闭源模型表现相当或更优。

Conclusion: 研究结果强调了在LCA中简单应用LLM的风险，特别是将其视为自由形式的神谕时，但也显示了在解释质量和减轻简单任务劳动强度方面的益处。没有基础机制支持的通用LLM使用存在风险。

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [17] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: 本博士论文通过改进预训练神经语言模型的迁移学习方法，提升了序列标注任务的性能，提出了三种改进策略：多任务模型、架构修改和生成式上下文微调框架。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在序列标注任务中的迁移学习效果有待提升，需要更有效的适应方法来充分发挥模型潜力。

Method: 1) 多任务模型引入领域无关文本处理系统的额外信号；2) 修改自回归大语言模型架构，实现层间双向信息流；3) 采用生成式监督上下文微调框架，结合响应导向适应策略。

Result: 提出的模型、方法和框架表明，通过针对性迁移学习范式适应的预训练神经语言模型在序列标注任务中能达到最佳性能。

Conclusion: 预训练语言模型在序列标注任务中的最佳性能需要通过有针对性的迁移学习策略来实现，包括多任务学习、架构修改和上下文微调等方法。

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [18] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: ToolScope通过自动合并冗余工具和智能检索相关工具，解决了LLM工具使用中的冗余和上下文限制问题，显著提升了工具选择准确率。


<details>
  <summary>Details</summary>
Motivation: 现实工具集中存在大量冗余工具，工具名称和描述重叠导致选择模糊性，同时LLM面临严格的输入上下文限制，无法有效处理大型工具集。

Method: 提出ToolScope框架，包含：(1) ToolScopeMerger带自动校正功能，自动审计和修复工具合并以减少冗余；(2) ToolScopeRetriever对每个查询进行工具排序和选择，压缩工具集以适应上下文限制。

Result: 在三个最先进LLM和三个开源工具使用基准测试中，工具选择准确率提升了8.38%到38.6%。

Conclusion: ToolScope能有效增强LLM的工具使用能力，解决工具冗余和上下文限制问题。

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>
