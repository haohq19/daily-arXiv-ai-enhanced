<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD：基于交叉伪标签的双分支弱监督视频异常检测框架，通过异常检测分支和类别分类分支交换伪标签，结合时间定位精度与语义判别能力，在异常检测和异常类别识别上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 弱监督视频异常检测需要同时检测异常并识别异常类别，但现有方法在时间定位精度和语义判别能力之间存在权衡。需要一种能够结合两者优势的方法。

Method: 提出CPL-VAD双分支框架：1）二元异常检测分支专注于片段级异常定位；2）类别分类分支利用视觉-语言对齐识别异常事件类别。两个分支通过交换伪标签相互增强，实现互补优势的结合。

Result: 在XD-Violence和UCF-Crime数据集上的实验表明，CPL-VAD在异常检测和异常类别分类任务上均达到了最先进的性能。

Conclusion: CPL-VAD通过交叉伪标签机制有效结合了时间定位精度和语义判别能力，为弱监督视频异常检测提供了一种有效的解决方案。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [2] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: 提出ComptonUNet混合深度学习框架，用于在低光子统计和高背景噪声条件下实现伽马射线暴的鲁棒定位，相比现有方法显著提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 微弱伽马射线暴（GRBs）能提供早期恒星形成的独特见解，但由于低光子统计和强背景噪声，检测和定位这些弱源仍然具有挑战性。现有机器学习模型难以在统计鲁棒性和噪声抑制之间取得平衡。

Method: 提出ComptonUNet混合深度学习框架，联合处理原始数据并重建图像，结合直接重建模型的统计效率和基于图像架构的去噪能力，在低光子统计和强背景污染条件下有效工作。

Result: 通过模拟低地球轨道任务背景环境中的GRB样事件进行评估，ComptonUNet显著优于现有方法，在广泛的低统计和高背景场景中实现了改进的定位精度。

Conclusion: ComptonUNet为微弱伽马射线暴的检测和定位提供了有效的解决方案，在低光子统计和高背景噪声条件下表现出优越性能，有望推动高能天体物理研究。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [3] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker：通过强化微调构建事件级场景图并增强视觉基础，减少视频推理中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 视频推理需要理解事件间的因果关系，但这些关系通常是隐式的且标注成本高。现有MLLM通过密集描述或视频摘要进行推理，缺乏因果结构建模，导致推理中出现幻觉问题。

Method: 1) 使用MLLM构建事件级视频场景图(EVSG)，显式建模事件内和事件间关系；2) 将场景图作为中间思维过程融入MLLM；3) 在强化微调中引入视觉注意力奖励，增强视频基础并减少幻觉。

Result: 在RexTime和VidHalluc数据集上评估，GraphThinker在捕捉对象和事件关系方面表现优异，具有更精确的事件定位能力，相比先前方法显著减少了视频推理中的幻觉。

Conclusion: 通过显式建模事件级场景图结构并增强视觉基础，GraphThinker有效减少了视频推理中的幻觉问题，提升了因果理解能力。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [4] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: ArtToMus：首个直接根据艺术作品生成音乐的框架，无需图像到文本转换或语言监督，使用视觉嵌入指导潜在扩散模型生成音乐。


<details>
  <summary>Details</summary>
Motivation: 现有图像条件音乐生成系统存在两个根本限制：1）通常在自然照片上训练，难以捕捉艺术作品更丰富的语义、风格和文化内容；2）大多依赖图像到文本转换阶段，使用语言作为语义捷径，阻碍了直接的视觉到音频学习。

Method: 提出ArtSound数据集（105,884个艺术作品-音乐配对，带双模态描述），并开发ArtToMus框架，将视觉嵌入投影到潜在扩散模型的调节空间，实现无需语言监督的直接艺术作品到音乐生成。

Result: ArtToMus生成音乐连贯、风格一致，能反映源艺术作品的显著视觉线索。虽然绝对对齐分数低于文本条件系统（考虑到去除语言监督的难度），但在感知质量和有意义的跨模态对应方面具有竞争力。

Conclusion: 这项工作确立了直接视觉到音乐生成作为一个独特且具有挑战性的研究方向，为多媒体艺术、文化遗产和AI辅助创意实践提供了资源和支持。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 论文通过几何分析发现，Transformer在模块化算术任务中的grokking现象源于从低维执行子空间的亚稳态逃逸，横向曲率增长先于泛化发生。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer在小型算法任务中出现的"grokking"现象——从记忆到泛化的延迟转变机制，目前仍缺乏清晰的理论解释。

Method: 1) 对注意力权重轨迹进行PCA分析，识别低维执行子空间；2) 测量交换子缺陷（连续梯度步的非交换性）来探测损失景观几何；3) 将曲率投影到学习到的子空间；4) 进行因果干预实验验证几何机制。

Result: 1) 训练主要发生在低维执行子空间内（单个主成分解释68-83%方差）；2) 横向于执行子空间的方向曲率急剧增长；3) 曲率增长始终先于泛化发生，且领先时间服从幂律关系；4) 沿学习子空间的运动对grokking是必要的，而人为增加曲率不足够。

Conclusion: grokking反映了从亚稳态区域的逃逸，该区域特征为低维约束和横向曲率积累。正交梯度流对grokking是必要但不充分的，所有发现在不同学习率范围和随机种子下均可复现。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [6] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 论文提出残差感知累积注意力展开理论，解释因果Transformer中位置偏置的架构起源，说明残差连接防止注意力崩溃，并证明有限深度下形成U形位置偏置，为"迷失在中间"现象提供理论解释。


<details>
  <summary>Details</summary>
Motivation: Transformer模型系统性地偏好某些token位置，但这种位置偏置的架构起源仍不清楚。先前理论分析预测因果掩码下注意力会崩溃到第一个token，但这在实际中并未发生，需要解决这一理论与实践的差异。

Method: 提出残差感知累积注意力展开理论，将残差连接纳入分析框架。在无限深度条件下证明残差连接防止注意力崩溃，在有限深度下分析因果Transformer的位置偏置模式。

Result: 证明残差连接在现实条件下防止注意力崩溃。在有限深度下，因果Transformer产生U形位置偏置，注意力集中在早期和晚期token上，这为"迷失在中间"现象提供了架构层面的理论解释。

Conclusion: 残差连接是Transformer位置偏置的关键架构因素，防止注意力崩溃并导致U形位置偏置模式。该理论解释了Transformer中观察到的"迷失在中间"现象，为理解注意力机制提供了新的理论框架。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [7] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 研究离线数据驱动的报童问题，需求数据被库存水平截断，只能观测到销售数据而非真实需求。提出计算经典库存策略在最坏情况下的精确后悔值的方法，并证明无限维非凸优化可简化为有限维问题。分析显示需求截断限制了从被动销售数据中学习的能力，但少量有针对性的探索可显著改善性能。


<details>
  <summary>Details</summary>
Motivation: 传统报童问题研究通常假设需求数据完全可观测，但实际零售场景中需求常被库存水平截断，只能观测到销售数据而非真实需求。现有研究对需求截断情况下的数据驱动库存策略性能缺乏精确分析，需要理解需求截断如何影响离线学习能力以及不同信息质量对策略性能的影响。

Method: 提出通用程序计算经典数据驱动库存策略在所有需求分布下的精确最坏情况后悔值。主要技术贡献是将无限维非凸优化问题简化为有限维问题，从而能够精确刻画任意样本量和截断水平下的策略性能。利用该简化分析标准库存策略在需求截断下的可实现性能。

Result: 对Kaplan-Meier策略的分析表明，需求截断从根本上限制了从被动销售数据中学习的能力，但在高库存水平进行少量有针对性的探索可显著改善最坏情况保证，即使在严重截断下也能实现接近最优的性能。相比之下，将销售视为需求的启发式方法会随着截断数据积累而遭受严重性能下降。

Conclusion: 销售点信息的质量对离线学习能力具有关键影响。需求截断限制了被动数据的学习能力，但通过有针对性的探索可以缓解这一问题。将销售视为需求的常见做法在截断数据积累时会导致性能严重下降，强调了在库存管理中正确处理截断数据的重要性。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [8] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 研究开发了针对巴西中老年人的痴呆症分类模型，结合变量选择和多元分析，使用低成本可干预变量，发现文盲、高龄、低体重、握力差、黑人、缺乏运动、听力损失和抑郁症状是痴呆风险因素，而高等教育、生活满意度和就业是保护因素。


<details>
  <summary>Details</summary>
Motivation: 建立适用于巴西中老年人群的痴呆症预测模型，使用低成本且具有干预潜力的变量，以帮助识别高危人群并指导公共卫生资源分配。

Method: 采用横断面设计的观察性研究，使用巴西老龄化纵向研究（ELSI-Brazil）的9,412名参与者数据。痴呆症通过神经心理学评估和知情者认知功能评估确定。使用随机森林和多元逻辑回归进行分析，结合变量选择方法。

Result: 痴呆症患病率为9.6%。主要风险因素包括：文盲（OR=7.42）、90岁以上（OR=11.00）、低体重（OR=2.11）、握力差（OR=2.50）、黑人（OR=1.47）、缺乏运动（OR=1.61）、听力损失（OR=1.65）和抑郁症状（OR=1.72）。保护因素包括高等教育（OR=0.44）、生活满意度高（OR=0.72）和就业（OR=0.78）。随机森林模型表现优于逻辑回归，AUC为0.776。

Conclusion: 研究证实了痴呆症的多维性，强调了使用可及性因素识别高危人群的重要性。加强关注脑健康的公共政策有助于优化初级保健资源分配和痴呆预防。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [9] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 研究发现，在序列学习任务中，交换子缺陷（梯度更新的非交换性度量）是延迟泛化的早期预警信号，与模块算术中的Grokking现象机制一致。


<details>
  <summary>Details</summary>
Motivation: Grokking现象（从记忆到泛化的突然转变）在模块算术中已被发现与低维执行流形相关，但这一机制是否适用于其他任务尚不清楚。本研究旨在探索序列学习任务中Grokking的普遍机制。

Method: 研究两个序列学习基准：SCAN组合泛化和Dyck-1深度预测。使用交换子缺陷（梯度更新的非交换性度量）作为指标，通过权重空间PCA分析谱集中性，并进行因果干预实验（放大非交换性和抑制正交梯度流）。

Result: 交换子缺陷在泛化前显著上升，提前时间遵循超线性幂律（SCAN约1.18，Dyck约1.13）。谱集中性不是普遍前兆，但交换子缺陷是。因果干预显示：放大非交换性加速Grokking（SCAN约32%，Dyck约50%），抑制正交梯度流延迟或阻止Grokking。

Conclusion: 交换子缺陷是Transformer架构中延迟泛化的稳健、架构无关、因果相关的早期预警信号。三个任务家族形成因果敏感性谱系，但抑制正交梯度流在所有情况下都会延迟或阻止Grokking，表明其必要性是普遍发现。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [10] [Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles](https://arxiv.org/abs/2602.17028)
*Hyeongwon Kang,Jinwoo Park,Seunghun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: FATE是一个无监督的时间序列异常预测框架，通过集成多个预测模型的预测不确定性来检测异常前兆，无需异常标签即可提供早期预警。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法大多是反应式的，只能在异常发生后检测，缺乏提供主动早期预警信号的能力。在工业运营、金融和网络安全等领域，早期识别异常模式对确保系统可靠性和实现预防性维护至关重要。

Method: FATE采用无监督框架，通过集成多个时间序列预测模型来量化预测不确定性。它预测未来值并利用集成模型之间的分歧来指示潜在异常的早期迹象，无需在推理时访问目标值。同时提出了PTaPR评估指标，综合考虑段级准确性、段内覆盖率和早期预测的时间及时性。

Result: 在五个真实世界基准数据集上的实验表明，FATE在PTaPR AUC上平均提高了19.9个百分点，在早期检测F1分数上平均提高了20.02个百分点，优于基线方法且无需异常标签。

Conclusion: FATE在复杂时间序列环境中实现了有效的实时无监督早期预警，展示了其在实际应用中的有效性和实用性。

Abstract: Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.

</details>


### [11] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 提出MPZCH索引机制，基于线性探测解决嵌入表碰撞问题，实现零碰撞并保持生产级效率


<details>
  <summary>Details</summary>
Motivation: 传统哈希索引方法在推荐系统中随着唯一ID数量增加会出现碰撞，导致模型性能下降和个性化质量降低

Method: 基于线性探测的多探测零碰撞哈希机制，使用辅助张量和高性能CUDA内核实现可配置探测和主动驱逐策略

Result: 实现用户嵌入零碰撞，显著提升项目嵌入新鲜度和质量，训练QPS和推理延迟与现有方法相当

Conclusion: MPZCH有效解决嵌入碰撞问题，已在TorchRec开源库中发布，为推荐系统提供高效索引解决方案

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [12] [AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation](https://arxiv.org/abs/2602.17071)
*Rong Fu,Muge Qi,Chunlei Meng,Shuo Yin,Kun Liu,Zhaolu Kang,Simon Fong*

Main category: cs.LG

TL;DR: AdvSynGNN：一种用于弹性节点级表示学习的综合架构，通过多分辨率结构合成、对比目标、自适应Transformer和对抗传播引擎来应对结构噪声和非同配拓扑的挑战。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在面对结构噪声或非同配拓扑时经常出现显著的性能下降，存在系统性脆弱性，需要开发能够适应这些挑战的弹性学习框架。

Method: 1. 多分辨率结构合成与对比目标建立几何敏感初始化；2. 自适应Transformer通过学习的拓扑信号调节注意力机制以适应异配性；3. 集成对抗传播引擎（生成器识别潜在连接变化，判别器强制全局一致性）；4. 基于节点置信度指标的残差校正方案进行标签细化。

Result: 经验评估表明，这种协同方法能有效优化不同图分布上的预测准确性，同时保持计算效率。

Conclusion: 研究提出了AdvSynGNN系统的实际实现协议，确保其在大规模环境中的稳健部署，为解决图神经网络的结构脆弱性问题提供了综合解决方案。

Abstract: Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.

</details>


### [13] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 论文提出了一个基于无服务器架构的MLOps框架，通过事件驱动管道管理完整的机器学习生命周期，并在HS代码预测的工业应用中验证了其实用性，实现了98%的准确率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决HS代码预测这一合规关键任务中的挑战：频繁更新、模糊描述导致分类困难，错误会造成货运延误和经济损失。同时需要解决ML模型工业化部署中的可重复性、可审计性和成本效率问题。

Method: 采用无服务器MLOps框架，使用事件驱动管道和托管服务。构建自定义文本嵌入编码器，结合多种深度学习架构（特别是Text-CNN），实现模型无关的标准化接口。包含自动A/B测试、动态模型选择和自动扩展功能。

Result: 在HS代码预测任务中，Text-CNN模型在真实数据上达到98%的准确率。框架确保了可重复性、可审计性和SLA遵守，在可变负载下通过自动扩展保持性能。相比Transformer模型，在保持相似准确率的同时显著降低了长期运营成本。

Conclusion: 该无服务器MLOps框架为企业提供了一个可复制的ML工业化蓝图，能够在优化性能和经济效益的同时实现规模化部署。虽然优先考虑确定性分类和可解释性，但架构可扩展支持Transformer变体和基于LLM的推理。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [14] [The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound](https://arxiv.org/abs/2602.17321)
*Christoph Balada,Aida Romano-Martinez,Payal Varshney,Vincent ten Cate,Katharina Geschke,Jonas Tesarz,Paul Claßen,Alexander K. Schuster,Dativa Tibyampansha,Karl-Patrik Kresoja,Philipp S. Wild,Sheraz Ahmed,Andreas Dengel*

Main category: cs.LG

TL;DR: 利用机器学习从颈动脉超声视频中提取血管损伤表征，以高血压为弱标签，实现心血管风险分层，性能媲美或优于传统风险模型


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，但早期风险检测受限于现有诊断方法。颈动脉超声作为非侵入性、广泛可及的模态，蕴含丰富的结构和血流动力学信息尚未被充分利用。

Method: 提出机器学习框架，从颈动脉超声视频中提取血管损伤的临床意义表征，使用高血压作为弱代理标签。模型学习具有生物学合理性、可解释性的稳健特征。

Result: 高血管损伤评分能有效分层心肌梗死、心源性死亡和全因死亡风险，匹配或优于SCORE2等传统风险模型。可解释AI分析显示模型依赖血管形态和血管周围组织特征。

Conclusion: 常规颈动脉超声包含比以往认知更多的预后信息。该方法为人群心血管风险评估提供了可扩展、非侵入性、成本效益高的工具，无需依赖实验室检测或复杂临床输入。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.

</details>


### [15] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 软DTW-CUDA-Torch：一个开源的PyTorch库，用于在GPU上计算软动态时间规整（SoftDTW），解决了现有GPU实现的三个关键限制：序列长度上限、数值不稳定性和内存消耗问题。


<details>
  <summary>Details</summary>
Motivation: 现有GPU实现存在三个主要问题：1）硬序列长度上限为1024；2）小平滑参数下反向传播数值不稳定；3）计算成对距离张量导致GPU内存消耗过大。这些限制阻碍了SoftDTW在大规模序列数据上的应用。

Method: 提出了三种关键技术：1）平铺反对角线核执行，消除序列长度限制；2）对数空间反向传播，防止浮点溢出；3）融合距离计算模式，消除O(BNM)中间距离张量，减少内存消耗。

Result: 该库支持任意序列长度，完整的PyTorch自动微分集成，以及软DTW重心计算。相比先前工作，内存减少高达98%。代码已开源。

Conclusion: softdtw-cuda-torch是一个高效、稳定且内存友好的GPU SoftDTW实现，解决了现有实现的局限性，为大规模序列分析提供了更好的工具支持。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [16] [LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy](https://arxiv.org/abs/2602.17312)
*Hsin-Jung Yang,Zhanhong Jiang,Prajwal Koirala,Qisai Liu,Cody Fleming,Soumik Sarkar*

Main category: cs.LG

TL;DR: LexiSafe是一个词典序离线安全强化学习框架，通过词典序优先级和结构偏置来防止安全漂移，为安全关键系统提供理论和实践保障。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全RL方法通常通过约束松弛或联合优化来平衡奖励-安全权衡，但缺乏防止安全漂移的结构机制。对于网络物理系统（CPS），训练期间的安全违规是不可接受的，且只有预收集数据可用。

Method: 提出LexiSafe词典序离线RL框架：1) LexiSafe-SC单成本公式用于标准离线安全RL，推导安全违规和性能次优性边界；2) LexiSafe-MC扩展支持多安全成本的层次安全需求，并进行样本复杂度分析。

Result: 经验上，LexiSafe相比约束离线基线方法减少了安全违规并提高了任务性能。理论上有样本复杂度保证，统一了词典序优先级与结构偏置。

Conclusion: LexiSafe为安全关键CPS决策提供了实用且理论可靠的方法，通过词典序优先级和结构偏置机制有效防止安全漂移。

Abstract: Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.

</details>


### [17] [From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection](https://arxiv.org/abs/2602.17342)
*Luzhi Wang,Xuanshuo Fu,He Zhang,Chuang Liu,Xiaobao Wang,Hongbo Liu*

Main category: cs.LG

TL;DR: SIGOOD是一个自改进的图OOD检测框架，通过提示增强和能量偏好优化实现无监督的图分布外检测。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法大多采用单次推理范式，无法渐进修正错误预测以放大OOD信号，需要更有效的无监督检测框架。

Method: 提出SIGOOD框架，通过生成提示构建提示增强图来放大OOD信号，引入能量偏好优化损失，在自改进循环中迭代优化提示，最终使用最优提示增强图进行OOD检测。

Result: 在21个真实世界数据集上的综合评估证实了SIGOOD的有效性和优越性能。

Conclusion: SIGOOD通过集成持续自学习和测试时训练，为图OOD检测提供了一个有效的无监督框架，能够渐进修正预测并放大OOD信号。

Abstract: Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \textbf{S}elf-\textbf{I}mproving \textbf{G}raph \textbf{O}ut-\textbf{o}f-\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态感知的安全监控框架，通过RNN架构追踪多轮对话中的用户意图演变，显著提升越狱攻击检测性能，同时保持实时推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护大多是无状态的，将多轮对话视为离散事件，导致无法检测跨轮次逐步积累的恶意意图（如Crescendo和ActorAttack攻击），存在"安全漏洞"。

Method: 提出DeepContext框架，采用RNN架构处理序列化的细粒度轮次嵌入，通过隐藏状态在对话中传播，捕捉风险累积过程，替代传统的孤立评估模型。

Result: 在越狱检测任务中达到SOTA的F1分数0.84，显著优于云服务商防护和开源模型（Llama-Prompt-Guard-2和Granite-Guardian均为0.67），在T4 GPU上保持20ms以下的推理延迟。

Conclusion: 建模意图的序列演化比部署大规模无状态模型更有效且计算高效，为实时应用提供了可行的状态感知安全解决方案。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [19] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过搜索-验证流程解决时间序列数据库的自然语言查询问题，使用SQL搜索候选窗口，然后用Python程序验证原始信号。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据，需要专门解决时间序列数据库自然语言查询的方案。

Method: 提出Sonar-TS神经符号框架，采用搜索-验证流程：1）使用特征索引通过SQL搜索候选窗口；2）生成Python程序锁定并验证候选窗口与原始信号的匹配。

Result: 实验表明Sonar-TS能有效处理传统方法失败的复杂时间查询，并创建了NLQTSBench作为首个大规模时间序列数据库自然语言查询基准。

Conclusion: 这是对时间序列数据库自然语言查询的首次系统研究，提供了通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [20] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 提出PA-MoE方法解决RL中单一策略网络导致的简单任务偏差问题，通过相位感知的专家混合架构让不同专家专注于不同任务阶段


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单一策略网络导致简单任务占据大部分参数和梯度更新，复杂任务得不到足够容量。传统MoE的token级路由会分散相位一致模式，破坏专家专业化

Method: 提出相位感知专家混合(PA-MoE)：1) 轻量级相位路由器直接从RL目标学习潜在相位边界，无需预定义相位类别；2) 相位路由器为相同专家分配时间一致的分配，让专家保持相位特定专业知识

Result: 实验结果证明了PA-MoE的有效性

Conclusion: PA-MoE通过相位感知路由解决了传统MoE在RL中的局限性，实现了更好的专家专业化，提升了RL代理解决复杂任务的能力

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [21] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLM预测中的时间知识泄漏，并开发TimeSPEC方法通过声明验证来减少泄漏


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测未来事件能力需要进行回溯测试，但模型可能在训练中无意间编码了截止日期后的知识，导致评估结果不可靠

Method: 1) 提出Shapley-DCLR指标：将模型推理分解为原子声明，按时间可验证性分类，用Shapley值衡量每个声明对预测的贡献；2) 开发TimeSPEC方法：在生成过程中插入声明验证和重新生成，主动过滤时间污染

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）的实验显示，标准提示基线存在显著泄漏。TimeSPEC在保持任务性能的同时降低了Shapley-DCLR

Conclusion: 显式的、可解释的声明级验证优于基于提示的时间约束，能够更可靠地进行回溯测试。TimeSPEC通过主动过滤时间污染，产生可追溯至截止日期前来源的预测

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: ReIn是一种无需修改模型参数或系统提示的测试时干预方法，通过植入初始推理来帮助对话代理从用户诱导的错误中恢复，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的对话代理在固定任务导向数据集上表现良好，但对用户诱导的意外错误很脆弱。现有方法主要关注错误预防而非恢复，且模型微调或提示修改成本高昂、耗时，因此需要一种无需修改模型参数和提示的错误恢复方法。

Method: 提出Reasoning Inception (ReIn)方法：外部起始模块识别对话上下文中的预定义错误并生成恢复计划，然后将这些计划整合到代理的内部推理过程中以指导纠正行动，整个过程不修改模型参数或系统提示。

Result: ReIn显著提高了任务成功率，能够泛化到未见过的错误类型，并且持续优于显式提示修改方法。在多种代理模型和起始模块组合下都表现良好。

Conclusion: ReIn是一种高效、即时的错误恢复方法，通过联合定义恢复工具可以作为提高对话代理弹性的安全有效策略，无需修改骨干模型或系统提示。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [23] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 论文提出修订的跨文档共指消解标注方案，将共指链视为话语元素，同时容纳身份和近身份关系，以分析新闻话语中的词汇多样性和框架变化。


<details>
  <summary>Details</summary>
Motivation: 现有跨文档共指消解数据集主要关注事件消解，采用狭窄的共指定义，限制了其在分析词汇变化广泛的多样化、极化新闻报道中的有效性。

Method: 提出修订的CDCR标注方案，将共指链视为话语元素和分析的概念单元，容纳身份和近身份关系。重新标注NewsWCL50数据集和ECB+子集，使用统一的标注手册。

Result: 重新标注的数据集在词汇多样性指标和相同词头词元基线评估中表现一致，介于原始ECB+和NewsWCL50之间，支持新闻领域平衡且具有话语意识的CDCR研究。

Conclusion: 修订的标注方案能够更好地捕捉新闻话语中的词汇多样性和框架变化，为新闻领域的跨文档共指消解研究提供了更平衡、更具话语意识的数据基础。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [24] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: 提出一种用于自主叉车在倾斜表面上卸货的控制方法，通过ICP算法实时跟踪货盘与货叉的相对位置和姿态，使货叉与目标表面平行对齐，从而避免拖拽货盘。


<details>
  <summary>Details</summary>
Motivation: 自主叉车在倾斜表面（如卡车倾斜床）上卸货时，传统方法可能导致货盘被拖拽损坏。需要一种控制方法使货叉能够平行于倾斜表面，实现无拖拽的平稳卸货。

Method: 使用迭代最近点（ICP）算法处理货盘上部区域测量的点云数据，实时跟踪货盘与货叉之间的相对位置和姿态角度差异。根据跟踪结果调整货叉姿态，使其与目标倾斜表面平行对齐，然后沿倾斜方向撤回货叉完成卸货。

Result: 通过动态仿真和真实叉车实验验证了方法的有效性，成功在卡车倾斜床等倾斜表面上实现了无拖拽的平稳卸货操作。

Conclusion: 提出的基于ICP算法的控制方法能够有效解决自主叉车在倾斜表面上卸货时的拖拽问题，通过实时姿态调整实现平稳卸货，具有实际应用价值。

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [25] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: 提出一种仅使用IMU和电机测量的纯本体感知状态估计器，通过接触腿作为运动学锚点、扭矩估计选择可靠接触、脚落位置提供世界坐标系约束来抑制长期漂移，适用于双足、四足和轮腿机器人。


<details>
  <summary>Details</summary>
Motivation: 解决无摄像头或LiDAR的腿式机器人里程计问题，IMU漂移和关节速度噪声使得可靠里程计具有挑战性，需要一种仅依赖本体传感器的状态估计方法。

Method: 1) 将接触腿作为运动学锚点，基于关节扭矩估计脚部力选择可靠接触；2) 脚落位置提供间歇性世界坐标系约束抑制漂移；3) 轻量级高度聚类和时间衰减校正防止高程漂移；4) 逆运动学立方卡尔曼滤波器改善脚端速度观测；5) 多接触几何一致性减少偏航漂移。

Result: 在四种四足平台上评估：Astrall点足机器人A完成约200米水平回路误差0.1638米，约15米垂直回路误差0.219米；轮腿机器人B相应误差为0.2264米和0.199米；轮腿机器人C约700米水平回路误差7.68米，约20米垂直回路误差0.540米；Unitree Go2 EDU约120米水平回路误差2.2138米，约8米垂直回路垂直误差小于0.1米。

Conclusion: 该方法仅使用IMU和电机测量就能实现可靠的位姿和速度估计，通过接触腿的运动学锚点约束有效抑制长期漂移，适用于多种腿式机器人平台，在水平和垂直方向都表现出良好的精度。

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [26] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow是一个无监督的安全关键异常检测框架，利用最优传输条件流匹配来建模专家人类驾驶行为的概率密度，通过PCA瓶颈约束生成过程到低秩谱流形，实现稳定、确定性的对数似然估计，用于自动驾驶安全验证。


<details>
  <summary>Details</summary>
Motivation: 当前L4级自动驾驶的安全验证面临瓶颈，传统基于规则的启发式方法难以扩展检测罕见的高风险长尾场景，需要更有效的异常检测方法来支持自动驾驶车队的安全部署。

Method: 1. 使用最优传输条件流匹配(OT-CFM)建模专家驾驶行为的连续概率密度；2. 通过PCA瓶颈将生成过程约束到低秩谱流形，确保运动学平滑性；3. 使用早期融合Transformer编码器处理多模态歧义，具有车道感知目标条件；4. 引入运动学复杂度加权方案，在无模拟训练过程中优先处理高能量机动；5. 计算精确的雅可比迹实现数值稳定的确定性对数似然估计。

Result: 在Waymo开放运动数据集(WOMD)上评估，框架实现了0.766的AUC-ROC（相对于安全关键事件的启发式黄金集）。更重要的是，分析揭示了运动学危险和语义违规之间的根本区别，Deep-Flow通过发现传统安全过滤器忽略的分布外行为（如车道边界违规和非规范路口机动）来识别关键的可预测性差距。

Conclusion: 这项工作为定义统计安全门提供了数学严谨的基础，使基于数据的客观验证成为可能，支持自动驾驶车队的安全部署。Deep-Flow能够识别传统方法忽略的安全关键异常，特别是在运动学危险和语义合规性之间的差异方面。

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>
