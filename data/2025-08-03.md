<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种结合Corner Loss和R-CBAM模块的图像去雨网络，有效保留细节并提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 单幅图像去雨任务需要同时抑制雨痕和保留细节，传统方法难以兼顾。

Method: 引入Corner Loss防止边界和纹理信息丢失，并设计R-CBAM模块动态调整特征重要性。

Result: 在Rain100L和Rain100H数据集上PSNR分别达到33.29 dB和26.16 dB，优于现有方法。

Conclusion: 该方法在去雨任务中表现出色，尤其在细节保留和视觉质量上取得显著提升。

Abstract: The problem of single-image rain streak removal goes beyond simple noise
suppression, requiring the simultaneous preservation of fine structural details
and overall visual quality. In this study, we propose a novel image restoration
network that effectively constrains the restoration process by introducing a
Corner Loss, which prevents the loss of object boundaries and detailed texture
information during restoration. Furthermore, we propose a Residual
Convolutional Block Attention Module (R-CBAM) Block into the encoder and
decoder to dynamically adjust the importance of features in both spatial and
channel dimensions, enabling the network to focus more effectively on regions
heavily affected by rain streaks. Quantitative evaluations conducted on the
Rain100L and Rain100H datasets demonstrate that the proposed method
significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on
Rain100L and 26.16 dB on Rain100H.

</details>


### [2] [FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks](https://arxiv.org/abs/2507.23643)
*Changqing Xu,Ziqiang Yang,Yi Liu,Xinfang Liao,Guiqi Mo,Hao Zeng,Yintang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于Forward-Forward（FF）的无梯度近似训练框架，用于高效训练SNN，并通过类感知复杂度适应机制优化资源分配，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: SNN因其生物可塑性和能效优势在神经形态计算中具有潜力，但传统训练方法因不可微分性和计算复杂度高而受限。

Method: 采用FF框架避免梯度近似，将脉冲激活视为黑盒模块，并引入类感知复杂度适应机制动态优化损失函数。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上分别达到99.58%、92.13%和75.64%的测试准确率，优于现有FF方法，且能效显著提升。

Conclusion: 提出的方法为SNN训练提供了一种高效、低复杂度的解决方案，适用于边缘设备部署。

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible framework for
energy-efficient neuromorphic computing. However, it is a challenge to train
SNNs due to their non-differentiability, efficiently. Existing gradient
approximation approaches frequently sacrifice accuracy and face deployment
limitations on edge devices due to the substantial computational requirements
of backpropagation. To address these challenges, we propose a Forward-Forward
(FF) based gradient approximation-free training framework for Spiking Neural
Networks, which treats spiking activations as black-box modules, thereby
eliminating the need for gradient approximation while significantly reducing
computational complexity. Furthermore, we introduce a class-aware complexity
adaptation mechanism that dynamically optimizes the loss function based on
inter-class difficulty metrics, enabling efficient allocation of network
resources across different categories. Experimental results demonstrate that
our proposed training framework achieves test accuracies of 99.58%, 92.13%, and
75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,
surpassing all existing FF-based SNN approaches. Additionally, our proposed
method exhibits significant advantages in terms of memory access and
computational power consumption.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations](https://arxiv.org/abs/2507.22962)
*Boyuan Zheng,Victor W. Chu*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度学习与可解释AI的多灾害预警框架，用于农业气候风险管理，实验验证了其高预测准确性和解释能力。


<details>
  <summary>Details</summary>
Motivation: 气候极端事件对农业的威胁日益加剧，传统单灾害预测方法无法捕捉复杂的气候交互作用，因此需要智能化的多灾害预警系统。

Method: 结合序列深度学习模型（如BiLSTM）和可解释AI技术（如TimeSHAP），构建多灾害预测框架，并利用美国四个农业区的气象数据进行验证。

Result: 实验结果显示该框架具有高预测准确性，尤其是BiLSTM架构，并能提供详细的时间解释，揭示关键气候特征及其影响时机。

Conclusion: 该研究显著提升了多灾害预警系统的可解释性和实用性，有助于农业气候风险管理的跨学科信任与决策。

Abstract: Climate extremes present escalating risks to agriculture intensifying the
need for reliable multi-hazard early warning systems (EWS). The situation is
evolving due to climate change and hence such systems should have the
intelligent to continue to learn from recent climate behaviours. However,
traditional single-hazard forecasting methods fall short in capturing complex
interactions among concurrent climatic events. To address this deficiency, in
this paper, we combine sequential deep learning models and advanced Explainable
Artificial Intelligence (XAI) techniques to introduce a multi-hazard
forecasting framework for agriculture. In our experiments, we utilize
meteorological data from four prominent agricultural regions in the United
States (between 2010 and 2023) to validate the predictive accuracy of our
framework on multiple severe event types, which are extreme cold, floods,
frost, hail, heatwaves, and heavy rainfall, with tailored models for each area.
The framework uniquely integrates attention mechanisms with TimeSHAP (a
recurrent XAI explainer for time series) to provide comprehensive temporal
explanations revealing not only which climatic features are influential but
precisely when their impacts occur. Our results demonstrate strong predictive
accuracy, particularly with the BiLSTM architecture, and highlight the system's
capacity to inform nuanced, proactive risk management strategies. This research
significantly advances the explainability and applicability of multi-hazard
EWS, fostering interdisciplinary trust and effective decision-making process
for climate risk management in the agricultural industry.

</details>


### [4] [Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation](https://arxiv.org/abs/2507.23000)
*Shengao Yi,Xiaojiang Li,Wei Tu,Tianhong Zhao*

Main category: cs.LG

TL;DR: GSM-UTCI是一种基于深度学习的框架，用于预测城市尺度的热应力，具有高精度和高效性。


<details>
  <summary>Details</summary>
Motivation: 极端热事件加剧，传统物理模型计算成本高，限制了城市尺度规划的应用。

Method: 提出GSM-UTCI，融合地表形态、土地覆盖数据和气象条件，采用FiLM架构动态调整空间特征。

Result: 模型R2为0.9151，MAE为0.41°C，计算时间从小时级降至分钟级，应用于费城绿化策略评估。

Conclusion: GSM-UTCI是城市气候适应的有效工具，支持精细化决策和绿化策略评估。

Abstract: As extreme heat events intensify due to climate change and urbanization,
cities face increasing challenges in mitigating outdoor heat stress. While
traditional physical models such as SOLWEIG and ENVI-met provide detailed
assessments of human-perceived heat exposure, their computational demands limit
scalability for city-wide planning. In this study, we propose GSM-UTCI, a
multimodal deep learning framework designed to predict daytime average
Universal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The
model fuses surface morphology (nDSM), high-resolution land cover data, and
hourly meteorological conditions using a feature-wise linear modulation (FiLM)
architecture that dynamically conditions spatial features on atmospheric
context. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical
accuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\deg}C,
while reducing inference time from hours to under five minutes for an entire
city. To demonstrate its planning relevance, we apply GSM-UTCI to simulate
systematic landscape transformation scenarios in Philadelphia, replacing bare
earth, grass, and impervious surfaces with tree canopy. Results show spatially
heterogeneous but consistently strong cooling effects, with impervious-to-tree
conversion producing the highest aggregated benefit (-4.18{\deg}C average
change in UTCI across 270.7 km2). Tract-level bivariate analysis further
reveals strong alignment between thermal reduction potential and land cover
proportions. These findings underscore the utility of GSM-UTCI as a scalable,
fine-grained decision support tool for urban climate adaptation, enabling
scenario-based evaluation of greening strategies across diverse urban
environments.

</details>


### [5] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: KLLM是一个硬件-软件协同设计框架，通过K-Means量化和高效的异常值检测，显著提升了LLM推理的速度和能效。


<details>
  <summary>Details</summary>
Motivation: 解决现有WAQ设计中因均匀量化导致的精度下降问题，以及激活异常值对低精度WAQ的阻碍。

Method: 提出KLLM框架，包括基于索引的计算方案和在线异常值检测引擎Orizuru。

Result: 实验显示，KLLM在A100 GPU和Atom上分别实现了9.67x和7.03x的速度提升，以及229.50x和150.21x的能效改进。

Conclusion: KLLM通过硬件-软件协同设计，有效解决了LLM推理中的量化挑战，显著提升了性能和能效。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [6] [Linking Actor Behavior to Process Performance Over Time](https://arxiv.org/abs/2507.23037)
*Aurélie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 论文提出了一种结合行为分析和Granger因果关系的时序方法，用于分析个体行为对流程结果的影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽略了行为的时间动态性，无法准确捕捉个体行为对流程的影响。

Method: 整合行为分析和Granger因果关系，利用Group Lasso选择滞后项，分析时序数据中的因果关系。

Result: 发现行为对流程性能（如吞吐时间）有直接且可测量的影响。

Conclusion: 基于行为和时间序列的方法能更细致地揭示个体行为对流程效率的影响。

Abstract: Understanding how actor behavior influences process outcomes is a critical
aspect of process mining. Traditional approaches often use aggregate and static
process data, overlooking the temporal and causal dynamics that arise from
individual actor behavior. This limits the ability to accurately capture the
complexity of real-world processes, where individual actor behavior and
interactions between actors significantly shape performance. In this work, we
address this gap by integrating actor behavior analysis with Granger causality
to identify correlating links in time series data. We apply this approach to
realworld event logs, constructing time series for actor interactions, i.e.
continuation, interruption, and handovers, and process outcomes. Using Group
Lasso for lag selection, we identify a small but consistently influential set
of lags that capture the majority of causal influence, revealing that actor
behavior has direct and measurable impacts on process performance, particularly
throughput time. These findings demonstrate the potential of actor-centric,
time series-based methods for uncovering the temporal dependencies that drive
process outcomes, offering a more nuanced understanding of how individual
behaviors impact overall process efficiency.

</details>


### [7] [An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items](https://arxiv.org/abs/2507.23303)
*Luca Corbucci,Javier Alejandro Borges Legrottaglie,Francesco Spinnato,Anna Monreale,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 论文提出了一种解决超市购物中遗忘物品预测问题的新方法，填补了现有NBP领域的空白，并提供了可解释的推荐算法。


<details>
  <summary>Details</summary>
Motivation: 现有NBP方法仅关注未来购买预测，未解决遗忘物品检测问题，且缺乏透明性。

Method: 提出了两种新颖的可解释算法，专门用于识别遗忘物品并提供直观解释。

Result: 在真实零售数据集上，新算法比现有NBP基线方法性能提升10-15%。

Conclusion: 论文填补了NBP领域的遗忘物品预测空白，并展示了可解释算法的优越性。

Abstract: Accurately identifying items forgotten during a supermarket visit and
providing clear, interpretable explanations for recommending them remains an
underexplored problem within the Next Basket Prediction (NBP) domain. Existing
NBP approaches typically only focus on forecasting future purchases, without
explicitly addressing the detection of unintentionally omitted items. This gap
is partly due to the scarcity of real-world datasets that allow for the
reliable estimation of forgotten items. Furthermore, most current NBP methods
rely on black-box models, which lack transparency and limit the ability to
justify recommendations to end users. In this paper, we formally introduce the
forgotten item prediction task and propose two novel interpretable-by-design
algorithms. These methods are tailored to identify forgotten items while
offering intuitive, human-understandable explanations. Experiments on a
real-world retail dataset show our algorithms outperform state-of-the-art NBP
baselines by 10-15% across multiple evaluation metrics.

</details>


### [8] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: 论文提出了一种基于量化脉冲神经网络（SNN）的强化学习算法，用于解决经典控制任务，并在SpiNNaker2神经形态芯片上实现低功耗部署。与GPU相比，SpiNNaker2在能耗上实现了高达32倍的降低，同时推理延迟与GPU相当。


<details>
  <summary>Details</summary>
Motivation: 探索SNN在神经形态硬件上的低功耗和低延迟潜力，特别是在强化学习任务中的应用。

Method: 使用Q学习算法训练SNN，然后对网络进行微调和8位量化，并在SpiNNaker2芯片上部署。

Result: SpiNNaker2在能耗上比GPU降低了32倍，推理延迟与GPU相当，部分任务中表现更优。

Conclusion: SpiNNaker2展示了在可扩展、低能耗神经形态计算中的强大潜力，为高效深度Q学习提供了有前景的方向。

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 论文提出并定义了预测中的‘论证一致性’属性，通过实验证明过滤不一致预测可提高准确性，但用户实验显示人们对此属性认知不足。


<details>
  <summary>Details</summary>
Motivation: 研究人类和LLM预测中论证一致性的作用，以提高预测准确性。

Method: 定义论证一致性，通过实验评估其对人类和LLM预测的影响，并进行用户实验。

Result: 过滤不一致预测显著提高准确性，但用户普遍未意识到其重要性。

Conclusion: 论证一致性对预测准确性至关重要，需在群体预测中引入过滤机制。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 论文提出了一种基于临床试验注册信息的预测模型，用于预测严重不良事件（SAE）的发生率，以优化试验设计并减少风险。


<details>
  <summary>Details</summary>
Motivation: 通过准确预测SAE结果，可以避免临床试验终止并减少参与者暴露于不必要的风险。

Method: 使用预训练语言模型（如ClinicalT5、BioBERT）进行特征提取，结合滑动窗口方法处理长文本，开发了分类和回归模型。

Result: 最佳模型（ClinicalT5+Transformer+MLP）在预测SAE发生率时AUC为77.6%，控制组SAE比例的RMSE为18.6%。

Conclusion: 临床试验注册数据未被充分利用，预测模型为优化试验设计和发现安全结果差异提供了机会。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [11] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

TL;DR: 论文提出了一种结合自然语言处理（NLP）和多波自适应采样的高效验证方法，用于验证基于代码的算法性能，减少人工审查时间并提高效率。


<details>
  <summary>Details</summary>
Motivation: 通过验证基于代码的算法的测量特性，可以增强大型索赔数据库的分析，但传统方法需要大量时间和资源。本研究旨在提高验证效率。

Method: 使用NLP减少人工审查时间，并采用多波自适应采样方法，通过预定义标准在达到足够精度时停止验证。

Result: NLP辅助注释减少了40%的审查时间，多波采样方法避免了77%的图表审查，且精度损失有限。

Conclusion: 该方法可促进基于代码的算法的常规验证，提高数据库研究结果的可靠性。

Abstract: Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [12] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
*Diego Garat,Guillermo Moncecchi,Dina Wonsever*

Main category: cs.CL

TL;DR: 论文研究了利用大型语言模型（LLMs）的上下文学习（ICL）进行框架语义解析（FSP），无需微调模型。通过自动生成任务特定提示，在FrameNet数据库上实现框架识别（FI）和框架语义角色标注（FSRL），并在暴力事件相关帧上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 探索无需微调模型的方法，利用ICL和LLMs完成FSP任务，提供一种更实用的替代方案。

Method: 自动生成任务特定提示，基于FrameNet数据库的框架定义和标注示例，指导六种不同LLMs完成FI和FSRL子任务。

Result: 在暴力事件相关帧上，FI的F1得分为94.3%，FSRL为77.4%。

Conclusion: ICL为特定领域的FSP任务提供了一种实用且有效的替代传统微调的方法。

Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling
their arguments according to Frame Semantics. This paper investigates the use
of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP
without model fine-tuning. We propose a method that automatically generates
task-specific prompts for the Frame Identification (FI) and Frame Semantic Role
Labeling (FSRL) subtasks, relying solely on the FrameNet database. These
prompts, constructed from frame definitions and annotated examples, are used to
guide six different LLMs. Experiments are conducted on a subset of frames
related to violent events. The method achieves competitive results, with F1
scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers
a practical and effective alternative to traditional fine-tuning for
domain-specific FSP tasks.

</details>


### [13] [Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration](https://arxiv.org/abs/2507.23407)
*Ante Wang,Yujie Lin,Jingyao Liu,Suhang Wu,Hao Liu,Xinyan Xiao,Jinsong Su*

Main category: cs.CL

TL;DR: 论文提出了一种主动批判性思维范式，要求AI模型主动寻求缺失或澄清信息以解决用户查询，并设计了GSM-MC和GSM-MCE两个新基准进行评估。实验表明，强化学习能显著提升模型在此任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注被动批判性思维（模型仅拒绝问题查询），而缺乏主动解决用户需求的能力。本文旨在填补这一空白，推动AI与用户更有效的协作。

Method: 引入主动批判性思维范式，设计GSM-MC（缺失关键变量）和GSM-MCE（含无关干扰）两个基准，使用强化学习（RL）提升模型表现。

Result: 实验显示，传统模型在主动批判性思维任务上表现不佳，但通过RL算法显著提升（如Qwen3-1.7B在GSM-MC上的准确率从0.15%提升至73.98%）。

Conclusion: 主动批判性思维能有效提升AI与用户协作解决问题的能力，强化学习是实现这一目标的有效方法。

Abstract: Critical thinking is essential for building robust AI systems, preventing
them from blindly accepting flawed data or biased reasoning. However, prior
work has primarily focused on passive critical thinking, where models simply
reject problematic queries without taking constructive steps to address user
requests. In this work, we introduce proactive critical thinking, a paradigm
where models actively seek missing or clarifying information from users to
resolve their queries better. To evaluate this capability, we present GSM-MC
and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical
reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math
problems with a key variable deliberately removed, requiring models to identify
and request the missing information. GSM-MCE further increases the difficulty
by introducing irrelevant details to test robustness against distractions.
Experiments on Qwen3 and Llama series models show that, while these models
excel in traditional reasoning tasks due to extensive post-training and
inference-time scaling, they struggle with proactive critical thinking,
especially smaller ones. However, we demonstrate that reinforcement learning
(RL) can significantly improve this ability. Using our enhanced RL algorithm,
we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to
73.98% on GSM-MC. We hope this work advances models that collaborate more
effectively with users in problem-solving through proactive critical thinking.

</details>


### [14] [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)
*Saeed Almheiri,Yerulan Kongrat,Adrian Santosh,Ruslan Tasmukhanov,Josemaria Vera,Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: 研究探讨如何通过微调大型语言模型（LLMs）使其根据用户角色生成响应，提出三种建模策略，并构建两个数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 企业环境中，LLMs的行为需基于用户角色进行控制，现有方法未解决角色特定访问限制问题。

Method: 探索三种策略：基于BERT的分类器、基于LLM的分类器、角色条件生成，并构建两个数据集进行评估。

Result: 评估模型在不同组织结构中的表现，分析其对提示注入、角色不匹配和越狱尝试的鲁棒性。

Conclusion: 研究表明LLMs可通过微调实现角色特定的响应生成，为实际企业应用提供了潜在解决方案。

Abstract: As large language models (LLMs) are increasingly deployed in enterprise
settings, controlling model behavior based on user roles becomes an essential
requirement. Existing safety methods typically assume uniform access and focus
on preventing harmful or toxic outputs, without addressing role-specific access
constraints. In this work, we investigate whether LLMs can be fine-tuned to
generate responses that reflect the access privileges associated with different
organizational roles. We explore three modeling strategies: a BERT-based
classifier, an LLM-based classifier, and role-conditioned generation. To
evaluate these approaches, we construct two complementary datasets. The first
is adapted from existing instruction-tuning corpora through clustering and role
labeling, while the second is synthetically generated to reflect realistic,
role-sensitive enterprise scenarios. We assess model performance across varying
organizational structures and analyze robustness to prompt injection, role
mismatch, and jailbreak attempts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [15] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种结合LiDAR、惯性和视觉的在线映射系统，通过全局位姿图优化和高效高斯初始化策略，解决了3D高斯溅射（3DGS）在稀疏数据和长优化时间上的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机传感器（包括RGB-D）的3DGS方法存在计算负载高、在纹理或光照差的环境中失效以及操作范围短的问题，而LiDAR虽为替代方案，但其与3DGS结合时面临全局对齐和稀疏数据的挑战。

Method: 提出GSFusion系统，采用全局位姿图优化中的面元到面元约束确保高精度地图一致性，并通过像素感知高斯初始化策略和边界Sigmoid约束处理稀疏数据。

Result: 在公开和自有数据集上的实验表明，该系统在渲染质量和地图构建效率上优于现有3DGS SLAM系统。

Conclusion: GSFusion通过创新方法有效解决了LiDAR与3DGS结合的挑战，提升了映射系统的性能和实用性。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,
conventional approaches based on camera sensor, even RGB-D, suffer from
fundamental limitations such as high computational load, failure in
environments with poor texture or illumination, and short operational ranges.
LiDAR emerges as a robust alternative, but its integration with 3DGS introduces
new challenges, such as the need for exceptional global alignment for
photorealistic quality and prolonged optimization times caused by sparse data.
To address these challenges, we propose GSFusion, an online
LiDAR-Inertial-Visual mapping system that ensures high-precision map
consistency through a surfel-to-surfel constraint in the global pose-graph
optimization. To handle sparse data, our system employs a pixel-aware Gaussian
initialization strategy for efficient representation and a bounded sigmoid
constraint to prevent uncontrolled Gaussian growth. Experiments on public and
our datasets demonstrate our system outperforms existing 3DGS SLAM systems in
terms of rendering quality and map-building efficiency.

</details>


### [16] [Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications](https://arxiv.org/abs/2507.23350)
*Mahmoud Ghorab,Matthias Lorenzen*

Main category: cs.RO

TL;DR: 论文提出了一种结合全局路径规划（基于DTSP）和局部路径控制（基于NMPC）的导航框架，用于农业环境中的自主移动机器人，显著优化了路径长度和平滑度。


<details>
  <summary>Details</summary>
Motivation: 农业环境中自主移动机器人需要高效导航，同时满足路径长度和曲率约束以避免土壤和植被破坏。

Method: 结合Dubins旅行商问题（DTSP）进行全局路径规划，使用非线性模型预测控制（NMPC）进行局部路径优化和控制。

Result: 仿真验证显示，该方法比解耦方法路径更平滑且缩短约16%，NMPC控制器能准确导航并满足约束。

Conclusion: 该框架在农业环境中具有高效自主导航的潜力。

Abstract: There is a growing demand for autonomous mobile robots capable of navigating
unstructured agricultural environments. Tasks such as weed control in meadows
require efficient path planning through an unordered set of coordinates while
minimizing travel distance and adhering to curvature constraints to prevent
soil damage and protect vegetation. This paper presents an integrated
navigation framework combining a global path planner based on the Dubins
Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control
(NMPC) strategy for local path planning and control. The DTSP generates a
minimum-length, curvature-constrained path that efficiently visits all targets,
while the NMPC leverages this path to compute control signals to accurately
reach each waypoint. The system's performance was validated through comparative
simulation analysis on real-world field datasets, demonstrating that the
coupled DTSP-based planner produced smoother and shorter paths, with a
reduction of about 16% in the provided scenario, compared to decoupled methods.
Based thereon, the NMPC controller effectively steered the robot to the desired
waypoints, while locally optimizing the trajectory and ensuring adherence to
constraints. These findings demonstrate the potential of the proposed framework
for efficient autonomous navigation in agricultural environments.

</details>
