{"id": "2602.16744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16744", "abs": "https://arxiv.org/abs/2602.16744", "authors": ["Takuro Kato", "Mitsuharu Morisawa"], "title": "ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts", "comment": "Accepted and published in IEEE/SICE SII 2024", "summary": "This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\u4e0a\u5378\u8d27\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7ICP\u7b97\u6cd5\u5b9e\u65f6\u8ddf\u8e2a\u8d27\u76d8\u4e0e\u8d27\u53c9\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\uff0c\u4f7f\u8d27\u53c9\u4e0e\u76ee\u6807\u8868\u9762\u5e73\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u907f\u514d\u62d6\u62fd\u8d27\u76d8\u3002", "motivation": "\u81ea\u4e3b\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\uff08\u5982\u5361\u8f66\u503e\u659c\u5e8a\uff09\u4e0a\u5378\u8d27\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8d27\u76d8\u88ab\u62d6\u62fd\u635f\u574f\u3002\u9700\u8981\u4e00\u79cd\u63a7\u5236\u65b9\u6cd5\u4f7f\u8d27\u53c9\u80fd\u591f\u5e73\u884c\u4e8e\u503e\u659c\u8868\u9762\uff0c\u5b9e\u73b0\u65e0\u62d6\u62fd\u7684\u5e73\u7a33\u5378\u8d27\u3002", "method": "\u4f7f\u7528\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u7b97\u6cd5\u5904\u7406\u8d27\u76d8\u4e0a\u90e8\u533a\u57df\u6d4b\u91cf\u7684\u70b9\u4e91\u6570\u636e\uff0c\u5b9e\u65f6\u8ddf\u8e2a\u8d27\u76d8\u4e0e\u8d27\u53c9\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\u89d2\u5ea6\u5dee\u5f02\u3002\u6839\u636e\u8ddf\u8e2a\u7ed3\u679c\u8c03\u6574\u8d27\u53c9\u59ff\u6001\uff0c\u4f7f\u5176\u4e0e\u76ee\u6807\u503e\u659c\u8868\u9762\u5e73\u884c\u5bf9\u9f50\uff0c\u7136\u540e\u6cbf\u503e\u659c\u65b9\u5411\u64a4\u56de\u8d27\u53c9\u5b8c\u6210\u5378\u8d27\u3002", "result": "\u901a\u8fc7\u52a8\u6001\u4eff\u771f\u548c\u771f\u5b9e\u53c9\u8f66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5728\u5361\u8f66\u503e\u659c\u5e8a\u7b49\u503e\u659c\u8868\u9762\u4e0a\u5b9e\u73b0\u4e86\u65e0\u62d6\u62fd\u7684\u5e73\u7a33\u5378\u8d27\u64cd\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eICP\u7b97\u6cd5\u7684\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u81ea\u4e3b\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\u4e0a\u5378\u8d27\u65f6\u7684\u62d6\u62fd\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u59ff\u6001\u8c03\u6574\u5b9e\u73b0\u5e73\u7a33\u5378\u8d27\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.16746", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16746", "abs": "https://arxiv.org/abs/2602.16746", "authors": ["Yongzhong Xu"], "title": "Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking", "comment": "29 pages, 22 figures", "summary": "Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u53d1\u73b0\uff0cTransformer\u5728\u6a21\u5757\u5316\u7b97\u672f\u4efb\u52a1\u4e2d\u7684grokking\u73b0\u8c61\u6e90\u4e8e\u4ece\u4f4e\u7ef4\u6267\u884c\u5b50\u7a7a\u95f4\u7684\u4e9a\u7a33\u6001\u9003\u9038\uff0c\u6a2a\u5411\u66f2\u7387\u589e\u957f\u5148\u4e8e\u6cdb\u5316\u53d1\u751f\u3002", "motivation": "\u7406\u89e3Transformer\u5728\u5c0f\u578b\u7b97\u6cd5\u4efb\u52a1\u4e2d\u51fa\u73b0\u7684\"grokking\"\u73b0\u8c61\u2014\u2014\u4ece\u8bb0\u5fc6\u5230\u6cdb\u5316\u7684\u5ef6\u8fdf\u8f6c\u53d8\u673a\u5236\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u6e05\u6670\u7684\u7406\u8bba\u89e3\u91ca\u3002", "method": "1) \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u8f68\u8ff9\u8fdb\u884cPCA\u5206\u6790\uff0c\u8bc6\u522b\u4f4e\u7ef4\u6267\u884c\u5b50\u7a7a\u95f4\uff1b2) \u6d4b\u91cf\u4ea4\u6362\u5b50\u7f3a\u9677\uff08\u8fde\u7eed\u68af\u5ea6\u6b65\u7684\u975e\u4ea4\u6362\u6027\uff09\u6765\u63a2\u6d4b\u635f\u5931\u666f\u89c2\u51e0\u4f55\uff1b3) \u5c06\u66f2\u7387\u6295\u5f71\u5230\u5b66\u4e60\u5230\u7684\u5b50\u7a7a\u95f4\uff1b4) \u8fdb\u884c\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\u9a8c\u8bc1\u51e0\u4f55\u673a\u5236\u3002", "result": "1) \u8bad\u7ec3\u4e3b\u8981\u53d1\u751f\u5728\u4f4e\u7ef4\u6267\u884c\u5b50\u7a7a\u95f4\u5185\uff08\u5355\u4e2a\u4e3b\u6210\u5206\u89e3\u91ca68-83%\u65b9\u5dee\uff09\uff1b2) \u6a2a\u5411\u4e8e\u6267\u884c\u5b50\u7a7a\u95f4\u7684\u65b9\u5411\u66f2\u7387\u6025\u5267\u589e\u957f\uff1b3) \u66f2\u7387\u589e\u957f\u59cb\u7ec8\u5148\u4e8e\u6cdb\u5316\u53d1\u751f\uff0c\u4e14\u9886\u5148\u65f6\u95f4\u670d\u4ece\u5e42\u5f8b\u5173\u7cfb\uff1b4) \u6cbf\u5b66\u4e60\u5b50\u7a7a\u95f4\u7684\u8fd0\u52a8\u5bf9grokking\u662f\u5fc5\u8981\u7684\uff0c\u800c\u4eba\u4e3a\u589e\u52a0\u66f2\u7387\u4e0d\u8db3\u591f\u3002", "conclusion": "grokking\u53cd\u6620\u4e86\u4ece\u4e9a\u7a33\u6001\u533a\u57df\u7684\u9003\u9038\uff0c\u8be5\u533a\u57df\u7279\u5f81\u4e3a\u4f4e\u7ef4\u7ea6\u675f\u548c\u6a2a\u5411\u66f2\u7387\u79ef\u7d2f\u3002\u6b63\u4ea4\u68af\u5ea6\u6d41\u5bf9grokking\u662f\u5fc5\u8981\u4f46\u4e0d\u5145\u5206\u7684\uff0c\u6240\u6709\u53d1\u73b0\u5728\u4e0d\u540c\u5b66\u4e60\u7387\u8303\u56f4\u548c\u968f\u673a\u79cd\u5b50\u4e0b\u5747\u53ef\u590d\u73b0\u3002"}}
{"id": "2602.17022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17022", "abs": "https://arxiv.org/abs/2602.17022", "authors": ["Takyoung Kim", "Jinseok Nam", "Chandrayee Basu", "Xing Fan", "Chengyuan Ma", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "title": "ReIn: Conversational Error Recovery with Reasoning Inception", "comment": "ICLR 2026", "summary": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.", "AI": {"tldr": "ReIn\u662f\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u7cfb\u7edf\u63d0\u793a\u7684\u6d4b\u8bd5\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u690d\u5165\u521d\u59cb\u63a8\u7406\u6765\u5e2e\u52a9\u5bf9\u8bdd\u4ee3\u7406\u4ece\u7528\u6237\u8bf1\u5bfc\u7684\u9519\u8bef\u4e2d\u6062\u590d\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u4ee3\u7406\u5728\u56fa\u5b9a\u4efb\u52a1\u5bfc\u5411\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7528\u6237\u8bf1\u5bfc\u7684\u610f\u5916\u9519\u8bef\u5f88\u8106\u5f31\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9519\u8bef\u9884\u9632\u800c\u975e\u6062\u590d\uff0c\u4e14\u6a21\u578b\u5fae\u8c03\u6216\u63d0\u793a\u4fee\u6539\u6210\u672c\u9ad8\u6602\u3001\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u548c\u63d0\u793a\u7684\u9519\u8bef\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faReasoning Inception (ReIn)\u65b9\u6cd5\uff1a\u5916\u90e8\u8d77\u59cb\u6a21\u5757\u8bc6\u522b\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e2d\u7684\u9884\u5b9a\u4e49\u9519\u8bef\u5e76\u751f\u6210\u6062\u590d\u8ba1\u5212\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8ba1\u5212\u6574\u5408\u5230\u4ee3\u7406\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ee5\u6307\u5bfc\u7ea0\u6b63\u884c\u52a8\uff0c\u6574\u4e2a\u8fc7\u7a0b\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u7cfb\u7edf\u63d0\u793a\u3002", "result": "ReIn\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u4e14\u6301\u7eed\u4f18\u4e8e\u663e\u5f0f\u63d0\u793a\u4fee\u6539\u65b9\u6cd5\u3002\u5728\u591a\u79cd\u4ee3\u7406\u6a21\u578b\u548c\u8d77\u59cb\u6a21\u5757\u7ec4\u5408\u4e0b\u90fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "ReIn\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u65f6\u7684\u9519\u8bef\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b9a\u4e49\u6062\u590d\u5de5\u5177\u53ef\u4ee5\u4f5c\u4e3a\u63d0\u9ad8\u5bf9\u8bdd\u4ee3\u7406\u5f39\u6027\u7684\u5b89\u5168\u6709\u6548\u7b56\u7565\uff0c\u65e0\u9700\u4fee\u6539\u9aa8\u5e72\u6a21\u578b\u6216\u7cfb\u7edf\u63d0\u793a\u3002"}}
{"id": "2602.16935", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16935", "abs": "https://arxiv.org/abs/2602.16935", "authors": ["Justin Albrethsen", "Yash Datta", "Kunal Kumar", "Sharath Rajasekar"], "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs", "comment": "18 Pages, 7 Tables, 1 Figure", "summary": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.", "AI": {"tldr": "DeepContext\u662f\u4e00\u4e2a\u72b6\u6001\u611f\u77e5\u7684\u5b89\u5168\u76d1\u63a7\u6846\u67b6\uff0c\u901a\u8fc7RNN\u67b6\u6784\u8ffd\u8e2a\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7528\u6237\u610f\u56fe\u6f14\u53d8\uff0c\u663e\u8457\u63d0\u5347\u8d8a\u72f1\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u9632\u62a4\u5927\u591a\u662f\u65e0\u72b6\u6001\u7684\uff0c\u5c06\u591a\u8f6e\u5bf9\u8bdd\u89c6\u4e3a\u79bb\u6563\u4e8b\u4ef6\uff0c\u5bfc\u81f4\u65e0\u6cd5\u68c0\u6d4b\u8de8\u8f6e\u6b21\u9010\u6b65\u79ef\u7d2f\u7684\u6076\u610f\u610f\u56fe\uff08\u5982Crescendo\u548cActorAttack\u653b\u51fb\uff09\uff0c\u5b58\u5728\"\u5b89\u5168\u6f0f\u6d1e\"\u3002", "method": "\u63d0\u51faDeepContext\u6846\u67b6\uff0c\u91c7\u7528RNN\u67b6\u6784\u5904\u7406\u5e8f\u5217\u5316\u7684\u7ec6\u7c92\u5ea6\u8f6e\u6b21\u5d4c\u5165\uff0c\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u5728\u5bf9\u8bdd\u4e2d\u4f20\u64ad\uff0c\u6355\u6349\u98ce\u9669\u7d2f\u79ef\u8fc7\u7a0b\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5b64\u7acb\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5728\u8d8a\u72f1\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u7684F1\u5206\u65700.84\uff0c\u663e\u8457\u4f18\u4e8e\u4e91\u670d\u52a1\u5546\u9632\u62a4\u548c\u5f00\u6e90\u6a21\u578b\uff08Llama-Prompt-Guard-2\u548cGranite-Guardian\u5747\u4e3a0.67\uff09\uff0c\u5728T4 GPU\u4e0a\u4fdd\u630120ms\u4ee5\u4e0b\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u5efa\u6a21\u610f\u56fe\u7684\u5e8f\u5217\u6f14\u5316\u6bd4\u90e8\u7f72\u5927\u89c4\u6a21\u65e0\u72b6\u6001\u6a21\u578b\u66f4\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u72b6\u6001\u611f\u77e5\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17393", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.17393", "abs": "https://arxiv.org/abs/2602.17393", "authors": ["Minxing Sun", "Yao Mao"], "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots", "comment": "28 pages, 30 figures", "summary": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528IMU\u548c\u7535\u673a\u6d4b\u91cf\u7684\u7eaf\u672c\u4f53\u611f\u77e5\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u63a5\u89e6\u817f\u4f5c\u4e3a\u8fd0\u52a8\u5b66\u951a\u70b9\u3001\u626d\u77e9\u4f30\u8ba1\u9009\u62e9\u53ef\u9760\u63a5\u89e6\u3001\u811a\u843d\u4f4d\u7f6e\u63d0\u4f9b\u4e16\u754c\u5750\u6807\u7cfb\u7ea6\u675f\u6765\u6291\u5236\u957f\u671f\u6f02\u79fb\uff0c\u9002\u7528\u4e8e\u53cc\u8db3\u3001\u56db\u8db3\u548c\u8f6e\u817f\u673a\u5668\u4eba\u3002", "motivation": "\u89e3\u51b3\u65e0\u6444\u50cf\u5934\u6216LiDAR\u7684\u817f\u5f0f\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u95ee\u9898\uff0cIMU\u6f02\u79fb\u548c\u5173\u8282\u901f\u5ea6\u566a\u58f0\u4f7f\u5f97\u53ef\u9760\u91cc\u7a0b\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u672c\u4f53\u4f20\u611f\u5668\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "1) \u5c06\u63a5\u89e6\u817f\u4f5c\u4e3a\u8fd0\u52a8\u5b66\u951a\u70b9\uff0c\u57fa\u4e8e\u5173\u8282\u626d\u77e9\u4f30\u8ba1\u811a\u90e8\u529b\u9009\u62e9\u53ef\u9760\u63a5\u89e6\uff1b2) \u811a\u843d\u4f4d\u7f6e\u63d0\u4f9b\u95f4\u6b47\u6027\u4e16\u754c\u5750\u6807\u7cfb\u7ea6\u675f\u6291\u5236\u6f02\u79fb\uff1b3) \u8f7b\u91cf\u7ea7\u9ad8\u5ea6\u805a\u7c7b\u548c\u65f6\u95f4\u8870\u51cf\u6821\u6b63\u9632\u6b62\u9ad8\u7a0b\u6f02\u79fb\uff1b4) \u9006\u8fd0\u52a8\u5b66\u7acb\u65b9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6539\u5584\u811a\u7aef\u901f\u5ea6\u89c2\u6d4b\uff1b5) \u591a\u63a5\u89e6\u51e0\u4f55\u4e00\u81f4\u6027\u51cf\u5c11\u504f\u822a\u6f02\u79fb\u3002", "result": "\u5728\u56db\u79cd\u56db\u8db3\u5e73\u53f0\u4e0a\u8bc4\u4f30\uff1aAstrall\u70b9\u8db3\u673a\u5668\u4ebaA\u5b8c\u6210\u7ea6200\u7c73\u6c34\u5e73\u56de\u8def\u8bef\u5dee0.1638\u7c73\uff0c\u7ea615\u7c73\u5782\u76f4\u56de\u8def\u8bef\u5dee0.219\u7c73\uff1b\u8f6e\u817f\u673a\u5668\u4ebaB\u76f8\u5e94\u8bef\u5dee\u4e3a0.2264\u7c73\u548c0.199\u7c73\uff1b\u8f6e\u817f\u673a\u5668\u4ebaC\u7ea6700\u7c73\u6c34\u5e73\u56de\u8def\u8bef\u5dee7.68\u7c73\uff0c\u7ea620\u7c73\u5782\u76f4\u56de\u8def\u8bef\u5dee0.540\u7c73\uff1bUnitree Go2 EDU\u7ea6120\u7c73\u6c34\u5e73\u56de\u8def\u8bef\u5dee2.2138\u7c73\uff0c\u7ea68\u7c73\u5782\u76f4\u56de\u8def\u5782\u76f4\u8bef\u5dee\u5c0f\u4e8e0.1\u7c73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528IMU\u548c\u7535\u673a\u6d4b\u91cf\u5c31\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u4f4d\u59ff\u548c\u901f\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u63a5\u89e6\u817f\u7684\u8fd0\u52a8\u5b66\u951a\u70b9\u7ea6\u675f\u6709\u6548\u6291\u5236\u957f\u671f\u6f02\u79fb\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u817f\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5728\u6c34\u5e73\u548c\u5782\u76f4\u65b9\u5411\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2602.17077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17077", "abs": "https://arxiv.org/abs/2602.17077", "authors": ["Lee Dayeon", "Kim Dongheyong", "Park Chaewon", "Woo Sungmin", "Lee Sangyoun"], "title": "Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection", "comment": "ICASSP 2026", "summary": "Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.", "AI": {"tldr": "CPL-VAD\uff1a\u57fa\u4e8e\u4ea4\u53c9\u4f2a\u6807\u7b7e\u7684\u53cc\u5206\u652f\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u5206\u652f\u548c\u7c7b\u522b\u5206\u7c7b\u5206\u652f\u4ea4\u6362\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u65f6\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u8bed\u4e49\u5224\u522b\u80fd\u529b\uff0c\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u5f02\u5e38\u7c7b\u522b\u8bc6\u522b\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u9700\u8981\u540c\u65f6\u68c0\u6d4b\u5f02\u5e38\u5e76\u8bc6\u522b\u5f02\u5e38\u7c7b\u522b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8bed\u4e49\u5224\u522b\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCPL-VAD\u53cc\u5206\u652f\u6846\u67b6\uff1a1\uff09\u4e8c\u5143\u5f02\u5e38\u68c0\u6d4b\u5206\u652f\u4e13\u6ce8\u4e8e\u7247\u6bb5\u7ea7\u5f02\u5e38\u5b9a\u4f4d\uff1b2\uff09\u7c7b\u522b\u5206\u7c7b\u5206\u652f\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u8bc6\u522b\u5f02\u5e38\u4e8b\u4ef6\u7c7b\u522b\u3002\u4e24\u4e2a\u5206\u652f\u901a\u8fc7\u4ea4\u6362\u4f2a\u6807\u7b7e\u76f8\u4e92\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e92\u8865\u4f18\u52bf\u7684\u7ed3\u5408\u3002", "result": "\u5728XD-Violence\u548cUCF-Crime\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCPL-VAD\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u5f02\u5e38\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CPL-VAD\u901a\u8fc7\u4ea4\u53c9\u4f2a\u6807\u7b7e\u673a\u5236\u6709\u6548\u7ed3\u5408\u4e86\u65f6\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8bed\u4e49\u5224\u522b\u80fd\u529b\uff0c\u4e3a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17085", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.17085", "abs": "https://arxiv.org/abs/2602.17085", "authors": ["Shogo Sato", "Kazuo Tanaka", "Shojun Ogasawara", "Kazuki Yamamoto", "Kazuhiko Murasaki", "Ryuichi Tanida", "Jun Kataoka"], "title": "ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions", "comment": "Accepted by ApJ", "summary": "Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.", "AI": {"tldr": "\u63d0\u51faComptonUNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u5149\u5b50\u7edf\u8ba1\u548c\u9ad8\u80cc\u666f\u566a\u58f0\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4f3d\u9a6c\u5c04\u7ebf\u66b4\u7684\u9c81\u68d2\u5b9a\u4f4d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5fae\u5f31\u4f3d\u9a6c\u5c04\u7ebf\u66b4\uff08GRBs\uff09\u80fd\u63d0\u4f9b\u65e9\u671f\u6052\u661f\u5f62\u6210\u7684\u72ec\u7279\u89c1\u89e3\uff0c\u4f46\u7531\u4e8e\u4f4e\u5149\u5b50\u7edf\u8ba1\u548c\u5f3a\u80cc\u666f\u566a\u58f0\uff0c\u68c0\u6d4b\u548c\u5b9a\u4f4d\u8fd9\u4e9b\u5f31\u6e90\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5728\u7edf\u8ba1\u9c81\u68d2\u6027\u548c\u566a\u58f0\u6291\u5236\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faComptonUNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u5904\u7406\u539f\u59cb\u6570\u636e\u5e76\u91cd\u5efa\u56fe\u50cf\uff0c\u7ed3\u5408\u76f4\u63a5\u91cd\u5efa\u6a21\u578b\u7684\u7edf\u8ba1\u6548\u7387\u548c\u57fa\u4e8e\u56fe\u50cf\u67b6\u6784\u7684\u53bb\u566a\u80fd\u529b\uff0c\u5728\u4f4e\u5149\u5b50\u7edf\u8ba1\u548c\u5f3a\u80cc\u666f\u6c61\u67d3\u6761\u4ef6\u4e0b\u6709\u6548\u5de5\u4f5c\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u4f4e\u5730\u7403\u8f68\u9053\u4efb\u52a1\u80cc\u666f\u73af\u5883\u4e2d\u7684GRB\u6837\u4e8b\u4ef6\u8fdb\u884c\u8bc4\u4f30\uff0cComptonUNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5e7f\u6cdb\u7684\u4f4e\u7edf\u8ba1\u548c\u9ad8\u80cc\u666f\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6539\u8fdb\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "ComptonUNet\u4e3a\u5fae\u5f31\u4f3d\u9a6c\u5c04\u7ebf\u66b4\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4f4e\u5149\u5b50\u7edf\u8ba1\u548c\u9ad8\u80cc\u666f\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u671b\u63a8\u52a8\u9ad8\u80fd\u5929\u4f53\u7269\u7406\u7814\u7a76\u3002"}}
{"id": "2602.16837", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16837", "abs": "https://arxiv.org/abs/2602.16837", "authors": ["Hanna Herasimchyk", "Robin Labryga", "Tomislav Prusina", "S\u00f6ren Laue"], "title": "A Residual-Aware Theory of Position Bias in Transformers", "comment": null, "summary": "Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6b8b\u5dee\u611f\u77e5\u7d2f\u79ef\u6ce8\u610f\u529b\u5c55\u5f00\u7406\u8bba\uff0c\u89e3\u91ca\u56e0\u679cTransformer\u4e2d\u4f4d\u7f6e\u504f\u7f6e\u7684\u67b6\u6784\u8d77\u6e90\uff0c\u8bf4\u660e\u6b8b\u5dee\u8fde\u63a5\u9632\u6b62\u6ce8\u610f\u529b\u5d29\u6e83\uff0c\u5e76\u8bc1\u660e\u6709\u9650\u6df1\u5ea6\u4e0b\u5f62\u6210U\u5f62\u4f4d\u7f6e\u504f\u7f6e\uff0c\u4e3a\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "Transformer\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u504f\u597d\u67d0\u4e9btoken\u4f4d\u7f6e\uff0c\u4f46\u8fd9\u79cd\u4f4d\u7f6e\u504f\u7f6e\u7684\u67b6\u6784\u8d77\u6e90\u4ecd\u4e0d\u6e05\u695a\u3002\u5148\u524d\u7406\u8bba\u5206\u6790\u9884\u6d4b\u56e0\u679c\u63a9\u7801\u4e0b\u6ce8\u610f\u529b\u4f1a\u5d29\u6e83\u5230\u7b2c\u4e00\u4e2atoken\uff0c\u4f46\u8fd9\u5728\u5b9e\u9645\u4e2d\u5e76\u672a\u53d1\u751f\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u6b8b\u5dee\u611f\u77e5\u7d2f\u79ef\u6ce8\u610f\u529b\u5c55\u5f00\u7406\u8bba\uff0c\u5c06\u6b8b\u5dee\u8fde\u63a5\u7eb3\u5165\u5206\u6790\u6846\u67b6\u3002\u5728\u65e0\u9650\u6df1\u5ea6\u6761\u4ef6\u4e0b\u8bc1\u660e\u6b8b\u5dee\u8fde\u63a5\u9632\u6b62\u6ce8\u610f\u529b\u5d29\u6e83\uff0c\u5728\u6709\u9650\u6df1\u5ea6\u4e0b\u5206\u6790\u56e0\u679cTransformer\u7684\u4f4d\u7f6e\u504f\u7f6e\u6a21\u5f0f\u3002", "result": "\u8bc1\u660e\u6b8b\u5dee\u8fde\u63a5\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u9632\u6b62\u6ce8\u610f\u529b\u5d29\u6e83\u3002\u5728\u6709\u9650\u6df1\u5ea6\u4e0b\uff0c\u56e0\u679cTransformer\u4ea7\u751fU\u5f62\u4f4d\u7f6e\u504f\u7f6e\uff0c\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u65e9\u671f\u548c\u665a\u671ftoken\u4e0a\uff0c\u8fd9\u4e3a\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\u63d0\u4f9b\u4e86\u67b6\u6784\u5c42\u9762\u7684\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u662fTransformer\u4f4d\u7f6e\u504f\u7f6e\u7684\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff0c\u9632\u6b62\u6ce8\u610f\u529b\u5d29\u6e83\u5e76\u5bfc\u81f4U\u5f62\u4f4d\u7f6e\u504f\u7f6e\u6a21\u5f0f\u3002\u8be5\u7406\u8bba\u89e3\u91ca\u4e86Transformer\u4e2d\u89c2\u5bdf\u5230\u7684\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\uff0c\u4e3a\u7406\u89e3\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.17001", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.17001", "abs": "https://arxiv.org/abs/2602.17001", "authors": ["Zhao Tan", "Yiji Zhao", "Shiyu Wang", "Chang Xu", "Yuxuan Liang", "Xiping Liu", "Shirui Pan", "Ming Jin"], "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases", "comment": null, "summary": "Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.", "AI": {"tldr": "Sonar-TS\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u641c\u7d22-\u9a8c\u8bc1\u6d41\u7a0b\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u95ee\u9898\uff0c\u4f7f\u7528SQL\u641c\u7d22\u5019\u9009\u7a97\u53e3\uff0c\u7136\u540e\u7528Python\u7a0b\u5e8f\u9a8c\u8bc1\u539f\u59cb\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fde\u7eed\u5f62\u6001\u610f\u56fe\uff08\u5982\u5f62\u72b6\u6216\u5f02\u5e38\uff09\uff0c\u800c\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u96be\u4ee5\u5904\u7406\u8d85\u957f\u5386\u53f2\u6570\u636e\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51faSonar-TS\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u91c7\u7528\u641c\u7d22-\u9a8c\u8bc1\u6d41\u7a0b\uff1a1\uff09\u4f7f\u7528\u7279\u5f81\u7d22\u5f15\u901a\u8fc7SQL\u641c\u7d22\u5019\u9009\u7a97\u53e3\uff1b2\uff09\u751f\u6210Python\u7a0b\u5e8f\u9501\u5b9a\u5e76\u9a8c\u8bc1\u5019\u9009\u7a97\u53e3\u4e0e\u539f\u59cb\u4fe1\u53f7\u7684\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSonar-TS\u80fd\u6709\u6548\u5904\u7406\u4f20\u7edf\u65b9\u6cd5\u5931\u8d25\u7684\u590d\u6742\u65f6\u95f4\u67e5\u8be2\uff0c\u5e76\u521b\u5efa\u4e86NLQTSBench\u4f5c\u4e3a\u9996\u4e2a\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u57fa\u51c6\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16842", "abs": "https://arxiv.org/abs/2602.16842", "authors": ["Rachitesh Kumar", "Omar Mouchtaki"], "title": "What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor", "comment": null, "summary": "We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.", "AI": {"tldr": "\u7814\u7a76\u79bb\u7ebf\u6570\u636e\u9a71\u52a8\u7684\u62a5\u7ae5\u95ee\u9898\uff0c\u9700\u6c42\u6570\u636e\u88ab\u5e93\u5b58\u6c34\u5e73\u622a\u65ad\uff0c\u53ea\u80fd\u89c2\u6d4b\u5230\u9500\u552e\u6570\u636e\u800c\u975e\u771f\u5b9e\u9700\u6c42\u3002\u63d0\u51fa\u8ba1\u7b97\u7ecf\u5178\u5e93\u5b58\u7b56\u7565\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u7cbe\u786e\u540e\u6094\u503c\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u65e0\u9650\u7ef4\u975e\u51f8\u4f18\u5316\u53ef\u7b80\u5316\u4e3a\u6709\u9650\u7ef4\u95ee\u9898\u3002\u5206\u6790\u663e\u793a\u9700\u6c42\u622a\u65ad\u9650\u5236\u4e86\u4ece\u88ab\u52a8\u9500\u552e\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u4f46\u5c11\u91cf\u6709\u9488\u5bf9\u6027\u7684\u63a2\u7d22\u53ef\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u62a5\u7ae5\u95ee\u9898\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u9700\u6c42\u6570\u636e\u5b8c\u5168\u53ef\u89c2\u6d4b\uff0c\u4f46\u5b9e\u9645\u96f6\u552e\u573a\u666f\u4e2d\u9700\u6c42\u5e38\u88ab\u5e93\u5b58\u6c34\u5e73\u622a\u65ad\uff0c\u53ea\u80fd\u89c2\u6d4b\u5230\u9500\u552e\u6570\u636e\u800c\u975e\u771f\u5b9e\u9700\u6c42\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u9700\u6c42\u622a\u65ad\u60c5\u51b5\u4e0b\u7684\u6570\u636e\u9a71\u52a8\u5e93\u5b58\u7b56\u7565\u6027\u80fd\u7f3a\u4e4f\u7cbe\u786e\u5206\u6790\uff0c\u9700\u8981\u7406\u89e3\u9700\u6c42\u622a\u65ad\u5982\u4f55\u5f71\u54cd\u79bb\u7ebf\u5b66\u4e60\u80fd\u529b\u4ee5\u53ca\u4e0d\u540c\u4fe1\u606f\u8d28\u91cf\u5bf9\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u901a\u7528\u7a0b\u5e8f\u8ba1\u7b97\u7ecf\u5178\u6570\u636e\u9a71\u52a8\u5e93\u5b58\u7b56\u7565\u5728\u6240\u6709\u9700\u6c42\u5206\u5e03\u4e0b\u7684\u7cbe\u786e\u6700\u574f\u60c5\u51b5\u540e\u6094\u503c\u3002\u4e3b\u8981\u6280\u672f\u8d21\u732e\u662f\u5c06\u65e0\u9650\u7ef4\u975e\u51f8\u4f18\u5316\u95ee\u9898\u7b80\u5316\u4e3a\u6709\u9650\u7ef4\u95ee\u9898\uff0c\u4ece\u800c\u80fd\u591f\u7cbe\u786e\u523b\u753b\u4efb\u610f\u6837\u672c\u91cf\u548c\u622a\u65ad\u6c34\u5e73\u4e0b\u7684\u7b56\u7565\u6027\u80fd\u3002\u5229\u7528\u8be5\u7b80\u5316\u5206\u6790\u6807\u51c6\u5e93\u5b58\u7b56\u7565\u5728\u9700\u6c42\u622a\u65ad\u4e0b\u7684\u53ef\u5b9e\u73b0\u6027\u80fd\u3002", "result": "\u5bf9Kaplan-Meier\u7b56\u7565\u7684\u5206\u6790\u8868\u660e\uff0c\u9700\u6c42\u622a\u65ad\u4ece\u6839\u672c\u4e0a\u9650\u5236\u4e86\u4ece\u88ab\u52a8\u9500\u552e\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u4f46\u5728\u9ad8\u5e93\u5b58\u6c34\u5e73\u8fdb\u884c\u5c11\u91cf\u6709\u9488\u5bf9\u6027\u7684\u63a2\u7d22\u53ef\u663e\u8457\u6539\u5584\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff0c\u5373\u4f7f\u5728\u4e25\u91cd\u622a\u65ad\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5c06\u9500\u552e\u89c6\u4e3a\u9700\u6c42\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f1a\u968f\u7740\u622a\u65ad\u6570\u636e\u79ef\u7d2f\u800c\u906d\u53d7\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u9500\u552e\u70b9\u4fe1\u606f\u7684\u8d28\u91cf\u5bf9\u79bb\u7ebf\u5b66\u4e60\u80fd\u529b\u5177\u6709\u5173\u952e\u5f71\u54cd\u3002\u9700\u6c42\u622a\u65ad\u9650\u5236\u4e86\u88ab\u52a8\u6570\u636e\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u63a2\u7d22\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u5c06\u9500\u552e\u89c6\u4e3a\u9700\u6c42\u7684\u5e38\u89c1\u505a\u6cd5\u5728\u622a\u65ad\u6570\u636e\u79ef\u7d2f\u65f6\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u5f3a\u8c03\u4e86\u5728\u5e93\u5b58\u7ba1\u7406\u4e2d\u6b63\u786e\u5904\u7406\u622a\u65ad\u6570\u636e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.17586", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17586", "abs": "https://arxiv.org/abs/2602.17586", "authors": ["Antonio Guillen-Perez"], "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space", "comment": null, "summary": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.", "AI": {"tldr": "Deep-Flow\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u5b89\u5168\u5173\u952e\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6761\u4ef6\u6d41\u5339\u914d\u6765\u5efa\u6a21\u4e13\u5bb6\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u901a\u8fc7PCA\u74f6\u9888\u7ea6\u675f\u751f\u6210\u8fc7\u7a0b\u5230\u4f4e\u79e9\u8c31\u6d41\u5f62\uff0c\u5b9e\u73b0\u7a33\u5b9a\u3001\u786e\u5b9a\u6027\u7684\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524dL4\u7ea7\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u9a8c\u8bc1\u9762\u4e34\u74f6\u9888\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u68c0\u6d4b\u7f55\u89c1\u7684\u9ad8\u98ce\u9669\u957f\u5c3e\u573a\u666f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u6765\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "1. \u4f7f\u7528\u6700\u4f18\u4f20\u8f93\u6761\u4ef6\u6d41\u5339\u914d(OT-CFM)\u5efa\u6a21\u4e13\u5bb6\u9a7e\u9a76\u884c\u4e3a\u7684\u8fde\u7eed\u6982\u7387\u5bc6\u5ea6\uff1b2. \u901a\u8fc7PCA\u74f6\u9888\u5c06\u751f\u6210\u8fc7\u7a0b\u7ea6\u675f\u5230\u4f4e\u79e9\u8c31\u6d41\u5f62\uff0c\u786e\u4fdd\u8fd0\u52a8\u5b66\u5e73\u6ed1\u6027\uff1b3. \u4f7f\u7528\u65e9\u671f\u878d\u5408Transformer\u7f16\u7801\u5668\u5904\u7406\u591a\u6a21\u6001\u6b67\u4e49\uff0c\u5177\u6709\u8f66\u9053\u611f\u77e5\u76ee\u6807\u6761\u4ef6\uff1b4. \u5f15\u5165\u8fd0\u52a8\u5b66\u590d\u6742\u5ea6\u52a0\u6743\u65b9\u6848\uff0c\u5728\u65e0\u6a21\u62df\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5148\u5904\u7406\u9ad8\u80fd\u91cf\u673a\u52a8\uff1b5. \u8ba1\u7b97\u7cbe\u786e\u7684\u96c5\u53ef\u6bd4\u8ff9\u5b9e\u73b0\u6570\u503c\u7a33\u5b9a\u7684\u786e\u5b9a\u6027\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u3002", "result": "\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6(WOMD)\u4e0a\u8bc4\u4f30\uff0c\u6846\u67b6\u5b9e\u73b0\u4e860.766\u7684AUC-ROC\uff08\u76f8\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7684\u542f\u53d1\u5f0f\u9ec4\u91d1\u96c6\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5206\u6790\u63ed\u793a\u4e86\u8fd0\u52a8\u5b66\u5371\u9669\u548c\u8bed\u4e49\u8fdd\u89c4\u4e4b\u95f4\u7684\u6839\u672c\u533a\u522b\uff0cDeep-Flow\u901a\u8fc7\u53d1\u73b0\u4f20\u7edf\u5b89\u5168\u8fc7\u6ee4\u5668\u5ffd\u7565\u7684\u5206\u5e03\u5916\u884c\u4e3a\uff08\u5982\u8f66\u9053\u8fb9\u754c\u8fdd\u89c4\u548c\u975e\u89c4\u8303\u8def\u53e3\u673a\u52a8\uff09\u6765\u8bc6\u522b\u5173\u952e\u7684\u53ef\u9884\u6d4b\u6027\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9a\u4e49\u7edf\u8ba1\u5b89\u5168\u95e8\u63d0\u4f9b\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u57fa\u7840\uff0c\u4f7f\u57fa\u4e8e\u6570\u636e\u7684\u5ba2\u89c2\u9a8c\u8bc1\u6210\u4e3a\u53ef\u80fd\uff0c\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u7684\u5b89\u5168\u90e8\u7f72\u3002Deep-Flow\u80fd\u591f\u8bc6\u522b\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u5b89\u5168\u5173\u952e\u5f02\u5e38\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u5b66\u5371\u9669\u548c\u8bed\u4e49\u5408\u89c4\u6027\u4e4b\u95f4\u7684\u5dee\u5f02\u65b9\u9762\u3002"}}
{"id": "2602.17038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17038", "abs": "https://arxiv.org/abs/2602.17038", "authors": ["Shengtian Yang", "Yu Li", "Shuo He", "Yewen Li", "Qingpeng Cai", "Peng Jiang", "Lei Feng"], "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning", "comment": "16 pages", "summary": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \\textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \\emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.", "AI": {"tldr": "\u63d0\u51faPA-MoE\u65b9\u6cd5\u89e3\u51b3RL\u4e2d\u5355\u4e00\u7b56\u7565\u7f51\u7edc\u5bfc\u81f4\u7684\u7b80\u5355\u4efb\u52a1\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u76f8\u4f4d\u611f\u77e5\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u8ba9\u4e0d\u540c\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u4e0d\u540c\u4efb\u52a1\u9636\u6bb5", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u7f51\u7edc\u5bfc\u81f4\u7b80\u5355\u4efb\u52a1\u5360\u636e\u5927\u90e8\u5206\u53c2\u6570\u548c\u68af\u5ea6\u66f4\u65b0\uff0c\u590d\u6742\u4efb\u52a1\u5f97\u4e0d\u5230\u8db3\u591f\u5bb9\u91cf\u3002\u4f20\u7edfMoE\u7684token\u7ea7\u8def\u7531\u4f1a\u5206\u6563\u76f8\u4f4d\u4e00\u81f4\u6a21\u5f0f\uff0c\u7834\u574f\u4e13\u5bb6\u4e13\u4e1a\u5316", "method": "\u63d0\u51fa\u76f8\u4f4d\u611f\u77e5\u4e13\u5bb6\u6df7\u5408(PA-MoE)\uff1a1) \u8f7b\u91cf\u7ea7\u76f8\u4f4d\u8def\u7531\u5668\u76f4\u63a5\u4eceRL\u76ee\u6807\u5b66\u4e60\u6f5c\u5728\u76f8\u4f4d\u8fb9\u754c\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u76f8\u4f4d\u7c7b\u522b\uff1b2) \u76f8\u4f4d\u8def\u7531\u5668\u4e3a\u76f8\u540c\u4e13\u5bb6\u5206\u914d\u65f6\u95f4\u4e00\u81f4\u7684\u5206\u914d\uff0c\u8ba9\u4e13\u5bb6\u4fdd\u6301\u76f8\u4f4d\u7279\u5b9a\u4e13\u4e1a\u77e5\u8bc6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86PA-MoE\u7684\u6709\u6548\u6027", "conclusion": "PA-MoE\u901a\u8fc7\u76f8\u4f4d\u611f\u77e5\u8def\u7531\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u5728RL\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u63d0\u5347\u4e86RL\u4ee3\u7406\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b"}}
{"id": "2602.16887", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.16887", "abs": "https://arxiv.org/abs/2602.16887", "authors": ["F. S. Menezes", "M. C. F. G. Barretto", "E. Q. C. Garcia", "T. A. E. Ferreira", "J. G. Alvez"], "title": "Construction of a classification model for dementia among Brazilian adults aged 50 and over", "comment": "38 pages; 3 figures", "summary": "To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u9488\u5bf9\u5df4\u897f\u4e2d\u8001\u5e74\u4eba\u7684\u75f4\u5446\u75c7\u5206\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u53d8\u91cf\u9009\u62e9\u548c\u591a\u5143\u5206\u6790\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u5e72\u9884\u53d8\u91cf\uff0c\u53d1\u73b0\u6587\u76f2\u3001\u9ad8\u9f84\u3001\u4f4e\u4f53\u91cd\u3001\u63e1\u529b\u5dee\u3001\u9ed1\u4eba\u3001\u7f3a\u4e4f\u8fd0\u52a8\u3001\u542c\u529b\u635f\u5931\u548c\u6291\u90c1\u75c7\u72b6\u662f\u75f4\u5446\u98ce\u9669\u56e0\u7d20\uff0c\u800c\u9ad8\u7b49\u6559\u80b2\u3001\u751f\u6d3b\u6ee1\u610f\u5ea6\u548c\u5c31\u4e1a\u662f\u4fdd\u62a4\u56e0\u7d20\u3002", "motivation": "\u5efa\u7acb\u9002\u7528\u4e8e\u5df4\u897f\u4e2d\u8001\u5e74\u4eba\u7fa4\u7684\u75f4\u5446\u75c7\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u4e14\u5177\u6709\u5e72\u9884\u6f5c\u529b\u7684\u53d8\u91cf\uff0c\u4ee5\u5e2e\u52a9\u8bc6\u522b\u9ad8\u5371\u4eba\u7fa4\u5e76\u6307\u5bfc\u516c\u5171\u536b\u751f\u8d44\u6e90\u5206\u914d\u3002", "method": "\u91c7\u7528\u6a2a\u65ad\u9762\u8bbe\u8ba1\u7684\u89c2\u5bdf\u6027\u7814\u7a76\uff0c\u4f7f\u7528\u5df4\u897f\u8001\u9f84\u5316\u7eb5\u5411\u7814\u7a76\uff08ELSI-Brazil\uff09\u76849,412\u540d\u53c2\u4e0e\u8005\u6570\u636e\u3002\u75f4\u5446\u75c7\u901a\u8fc7\u795e\u7ecf\u5fc3\u7406\u5b66\u8bc4\u4f30\u548c\u77e5\u60c5\u8005\u8ba4\u77e5\u529f\u80fd\u8bc4\u4f30\u786e\u5b9a\u3002\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548c\u591a\u5143\u903b\u8f91\u56de\u5f52\u8fdb\u884c\u5206\u6790\uff0c\u7ed3\u5408\u53d8\u91cf\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u75f4\u5446\u75c7\u60a3\u75c5\u7387\u4e3a9.6%\u3002\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\u5305\u62ec\uff1a\u6587\u76f2\uff08OR=7.42\uff09\u300190\u5c81\u4ee5\u4e0a\uff08OR=11.00\uff09\u3001\u4f4e\u4f53\u91cd\uff08OR=2.11\uff09\u3001\u63e1\u529b\u5dee\uff08OR=2.50\uff09\u3001\u9ed1\u4eba\uff08OR=1.47\uff09\u3001\u7f3a\u4e4f\u8fd0\u52a8\uff08OR=1.61\uff09\u3001\u542c\u529b\u635f\u5931\uff08OR=1.65\uff09\u548c\u6291\u90c1\u75c7\u72b6\uff08OR=1.72\uff09\u3002\u4fdd\u62a4\u56e0\u7d20\u5305\u62ec\u9ad8\u7b49\u6559\u80b2\uff08OR=0.44\uff09\u3001\u751f\u6d3b\u6ee1\u610f\u5ea6\u9ad8\uff08OR=0.72\uff09\u548c\u5c31\u4e1a\uff08OR=0.78\uff09\u3002\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\uff0cAUC\u4e3a0.776\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u75f4\u5446\u75c7\u7684\u591a\u7ef4\u6027\uff0c\u5f3a\u8c03\u4e86\u4f7f\u7528\u53ef\u53ca\u6027\u56e0\u7d20\u8bc6\u522b\u9ad8\u5371\u4eba\u7fa4\u7684\u91cd\u8981\u6027\u3002\u52a0\u5f3a\u5173\u6ce8\u8111\u5065\u5eb7\u7684\u516c\u5171\u653f\u7b56\u6709\u52a9\u4e8e\u4f18\u5316\u521d\u7ea7\u4fdd\u5065\u8d44\u6e90\u5206\u914d\u548c\u75f4\u5446\u9884\u9632\u3002"}}
{"id": "2602.17424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17424", "abs": "https://arxiv.org/abs/2602.17424", "authors": ["Anastasia Zhukova", "Felix Hamborg", "Karsten Donnay", "Norman Meuschke", "Bela Gipp"], "title": "Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference", "comment": null, "summary": "Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking \"the caravan\" - \"asylum seekers\" - \"those contemplating illegal entry\", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4fee\u8ba2\u7684\u8de8\u6587\u6863\u5171\u6307\u6d88\u89e3\u6807\u6ce8\u65b9\u6848\uff0c\u5c06\u5171\u6307\u94fe\u89c6\u4e3a\u8bdd\u8bed\u5143\u7d20\uff0c\u540c\u65f6\u5bb9\u7eb3\u8eab\u4efd\u548c\u8fd1\u8eab\u4efd\u5173\u7cfb\uff0c\u4ee5\u5206\u6790\u65b0\u95fb\u8bdd\u8bed\u4e2d\u7684\u8bcd\u6c47\u591a\u6837\u6027\u548c\u6846\u67b6\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u8de8\u6587\u6863\u5171\u6307\u6d88\u89e3\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4e8b\u4ef6\u6d88\u89e3\uff0c\u91c7\u7528\u72ed\u7a84\u7684\u5171\u6307\u5b9a\u4e49\uff0c\u9650\u5236\u4e86\u5176\u5728\u5206\u6790\u8bcd\u6c47\u53d8\u5316\u5e7f\u6cdb\u7684\u591a\u6837\u5316\u3001\u6781\u5316\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4fee\u8ba2\u7684CDCR\u6807\u6ce8\u65b9\u6848\uff0c\u5c06\u5171\u6307\u94fe\u89c6\u4e3a\u8bdd\u8bed\u5143\u7d20\u548c\u5206\u6790\u7684\u6982\u5ff5\u5355\u5143\uff0c\u5bb9\u7eb3\u8eab\u4efd\u548c\u8fd1\u8eab\u4efd\u5173\u7cfb\u3002\u91cd\u65b0\u6807\u6ce8NewsWCL50\u6570\u636e\u96c6\u548cECB+\u5b50\u96c6\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6807\u6ce8\u624b\u518c\u3002", "result": "\u91cd\u65b0\u6807\u6ce8\u7684\u6570\u636e\u96c6\u5728\u8bcd\u6c47\u591a\u6837\u6027\u6307\u6807\u548c\u76f8\u540c\u8bcd\u5934\u8bcd\u5143\u57fa\u7ebf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u4ecb\u4e8e\u539f\u59cbECB+\u548cNewsWCL50\u4e4b\u95f4\uff0c\u652f\u6301\u65b0\u95fb\u9886\u57df\u5e73\u8861\u4e14\u5177\u6709\u8bdd\u8bed\u610f\u8bc6\u7684CDCR\u7814\u7a76\u3002", "conclusion": "\u4fee\u8ba2\u7684\u6807\u6ce8\u65b9\u6848\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u65b0\u95fb\u8bdd\u8bed\u4e2d\u7684\u8bcd\u6c47\u591a\u6837\u6027\u548c\u6846\u67b6\u53d8\u5316\uff0c\u4e3a\u65b0\u95fb\u9886\u57df\u7684\u8de8\u6587\u6863\u5171\u6307\u6d88\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5e73\u8861\u3001\u66f4\u5177\u8bdd\u8bed\u610f\u8bc6\u7684\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2602.16967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16967", "abs": "https://arxiv.org/abs/2602.16967", "authors": ["Yongzhong Xu"], "title": "Early-Warning Signals of Grokking via Loss-Landscape Geometry", "comment": "26 pages, 13 figures", "summary": "Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5e8f\u5217\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u4ea4\u6362\u5b50\u7f3a\u9677\uff08\u68af\u5ea6\u66f4\u65b0\u7684\u975e\u4ea4\u6362\u6027\u5ea6\u91cf\uff09\u662f\u5ef6\u8fdf\u6cdb\u5316\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff0c\u4e0e\u6a21\u5757\u7b97\u672f\u4e2d\u7684Grokking\u73b0\u8c61\u673a\u5236\u4e00\u81f4\u3002", "motivation": "Grokking\u73b0\u8c61\uff08\u4ece\u8bb0\u5fc6\u5230\u6cdb\u5316\u7684\u7a81\u7136\u8f6c\u53d8\uff09\u5728\u6a21\u5757\u7b97\u672f\u4e2d\u5df2\u88ab\u53d1\u73b0\u4e0e\u4f4e\u7ef4\u6267\u884c\u6d41\u5f62\u76f8\u5173\uff0c\u4f46\u8fd9\u4e00\u673a\u5236\u662f\u5426\u9002\u7528\u4e8e\u5176\u4ed6\u4efb\u52a1\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5e8f\u5217\u5b66\u4e60\u4efb\u52a1\u4e2dGrokking\u7684\u666e\u904d\u673a\u5236\u3002", "method": "\u7814\u7a76\u4e24\u4e2a\u5e8f\u5217\u5b66\u4e60\u57fa\u51c6\uff1aSCAN\u7ec4\u5408\u6cdb\u5316\u548cDyck-1\u6df1\u5ea6\u9884\u6d4b\u3002\u4f7f\u7528\u4ea4\u6362\u5b50\u7f3a\u9677\uff08\u68af\u5ea6\u66f4\u65b0\u7684\u975e\u4ea4\u6362\u6027\u5ea6\u91cf\uff09\u4f5c\u4e3a\u6307\u6807\uff0c\u901a\u8fc7\u6743\u91cd\u7a7a\u95f4PCA\u5206\u6790\u8c31\u96c6\u4e2d\u6027\uff0c\u5e76\u8fdb\u884c\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\uff08\u653e\u5927\u975e\u4ea4\u6362\u6027\u548c\u6291\u5236\u6b63\u4ea4\u68af\u5ea6\u6d41\uff09\u3002", "result": "\u4ea4\u6362\u5b50\u7f3a\u9677\u5728\u6cdb\u5316\u524d\u663e\u8457\u4e0a\u5347\uff0c\u63d0\u524d\u65f6\u95f4\u9075\u5faa\u8d85\u7ebf\u6027\u5e42\u5f8b\uff08SCAN\u7ea61.18\uff0cDyck\u7ea61.13\uff09\u3002\u8c31\u96c6\u4e2d\u6027\u4e0d\u662f\u666e\u904d\u524d\u5146\uff0c\u4f46\u4ea4\u6362\u5b50\u7f3a\u9677\u662f\u3002\u56e0\u679c\u5e72\u9884\u663e\u793a\uff1a\u653e\u5927\u975e\u4ea4\u6362\u6027\u52a0\u901fGrokking\uff08SCAN\u7ea632%\uff0cDyck\u7ea650%\uff09\uff0c\u6291\u5236\u6b63\u4ea4\u68af\u5ea6\u6d41\u5ef6\u8fdf\u6216\u963b\u6b62Grokking\u3002", "conclusion": "\u4ea4\u6362\u5b50\u7f3a\u9677\u662fTransformer\u67b6\u6784\u4e2d\u5ef6\u8fdf\u6cdb\u5316\u7684\u7a33\u5065\u3001\u67b6\u6784\u65e0\u5173\u3001\u56e0\u679c\u76f8\u5173\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u3002\u4e09\u4e2a\u4efb\u52a1\u5bb6\u65cf\u5f62\u6210\u56e0\u679c\u654f\u611f\u6027\u8c31\u7cfb\uff0c\u4f46\u6291\u5236\u6b63\u4ea4\u68af\u5ea6\u6d41\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f1a\u5ef6\u8fdf\u6216\u963b\u6b62Grokking\uff0c\u8868\u660e\u5176\u5fc5\u8981\u6027\u662f\u666e\u904d\u53d1\u73b0\u3002"}}
{"id": "2602.17028", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17028", "abs": "https://arxiv.org/abs/2602.17028", "authors": ["Hyeongwon Kang", "Jinwoo Park", "Seunghun Han", "Pilsung Kang"], "title": "Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles", "comment": "This manuscript contains 14 pages and 8 figures. It is currently under review at IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "summary": "Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.", "AI": {"tldr": "FATE\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u9884\u6d4b\u6a21\u578b\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u68c0\u6d4b\u5f02\u5e38\u524d\u5146\uff0c\u65e0\u9700\u5f02\u5e38\u6807\u7b7e\u5373\u53ef\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u662f\u53cd\u5e94\u5f0f\u7684\uff0c\u53ea\u80fd\u5728\u5f02\u5e38\u53d1\u751f\u540e\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u63d0\u4f9b\u4e3b\u52a8\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u7684\u80fd\u529b\u3002\u5728\u5de5\u4e1a\u8fd0\u8425\u3001\u91d1\u878d\u548c\u7f51\u7edc\u5b89\u5168\u7b49\u9886\u57df\uff0c\u65e9\u671f\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\u5bf9\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b9e\u73b0\u9884\u9632\u6027\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\u3002", "method": "FATE\u91c7\u7528\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u6765\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u5b83\u9884\u6d4b\u672a\u6765\u503c\u5e76\u5229\u7528\u96c6\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u5206\u6b67\u6765\u6307\u793a\u6f5c\u5728\u5f02\u5e38\u7684\u65e9\u671f\u8ff9\u8c61\uff0c\u65e0\u9700\u5728\u63a8\u7406\u65f6\u8bbf\u95ee\u76ee\u6807\u503c\u3002\u540c\u65f6\u63d0\u51fa\u4e86PTaPR\u8bc4\u4f30\u6307\u6807\uff0c\u7efc\u5408\u8003\u8651\u6bb5\u7ea7\u51c6\u786e\u6027\u3001\u6bb5\u5185\u8986\u76d6\u7387\u548c\u65e9\u671f\u9884\u6d4b\u7684\u65f6\u95f4\u53ca\u65f6\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFATE\u5728PTaPR AUC\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8619.9\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u65e9\u671f\u68c0\u6d4bF1\u5206\u6570\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8620.02\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u4e14\u65e0\u9700\u5f02\u5e38\u6807\u7b7e\u3002", "conclusion": "FATE\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u65e0\u76d1\u7763\u65e9\u671f\u9884\u8b66\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.17050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17050", "abs": "https://arxiv.org/abs/2602.17050", "authors": ["Ziliang Zhao", "Bi Xue", "Emma Lin", "Mengjiao Zhou", "Kaustubh Vartak", "Shakhzod Ali-Zade", "Carson Lu", "Tao Li", "Bin Kuang", "Rui Jian", "Bin Wen", "Dennis van der Staay", "Yixin Bao", "Eddy Li", "Chao Deng", "Songbin Liu", "Qifan Wang", "Kai Ren"], "title": "Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders", "comment": "10 pages, 6 figures", "summary": "Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.", "AI": {"tldr": "\u63d0\u51faMPZCH\u7d22\u5f15\u673a\u5236\uff0c\u57fa\u4e8e\u7ebf\u6027\u63a2\u6d4b\u89e3\u51b3\u5d4c\u5165\u8868\u78b0\u649e\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u78b0\u649e\u5e76\u4fdd\u6301\u751f\u4ea7\u7ea7\u6548\u7387", "motivation": "\u4f20\u7edf\u54c8\u5e0c\u7d22\u5f15\u65b9\u6cd5\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u968f\u7740\u552f\u4e00ID\u6570\u91cf\u589e\u52a0\u4f1a\u51fa\u73b0\u78b0\u649e\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u548c\u4e2a\u6027\u5316\u8d28\u91cf\u964d\u4f4e", "method": "\u57fa\u4e8e\u7ebf\u6027\u63a2\u6d4b\u7684\u591a\u63a2\u6d4b\u96f6\u78b0\u649e\u54c8\u5e0c\u673a\u5236\uff0c\u4f7f\u7528\u8f85\u52a9\u5f20\u91cf\u548c\u9ad8\u6027\u80fdCUDA\u5185\u6838\u5b9e\u73b0\u53ef\u914d\u7f6e\u63a2\u6d4b\u548c\u4e3b\u52a8\u9a71\u9010\u7b56\u7565", "result": "\u5b9e\u73b0\u7528\u6237\u5d4c\u5165\u96f6\u78b0\u649e\uff0c\u663e\u8457\u63d0\u5347\u9879\u76ee\u5d4c\u5165\u65b0\u9c9c\u5ea6\u548c\u8d28\u91cf\uff0c\u8bad\u7ec3QPS\u548c\u63a8\u7406\u5ef6\u8fdf\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53", "conclusion": "MPZCH\u6709\u6548\u89e3\u51b3\u5d4c\u5165\u78b0\u649e\u95ee\u9898\uff0c\u5df2\u5728TorchRec\u5f00\u6e90\u5e93\u4e2d\u53d1\u5e03\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u7d22\u5f15\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17555", "abs": "https://arxiv.org/abs/2602.17555", "authors": ["Zixu Cheng", "Da Li", "Jian Hu", "Ziquan Liu", "Wei Li", "Shaogang Gong"], "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking", "comment": "Under review", "summary": "Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.", "AI": {"tldr": "GraphThinker\uff1a\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u6784\u5efa\u4e8b\u4ef6\u7ea7\u573a\u666f\u56fe\u5e76\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\uff0c\u51cf\u5c11\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u89c6\u9891\u63a8\u7406\u9700\u8981\u7406\u89e3\u4e8b\u4ef6\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4f46\u8fd9\u4e9b\u5173\u7cfb\u901a\u5e38\u662f\u9690\u5f0f\u7684\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u3002\u73b0\u6709MLLM\u901a\u8fc7\u5bc6\u96c6\u63cf\u8ff0\u6216\u89c6\u9891\u6458\u8981\u8fdb\u884c\u63a8\u7406\uff0c\u7f3a\u4e4f\u56e0\u679c\u7ed3\u6784\u5efa\u6a21\uff0c\u5bfc\u81f4\u63a8\u7406\u4e2d\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\u3002", "method": "1) \u4f7f\u7528MLLM\u6784\u5efa\u4e8b\u4ef6\u7ea7\u89c6\u9891\u573a\u666f\u56fe(EVSG)\uff0c\u663e\u5f0f\u5efa\u6a21\u4e8b\u4ef6\u5185\u548c\u4e8b\u4ef6\u95f4\u5173\u7cfb\uff1b2) \u5c06\u573a\u666f\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u601d\u7ef4\u8fc7\u7a0b\u878d\u5165MLLM\uff1b3) \u5728\u5f3a\u5316\u5fae\u8c03\u4e2d\u5f15\u5165\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\uff0c\u589e\u5f3a\u89c6\u9891\u57fa\u7840\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728RexTime\u548cVidHalluc\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cGraphThinker\u5728\u6355\u6349\u5bf9\u8c61\u548c\u4e8b\u4ef6\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u66f4\u7cbe\u786e\u7684\u4e8b\u4ef6\u5b9a\u4f4d\u80fd\u529b\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e8b\u4ef6\u7ea7\u573a\u666f\u56fe\u7ed3\u6784\u5e76\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\uff0cGraphThinker\u6709\u6548\u51cf\u5c11\u4e86\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56e0\u679c\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.17599", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.17599", "abs": "https://arxiv.org/abs/2602.17599", "authors": ["Ivan Rinaldi", "Matteo Mendula", "Nicola Fanelli", "Florence Lev\u00e9", "Matteo Testi", "Giovanna Castellano", "Gennaro Vessio"], "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment", "comment": null, "summary": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.", "AI": {"tldr": "ArtToMus\uff1a\u9996\u4e2a\u76f4\u63a5\u6839\u636e\u827a\u672f\u4f5c\u54c1\u751f\u6210\u97f3\u4e50\u7684\u6846\u67b6\uff0c\u65e0\u9700\u56fe\u50cf\u5230\u6587\u672c\u8f6c\u6362\u6216\u8bed\u8a00\u76d1\u7763\uff0c\u4f7f\u7528\u89c6\u89c9\u5d4c\u5165\u6307\u5bfc\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u97f3\u4e50\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6761\u4ef6\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a1\uff09\u901a\u5e38\u5728\u81ea\u7136\u7167\u7247\u4e0a\u8bad\u7ec3\uff0c\u96be\u4ee5\u6355\u6349\u827a\u672f\u4f5c\u54c1\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u3001\u98ce\u683c\u548c\u6587\u5316\u5185\u5bb9\uff1b2\uff09\u5927\u591a\u4f9d\u8d56\u56fe\u50cf\u5230\u6587\u672c\u8f6c\u6362\u9636\u6bb5\uff0c\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u8bed\u4e49\u6377\u5f84\uff0c\u963b\u788d\u4e86\u76f4\u63a5\u7684\u89c6\u89c9\u5230\u97f3\u9891\u5b66\u4e60\u3002", "method": "\u63d0\u51faArtSound\u6570\u636e\u96c6\uff08105,884\u4e2a\u827a\u672f\u4f5c\u54c1-\u97f3\u4e50\u914d\u5bf9\uff0c\u5e26\u53cc\u6a21\u6001\u63cf\u8ff0\uff09\uff0c\u5e76\u5f00\u53d1ArtToMus\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u5d4c\u5165\u6295\u5f71\u5230\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u8c03\u8282\u7a7a\u95f4\uff0c\u5b9e\u73b0\u65e0\u9700\u8bed\u8a00\u76d1\u7763\u7684\u76f4\u63a5\u827a\u672f\u4f5c\u54c1\u5230\u97f3\u4e50\u751f\u6210\u3002", "result": "ArtToMus\u751f\u6210\u97f3\u4e50\u8fde\u8d2f\u3001\u98ce\u683c\u4e00\u81f4\uff0c\u80fd\u53cd\u6620\u6e90\u827a\u672f\u4f5c\u54c1\u7684\u663e\u8457\u89c6\u89c9\u7ebf\u7d22\u3002\u867d\u7136\u7edd\u5bf9\u5bf9\u9f50\u5206\u6570\u4f4e\u4e8e\u6587\u672c\u6761\u4ef6\u7cfb\u7edf\uff08\u8003\u8651\u5230\u53bb\u9664\u8bed\u8a00\u76d1\u7763\u7684\u96be\u5ea6\uff09\uff0c\u4f46\u5728\u611f\u77e5\u8d28\u91cf\u548c\u6709\u610f\u4e49\u7684\u8de8\u6a21\u6001\u5bf9\u5e94\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u76f4\u63a5\u89c6\u89c9\u5230\u97f3\u4e50\u751f\u6210\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7279\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u591a\u5a92\u4f53\u827a\u672f\u3001\u6587\u5316\u9057\u4ea7\u548cAI\u8f85\u52a9\u521b\u610f\u5b9e\u8df5\u63d0\u4f9b\u4e86\u8d44\u6e90\u548c\u652f\u6301\u3002"}}
{"id": "2602.17071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17071", "abs": "https://arxiv.org/abs/2602.17071", "authors": ["Rong Fu", "Muge Qi", "Chunlei Meng", "Shuo Yin", "Kun Liu", "Zhaolu Kang", "Simon Fong"], "title": "AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation", "comment": "32 pages, 8 figures", "summary": "Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.", "AI": {"tldr": "AdvSynGNN\uff1a\u4e00\u79cd\u7528\u4e8e\u5f39\u6027\u8282\u70b9\u7ea7\u8868\u793a\u5b66\u4e60\u7684\u7efc\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u7ed3\u6784\u5408\u6210\u3001\u5bf9\u6bd4\u76ee\u6807\u3001\u81ea\u9002\u5e94Transformer\u548c\u5bf9\u6297\u4f20\u64ad\u5f15\u64ce\u6765\u5e94\u5bf9\u7ed3\u6784\u566a\u58f0\u548c\u975e\u540c\u914d\u62d3\u6251\u7684\u6311\u6218\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u9762\u5bf9\u7ed3\u6784\u566a\u58f0\u6216\u975e\u540c\u914d\u62d3\u6251\u65f6\u7ecf\u5e38\u51fa\u73b0\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u8106\u5f31\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u8fd9\u4e9b\u6311\u6218\u7684\u5f39\u6027\u5b66\u4e60\u6846\u67b6\u3002", "method": "1. \u591a\u5206\u8fa8\u7387\u7ed3\u6784\u5408\u6210\u4e0e\u5bf9\u6bd4\u76ee\u6807\u5efa\u7acb\u51e0\u4f55\u654f\u611f\u521d\u59cb\u5316\uff1b2. \u81ea\u9002\u5e94Transformer\u901a\u8fc7\u5b66\u4e60\u7684\u62d3\u6251\u4fe1\u53f7\u8c03\u8282\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u9002\u5e94\u5f02\u914d\u6027\uff1b3. \u96c6\u6210\u5bf9\u6297\u4f20\u64ad\u5f15\u64ce\uff08\u751f\u6210\u5668\u8bc6\u522b\u6f5c\u5728\u8fde\u63a5\u53d8\u5316\uff0c\u5224\u522b\u5668\u5f3a\u5236\u5168\u5c40\u4e00\u81f4\u6027\uff09\uff1b4. \u57fa\u4e8e\u8282\u70b9\u7f6e\u4fe1\u5ea6\u6307\u6807\u7684\u6b8b\u5dee\u6821\u6b63\u65b9\u6848\u8fdb\u884c\u6807\u7b7e\u7ec6\u5316\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u79cd\u534f\u540c\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u4e0d\u540c\u56fe\u5206\u5e03\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86AdvSynGNN\u7cfb\u7edf\u7684\u5b9e\u9645\u5b9e\u73b0\u534f\u8bae\uff0c\u786e\u4fdd\u5176\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u7a33\u5065\u90e8\u7f72\uff0c\u4e3a\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u8106\u5f31\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17234", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17234", "abs": "https://arxiv.org/abs/2602.17234", "authors": ["Zeyu Zhang", "Ryan Chen", "Bradly C. Stadie"], "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting", "comment": "8 pages plus appendix", "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \\textbf{Time}-\\textbf{S}upervised \\textbf{P}rediction with \\textbf{E}xtracted \\textbf{C}laims (\\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.", "AI": {"tldr": "\u63d0\u51faShapley-DCLR\u6307\u6807\u91cf\u5316LLM\u9884\u6d4b\u4e2d\u7684\u65f6\u95f4\u77e5\u8bc6\u6cc4\u6f0f\uff0c\u5e76\u5f00\u53d1TimeSPEC\u65b9\u6cd5\u901a\u8fc7\u58f0\u660e\u9a8c\u8bc1\u6765\u51cf\u5c11\u6cc4\u6f0f", "motivation": "\u8bc4\u4f30LLM\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u80fd\u529b\u9700\u8981\u8fdb\u884c\u56de\u6eaf\u6d4b\u8bd5\uff0c\u4f46\u6a21\u578b\u53ef\u80fd\u5728\u8bad\u7ec3\u4e2d\u65e0\u610f\u95f4\u7f16\u7801\u4e86\u622a\u6b62\u65e5\u671f\u540e\u7684\u77e5\u8bc6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u53ef\u9760", "method": "1) \u63d0\u51faShapley-DCLR\u6307\u6807\uff1a\u5c06\u6a21\u578b\u63a8\u7406\u5206\u89e3\u4e3a\u539f\u5b50\u58f0\u660e\uff0c\u6309\u65f6\u95f4\u53ef\u9a8c\u8bc1\u6027\u5206\u7c7b\uff0c\u7528Shapley\u503c\u8861\u91cf\u6bcf\u4e2a\u58f0\u660e\u5bf9\u9884\u6d4b\u7684\u8d21\u732e\uff1b2) \u5f00\u53d1TimeSPEC\u65b9\u6cd5\uff1a\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d2\u5165\u58f0\u660e\u9a8c\u8bc1\u548c\u91cd\u65b0\u751f\u6210\uff0c\u4e3b\u52a8\u8fc7\u6ee4\u65f6\u95f4\u6c61\u67d3", "result": "\u5728350\u4e2a\u5b9e\u4f8b\uff08\u7f8e\u56fd\u6700\u9ad8\u6cd5\u9662\u6848\u4ef6\u9884\u6d4b\u3001NBA\u85aa\u8d44\u4f30\u8ba1\u3001\u80a1\u7968\u56de\u62a5\u6392\u540d\uff09\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6807\u51c6\u63d0\u793a\u57fa\u7ebf\u5b58\u5728\u663e\u8457\u6cc4\u6f0f\u3002TimeSPEC\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86Shapley-DCLR", "conclusion": "\u663e\u5f0f\u7684\u3001\u53ef\u89e3\u91ca\u7684\u58f0\u660e\u7ea7\u9a8c\u8bc1\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u65f6\u95f4\u7ea6\u675f\uff0c\u80fd\u591f\u66f4\u53ef\u9760\u5730\u8fdb\u884c\u56de\u6eaf\u6d4b\u8bd5\u3002TimeSPEC\u901a\u8fc7\u4e3b\u52a8\u8fc7\u6ee4\u65f6\u95f4\u6c61\u67d3\uff0c\u4ea7\u751f\u53ef\u8ffd\u6eaf\u81f3\u622a\u6b62\u65e5\u671f\u524d\u6765\u6e90\u7684\u9884\u6d4b"}}
{"id": "2602.17102", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17102", "abs": "https://arxiv.org/abs/2602.17102", "authors": ["Sai Vineeth Kandappareddigari", "Santhoshkumar Jagadish", "Gauri Verma", "Ilhuicamina Contreras", "Christopher Dignam", "Anmol Srivastava", "Benjamin Demers"], "title": "Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction", "comment": "13 pages. ICAD '26", "summary": "This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u65e0\u670d\u52a1\u5668\u67b6\u6784\u7684MLOps\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7ba1\u9053\u7ba1\u7406\u5b8c\u6574\u7684\u673a\u5668\u5b66\u4e60\u751f\u547d\u5468\u671f\uff0c\u5e76\u5728HS\u4ee3\u7801\u9884\u6d4b\u7684\u5de5\u4e1a\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u4e8698%\u7684\u51c6\u786e\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u89e3\u51b3HS\u4ee3\u7801\u9884\u6d4b\u8fd9\u4e00\u5408\u89c4\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff1a\u9891\u7e41\u66f4\u65b0\u3001\u6a21\u7cca\u63cf\u8ff0\u5bfc\u81f4\u5206\u7c7b\u56f0\u96be\uff0c\u9519\u8bef\u4f1a\u9020\u6210\u8d27\u8fd0\u5ef6\u8bef\u548c\u7ecf\u6d4e\u635f\u5931\u3002\u540c\u65f6\u9700\u8981\u89e3\u51b3ML\u6a21\u578b\u5de5\u4e1a\u5316\u90e8\u7f72\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u6210\u672c\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65e0\u670d\u52a1\u5668MLOps\u6846\u67b6\uff0c\u4f7f\u7528\u4e8b\u4ef6\u9a71\u52a8\u7ba1\u9053\u548c\u6258\u7ba1\u670d\u52a1\u3002\u6784\u5efa\u81ea\u5b9a\u4e49\u6587\u672c\u5d4c\u5165\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u7279\u522b\u662fText-CNN\uff09\uff0c\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u7684\u6807\u51c6\u5316\u63a5\u53e3\u3002\u5305\u542b\u81ea\u52a8A/B\u6d4b\u8bd5\u3001\u52a8\u6001\u6a21\u578b\u9009\u62e9\u548c\u81ea\u52a8\u6269\u5c55\u529f\u80fd\u3002", "result": "\u5728HS\u4ee3\u7801\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cText-CNN\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fbe\u523098%\u7684\u51c6\u786e\u7387\u3002\u6846\u67b6\u786e\u4fdd\u4e86\u53ef\u91cd\u590d\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548cSLA\u9075\u5b88\uff0c\u5728\u53ef\u53d8\u8d1f\u8f7d\u4e0b\u901a\u8fc7\u81ea\u52a8\u6269\u5c55\u4fdd\u6301\u6027\u80fd\u3002\u76f8\u6bd4Transformer\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u957f\u671f\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "\u8be5\u65e0\u670d\u52a1\u5668MLOps\u6846\u67b6\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u7684ML\u5de5\u4e1a\u5316\u84dd\u56fe\uff0c\u80fd\u591f\u5728\u4f18\u5316\u6027\u80fd\u548c\u7ecf\u6d4e\u6548\u76ca\u7684\u540c\u65f6\u5b9e\u73b0\u89c4\u6a21\u5316\u90e8\u7f72\u3002\u867d\u7136\u4f18\u5148\u8003\u8651\u786e\u5b9a\u6027\u5206\u7c7b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u67b6\u6784\u53ef\u6269\u5c55\u652f\u6301Transformer\u53d8\u4f53\u548c\u57fa\u4e8eLLM\u7684\u63a8\u7406\u3002"}}
{"id": "2602.17321", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17321", "abs": "https://arxiv.org/abs/2602.17321", "authors": ["Christoph Balada", "Aida Romano-Martinez", "Payal Varshney", "Vincent ten Cate", "Katharina Geschke", "Jonas Tesarz", "Paul Cla\u00dfen", "Alexander K. Schuster", "Dativa Tibyampansha", "Karl-Patrik Kresoja", "Philipp S. Wild", "Sheraz Ahmed", "Andreas Dengel"], "title": "The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound", "comment": null, "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u4ece\u9888\u52a8\u8109\u8d85\u58f0\u89c6\u9891\u4e2d\u63d0\u53d6\u8840\u7ba1\u635f\u4f24\u8868\u5f81\uff0c\u4ee5\u9ad8\u8840\u538b\u4e3a\u5f31\u6807\u7b7e\uff0c\u5b9e\u73b0\u5fc3\u8840\u7ba1\u98ce\u9669\u5206\u5c42\uff0c\u6027\u80fd\u5ab2\u7f8e\u6216\u4f18\u4e8e\u4f20\u7edf\u98ce\u9669\u6a21\u578b", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u4f46\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u53d7\u9650\u4e8e\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u3002\u9888\u52a8\u8109\u8d85\u58f0\u4f5c\u4e3a\u975e\u4fb5\u5165\u6027\u3001\u5e7f\u6cdb\u53ef\u53ca\u7684\u6a21\u6001\uff0c\u8574\u542b\u4e30\u5bcc\u7684\u7ed3\u6784\u548c\u8840\u6d41\u52a8\u529b\u5b66\u4fe1\u606f\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51fa\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u9888\u52a8\u8109\u8d85\u58f0\u89c6\u9891\u4e2d\u63d0\u53d6\u8840\u7ba1\u635f\u4f24\u7684\u4e34\u5e8a\u610f\u4e49\u8868\u5f81\uff0c\u4f7f\u7528\u9ad8\u8840\u538b\u4f5c\u4e3a\u5f31\u4ee3\u7406\u6807\u7b7e\u3002\u6a21\u578b\u5b66\u4e60\u5177\u6709\u751f\u7269\u5b66\u5408\u7406\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7684\u7a33\u5065\u7279\u5f81\u3002", "result": "\u9ad8\u8840\u7ba1\u635f\u4f24\u8bc4\u5206\u80fd\u6709\u6548\u5206\u5c42\u5fc3\u808c\u6897\u6b7b\u3001\u5fc3\u6e90\u6027\u6b7b\u4ea1\u548c\u5168\u56e0\u6b7b\u4ea1\u98ce\u9669\uff0c\u5339\u914d\u6216\u4f18\u4e8eSCORE2\u7b49\u4f20\u7edf\u98ce\u9669\u6a21\u578b\u3002\u53ef\u89e3\u91caAI\u5206\u6790\u663e\u793a\u6a21\u578b\u4f9d\u8d56\u8840\u7ba1\u5f62\u6001\u548c\u8840\u7ba1\u5468\u56f4\u7ec4\u7ec7\u7279\u5f81\u3002", "conclusion": "\u5e38\u89c4\u9888\u52a8\u8109\u8d85\u58f0\u5305\u542b\u6bd4\u4ee5\u5f80\u8ba4\u77e5\u66f4\u591a\u7684\u9884\u540e\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u4e3a\u4eba\u7fa4\u5fc3\u8840\u7ba1\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u975e\u4fb5\u5165\u6027\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5de5\u5177\uff0c\u65e0\u9700\u4f9d\u8d56\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u6216\u590d\u6742\u4e34\u5e8a\u8f93\u5165\u3002"}}
{"id": "2602.17206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17206", "abs": "https://arxiv.org/abs/2602.17206", "authors": ["Ron Shapira Weber", "Oren Freifeld"], "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch", "comment": "Technical Report", "summary": "We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.", "AI": {"tldr": "\u8f6fDTW-CUDA-Torch\uff1a\u4e00\u4e2a\u5f00\u6e90\u7684PyTorch\u5e93\uff0c\u7528\u4e8e\u5728GPU\u4e0a\u8ba1\u7b97\u8f6f\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08SoftDTW\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GPU\u5b9e\u73b0\u7684\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5e8f\u5217\u957f\u5ea6\u4e0a\u9650\u3001\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u548c\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u73b0\u6709GPU\u5b9e\u73b0\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u786c\u5e8f\u5217\u957f\u5ea6\u4e0a\u9650\u4e3a1024\uff1b2\uff09\u5c0f\u5e73\u6ed1\u53c2\u6570\u4e0b\u53cd\u5411\u4f20\u64ad\u6570\u503c\u4e0d\u7a33\u5b9a\uff1b3\uff09\u8ba1\u7b97\u6210\u5bf9\u8ddd\u79bb\u5f20\u91cf\u5bfc\u81f4GPU\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86SoftDTW\u5728\u5927\u89c4\u6a21\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a1\uff09\u5e73\u94fa\u53cd\u5bf9\u89d2\u7ebf\u6838\u6267\u884c\uff0c\u6d88\u9664\u5e8f\u5217\u957f\u5ea6\u9650\u5236\uff1b2\uff09\u5bf9\u6570\u7a7a\u95f4\u53cd\u5411\u4f20\u64ad\uff0c\u9632\u6b62\u6d6e\u70b9\u6ea2\u51fa\uff1b3\uff09\u878d\u5408\u8ddd\u79bb\u8ba1\u7b97\u6a21\u5f0f\uff0c\u6d88\u9664O(BNM)\u4e2d\u95f4\u8ddd\u79bb\u5f20\u91cf\uff0c\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "\u8be5\u5e93\u652f\u6301\u4efb\u610f\u5e8f\u5217\u957f\u5ea6\uff0c\u5b8c\u6574\u7684PyTorch\u81ea\u52a8\u5fae\u5206\u96c6\u6210\uff0c\u4ee5\u53ca\u8f6fDTW\u91cd\u5fc3\u8ba1\u7b97\u3002\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\uff0c\u5185\u5b58\u51cf\u5c11\u9ad8\u8fbe98%\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "softdtw-cuda-torch\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u5185\u5b58\u53cb\u597d\u7684GPU SoftDTW\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5b9e\u73b0\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2602.17312", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17312", "abs": "https://arxiv.org/abs/2602.17312", "authors": ["Hsin-Jung Yang", "Zhanhong Jiang", "Prajwal Koirala", "Qisai Liu", "Cody Fleming", "Soumik Sarkar"], "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy", "comment": "17th ACM/IEEE International Conference on Cyber-Physical Systems", "summary": "Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.", "AI": {"tldr": "LexiSafe\u662f\u4e00\u4e2a\u8bcd\u5178\u5e8f\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u5178\u5e8f\u4f18\u5148\u7ea7\u548c\u7ed3\u6784\u504f\u7f6e\u6765\u9632\u6b62\u5b89\u5168\u6f02\u79fb\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u548c\u5b9e\u8df5\u4fdd\u969c\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5b89\u5168RL\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u7ea6\u675f\u677e\u5f1b\u6216\u8054\u5408\u4f18\u5316\u6765\u5e73\u8861\u5956\u52b1-\u5b89\u5168\u6743\u8861\uff0c\u4f46\u7f3a\u4e4f\u9632\u6b62\u5b89\u5168\u6f02\u79fb\u7684\u7ed3\u6784\u673a\u5236\u3002\u5bf9\u4e8e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\uff0c\u8bad\u7ec3\u671f\u95f4\u7684\u5b89\u5168\u8fdd\u89c4\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\uff0c\u4e14\u53ea\u6709\u9884\u6536\u96c6\u6570\u636e\u53ef\u7528\u3002", "method": "\u63d0\u51faLexiSafe\u8bcd\u5178\u5e8f\u79bb\u7ebfRL\u6846\u67b6\uff1a1) LexiSafe-SC\u5355\u6210\u672c\u516c\u5f0f\u7528\u4e8e\u6807\u51c6\u79bb\u7ebf\u5b89\u5168RL\uff0c\u63a8\u5bfc\u5b89\u5168\u8fdd\u89c4\u548c\u6027\u80fd\u6b21\u4f18\u6027\u8fb9\u754c\uff1b2) LexiSafe-MC\u6269\u5c55\u652f\u6301\u591a\u5b89\u5168\u6210\u672c\u7684\u5c42\u6b21\u5b89\u5168\u9700\u6c42\uff0c\u5e76\u8fdb\u884c\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\u3002", "result": "\u7ecf\u9a8c\u4e0a\uff0cLexiSafe\u76f8\u6bd4\u7ea6\u675f\u79bb\u7ebf\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e86\u5b89\u5168\u8fdd\u89c4\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u6027\u80fd\u3002\u7406\u8bba\u4e0a\u6709\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u7edf\u4e00\u4e86\u8bcd\u5178\u5e8f\u4f18\u5148\u7ea7\u4e0e\u7ed3\u6784\u504f\u7f6e\u3002", "conclusion": "LexiSafe\u4e3a\u5b89\u5168\u5173\u952eCPS\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bcd\u5178\u5e8f\u4f18\u5148\u7ea7\u548c\u7ed3\u6784\u504f\u7f6e\u673a\u5236\u6709\u6548\u9632\u6b62\u5b89\u5168\u6f02\u79fb\u3002"}}
{"id": "2602.17342", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17342", "abs": "https://arxiv.org/abs/2602.17342", "authors": ["Luzhi Wang", "Xuanshuo Fu", "He Zhang", "Chuang Liu", "Xiaobao Wang", "Hongbo Liu"], "title": "From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection", "comment": "9pages, 5 figures", "summary": "Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \\textbf{S}elf-\\textbf{I}mproving \\textbf{G}raph \\textbf{O}ut-\\textbf{o}f-\\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.", "AI": {"tldr": "SIGOOD\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u7684\u56feOOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u589e\u5f3a\u548c\u80fd\u91cf\u504f\u597d\u4f18\u5316\u5b9e\u73b0\u65e0\u76d1\u7763\u7684\u56fe\u5206\u5e03\u5916\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u56feOOD\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u5355\u6b21\u63a8\u7406\u8303\u5f0f\uff0c\u65e0\u6cd5\u6e10\u8fdb\u4fee\u6b63\u9519\u8bef\u9884\u6d4b\u4ee5\u653e\u5927OOD\u4fe1\u53f7\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65e0\u76d1\u7763\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51faSIGOOD\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u63d0\u793a\u6784\u5efa\u63d0\u793a\u589e\u5f3a\u56fe\u6765\u653e\u5927OOD\u4fe1\u53f7\uff0c\u5f15\u5165\u80fd\u91cf\u504f\u597d\u4f18\u5316\u635f\u5931\uff0c\u5728\u81ea\u6539\u8fdb\u5faa\u73af\u4e2d\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u6700\u7ec8\u4f7f\u7528\u6700\u4f18\u63d0\u793a\u589e\u5f3a\u56fe\u8fdb\u884cOOD\u68c0\u6d4b\u3002", "result": "\u572821\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8bc1\u5b9e\u4e86SIGOOD\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "SIGOOD\u901a\u8fc7\u96c6\u6210\u6301\u7eed\u81ea\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u4e3a\u56feOOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u6e10\u8fdb\u4fee\u6b63\u9884\u6d4b\u5e76\u653e\u5927OOD\u4fe1\u53f7\u3002"}}
