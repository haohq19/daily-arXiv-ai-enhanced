{"id": "2511.05516", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.05516", "abs": "https://arxiv.org/abs/2511.05516", "authors": ["Canxiang Yan", "Chunxiang Jin", "Dawei Huang", "Haibing Yu", "Han Peng", "Hui Zhan", "Jie Gao", "Jing Peng", "Jingdong Chen", "Jun Zhou", "Kaimeng Ren", "Ming Yang", "Mingxue Yang", "Qiang Xu", "Qin Zhao", "Ruijie Xiong", "Shaoxiong Lin", "Xuezhi Wang", "Yi Yuan", "Yifei Wu", "Yongjie Lyu", "Zhengyu He", "Zhihao Qiu", "Zhiqiang Fang", "Ziyuan Huang"], "title": "Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation", "comment": "32 pages, 8 figures", "summary": "Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8bed\u97f3\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668MingTok-Audio\u6574\u5408\u8bed\u4e49\u548c\u58f0\u5b66\u7279\u5f81\uff0c\u5f00\u53d1\u4e86\u8bed\u97f3\u8bed\u8a00\u6a21\u578bMing-UniAudio\u53ca\u5176\u7f16\u8f91\u7248\u672cMing-UniAudio-Edit\uff0c\u9996\u4e2a\u4ec5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u901a\u7528\u81ea\u7531\u5f62\u5f0f\u8bed\u97f3\u7f16\u8f91\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5b58\u5728\u8868\u5f81\u51b2\u7a81\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u6307\u4ee4\u7684\u81ea\u7531\u5f62\u5f0f\u8bed\u97f3\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u7684\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668MingTok-Audio\u6574\u5408\u8bed\u4e49\u548c\u58f0\u5b66\u7279\u5f81\uff0c\u57fa\u4e8e\u6b64\u6784\u5efa\u8bed\u97f3\u8bed\u8a00\u6a21\u578bMing-UniAudio\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bad\u7ec3\u4e13\u95e8\u7684\u8bed\u97f3\u7f16\u8f91\u6a21\u578bMing-UniAudio-Edit\u3002", "result": "\u5728ContextASR\u57fa\u51c6\u6d4b\u8bd5\u4e2d12\u4e2a\u6307\u6807\u4e2d\u76848\u4e2a\u8fbe\u5230\u65b0SOTA\uff0c\u4e2d\u6587\u8bed\u97f3\u514b\u9686\u7684Seed-TTS-WER\u8fbe\u52300.95\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u81ea\u7531\u5f62\u5f0f\u8bed\u97f3\u7f16\u8f91\u57fa\u51c6Ming-Freeform-Audio-Edit\u3002", "conclusion": "\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u8868\u5f81\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u8bed\u97f3\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.05855", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05855", "abs": "https://arxiv.org/abs/2511.05855", "authors": ["Jiayu Zhou", "Qiwei Wu", "Jian Li", "Zhe Chen", "Xiaogang Xiong", "Renjing Xu"], "title": "Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills", "comment": "Accepted for the 40th Annual AAAI Conference on Artificial Intelligence (2026)", "summary": "Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5206\u5c42\u8bed\u4e49\u5206\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u6267\u884c\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u7c7b\u6f14\u793a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548c\u4e13\u5bb6\u5de5\u7a0b\uff0c\u5b58\u5728\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u539f\u5b50\u6280\u80fd\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7528RL\u8bad\u7ec3\u6bcf\u4e2a\u539f\u59cb\u6280\u80fd\u7684\u7b56\u7565\uff0c\u5e76\u52a0\u5165\u660e\u786e\u7684\u529b\u7ea6\u675f\u9632\u6b62\u7269\u4f53\u635f\u574f\u3002\u4f7f\u7528VLM\u8fdb\u884c\u9ad8\u5c42\u4efb\u52a1\u5206\u89e3\u548c\u6280\u80fd\u89c4\u5212\uff0c\u751f\u6210\u591a\u6837\u5316\u4e13\u5bb6\u6f14\u793a\uff0c\u901a\u8fc7\u89c6\u89c9-\u89e6\u89c9\u6269\u6563\u7b56\u7565\u84b8\u998f\u4e3a\u7edf\u4e00\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5168\u9762\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u4e86\u6700\u4f18\u6f14\u793a\u751f\u6210\u6d41\u7a0b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u6280\u80fd\u84b8\u998f\u7684\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u3002\u4eff\u771f\u5b9e\u9a8c\u548c\u7269\u7406\u90e8\u7f72\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u957f\u65f6\u7a0b\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\uff0c\u4e14VLM\u5f15\u5bfc\u7684\u539f\u5b50\u6280\u80fd\u6846\u67b6\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6cdb\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\uff0cVLM\u5f15\u5bfc\u7684\u539f\u5b50\u6280\u80fd\u5206\u89e3\u4e3a\u591a\u6837\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.05591", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05591", "abs": "https://arxiv.org/abs/2511.05591", "authors": ["Chaimaa Medjadji", "Sadi Alawadi", "Feras M. Awaysheh", "Guilain Leduc", "Sylvain Kubler", "Yves Le Traon"], "title": "FedSparQ: Adaptive Sparse Quantization with Error Feedback for Robust & Efficient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across decentralized clients while preserving data privacy by keeping raw data local. However, FL suffers from significant communication overhead due to the frequent exchange of high-dimensional model updates over constrained networks. In this paper, we present FedSparQ, a lightweight compression framework that dynamically sparsifies the gradient of each client through an adaptive threshold, applies half-precision quantization to retained entries and integrates residuals from error feedback to prevent loss of information. FedSparQ requires no manual tuning of sparsity rates or quantization schedules, adapts seamlessly to both homogeneous and heterogeneous data distributions, and is agnostic to model architecture. Through extensive empirical evaluation on vision benchmarks under independent and identically distributed (IID) and non-IID data, we show that FedSparQ substantially reduces communication overhead (reducing by 90% of bytes sent compared to FedAvg) while preserving or improving model accuracy (improving by 6% compared to FedAvg non-compressed solution or to state-of-the-art compression models) and enhancing convergence robustness (by 50%, compared to the other baselines). Our approach provides a practical, easy-to-deploy solution for bandwidth-constrained federated deployments and lays the groundwork for future extensions in adaptive precision and privacy-preserving protocols.", "AI": {"tldr": "FedSparQ\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8054\u90a6\u5b66\u4e60\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u5316\u3001\u534a\u7cbe\u5ea6\u91cf\u5316\u548c\u8bef\u5dee\u53cd\u9988\u6765\u51cf\u5c1190%\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u9762\u4e34\u663e\u8457\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u56e0\u4e3a\u9700\u8981\u5728\u53d7\u9650\u7f51\u7edc\u4e0a\u9891\u7e41\u4ea4\u6362\u9ad8\u7ef4\u6a21\u578b\u66f4\u65b0\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u9608\u503c\u52a8\u6001\u7a00\u758f\u5316\u5ba2\u6237\u7aef\u68af\u5ea6\uff0c\u5bf9\u4fdd\u7559\u6761\u76ee\u5e94\u7528\u534a\u7cbe\u5ea6\u91cf\u5316\uff0c\u5e76\u96c6\u6210\u8bef\u5dee\u53cd\u9988\u4ee5\u9632\u6b62\u4fe1\u606f\u4e22\u5931\u3002\u65e0\u9700\u624b\u52a8\u8c03\u6574\u7a00\u758f\u7387\u6216\u91cf\u5316\u8ba1\u5212\uff0c\u9002\u5e94\u540c\u8d28\u548c\u5f02\u8d28\u6570\u636e\u5206\u5e03\uff0c\u4e14\u4e0e\u6a21\u578b\u67b6\u6784\u65e0\u5173\u3002", "result": "\u5728IID\u548c\u975eIID\u6570\u636e\u4e0b\u7684\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedSparQ\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff08\u76f8\u6bd4FedAvg\u51cf\u5c1190%\u5b57\u8282\u4f20\u8f93\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\uff08\u76f8\u6bd4FedAvg\u975e\u538b\u7f29\u65b9\u6848\u6216\u6700\u5148\u8fdb\u538b\u7f29\u6a21\u578b\u63d0\u9ad86%\uff09\uff0c\u5e76\u589e\u5f3a\u6536\u655b\u9c81\u68d2\u6027\uff08\u76f8\u6bd4\u5176\u4ed6\u57fa\u7ebf\u63d0\u9ad850%\uff09\u3002", "conclusion": "FedSparQ\u4e3a\u5e26\u5bbd\u53d7\u9650\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u6613\u4e8e\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u81ea\u9002\u5e94\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u534f\u8bae\u7684\u672a\u6765\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.05759", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05759", "abs": "https://arxiv.org/abs/2511.05759", "authors": ["Marcelo Arenas", "Pablo Barcel\u00f3", "Luis Cofr\u00e9", "Alexander Kozachinskiy"], "title": "Language Generation: Complexity Barriers and Implications for Learning", "comment": null, "summary": "Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u8bed\u8a00\u751f\u6210\u7406\u8bba\u53ef\u80fd\u6027\u4e0e\u5b9e\u9645\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u5373\u4f7f\u5bf9\u4e8e\u7b80\u5355\u7684\u6b63\u5219\u548c\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff0c\u6210\u529f\u751f\u6210\u6240\u9700\u7684\u6837\u672c\u6570\u91cf\u4e5f\u53ef\u80fd\u6781\u5176\u5e9e\u5927\u751a\u81f3\u4e0d\u53ef\u8ba1\u7b97\u3002", "motivation": "\u867d\u7136Kleinberg\u548cMullainathan\u8bc1\u660e\u4e86\u8bed\u8a00\u751f\u6210\u5728\u7406\u8bba\u4e0a\u662f\u53ef\u80fd\u7684\uff0c\u4f46\u8fd9\u79cd\u7406\u8bba\u4fdd\u8bc1\u5e76\u672a\u8bf4\u660e\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8bed\u8a00\u751f\u6210\u5728\u5b9e\u9645\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7b80\u5355\u4e14\u7ecf\u8fc7\u5145\u5206\u7814\u7a76\u7684\u8bed\u8a00\u5bb6\u65cf\uff08\u5982\u6b63\u5219\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff09\uff0c\u7814\u7a76\u8fd9\u4e9b\u8bed\u8a00\u751f\u6210\u6240\u9700\u7684\u6837\u672c\u6570\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u8bed\u8a00\u5bb6\u65cf\uff0c\u6210\u529f\u751f\u6210\u6240\u9700\u7684\u6837\u672c\u6570\u91cf\u4e5f\u53ef\u80fd\u6781\u5176\u5e9e\u5927\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u65e0\u6cd5\u7528\u4efb\u4f55\u53ef\u8ba1\u7b97\u51fd\u6570\u6765\u754c\u5b9a\u3002", "conclusion": "\u7406\u8bba\u53ef\u80fd\u6027\u4e0e\u9ad8\u6548\u53ef\u5b66\u4e60\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u89e3\u91ca\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u7684\u7ecf\u9a8c\u6210\u529f\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u89c6\u89d2\uff0c\u8003\u8651\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u6784\u7279\u6027\u5982\u4f55\u4f7f\u6709\u6548\u751f\u6210\u5728\u5b9e\u8df5\u4e2d\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2511.06202", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06202", "abs": "https://arxiv.org/abs/2511.06202", "authors": ["Shahram Najam Syed", "Yatharth Ahuja", "Arthur Jakobsson", "Jeff Ichnowski"], "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval", "comment": "10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors", "summary": "Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.", "AI": {"tldr": "ExpReS-VLA\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u548c\u68c0\u7d22\u673a\u5236\uff0c\u5728\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u7684\u540c\u65f6\uff0c\u4e13\u95e8\u5316\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9002\u5e94\u65b0\u90e8\u7f72\u73af\u5883\u65f6\u6548\u7387\u4e0d\u9ad8\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5728\u6709\u9650\u4efb\u52a1\u96c6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u9ad8\u6027\u80fd\u6bd4\u5e7f\u6cdb\u6cdb\u5316\u66f4\u91cd\u8981\u3002", "method": "\u5b58\u50a8\u51bb\u7ed3\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u7684\u7d27\u51d1\u7279\u5f81\u8868\u793a\u800c\u975e\u539f\u59cb\u56fe\u50cf-\u52a8\u4f5c\u5bf9\uff0c\u51cf\u5c1197%\u5185\u5b58\u4f7f\u7528\uff1b\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u6307\u5bfc\u9002\u5e94\uff1b\u4f18\u5148\u56de\u653e\u6210\u529f\u8f68\u8ff9\uff1b\u5f15\u5165\u9608\u503c\u6df7\u5408\u5bf9\u6bd4\u635f\u5931\uff0c\u4ece\u6210\u529f\u548c\u5931\u8d25\u5c1d\u8bd5\u4e2d\u5b66\u4e60\u3002", "result": "\u5728LIBERO\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u6210\u529f\u7387\u4ece82.6%\u63d0\u5347\u81f393.1%\uff0c\u957f\u65f6\u7a0b\u4efb\u52a1\u4ece61%\u63d0\u5347\u81f372.3%\uff1b\u5728\u7269\u7406\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u57285\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fbe\u523098%\u6210\u529f\u7387\uff0c\u800c\u6734\u7d20\u5fae\u8c03\u65b9\u6cd5\u5206\u522b\u4e3a84.7%\u548c32%\u3002", "conclusion": "ExpReS-VLA\u80fd\u591f\u9ad8\u6548\u9002\u5e94\u65b0\u73af\u5883\uff0c\u4ec5\u970031\u79d2\u548c12\u4e2a\u6f14\u793a\u6837\u672c\u5373\u53ef\u5b8c\u6210\u9002\u5e94\uff0c\u4e3a\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06267", "abs": "https://arxiv.org/abs/2511.06267", "authors": ["Jiayi Chen", "Wei Zhao", "Liangwang Ruan", "Baoquan Chen", "He Wang"], "title": "Robust Differentiable Collision Detection for General Objects", "comment": null, "summary": "Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u9ad8\u6548\u7684\u5fae\u5206\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\uff0c\u652f\u6301\u51f8\u9762\u548c\u51f9\u9762\u7269\u4f53\uff0c\u901a\u8fc7\u8ddd\u79bb\u5bfc\u5411\u7684\u968f\u673a\u5e73\u6ed1\u3001\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7b49\u6548\u68af\u5ea6\u4f20\u8f93\u5b9e\u73b0\u7a33\u5065\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u78b0\u649e\u68c0\u6d4b\u7b97\u6cd5\u5982GJK+EPA\u4e0d\u53ef\u5fae\u5206\uff0c\u963b\u788d\u4e86\u68af\u5ea6\u6d41\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002\u73b0\u6709\u5fae\u5206\u65b9\u6cd5\u4ec5\u9650\u4e8e\u51f8\u9762\u7269\u4f53\u4e14\u5bf9\u590d\u6742\u51e0\u4f55\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u8ddd\u79bb\u5bfc\u5411\u7684\u4e00\u9636\u968f\u673a\u5e73\u6ed1\u3001\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7b49\u6548\u68af\u5ea6\u4f20\u8f93\u6280\u672f\uff0c\u6784\u5efa\u652f\u6301\u51f8\u9762\u548c\u51f9\u9762\u7269\u4f53\u7684\u5fae\u5206\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\u3002", "result": "\u5728DexGraspNet\u548cObjaverse\u7684\u590d\u6742\u7f51\u683c\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u7075\u5de7\u6293\u53d6\u5408\u6210\u4ee5\u63d0\u5347\u6293\u53d6\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5fae\u5206\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\u5728\u590d\u6742\u51e0\u4f55\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u68af\u5ea6\u4f18\u5316\u652f\u6301\u3002"}}
{"id": "2511.06311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06311", "abs": "https://arxiv.org/abs/2511.06311", "authors": ["Seiichi Yamamoto", "Hiroki Ishizuka", "Takumi Kawasetsu", "Koh Hosoda", "Takayuki Kameoka", "Kango Yanagida", "Takato Horii", "Sei Ikeda", "Osamu Oshiro"], "title": "External Photoreflective Tactile Sensing Based on Surface Deformation Measurement", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f6f\u673a\u5668\u4eba\u673a\u68b0\u67d4\u987a\u6027\u7684\u89e6\u89c9\u611f\u77e5\u65b9\u6cd5\uff0c\u4f7f\u7528\u5916\u90e8\u53ef\u9644\u52a0\u7684\u5149\u53cd\u5c04\u6a21\u5757\u8bfb\u53d6\u7845\u80f6\u76ae\u80a4\u8868\u9762\u53d8\u5f62\u6765\u4f30\u8ba1\u63a5\u89e6\u529b\uff0c\u65e0\u9700\u5d4c\u5165\u89e6\u89c9\u4f20\u611f\u5668\u3002", "motivation": "\u5c06\u4f20\u611f\u5668\u7f6e\u4e8e\u63a5\u89e6\u754c\u9762\u4e4b\u5916\u53ef\u964d\u4f4e\u635f\u574f\u98ce\u9669\u3001\u4fdd\u6301\u67d4\u8f6f\u6027\uff0c\u5e76\u7b80\u5316\u5236\u9020\u548c\u7ef4\u62a4\u3002\u76f8\u6bd4\u6db2\u4f53\u586b\u5145\u6216\u7ebf\u7f06\u5d4c\u5165\u7684\u89e6\u89c9\u76ae\u80a4\uff0c\u8be5\u6a21\u5757\u5316\u9644\u52a0\u67b6\u6784\u589e\u5f3a\u4e86\u8010\u7528\u6027\u3001\u51cf\u5c11\u4e86\u5e03\u7ebf\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u5149\u5b66\u4f20\u611f\u5143\u4ef6\u548c\u67d4\u987a\u76ae\u80a4\u7684\u8868\u5f81\uff0c\u8bbe\u8ba1\u539f\u578b\u89e6\u89c9\u4f20\u611f\u5668\u3002\u5229\u7528\u5916\u90e8\u5149\u5b66\u6a21\u5757\u8bfb\u53d6\u76ae\u80a4\u5e94\u53d8\u6a21\u5f0f\u6765\u611f\u77e5\u63a5\u89e6\u529b\u3002", "result": "\u538b\u7f29\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u4e0e\u7406\u8bba\u4e00\u81f4\u7684\u5355\u8c03\u529b\u8f93\u51fa\u5173\u7cfb\u3001\u4f4e\u6ede\u540e\u6027\u3001\u9ad8\u91cd\u590d\u6027\u4ee5\u53ca\u5bf9\u538b\u75d5\u901f\u5ea6\u7684\u5c0f\u54cd\u5e94\u3002\u5728\u8f6f\u673a\u5668\u4eba\u6293\u624b\u4e0a\u7684\u96c6\u6210\u6f14\u793a\u4e2d\uff0c\u6a21\u5757\u80fd\u53ef\u9760\u68c0\u6d4b\u6293\u63e1\u4e8b\u4ef6\u3002", "conclusion": "\u5229\u7528\u8868\u9762\u67d4\u987a\u6027\u4e0e\u5916\u90e8\u5149\u5b66\u6a21\u5757\u4e3a\u8f6f\u673a\u5668\u4eba\u63d0\u4f9b\u529b\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u7a33\u5065\u7684\u9014\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u7075\u6d3b\u6027\u548c\u53ef\u5236\u9020\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u548c\u5b89\u5168\u4eba\u673a\u534f\u4f5c\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.06175", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.06175", "abs": "https://arxiv.org/abs/2511.06175", "authors": ["Kaijie Xu", "Fandi Meng", "Clark Verbrugge", "Simon Lucas"], "title": "CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference", "comment": null, "summary": "In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary \"reasoning tool.\" Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.", "AI": {"tldr": "CSP4SDG\u662f\u4e00\u4e2a\u7528\u4e8e\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u7684\u7ea6\u675f\u6ee1\u8db3\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6e38\u620f\u4e8b\u4ef6\u548c\u5bf9\u8bdd\u6765\u63a8\u65ad\u73a9\u5bb6\u9690\u85cf\u8eab\u4efd\uff0c\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\uff0c\u73a9\u5bb6\u9690\u85cf\u8eab\u4efd\u5e76\u6545\u610f\u8bef\u5bfc\u4ed6\u4eba\uff0c\u4f7f\u5f97\u89d2\u8272\u63a8\u65ad\u6210\u4e3a\u6838\u5fc3\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u51c6\u786e\u7684\u89d2\u8272\u8bc6\u522b\u662f\u73a9\u5bb6\u548cAI\u8868\u73b0\u7684\u5173\u952e\u57fa\u7840\u3002", "method": "\u63d0\u51faCSP4SDG\u6846\u67b6\uff0c\u5c06\u6e38\u620f\u4e8b\u4ef6\u548c\u5bf9\u8bdd\u6620\u5c04\u5230\u56db\u4e2a\u8bed\u8a00\u65e0\u5173\u7684\u7ea6\u675f\u7c7b\u522b\uff1a\u8bc1\u636e\u3001\u73b0\u8c61\u3001\u65ad\u8a00\u548c\u5047\u8bbe\u3002\u4f7f\u7528\u786c\u7ea6\u675f\u4fee\u526a\u4e0d\u53ef\u80fd\u7684\u89d2\u8272\u5206\u914d\uff0c\u52a0\u6743\u8f6f\u7ea6\u675f\u5bf9\u5269\u4f59\u5206\u914d\u8fdb\u884c\u8bc4\u5206\uff0c\u4fe1\u606f\u589e\u76ca\u52a0\u6743\u5c06\u6bcf\u4e2a\u5047\u8bbe\u4e0e\u5176\u5728\u71b5\u51cf\u5c11\u4e0b\u7684\u671f\u671b\u503c\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSP4SDG\u5728\u6240\u6709\u63a8\u7406\u573a\u666f\u4e2d\u90fd\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5f53\u4f5c\u4e3a\u8f85\u52a9\"\u63a8\u7406\u5de5\u5177\"\u63d0\u4f9b\u7ed9LLM\u65f6\u80fd\u63d0\u5347\u5176\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u539f\u5219\u6027\u6982\u7387\u63a8\u7406\u662f\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u91cd\u578b\u795e\u7ecf\u6a21\u578b\u7684\u53ef\u6269\u5c55\u66ff\u4ee3\u6216\u8865\u5145\u65b9\u6848\uff0c\u4ea7\u751f\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u89d2\u8272\u540e\u9a8c\u5206\u5e03\u5e76\u80fd\u5b9e\u65f6\u66f4\u65b0\u3002"}}
{"id": "2511.05604", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05604", "abs": "https://arxiv.org/abs/2511.05604", "authors": ["Subash Gautam", "Alejandro Vargas-Uscategui", "Peter King", "Hans Lohr", "Alireza Bab-Hadiashar", "Ivan Cole", "Ehsan Asadi"], "title": "In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing", "comment": null, "summary": "Additive manufacturing (AM) is an emerging digital manufacturing technology to produce complex and freeform objects through a layer-wise deposition. High deposition rate robotic AM (HDRRAM) processes, such as cold spray additive manufacturing (CSAM), offer significantly increased build speeds by delivering large volumes of material per unit time. However, maintaining shape accuracy remains a critical challenge, particularly due to process instabilities in current open-loop systems. Detecting these deviations as they occur is essential to prevent error propagation, ensure part quality, and minimize post-processing requirements. This study presents a real-time monitoring system to acquire and reconstruct the growing part and directly compares it with a near-net reference model to detect the shape deviation during the manufacturing process. The early identification of shape inconsistencies, followed by segmenting and tracking each deviation region, paves the way for timely intervention and compensation to achieve consistent part quality.", "AI": {"tldr": "\u5f00\u53d1\u5b9e\u65f6\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5728\u9ad8\u901f\u673a\u5668\u4eba\u589e\u6750\u5236\u9020\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u548c\u8ddf\u8e2a\u5f62\u72b6\u504f\u5dee\uff0c\u5b9e\u73b0\u53ca\u65f6\u5e72\u9884\u4ee5\u4fdd\u8bc1\u96f6\u4ef6\u8d28\u91cf", "motivation": "\u9ad8\u901f\u673a\u5668\u4eba\u589e\u6750\u5236\u9020\uff08\u5982\u51b7\u55b7\u6d82\uff09\u867d\u7136\u6c89\u79ef\u901f\u7387\u9ad8\uff0c\u4f46\u5f53\u524d\u5f00\u73af\u7cfb\u7edf\u5b58\u5728\u8fc7\u7a0b\u4e0d\u7a33\u5b9a\u6027\uff0c\u96be\u4ee5\u4fdd\u6301\u5f62\u72b6\u7cbe\u5ea6\uff0c\u9700\u8981\u5b9e\u65f6\u68c0\u6d4b\u504f\u5dee\u4ee5\u9632\u6b62\u8bef\u5dee\u4f20\u64ad", "method": "\u63d0\u51fa\u5b9e\u65f6\u76d1\u6d4b\u7cfb\u7edf\uff0c\u91c7\u96c6\u548c\u91cd\u5efa\u751f\u957f\u4e2d\u7684\u96f6\u4ef6\uff0c\u4e0e\u8fd1\u51c0\u5f62\u53c2\u8003\u6a21\u578b\u76f4\u63a5\u6bd4\u8f83\uff0c\u68c0\u6d4b\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u5f62\u72b6\u504f\u5dee\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u504f\u5dee\u533a\u57df\u8fdb\u884c\u5206\u5272\u548c\u8ddf\u8e2a", "result": "\u5b9e\u73b0\u4e86\u5f62\u72b6\u4e0d\u4e00\u81f4\u6027\u7684\u65e9\u671f\u8bc6\u522b\uff0c\u4e3a\u53ca\u65f6\u5e72\u9884\u548c\u8865\u507f\u63d0\u4f9b\u4e86\u57fa\u7840", "conclusion": "\u8be5\u76d1\u6d4b\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u4e00\u81f4\u7684\u96f6\u4ef6\u8d28\u91cf\uff0c\u51cf\u5c11\u540e\u5904\u7406\u9700\u6c42"}}
{"id": "2511.06262", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06262", "abs": "https://arxiv.org/abs/2511.06262", "authors": ["Siming Zhao", "Qi Li"], "title": "GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening", "comment": null, "summary": "Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.", "AI": {"tldr": "GAIA\u662f\u4e00\u4e2a\u9762\u5411B2B\u8c08\u5224\u548c\u7b5b\u9009\u7684\u6cbb\u7406\u4f18\u5148\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u89d2\u8272\u3001\u4fe1\u606f\u95e8\u63a7\u8fdb\u5c55\u3001\u53cc\u91cd\u53cd\u9988\u96c6\u6210\u548c\u6388\u6743\u8fb9\u754c\u673a\u5236\uff0c\u786e\u4fddAI\u59d4\u6258\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u9ad8\u98ce\u9669B2B\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u53d7\u5230\u6cbb\u7406\u9650\u5236\uff0c\u5305\u62ec\u9632\u6b62\u672a\u7ecf\u6388\u6743\u627f\u8bfa\u3001\u786e\u4fdd\u5145\u5206\u4fe1\u606f\u6536\u96c6\u4ee5\u53ca\u7ef4\u6301\u6709\u6548\u4eba\u7c7b\u76d1\u7763\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u81ea\u4e3b\u8c08\u5224\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u9700\u6c42\u5982\u5206\u9636\u6bb5\u4fe1\u606f\u6536\u96c6\u3001\u660e\u786e\u6388\u6743\u8fb9\u754c\u548c\u7cfb\u7edf\u53cd\u9988\u96c6\u6210\u3002", "method": "GAIA\u6846\u67b6\u5b9a\u4e49\u4e09\u4e2a\u6838\u5fc3\u89d2\u8272\uff08\u59d4\u6258\u4eba\u3001\u4ee3\u7406\u3001\u5bf9\u65b9\uff09\u548c\u53ef\u9009\u6279\u8bc4\u8005\u89d2\u8272\uff0c\u901a\u8fc7\u4e09\u4e2a\u673a\u5236\u7ec4\u7ec7\u4ea4\u4e92\uff1a\u4fe1\u606f\u95e8\u63a7\u8fdb\u5c55\u5206\u79bb\u7b5b\u9009\u4e0e\u8c08\u5224\uff1b\u53cc\u91cd\u53cd\u9988\u96c6\u6210\u7ed3\u5408AI\u6279\u8bc4\u4e0e\u4eba\u7c7b\u4fee\u6b63\uff1b\u6388\u6743\u8fb9\u754c\u4e0e\u660e\u786e\u5347\u7ea7\u8def\u5f84\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6b63\u5f0f\u7684\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u5b89\u5168\u4e0d\u53d8\u91cf\u7684\u534f\u8c03\u673a\u5236\uff0c\u901a\u8fc7\u4efb\u52a1\u5b8c\u6574\u6027\u8ddf\u8e2a\u5b9e\u73b0\u4fe1\u606f\u95e8\u63a7\u8fdb\u5c55\uff0c\u901a\u8fc7\u5e76\u884c\u5b66\u4e60\u901a\u9053\u5b9e\u73b0\u53cc\u91cd\u53cd\u9988\u96c6\u6210\uff0c\u5e76\u5efa\u7acb\u4e86\u7ed3\u5408\u81ea\u52a8\u5316\u534f\u8bae\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u6df7\u5408\u9a8c\u8bc1\u84dd\u56fe\u3002", "conclusion": "GAIA\u901a\u8fc7\u8fde\u63a5\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u89c4\u8303\uff0c\u7528\u4e8e\u5728\u91c7\u8d2d\u3001\u623f\u5730\u4ea7\u548c\u4eba\u5458\u914d\u7f6e\u7b49\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u548c\u53ef\u95ee\u8d23\u7684AI\u59d4\u6258\u3002"}}
{"id": "2511.06301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06301", "abs": "https://arxiv.org/abs/2511.06301", "authors": ["Azanzi Jiomekong", "Jean Bikim", "Patricia Negoue", "Joyce Chin"], "title": "Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems", "comment": "Submitted to Nature Scientific Data", "summary": "Evaluating semantic tables interpretation (STI) systems, (particularly, those based on Large Language Models- LLMs) especially in domain-specific contexts such as the security domain, depends heavily on the dataset. However, in the security domain, tabular datasets for state-of-the-art are not publicly available. In this paper, we introduce Secu-Table dataset, composed of more than 1500 tables with more than 15k entities constructed using security data extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES CSKG). Along with the dataset, all the code is publicly released. This dataset is made available to the research community in the context of the SemTab challenge on Tabular to Knowledge Graph Matching. This challenge aims to evaluate the performance of several STI based on open source LLMs. Preliminary evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source LLM.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Secu-Table\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b1500\u591a\u4e2a\u8868\u683c\u548c15k+\u5b9e\u4f53\u7684\u5b89\u5168\u9886\u57df\u8bed\u4e49\u8868\u89e3\u91ca\u6570\u636e\u96c6\uff0c\u57fa\u4e8eCVE\u548cCWE\u6570\u636e\u6784\u5efa\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u8868\u89e3\u91ca\u7cfb\u7edf\u3002", "motivation": "\u5728\u5b89\u5168\u9886\u57df\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u4e49\u8868\u89e3\u91ca\u7cfb\u7edf\u7684\u8868\u683c\u6570\u636e\u96c6\u5c1a\u672a\u516c\u5f00\u53ef\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u4eceCVE\u548cCWE\u6570\u636e\u6e90\u63d0\u53d6\u5b89\u5168\u6570\u636e\u6784\u5efa\u8868\u683c\uff0c\u4f7f\u7528Wikidata\u548cSEPSES CSKG\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u6807\u6ce8\uff0c\u521b\u5efa\u4e86\u5305\u542b1500\u591a\u4e2a\u8868\u683c\u7684\u6570\u636e\u96c6\u3002", "result": "\u53d1\u5e03\u4e86Secu-Table\u6570\u636e\u96c6\u53ca\u76f8\u5173\u4ee3\u7801\uff0c\u4f5c\u4e3aSemTab\u6311\u6218\u8d5b\u7684\u4e00\u90e8\u5206\uff0c\u5e76\u8fdb\u884c\u4e86\u521d\u6b65\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86Falcon3-7b-instruct\u3001Mistral-7B-Instruct\u548cGPT-4o mini\u7b49LLM\u4f5c\u4e3a\u57fa\u7ebf\u3002", "conclusion": "Secu-Table\u6570\u636e\u96c6\u586b\u8865\u4e86\u5b89\u5168\u9886\u57df\u8bed\u4e49\u8868\u89e3\u91ca\u8bc4\u4f30\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u3002"}}
{"id": "2511.06380", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06380", "abs": "https://arxiv.org/abs/2511.06380", "authors": ["Chen He", "Xun Jiang", "Lei Wang", "Hao Yang", "Chong Peng", "Peng Yan", "Fumin Shen", "Xing Xu"], "title": "What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as \"Echo Reflection\". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAEPO\u65b9\u6cd5\u89e3\u51b3LLMs\u5728\u590d\u6742\u9886\u57df\u63a8\u7406\u4e2d\u7684\"\u56de\u97f3\u53cd\u5c04\"\u95ee\u9898\uff0c\u901a\u8fc7\u63a7\u5236\u4fe1\u606f\u6d41\u548c\u81ea\u9002\u5e94\u71b5\u4f18\u5316\u6765\u63d0\u5347\u53cd\u601d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6d89\u53ca\u590d\u6742\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\uff0cLLMs\u5728\u53cd\u601d\u9636\u6bb5\u65e0\u6cd5\u4ea7\u751f\u65b0\u7684\u8ba4\u77e5\u89c1\u89e3\uff0c\u800c\u662f\u673a\u68b0\u91cd\u590d\u65e9\u671f\u63a8\u7406\u6b65\u9aa4\uff0c\u5f62\u6210\"\u56de\u97f3\u53cd\u5c04\"\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u71b5\u7b56\u7565\u4f18\u5316(AEPO)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53cd\u601d\u611f\u77e5\u4fe1\u606f\u8fc7\u6ee4(\u91cf\u5316\u8ba4\u77e5\u4fe1\u606f\u6d41\uff0c\u9632\u6b62\u6700\u7ec8\u7b54\u6848\u53d7\u65e9\u671f\u9519\u8bef\u8ba4\u77e5\u5f71\u54cd)\u548c\u81ea\u9002\u5e94\u71b5\u4f18\u5316(\u52a8\u6001\u5e73\u8861\u4e0d\u540c\u63a8\u7406\u9636\u6bb5\u7684\u63a2\u7d22\u4e0e\u5229\u7528)\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eAEPO\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u4e3b\u6d41\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "AEPO\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u9886\u57df\u63a8\u7406\u4e2d\u7684\u53cd\u601d\u8d28\u91cf\u95ee\u9898\uff0c\u901a\u8fc7\u63a7\u5236\u4fe1\u606f\u6d41\u548c\u4f18\u5316\u63a2\u7d22\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8ba4\u77e5\u53cd\u601d\u80fd\u529b\u3002"}}
{"id": "2511.06230", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06230", "abs": "https://arxiv.org/abs/2511.06230", "authors": ["Juntao Li", "Haobin Yuan", "Ling Luo", "Tengxiao Lv", "Yan Jiang", "Fan Wang", "Ping Zhang", "Huiyi Lv", "Jian Wang", "Yuanyuan Sun", "Hongfei Lin"], "title": "Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records", "comment": null, "summary": "Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CHIP 2025\u5171\u4eab\u4efb\u52a12\u7ade\u8d5b\uff0c\u8be5\u7ade\u8d5b\u65e8\u5728\u5f00\u53d1\u57fa\u4e8e\u4e2d\u56fd\u771f\u5b9e\u7535\u5b50\u75c5\u5386\u6570\u636e\u7684\u51fa\u9662\u836f\u7269\u63a8\u8350\u7cfb\u7edf\u3002\u6784\u5efa\u4e86\u5305\u542b5,894\u6761\u4f4f\u9662\u8bb0\u5f55\u7684CDrugRed\u6570\u636e\u96c6\uff0c\u6700\u4f73\u56e2\u961f\u5728\u6700\u7ec8\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e860.5102\u7684Jaccard\u5206\u6570\u548c0.6267\u7684F1\u5206\u6570\u3002", "motivation": "\u51fa\u9662\u836f\u7269\u63a8\u8350\u5bf9\u4e8e\u786e\u4fdd\u6cbb\u7597\u8fde\u7eed\u6027\u3001\u9884\u9632\u518d\u5165\u9662\u548c\u6539\u5584\u6162\u6027\u4ee3\u8c22\u75be\u75c5\u60a3\u8005\u7684\u957f\u671f\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u8be5\u4efb\u52a1\u65e8\u5728\u5f00\u53d1\u57fa\u4e8e\u4e2d\u56fd\u771f\u5b9e\u7535\u5b50\u75c5\u5386\u6570\u636e\u7684\u81ea\u52a8\u836f\u7269\u63a8\u8350\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86CDrugRed\u6570\u636e\u96c6\uff0c\u5305\u542b5,894\u6761\u53bb\u6807\u8bc6\u5316\u7684\u4f4f\u9662\u8bb0\u5f55\uff0c\u6d89\u53ca3,190\u540d\u4e2d\u56fd\u60a3\u8005\u3002\u4efb\u52a1\u5177\u6709\u591a\u6807\u7b7e\u63a8\u8350\u3001\u5f02\u8d28\u4e34\u5e8a\u6587\u672c\u548c\u60a3\u8005\u7279\u5f02\u6027\u6cbb\u7597\u8ba1\u5212\u7b49\u6311\u6218\u3002\u4f7f\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u7cfb\u7edf\u8fdb\u884c\u836f\u7269\u63a8\u8350\u3002", "result": "\u5171\u6709526\u4e2a\u56e2\u961f\u6ce8\u518c\uff0c167\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86Phase A\u7684\u6709\u6548\u7ed3\u679c\uff0c95\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86Phase B\u7684\u6709\u6548\u7ed3\u679c\u3002\u6700\u4f73\u56e2\u961f\u5728\u6700\u7ec8\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e860.5102\u7684Jaccard\u5206\u6570\u548c0.6267\u7684F1\u5206\u6570\u3002", "conclusion": "\u7ed3\u679c\u663e\u793a\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u7cfb\u7edf\u5728\u4e2d\u56fd\u7535\u5b50\u75c5\u5386\u836f\u7269\u63a8\u8350\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u51f8\u663e\u4e86\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e8e\u836f\u7269\u63a8\u8350\u9886\u57df\u4ecd\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2511.05758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05758", "abs": "https://arxiv.org/abs/2511.05758", "authors": ["Anirudh Satheesh", "Sooraj Sathish", "Swetha Ganesh", "Keenan Powell", "Vaneet Aggarwal"], "title": "Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs", "comment": null, "summary": "In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \\(\u03b5\\)-feasibility and \\(\u03b5\\)-optimality, and we establish a sample complexities of \\(\\tilde{O}\\left(\u03b5^{-4}\\right)\\) and \\(\\tilde{O}\\left(\u03b5^{-6}\\right)\\) with and without slackness assumption, which is comparable to the discounted setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e73\u5747\u6210\u672c\u9c81\u68d2\u7ea6\u675fMDP\u7684actor-critic\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5bf9\u5076\u6027\u7f3a\u5931\u548c\u9c81\u68d2\u8d1d\u5c14\u66fc\u7b97\u5b50\u975e\u6536\u7f29\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u03b5-\u53ef\u884c\u6027\u548c\u03b5-\u6700\u4f18\u6027\u3002", "motivation": "\u5728\u9c81\u68d2\u7ea6\u675f\u5e73\u5747\u6210\u672cMDP\u4e2d\uff0c\u5f3a\u5bf9\u5076\u6027\u7684\u7f3a\u5931\u963b\u788d\u4e86\u6807\u51c6\u5bf9\u5076\u65b9\u6cd5\u7684\u5e94\u7528\uff0c\u4e14\u5e73\u5747\u6210\u672c\u8bbe\u7f6e\u4e0b\u9c81\u68d2\u8d1d\u5c14\u66fc\u7b97\u5b50\u4e0d\u662f\u6536\u7f29\u7b97\u5b50\uff0c\u8fd9\u5e26\u6765\u4e86\u989d\u5916\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdactor-critic\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9002\u5f53\u7684\u66f4\u65b0\u89c4\u5219\u6765\u5904\u7406\u9c81\u68d2\u7ea6\u675f\u548c\u5e73\u5747\u6210\u672c\u7279\u6027\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u03b5-\u53ef\u884c\u6027\u548c\u03b5-\u6700\u4f18\u6027\uff0c\u5728\u6709\u65e0\u677e\u5f1b\u5047\u8bbe\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u5206\u522b\u4e3a\u00d5(\u03b5\u207b\u2074)\u548c\u00d5(\u03b5\u207b\u2076)\uff0c\u4e0e\u6298\u6263\u8bbe\u7f6e\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u9c81\u68d2\u7ea6\u675f\u5e73\u5747\u6210\u672cMDP\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8fd9\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.05865", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05865", "abs": "https://arxiv.org/abs/2511.05865", "authors": ["Viet Nguyen", "Vishal M. Patel"], "title": "CGCE: Classifier-Guided Concept Erasure in Generative Models", "comment": "24 pages, 15 figures", "summary": "Recent advancements in large-scale generative models have enabled the creation of high-quality images and videos, but have also raised significant safety concerns regarding the generation of unsafe content. To mitigate this, concept erasure methods have been developed to remove undesirable concepts from pre-trained models. However, existing methods remain vulnerable to adversarial attacks that can regenerate the erased content. Moreover, achieving robust erasure often degrades the model's generative quality for safe, unrelated concepts, creating a difficult trade-off between safety and performance. To address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE), an efficient plug-and-play framework that provides robust concept erasure for diverse generative models without altering their original weights. CGCE uses a lightweight classifier operating on text embeddings to first detect and then refine prompts containing undesired concepts. This approach is highly scalable, allowing for multi-concept erasure by aggregating guidance from several classifiers. By modifying only unsafe embeddings at inference time, our method prevents harmful content generation while preserving the model's original quality on benign prompts. Extensive experiments show that CGCE achieves state-of-the-art robustness against a wide range of red-teaming attacks. Our approach also maintains high generative utility, demonstrating a superior balance between safety and performance. We showcase the versatility of CGCE through its successful application to various modern T2I and T2V models, establishing it as a practical and effective solution for safe generative AI.", "AI": {"tldr": "CGCE\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u68c0\u6d4b\u548c\u4f18\u5316\u5305\u542b\u4e0d\u826f\u6982\u5ff5\u7684\u6587\u672c\u5d4c\u5165\uff0c\u5b9e\u73b0\u591a\u6982\u5ff5\u64e6\u9664\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u4e14\u9c81\u68d2\u64e6\u9664\u5f80\u5f80\u4f1a\u964d\u4f4e\u6a21\u578b\u5bf9\u5b89\u5168\u6982\u5ff5\u7684\u751f\u6210\u8d28\u91cf\uff0c\u9700\u8981\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u505a\u51fa\u56f0\u96be\u6743\u8861\u3002", "method": "\u4f7f\u7528\u5728\u6587\u672c\u5d4c\u5165\u4e0a\u64cd\u4f5c\u7684\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u9996\u5148\u68c0\u6d4b\u7136\u540e\u4f18\u5316\u5305\u542b\u4e0d\u826f\u6982\u5ff5\u7684\u63d0\u793a\u8bcd\uff0c\u4ec5\u4fee\u6539\u63a8\u7406\u65f6\u7684\u4e0d\u5b89\u5168\u5d4c\u5165\uff0c\u4e0d\u6539\u53d8\u539f\u59cb\u6a21\u578b\u6743\u91cd\u3002", "result": "CGCE\u5728\u5e7f\u6cdb\u7684\u7ea2\u961f\u653b\u51fb\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u751f\u6210\u6548\u7528\uff0c\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\u3002", "conclusion": "CGCE\u662f\u4e00\u4e2a\u5b9e\u7528\u6709\u6548\u7684\u5b89\u5168\u751f\u6210AI\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5404\u79cd\u73b0\u4ee3T2I\u548cT2V\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2511.05890", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05890", "abs": "https://arxiv.org/abs/2511.05890", "authors": ["Ziqing Ma", "Chang Yang", "Zhichang Guo", "Yao Li"], "title": "Towards Frequency-Adaptive Learning for SAR Despeckling", "comment": "13 pages, 14 figures,9 tables", "summary": "Synthetic Aperture Radar (SAR) images are inherently corrupted by speckle noise, limiting their utility in high-precision applications. While deep learning methods have shown promise in SAR despeckling, most methods employ a single unified network to process the entire image, failing to account for the distinct speckle statistics associated with different spatial physical characteristics. It often leads to artifacts, blurred edges, and texture distortion. To address these issues, we propose SAR-FAH, a frequency-adaptive heterogeneous despeckling model based on a divide-and-conquer architecture. First, wavelet decomposition is used to separate the image into frequency sub-bands carrying different intrinsic characteristics. Inspired by their differing noise characteristics, we design specialized sub-networks for different frequency components. The tailored approach leverages statistical variations across frequencies, improving edge and texture preservation while suppressing noise. Specifically, for the low-frequency part, denoising is formulated as a continuous dynamic system via neural ordinary differential equations, ensuring structural fidelity and sufficient smoothness that prevents artifacts. For high-frequency sub-bands rich in edges and textures, we introduce an enhanced U-Net with deformable convolutions for noise suppression and enhanced features. Extensive experiments on synthetic and real SAR images validate the superior performance of the proposed model in noise removal and structural preservation.", "AI": {"tldr": "\u63d0\u51faSAR-FAH\u6a21\u578b\uff0c\u901a\u8fc7\u9891\u7387\u81ea\u9002\u5e94\u5f02\u6784\u53bb\u6591\u65b9\u6cd5\uff0c\u9488\u5bf9SAR\u56fe\u50cf\u4e0d\u540c\u9891\u7387\u5b50\u5e26\u8bbe\u8ba1\u4e13\u95e8\u5b50\u7f51\u7edc\uff0c\u6709\u6548\u6291\u5236\u6591\u70b9\u566a\u58f0\u5e76\u4fdd\u6301\u8fb9\u7f18\u7eb9\u7406\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7f51\u7edc\u5904\u7406\u6574\u4e2aSAR\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7a7a\u95f4\u7269\u7406\u7279\u5f81\u5bf9\u5e94\u7684\u6591\u70b9\u7edf\u8ba1\u7279\u6027\u5dee\u5f02\uff0c\u5bfc\u81f4\u4f2a\u5f71\u3001\u8fb9\u7f18\u6a21\u7cca\u548c\u7eb9\u7406\u5931\u771f\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u5206\u89e3\u5c06\u56fe\u50cf\u5206\u79bb\u4e3a\u4e0d\u540c\u9891\u7387\u5b50\u5e26\uff1b\u4e3a\u4f4e\u9891\u90e8\u5206\u8bbe\u8ba1\u57fa\u4e8e\u795e\u7ecfODE\u7684\u52a8\u6001\u7cfb\u7edf\u53bb\u566a\uff1b\u4e3a\u9ad8\u9891\u5b50\u5e26\u8bbe\u8ba1\u589e\u5f3a\u578bU-Net\u7ed3\u5408\u53ef\u53d8\u5f62\u5377\u79ef\u8fdb\u884c\u566a\u58f0\u6291\u5236\u548c\u7279\u5f81\u589e\u5f3a\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eSAR\u56fe\u50cf\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6a21\u578b\u5728\u566a\u58f0\u53bb\u9664\u548c\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "SAR-FAH\u6a21\u578b\u901a\u8fc7\u9891\u7387\u81ea\u9002\u5e94\u5f02\u6784\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86SAR\u56fe\u50cf\u53bb\u6591\u4e2d\u7684\u4f2a\u5f71\u548c\u7eb9\u7406\u5931\u771f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u566a\u58f0\u6291\u5236\u548c\u7ed3\u6784\u4fdd\u6301\u6548\u679c\u3002"}}
{"id": "2511.07407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07407", "abs": "https://arxiv.org/abs/2511.07407", "authors": ["Zhengjie Xu", "Ye Li", "Kwan-yee Lin", "Stella X. Yu"], "title": "Unified Humanoid Fall-Safety Policy from a Few Demonstrations", "comment": null, "summary": "Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7b56\u7565\uff0c\u5c06\u9632\u8dcc\u5012\u3001\u51b2\u51fb\u7f13\u89e3\u548c\u5feb\u901f\u6062\u590d\u6574\u5408\u5230\u4e00\u4e2a\u7b56\u7565\u4e2d\uff0c\u901a\u8fc7\u878d\u5408\u7a00\u758f\u4eba\u7c7b\u6f14\u793a\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6269\u6563\u8bb0\u5fc6\u6765\u5b9e\u73b0", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8dcc\u5012\u65f6\u7684\u7efc\u5408\u5b89\u5168\u95ee\u9898\uff0c\u8d85\u8d8a\u5355\u7eaf\u4fdd\u6301\u5e73\u8861\uff0c\u4f7f\u6574\u4e2a\u8dcc\u5012-\u6062\u590d\u8fc7\u7a0b\u5b89\u5168\u81ea\u4e3b", "method": "\u878d\u5408\u7a00\u758f\u4eba\u7c7b\u6f14\u793a\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6269\u6563\u8bb0\u5fc6\u5b66\u4e60\u5b89\u5168\u53cd\u5e94\uff0c\u8bad\u7ec3\u7edf\u4e00\u7b56\u7565", "result": "\u5728\u4eff\u771f\u548cUnitree G1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\uff0c\u964d\u4f4e\u51b2\u51fb\u529b\uff0c\u8de8\u591a\u6837\u5e72\u6270\u5b9e\u73b0\u5feb\u901f\u6062\u590d", "conclusion": "\u8be5\u65b9\u6cd5\u6307\u5411\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u5177\u97e7\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba"}}
{"id": "2511.05960", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05960", "abs": "https://arxiv.org/abs/2511.05960", "authors": ["Enrico Manzini", "Thomas Gonzalez Saito", "Joan Escudero", "Ana G\u00e9nova", "Cristina Caso", "Tomas Perez-Porcuna", "Alexandre Perera-Lluna"], "title": "Deep Survival Analysis of Longitudinal EHR Data for Joint Prediction of Hospitalization and Death in COPD Patients", "comment": null, "summary": "Patients with chronic obstructive pulmonary disease (COPD) have an increased risk of hospitalizations, strongly associated with decreased survival, yet predicting the timing of these events remains challenging and has received limited attention in the literature. In this study, we performed survival analysis to predict hospitalization and death in COPD patients using longitudinal electronic health records (EHRs), comparing statistical models, machine learning (ML), and deep learning (DL) approaches. We analyzed data from more than 150k patients from the SIDIAP database in Catalonia, Spain, from 2013 to 2017, modeling hospitalization as a first event and death as a semi-competing terminal event. Multiple models were evaluated, including Cox proportional hazards, SurvivalBoost, DeepPseudo, SurvTRACE, Dynamic Deep-Hit, and Deep Recurrent Survival Machine. Results showed that DL models utilizing recurrent architectures outperformed both ML and linear approaches in concordance and time-dependent AUC, especially for hospitalization, which proved to be the harder event to predict. This study is, to our knowledge, the first to apply deep survival analysis on longitudinal EHR data to jointly predict multiple time-to-event outcomes in COPD patients, highlighting the potential of DL approaches to capture temporal patterns and improve risk stratification.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u6bd4\u8f83\u7edf\u8ba1\u6a21\u578b\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4bCOPD\u60a3\u8005\u7684\u4f4f\u9662\u548c\u6b7b\u4ea1\u98ce\u9669\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "COPD\u60a3\u8005\u4f4f\u9662\u98ce\u9669\u589e\u52a0\u4e14\u4e0e\u751f\u5b58\u7387\u4e0b\u964d\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u9884\u6d4b\u8fd9\u4e9b\u4e8b\u4ef6\u53d1\u751f\u65f6\u95f4\u5177\u6709\u6311\u6218\u6027\uff0c\u6587\u732e\u4e2d\u5bf9\u6b64\u5173\u6ce8\u6709\u9650\u3002", "method": "\u4f7f\u7528\u897f\u73ed\u7259\u52a0\u6cf0\u7f57\u5c3c\u4e9aSIDIAP\u6570\u636e\u5e93\u4e2d2013-2017\u5e74\u8d85\u8fc715\u4e07\u60a3\u8005\u7684\u6570\u636e\uff0c\u5c06\u4f4f\u9662\u5efa\u6a21\u4e3a\u9996\u6b21\u4e8b\u4ef6\uff0c\u6b7b\u4ea1\u5efa\u6a21\u4e3a\u534a\u7ade\u4e89\u6027\u7ec8\u70b9\u4e8b\u4ef6\uff0c\u6bd4\u8f83\u4e86Cox\u6bd4\u4f8b\u98ce\u9669\u3001SurvivalBoost\u3001DeepPseudo\u3001SurvTRACE\u3001Dynamic Deep-Hit\u548cDeep Recurrent Survival Machine\u7b49\u591a\u79cd\u6a21\u578b\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7279\u522b\u662f\u5faa\u73af\u67b6\u6784\u6a21\u578b\u5728\u4e00\u81f4\u6027\u6307\u6570\u548c\u65f6\u95f4\u4f9d\u8d56\u6027AUC\u65b9\u9762\u4f18\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u7ebf\u6027\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5bf9\u66f4\u96be\u9884\u6d4b\u7684\u4f4f\u9662\u4e8b\u4ef6\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728COPD\u60a3\u8005\u7eb5\u5411EHR\u6570\u636e\u4e0a\u5e94\u7528\u6df1\u5ea6\u751f\u5b58\u5206\u6790\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u65f6\u95f4\u5230\u4e8b\u4ef6\u7ed3\u679c\u7684\u7814\u7a76\uff0c\u7a81\u663e\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u6a21\u5f0f\u548c\u6539\u8fdb\u98ce\u9669\u5206\u5c42\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.05968", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05968", "abs": "https://arxiv.org/abs/2511.05968", "authors": ["Nagur Shareef Shaik", "Teja Krishna Cherukuri", "Adnan Masood", "Dong Hye Ye"], "title": "DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities", "comment": "Accepted for Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), Main Technical Track", "summary": "The integration of medical images with clinical context is essential for generating accurate and clinically interpretable radiology reports. However, current automated methods often rely on resource-heavy Large Language Models (LLMs) or static knowledge graphs and struggle with two fundamental challenges in real-world clinical data: (1) missing modalities, such as incomplete clinical context , and (2) feature entanglement, where mixed modality-specific and shared information leads to suboptimal fusion and clinically unfaithful hallucinated findings. To address these challenges, we propose the DiA-gnostic VLVAE, which achieves robust radiology reporting through Disentangled Alignment. Our framework is designed to be resilient to missing modalities by disentangling shared and modality-specific features using a Mixture-of-Experts (MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained optimization objective enforces orthogonality and alignment between these latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder then uses these disentangled representations to generate reports efficiently. On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4 scores of 0.266 and 0.134, respectively. Experimental results show that the proposed method significantly outperforms state-of-the-art models.", "AI": {"tldr": "\u63d0\u51faDiA-gnostic VLVAE\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5bf9\u9f50\u5b9e\u73b0\u7a33\u5065\u7684\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u89e3\u51b3\u4e34\u5e8a\u6570\u636e\u4e2d\u6a21\u6001\u7f3a\u5931\u548c\u7279\u5f81\u7ea0\u7f20\u95ee\u9898", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u5927\u8bed\u8a00\u6a21\u578b\u6216\u9759\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u548c\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u6b21\u4f18\u878d\u5408\u548c\u4e34\u5e8a\u4e0d\u5fe0\u5b9e\u7684\u5e7b\u89c9\u53d1\u73b0", "method": "\u4f7f\u7528\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u7684\u89c6\u89c9\u8bed\u8a00\u53d8\u5206\u81ea\u7f16\u7801\u5668\u89e3\u8026\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u76ee\u6807\u5f3a\u5236\u6f5c\u5728\u8868\u793a\u7684\u6b63\u4ea4\u6027\u548c\u5bf9\u9f50\uff0c\u6700\u540e\u4f7f\u7528\u7d27\u51d1\u7684LLaMA-X\u89e3\u7801\u5668\u751f\u6210\u62a5\u544a", "result": "\u5728IU X-Ray\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.266\u548c0.134\u7684BLEU@4\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b", "conclusion": "DiA\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u548c\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210"}}
{"id": "2511.06032", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06032", "abs": "https://arxiv.org/abs/2511.06032", "authors": ["Wang-Tao Zhou", "Zhao Kang", "Ke Yan", "Ling Tian"], "title": "ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes", "comment": "Accepted to AAAI'26 Poster", "summary": "Marked Temporal Point Processes (MTPPs) provide a principled framework for modeling asynchronous event sequences by conditioning on the history of past events. However, most existing MTPP models rely on channel-mixing strategies that encode information from different event types into a single, fixed-size latent representation. This entanglement can obscure type-specific dynamics, leading to performance degradation and increased risk of overfitting. In this work, we introduce ITPP, a novel channel-independent architecture for MTPP modeling that decouples event type information using an encoder-decoder framework with an ODE-based backbone. Central to ITPP is a type-aware inverted self-attention mechanism, designed to explicitly model inter-channel correlations among heterogeneous event types. This architecture enhances effectiveness and robustness while reducing overfitting. Comprehensive experiments on multiple real-world and synthetic datasets demonstrate that ITPP consistently outperforms state-of-the-art MTPP models in both predictive accuracy and generalization.", "AI": {"tldr": "ITPP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u901a\u9053\u72ec\u7acbMTPP\u67b6\u6784\uff0c\u901a\u8fc7ODE\u9aa8\u5e72\u7f51\u7edc\u548c\u7c7b\u578b\u611f\u77e5\u5012\u7f6e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u8026\u4e8b\u4ef6\u7c7b\u578b\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MTPP\u6a21\u578b\u4f7f\u7528\u901a\u9053\u6df7\u5408\u7b56\u7565\u5c06\u4e0d\u540c\u4e8b\u4ef6\u7c7b\u578b\u4fe1\u606f\u7f16\u7801\u5230\u5355\u4e00\u6f5c\u5728\u8868\u793a\u4e2d\uff0c\u8fd9\u79cd\u7ea0\u7f20\u4f1a\u6a21\u7cca\u7c7b\u578b\u7279\u5b9a\u52a8\u6001\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u62df\u5408\u98ce\u9669\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u901a\u9053\u72ec\u7acb\u67b6\u6784ITPP\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u548cODE\u9aa8\u5e72\u7f51\u7edc\uff0c\u6838\u5fc3\u662f\u7c7b\u578b\u611f\u77e5\u5012\u7f6e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u5f0f\u5efa\u6a21\u5f02\u6784\u4e8b\u4ef6\u7c7b\u578b\u95f4\u7684\u901a\u9053\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cITPP\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684MTPP\u6a21\u578b\u3002", "conclusion": "ITPP\u901a\u8fc7\u89e3\u8026\u4e8b\u4ef6\u7c7b\u578b\u4fe1\u606f\u5e76\u663e\u5f0f\u5efa\u6a21\u901a\u9053\u95f4\u76f8\u5173\u6027\uff0c\u63d0\u9ad8\u4e86MTPP\u5efa\u6a21\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u3002"}}
{"id": "2511.06002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06002", "abs": "https://arxiv.org/abs/2511.06002", "authors": ["Shivank Saxena", "Dhruv Srivastava", "Makarand Tapaswi"], "title": "MALeR: Improving Compositional Fidelity in Layout-Guided Generation", "comment": "ACM TOG Dec 2025, Siggraph Asia, Project page: https://katha-ai.github.io/projects/maler/", "summary": "Recent advances in text-to-image models have enabled a new era of creative and controllable image generation. However, generating compositional scenes with multiple subjects and attributes remains a significant challenge. To enhance user control over subject placement, several layout-guided methods have been proposed. However, these methods face numerous challenges, particularly in compositional scenes. Unintended subjects often appear outside the layouts, generated images can be out-of-distribution and contain unnatural artifacts, or attributes bleed across subjects, leading to incorrect visual outputs. In this work, we propose MALeR, a method that addresses each of these challenges. Given a text prompt and corresponding layouts, our method prevents subjects from appearing outside the given layouts while being in-distribution. Additionally, we propose a masked, attribute-aware binding mechanism that prevents attribute leakage, enabling accurate rendering of subjects with multiple attributes, even in complex compositional scenes. Qualitative and quantitative evaluation demonstrates that our method achieves superior performance in compositional accuracy, generation consistency, and attribute binding compared to previous work. MALeR is particularly adept at generating images of scenes with multiple subjects and multiple attributes per subject.", "AI": {"tldr": "MALeR\u662f\u4e00\u4e2a\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u591a\u4e3b\u4f53\u7ec4\u5408\u573a\u666f\u5e03\u5c40\u63a7\u5236\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u9632\u6b62\u4e3b\u4f53\u51fa\u73b0\u5728\u5e03\u5c40\u5916\u3001\u907f\u514d\u5c5e\u6027\u6cc4\u9732\uff0c\u5e76\u751f\u6210\u5206\u5e03\u5185\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684\u5e03\u5c40\u5f15\u5bfc\u65b9\u6cd5\u5728\u591a\u4e3b\u4f53\u7ec4\u5408\u573a\u666f\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u4e3b\u4f53\u51fa\u73b0\u5728\u5e03\u5c40\u5916\u3001\u751f\u6210\u56fe\u50cf\u8d85\u51fa\u5206\u5e03\u8303\u56f4\u5305\u542b\u4e0d\u81ea\u7136\u4f2a\u5f71\u3001\u5c5e\u6027\u5728\u4e3b\u4f53\u95f4\u6cc4\u9732\u5bfc\u81f4\u89c6\u89c9\u8f93\u51fa\u9519\u8bef\u3002", "method": "\u63d0\u51faMALeR\u65b9\u6cd5\uff0c\u7ed9\u5b9a\u6587\u672c\u63d0\u793a\u548c\u5bf9\u5e94\u5e03\u5c40\uff0c\u9632\u6b62\u4e3b\u4f53\u51fa\u73b0\u5728\u7ed9\u5b9a\u5e03\u5c40\u5916\u5e76\u4fdd\u6301\u5206\u5e03\u5185\u751f\u6210\uff1b\u63d0\u51fa\u63a9\u7801\u5c5e\u6027\u611f\u77e5\u7ed1\u5b9a\u673a\u5236\u9632\u6b62\u5c5e\u6027\u6cc4\u9732\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0cMALeR\u5728\u7ec4\u5408\u51c6\u786e\u6027\u3001\u751f\u6210\u4e00\u81f4\u6027\u548c\u5c5e\u6027\u7ed1\u5b9a\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u7279\u522b\u64c5\u957f\u751f\u6210\u5177\u6709\u591a\u4e2a\u4e3b\u4f53\u548c\u6bcf\u4e2a\u4e3b\u4f53\u591a\u4e2a\u5c5e\u6027\u7684\u573a\u666f\u56fe\u50cf\u3002", "conclusion": "MALeR\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u7ec4\u5408\u573a\u666f\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u5e03\u5c40\u63a7\u5236\u3001\u5c5e\u6027\u7ed1\u5b9a\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.06942", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.06942", "abs": "https://arxiv.org/abs/2511.06942", "authors": ["Fangqi Dai", "Xingjian Jiang", "Zizhuang Deng"], "title": "HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection", "comment": "9 pages, 3 figures, accepted by AAAI'26", "summary": "To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.", "AI": {"tldr": "\u63d0\u51fa\u4e86HLPD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u5bf9\u9f50\u4f18\u5316\u4f7f\u8bc4\u5206\u6a21\u578b\u66f4\u504f\u597d\u4eba\u7c7b\u5199\u4f5c\u98ce\u683c\uff0c\u4ece\u800c\u5728\u5bf9\u6297\u6027\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u66f4\u6709\u6548\u68c0\u6d4b\u673a\u5668\u4fee\u8ba2\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u5148\u8fdbLLM\u8f93\u51fa\u6216\u7ecf\u8fc7\u5bf9\u6297\u6027\u591a\u4efb\u52a1\u673a\u5668\u4fee\u8ba2\u7684\u6587\u672c\uff0c\u7279\u522b\u662f\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u751f\u6210\u6a21\u578b\u672a\u77e5\u65f6\u3002", "method": "\u57fa\u4e8e\u4eba\u7c7b\u5199\u4f5c\u5177\u6709\u72ec\u7279\u98ce\u683c\u6a21\u5f0f\u7684\u5047\u8bbe\uff0c\u63d0\u51faHLPD\u65b9\u6cd5\uff0c\u4f7f\u7528HLPO\u5956\u52b1\u5bf9\u9f50\u8fc7\u7a0b\u5c06\u8bc4\u5206\u6a21\u578b\u7684\u6807\u8bb0\u5206\u5e03\u5411\u4eba\u7c7b\u5199\u4f5c\u98ce\u683c\u504f\u79fb\u3002", "result": "\u5728\u68c0\u6d4bGPT\u7cfb\u5217\u6a21\u578b\u4fee\u8ba2\u6587\u672c\u65f6\uff0cHLPD\u76f8\u6bd4ImBD\u76f8\u5bf9\u63d0\u534715.11% AUROC\uff0c\u6bd4Fast-DetectGPT\u63d0\u534745.56%\uff1b\u5728\u5148\u8fdbLLM\u751f\u6210\u6587\u672c\u4e0a\uff0cHLPD\u5e73\u5747AUROC\u6700\u9ad8\uff0c\u8d85\u8fc7ImBD 5.53%\uff0c\u8d85\u8fc7Fast-DetectGPT 34.14%\u3002", "conclusion": "HLPD\u901a\u8fc7\u504f\u597d\u4eba\u7c7b\u5199\u4f5c\u98ce\u683c\u7684\u65b9\u6cd5\uff0c\u5728\u5bf9\u6297\u6027\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4fee\u8ba2\u6587\u672c\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.06083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06083", "abs": "https://arxiv.org/abs/2511.06083", "authors": ["Shailesh Garg", "Souvik Chakraborty"], "title": "Event-driven physics-informed operator learning for reliability analysis", "comment": null, "summary": "Reliability analysis of engineering systems under uncertainty poses significant computational challenges, particularly for problems involving high-dimensional stochastic inputs, nonlinear system responses, and multiphysics couplings. Traditional surrogate modeling approaches often incur high energy consumption, which severely limits their scalability and deployability in resource-constrained environments. We introduce NeuroPOL, \\textit{the first neuroscience-inspired physics-informed operator learning framework} for reliability analysis. NeuroPOL incorporates Variable Spiking Neurons into a physics-informed operator architecture, replacing continuous activations with event-driven spiking dynamics. This innovation promotes sparse communication, significantly reduces computational load, and enables an energy-efficient surrogate model. The proposed framework lowers both computational and power demands, supporting real-time reliability assessment and deployment on edge devices and digital twins. By embedding governing physical laws into operator learning, NeuroPOL builds physics-consistent surrogates capable of accurate uncertainty propagation and efficient failure probability estimation, even for high-dimensional problems. We evaluate NeuroPOL on five canonical benchmarks, the Burgers equation, Nagumo equation, two-dimensional Poisson equation, two-dimensional Darcy equation, and incompressible Navier-Stokes equation with energy coupling. Results show that NeuroPOL achieves reliability measures comparable to standard physics-informed operators, while introducing significant communication sparsity, enabling scalable, distributed, and energy-efficient deployment.", "AI": {"tldr": "NeuroPOL\u662f\u9996\u4e2a\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u9760\u6027\u5206\u6790\u3002\u5b83\u901a\u8fc7\u5f15\u5165\u53ef\u53d8\u8109\u51b2\u795e\u7ecf\u5143\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u8109\u51b2\u52a8\u529b\u5b66\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u548c\u80fd\u8017\uff0c\u652f\u6301\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7406\u6a21\u578b\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5de5\u7a0b\u7cfb\u7edf\u53ef\u9760\u6027\u5206\u6790\u4e2d\u9762\u4e34\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u80fd\u529b\u3002", "method": "\u5c06\u53ef\u53d8\u8109\u51b2\u795e\u7ecf\u5143\u96c6\u6210\u5230\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u67b6\u6784\u4e2d\uff0c\u7528\u4e8b\u4ef6\u9a71\u52a8\u7684\u8109\u51b2\u52a8\u529b\u5b66\u66ff\u4ee3\u8fde\u7eed\u6fc0\u6d3b\u51fd\u6570\uff0c\u5b9e\u73b0\u7a00\u758f\u901a\u4fe1\u548c\u80fd\u91cf\u9ad8\u6548\u7684\u4ee3\u7406\u5efa\u6a21\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeuroPOL\u5b9e\u73b0\u4e86\u4e0e\u6807\u51c6\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u76f8\u5f53\u7684\u53ef\u9760\u6027\u5ea6\u91cf\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u663e\u8457\u7684\u901a\u4fe1\u7a00\u758f\u6027\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u5206\u5e03\u5f0f\u548c\u80fd\u91cf\u9ad8\u6548\u7684\u90e8\u7f72\u3002", "conclusion": "NeuroPOL\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u80fd\u8017\u9700\u6c42\uff0c\u4e3a\u9ad8\u7ef4\u95ee\u9898\u7684\u5b9e\u65f6\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07044", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07044", "abs": "https://arxiv.org/abs/2511.07044", "authors": ["Mihael Arcan", "David-Paul Niland"], "title": "Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data", "comment": null, "summary": "Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86LLM\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u57fa\u4e8etransformer\u7684\u6a21\u578b\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u80fd\u6709\u6548\u63d0\u5347\u7126\u8651\u3001\u6291\u90c1\u548c\u538b\u529b\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u8d85\u8fc7\u4e94\u5206\u4e4b\u4e00\u6210\u5e74\u4eba\u53d7\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u5f71\u54cd\uff0c\u4f46\u6587\u672c\u4e2d\u75c7\u72b6\u8868\u8fbe\u7684\u5fae\u5999\u6027\u548c\u591a\u6837\u6027\u4f7f\u5f97\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528DAIC-WOZ\u4e34\u5e8a\u8bbf\u8c08\u6570\u636e\u96c6\uff0c\u5bf9Llama\u3001GPT\u7b49LLM\u4ee5\u53caBERT\u3001XLNet\u3001Distil-RoBERTa\u7b49\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e94\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "Distil-RoBERTa\u5728GAD-2\u4efb\u52a1\u4e2d\u83b7\u5f97\u6700\u9ad8F1\u5206\u6570(0.883)\uff0cXLNet\u5728PHQ\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73(F1\u8fbe0.891)\uff0c\u96f6\u6837\u672c\u5408\u6210\u65b9\u6cd5\u5728\u538b\u529b\u68c0\u6d4b\u4e2d\u8fbe\u5230F1 0.884\u548cROC AUC 0.886\u3002", "conclusion": "transformer\u6a21\u578b\u548c\u5408\u6210\u6570\u636e\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u5316\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\uff0c\u4f46\u9700\u8981\u8c28\u614e\u6821\u51c6\u4ee5\u907f\u514d\u7cbe\u5ea6\u635f\u5931\u3002"}}
{"id": "2511.07162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07162", "abs": "https://arxiv.org/abs/2511.07162", "authors": ["Lynn Greschner", "Meike Bauer", "Sabine Weber", "Roman Klinger"], "title": "Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?", "comment": null, "summary": "The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u8bc4\u4ef7\u7406\u8bba\u5728\u8bba\u8bc1\u60c5\u611f\u5206\u6790\u4e2d\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u8bc4\u4ef7\u4fe1\u606f\u6bd4\u5206\u7c7b\u60c5\u611f\u4fe1\u606f\u66f4\u80fd\u6709\u6548\u9884\u6d4b\u8bba\u8bc1\u7684\u8bf4\u670d\u529b\u3002", "motivation": "\u8bba\u8bc1\u7684\u8bf4\u670d\u529b\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u903b\u8f91\u7ed3\u6784\u548c\u8bba\u8bc1\u8005\u4fe1\u8a89\uff0c\u8fd8\u53d6\u51b3\u4e8e\u63a5\u6536\u8005\u7684\u60c5\u611f\u53cd\u5e94\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u60c5\u611f\u7684\u5f3a\u5ea6\u548c\u5206\u7c7b\uff0c\u4f46\u5ffd\u7565\u4e86\u60c5\u611f\u7684\u4e3b\u89c2\u6027\u7279\u5f81\u3002", "method": "\u57fa\u4e8eContArgA\u8bed\u6599\u5e93\u7684\u6807\u6ce8\uff0c\u8fdb\u884c\u96f6\u6837\u672c\u63d0\u793a\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u9ec4\u91d1\u6807\u6ce8\u548c\u9884\u6d4b\u7684\u60c5\u611f\u53ca\u8bc4\u4ef7\u5bf9\u4e3b\u89c2\u8bf4\u670d\u529b\u6807\u7b7e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "result": "\u5206\u7c7b\u60c5\u611f\u4fe1\u606f\u786e\u5b9e\u80fd\u6539\u5584\u8bf4\u670d\u529b\u9884\u6d4b\uff0c\u4f46\u8bc4\u4ef7\u4fe1\u606f\u5e26\u6765\u7684\u6539\u5584\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u60c5\u611f\u6a21\u578b\u5728\u8bf4\u670d\u529b\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u8bc4\u4ef7\u7406\u8bba\u7684\u4f18\u52bf\uff0c\u4e3a\u8ba1\u7b97\u8bba\u8bc1\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2511.06238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06238", "abs": "https://arxiv.org/abs/2511.06238", "authors": ["Ruihao Xia", "Junhong Cai", "Luziwei Leng", "Liuyi Wang", "Chengju Liu", "Ran Cheng", "Yang Tang", "Pan Zhou"], "title": "Temporal-Guided Visual Foundation Models for Event-Based Vision", "comment": null, "summary": "Event cameras offer unique advantages for vision tasks in challenging environments, yet processing asynchronous event streams remains an open challenge. While existing methods rely on specialized architectures or resource-intensive training, the potential of leveraging modern Visual Foundation Models (VFMs) pretrained on image data remains under-explored for event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a novel framework that integrates VFMs with our temporal context fusion block seamlessly to bridge this gap. Our temporal block introduces three key components: (1) Long-Range Temporal Attention to model global temporal dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal features. By retraining event-to-video models on real-world data and leveraging transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while harnessing pretrained representations. Experiments demonstrate SoTA performance across semantic segmentation, depth estimation, and object detection, with improvements of 16%, 21%, and 16% over existing methods, respectively. Overall, this work unlocks the cross-modality potential of image-based VFMs for event-based vision with temporal reasoning. Code is available at https://github.com/XiaRho/TGVFM.", "AI": {"tldr": "\u63d0\u51faTGVFM\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u65f6\u95f4\u4e0a\u4e0b\u6587\u878d\u5408\u5757\u7ed3\u5408\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5728\u8bed\u4e49\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u76ee\u6807\u68c0\u6d4b\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u5904\u7406\u5f02\u6b65\u4e8b\u4ef6\u6d41\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u95e8\u67b6\u6784\u6216\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u800c\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u4e8b\u4ef6\u89c6\u89c9\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "TGVFM\u6846\u67b6\u5305\u542b\u65f6\u95f4\u4e0a\u4e0b\u6587\u878d\u5408\u5757\uff0c\u5177\u6709\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u957f\u7a0b\u65f6\u95f4\u6ce8\u610f\u529b\u5efa\u6a21\u5168\u5c40\u65f6\u95f4\u4f9d\u8d56\u3001\u53cc\u65f6\u7a7a\u6ce8\u610f\u529b\u8fdb\u884c\u591a\u5c3a\u5ea6\u5e27\u5173\u8054\u3001\u6df1\u5ea6\u7279\u5f81\u5f15\u5bfc\u673a\u5236\u878d\u5408\u8bed\u4e49-\u65f6\u95f4\u7279\u5f81\u3002\u901a\u8fc7\u91cd\u65b0\u8bad\u7ec3\u4e8b\u4ef6\u5230\u89c6\u9891\u6a21\u578b\u5e76\u5229\u7528\u57fa\u4e8etransformer\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u8bed\u4e49\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534716%\u300121%\u548c16%\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u65f6\u95f4\u63a8\u7406\u89e3\u9501\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u4e8b\u4ef6\u89c6\u89c9\u4e2d\u7684\u8de8\u6a21\u6001\u6f5c\u529b\u3002"}}
{"id": "2511.06237", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06237", "abs": "https://arxiv.org/abs/2511.06237", "authors": ["Haeyong Kang"], "title": "Mixtures of SubExperts for Large Language Continual Learning", "comment": null, "summary": "Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \\textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.", "AI": {"tldr": "\u63d0\u51faMoSEs\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u5b50\u4e13\u5bb6\u548c\u4efb\u52a1\u7279\u5b9a\u8def\u7531\u673a\u5236\u89e3\u51b3LLMs\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u7ebf\u6027\u589e\u957f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u4e24\u96be\uff1a\u91cd\u7528\u53c2\u6570\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5206\u914d\u72ec\u7acb\u53c2\u6570\u5bfc\u81f4\u6a21\u578b\u7ebf\u6027\u589e\u957f\u4e14\u65e0\u6cd5\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "\u5728transformer\u5c42\u96c6\u6210\u7a00\u758f\u5b50\u4e13\u5bb6\u6df7\u5408\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u8def\u7531\u673a\u5236\u9694\u79bb\u4fdd\u62a4\u77e5\u8bc6\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u548c\u7ec4\u5408\u5df2\u6709\u53c2\u6570\u3002", "result": "\u5728TRACE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u77e5\u8bc6\u4fdd\u7559\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8fbe\u5230SOTA\uff0c\u5927\u5e45\u8282\u7701\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "MoSEs\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u63a7\u5236\u548c\u53c2\u6570\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u589e\u957f\u548c\u77e5\u8bc6\u8fc1\u79fb\u3002"}}
{"id": "2511.07296", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07296", "abs": "https://arxiv.org/abs/2511.07296", "authors": ["Jorge Gab\u00edn", "M. Eduardo Ares", "Javier Parapar"], "title": "Who Is the Story About? Protagonist Entity Recognition in News", "comment": null, "summary": "News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e3b\u89d2\u5b9e\u4f53\u8bc6\u522b\uff08PER\uff09\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc6\u522b\u65b0\u95fb\u6545\u4e8b\u4e2d\u951a\u5b9a\u53d9\u4e8b\u5e76\u63a8\u52a8\u4e3b\u8981\u53d1\u5c55\u7684\u7ec4\u7ec7\u5b9e\u4f53\uff0c\u76f8\u6bd4\u4f20\u7edfNER\u66f4\u5173\u6ce8\u53d9\u4e8b\u91cd\u8981\u6027\u3002", "motivation": "\u4f20\u7edfNER\u5c06\u6240\u6709\u5b9e\u4f53\u63d0\u53ca\u540c\u7b49\u5bf9\u5f85\uff0c\u65e0\u6cd5\u8bc6\u522b\u771f\u6b63\u9a71\u52a8\u53d9\u4e8b\u7684\u6838\u5fc3\u7ec4\u7ec7\uff0c\u9650\u5236\u4e86\u7406\u89e3\u4e8b\u4ef6\u663e\u8457\u6027\u3001\u5f71\u54cd\u529b\u6216\u53d9\u4e8b\u7126\u5ea6\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u6bd4\u8f83LLMs\u9884\u6d4b\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684\u4e00\u81f4\u6027\uff0c\u4f7f\u7528NER\u5f15\u5bfc\u63d0\u793a\u81ea\u52a8\u6807\u6ce8\u5927\u89c4\u6a21\u65b0\u95fb\u6570\u636e\uff0c\u8bc4\u4f30LLMs\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u548c\u65e0\u660e\u786e\u5019\u9009\u6307\u5bfc\u4e0b\u7684\u4e3b\u89d2\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5efa\u7acb\u4e86\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u548c\u4eba-LLM\u4e00\u81f4\u6027\uff0c\u8bc1\u660ePER\u662f\u53ef\u884c\u4e14\u6709\u610f\u4e49\u7684\u4efb\u52a1\u6269\u5c55\uff0c\u5f15\u5bfc\u7684LLMs\u80fd\u591f\u5927\u89c4\u6a21\u8fd1\u4f3c\u4eba\u7c7b\u5bf9\u53d9\u4e8b\u91cd\u8981\u6027\u7684\u5224\u65ad\u3002", "conclusion": "PER\u662f\u9762\u5411\u53d9\u4e8b\u4e2d\u5fc3\u4fe1\u606f\u63d0\u53d6\u7684\u6709\u610f\u4e49\u6269\u5c55\uff0c\u5f15\u5bfc\u7684LLMs\u80fd\u591f\u6709\u6548\u8bc6\u522b\u65b0\u95fb\u6545\u4e8b\u4e2d\u7684\u4e3b\u89d2\u5b9e\u4f53\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.06273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06273", "abs": "https://arxiv.org/abs/2511.06273", "authors": ["Boyan Tang", "Yilong Zeng", "Xuanhao Ren", "Peng Xiao", "Yuhan Zhao", "Raymond Lee", "Jianghua Wu"], "title": "COTN: A Chaotic Oscillatory Transformer Network for Complex Volatile Systems under Extreme Conditions", "comment": "Submitted to IEEE Transactions on Neural Networks and Learning Systems", "summary": "Accurate prediction of financial and electricity markets, especially under extreme conditions, remains a significant challenge due to their intrinsic nonlinearity, rapid fluctuations, and chaotic patterns. To address these limitations, we propose the Chaotic Oscillatory Transformer Network (COTN). COTN innovatively combines a Transformer architecture with a novel Lee Oscillator activation function, processed through Max-over-Time pooling and a lambda-gating mechanism. This design is specifically tailored to effectively capture chaotic dynamics and improve responsiveness during periods of heightened volatility, where conventional activation functions (e.g., ReLU, GELU) tend to saturate. Furthermore, COTN incorporates an Autoencoder Self-Regressive (ASR) module to detect and isolate abnormal market patterns, such as sudden price spikes or crashes, thereby preventing corruption of the core prediction process and enhancing robustness. Extensive experiments across electricity spot markets and financial markets demonstrate the practical applicability and resilience of COTN. Our approach outperforms state-of-the-art deep learning models like Informer by up to 17% and traditional statistical methods like GARCH by as much as 40%. These results underscore COTN's effectiveness in navigating real-world market uncertainty and complexity, offering a powerful tool for forecasting highly volatile systems under duress.", "AI": {"tldr": "\u63d0\u51faCOTN\u6a21\u578b\uff0c\u7ed3\u5408Transformer\u67b6\u6784\u548cLee\u632f\u8361\u5668\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7Max-over-Time\u6c60\u5316\u548clambda\u95e8\u63a7\u673a\u5236\uff0c\u6709\u6548\u6355\u6349\u6df7\u6c8c\u52a8\u6001\u5e76\u63d0\u5347\u5728\u6781\u7aef\u6ce2\u52a8\u671f\u95f4\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u548c\u7535\u529b\u5e02\u573a\u7531\u4e8e\u5185\u5728\u975e\u7ebf\u6027\u3001\u5feb\u901f\u6ce2\u52a8\u548c\u6df7\u6c8c\u6a21\u5f0f\uff0c\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\u5728\u9ad8\u5ea6\u6ce2\u52a8\u671f\u95f4\u5bb9\u6613\u9971\u548c\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "COTN\u7ed3\u5408Transformer\u67b6\u6784\u4e0e\u65b0\u578bLee\u632f\u8361\u5668\u6fc0\u6d3b\u51fd\u6570\uff0c\u91c7\u7528Max-over-Time\u6c60\u5316\u548clambda\u95e8\u63a7\u673a\u5236\u3002\u8fd8\u5305\u542b\u81ea\u7f16\u7801\u5668\u81ea\u56de\u5f52\u6a21\u5757\u6765\u68c0\u6d4b\u548c\u9694\u79bb\u5f02\u5e38\u5e02\u573a\u6a21\u5f0f\u3002", "result": "\u5728\u7535\u529b\u73b0\u8d27\u5e02\u573a\u548c\u91d1\u878d\u5e02\u573a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cCOTN\u6bd4\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bInformer\u6027\u80fd\u63d0\u5347\u8fbe17%\uff0c\u6bd4\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5GARCH\u63d0\u5347\u8fbe40%\u3002", "conclusion": "COTN\u5728\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5728\u538b\u529b\u4e0b\u9884\u6d4b\u9ad8\u6ce2\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2511.06284", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.06284", "abs": "https://arxiv.org/abs/2511.06284", "authors": ["Bing Wang", "Ximing Li", "Yanjun Wang", "Changchun Li", "Lin Yuanbo Wu", "Buyu Wang", "Shengsheng Wang"], "title": "Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective", "comment": "Accepted by AAAI 2026. 13 pages, 6 figures. Code: https://github.com/wangbing1416/RETSIMD", "summary": "Multimodal Misinformation Detection (MMD) refers to the task of detecting social media posts involving misinformation, where the post often contains text and image modalities. However, by observing the MMD posts, we hold that the text modality may be much more informative than the image modality because the text generally describes the whole event/story of the current post but the image often presents partial scenes only. Our preliminary empirical results indicate that the image modality exactly contributes less to MMD. Upon this idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that each text can be divided into several segments, and each text segment describes a partial scene that can be presented by an image. Accordingly, we split the text into a sequence of segments, and feed these segments into a pre-trained text-to-image generator to augment a sequence of images. We further incorporate two auxiliary objectives concerning text-image and image-label mutual information, and further post-train the generator over an auxiliary text-to-image generation benchmark dataset. Additionally, we propose a graph structure by defining three heuristic relationships between images, and use a graph neural network to generate the fused features. Extensive empirical results validate the effectiveness of RETSIMD.", "AI": {"tldr": "\u63d0\u51faRETSIMD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5206\u5272\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6765\u589e\u5f3a\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u878d\u5408\u7279\u5f81", "motivation": "\u89c2\u5bdf\u5230\u5728\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\uff0c\u6587\u672c\u6a21\u6001\u901a\u5e38\u6bd4\u56fe\u50cf\u6a21\u6001\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u56e0\u4e3a\u6587\u672c\u63cf\u8ff0\u5b8c\u6574\u4e8b\u4ef6\u800c\u56fe\u50cf\u53ea\u5448\u73b0\u90e8\u5206\u573a\u666f", "method": "\u5c06\u6587\u672c\u5206\u5272\u6210\u591a\u4e2a\u7247\u6bb5\uff0c\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5668\u751f\u6210\u5bf9\u5e94\u56fe\u50cf\u5e8f\u5217\uff0c\u7ed3\u5408\u6587\u672c-\u56fe\u50cf\u548c\u56fe\u50cf-\u6807\u7b7e\u4e92\u4fe1\u606f\u8f85\u52a9\u76ee\u6807\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u878d\u5408\u56fe\u50cf\u7279\u5f81", "result": "\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86RETSIMD\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684RETSIMD\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u901a\u8fc7\u6587\u672c\u5206\u5272\u548c\u56fe\u50cf\u751f\u6210\u589e\u5f3a\u4e86\u68c0\u6d4b\u80fd\u529b"}}
{"id": "2511.06374", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.06374", "abs": "https://arxiv.org/abs/2511.06374", "authors": ["Mang Li", "Wei Lyu"], "title": "Adaptive Regularization for Large-Scale Sparse Feature Embedding Models", "comment": null, "summary": "The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, they have not clearly identified the fundamental cause of this phenomenon. In this work, we provide a theoretical analysis that explains why overfitting occurs in models that use large-scale sparse categorical features. Based on this analysis, we propose an adaptive regularization method to address it. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9CTR\u548cCVR\u9884\u4f30\u6a21\u578b\u4e2d\u7684\u5355\u8f6e\u8fc7\u62df\u5408\u95ee\u9898\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u8f6e\u8bad\u7ec3\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5728\u641c\u7d22\u3001\u5e7f\u544a\u548c\u63a8\u8350\u9886\u57df\uff0c\u4f9d\u8d56\u5927\u89c4\u6a21\u7a00\u758f\u7c7b\u522b\u7279\u5f81\u7684CTR\u548cCVR\u9884\u4f30\u6a21\u578b\u5728\u591a\u8f6e\u8bad\u7ec3\u65f6\u4f1a\u51fa\u73b0\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u80fd\u660e\u786e\u8fd9\u4e00\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u9996\u5148\u5bf9\u5927\u89c4\u6a21\u7a00\u758f\u7c7b\u522b\u7279\u5f81\u5bfc\u81f4\u8fc7\u62df\u5408\u7684\u539f\u56e0\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u7136\u540e\u57fa\u4e8e\u5206\u6790\u63d0\u51fa\u81ea\u9002\u5e94\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9632\u6b62\u4e86\u591a\u8f6e\u8bad\u7ec3\u65f6\u7684\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\uff0c\u8fd8\u63d0\u9ad8\u4e86\u5355\u8f6e\u8bad\u7ec3\u65f6\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5df2\u5728\u5b9e\u9645\u751f\u4ea7\u7cfb\u7edf\u4e2d\u90e8\u7f72\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc6\u522b\u4e86\u8fc7\u62df\u5408\u7684\u6839\u672c\u539f\u56e0\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u8f6e\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.06492", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06492", "abs": "https://arxiv.org/abs/2511.06492", "authors": ["Atharva Thakur", "Shruti Dhumal"], "title": "Explainable AI For Early Detection Of Sepsis", "comment": null, "summary": "Sepsis is a life-threatening condition that requires rapid detection and treatment to prevent progression to severe sepsis, septic shock, or multi-organ failure. Despite advances in medical technology, it remains a major challenge for clinicians. While recent machine learning models have shown promise in predicting sepsis onset, their black-box nature limits interpretability and clinical trust. In this study, we present an interpretable AI approach for sepsis analysis that integrates machine learning with clinical knowledge. Our method not only delivers accurate predictions of sepsis onset but also enables clinicians to understand, validate, and align model outputs with established medical expertise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684AI\u65b9\u6cd5\u7528\u4e8e\u8113\u6bd2\u75c7\u5206\u6790\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u65e2\u63d0\u4f9b\u51c6\u786e\u7684\u8113\u6bd2\u75c7\u53d1\u75c5\u9884\u6d4b\uff0c\u53c8\u8ba9\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u7406\u89e3\u548c\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\u3002", "motivation": "\u8113\u6bd2\u75c7\u662f\u4e00\u79cd\u5371\u53ca\u751f\u547d\u7684\u75be\u75c5\uff0c\u9700\u8981\u5feb\u901f\u68c0\u6d4b\u548c\u6cbb\u7597\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u8113\u6bd2\u75c7\u53d1\u75c5\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9ed1\u76d2\u6027\u8d28\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684AI\u65b9\u6cd5\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u4f7f\u6a21\u578b\u8f93\u51fa\u80fd\u591f\u4e0e\u65e2\u5b9a\u7684\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u8113\u6bd2\u75c7\u53d1\u75c5\uff0c\u8fd8\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u7406\u89e3\u3001\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u89e3\u51b3\u4e86\u8113\u6bd2\u75c7\u9884\u6d4b\u4e2d\u9ed1\u76d2\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.06475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06475", "abs": "https://arxiv.org/abs/2511.06475", "authors": ["Kyuho Lee", "Euntae Kim", "Jinwoo Choi", "Buru Chang"], "title": "NOAH: Benchmarking Narrative Prior driven Hallucination and Omission in Video Large Language Models", "comment": "18 pages, 9 figures. Preprint", "summary": "Video large language models (Video LLMs) have recently achieved strong performance on tasks such as captioning, summarization, and question answering. Many models and training methods explicitly encourage continuity across events to enhance narrative coherence. While this improves fluency, it also introduces an inductive bias that prioritizes storyline consistency over strict grounding in visual evidence. We identify this bias, which we call narrative prior, as a key driver of two errors: hallucinations, where non-existent events are introduced or existing ones are misinterpreted, and omissions, where factual events are suppressed because they are misaligned with surrounding context. To systematically evaluate narrative prior-induced errors, we introduce NOAH, a large-scale benchmark that constructs composite videos by inserting clips from other sources into target videos. By varying semantic similarity and insertion position, our benchmark enables controlled and scalable analysis of narrative priors. We design one captioning task with tailored metrics and three QA tasks - Existence, Temporal, and Narrative - yielding more than 60K evaluation samples. Extensive experiments yield three key findings: (i) most Video LLMs exhibit hallucinations and omissions driven by narrative priors, (ii) the patterns of these errors vary across architectures and depend on event similarity and insertion position, and (iii) reliance on narrative priors intensifies under sampling with fewer frames, amplifying errors when event continuity is weak. We establish NOAH as the first standardized evaluation of narrative prior-induced hallucination and omission in Video LLMs, providing a foundation for developing more reliable and trustworthy models. Our benchmark and code are available at https://anonymous550520.github.io/.", "AI": {"tldr": "NOAH\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53d9\u4e8b\u5148\u9a8c\u5f15\u8d77\u5e7b\u89c9\u548c\u9057\u6f0f\u9519\u8bef\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u901a\u8fc7\u63d2\u5165\u5916\u6765\u89c6\u9891\u7247\u6bb5\u6784\u5efa\u590d\u5408\u89c6\u9891\uff0c\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u56e0\u8ffd\u6c42\u53d9\u4e8b\u8fde\u8d2f\u6027\u800c\u5ffd\u89c6\u89c6\u89c9\u8bc1\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ffd\u6c42\u53d9\u4e8b\u8fde\u8d2f\u6027\u65f6\uff0c\u4f1a\u5f15\u5165\u53d9\u4e8b\u5148\u9a8c\u504f\u5dee\uff0c\u5bfc\u81f4\u4ea7\u751f\u5e7b\u89c9\uff08\u5f15\u5165\u4e0d\u5b58\u5728\u4e8b\u4ef6\uff09\u548c\u9057\u6f0f\uff08\u6291\u5236\u4e0e\u4e0a\u4e0b\u6587\u4e0d\u7b26\u7684\u4e8b\u5b9e\u4e8b\u4ef6\uff09\u4e24\u79cd\u9519\u8bef\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6784\u5efaNOAH\u57fa\u51c6\uff0c\u901a\u8fc7\u5c06\u5176\u4ed6\u6765\u6e90\u7684\u89c6\u9891\u7247\u6bb5\u63d2\u5165\u76ee\u6807\u89c6\u9891\u4e2d\uff0c\u63a7\u5236\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u63d2\u5165\u4f4d\u7f6e\uff0c\u521b\u5efa\u5305\u542b60K+\u8bc4\u4f30\u6837\u672c\u7684\u6807\u6ce8\u4efb\u52a1\uff08\u4e00\u4e2a\u63cf\u8ff0\u4efb\u52a1\u548c\u4e09\u4e2aQA\u4efb\u52a1\uff09\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(i)\u5927\u591a\u6570\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u53d9\u4e8b\u5148\u9a8c\u9a71\u52a8\u7684\u5e7b\u89c9\u548c\u9057\u6f0f\uff1b(ii)\u9519\u8bef\u6a21\u5f0f\u56e0\u67b6\u6784\u3001\u4e8b\u4ef6\u76f8\u4f3c\u5ea6\u548c\u63d2\u5165\u4f4d\u7f6e\u800c\u5f02\uff1b(iii)\u5e27\u6570\u8f83\u5c11\u65f6\u53d9\u4e8b\u5148\u9a8c\u4f9d\u8d56\u66f4\u5f3a\uff0c\u9519\u8bef\u66f4\u4e25\u91cd\u3002", "conclusion": "NOAH\u662f\u9996\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53d9\u4e8b\u5148\u9a8c\u5f15\u8d77\u5e7b\u89c9\u548c\u9057\u6f0f\u7684\u57fa\u51c6\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u53ef\u4fe1\u7684\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.06598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06598", "abs": "https://arxiv.org/abs/2511.06598", "authors": ["Mohammad Shirzadi", "Ali Safarpoor Dehkordi", "Ahad N. Zehmakan"], "title": "Adaptive Initial Residual Connections for GNNs with Theoretical Guarantees", "comment": "This is the full version of the paper accepted to the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-2026)", "summary": "Message passing is the core operation in graph neural networks, where each node updates its embeddings by aggregating information from its neighbors. However, in deep architectures, this process often leads to diminished expressiveness. A popular solution is to use residual connections, where the input from the current (or initial) layer is added to aggregated neighbor information to preserve embeddings across layers. Following a recent line of research, we investigate an adaptive residual scheme in which different nodes have varying residual strengths. We prove that this approach prevents oversmoothing; particularly, we show that the Dirichlet energy of the embeddings remains bounded away from zero. This is the first theoretical guarantee not only for the adaptive setting, but also for static residual connections (where residual strengths are shared across nodes) with activation functions. Furthermore, extensive experiments show that this adaptive approach outperforms standard and state-of-the-art message passing mechanisms, especially on heterophilic graphs. To improve the time complexity of our approach, we introduce a variant in which residual strengths are not learned but instead set heuristically, a choice that performs as well as the learnable version.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u81ea\u9002\u5e94\u6b8b\u5dee\u8fde\u63a5\u65b9\u6848\uff0c\u8bc1\u660e\u5176\u80fd\u9632\u6b62\u8fc7\u5e73\u6ed1\u73b0\u8c61\uff0c\u5e76\u5728\u5f02\u914d\u56fe\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u6d88\u606f\u4f20\u9012\u4f1a\u5bfc\u81f4\u8868\u8fbe\u80fd\u529b\u4e0b\u964d\uff0c\u73b0\u6709\u6b8b\u5dee\u8fde\u63a5\u65b9\u6848\u4e0d\u591f\u7075\u6d3b\uff0c\u9700\u8981\u7814\u7a76\u81ea\u9002\u5e94\u6b8b\u5dee\u5f3a\u5ea6\u7684\u65b9\u6cd5\u6765\u4fdd\u6301\u8282\u70b9\u5d4c\u5165\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6b8b\u5dee\u65b9\u6848\uff0c\u4e0d\u540c\u8282\u70b9\u5177\u6709\u4e0d\u540c\u7684\u6b8b\u5dee\u5f3a\u5ea6\uff1b\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u4fdd\u6301Dirichlet\u80fd\u91cf\u8fdc\u79bb\u96f6\uff1b\u8fd8\u5f15\u5165\u542f\u53d1\u5f0f\u8bbe\u7f6e\u6b8b\u5dee\u5f3a\u5ea6\u7684\u53d8\u4f53\u4ee5\u964d\u4f4e\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u9632\u6b62\u8fc7\u5e73\u6ed1\uff1b\u5b9e\u9a8c\u8868\u660e\u5728\u5f02\u914d\u56fe\u4e0a\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u65b9\u6cd5\uff1b\u542f\u53d1\u5f0f\u53d8\u4f53\u4e0e\u53ef\u5b66\u4e60\u7248\u672c\u6027\u80fd\u76f8\u5f53\u4f46\u66f4\u9ad8\u6548\u3002", "conclusion": "\u81ea\u9002\u5e94\u6b8b\u5dee\u8fde\u63a5\u662f\u9632\u6b62\u56fe\u795e\u7ecf\u7f51\u7edc\u8fc7\u5e73\u6ed1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u5f02\u914d\u56fe\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u542f\u53d1\u5f0f\u8bbe\u7f6e\u6b8b\u5dee\u5f3a\u5ea6\u53ef\u8fbe\u5230\u76f8\u4f3c\u6027\u80fd\u4f46\u66f4\u9ad8\u6548\u3002"}}
{"id": "2511.06610", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06610", "abs": "https://arxiv.org/abs/2511.06610", "authors": ["Kaidong Wang", "Jiale Li", "Shao-Bo Lin", "Yao Wang"], "title": "Non-Rival Data as Rival Products: An Encapsulation-Forging Approach for Data Synthesis", "comment": null, "summary": "The non-rival nature of data creates a dilemma for firms: sharing data unlocks value but risks eroding competitive advantage. Existing data synthesis methods often exacerbate this problem by creating data with symmetric utility, allowing any party to extract its value. This paper introduces the Encapsulation-Forging (EnFo) framework, a novel approach to generate rival synthetic data with asymmetric utility. EnFo operates in two stages: it first encapsulates predictive knowledge from the original data into a designated ``key'' model, and then forges a synthetic dataset by optimizing the data to intentionally overfit this key model. This process transforms non-rival data into a rival product, ensuring its value is accessible only to the intended model, thereby preventing unauthorized use and preserving the data owner's competitive edge. Our framework demonstrates remarkable sample efficiency, matching the original data's performance with a fraction of its size, while providing robust privacy protection and resistance to misuse. EnFo offers a practical solution for firms to collaborate strategically without compromising their core analytical advantage.", "AI": {"tldr": "EnFo\u6846\u67b6\u901a\u8fc7\u5c06\u539f\u59cb\u6570\u636e\u7684\u9884\u6d4b\u77e5\u8bc6\u5c01\u88c5\u5230\u7279\u5b9a\"\u5bc6\u94a5\"\u6a21\u578b\u4e2d\uff0c\u7136\u540e\u4f18\u5316\u5408\u6210\u6570\u636e\u4f7f\u5176\u8fc7\u5ea6\u62df\u5408\u8be5\u6a21\u578b\uff0c\u4ece\u800c\u521b\u5efa\u5177\u6709\u4e0d\u5bf9\u79f0\u6548\u7528\u7684\u7ade\u4e89\u6027\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u6570\u636e\u5171\u4eab\u4e0e\u7ade\u4e89\u4f18\u52bf\u4fdd\u62a4\u7684\u56f0\u5883\u3002", "motivation": "\u6570\u636e\u7684\u975e\u7ade\u4e89\u6027\u4f7f\u4f01\u4e1a\u5728\u5171\u4eab\u6570\u636e\u521b\u9020\u4ef7\u503c\u4e0e\u4fdd\u62a4\u7ade\u4e89\u4f18\u52bf\u4e4b\u95f4\u9762\u4e34\u4e24\u96be\uff0c\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u4ea7\u751f\u5bf9\u79f0\u6548\u7528\u7684\u6570\u636e\uff0c\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u6570\u636e\u6240\u6709\u8005\u7684\u7ade\u4e89\u4f18\u52bf\u3002", "method": "EnFo\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5c06\u539f\u59cb\u6570\u636e\u7684\u9884\u6d4b\u77e5\u8bc6\u5c01\u88c5\u5230\u6307\u5b9a\u7684\"\u5bc6\u94a5\"\u6a21\u578b\u4e2d\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316\u5408\u6210\u6570\u636e\u4f7f\u5176\u6545\u610f\u8fc7\u5ea6\u62df\u5408\u8be5\u5bc6\u94a5\u6a21\u578b\uff0c\u4ece\u800c\u521b\u5efa\u5177\u6709\u4e0d\u5bf9\u79f0\u6548\u7528\u7684\u7ade\u4e89\u6027\u5408\u6210\u6570\u636e\u3002", "result": "EnFo\u6846\u67b6\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6837\u672c\u6548\u7387\uff0c\u4ec5\u9700\u539f\u59cb\u6570\u636e\u7684\u4e00\u5c0f\u90e8\u5206\u5373\u53ef\u8fbe\u5230\u76f8\u540c\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6297\u6ee5\u7528\u80fd\u529b\u3002", "conclusion": "EnFo\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6218\u7565\u5408\u4f5c\u4e2d\u5171\u4eab\u6570\u636e\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6838\u5fc3\u5206\u6790\u4f18\u52bf\u3002"}}
{"id": "2511.06662", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.06662", "abs": "https://arxiv.org/abs/2511.06662", "authors": ["Franklin Lee", "Tengfei Ma"], "title": "Dual-Pathway Fusion of EHRs and Knowledge Graphs for Predicting Unseen Drug-Drug Interactions", "comment": "ML4H 2025 Findings", "summary": "Drug-drug interactions (DDIs) remain a major source of preventable harm, and many clinically important mechanisms are still unknown. Existing models either rely on pharmacologic knowledge graphs (KGs), which fail on unseen drugs, or on electronic health records (EHRs), which are noisy, temporal, and site-dependent. We introduce, to our knowledge, the first system that conditions KG relation scoring on patient-level EHR context and distills that reasoning into an EHR-only model for zero-shot inference. A fusion \"Teacher\" learns mechanism-specific relations for drug pairs represented in both sources, while a distilled \"Student\" generalizes to new or rarely used drugs without KG access at inference. Both operate under a shared ontology (set) of pharmacologic mechanisms (drug relations) to produce interpretable, auditable alerts rather than opaque risk scores. Trained on a multi-institution EHR corpus paired with a curated DrugBank DDI graph, and evaluated using a clinically aligned, decision-focused protocol with leakage-safe negatives that avoid artificially easy pairs, the system maintains precision across multi-institutuion test data, produces mechanism-specific, clinically consistent predictions, reduces false alerts (higher precision) at comparable overall detection performance (F1), and misses fewer true interactions compared to prior methods. Case studies further show zero-shot identification of clinically recognized CYP-mediated and pharmacodynamic mechanisms for drugs absent from the KG, supporting real-world use in clinical decision support and pharmacovigilance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u8981\u4e48\u4f9d\u8d56\u77e5\u8bc6\u56fe\u8c31\uff08\u65e0\u6cd5\u5904\u7406\u672a\u89c1\u836f\u7269\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08\u566a\u58f0\u5927\u3001\u65f6\u95f4\u4f9d\u8d56\u6027\u5f3a\u3001\u7ad9\u70b9\u4f9d\u8d56\u6027\uff09\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u878d\u5408\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u77e5\u8bc6\u56fe\u8c31\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u836f\u7269\u5173\u7cfb\uff0c\u7136\u540e\u84b8\u998f\u5230\u4ec5\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u7406\u3002\u7cfb\u7edf\u57fa\u4e8e\u5171\u4eab\u836f\u7406\u5b66\u673a\u5236\u672c\u4f53\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u8b66\u62a5\u800c\u975e\u4e0d\u900f\u660e\u7684\u98ce\u9669\u8bc4\u5206\u3002", "result": "\u5728\u591a\u673a\u6784\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7cfb\u7edf\u4fdd\u6301\u7cbe\u5ea6\uff0c\u4ea7\u751f\u673a\u5236\u7279\u5f02\u6027\u9884\u6d4b\uff0c\u51cf\u5c11\u8bef\u62a5\uff0c\u5728\u53ef\u6bd4\u68c0\u6d4b\u6027\u80fd\u4e0b\u6f0f\u62a5\u66f4\u5c11\u771f\u5b9e\u76f8\u4e92\u4f5c\u7528\u3002\u6848\u4f8b\u7814\u7a76\u663e\u793a\u80fd\u96f6\u6837\u672c\u8bc6\u522b\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7f3a\u5931\u836f\u7269\u7684CYP\u4ecb\u5bfc\u548c\u836f\u6548\u5b66\u673a\u5236\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u548c\u836f\u7269\u8b66\u6212\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u836f\u6216\u7f55\u89c1\u836f\u7269\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8bbf\u95ee\u77e5\u8bc6\u56fe\u8c31\u3002"}}
{"id": "2511.06702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06702", "abs": "https://arxiv.org/abs/2511.06702", "authors": ["Yifan Wang", "Yian Zhao", "Fanqi Pu", "Xiaochen Yang", "Yang Tang", "Xi Chen", "Wenming Yang"], "title": "SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection", "comment": null, "summary": "Existing monocular 3D detectors typically tame the pronounced nonlinear regression of 3D bounding box through decoupled prediction paradigm, which employs multiple branches to estimate geometric center, depth, dimensions, and rotation angle separately. Although this decoupling strategy simplifies the learning process, it inherently ignores the geometric collaborative constraints between different attributes, resulting in the lack of geometric consistency prior, thereby leading to suboptimal performance. To address this issue, we propose novel Spatial-Projection Alignment (SPAN) with two pivotal components: (i). Spatial Point Alignment enforces an explicit global spatial constraint between the predicted and ground-truth 3D bounding boxes, thereby rectifying spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection Alignment ensures that the projected 3D box is aligned tightly within its corresponding 2D detection bounding box on the image plane, mitigating projection misalignment overlooked in previous works. To ensure training stability, we further introduce a Hierarchical Task Learning strategy that progressively incorporates spatial-projection alignment as 3D attribute predictions refine, preventing early stage error propagation across attributes. Extensive experiments demonstrate that the proposed method can be easily integrated into any established monocular 3D detector and delivers significant performance improvements.", "AI": {"tldr": "\u63d0\u51faSPAN\u65b9\u6cd5\u89e3\u51b3\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u89e3\u8026\u9884\u6d4b\u5ffd\u7565\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7a7a\u95f4\u70b9\u5bf9\u9f50\u548c3D-2D\u6295\u5f71\u5bf9\u9f50\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u4fdd\u8bc1\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5355\u76ee3D\u68c0\u6d4b\u5668\u91c7\u7528\u89e3\u8026\u9884\u6d4b\u8303\u5f0f\u5206\u522b\u4f30\u8ba1\u51e0\u4f55\u4e2d\u5fc3\u3001\u6df1\u5ea6\u3001\u5c3a\u5bf8\u548c\u65cb\u8f6c\u89d2\u5ea6\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u5c5e\u6027\u95f4\u7684\u51e0\u4f55\u534f\u4f5c\u7ea6\u675f\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u51e0\u4f55\u4e00\u81f4\u6027\u5148\u9a8c\uff0c\u6027\u80fd\u6b21\u4f18\u3002", "method": "SPAN\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7a7a\u95f4\u70b9\u5bf9\u9f50\u5728\u9884\u6d4b\u548c\u771f\u5b9e3D\u8fb9\u754c\u6846\u4e4b\u95f4\u65bd\u52a0\u5168\u5c40\u7a7a\u95f4\u7ea6\u675f\uff1b3D-2D\u6295\u5f71\u5bf9\u9f50\u786e\u4fdd\u6295\u5f71\u76843D\u6846\u4e0e2D\u68c0\u6d4b\u6846\u7d27\u5bc6\u5bf9\u9f50\u3002\u91c7\u7528\u5206\u5c42\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u9010\u6b65\u5f15\u5165\u5bf9\u9f50\u7ea6\u675f\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u4efb\u4f55\u73b0\u6709\u5355\u76ee3D\u68c0\u6d4b\u5668\u4e2d\uff0c\u5e76\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SPAN\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u89e3\u51b3\u4e86\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u89e3\u8026\u9884\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.06723", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06723", "abs": "https://arxiv.org/abs/2511.06723", "authors": ["Evelyn Chee", "Wynne Hsu", "Mong Li Lee"], "title": "Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation", "comment": "Accepted to ECAI 2025", "summary": "Continual learning is essential for adapting models to new tasks while retaining previously acquired knowledge. While existing approaches predominantly focus on uni-modal data, multi-modal learning offers substantial benefits by utilizing diverse sensory inputs, akin to human perception. However, multi-modal continual learning presents additional challenges, as the model must effectively integrate new information from various modalities while preventing catastrophic forgetting. In this work, we propose a pre-trained model-based framework for multi-modal continual learning. Our framework includes a novel cross-modality adapter with a mixture-of-experts structure to facilitate effective integration of multi-modal information across tasks. We also introduce a representation alignment loss that fosters learning of robust multi-modal representations, and regularize relationships between learned representations to preserve knowledge from previous tasks. Experiments on several multi-modal datasets demonstrate that our approach consistently outperforms baselines in both class-incremental and domain-incremental learning, achieving higher accuracy and reduced forgetting.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u6a21\u6001\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u9002\u914d\u5668\u548c\u8868\u793a\u5bf9\u9f50\u635f\u5931\u6765\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6570\u636e\uff0c\u800c\u591a\u6a21\u6001\u5b66\u4e60\u80fd\u5229\u7528\u591a\u6837\u5316\u611f\u5b98\u8f93\u5165\uff0c\u4f46\u591a\u6a21\u6001\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u6574\u5408\u65b0\u4fe1\u606f\u548c\u9632\u6b62\u9057\u5fd8\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6846\u67b6\uff0c\u5305\u542b\u6df7\u5408\u4e13\u5bb6\u7ed3\u6784\u7684\u8de8\u6a21\u6001\u9002\u914d\u5668\u3001\u8868\u793a\u5bf9\u9f50\u635f\u5931\u4ee5\u53ca\u8868\u793a\u5173\u7cfb\u6b63\u5219\u5316\u6765\u4fdd\u62a4\u5148\u524d\u4efb\u52a1\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7c7b\u522b\u589e\u91cf\u548c\u57df\u589e\u91cf\u5b66\u4e60\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u9057\u5fd8\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5148\u524d\u77e5\u8bc6\u7684\u540c\u65f6\u6574\u5408\u65b0\u6a21\u6001\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.06856", "categories": ["cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.06856", "abs": "https://arxiv.org/abs/2511.06856", "authors": ["Andrea Testa", "Soren Hauberg", "Tamim Asfour", "Leonel Rozo"], "title": "Contact Wasserstein Geodesics for Non-Conservative Schrodinger Bridges", "comment": "38 pages, 18 figures", "summary": "The Schr\u00f6dinger Bridge provides a principled framework for modeling stochastic processes between distributions; however, existing methods are limited by energy-conservation assumptions, which constrains the bridge's shape preventing it from model varying-energy phenomena. To overcome this, we introduce the non-conservative generalized Schr\u00f6dinger bridge (NCGSB), a novel, energy-varying reformulation based on contact Hamiltonian mechanics. By allowing energy to change over time, the NCGSB provides a broader class of real-world stochastic processes, capturing richer and more faithful intermediate dynamics. By parameterizing the Wasserstein manifold, we lift the bridge problem to a tractable geodesic computation in a finite-dimensional space. Unlike computationally expensive iterative solutions, our contact Wasserstein geodesic (CWG) is naturally implemented via a ResNet architecture and relies on a non-iterative solver with near-linear complexity. Furthermore, CWG supports guided generation by modulating a task-specific distance metric. We validate our framework on tasks including manifold navigation, molecular dynamics predictions, and image generation, demonstrating its practical benefits and versatility.", "AI": {"tldr": "\u63d0\u51fa\u975e\u4fdd\u5b88\u5e7f\u4e49\u859b\u5b9a\u8c14\u6865(NCGSB)\uff0c\u57fa\u4e8e\u63a5\u89e6\u54c8\u5bc6\u987f\u529b\u5b66\uff0c\u5141\u8bb8\u80fd\u91cf\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u80fd\u5efa\u6a21\u66f4\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u968f\u673a\u8fc7\u7a0b\u3002\u901a\u8fc7\u53c2\u6570\u5316Wasserstein\u6d41\u5f62\uff0c\u5c06\u6865\u95ee\u9898\u8f6c\u5316\u4e3a\u6709\u9650\u7ef4\u7a7a\u95f4\u4e2d\u7684\u53ef\u5904\u7406\u6d4b\u5730\u7ebf\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u53d7\u9650\u4e8e\u80fd\u91cf\u5b88\u6052\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u6865\u7684\u5f62\u72b6\uff0c\u65e0\u6cd5\u5efa\u6a21\u80fd\u91cf\u53d8\u5316\u7684\u73b0\u8c61\u3002\u9700\u8981\u514b\u670d\u8fd9\u4e00\u9650\u5236\u6765\u6355\u6349\u66f4\u4e30\u5bcc\u548c\u771f\u5b9e\u7684\u4e2d\u95f4\u52a8\u6001\u3002", "method": "\u5f15\u5165NCGSB\u6846\u67b6\uff0c\u57fa\u4e8e\u63a5\u89e6\u54c8\u5bc6\u987f\u529b\u5b66\u5141\u8bb8\u80fd\u91cf\u53d8\u5316\u3002\u901a\u8fc7\u53c2\u6570\u5316Wasserstein\u6d41\u5f62\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u63a5\u89e6Wasserstein\u6d4b\u5730\u7ebf(CWG)\u8ba1\u7b97\uff0c\u4f7f\u7528ResNet\u67b6\u6784\u5b9e\u73b0\u975e\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u5177\u6709\u8fd1\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "\u5728\u6d41\u5f62\u5bfc\u822a\u3001\u5206\u5b50\u52a8\u529b\u5b66\u9884\u6d4b\u548c\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u4f18\u52bf\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "NCGSB\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u901a\u7528\u7684\u968f\u673a\u8fc7\u7a0b\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u6355\u6349\u80fd\u91cf\u53d8\u5316\u7684\u771f\u5b9e\u4e16\u754c\u73b0\u8c61\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u652f\u6301\u5f15\u5bfc\u751f\u6210\u3002"}}
{"id": "2511.06863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06863", "abs": "https://arxiv.org/abs/2511.06863", "authors": ["Sicheng Yang", "Xing Hu", "Qiang Wu", "Dawei Yang"], "title": "VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling", "comment": null, "summary": "Vector quantization (VQ) transforms continuous image features into discrete representations, providing compressed, tokenized inputs for generative models. However, VQ-based frameworks suffer from several issues, such as non-smooth latent spaces, weak alignment between representations before and after quantization, and poor coherence between the continuous and discrete domains. These issues lead to unstable codeword learning and underutilized codebooks, ultimately degrading the performance of both reconstruction and downstream generation tasks. To this end, we propose VAEVQ, which comprises three key components: (1) Variational Latent Quantization (VLQ), replacing the AE with a VAE for quantization to leverage its structured and smooth latent space, thereby facilitating more effective codeword activation; (2) Representation Coherence Strategy (RCS), adaptively modulating the alignment strength between pre- and post-quantization features to enhance consistency and prevent overfitting to noise; and (3) Distribution Consistency Regularization (DCR), aligning the entire codebook distribution with the continuous latent distribution to improve utilization. Extensive experiments on two benchmark datasets demonstrate that VAEVQ outperforms state-of-the-art methods.", "AI": {"tldr": "VAEVQ\u901a\u8fc7\u53d8\u5206\u6f5c\u5728\u91cf\u5316\u3001\u8868\u793a\u4e00\u81f4\u6027\u7b56\u7565\u548c\u5206\u5e03\u4e00\u81f4\u6027\u6b63\u5219\u5316\u89e3\u51b3\u4e86\u4f20\u7edfVQ\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u5e73\u6ed1\u6027\u3001\u8868\u793a\u5bf9\u9f50\u548c\u57df\u4e00\u81f4\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5411\u91cf\u91cf\u5316(VQ)\u65b9\u6cd5\u5b58\u5728\u6f5c\u5728\u7a7a\u95f4\u4e0d\u5e73\u6ed1\u3001\u91cf\u5316\u524d\u540e\u8868\u793a\u5bf9\u9f50\u5f31\u3001\u8fde\u7eed\u4e0e\u79bb\u6563\u57df\u4e00\u81f4\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7801\u5b57\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u7801\u672c\u5229\u7528\u7387\u4f4e\uff0c\u5f71\u54cd\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faVAEVQ\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u53d8\u5206\u6f5c\u5728\u91cf\u5316(VLQ)\uff0c\u7528VAE\u66ff\u4ee3AE\u8fdb\u884c\u91cf\u5316\uff1b2) \u8868\u793a\u4e00\u81f4\u6027\u7b56\u7565(RCS)\uff0c\u81ea\u9002\u5e94\u8c03\u8282\u91cf\u5316\u524d\u540e\u7279\u5f81\u5bf9\u9f50\u5f3a\u5ea6\uff1b3) \u5206\u5e03\u4e00\u81f4\u6027\u6b63\u5219\u5316(DCR)\uff0c\u5bf9\u9f50\u7801\u672c\u5206\u5e03\u4e0e\u8fde\u7eed\u6f5c\u5728\u5206\u5e03\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVAEVQ\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "VAEVQ\u901a\u8fc7\u6539\u8fdb\u91cf\u5316\u673a\u5236\u548c\u589e\u5f3a\u8868\u793a\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfVQ\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.07276", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07276", "abs": "https://arxiv.org/abs/2511.07276", "authors": ["Salem AlMarri", "Muhammad Irzam Liaqat", "Muhammad Zaigham Zaheer", "Shah Nawaz", "Karthik Nandakumar", "Markus Schedl"], "title": "RobustA: Robust Anomaly Detection in Multimodal Data", "comment": "Submitted to IEEE Transactions on Image Processing", "summary": "In recent years, multimodal anomaly detection methods have demonstrated remarkable performance improvements over video-only models. However, real-world multimodal data is often corrupted due to unforeseen environmental distortions. In this paper, we present the first-of-its-kind work that comprehensively investigates the adverse effects of corrupted modalities on multimodal anomaly detection task. To streamline this work, we propose RobustA, a carefully curated evaluation dataset to systematically observe the impacts of audio and visual corruptions on the overall effectiveness of anomaly detection systems. Furthermore, we propose a multimodal anomaly detection method, which shows notable resilience against corrupted modalities. The proposed method learns a shared representation space for different modalities and employs a dynamic weighting scheme during inference based on the estimated level of corruption. Our work represents a significant step forward in enabling the real-world application of multimodal anomaly detection, addressing situations where the likely events of modality corruptions occur. The proposed evaluation dataset with corrupted modalities and respective extracted features will be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u6a21\u6001\u635f\u574f\u5bf9\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u97f3\u9891\u548c\u89c6\u89c9\u635f\u574f\u7684\u8bc4\u4f30\u6570\u636e\u96c6RobustA\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u635f\u574f\u6a21\u6001\u5177\u6709\u9c81\u68d2\u6027\u7684\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\u7ecf\u5e38\u56e0\u73af\u5883\u5931\u771f\u800c\u635f\u574f\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u6a21\u6001\u635f\u574f\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\u7684\u5171\u4eab\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u57fa\u4e8e\u4f30\u8ba1\u7684\u635f\u574f\u7a0b\u5ea6\u91c7\u7528\u52a8\u6001\u52a0\u6743\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5bf9\u635f\u574f\u6a21\u6001\u8868\u73b0\u51fa\u663e\u8457\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u53ef\u80fd\u53d1\u751f\u7684\u6a21\u6001\u635f\u574f\u60c5\u51b5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u635f\u574f\u53ef\u80fd\u53d1\u751f\u7684\u60c5\u51b5\uff0c\u76f8\u5173\u6570\u636e\u96c6\u548c\u7279\u5f81\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2511.07299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07299", "abs": "https://arxiv.org/abs/2511.07299", "authors": ["Ying Cheng", "Yu-Ho Lin", "Min-Hung Chen", "Fu-En Yang", "Shang-Hong Lai"], "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models", "comment": null, "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.", "AI": {"tldr": "VADER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5173\u952e\u5e27\u5bf9\u8c61\u5173\u7cfb\u7279\u5f81\u548c\u89c6\u89c9\u7ebf\u7d22\u6765\u589e\u5f3a\u5f02\u5e38\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u5ffd\u89c6\u4e86\u5bf9\u8c61\u95f4\u66f4\u6df1\u5c42\u6b21\u7684\u56e0\u679c\u5173\u7cfb\u548c\u4ea4\u4e92\uff0c\u8fd9\u4e9b\u5bf9\u4e8e\u7406\u89e3\u5f02\u5e38\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "method": "VADER\u9996\u5148\u5e94\u7528\u5f02\u5e38\u8bc4\u5206\u5668\u5206\u914d\u6bcf\u5e27\u5f02\u5e38\u5206\u6570\uff0c\u7136\u540e\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u91c7\u6837\u7b56\u7565\u6355\u83b7\u5f02\u5e38\u4e8b\u4ef6\u7684\u56e0\u679c\u4e0a\u4e0b\u6587\u3002\u901a\u8fc7\u5173\u7cfb\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5bf9\u6bd4\u5173\u7cfb\u7f16\u7801\u5668\u8054\u5408\u5efa\u6a21\u52a8\u6001\u5bf9\u8c61\u4ea4\u4e92\uff0c\u751f\u6210\u7d27\u51d1\u7684\u5173\u7cfb\u8868\u793a\uff0c\u6700\u540e\u4e0eLLM\u96c6\u6210\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754cVAU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVADER\u5728\u5f02\u5e38\u63cf\u8ff0\u3001\u89e3\u91ca\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u7ed3\u679c\u3002", "conclusion": "VADER\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u89c6\u9891\u5f02\u5e38\u5206\u6790\u7684\u524d\u6cbf\uff0c\u80fd\u591f\u751f\u6210\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u8be6\u7ec6\u63cf\u8ff0\u5e76\u652f\u6301\u7a33\u5065\u7684\u5f02\u5e38\u76f8\u5173\u95ee\u7b54\u3002"}}
{"id": "2511.07321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07321", "abs": "https://arxiv.org/abs/2511.07321", "authors": ["Botao Ye", "Boqi Chen", "Haofei Xu", "Daniel Barath", "Marc Pollefeys"], "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting", "comment": null, "summary": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.", "AI": {"tldr": "YoNoSplat\u662f\u4e00\u4e2a\u524d\u9988\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u6570\u91cf\u7684\u65e0\u7ed3\u6784\u56fe\u50cf\u4e2d\u91cd\u5efa\u9ad8\u8d28\u91cf\u76843D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\uff0c\u652f\u6301\u6709\u59ff\u6001\u548c\u65e0\u59ff\u6001\u3001\u6709\u6807\u5b9a\u548c\u65e0\u6807\u5b9a\u7684\u8f93\u5165\uff0c\u5177\u6709\u9ad8\u6548\u548c\u591a\u529f\u80fd\u7684\u7279\u70b9\u3002", "motivation": "\u89e3\u51b3\u4ece\u65e0\u7ed3\u6784\u56fe\u50cf\u96c6\u5408\u4e2d\u8fdb\u884c\u5feb\u901f\u7075\u6d3b\u76843D\u573a\u666f\u91cd\u5efa\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65e0\u59ff\u6001\u548c\u65e0\u6807\u5b9a\u8f93\u5165\u65f6\u7684\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u524d\u9988\u6a21\u578b\u9884\u6d4b\u5c40\u90e8\u9ad8\u65af\u548c\u76f8\u673a\u59ff\u6001\uff0c\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u89e3\u51b33D\u9ad8\u65af\u548c\u76f8\u673a\u53c2\u6570\u8054\u5408\u5b66\u4e60\u7684\u56f0\u96be\uff0c\u91c7\u7528\u6210\u5bf9\u76f8\u673a\u8ddd\u79bb\u5f52\u4e00\u5316\u65b9\u6848\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5d4c\u5165\u76f8\u673a\u5185\u53c2\u5230\u7f51\u7edc\u4e2d\u3002", "result": "\u5728NVIDIA GH200 GPU\u4e0a\uff0c\u4ece100\u4e2a\u89c6\u56fe\uff08280x518\u5206\u8fa8\u7387\uff09\u91cd\u5efa\u573a\u666f\u4ec5\u97002.69\u79d2\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u65e0\u59ff\u6001\u548c\u6709\u59ff\u6001\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "YoNoSplat\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u6027\u80fd\u4f18\u8d8a\u76843D\u573a\u666f\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u65e0\u7ed3\u6784\u56fe\u50cf\u96c6\u5408\u3002"}}
{"id": "2511.07325", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07325", "abs": "https://arxiv.org/abs/2511.07325", "authors": ["R. Kumar", "A. Lall", "S. Chaudhari", "M. Kale", "A. Vattem"], "title": "Garbage Vulnerable Point Monitoring using IoT and Computer Vision", "comment": null, "summary": "This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u8054\u7f51\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u667a\u80fd\u57ce\u5e02\u5783\u573e\u7ba1\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u76d1\u63a7\u5783\u573e\u6613\u53d1\u70b9\u7684\u975e\u6cd5\u503e\u5012\u884c\u4e3a\uff0cYOLO11m\u6a21\u578b\u5728\u5783\u573e\u68c0\u6d4b\u4e2d\u8fbe\u523092.39%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u5783\u573e\u6613\u53d1\u70b9\u7684\u975e\u6cd5\u503e\u5012\u95ee\u9898\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u5b9e\u73b0\u9ad8\u6548\u76d1\u63a7\u548c\u7ba1\u7406\uff0c\u6539\u5584\u57ce\u5e02\u73af\u5883\u536b\u751f\u3002", "method": "\u4f7f\u7528\u8857\u7ea7\u6444\u50cf\u5934\u548c\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff08\u5305\u62ecYOLOv8\u3001YOLOv10\u3001YOLO11m\u548cRT-DETR\uff09\u8fdb\u884c\u5783\u573e\u68c0\u6d4b\uff0c\u6570\u636e\u6765\u81ea\u5370\u5ea6\u7279\u4f26\u7518\u7eb3\u90a6Sangareddy\u5730\u533a\u3002", "result": "YOLO11m\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5783\u573e\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe92.39%\uff0cmAP@50\u4e3a0.91\uff0c\u80fd\u6709\u6548\u6355\u83b7\u5783\u573e\u503e\u5012\u7684\u65f6\u3001\u65e5\u3001\u5468\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5168\u5929\u5019\u76d1\u63a7\u3002", "conclusion": "\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u9002\u7528\u4e8e\u5783\u573e\u6613\u53d1\u70b9\u7684\u76d1\u63a7\u548c\u8ffd\u8e2a\uff0c\u7cfb\u7edf\u80fd\u5168\u9762\u76d1\u6d4b\u5783\u573e\u503e\u5012\u884c\u4e3a\uff0c\u4e3a\u57ce\u5e02\u5783\u573e\u7ba1\u7406\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
