<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VideoEraser: Concept Erasure in Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.15314)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Zhi Chen,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CV

TL;DR: VideoEraser是一个无需训练的即插即用框架，通过两阶段处理防止文本到视频扩散模型生成包含不良概念的内容，在多个擦除任务中平均减少46%的不良内容生成。


<details>
  <summary>Details</summary>
Motivation: 文本到视频扩散模型的快速发展引发了隐私、版权和安全方面的担忧，这些模型可能在未经授权的情况下使用个人身份、艺术创作和有害材料进行训练，导致不良内容的不可控生产和传播。

Method: 提出VideoEraser框架，采用两阶段处理：选择性提示嵌入调整（SPEA）和抗干扰噪声引导（ARNG），作为即插即用模块与现有T2V扩散模型无缝集成。

Result: 在对象擦除、艺术风格擦除、名人擦除和显式内容擦除四个任务中，VideoEraser在效果、完整性、保真度、鲁棒性和泛化性方面均优于现有方法，平均减少46%的不良内容生成。

Conclusion: VideoEraser在抑制T2V生成中的不良内容方面达到了最先进的性能，为解决扩散模型的安全和伦理问题提供了有效的解决方案。

Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns
about privacy, copyright, and safety due to their potential misuse in
generating harmful or misleading content. These models are often trained on
numerous datasets, including unauthorized personal identities, artistic
creations, and harmful materials, which can lead to uncontrolled production and
distribution of such content. To address this, we propose VideoEraser, a
training-free framework that prevents T2V diffusion models from generating
videos with undesirable concepts, even when explicitly prompted with those
concepts. Designed as a plug-and-play module, VideoEraser can seamlessly
integrate with representative T2V diffusion models via a two-stage process:
Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise
Guidance (ARNG). We conduct extensive evaluations across four tasks, including
object erasure, artistic style erasure, celebrity erasure, and explicit content
erasure. Experimental results show that VideoEraser consistently outperforms
prior methods regarding efficacy, integrity, fidelity, robustness, and
generalizability. Notably, VideoEraser achieves state-of-the-art performance in
suppressing undesirable content during T2V generation, reducing it by 46% on
average across four tasks compared to baselines.

</details>


### [2] [Spiking Variational Graph Representation Inference for Video Summarization](https://arxiv.org/abs/2508.15389)
*Wenrui Li,Wei Han,Liang-Jian Deng,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出SpiVG网络，通过脉冲神经网络和动态聚合图推理器解决视频摘要中的时序依赖和语义连贯性问题，使用变分推理处理多通道特征融合噪声，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着短视频内容兴起，现有视频摘要方法难以捕捉全局时序依赖和保持语义连贯性，且多通道特征融合易受噪声影响，需要更高效的信息密度和计算复杂度优化方案。

Method: 设计基于脉冲神经网络(SNN)的关键帧提取器，利用事件驱动机制自主学习关键帧特征；引入动态聚合图推理器实现细粒度跨帧推理；提出变分推理重建模块，使用ELBO优化捕获多通道特征分布的潜在结构。

Result: 在SumMe、TVSum、VideoXum和QFVS等多个数据集上的实验结果表明，SpiVG超越了现有方法。

Conclusion: SpiVG网络通过结合脉冲神经网络、动态图推理和变分推理技术，有效解决了视频摘要中的关键挑战，提供了更高的信息密度和更低的计算复杂度。

Abstract: With the rise of short video content, efficient video summarization
techniques for extracting key information have become crucial. However,
existing methods struggle to capture the global temporal dependencies and
maintain the semantic coherence of video content. Additionally, these methods
are also influenced by noise during multi-channel feature fusion. We propose a
Spiking Variational Graph (SpiVG) Network, which enhances information density
and reduces computational complexity. First, we design a keyframe extractor
based on Spiking Neural Networks (SNN), leveraging the event-driven computation
mechanism of SNNs to learn keyframe features autonomously. To enable
fine-grained and adaptable reasoning across video frames, we introduce a
Dynamic Aggregation Graph Reasoner, which decouples contextual object
consistency from semantic perspective coherence. We present a Variational
Inference Reconstruction Module to address uncertainty and noise arising during
multi-channel feature fusion. In this module, we employ Evidence Lower Bound
Optimization (ELBO) to capture the latent structure of multi-channel feature
distributions, using posterior distribution regularization to reduce
overfitting. Experimental results show that SpiVG surpasses existing methods
across multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes
and pre-trained models are available at https://github.com/liwrui/SpiVG.

</details>


### [3] [Aligning Moments in Time using Video Queries](https://arxiv.org/abs/2508.15439)
*Yogesh Kumar,Uday Agarwal,Manish Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: MATR是一个基于Transformer的视频到视频时刻检索模型，通过双阶段序列对齐和自监督预训练，在ActivityNet-VRL和SportsMoments数据集上显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频到视频时刻检索任务需要语义帧级对齐和建模复杂依赖关系，现有方法难以有效处理这些挑战。

Method: 提出MATR模型，使用双阶段序列对齐编码查询和目标视频的相关性，结合前景/背景分类和边界预测头，并采用自监督预训练技术。

Result: 在ActivityNet-VRL数据集上R@1提升13.1%，mIoU提升8.1%；在SportsMoments数据集上R@1提升14.7%，mIoU提升14.4%。

Conclusion: MATR通过有效的序列对齐和预训练策略，显著提升了视频到视频时刻检索的性能，为该任务提供了强有力的解决方案。

Abstract: Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen
events or moments in a target video using a query video. This task poses
several challenges, such as the need for semantic frame-level alignment and
modeling complex dependencies between query and target videos. To tackle this
challenging problem, we introduce MATR (Moment Alignment TRansformer), a
transformer-based model designed to capture semantic context as well as the
temporal details necessary for precise moment localization. MATR conditions
target video representations on query video features using dual-stage sequence
alignment that encodes the required correlations and dependencies. These
representations are then used to guide foreground/background classification and
boundary prediction heads, enabling the model to accurately identify moments in
the target video that semantically match with the query video. Additionally, to
provide a strong task-specific initialization for MATR, we propose a
self-supervised pre-training technique that involves training the model to
localize random clips within videos. Extensive experiments demonstrate that
MATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU
on an absolute scale compared to state-of-the-art methods on the popular
ActivityNet-VRL dataset. Additionally, on our newly proposed dataset,
SportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an
absolute scale over strong baselines.

</details>


### [4] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT是一个视频大语言模型，通过扩散时序潜在编码器、对象接地表示和混合令牌方案，显著提升了视频时序感知和实体定位能力，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM在整体推理方面取得显著进展，但在时序感知方面仍显粗糙：时间戳仅隐式编码、帧级特征连续性捕捉能力弱、语言视觉对齐容易偏离关注实体。

Method: 提出三个关键创新：1)扩散时序潜在编码器增强边界敏感性和时序一致性；2)对象接地表示显式绑定查询实体到局部视觉证据；3)混合令牌方案提供显式时间戳建模。

Result: 在Charades STA、NExT GQA和多个VideoQA基准测试中取得了最先进的结果，验证了强大的接地能力。

Conclusion: Grounded VideoDiT通过创新的时序编码和实体接地机制，成功解决了视频理解中的精细时序感知和实体交互定位问题，为视频LLM的发展提供了新方向。

Abstract: Understanding videos requires more than answering open ended questions, it
demands the ability to pinpoint when events occur and how entities interact
across time. While recent Video LLMs have achieved remarkable progress in
holistic reasoning, they remain coarse in temporal perception: timestamps are
encoded only implicitly, frame level features are weak in capturing continuity,
and language vision alignment often drifts from the entities of interest. In
this paper, we present Grounded VideoDiT, a Video LLM designed to overcome
these limitations by introducing three key innovations. First, a Diffusion
Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains
temporal consistency. Second, object grounded representations explicitly bind
query entities to localized visual evidence, strengthening alignment. Third, a
mixed token scheme with discrete temporal tokens provides explicit timestamp
modeling, enabling fine grained temporal reasoning. Together, these designs
equip Grounded VideoDiT with robust grounding capabilities, as validated by
state of the art results on Charades STA, NExT GQA, and multiple VideoQA
benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving](https://arxiv.org/abs/2508.14926)
*Dianzhao Li,Ostap Okhrin*

Main category: cs.LG

TL;DR: 提出了一个分层安全强化学习框架，将道德考量与标准驾驶目标结合，通过伦理风险成本训练智能体，在真实交通场景中减少伦理风险并保持驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要嵌入稳健的伦理推理能力，以确保在常规和紧急操作中做出道德决策，这是广泛采用的关键。

Method: 分层安全强化学习框架：决策层使用包含碰撞概率和伤害严重性的伦理风险成本训练Safe RL智能体；执行层采用多项式路径规划和PID/Stanley控制器实现平滑轨迹；动态优先经验回放机制强化高风险事件学习。

Result: 在包含多样化车辆、骑行者和行人的真实交通数据集上验证，该方法在减少伦理风险和保持驾驶性能方面优于基线方法。

Conclusion: 这是首个通过安全强化学习在真实场景中研究自动驾驶车辆伦理决策的工作，结合形式控制理论和数据驱动学习，有望推动复杂人车混合交通环境中的伦理问责自治。

Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and
improving transportation efficiency, yet their widespread adoption hinges on
embedding robust ethical reasoning into routine and emergency maneuvers. Here,
we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that
explicitly integrates moral considerations with standard driving objectives. At
the decision level, a Safe RL agent is trained using a composite ethical risk
cost, combining collision probability and harm severity, to generate high-level
motion targets. A dynamic Prioritized Experience Replay mechanism amplifies
learning from rare but critical, high-risk events. At the execution level,
polynomial path planning coupled with Proportional-Integral-Derivative (PID)
and Stanley controllers translates these targets into smooth, feasible
trajectories, ensuring both accuracy and comfort. We train and validate our
approach on rich, real-world traffic datasets encompassing diverse vehicles,
cyclists, and pedestrians, and demonstrate that it outperforms baseline methods
in reducing ethical risk and maintaining driving performance. To our knowledge,
this is the first study of ethical decision-making for autonomous vehicles via
Safe RL in real-world scenarios. Our results highlight the potential of
combining formal control theory and data-driven learning to advance ethically
accountable autonomy in complex, human-mixed traffic environments.

</details>


### [6] [Wormhole Dynamics in Deep Neural Networks](https://arxiv.org/abs/2508.15086)
*Yen-Lung Lai,Zhe Jin*

Main category: cs.LG

TL;DR: 该研究分析了深度神经网络在过参数化状态下的泛化行为，揭示了输出特征空间坍塌现象，提出了绕过退化状态的"虫洞"解决方案，为理解DNN泛化和捷径学习提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络中"愚弄样本"现象，即DNN对看似随机的输入产生高置信度分类，旨在理解DNN的泛化行为并探索传统梯度优化方法之外的解析框架。

Method: 采用基于最大似然估计的解析框架，避免依赖梯度优化和显式标签的传统数值方法，分析过参数化DNN的输出特征空间坍塌现象。

Result: 发现DNN在过参数化状态下会出现输出特征空间坍塌，虽然改善泛化但增加层数会导致退化状态；提出的"虫洞"解决方案能够绕过退化，调和随机输入与有意义标签。

Conclusion: 研究为理解DNN泛化机制提供了新见解，揭示了特征空间坍塌与模型退化的关系，"虫洞"解决方案为捷径学习和无监督学习动力学研究指明了新方向。

Abstract: This work investigates the generalization behavior of deep neural networks
(DNNs), focusing on the phenomenon of "fooling examples," where DNNs
confidently classify inputs that appear random or unstructured to humans. To
explore this phenomenon, we introduce an analytical framework based on maximum
likelihood estimation, without adhering to conventional numerical approaches
that rely on gradient-based optimization and explicit labels. Our analysis
reveals that DNNs operating in an overparameterized regime exhibit a collapse
in the output feature space. While this collapse improves network
generalization, adding more layers eventually leads to a state of degeneracy,
where the model learns trivial solutions by mapping distinct inputs to the same
output, resulting in zero loss. Further investigation demonstrates that this
degeneracy can be bypassed using our newly derived "wormhole" solution. The
wormhole solution, when applied to arbitrary fooling examples, reconciles
meaningful labels with random ones and provides a novel perspective on shortcut
learning. These findings offer deeper insights into DNN generalization and
highlight directions for future research on learning dynamics in unsupervised
settings to bridge the gap between theory and practice.

</details>


### [7] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 该论文研究了一个输入驱动的单状态微分方程模型，该模型最初为实验证明的动态分子开关开发，具有类似大脑突触的切换特性。模型具有精确可解性、收敛性和渐逝记忆等数学特性，能够稳定处理时变输入，同时兼具生物启发行为和稳定学习的数学特性。


<details>
  <summary>Details</summary>
Motivation: 研究动态分子开关的计算特性，探索其作为神经形态计算单元的理论基础，旨在开发兼具生物启发行为和数学稳定性的计算模型。

Method: 采用线性状态、非线性输入的微分方程模型，通过数学分析证明模型的精确可解性、收敛性和渐逝记忆特性。

Result: 模型展现出稳定的时间序列处理能力，支持在深度前馈和循环架构中使用动态分子开关作为计算单元，为神经形态计算提供理论支持。

Conclusion: 该研究为动态分子开关在神经形态计算中的应用奠定了理论基础，同时启发了开发更通用的精确可解模型来模拟物理器件和实现脑启发计算。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


### [8] [Classification errors distort findings in automated speech processing: examples and solutions from child-development research](https://arxiv.org/abs/2508.15637)
*Lucas Gautheron,Evan Kidd,Anton Malko,Marvin Lavechin,Alejandrina Cristia*

Main category: cs.LG

TL;DR: 本文提出贝叶斯方法来研究自动分类器错误对语言发展研究的影响，发现在LENA和ACLEW系统中分类错误会显著扭曲估计结果，并展示了贝叶斯校准方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴录音设备的普及，研究者越来越多地使用自动分析方法处理音频数据来研究儿童语言发展。虽然已有大量文献报告自动分类器的准确性，但关于分类错误对测量结果和统计推断的下游影响研究较少。

Method: 采用贝叶斯方法来研究算法错误对关键科学问题的影响，包括兄弟姐妹对儿童语言经验的影响以及儿童产出与输入之间的关联。在LENA和ACLEW系统的Voice Type Classifier两种自动分类器上进行测试。

Result: 发现分类错误会显著扭曲估计结果，例如自动注释低估了兄弟姐妹对成人输入的负面影响达20-80%，可能使其低于统计显著性阈值。贝叶斯校准方法在恢复无偏效应大小估计方面有效但不完美。

Conclusion: 分类错误会严重影响语言发展研究的统计推断，贝叶斯校准提供了一种有效的解决方案，但并非万无一失。这些发现适用于任何涉及事件检测和分类且错误率非零的分类器。

Abstract: With the advent of wearable recorders, scientists are increasingly turning to
automated methods of analysis of audio and video data in order to measure
children's experience, behavior, and outcomes, with a sizable literature
employing long-form audio-recordings to study language acquisition. While
numerous articles report on the accuracy and reliability of the most popular
automated classifiers, less has been written on the downstream effects of
classification errors on measurements and statistical inferences (e.g., the
estimate of correlations and effect sizes in regressions). This paper proposes
a Bayesian approach to study the effects of algorithmic errors on key
scientific questions, including the effect of siblings on children's language
experience and the association between children's production and their input.
In both the most commonly used \gls{lena}, and an open-source alternative (the
Voice Type Classifier from the ACLEW system), we find that classification
errors can significantly distort estimates. For instance, automated annotations
underestimated the negative effect of siblings on adult input by 20--80\%,
potentially placing it below statistical significance thresholds. We further
show that a Bayesian calibration approach for recovering unbiased estimates of
effect sizes can be effective and insightful, but does not provide a fool-proof
solution. Both the issue reported and our solution may apply to any classifier
involving event detection and classification with non-zero error rates.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis](https://arxiv.org/abs/2508.15189)
*Jiahao Xu,Changchang Yin,Odysseas Chatzipanagiotou,Diamantis Tsilimigras,Kevin Clear,Bingsheng Yao,Dakuo Wang,Timothy Pawlik,Ping Zhang*

Main category: cs.AI

TL;DR: 提出了首个开源手术伤口数据集SurgWound和基准测试，包含697张多类型手术伤口图像和8个临床属性标注。开发了三阶段学习框架WoundQwen，通过多模态大语言模型进行伤口特征预测、感染风险评估和综合报告生成。


<details>
  <summary>Details</summary>
Motivation: 手术部位感染是常见且昂贵的医疗相关感染，现有深度学习研究受限于数据隐私和专家标注成本高的问题，缺乏公开数据集和开源筛查工具。

Method: 1) 创建SurgWound数据集，包含697张手术伤口图像和8个临床属性标注；2) 建立包含视觉问答和报告生成的基准测试；3) 提出三阶段框架WoundQwen：第一阶段用5个MLLM预测伤口特征，第二阶段用2个MLLM进行感染风险评估，第三阶段整合结果生成综合报告。

Result: 开发了首个开源手术伤口数据集和基准测试，提出了能够分析详细伤口特征并提供个性化护理指导的三阶段框架。

Conclusion: 该研究为个性化伤口护理、及时干预和改善患者预后奠定了基础，解决了手术伤口筛查中的数据隐私和标注成本问题。

Abstract: Surgical site infection (SSI) is one of the most common and costly
healthcare-associated infections and and surgical wound care remains a
significant clinical challenge in preventing SSIs and improving patient
outcomes. While recent studies have explored the use of deep learning for
preliminary surgical wound screening, progress has been hindered by concerns
over data privacy and the high costs associated with expert annotation.
Currently, no publicly available dataset or benchmark encompasses various types
of surgical wounds, resulting in the absence of an open-source Surgical-Wound
screening tool. To address this gap: (1) we present SurgWound, the first
open-source dataset featuring a diverse array of surgical wound types. It
contains 697 surgical wound images annotated by 3 professional surgeons with
eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce
the first benchmark for surgical wound diagnosis, which includes visual
question answering (VQA) and report generation tasks to comprehensively
evaluate model performance. (3) Furthermore, we propose a three-stage learning
framework, WoundQwen, for surgical wound diagnosis. In the first stage, we
employ five independent MLLMs to accurately predict specific surgical wound
characteristics. In the second stage, these predictions serve as additional
knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess
infection risk and guide subsequent interventions. In the third stage, we train
a MLLM that integrates the diagnostic results from the previous two stages to
produce a comprehensive report. This three-stage framework can analyze detailed
surgical wound characteristics and provide subsequent instructions to patients
based on surgical images, paving the way for personalized wound care, timely
intervention, and improved patient outcomes.

</details>


### [10] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: 使用语言模型实时评估对话中的假社交关系线索，通过小型合成数据集验证能在前几轮对话中准确检测假社交关系形成


<details>
  <summary>Details</summary>
Motivation: 人与AI代理形成的假社交关系对人类健康造成严重危害，但防止这种动态具有挑战性

Method: 重新定向最先进语言模型构建实时响应评估框架，使用30个涵盖假社交、奇恶和中性对话的合成数据集进行测试

Result: 在宽松一致性规则下，迭代测试成功识别所有假社交对话且避免误报，检测通常在前几轮对话中完成

Conclusion: 评估代理可以为防止假社交关系提供可行的解决方案，为AI交互安全提供了初步证据

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)
*Lekshmi R Nair,Arun Sankar,Koninika Pal*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型自动挖掘时间常识的管道，并构建了TComQA数据集，该数据集在时间常识提取方面达到80%以上的精确度，训练出的模型在时间问答任务上优于现有数据集微调的LLM。


<details>
  <summary>Details</summary>
Motivation: 理解事件需要掌握其时间上下文，但时间常识在自然语言中很少明确提及，即使先进的大语言模型也难以生成需要时间常识推理的文本。自动挖掘时间常识有助于构建更强大的语言模型。

Method: 研究LLMs从文本中提取时间常识的能力，提出时间常识提取管道，利用LLMs自动挖掘时间常识，并从SAMSum和RealNews语料库构建TComQA数据集。

Result: TComQA数据集通过众包验证，在提取时间常识方面达到80%以上的精确度。使用TComQA训练的模型在时间问答任务上优于基于现有数据集微调的LLM。

Conclusion: 提出的时间常识提取管道和TComQA数据集有效提升了模型的时间常识推理能力，为构建更强大的语言模型提供了重要资源。

Abstract: Understanding events necessitates grasping their temporal context, which is
often not explicitly stated in natural language. For example, it is not a
trivial task for a machine to infer that a museum tour may last for a few
hours, but can not take months. Recent studies indicate that even advanced
large language models (LLMs) struggle in generating text that require reasoning
with temporal commonsense due to its infrequent explicit mention in text.
Therefore, automatically mining temporal commonsense for events enables the
creation of robust language models. In this work, we investigate the capacity
of LLMs to extract temporal commonsense from text and evaluate multiple
experimental setups to assess their effectiveness. Here, we propose a temporal
commonsense extraction pipeline that leverages LLMs to automatically mine
temporal commonsense and use it to construct TComQA, a dataset derived from
SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing
and achieves over 80\% precision in extracting temporal commonsense. The model
trained with TComQA also outperforms an LLM fine-tuned on existing dataset of
temporal question answering task.

</details>


### [12] [The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech](https://arxiv.org/abs/2508.15524)
*Naama Rivlin-Angert,Guy Mor-Lan*

Main category: cs.CL

TL;DR: 首个大规模政治去合法化话语(PDD)计算研究，构建希伯来语语料库并开发两阶段分类模型，发现PDD在过去30年显著上升，社交媒体比议会辩论更普遍，右翼政客使用更多


<details>
  <summary>Details</summary>
Motivation: 研究政治去合法化话语(PDD)的自动分析可行性，理解民主话语中象征性攻击政治实体规范有效性的现象

Method: 构建包含10,410句希伯来语句子的语料库，手动标注1,812个PDD实例；开发两阶段分类流水线，结合微调编码器模型和解码器LLM

Result: 最佳模型(DictaLM 2.0)在二元PDD检测上F1=0.74，在去合法化特征分类上macro-F1=0.67；发现PDD在过去30年显著上升，社交媒体比议会辩论更普遍，男性政客比女性使用更多，右翼行为者倾向更强

Conclusion: 自动化PDD分析对于理解民主话语具有可行性和重要价值，特别是在选举活动和重大政治事件期间表现出明显峰值

Abstract: We present the first large-scale computational study of political
delegitimization discourse (PDD), defined as symbolic attacks on the normative
validity of political entities. We curate and manually annotate a novel
Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches
(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which
1,812 instances (17.4\%) exhibit PDD and 642 carry additional annotations for
intensity, incivility, target type, and affective framing. We introduce a
two-stage classification pipeline combining finetuned encoder models and
decoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary
PDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization
characteristics. Applying this classifier to longitudinal and cross-platform
data, we see a marked rise in PDD over three decades, higher prevalence on
social media versus parliamentary debate, greater use by male than female
politicians, and stronger tendencies among right-leaning actors - with
pronounced spikes during election campaigns and major political events. Our
findings demonstrate the feasibility and value of automated PDD analysis for
understanding democratic discourse.

</details>


### [13] [Stemming -- The Evolution and Current State with a Focus on Bangla](https://arxiv.org/abs/2508.15711)
*Abhijit Paul,Mashiat Amin Farin,Sharif Md. Abdullah,Ahmedul Kabir,Zarif Masud,Shebuti Rayana*

Main category: cs.CL

TL;DR: 这篇论文对孜拉语词干提取技术进行了综述性评估，指出当前存在资源穷乏、实现缺乏和评估方法不足等问题，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 孜拉语作为世界第七大语言，在数字化表达方面较为落后，词干提取对于这种资源穷乏、语法变形丰富的语言至关重要，可以大大简化语言分析算法的复杂度。

Method: 论文采用综述性评估方法，对孜拉语词干提取技术进行了全面调研和分析，重点考察了现有文献中的缺口和问题。

Result: 研究发现孜拉语词干提取领域存在显著的研究空白，之前研究存在断层现象，可复现性强的实现方案缺乏，评估指标也不够相关和有效。

Conclusion: 论文建议应开发更健壮的孜拉语词干提取器，并将该领域的研究持续深入下去，以提升孜拉语的语言分析和处理能力。

Abstract: Bangla, the seventh most widely spoken language worldwide with 300 million
native speakers, faces digital under-representation due to limited resources
and lack of annotated datasets. Stemming, a critical preprocessing step in
language analysis, is essential for low-resource, highly-inflectional languages
like Bangla, because it can reduce the complexity of algorithms and models by
significantly reducing the number of words the algorithm needs to consider.
This paper conducts a comprehensive survey of stemming approaches, emphasizing
the importance of handling morphological variants effectively. While exploring
the landscape of Bangla stemming, it becomes evident that there is a
significant gap in the existing literature. The paper highlights the
discontinuity from previous research and the scarcity of accessible
implementations for replication. Furthermore, it critiques the evaluation
methodologies, stressing the need for more relevant metrics. In the context of
Bangla's rich morphology and diverse dialects, the paper acknowledges the
challenges it poses. To address these challenges, the paper suggests directions
for Bangla stemmer development. It concludes by advocating for robust Bangla
stemmers and continued research in the field to enhance language analysis and
processing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot](https://arxiv.org/abs/2508.14994)
*Murilo Vinicius da Silva,Matheus Hipolito Carvalho,Juliano Negri,Thiago Segreto,Gustavo J. G. Lahr,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出基于视觉姿态估计的直观四足机器人遥操作方法，通过外部摄像头检测操作者手腕位置，实时映射到机械臂控制，结合轨迹规划确保安全操作


<details>
  <summary>Details</summary>
Motivation: 在危险和远程环境中，四足机器人需要更安全和高效的操作。传统遥操作方法（如操纵杆）不直观且需要专业知识，认知负荷高，缺乏集成的障碍物检测功能，在受限或动态工作空间中碰撞风险高

Method: 利用基于视觉的姿态估计管道，使用外部摄像头和机器学习模型检测操作者手腕位置，将这些手腕运动实时映射为机械臂命令。集成轨迹规划器来检测和防止与障碍物及机械臂本身的碰撞

Result: 在真实机器人上验证了系统，展示了实时控制的鲁棒性能。该方法为工业应用提供了成本效益高的解决方案

Conclusion: 该遥操作方法在安全性、精确性和易用性至关重要的工业应用中提供了可靠且直观的机器人控制方案，特别适用于高风险环境

Abstract: In hazardous and remote environments, robotic systems perform critical tasks
demanding improved safety and efficiency. Among these, quadruped robots with
manipulator arms offer mobility and versatility for complex operations.
However, teleoperating quadruped robots is challenging due to the lack of
integrated obstacle detection and intuitive control methods for the robotic
arm, increasing collision risks in confined or dynamically changing workspaces.
Teleoperation via joysticks or pads can be non-intuitive and demands a high
level of expertise due to its complexity, culminating in a high cognitive load
on the operator. To address this challenge, a teleoperation approach that
directly maps human arm movements to the robotic manipulator offers a simpler
and more accessible solution. This work proposes an intuitive remote control by
leveraging a vision-based pose estimation pipeline that utilizes an external
camera with a machine learning-based model to detect the operator's wrist
position. The system maps these wrist movements into robotic arm commands to
control the robot's arm in real-time. A trajectory planner ensures safe
teleoperation by detecting and preventing collisions with both obstacles and
the robotic arm itself. The system was validated on the real robot,
demonstrating robust performance in real-time control. This teleoperation
approach provides a cost-effective solution for industrial applications where
safety, precision, and ease of use are paramount, ensuring reliable and
intuitive robotic control in high-risk environments.

</details>


### [15] [Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation](https://arxiv.org/abs/2508.15663)
*Nikita Kachaev,Andrei Spiridonov,Andrey Gorodetsky,Kirill Muravyev,Nikita Oskolkov,Aditya Narendra,Vlad Shakhuro,Dmitry Makarov,Aleksandr I. Panov,Polina Fedotova,Alexey K. Kovalev*

Main category: cs.RO

TL;DR: Kitchen-R是一个新颖的机器人基准测试，在模拟厨房环境中统一评估任务规划和低级控制，填补了高级语言指令跟随和低级机器人控制之间的评估空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在显著差距：高级语言指令跟随基准假设完美低级执行，而低级控制基准依赖简单单步指令，无法全面评估任务规划和物理执行都至关重要的集成系统。

Method: 基于Isaac Sim模拟器构建数字孪生厨房环境，包含500多个复杂语言指令，支持移动机械臂机器人。提供基于视觉语言模型的任务规划策略和基于扩散策略的低级控制策略基线方法，以及轨迹收集系统。

Result: Kitchen-R提供了三种评估模式的灵活框架：规划模块独立评估、控制策略独立评估，以及关键的系统集成评估。

Conclusion: 该基准测试填补了具身AI研究的关键空白，实现了对语言引导机器人代理更全面和现实的基准测试。

Abstract: Benchmarks are crucial for evaluating progress in robotics and embodied AI.
However, a significant gap exists between benchmarks designed for high-level
language instruction following, which often assume perfect low-level execution,
and those for low-level robot control, which rely on simple, one-step commands.
This disconnect prevents a comprehensive evaluation of integrated systems where
both task planning and physical execution are critical. To address this, we
propose Kitchen-R, a novel benchmark that unifies the evaluation of task
planning and low-level control within a simulated kitchen environment. Built as
a digital twin using the Isaac Sim simulator and featuring more than 500
complex language instructions, Kitchen-R supports a mobile manipulator robot.
We provide baseline methods for our benchmark, including a task-planning
strategy based on a vision-language model and a low-level control policy based
on diffusion policy. We also provide a trajectory collection system. Our
benchmark offers a flexible framework for three evaluation modes: independent
assessment of the planning module, independent assessment of the control
policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R
bridges a key gap in embodied AI research, enabling more holistic and realistic
benchmarking of language-guided robotic agents.

</details>
