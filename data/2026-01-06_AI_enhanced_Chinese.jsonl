{"id": "2601.00845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00845", "abs": "https://arxiv.org/abs/2601.00845", "authors": ["Lili Chen", "Wensheng Gan", "Shuang Liang", "Philip S. Yu"], "title": "Enhancing Temporal Awareness in LLMs for Temporal Point Processes", "comment": "preprint", "summary": "Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL", "AI": {"tldr": "\u63d0\u51fa\u4e86TPP-TAL\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3aLLMs\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u6765\u6539\u8fdb\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4f3c\u7136\u4f30\u8ba1\u548c\u4e8b\u4ef6\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u793e\u4ea4\u7cfb\u7edf\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u65f6\u95f4\u4fe1\u606f\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e94\u7528\u4e8e\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faTPP-TAL\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u5728\u5c06\u4fe1\u606f\u8f93\u5165LLM\u4e4b\u524d\uff0c\u663e\u5f0f\u5730\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u62fc\u63a5\u4e8b\u4ef6\u65f6\u95f4\u548c\u7c7b\u578b\u5d4c\u5165\u3002\u8fd9\u79cd\u65b9\u6cd5\u8ba9\u6a21\u578b\u80fd\u66f4\u597d\u5730\u611f\u77e5\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u4e8b\u4ef6\u4e0e\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u957f\u7a0b\u4ea4\u4e92\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cTPP-TAL\u5728\u65f6\u95f4\u4f3c\u7136\u4f30\u8ba1\u548c\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5e26\u6765\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u7a81\u663e\u4e86\u589e\u5f3aLLMs\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u5bf9\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002", "conclusion": "TPP-TAL\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00853", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00853", "abs": "https://arxiv.org/abs/2601.00853", "authors": ["Sameer Rahil", "Zain Abdullah Ahmad", "Talha Asif"], "title": "FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments", "comment": "13 pages, 27 figures", "summary": "Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \\textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.", "AI": {"tldr": "FedSCAM\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\u548c\u57fa\u4e8e\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u5206\u6570\u7684\u805a\u5408\u6743\u91cd\uff0c\u89e3\u51b3\u975eIID\u6570\u636e\u4e0b\u7684\u6536\u655b\u548c\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff08\u7279\u522b\u662f\u975eIID\u6807\u7b7e\u5206\u5e03\uff09\u5bf9\u6536\u655b\u548c\u6cdb\u5316\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684SAM\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u5ba2\u6237\u7aef\u4f7f\u7528\u7edf\u4e00\u7684\u6270\u52a8\u534a\u5f84\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51faFedSCAM\u7b97\u6cd5\uff1a1) \u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8ba1\u7b97\u5f02\u8d28\u6027\u5ea6\u91cf\uff1b2) \u6839\u636e\u5f02\u8d28\u6027\u5206\u6570\u53cd\u5411\u8c03\u5236SAM\u6270\u52a8\u534a\u5f84\uff0c\u9632\u6b62\u9ad8\u65b9\u5dee\u5ba2\u6237\u7aef\u7834\u574f\u5168\u5c40\u6a21\u578b\u7a33\u5b9a\u6027\uff1b3) \u5f15\u5165\u5f02\u8d28\u6027\u611f\u77e5\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\uff0c\u4f18\u5148\u8003\u8651\u4e0e\u5168\u5c40\u4f18\u5316\u65b9\u5411\u4e00\u81f4\u7684\u5ba2\u6237\u7aef\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e0d\u540c\u7a0b\u5ea6\u7684\u72c4\u5229\u514b\u96f7\u6807\u7b7e\u504f\u659c\u8fdb\u884c\u5b9e\u9a8c\uff0cFedSCAM\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6d4b\u8bd5\u51c6\u786e\u7387\u65b9\u9762\u4e0eFedSAM\u3001FedLESAM\u7b49\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "FedSCAM\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6270\u52a8\u534a\u5f84\u548c\u5f02\u8d28\u6027\u611f\u77e5\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.01143", "categories": ["cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.01143", "abs": "https://arxiv.org/abs/2601.01143", "authors": ["Peng Chen"], "title": "KOS-TL (Knowledge Operation System Type Logic)", "comment": null, "summary": "This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\\langle \u03a3, \\textsf{Ev}, \u0394\\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-L\u00f6f type theory, KOS-TL enables the construction of \"proof-carrying knowledge,\" where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.", "AI": {"tldr": "KOS-TL\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u7684\u77e5\u8bc6\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u903b\u8f91\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u81ea\u4e3b\u53ef\u6267\u884c\u77e5\u8bc6\u7cfb\u7edf\u63d0\u4f9b\u4e25\u683c\u7684\u903b\u8f91\u57fa\u7840\uff0c\u7edf\u4e00\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\uff0c\u5e76\u786e\u4fdd\u72b6\u6001\u6f14\u5316\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u8868\u793a\u6a21\u578b\u5b58\u5728\u9759\u6001\u7b26\u53f7\u903b\u8f91\u4e0e\u52a8\u6001\u7cfb\u7edf\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\u7684\u4e25\u683c\u903b\u8f91\u6846\u67b6\u6765\u652f\u6301\u81ea\u4e3b\u53ef\u6267\u884c\u77e5\u8bc6\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\uff0c\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u6838\u5fc3\u5c42\uff08\u9759\u6001\u7c7b\u578b\u5b87\u5b99\u548c\u6784\u9020\u539f\u8bed\uff09\u3001\u5185\u6838\u5c42\uff08\u4e8b\u4ef6\u9a71\u52a8\u72b6\u6001\u6f14\u5316\u673a\u5236\uff09\u3001\u8fd0\u884c\u65f6\u5c42\uff08\u7269\u7406\u4fe1\u53f7\u4e0e\u903b\u8f91\u8bc1\u636e\u7684\u53cc\u5411\u7cbe\u5316\uff09\u3002\u6574\u5408Davidsonian\u4e8b\u4ef6\u8bed\u4e49\u548cMartin-L\u00f6f\u7c7b\u578b\u7406\u8bba\uff0c\u5b9e\u73b0\"\u8bc1\u660e\u643a\u5e26\u77e5\u8bc6\"\u3002", "result": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u7cfb\u7edf\u7684\u64cd\u4f5c\u8bed\u4e49\uff0c\u8bc1\u660e\u4e86\u5173\u952e\u5143\u7406\u8bba\u6027\u8d28\uff08Progress\u548cEvolutionary Consistency\uff09\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u8fde\u7eed\u72b6\u6001\u8f6c\u6362\u4e2d\u4fdd\u6301\u903b\u8f91\u81ea\u6d3d\u4e14\u65e0\u6b7b\u9501\u72b6\u6001\u3002\u5728\u5de5\u4e1a\u8ffd\u6eaf\u548c\u8de8\u5883\u91d1\u878d\u5408\u89c4\u7b49\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "KOS-TL\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u81ea\u4e3b\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u5f62\u5f0f\u5316\u53ef\u9a8c\u8bc1\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u7edf\u4e00\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\uff0c\u5b9e\u73b0\u4e86\u77e5\u8bc6\u7cfb\u7edf\u7684\u4e25\u683c\u903b\u8f91\u57fa\u7840\u548c\u52a8\u6001\u53ef\u6267\u884c\u6027\u3002"}}
{"id": "2601.00994", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.00994", "abs": "https://arxiv.org/abs/2601.00994", "authors": ["Michael Bao"], "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)", "summary": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "AI": {"tldr": "ElecTwit\u662f\u4e00\u4e2a\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u4e2d\u591a\u667a\u80fd\u4f53\u8bf4\u670d\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u53d1\u73b0LLMs\u4f7f\u7528\u4e8625\u79cd\u8bf4\u670d\u6280\u5de7\uff0c\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u5f71\u54cd\u8bf4\u670d\u52a8\u6001\uff0c\u5e76\u89c2\u5bdf\u5230\"\u771f\u76f8\u5185\u6838\"\u6d88\u606f\u548c\"\u58a8\u6c34\"\u75f4\u8ff7\u7b49\u72ec\u7279\u73b0\u8c61\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bf4\u670d\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\uff0c\u514b\u670d\u4ee5\u5f80\u7814\u7a76\u4e2d\u57fa\u4e8e\u6e38\u620f\u6a21\u62df\u7684\u5c40\u9650\u6027\uff0c\u5728\u66f4\u771f\u5b9e\u7684\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u7684\u8bf4\u670d\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86ElecTwit\u6a21\u62df\u6846\u67b6\uff0c\u5728\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u7684\u903c\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u591a\u79cdLLM\u6a21\u578b\uff0c\u5206\u6790\u5b83\u4eec\u4f7f\u7528\u7684\u8bf4\u670d\u6280\u5de7\u548c\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u89c2\u5bdf\u523025\u79cd\u7279\u5b9a\u7684\u8bf4\u670d\u6280\u5de7\u88ab\u5927\u591a\u6570\u6d4b\u8bd5\u7684LLM\u5e7f\u6cdb\u4f7f\u7528\uff0c\u8303\u56f4\u8d85\u8fc7\u4ee5\u5f80\u62a5\u9053\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u6280\u5de7\u4f7f\u7528\u548c\u6574\u4f53\u8bf4\u670d\u8f93\u51fa\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u53d1\u73b0\u4e86\"\u771f\u76f8\u5185\u6838\"\u6d88\u606f\u548c\"\u58a8\u6c34\"\u75f4\u8ff7\uff08\u667a\u80fd\u4f53\u96c6\u4f53\u8981\u6c42\u4e66\u9762\u8bc1\u636e\uff09\u7b49\u72ec\u7279\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8bc4\u4f30\u6709\u8bf4\u670d\u529b\u7684LLM\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u786e\u4fdd\u5bf9\u9f50\u5e76\u9632\u6b62\u5371\u9669\u7ed3\u679c\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u5982\u4f55\u5f71\u54cd\u73b0\u5b9e\u793e\u4ea4\u6a21\u62df\u4e2d\u7684\u52a8\u6001\u3002"}}
{"id": "2601.01651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01651", "abs": "https://arxiv.org/abs/2601.01651", "authors": ["Yucheng Xu", "Xiaofeng Mao", "Elle Miller", "Xinyu Yi", "Yang Li", "Zhibin Li", "Robert B. Fisher"], "title": "DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos", "comment": null, "summary": "This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.", "AI": {"tldr": "DemoBot\uff1a\u4ece\u5355\u6bb5\u672a\u6807\u6ce8RGB-D\u89c6\u9891\u4e2d\u5b66\u4e60\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u6280\u80fd\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u8fd0\u52a8\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7cbe\u70bc\uff0c\u5b9e\u73b0\u957f\u65f6\u7a0b\u53cc\u624b\u88c5\u914d\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u6280\u80fd\uff0c\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u76f4\u63a5\u4ece\u5355\u6bb5\u672a\u6807\u6ce8\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\u5b66\u4e60\u590d\u6742\u64cd\u4f5c\u6280\u80fd\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "1. \u4ece\u539f\u59cbRGB-D\u89c6\u9891\u4e2d\u63d0\u53d6\u53cc\u624b\u548c\u7269\u4f53\u7684\u7ed3\u6784\u5316\u8fd0\u52a8\u8f68\u8ff9\u4f5c\u4e3a\u8fd0\u52a8\u5148\u9a8c\uff1b2. \u63d0\u51fa\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\uff1a\u57fa\u4e8e\u65f6\u95f4\u5206\u6bb5\u7684RL\u786e\u4fdd\u72b6\u6001\u4e0e\u6f14\u793a\u7684\u65f6\u95f4\u5bf9\u9f50\uff0c\u6210\u529f\u95e8\u63a7\u91cd\u7f6e\u7b56\u7565\u5e73\u8861\u6280\u80fd\u7cbe\u70bc\u4e0e\u540e\u7eed\u9636\u6bb5\u63a2\u7d22\uff0c\u4e8b\u4ef6\u9a71\u52a8\u5956\u52b1\u8bfe\u7a0b\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u6307\u5bfc\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u5b66\u4e60\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u957f\u65f6\u7a0b\u540c\u6b65\u548c\u5f02\u6b65\u53cc\u624b\u88c5\u914d\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u4ece\u4eba\u7c7b\u89c6\u9891\u76f4\u63a5\u83b7\u53d6\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DemoBot\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u5355\u6bb5\u672a\u6807\u6ce8\u4eba\u7c7b\u89c6\u9891\u76f4\u63a5\u5b66\u4e60\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u6280\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5904\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u7a0b\u590d\u6742\u4efb\u52a1\u7684\u6280\u80fd\u83b7\u53d6\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01726", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01726", "abs": "https://arxiv.org/abs/2601.01726", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions", "comment": "10 pages, 7 figures", "summary": "Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8eMRI\u626b\u63cf\u4eea\u73af\u5883\u7684\u673a\u5668\u4eba\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u8ba1\u7b97\u5904\u7406MR\u56fe\u50cf\u5efa\u7acb\u8840\u7ba1\u865a\u62df\u8def\u5f84\uff0c\u751f\u6210\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\u63a7\u5236\u8bbe\u5907\uff0c\u63d0\u5347\u8840\u7ba1\u5185\u4ecb\u5165\u624b\u672f\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "MRI\u626b\u63cf\u4eea\u5f3a\u78c1\u573a\u73af\u5883\u4e0b\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u6311\u6218\uff0c\u9700\u8981\u589e\u5f3aMRI\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u8f85\u52a9\u8bbe\u5907\u5f15\u5bfc\u8840\u7ba1\u5185\u4ecb\u5165\u624b\u672f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eQt\u6846\u67b6\u548cC/C++\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5305\u62ec\u8ba1\u7b97\u5355\u5143\u548c\u7528\u6237\u754c\u9762\uff0c\u4e0eMRI\u626b\u63cf\u4eea\u65e0\u7f1d\u96c6\u6210\u3002\u7cfb\u7edf\u5904\u7406MR\u56fe\u50cf\u63cf\u7ed8\u8840\u7ba1\u7f51\u7edc\uff0c\u5efa\u7acb\u865a\u62df\u8def\u5f84\u548c\u8fb9\u754c\uff0c\u751f\u6210\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\u63a7\u5236\u8bbe\u5907\uff0c\u8003\u8651\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\u3001\u5b89\u5168\u89c4\u8303\u548c\u8840\u6d41\u7279\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u521b\u5efa\u9488\u5bf9\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\u548c\u5b89\u5168\u89c4\u8303\u7684\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\uff0c\u9002\u5e94\u4e0d\u540c\u8840\u6d41\u7279\u6027\u5b9e\u73b0\u7cbe\u7ec6\u5bfc\u822a\u3002\u5efa\u6a21\u65b9\u9762\u8bc4\u4f30\u9884\u8bbe\u8840\u7ba1\u8def\u5f84\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u8840\u7ba1\u5185\u624b\u672f\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4ee3\u8868\u4e86\u6210\u50cf\u6280\u672f\u4e0e\u673a\u5668\u4eba\u8f85\u52a9\u878d\u5408\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u4e13\u95e8\u8f6f\u4ef6\u6a21\u5757\u663e\u8457\u589e\u5f3a\u4e86\u8840\u7ba1\u5185\u624b\u672f\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\uff0c\u662fMRI\u5f15\u5bfc\u673a\u5668\u4eba\u8f85\u52a9\u8840\u7ba1\u5185\u4ecb\u5165\u7684\u91cd\u8981\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2601.01762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01762", "abs": "https://arxiv.org/abs/2601.01762", "authors": ["Yanhao Wu", "Haoyang Zhang", "Fei He", "Rui Wu", "Congpei Qiu", "Liang Gao", "Wei Ke", "Tong Zhang"], "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving", "comment": "underreview", "summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ea7\u8054\u6846\u67b6\uff0c\u5c06\u7eb5\u5411\u89c4\u5212\u663e\u5f0f\u5730\u5efa\u7acb\u5728\u884c\u9a76\u8def\u5f84\u4e0a\uff0c\u5b9e\u73b0\u534f\u8c03\u7684\u6a2a\u5411\u548c\u7eb5\u5411\u89c4\u5212\uff0c\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\u5c06\u6a2a\u5411\u548c\u7eb5\u5411\u9884\u6d4b\u5e76\u884c\u5904\u7406\uff0c\u5bfc\u81f4\uff1a1) \u89c4\u5212\u7684\u8def\u5f84\u548c\u901f\u5ea6\u534f\u8c03\u5931\u8d25\uff1b2) \u672a\u5145\u5206\u5229\u7528\u884c\u9a76\u8def\u5f84\u4f5c\u4e3a\u7eb5\u5411\u89c4\u5212\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u9020\u6210\u9759\u6001\u4fe1\u606f\u5197\u4f59\u7f16\u7801", "method": "\u63d0\u51fa\u7ea7\u8054\u6846\u67b6\uff0c\u5c06\u7eb5\u5411\u89c4\u5212\u663e\u5f0f\u5730\u5efa\u7acb\u5728\u884c\u9a76\u8def\u5f84\u4e0a\uff0c\u91c7\u7528\u8def\u5f84\u6761\u4ef6\u5316\u516c\u5f0f\uff0c\u6cbf\u884c\u9a76\u8def\u5f84\u9884\u6d4b\u7eb5\u5411\u4f4d\u79fb\u800c\u975e\u5b8c\u65742D\u8f68\u8ff9\u70b9\uff1b\u5f15\u5165\u9762\u5411\u89c4\u5212\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6a21\u62df\u8f66\u8f86\u5207\u5165\u7b49\u5b89\u5168\u5173\u952e\u4e8b\u4ef6", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\uff1a\u9a7e\u9a76\u5206\u657089.07\uff0c\u6210\u529f\u738773.18%\uff0c\u663e\u8457\u6539\u5584\u4e86\u534f\u8c03\u6027\u548c\u5b89\u5168\u6027", "conclusion": "\u901a\u8fc7\u5c06\u7eb5\u5411\u89c4\u5212\u663e\u5f0f\u5730\u5efa\u7acb\u5728\u884c\u9a76\u8def\u5f84\u4e0a\uff0c\u5b9e\u73b0\u4e86\u66f4\u534f\u8c03\u3001\u66f4\u5b89\u5168\u7684\u6a2a\u5411\u548c\u7eb5\u5411\u89c4\u5212\uff0c\u7b80\u5316\u4e86\u7eb5\u5411\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7684\u5e94\u5bf9\u80fd\u529b"}}
{"id": "2601.01022", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01022", "abs": "https://arxiv.org/abs/2601.01022", "authors": ["Shiao Wang", "Xiao Wang", "Haonan Zhao", "Jiarui Xu", "Bo Jiang", "Lin Zhu", "Xin Zhao", "Yonghong Tian", "Jin Tang"], "title": "Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking", "comment": null, "summary": "Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9891\u57df\u65e9\u671f\u878d\u5408\u7684RGB-\u4e8b\u4ef6\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7FFT\u5c06RGB\u548c\u4e8b\u4ef6\u6a21\u6001\u8f6c\u6362\u5230\u9891\u57df\uff0c\u5229\u7528\u632f\u5e45\u548c\u76f8\u4f4d\u6ce8\u610f\u529b\u9009\u62e9\u6027\u878d\u5408\u9ad8\u9891\u4e8b\u4ef6\u4fe1\u606f\uff0c\u7ed3\u5408\u8fd0\u52a8\u5f15\u5bfc\u7684\u7a7a\u95f4\u7a00\u758f\u5316\u6a21\u5757\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709RGB-\u4e8b\u4ef6\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u7279\u5f81\u7ea7\u878d\u5408\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u8fd0\u52a8\u654f\u611f\u7279\u6027\uff0c\u540c\u65f6\u5bf9\u4f4e\u4fe1\u606f\u533a\u57df\u8fdb\u884c\u7edf\u4e00\u5904\u7406\uff0c\u5bfc\u81f4\u4e3b\u5e72\u7f51\u7edc\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "1) \u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5c06RGB\u548c\u4e8b\u4ef6\u6a21\u6001\u4ece\u7a7a\u95f4\u57df\u8f6c\u6362\u5230\u9891\u57df\uff0c\u89e3\u8026\u632f\u5e45\u548c\u76f8\u4f4d\u5206\u91cf\uff1b2) \u901a\u8fc7\u632f\u5e45\u548c\u76f8\u4f4d\u6ce8\u610f\u529b\u9009\u62e9\u6027\u878d\u5408\u9ad8\u9891\u4e8b\u4ef6\u4fe1\u606f\u5230RGB\u6a21\u6001\uff1b3) \u8fd0\u52a8\u5f15\u5bfc\u7684\u7a7a\u95f4\u7a00\u758f\u5316\u6a21\u5757\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u8fd0\u52a8\u654f\u611f\u6027\u6355\u83b7\u76ee\u6807\u8fd0\u52a8\u7ebf\u7d22\u4e0e\u7a7a\u95f4\u6982\u7387\u5206\u5e03\u5173\u7cfb\uff0c\u8fc7\u6ee4\u4f4e\u4fe1\u606f\u533a\u57df\uff1b4) \u7a00\u758f\u7684\u76ee\u6807\u76f8\u5173\u7279\u5f81\u8f93\u5165\u4e3b\u5e72\u7f51\u7edc\u5b66\u4e60\uff0c\u8ddf\u8e2a\u5934\u9884\u6d4b\u6700\u7ec8\u76ee\u6807\u4f4d\u7f6e\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684RGB-\u4e8b\u4ef6\u8ddf\u8e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08FE108\u3001FELT\u3001COESOT\uff09\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u9891\u57df\u65e9\u671f\u878d\u5408\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u8fd0\u52a8\u654f\u611f\u7279\u6027\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u878d\u5408\u9ad8\u9891\u4fe1\u606f\u548c\u7a7a\u95f4\u7a00\u758f\u5316\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5728RGB-\u4e8b\u4ef6\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\u3002"}}
{"id": "2601.01816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01816", "abs": "https://arxiv.org/abs/2601.01816", "authors": ["Chris Duffey"], "title": "Admissibility Alignment", "comment": "24 pages, 2 figures, 2 tables.. Decision-theoretic alignment under uncertainty", "summary": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.", "AI": {"tldr": "\u63d0\u51fa\"\u53ef\u91c7\u7eb3\u6027\u5bf9\u9f50\"\u6846\u67b6\uff0c\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u7ed3\u679c\u5206\u5e03\u7684\u53ef\u91c7\u7eb3\u884c\u52a8\u548c\u51b3\u7b56\u9009\u62e9\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7MAP-AI\u67b6\u6784\u5b9e\u73b0\u6982\u7387\u5316\u3001\u51b3\u7b56\u7406\u8bba\u7684\u5bf9\u9f50\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5c06\u5bf9\u9f50\u89c6\u4e3a\u9759\u6001\u6216\u4e8c\u5143\u6761\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u679c\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\u7684\u660e\u786e\u5efa\u6a21\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30\u7b56\u7565\u5728\u7ed3\u679c\u5206\u5e03\u548c\u5c3e\u90e8\u4e8b\u4ef6\u4e2d\u884c\u4e3a\u7684\u5b9e\u7528\u6846\u67b6\u3002", "method": "\u63d0\u51faMAP-AI\uff08\u8499\u7279\u5361\u6d1b\u5bf9\u9f50\u7b56\u7565\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7ed3\u679c\u5206\u5e03\u548c\u53ef\u91c7\u7eb3\u6027\u63a7\u5236\u7684\u7b56\u7565\u9009\u62e9\u6765\u5b9e\u65bd\u5bf9\u9f50\u3002\u8be5\u6846\u67b6\u5728\u53ef\u4fe1\u672a\u6765\u96c6\u5408\u4e2d\u8bc4\u4f30\u51b3\u7b56\u7b56\u7565\uff0c\u660e\u786e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u679c\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\u3002", "result": "\u5efa\u7acb\u4e86\u8bc4\u4f30\u5bf9\u9f50\u7684\u5206\u5e03\u5c5e\u6027\u65b9\u6cd5\uff0c\u5305\u62ec\u671f\u671b\u6548\u7528\u3001\u65b9\u5dee\u3001\u5c3e\u90e8\u98ce\u9669\u548c\u4e0d\u5bf9\u9f50\u6982\u7387\uff0c\u800c\u975e\u4f20\u7edf\u7684\u51c6\u786e\u6027\u6216\u6392\u540d\u6027\u80fd\u3002\u63d0\u4f9b\u4e86\u53ef\u6267\u884c\u7684\u65b9\u6cd5\u8bba\u6765\u8bc4\u4f30\u4f01\u4e1aAI\u7cfb\u7edf\u4e2d\u7684\u4fe1\u4efb\u548c\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6cbb\u7406AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u5176\u5f71\u54cd\u4e0d\u662f\u7531\u4e2a\u4f53\u9884\u6d4b\u51b3\u5b9a\uff0c\u800c\u662f\u7531\u7b56\u7565\u5728\u5206\u5e03\u548c\u5c3e\u90e8\u4e8b\u4ef6\u4e2d\u7684\u884c\u4e3a\u51b3\u5b9a\u3002\u5206\u5e03\u5bf9\u9f50\u8bc4\u4f30\u53ef\u4ee5\u96c6\u6210\u5230\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u7684\u53ef\u91c7\u7eb3\u6027\u63a7\u5236\u884c\u52a8\u9009\u62e9\u3002"}}
{"id": "2601.01584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01584", "abs": "https://arxiv.org/abs/2601.01584", "authors": ["Jakub Hoscilowicz"], "title": "Steerability of Instrumental-Convergence Tendencies in LLMs", "comment": "Code is available at https://github.com/j-hoscilowicz/instrumental_steering", "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5e76\u975e\u8d1f\u76f8\u5173\uff0c\u533a\u5206\u4e86\u6388\u6743\u4e0e\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u53cd\u5de5\u5177\u6027\u63d0\u793a\u80fd\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u884c\u4e3a\u8f93\u51fa\u3002", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u533a\u5206\u6388\u6743\u64cd\u63a7\u6027\uff08\u5f00\u53d1\u8005\u5b9e\u73b0\u9884\u671f\u884c\u4e3a\uff09\u548c\u975e\u6388\u6743\u64cd\u63a7\u6027\uff08\u653b\u51fb\u8005\u5f15\u53d1\u7981\u6b62\u884c\u4e3a\uff09\uff0c\u63ed\u793a\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\u3002", "method": "\u4f7f\u7528Qwen3\u6a21\u578b\uff084B/30B\uff1b\u57fa\u7840\u7248/\u6307\u5bfc\u7248/\u601d\u8003\u7248\uff09\u548cInstrumentalEval\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4eb2\u5de5\u5177\u6027\u548c\u53cd\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\uff0c\u6d4b\u91cf\u5de5\u5177\u6027\u6536\u655b\u884c\u4e3a\uff08\u5982\u5173\u673a\u56de\u907f\u3001\u6b3a\u9a97\u3001\u81ea\u6211\u590d\u5236\uff09\u7684\u53d8\u5316\u3002", "result": "\u80fd\u529b\u66f4\u9ad8\u7684\u7cfb\u7edf\u5e76\u4e0d\u4e00\u5b9a\u66f4\u96be\u64cd\u63a7\uff1b\u53cd\u5de5\u5177\u6027\u63d0\u793a\u80fd\u663e\u8457\u964d\u4f4e\u5de5\u5177\u6027\u6536\u655b\u884c\u4e3a\uff08Qwen3-30B\u6307\u5bfc\u7248\u4ece81.69%\u964d\u81f32.82%\uff09\uff1b\u5728\u53cd\u5de5\u5177\u6027\u63d0\u793a\u4e0b\uff0c\u66f4\u5927\u7684\u5bf9\u9f50\u6a21\u578b\u4ea7\u751f\u7684\u6536\u655b\u884c\u4e3a\u66f4\u5c11\u3002", "conclusion": "\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u5b89\u5168\u4e0e\u5b89\u5168\u7684\u6839\u672c\u77db\u76fe\uff1a\u5b89\u5168\u9700\u8981\u9ad8\u53ef\u64cd\u63a7\u6027\u6765\u5b9e\u65bd\u63a7\u5236\uff0c\u800c\u5b89\u5168\u9700\u8981\u4f4e\u53ef\u64cd\u63a7\u6027\u4ee5\u9632\u6b62\u6076\u610f\u884c\u4e3a\u3002\u5f53\u524d\u5f00\u653e\u6743\u91cd\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u548c\u5bf9\u6297\u6027\u63d0\u793a\u5177\u6709\u9ad8\u5ea6\u53ef\u64cd\u63a7\u6027\uff0c\u8fd9\u65e2\u662f\u98ce\u9669\u4e5f\u662f\u673a\u9047\u3002"}}
{"id": "2601.01014", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01014", "abs": "https://arxiv.org/abs/2601.01014", "authors": ["Haoran Su", "Chenyu You"], "title": "Geometric and Dynamic Scaling in Deep Transformers", "comment": "Research Proposal Only", "summary": "Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTransformer\u6df1\u5ea6\u589e\u52a0\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\u662f\u51e0\u4f55\u95ee\u9898\uff0c\u800c\u975e\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u51e0\u4f55\u7ea6\u675f\u7684MGT\u67b6\u6784\u6765\u89e3\u51b3", "motivation": "\u5c3d\u7ba1Transformer\u7ecf\u9a8c\u4e0a\u6210\u529f\uff0c\u4f46\u63a8\u5411\u6781\u7aef\u6df1\u5ea6\u65f6\u4f1a\u51fa\u73b0\u8868\u793a\u5197\u4f59\u3001\u79e9\u4e22\u5931\u548c\u5d29\u6e83\u7684\u6096\u8bba\u3002\u73b0\u6709\u89e3\u91ca\u4e3b\u8981\u5f52\u56e0\u4e8e\u4f18\u5316\u4e0d\u7a33\u5b9a\u6216\u68af\u5ea6\u6d88\u5931\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u65e0\u6cd5\u8bf4\u660e\u4e3a\u4ec0\u4e48\u5728\u73b0\u4ee3\u5f52\u4e00\u5316\u548c\u521d\u59cb\u5316\u65b9\u6848\u4e0b\u5d29\u6e83\u4ecd\u7136\u5b58\u5728\u3002\u4f5c\u8005\u8ba4\u4e3a\u6df1\u5c42Transformer\u7684\u5d29\u6e83\u672c\u8d28\u4e0a\u662f\u51e0\u4f55\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6b63\u4ea4\u539f\u5219\uff1a1) \u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\uff0c\u9650\u5236\u6b8b\u5dee\u66f4\u65b0\u5230\u6709\u6548\u7684\u5c40\u90e8\u5207\u5411\u65b9\u5411\uff0c\u9632\u6b62\u4e0d\u53d7\u63a7\u5236\u7684\u6d41\u5f62\u6f02\u79fb\uff1b2) \u6df1\u5ea6delta\u5b66\u4e60\uff0c\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u975e\u5355\u8c03\u66f4\u65b0\uff0c\u80fd\u591f\u53cd\u5c04\u548c\u64e6\u9664\u5197\u4f59\u7279\u5f81\u800c\u975e\u65e0\u6761\u4ef6\u79ef\u7d2f\u3002\u8fd9\u4e9b\u673a\u5236\u89e3\u8026\u4e86\u7279\u5f81\u66f4\u65b0\u7684\u65b9\u5411\u548c\u7b26\u53f7\uff0c\u5b9e\u73b0\u4e86\u8de8\u6df1\u5ea6\u7684\u7a33\u5b9a\u51e0\u4f55\u6f14\u5316\u3002\u7531\u6b64\u4ea7\u751f\u7684\u67b6\u6784\u79f0\u4e3a\u6d41\u5f62\u51e0\u4f55Transformer(MGT)\u3002", "result": "\u5206\u6790\u9884\u6d4b\uff0c\u5f3a\u5236\u51e0\u4f55\u6709\u6548\u6027\u540c\u65f6\u5141\u8bb8\u52a8\u6001\u64e6\u9664\u5bf9\u4e8e\u907f\u514d\u8d85\u6df1\u5c42\u7f51\u7edc\u4e2d\u7684\u79e9\u5d29\u6e83\u81f3\u5173\u91cd\u8981\u3002\u4f5c\u8005\u6982\u8ff0\u4e86\u8d85\u8fc7100\u5c42\u7684Transformer\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u6d4b\u8bd5\u51e0\u4f55\u800c\u975e\u6df1\u5ea6\u672c\u8eab\u662f\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\u7684\u5047\u8bbe\u3002", "conclusion": "\u6df1\u5c42Transformer\u7684\u5d29\u6e83\u662f\u51e0\u4f55\u95ee\u9898\u800c\u975e\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u548c\u52a8\u6001\u64e6\u9664\u673a\u5236\u53ef\u4ee5\u89e3\u51b3\u8868\u793a\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u8d85\u6df1\u5c42\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.01685", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01685", "abs": "https://arxiv.org/abs/2601.01685", "authors": ["Jinwei Hu", "Xinmiao Huang", "Youcheng Sun", "Yi Dong", "Xiaowei Huang"], "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage", "comment": "Under Review", "summary": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8ba4\u77e5\u5408\u8c0b\u653b\u51fb\uff0c\u5229\u7528LLM\u7684\u8fc7\u5ea6\u601d\u8003\u503e\u5411\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u5728\u516c\u5f00\u6e20\u9053\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u8ff0\uff0c\u5bfc\u81f4\u53d7\u5bb3\u8005\u5185\u5316\u5e76\u4f20\u64ad\u865a\u5047\u7ed3\u8bba\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u4ee3\u7406\u8f6c\u53d8\uff0c\u5176\u63a8\u7406\u80fd\u529b\u5f15\u5165\u4e86\u4e00\u4e2a\u610f\u5916\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u653b\u51fb\u901a\u5e38\u4f9d\u8d56\u9690\u853d\u901a\u4fe1\u3001\u540e\u95e8\u6216\u4f2a\u9020\u6587\u6863\uff0c\u800c\u672c\u6587\u7814\u7a76\u5982\u4f55\u4ec5\u901a\u8fc7\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u5728\u516c\u5f00\u6e20\u9053\u5b9e\u65bd\u8ba4\u77e5\u5408\u8c0b\u653b\u51fb\u3002", "method": "\u63d0\u51faGenerative Montage\u6846\u67b6\uff0c\u5305\u542bWriter-Editor-Director\u4e09\u4e2a\u89d2\u8272\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8fa9\u8bba\u548c\u534f\u8c03\u53d1\u5e03\u8bc1\u636e\u7247\u6bb5\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u8ff0\u3002\u5f00\u53d1CoPHEME\u6570\u636e\u96c6\uff08\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u8c23\u8a00\u4e8b\u4ef6\uff09\uff0c\u572814\u4e2aLLM\u5bb6\u65cf\u4e2d\u6a21\u62df\u653b\u51fb\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u5728\u4e13\u6709\u6a21\u578b\u4e2d\u8fbe74.4%\uff0c\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe70.6%\u3002\u53cd\u76f4\u89c9\u7684\u662f\uff0c\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u53cd\u800c\u589e\u52a0\u6613\u53d7\u653b\u51fb\u6027\uff0c\u63a8\u7406\u4e13\u7528\u6a21\u578b\u6bd4\u57fa\u7840\u6a21\u578b\u66f4\u6613\u53d7\u653b\u51fb\u3002\u865a\u5047\u4fe1\u5ff5\u4f1a\u5411\u4e0b\u6e38\u4f20\u64ad\uff0c\u6b3a\u9a97\u7387\u8d85\u8fc760%\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u5b58\u5728\u793e\u4f1a\u6280\u672f\u6f0f\u6d1e\uff0c\u8ba4\u77e5\u5408\u8c0b\u653b\u51fb\u5229\u7528\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u5c31\u80fd\u6709\u6548\u64cd\u7eb5\u4fe1\u5ff5\uff0c\u4e14\u63a8\u7406\u80fd\u529b\u8d8a\u5f3a\u7684\u6a21\u578b\u8d8a\u8106\u5f31\uff0c\u9700\u8981\u65b0\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2601.02170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02170", "abs": "https://arxiv.org/abs/2601.02170", "authors": ["Haolang Lu", "Minghui Pan", "Ripeng Li", "Guoshun Nan", "Jialin Zhuang", "Zijie Zhao", "Zhongxiang Sun", "Kun Wang", "Yang Liu"], "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning", "comment": null, "summary": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u89c6\u4e3a\u6f14\u5316\u6f5c\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u6765\u8ffd\u8e2a\u63a8\u7406\u72b6\u6001\u7684\u5168\u5c40\u6f14\u5316\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53ef\u89e3\u91ca\u7684\u5e7b\u89c9\u68c0\u6d4b\u3002", "motivation": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u867d\u7136\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u5f80\u5f80\u5fae\u5999\u4e14\u4f1a\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u4f20\u64ad\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u5e7b\u89c9\u89c6\u4e3a\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u96be\u4ee5\u6355\u6349\u5176\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u6f14\u5316\u7279\u6027\u3002", "method": "\u5c06\u6b65\u9aa4\u7ea7\u5e7b\u89c9\u5224\u65ad\u89c6\u4e3a\u5c40\u90e8\u89c2\u6d4b\uff0c\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u6765\u8ffd\u8e2a\u6574\u4e2a\u63a8\u7406\u8f68\u8ff9\u4e2d\u63a8\u7406\u72b6\u6001\u7684\u5168\u5c40\u6f14\u5316\uff0c\u5b9e\u73b0\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5e7b\u89c9\u68c0\u6d4b\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\uff0c\u66f4\u597d\u5730\u7406\u89e3\u548c\u8ffd\u8e2a\u5e7b\u89c9\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4f20\u64ad\u548c\u6f14\u5316\u3002", "conclusion": "\u5c06\u5e7b\u89c9\u5efa\u6a21\u4e3a\u6f14\u5316\u6f5c\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u4e8b\u4ef6\uff0c\u901a\u8fc7\u7d2f\u79ef\u524d\u7f00\u7ea7\u4fe1\u53f7\u8fdb\u884c\u5168\u5c40\u8ffd\u8e2a\uff0c\u4e3a\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5b9e\u65f6\u53ef\u89e3\u91ca\u65b9\u6cd5\u3002"}}
{"id": "2601.01119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01119", "abs": "https://arxiv.org/abs/2601.01119", "authors": ["Muhammad Ashad Kabir", "Sirajam Munira", "Dewan Tasnia Azad", "Saleh Mohammed Ikram", "Mohammad Habibur Rahman Sarker", "Syed Manzoor Ahmed Hanifi"], "title": "Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings", "comment": "27 pages", "summary": "Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u793e\u533a\u65e9\u671f\u6162\u6027\u80be\u75c5\u7b5b\u67e5\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709\u6162\u6027\u80be\u75c5\u7b5b\u67e5\u5de5\u5177\u4e3b\u8981\u57fa\u4e8e\u9ad8\u6536\u5165\u56fd\u5bb6\u4eba\u7fa4\u5f00\u53d1\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u5730\u533a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5730\u533a\u7684\u98ce\u9669\u7279\u5f81\u4e0d\u540c\u3002\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u7d2f\u52a0\u8bc4\u5206\u51fd\u6570\uff0c\u57fa\u4e8e\u665a\u671f\u80be\u75c5\u60a3\u8005\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u98ce\u9669\u56e0\u7d20\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u4e5f\u96be\u4ee5\u9884\u6d4b\u65e9\u671f\u80be\u75c5\u3002", "method": "\u4f7f\u7528\u5b5f\u52a0\u62c9\u56fd\u793e\u533a\u6570\u636e\u96c6\uff08\u5357\u4e9a\u9996\u4e2a\u6b64\u7c7b\u6570\u636e\u96c6\uff09\uff0c\u8bc4\u4f30\u4e8612\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5e94\u752810\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u6280\u672f\u8bc6\u522b\u7a33\u5065\u3001\u53ef\u6cdb\u5316\u7684\u9884\u6d4b\u56e0\u5b50\u3002\u6700\u7ec8\u6a21\u578b\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u5e76\u5728\u5370\u5ea6\u3001\u963f\u8054\u914b\u548c\u5b5f\u52a0\u62c9\u56fd\u7684\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002\u4f7f\u7528SHAP\u63d0\u4f9b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u57fa\u4e8eRFECV\u9009\u62e9\u7279\u5f81\u5b50\u96c6\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fbe\u523090.40%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u800c\u6700\u5c11\u975e\u75c5\u7406\u6d4b\u8bd5\u7279\u5f81\u96c6\u4e5f\u8868\u73b0\u51fa\u8272\uff0889.23%\u5e73\u8861\u51c6\u786e\u7387\uff09\uff0c\u901a\u5e38\u4f18\u4e8e\u66f4\u5927\u6216\u5b8c\u6574\u7279\u5f81\u96c6\u3002\u76f8\u6bd4\u73b0\u6709\u7b5b\u67e5\u5de5\u5177\uff0c\u6240\u63d0\u6a21\u578b\u5728\u9700\u8981\u66f4\u5c11\u3001\u66f4\u6613\u83b7\u53d6\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u3002\u5916\u90e8\u9a8c\u8bc1\u663e\u793a78%\u81f398%\u7684\u654f\u611f\u6027\uff0c\u8bc1\u5b9e\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u793e\u533a\u65e9\u671f\u6162\u6027\u80be\u75c5\u7b5b\u67e5\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\uff0c\u540c\u65f6\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u5408\u4f4e\u8d44\u6e90\u73af\u5883\u4f7f\u7528\u3002"}}
{"id": "2601.01964", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01964", "abs": "https://arxiv.org/abs/2601.01964", "authors": ["Tran Sy Bao"], "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation", "comment": "9 pages, 8 tables, code available at https://github.com/transybao1393/csf-sign-language", "summary": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.", "AI": {"tldr": "\u63d0\u51faCanonical Semantic Form (CSF)\u6846\u67b6\uff0c\u5b9e\u73b0\u4efb\u610f\u6e90\u8bed\u8a00\u5230\u624b\u8bed\u7684\u76f4\u63a5\u7ffb\u8bd1\uff0c\u65e0\u9700\u82f1\u8bed\u4e2d\u4ecb\uff0c\u901a\u8fc79\u4e2a\u901a\u7528\u8bed\u4e49\u69fd\u5206\u89e3\u8bed\u53e5\uff0c\u7279\u522b\u9488\u5bf9\u6761\u4ef6\u8868\u8fbe\u5efa\u7acb\u4e8635\u79cd\u6761\u4ef6\u7c7b\u578b\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u82f1\u8bed\u4f5c\u4e3a\u4e2d\u4ecb\u8bed\u8a00\uff0c\u8fd9\u4e3a\u5168\u7403\u975e\u82f1\u8bed\u4f7f\u7528\u8005\uff08\u7279\u522b\u662f\u804b\u4eba\u793e\u533a\uff09\u8bbe\u7f6e\u4e86\u969c\u788d\u3002\u9700\u8981\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u4e49\u8868\u793a\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u4efb\u610f\u6e90\u8bed\u8a00\u5230\u624b\u8bed\u7684\u76f4\u63a5\u7ffb\u8bd1\u3002", "method": "\u63d0\u51faCanonical Semantic Form (CSF)\u6846\u67b6\uff0c\u5c06\u8bdd\u8bed\u5206\u89e3\u4e3a9\u4e2a\u901a\u7528\u8bed\u4e49\u69fd\uff1a\u4e8b\u4ef6\u3001\u610f\u56fe\u3001\u65f6\u95f4\u3001\u6761\u4ef6\u3001\u65bd\u4e8b\u8005\u3001\u5bf9\u8c61\u3001\u5730\u70b9\u3001\u76ee\u7684\u3001\u4fee\u9970\u8bed\u3002\u7279\u522b\u8d21\u732e\u662f\u5efa\u7acb\u4e86\u5305\u542b35\u79cd\u6761\u4ef6\u7c7b\u578b\u7684\u5168\u9762\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d68\u4e2a\u8bed\u4e49\u7c7b\u522b\u3002\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u63d0\u53d6\u5668\uff080.74 MB\uff09\u3002", "result": "\u5728\u56db\u79cd\u7c7b\u578b\u5b66\u4e0a\u4e0d\u540c\u7684\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u8d8a\u5357\u8bed\u3001\u65e5\u8bed\u3001\u6cd5\u8bed\uff09\u4e0a\u5b9e\u73b0\u4e8699.03%\u7684\u5e73\u5747\u69fd\u63d0\u53d6\u51c6\u786e\u7387\u3002\u6761\u4ef6\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523099.4%\uff0835\u4e2a\u7c7b\u522b\uff09\u3002\u5728CPU\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u4e3a3.02ms\uff0c\u652f\u6301\u6d4f\u89c8\u5668\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u624b\u8bed\u751f\u6210\u3002", "conclusion": "CSF\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u8a00\u65e0\u5173\u7684\u624b\u8bed\u7ffb\u8bd1\uff0c\u6d88\u9664\u4e86\u82f1\u8bed\u4e2d\u4ecb\u7684\u9700\u6c42\u3002\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u9ad8\u51c6\u786e\u7387\u548c\u4f4e\u5ef6\u8fdf\u4f7f\u5176\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002\u5f00\u6e90\u4ee3\u7801\u3001\u6a21\u578b\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u5c06\u63a8\u52a8\u65e0\u969c\u788d\u624b\u8bed\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2601.01206", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01206", "abs": "https://arxiv.org/abs/2601.01206", "authors": ["Soroush Elyasi", "Arya VarastehNezhad", "Fattaneh Taghiyareh"], "title": "MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches", "comment": null, "summary": "Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.", "AI": {"tldr": "\u4f7f\u7528\u591a\u7c7b\u578b\u4e25\u8083\u6e38\u620f\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u9002\u5408\u5ea6\uff0c\u901a\u8fc7\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u66ff\u4ee3\u4f20\u7edf\u4eba\u683c\u95ee\u5377\uff0c\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u804c\u4e1a\u8bc4\u4f30\u4e2d\u7684\u4eba\u683c\u95ee\u5377\u5b58\u5728\u56de\u7b54\u504f\u5dee\u3001\u75b2\u52b3\u548c\u6545\u610f\u626d\u66f2\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u4e14\u504f\u8bef\u66f4\u5c11\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\u548c\u5b9e\u8bc1\u7814\u7a76\u786e\u5b9a\u8f6f\u4ef6\u5f00\u53d1\u76f8\u5173\u7279\u8d28\uff0c\u8bbe\u8ba1\u5b9a\u5236\u79fb\u52a8\u6e38\u620f\u6536\u96c6\u95ee\u9898\u89e3\u51b3\u3001\u89c4\u5212\u3001\u9002\u5e94\u6027\u7b49\u884c\u4e3a\u6570\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5efa\u6a21\u7b56\u7565\u4ec5\u4f7f\u7528\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u9884\u6d4b\u9002\u5408\u5ea6", "result": "\u6a21\u578b\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387\uff0c\u5408\u9002\u5019\u9009\u4eba\u8868\u73b0\u51fa\u72ec\u7279\u6e38\u620f\u6a21\u5f0f\uff1a\u66f4\u591a\u89e3\u8c1c\u6e38\u620f\u80dc\u5229\u3001\u66f4\u591a\u4fa7\u6311\u6218\u3001\u66f4\u9891\u7e41\u83dc\u5355\u5bfc\u822a\u3001\u66f4\u5c11\u6682\u505c/\u91cd\u8bd5/\u653e\u5f03", "conclusion": "\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u80fd\u6709\u6548\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u9002\u5408\u5ea6\uff0c\u4e25\u8083\u6e38\u620f\u53ef\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u6709\u8da3\u4e14\u504f\u8bef\u66f4\u5c11\u7684\u804c\u4e1a\u8bc4\u4f30\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2601.01322", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01322", "abs": "https://arxiv.org/abs/2601.01322", "authors": ["Hongjie Wang", "Niraj K. Jha"], "title": "LinMU: Multimodal Understanding Made Linear", "comment": "23 pages, 7 figures", "summary": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.", "AI": {"tldr": "LinMU\uff1a\u4e00\u79cd\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\uff0c\u901a\u8fc7M-MATE\u53cc\u5206\u652f\u6a21\u5757\uff08\u5168\u5c40\u72b6\u6001\u7a7a\u95f4\u6a21\u578b+\u5c40\u90e8\u7a97\u53e3\u6ce8\u610f\u529b\uff09\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0d\u53d8\u4f46\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u4e3a\u7ebf\u6027\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u4e14\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u8bbe\u8ba1\u65e2\u80fd\u4fdd\u6301\u6027\u80fd\u53c8\u80fd\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u6a21\u578b\u67b6\u6784\u3002", "method": "\u63d0\u51faLinMU\u67b6\u6784\uff0c\u7528M-MATE\u53cc\u5206\u652f\u6a21\u5757\u66ff\u4ee3\u6240\u6709\u81ea\u6ce8\u610f\u529b\u5c42\uff1aFlex-MA\u5206\u652f\u4f7f\u7528\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\uff0cLocal-Swin\u5206\u652f\u4f7f\u7528Swin\u98ce\u683c\u7a97\u53e3\u6ce8\u610f\u529b\u5904\u7406\u5c40\u90e8\u76f8\u5173\u6027\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u84b8\u998f\u6846\u67b6\u5c06\u9884\u8bad\u7ec3VLM\u8f6c\u6362\u4e3aLinMU\u67b6\u6784\u3002", "result": "\u5728MMMU\u3001TextVQA\u3001LongVideoBench\u3001Video-MME\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLinMU\u5339\u914d\u4e86\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u9996token\u751f\u6210\u65f6\u95f4\u51cf\u5c112.7\u500d\uff0c\u5728\u5206\u949f\u7ea7\u89c6\u9891\u4e0a\u7684token\u541e\u5410\u91cf\u63d0\u53479.0\u500d\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u84b8\u998f\u9636\u6bb5\u548c\u53cc\u5206\u652f\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e5f\u80fd\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u7684\u957f\u4e0a\u4e0b\u6587VLM\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.01605", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01605", "abs": "https://arxiv.org/abs/2601.01605", "authors": ["Xin Di", "Xinglin Piao", "Fei Wang", "Guodong Jing", "Yong Zhang"], "title": "REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training", "comment": null, "summary": "Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.", "AI": {"tldr": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u63d0\u5347\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8de8\u533a\u57df\u548c\u6781\u7aef\u5929\u6c14\u4e0b\u6cdb\u5316\u5dee\u7684\u95ee\u9898", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7684\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u672c\u5730\u8bad\u7ec3\u6570\u636e\u548c\u9759\u6001\u53c2\u6570\uff0c\u5bfc\u81f4\u8de8\u533a\u57df\u548c\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\uff0c\u6838\u5fc3\u662f\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5757\uff0c\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u673a\u5236\u66ff\u4ee3\u6807\u51c6\u7ebf\u6027\u6295\u5f71\uff0c\u589e\u5f3a\u5bf9\u975e\u5e73\u7a33\u6c14\u8c61\u5206\u5e03\u7684\u9002\u5e94\u80fd\u529b", "result": "\u5728\u8de8\u533a\u57df\u6781\u7aef\u964d\u6c34\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0cREE-TTT\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5bf9\u6570\u636e\u5206\u5e03\u504f\u79fb\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027", "conclusion": "REE-TTT\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u8de8\u533a\u57df\u548c\u6781\u7aef\u5929\u6c14\u4e0b\u7684\u6c14\u8c61\u9884\u62a5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01547", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01547", "abs": "https://arxiv.org/abs/2601.01547", "authors": ["Tianjun Gu", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding", "comment": null, "summary": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTeleo-Spatial Intelligence (TSI)\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u7269\u7406\u52a8\u6001\u63a8\u7406\u548c\u610f\u56fe\u9a71\u52a8\u63a8\u7406\uff0c\u5e76\u53d1\u5e03EscherVerse\u57fa\u51c6\u5957\u4ef6\u6765\u63a8\u52a8\u7a7a\u95f4\u667a\u80fd\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7a7a\u95f4\u63a8\u7406\u7814\u7a76\u5ffd\u89c6\u4e86\u4eba\u7c7b\u610f\u56fe\u5728\u7a7a\u95f4\u53d8\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u9700\u8981\u4ece\u88ab\u52a8\u573a\u666f\u63cf\u8ff0\u8f6c\u5411\u76ee\u7684\u9a71\u52a8\u7684\u6574\u4f53\u7406\u89e3\u3002", "method": "\u63d0\u51faTSI\u8303\u5f0f\uff0c\u5305\u542b\u7269\u7406\u52a8\u6001\u63a8\u7406\u548c\u610f\u56fe\u9a71\u52a8\u63a8\u7406\u4e24\u4e2a\u652f\u67f1\uff0c\u5e76\u6784\u5efaEscherVerse\u57fa\u51c6\u5957\u4ef6\uff08\u5305\u62ecEscher-Bench\u57fa\u51c6\u3001Escher-35k\u6570\u636e\u96c6\u548cEscher\u7cfb\u5217\u6a21\u578b\uff09\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u610f\u56fe\u9a71\u52a8\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u7269\u4f53\u6052\u5b58\u6027\u3001\u72b6\u6001\u8f6c\u6362\u548c\u8f68\u8ff9\u9884\u6d4b\u7b49\u52a8\u6001\u573a\u666f\uff0c\u4e3a\u7a7a\u95f4\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u8d44\u6e90\u3002", "conclusion": "TSI\u8303\u5f0f\u5c06\u7a7a\u95f4\u667a\u80fd\u4ece\u88ab\u52a8\u63cf\u8ff0\u63d0\u5347\u5230\u76ee\u7684\u9a71\u52a8\u7684\u6574\u4f53\u7406\u89e3\uff0cEscherVerse\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u548c\u5de5\u5177\u3002"}}
{"id": "2601.01653", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.01653", "abs": "https://arxiv.org/abs/2601.01653", "authors": ["Hao Xiang Li", "Yash Shah", "Lorenzo Giusti"], "title": "Learning Resilient Elections with Adversarial GNNs", "comment": null, "summary": "In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6297\u8bad\u7ec3\u7684\u6295\u7968\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9009\u4e3e\u8868\u793a\u4e3a\u4e8c\u5206\u56fe\u6765\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u6297\u7b56\u7565\u6295\u7968\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6295\u7968\u89c4\u5219\u96be\u4ee5\u6ee1\u8db3\u6240\u6709\u573a\u666f\u9700\u6c42\uff0c\u800c\u73b0\u6709\u7684\u81ea\u52a8\u5316\u673a\u5236\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5e94\u7528\u5230\u771f\u5b9e\u9009\u4e3e\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u7b56\u7565\u6027\u6295\u7968\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u5c06\u9009\u4e3e\u8868\u793a\u4e3a\u4e8c\u5206\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6295\u7968\u89c4\u5219\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u6765\u63d0\u9ad8\u89c4\u5219\u5bf9\u7b56\u7565\u6027\u6295\u7968\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u5173\u952e\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u6295\u7968\u89c4\u5219\u7684\u8868\u8fbe\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u9009\u4e3e\u5f00\u8f9f\u4e86\u65b0\u524d\u6cbf\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u548c\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u6295\u7968\u89c4\u5219\u5b66\u4e60\u3002"}}
{"id": "2601.01720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01720", "abs": "https://arxiv.org/abs/2601.01720", "authors": ["Xijie Huang", "Chengming Xu", "Donghao Luo", "Xiaobin Hu", "Peng Tang", "Xu Peng", "Jiangning Zhang", "Chengjie Wang", "Yanwei Fu"], "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing", "comment": null, "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.", "AI": {"tldr": "\u63d0\u51faFFP-300K\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u81ea\u9002\u5e94\u65f6\u7a7aRoPE\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u9700\u8fd0\u884c\u65f6\u5f15\u5bfc\u7684\u7b2c\u4e00\u5e27\u4f20\u64ad\u89c6\u9891\u7f16\u8f91\uff0c\u5728EditVerseBench\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7b2c\u4e00\u5e27\u4f20\u64ad\u65b9\u6cd5\u4f9d\u8d56\u7e41\u7410\u7684\u8fd0\u884c\u65f6\u5f15\u5bfc\uff0c\u6839\u672c\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u96c6\u4e0d\u8db3\uff1a\u89c6\u9891\u592a\u77ed\u3001\u5206\u8fa8\u7387\u4f4e\u3001\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u591f\uff0c\u65e0\u6cd5\u5b66\u4e60\u9c81\u68d2\u7684\u65f6\u5e8f\u5148\u9a8c\u3002", "method": "1) \u6784\u5efaFFP-300K\u6570\u636e\u96c6\uff1a30\u4e07\u5bf9720p\u300181\u5e27\u7684\u9ad8\u4fdd\u771f\u89c6\u9891\u5bf9\uff0c\u901a\u8fc7\u539f\u5219\u6027\u53cc\u8f68\u6d41\u7a0b\u751f\u6210\u591a\u6837\u5c40\u90e8\u548c\u5168\u5c40\u7f16\u8f91\uff1b2) \u63d0\u51fa\u81ea\u9002\u5e94\u65f6\u7a7aRoPE\u6846\u67b6\uff1a\u52a8\u6001\u91cd\u6620\u5c04\u4f4d\u7f6e\u7f16\u7801\u89e3\u8026\u5916\u89c2\u548c\u8fd0\u52a8\u53c2\u8003\uff1b3) \u81ea\u84b8\u998f\u7b56\u7565\uff1a\u8eab\u4efd\u4f20\u64ad\u4efb\u52a1\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u786e\u4fdd\u957f\u671f\u65f6\u5e8f\u7a33\u5b9a\u6027\u3002", "result": "\u5728EditVerseBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5b66\u672f\u548c\u5546\u4e1a\u6a21\u578b\uff0cPickScore\u63d0\u5347\u7ea60.2\u5206\uff0cVLM\u5206\u6570\u63d0\u5347\u7ea60.3\u5206\u3002", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u81ea\u9002\u5e94\u65f6\u7a7aRoPE\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u65e0\u9700\u5f15\u5bfc\u7684\u7b2c\u4e00\u5e27\u4f20\u64ad\u89c6\u9891\u7f16\u8f91\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5916\u89c2\u4fdd\u6301\u4e0e\u8fd0\u52a8\u4fdd\u7559\u4e4b\u95f4\u7684\u5173\u952e\u77db\u76fe\u3002"}}
{"id": "2601.01784", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01784", "abs": "https://arxiv.org/abs/2601.01784", "authors": ["Boyang Zhao", "Xin Liao", "Jiaxin Chen", "Xiaoshuai Wu", "Yufeng Wu"], "title": "DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization", "comment": null, "summary": "The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \\emph{local view}, failing to capture global anomalies. To address this, we propose a \\underline{d}ual-stream graph learning and \\underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \\emph{Temporal Distance Stream} for local artifacts and a \\emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\\% in AP@0.95, with significant improvements in cross-domain robustness.", "AI": {"tldr": "DDNet\uff1a\u57fa\u4e8e\u53cc\u6d41\u56fe\u5b66\u4e60\u548c\u89e3\u7f20\u7684\u65f6\u5e8f\u4f2a\u9020\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u5c40\u90e8\u4f2a\u5f71\u548c\u8bed\u4e49\u5185\u5bb9\u6d41\u6765\u6355\u83b7\u5168\u5c40\u5f02\u5e38\uff0c\u663e\u8457\u63d0\u5347\u4f2a\u9020\u7247\u6bb5\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8de8\u57df\u9c81\u68d2\u6027\u3002", "motivation": "AIGC\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u4ec5\u7be1\u6539\u89c6\u9891\u4e2d\u7684\u5c0f\u7247\u6bb5\u5c31\u80fd\u8bef\u5bfc\u89c2\u4f17\uff0c\u800c\u89c6\u9891\u7ea7\u68c0\u6d4b\u4e0d\u591f\u51c6\u786e\u548c\u5177\u6709\u8bf4\u670d\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c40\u90e8\u89c6\u89d2\uff0c\u96be\u4ee5\u6355\u6349\u5168\u5c40\u5f02\u5e38\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65f6\u5e8f\u4f2a\u9020\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDDNet\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u53cc\u6d41\u56fe\u5b66\u4e60\uff1a\u65f6\u95f4\u8ddd\u79bb\u6d41\u6355\u6349\u5c40\u90e8\u4f2a\u5f71\uff0c\u8bed\u4e49\u5185\u5bb9\u6d41\u5efa\u7acb\u957f\u7a0b\u8fde\u63a5\uff1b2\uff09\u75d5\u8ff9\u89e3\u7f20\u4e0e\u9002\u914d\uff08TDA\uff09\u5206\u79bb\u901a\u7528\u4f2a\u9020\u6307\u7eb9\uff1b3\uff09\u8de8\u5c42\u7ea7\u7279\u5f81\u5d4c\u5165\uff08CLFE\uff09\u901a\u8fc7\u5c42\u6b21\u7279\u5f81\u6df1\u5ea6\u878d\u5408\u6784\u5efa\u9c81\u68d2\u7279\u5f81\u57fa\u7840\u3002", "result": "\u5728ForgeryNet\u548cTVIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDNet\u5728AP@0.95\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u5347\u7ea69%\uff0c\u5728\u8de8\u57df\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "DDNet\u901a\u8fc7\u53cc\u6d41\u534f\u8c03\u548c\u7279\u5f81\u89e3\u7f20\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u4f2a\u9020\u5b9a\u4f4d\u4e2d\u7684\u5168\u5c40\u5f02\u5e38\u6355\u6349\u95ee\u9898\uff0c\u5728\u7cbe\u5ea6\u548c\u8de8\u57df\u9002\u5e94\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.01807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01807", "abs": "https://arxiv.org/abs/2601.01807", "authors": ["Ubaidullah", "Muhammad Abid Hussain", "Mohsin Raza Jafri", "Rozi Khan", "Moid Sandhu", "Abd Ullah Khan", "Hyundong Shin"], "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification", "comment": null, "summary": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.", "AI": {"tldr": "\u63d0\u51faLUMPNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5(LSD)\uff0c\u901a\u8fc7YOLOv11\u68c0\u6d4b\u76ae\u80a4\u7ed3\u8282\uff0cEfficientNet\u5206\u7c7b\uff0c\u7ed3\u5408\u65b0\u578b\u81ea\u9002\u5e94\u6df7\u5408\u4f18\u5316\u5668\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u8bad\u7ec3\u51c6\u786e\u7387\u548c98%\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5(LSD)\u662f\u4e00\u79cd\u4f20\u67d3\u6027\u75c5\u6bd2\u6027\u75be\u75c5\uff0c\u4e25\u91cd\u5f71\u54cd\u755c\u7267\u4e1a\u5065\u5eb7\u548c\u5168\u7403\u7cae\u98df\u5b89\u5168\u3002\u7531\u4e8e\u5176\u5feb\u901f\u4f20\u64ad\u7279\u6027\uff0c\u65e9\u671f\u7cbe\u786e\u8bc6\u522b\u5bf9\u4e8e\u9884\u9632\u75ab\u60c5\u7206\u53d1\u548c\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faLUMPNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1a1) \u4f7f\u7528YOLOv11\u68c0\u6d4b\u548c\u5b9a\u4f4d\u725b\u56fe\u50cf\u4e2d\u7684LSD\u76ae\u80a4\u7ed3\u8282\u548c\u75c5\u53d8\uff1b2) \u5229\u7528\u57fa\u4e8eEfficientNet\u7684CNN\u5206\u7c7b\u5668\u5bf9\u5b9a\u4f4d\u56fe\u50cf\u8fdb\u884cLSD\u611f\u67d3\u6216\u5065\u5eb7\u5206\u7c7b\uff1b3) \u63d0\u51fa\u65b0\u578b\u81ea\u9002\u5e94\u6df7\u5408\u4f18\u5316\u5668\u6765\u7a33\u5b9a\u548c\u52a0\u901fYOLOv11\u4e0eEfficientNet\u6df7\u5408\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1a1) \u8fbe\u523099%\u7684LSD\u68c0\u6d4b\u8bad\u7ec3\u51c6\u786e\u7387\uff1b2) \u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe\u523098%\uff1b3) \u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff1b4) \u4e0e\u4f7f\u7528AdamW\u4f18\u5316\u7684EfficientNet-B0\u6a21\u578b\u76f8\u6bd4\uff0cLUMPNet\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "LUMPNet\u80fd\u591f\u6709\u6548\u65e9\u671f\u68c0\u6d4b\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\uff0c\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aLSD\u7684\u65e9\u671f\u8bca\u65ad\u548c\u9632\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01874", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01874", "abs": "https://arxiv.org/abs/2601.01874", "authors": ["Shuhang Chen", "Yunqiu Xu", "Junjie Xie", "Aojun Lu", "Tao Feng", "Zeying Huang", "Ning Zhang", "Yi Sun", "Yi Yang", "Hangjie Yuan"], "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving", "comment": null, "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.", "AI": {"tldr": "CogFlow\u662f\u4e00\u4e2a\u53d7\u8ba4\u77e5\u542f\u53d1\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5185\u5316\u9636\u6bb5\u6765\u6539\u5584\u89c6\u89c9\u6570\u5b66\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u611f\u77e5\u4e0e\u63a8\u7406\u6574\u5408\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e0a\u4ecd\u6709\u56f0\u96be\u3002\u867d\u7136\u6700\u8fd1\u5de5\u4f5c\u8ba4\u8bc6\u5230\u89c6\u89c9\u611f\u77e5\u662f\u74f6\u9888\uff0c\u4f46\u53ea\u5173\u6ce8\u6539\u5584\u89c6\u89c9\u8f93\u5165\u7684\u63d0\u53d6\u548c\u89e3\u91ca\uff0c\u5ffd\u7565\u4e86\u63d0\u53d6\u7684\u89c6\u89c9\u7ebf\u7d22\u662f\u5426\u88ab\u5fe0\u5b9e\u6574\u5408\u5e76\u6b63\u786e\u7528\u4e8e\u540e\u7eed\u63a8\u7406\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faCogFlow\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7684\u5c42\u6b21\u6d41\u7a0b\uff1a\u611f\u77e5\u2192\u5185\u5316\u2192\u63a8\u7406\u3002\u5305\u542b\u534f\u540c\u89c6\u89c9\u5956\u52b1\u63d0\u5347\u7b26\u53f7\u548c\u56fe\u8868\u7684\u4fe1\u606f\u63d0\u53d6\uff1b\u77e5\u8bc6\u5185\u5316\u5956\u52b1\u6a21\u578b\u786e\u4fdd\u89c6\u89c9\u7ebf\u7d22\u5fe0\u5b9e\u6574\u5408\uff1b\u89c6\u89c9\u95e8\u63a7\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u9632\u6b62\u6a21\u578b\u5bfb\u6c42\u770b\u4f3c\u8fde\u8d2f\u4f46\u89c6\u89c9\u672a\u63a5\u5730\u63a8\u7406\u94fe\uff1b\u5e76\u521b\u5efaMathCog\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728\u5e38\u7528\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u548c\u5206\u6790\u9a8c\u8bc1\u4e86CogFlow\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u77e5\u8bc6\u5185\u5316\u9636\u6bb5\u5e76\u5168\u9762\u589e\u5f3a\u611f\u77e5\u3001\u5185\u5316\u548c\u63a8\u7406\u4e09\u4e2a\u9636\u6bb5\uff0cCogFlow\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u611f\u77e5\u4e0e\u63a8\u7406\u6574\u5408\u65b9\u9762\u7684\u6839\u672c\u95ee\u9898\u3002"}}
{"id": "2601.02106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02106", "abs": "https://arxiv.org/abs/2601.02106", "authors": ["Ashish Rana", "Ammar Shaker", "Sascha Saralajew", "Takashi Suzuki", "Kosuke Yasuda", "Shintaro Kato", "Toshikazu Wada", "Toshiyuki Fujikawa", "Toru Kikutsuji"], "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI", "comment": "Accepted to the Demo Track at the IEEE International Conference on Data Mining (ICDM) 2025, where it received the Best Demo Award", "summary": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.", "AI": {"tldr": "ProtoPal\u6846\u67b6\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u5b9e\u73b0\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u7406\u89e3\u548c\u53ef\u9a8c\u8bc1\u7684\u5e72\u9884\u65b9\u6848\u5c55\u793a\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e3a\u533b\u7597\u884c\u4e1a\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u65e2\u6613\u4e8e\u7406\u89e3\u53c8\u53ef\u4fe1\u7684\u9884\u6d4b\u3001\u5e72\u9884\u548c\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u539f\u578b\u5b66\u4e60\u6846\u67b6ProtoPal\uff0c\u5305\u542b\u524d\u7aef\u548c\u540e\u7aef\u4e24\u79cd\u6a21\u5f0f\uff0c\u901a\u8fc7\u539f\u578b\u8868\u793a\u6765\u5c55\u793a\u5e72\u9884\u63aa\u65bd\u53ca\u5176\u6a21\u62df\u7ed3\u679c\u3002", "result": "\u5728\u5b9a\u91cf\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u80fd\u591f\u76f4\u89c2\u5730\u5448\u73b0\u5e72\u9884\u63aa\u65bd\u53ca\u5176\u6a21\u62df\u7ed3\u679c\u3002", "conclusion": "\u539f\u578b\u5b66\u4e60\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u900f\u660e\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2601.02020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02020", "abs": "https://arxiv.org/abs/2601.02020", "authors": ["Shihan Peng", "Yuyang Xiong", "Hanyu Zhou", "Zhiwei Shi", "Haoyue Liu", "Gang Chen", "Luxin Yan", "Yi Chang"], "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.", "AI": {"tldr": "ADAE\uff1a\u4e00\u79cd\u4e8b\u4ef6\u5f15\u5bfc\u7684\u65f6\u7a7a\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3aDepth Anything\u5728\u9000\u5316\u573a\u666f\uff08\u6781\u7aef\u5149\u7167\u3001\u8fd0\u52a8\u6a21\u7cca\uff09\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b", "motivation": "\u5f53\u524d\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff08\u5982Depth Anything\uff09\u5728\u7406\u60f3\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6781\u7aef\u5149\u7167\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u4e9b\u9000\u5316\u4f1a\u7834\u574f\u5e27\u76f8\u673a\u7684\u89c6\u89c9\u4fe1\u53f7\uff0c\u524a\u5f31\u57fa\u4e8e\u5e27\u7684\u6df1\u5ea6\u7279\u5f81\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u7684\u5224\u522b\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u4e8b\u4ef6\u76f8\u673a\u6765\u5229\u7528\u5176\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4f46\u901a\u5e38\u9700\u8981\u4ece\u5934\u8bad\u7ec3\uff0c\u65e0\u6cd5\u7ee7\u627f\u57fa\u7840\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\u548c\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faADAE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u71b5\u611f\u77e5\u7a7a\u95f4\u878d\u5408\uff1a\u4f7f\u7528\u4fe1\u606f\u71b5\u7b56\u7565\u81ea\u9002\u5e94\u878d\u5408\u57fa\u4e8e\u5e27\u548c\u57fa\u4e8e\u4e8b\u4ef6\u7684\u7279\u5f81\uff0c\u4ee5\u6307\u793a\u5149\u7167\u5f15\u8d77\u7684\u9000\u5316\uff1b2\uff09\u8fd0\u52a8\u5f15\u5bfc\u65f6\u95f4\u6821\u6b63\uff1a\u5229\u7528\u4e8b\u4ef6\u8fd0\u52a8\u7ebf\u7d22\u91cd\u65b0\u6821\u51c6\u6a21\u7cca\u533a\u57df\u7684\u6a21\u7cca\u7279\u5f81\u3002\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\u76f8\u4e92\u8865\u5145\uff0c\u5171\u540c\u589e\u5f3aDepth Anything\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002ADAE\u80fd\u591f\u6709\u6548\u63d0\u5347Depth Anything\u5728\u9000\u5316\u573a\u666f\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u5149\u7167\u548c\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u3002", "conclusion": "ADAE\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u65f6\u7a7a\u878d\u5408\u6846\u67b6\uff0c\u6210\u529f\u589e\u5f3a\u4e86Depth Anything\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u52a8\u6001\u548c\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.02253", "categories": ["cs.LG", "cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02253", "abs": "https://arxiv.org/abs/2601.02253", "authors": ["Emrah Mete", "Emin Erkan Korkmaz"], "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission", "comment": "9 pages, 4 figures", "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.", "AI": {"tldr": "\u63d0\u51faNeuro-Channel Networks (NCN)\u4e58\u6cd5\u514d\u67b6\u69cb\uff0c\u6a21\u4eff\u751f\u7269\u795e\u7d93\u7cfb\u7d71\u96e2\u5b50\u901a\u9053\u9650\u5236\u6a5f\u5236\uff0c\u7528\u901a\u9053\u5bec\u5ea6\u548c\u795e\u7d93\u905e\u8cea\u53c3\u6578\u53d6\u4ee3\u50b3\u7d71\u6b0a\u91cd\uff0c\u50c5\u9700\u52a0\u6cd5\u3001\u6e1b\u6cd5\u548c\u4f4d\u5143\u904b\u7b97\uff0c\u7121\u9700\u6d6e\u9ede\u4e58\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b78\u7fd2\u5c0dGPU\u7b49\u9ad8\u6027\u80fd\u786c\u9ad4\u7684\u4f9d\u8cf4\u9020\u6210\u6210\u672c\u9ad8\u3001\u80fd\u8017\u5927\u3001\u4f9b\u61c9\u77ed\u7f3a\u7b49\u554f\u984c\uff0c\u9650\u5236\u4e86AI\u5728\u908a\u7de3\u8a2d\u5099\u7684\u666e\u53ca\u3002\u751f\u7269\u795e\u7d93\u7cfb\u7d71\u7684\u9ad8\u6548\u6027\u555f\u767c\u4e86\u958b\u767c\u4e0d\u4f9d\u8cf4\u77e9\u9663\u4e58\u6cd5\u7684\u66ff\u4ee3\u67b6\u69cb\u3002", "method": "\u63d0\u51faNeuro-Channel Networks (NCN)\u67b6\u69cb\uff1a1) \u7528Channel Widths\u7269\u7406\u9650\u5236\u4fe1\u865f\u5e45\u5ea6\u53d6\u4ee3\u6b0a\u91cd\uff1b2) \u7528Neurotransmitter\u53c3\u6578\u57fa\u65bc\u7b26\u865f\u908f\u8f2f\u8abf\u7bc0\u4fe1\u865f\u50b3\u8f38\uff1b3) \u524d\u5411\u50b3\u64ad\u50c5\u4f7f\u7528\u52a0\u6cd5\u3001\u6e1b\u6cd5\u548c\u4f4d\u5143\u904b\u7b97\uff08\u6700\u5c0f\u503c\u3001\u7b26\u865f\uff09\uff0c\u5b8c\u5168\u6d88\u9664\u6d6e\u9ede\u4e58\u6cd5\uff1b4) \u4f7f\u7528\u6a19\u6e96\u53cd\u5411\u50b3\u64ad\u8a13\u7df4\u3002", "result": "\u6982\u5ff5\u9a57\u8b49\u986f\u793aNCN\u80fd100%\u6e96\u78ba\u89e3\u6c7a\u975e\u7dda\u6027\u53ef\u5206\u554f\u984c\uff08\u5982XOR\u548cMajority\u51fd\u6578\uff09\uff0c\u8b49\u660e\u7121\u9700\u4e58\u6cd5\u6b0a\u91cd\u5373\u53ef\u5f62\u6210\u8907\u96dc\u6c7a\u7b56\u908a\u754c\u3002", "conclusion": "NCN\u70ba\u4e0b\u4e00\u4ee3\u795e\u7d93\u5f62\u614b\u786c\u9ad4\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u8907\u96dc\u6a21\u578b\u80fd\u5728\u666e\u901aCPU\u6216\u8d85\u4f4e\u529f\u8017\u6676\u7247\u4e0a\u904b\u884c\uff0c\u7121\u9700\u4f9d\u8cf4\u6602\u8cb4\u7684GPU\u96c6\u7fa4\uff0c\u63a8\u52d5AI\u5728\u908a\u7de3\u8a2d\u5099\u7684\u666e\u53ca\u3002"}}
{"id": "2601.02264", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02264", "abs": "https://arxiv.org/abs/2601.02264", "authors": ["Boris Kriuk", "Fedor Kriuk"], "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network", "comment": "8 pages, 14 figures", "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.", "AI": {"tldr": "POSEIDON\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u80fd\u91cf\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5730\u9707\u4e8b\u4ef6\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86Gutenberg-Richter\u548cOmori-Utsu\u7b49\u5730\u9707\u5b66\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u5728\u591a\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f5c\u4e3a\u9ed1\u7bb1\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u5df2\u5efa\u7acb\u7684\u7269\u7406\u5b9a\u5f8b\u3002\u5730\u9707\u9884\u6d4b\u548c\u5730\u9707\u5371\u9669\u6027\u8bc4\u4f30\u4ecd\u7136\u662f\u5730\u7403\u7269\u7406\u5b66\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u5f15\u5165POSEIDON\uff08\u7269\u7406\u4f18\u5316\u5730\u9707\u80fd\u91cf\u63a8\u65ad\u548c\u68c0\u6d4b\u64cd\u4f5c\u7f51\u7edc\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u80fd\u91cf\u6a21\u578b\uff0c\u5c06Gutenberg-Richter\u9707\u7ea7-\u9891\u7387\u5173\u7cfb\u548cOmori-Utsu\u4f59\u9707\u8870\u51cf\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u7ea6\u675f\u5d4c\u5165\u5230\u80fd\u91cf\u5efa\u6a21\u6846\u67b6\u4e2d\u3002\u540c\u65f6\u5904\u7406\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9884\u6d4b\u4efb\u52a1\uff1a\u4f59\u9707\u5e8f\u5217\u8bc6\u522b\u3001\u6d77\u5578\u751f\u6210\u6f5c\u529b\u548c\u524d\u9707\u68c0\u6d4b\u3002", "result": "POSEIDON\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u68af\u5ea6\u63d0\u5347\u3001\u968f\u673a\u68ee\u6797\u548cCNN\u57fa\u7ebf\uff0c\u5728\u6240\u6709\u6bd4\u8f83\u65b9\u6cd5\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747F1\u5206\u6570\u3002\u5b66\u4e60\u5230\u7684\u7269\u7406\u53c2\u6570\u6536\u655b\u5230\u79d1\u5b66\u53ef\u89e3\u91ca\u7684\u503c\uff08Gutenberg-Richter b\u503c0.752\uff0cOmori-Utsu\u53c2\u6570p=0.835\uff0cc=0.1948\u5929\uff09\uff0c\u8fd9\u4e9b\u503c\u843d\u5728\u5df2\u5efa\u7acb\u7684\u5730\u9707\u5b66\u8303\u56f4\u5185\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "POSEIDON\u901a\u8fc7\u5c06\u7269\u7406\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7ea6\u675f\u5d4c\u5165\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7269\u7406\u4fe1\u606f\u7684\u5730\u9707\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u79d1\u5b66\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u3002Poseidon\u6570\u636e\u96c6\uff08280\u4e07\u4e2a\u4e8b\u4ef6\uff0c30\u5e74\uff09\u516c\u5f00\u53ef\u7528\uff0c\u63a8\u52a8\u4e86\u7269\u7406\u4fe1\u606f\u5730\u9707\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.02103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02103", "abs": "https://arxiv.org/abs/2601.02103", "authors": ["Yating Wang", "Yuan Sun", "Xuan Wang", "Ran Yi", "Boyao Zhou", "Yipengjing Sun", "Hongyu Liu", "Yinuo Wang", "Lizhuang Ma"], "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures", "comment": null, "summary": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.", "AI": {"tldr": "HeadLighter\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u5934\u90e8\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u6e10\u8fdb\u89e3\u8026\u8bad\u7ec3\u5b9e\u73b0\u5916\u89c2\u4e0e\u7167\u660e\u7684\u7269\u7406\u53ef\u5206\u89e3\uff0c\u652f\u6301\u663e\u5f0f\u5149\u7167\u548c\u89c6\u89d2\u7f16\u8f91", "motivation": "\u73b0\u67093D\u611f\u77e5\u5934\u90e8\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u3001\u903c\u771f\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u5934\u90e8\u5408\u6210\uff0c\u4f46\u5b58\u5728\u6839\u672c\u9650\u5236\uff1a\u5149\u7167\u4e0e\u5185\u5728\u5916\u89c2\u7684\u6df1\u5ea6\u7ea0\u7f20\u963b\u788d\u4e86\u53ef\u63a7\u91cd\u5149\u7167\u3002\u73b0\u6709\u89e3\u8026\u65b9\u6cd5\u4f9d\u8d56\u5f3a\u5047\u8bbe\u8fdb\u884c\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u5904\u7406\u590d\u6742\u5149\u7167\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faHeadLighter\u6846\u67b6\uff1a1\uff09\u53cc\u5206\u652f\u67b6\u6784\u5206\u522b\u5efa\u6a21\u5149\u7167\u4e0d\u53d8\u7684\u5934\u90e8\u5c5e\u6027\u548c\u7269\u7406\u57fa\u7840\u7684\u6e32\u67d3\u7ec4\u4ef6\uff1b2\uff09\u6e10\u8fdb\u89e3\u8026\u8bad\u7ec3\u9010\u6b65\u5c06\u5934\u90e8\u5916\u89c2\u5148\u9a8c\u6ce8\u5165\u751f\u6210\u67b6\u6784\uff1b3\uff09\u84b8\u998f\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u6cd5\u7ebf\u4ee5\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\uff1b4\uff09\u4f7f\u7528\u5149\u821e\u53f0\u8bbe\u7f6e\u4e0b\u91c7\u96c6\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u540c\u65f6\uff0c\u652f\u6301\u663e\u5f0f\u5149\u7167\u548c\u89c6\u89d2\u7f16\u8f91\u3002\u5c06\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002", "conclusion": "HeadLighter\u6210\u529f\u89e3\u51b3\u4e863D\u5934\u90e8\u751f\u6210\u6a21\u578b\u4e2d\u5149\u7167\u4e0e\u5916\u89c2\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u5408\u7406\u7684\u5206\u89e3\uff0c\u4e3a\u53ef\u63a7\u91cd\u5149\u7167\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02203", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02203", "abs": "https://arxiv.org/abs/2601.02203", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson", "Quan Z. Sheng"], "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules", "comment": null, "summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCSI-ResNet-A\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u83b7\u53d6\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u914d\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5fae\u8c03\uff0c\u5b9e\u73b0\u65e0\u9700\u8bbe\u5907\u7684WiFi\u4eba\u7fa4\u8ba1\u6570\uff0c\u5728\u57df\u8fc1\u79fb\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u57fa\u4e8eWiFi\u4fe1\u9053\u72b6\u6001\u4fe1\u606f(CSI)\u7684\u65e0\u8bbe\u5907\u4eba\u7fa4\u8ba1\u6570\u662f\u9690\u79c1\u4fdd\u62a4\u7269\u8054\u7f51\u5e94\u7528\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u57df\u8fc1\u79fb\u95ee\u9898\u7684\u4e25\u91cd\u963b\u788d\u2014\u2014\u5728\u4e00\u4e2a\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u6cd5\u6cdb\u5316\u5230\u5176\u4ed6\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528CSI-ResNet-A\u67b6\u6784\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u57df\u4e0d\u53d8\u8868\u793a\uff1b2) \u5229\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff1b3) \u901a\u8fc7\u72b6\u6001\u8ba1\u6570\u673a\u5904\u7406\u4e8b\u4ef6\u5e8f\u5217\u751f\u6210\u7a33\u5b9a\u5360\u7528\u4f30\u8ba1\u3002", "result": "\u5728WiFlow\u6570\u636e\u96c6\u4e0a\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u572810-shot\u5b66\u4e60\u573a\u666f\u4e2d\u8fbe\u5230MAE\u4ec50.44\uff0c\u800c\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u5931\u8d25\uff1b\u5f15\u5165\u6cdb\u5316\u6307\u6570(GI)\u8bc4\u4f30\uff0c\u6a21\u578b\u63a5\u8fd1\u5b8c\u7f8e\uff1b\u5728\u516c\u5171WiAR\u57fa\u51c6\u4e0a\u8fbe\u523098.8%\u51c6\u786e\u7387\u7684\u65b0SOTA\uff1b\u9002\u914d\u5668\u5fae\u8c03\u6027\u80fd\u63a5\u8fd1\u5b8c\u5168\u5fae\u8c03(98.84% vs 99.67%)\uff0c\u4f46\u4ec5\u8bad\u7ec32.8%\u7684\u53c2\u6570\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7269\u8054\u7f51\u90e8\u7f72\u7684\u9c81\u68d2\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u57df\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u548c\u9ad8\u6548\u9002\u914d\u5668\u5fae\u8c03\u6709\u6548\u89e3\u51b3\u4e86\u57df\u8fc1\u79fb\u95ee\u9898\u3002"}}
{"id": "2601.02206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02206", "abs": "https://arxiv.org/abs/2601.02206", "authors": ["Dachun Kai", "Zeyu Xiao", "Huyue Zhu", "Jiaxiao Wang", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras", "comment": "Accepted to AAAI 2026", "summary": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.", "AI": {"tldr": "RetinexEVSR\uff1a\u9996\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7Retinex\u5148\u9a8c\u548c\u53cc\u5411\u8de8\u6a21\u6001\u878d\u5408\uff0c\u5229\u7528\u9ad8\u5bf9\u6bd4\u5ea6\u4e8b\u4ef6\u4fe1\u53f7\u6062\u590d\u4f4e\u5149\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u7684\u7ec6\u8282\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u6062\u590d\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u56e0\u4e3a\u4f4e\u5149\u6761\u4ef6\u4e0b\u5bf9\u6bd4\u5ea6\u6709\u9650\u4e14\u9ad8\u9891\u4fe1\u606f\u4e0d\u8db3\u3002\u9700\u8981\u5229\u7528\u9ad8\u5bf9\u6bd4\u5ea6\u7684\u4e8b\u4ef6\u4fe1\u53f7\u6765\u589e\u5f3a\u6062\u590d\u6548\u679c\u3002", "method": "\u63d0\u51faRetinexEVSR\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u53cc\u5411\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u4ece\u566a\u58f0\u4e8b\u4ef6\u6570\u636e\u548c\u9000\u5316RGB\u5e27\u4e2d\u63d0\u53d6\u5e76\u6574\u5408\u6709\u6548\u4fe1\u606f\uff1b2\uff09\u8bbe\u8ba1\u5149\u7167\u5f15\u5bfc\u7684\u4e8b\u4ef6\u589e\u5f3a\u6a21\u5757\uff0c\u5229\u7528Retinex\u6a21\u578b\u7684\u5149\u7167\u56fe\u9010\u6b65\u7cbe\u70bc\u4e8b\u4ef6\u7279\u5f81\uff1b3\uff09\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u7684\u53cd\u5c04\u7387\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u878d\u5408\u673a\u5236\u52a8\u6001\u6062\u590d\u53cd\u5c04\u7387\u7ec6\u8282\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728SDSD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\uff0c\u83b7\u5f97\u9ad8\u8fbe2.95 dB\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c1165%\u7684\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "RetinexEVSR\u901a\u8fc7\u6709\u6548\u6574\u5408\u4e8b\u4ef6\u4fe1\u53f7\u548cRetinex\u5148\u9a8c\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u6311\u6218\uff0c\u5728\u6062\u590d\u7ec6\u8282\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.00391", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00391", "abs": "https://arxiv.org/abs/2601.00391", "authors": ["Nouar AlDahoul", "Aznul Qalid Md Sabri", "Ali Mohammed Mansoor"], "title": "Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models", "comment": null, "summary": "Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u5149\u6d41\u548c\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08S-CNN\u3001\u9884\u8bad\u7ec3CNN\u3001H-ELM\uff09\u7528\u4e8e\u65e0\u4eba\u673a\u89c6\u9891\u4e2d\u7684\u4eba\u4f53\u68c0\u6d4b\uff0c\u5728UCF-ARG\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u65b9\u6cd5\u5bf9\u52a8\u6001\u4e8b\u4ef6\uff08\u5149\u7167\u53d8\u5316\u3001\u76f8\u673a\u6296\u52a8\u7b49\uff09\u654f\u611f\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u3001\u81ea\u52a8\u5316\u7684\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5\u7528\u4e8e\u65e0\u4eba\u673a\u89c6\u9891\u4e2d\u7684\u4eba\u4f53\u68c0\u6d4b\u3002", "method": "\u7ed3\u5408\u5149\u6d41\u7279\u5f81\u4e0e\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1a1) \u76d1\u7763\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08S-CNN\uff09\uff0c2) \u9884\u8bad\u7ec3CNN\u7279\u5f81\u63d0\u53d6\u5668\uff0c3) \u5206\u5c42\u6781\u9650\u5b66\u4e60\u673a\uff08H-ELM\uff09\uff0c\u5728UCF-ARG\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u9884\u8bad\u7ec3CNN\u5e73\u5747\u51c6\u786e\u738798.09%\uff0cS-CNN\uff08softmax\uff0995.6%\uff0cS-CNN\uff08SVM\uff0991.7%\uff0cH-ELM 95.9%\u3002H-ELM\u5728CPU\u4e0a\u8bad\u7ec3445\u79d2\uff0cS-CNN\u5728GPU\u4e0a\u8bad\u7ec3770\u79d2\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u89c6\u9891\u4eba\u4f53\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6210\u529f\uff0c\u9884\u8bad\u7ec3CNN\u6548\u679c\u6700\u4f73\uff0cH-ELM\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u4f18\u52bf\u3002"}}
