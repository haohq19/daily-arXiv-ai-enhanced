{"id": "2508.06554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06554", "abs": "https://arxiv.org/abs/2508.06554", "authors": ["Abdelhaleem Saad", "Waseem Akram", "Irfan Hussain"], "title": "AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance", "comment": null, "summary": "Inspection of aquaculture net pens is essential for ensuring the structural\nintegrity and sustainable operation of offshore fish farming systems.\nTraditional methods, typically based on manually operated or single-ROV\nsystems, offer limited adaptability to real-time constraints such as energy\nconsumption, hardware faults, and dynamic underwater conditions. This paper\nintroduces AquaChat++, a novel multi-ROV inspection framework that uses Large\nLanguage Models (LLMs) to enable adaptive mission planning, coordinated task\nexecution, and fault-tolerant control in complex aquaculture environments. The\nproposed system consists of a two-layered architecture. The high-level plan\ngeneration layer employs an LLM, such as ChatGPT-4, to translate natural\nlanguage user commands into symbolic, multi-agent inspection plans. A task\nmanager dynamically allocates and schedules actions among ROVs based on their\nreal-time status and operational constraints, including thruster faults and\nbattery levels. The low-level control layer ensures accurate trajectory\ntracking and integrates thruster fault detection and compensation mechanisms.\nBy incorporating real-time feedback and event-triggered replanning, AquaChat++\nenhances system robustness and operational efficiency. Simulated experiments in\na physics-based aquaculture environment demonstrate improved inspection\ncoverage, energy-efficient behavior, and resilience to actuator failures. These\nfindings highlight the potential of LLM-driven frameworks to support scalable,\nintelligent, and autonomous underwater robotic operations within the\naquaculture sector.", "AI": {"tldr": "AquaChat++\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591aROV\u68c0\u67e5\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u7684\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u548c\u5bb9\u9519\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\u65b9\u6cd5\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5b9e\u65f6\u7ea6\u675f\uff08\u5982\u80fd\u8017\u3001\u786c\u4ef6\u6545\u969c\u548c\u52a8\u6001\u6c34\u4e0b\u73af\u5883\uff09\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u4f7f\u7528LLM\u751f\u6210\u591a\u667a\u80fd\u4f53\u68c0\u67e5\u8ba1\u5212\uff0c\u4f4e\u5c42\u5b9e\u73b0\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6545\u969c\u8865\u507f\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u63d0\u9ad8\u4e86\u68c0\u67e5\u8986\u76d6\u7387\u3001\u80fd\u6548\u548c\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6846\u67b6\u6709\u671b\u652f\u6301\u6c34\u4ea7\u517b\u6b96\u4e2d\u53ef\u6269\u5c55\u3001\u667a\u80fd\u548c\u81ea\u4e3b\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2508.06601", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06601", "abs": "https://arxiv.org/abs/2508.06601", "authors": ["Kyle O'Brien", "Stephen Casper", "Quentin Anthony", "Tomek Korbak", "Robert Kirk", "Xander Davies", "Ishan Mishra", "Geoffrey Irving", "Yarin Gal", "Stella Biderman"], "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs", "comment": "https://deepignorance.ai/", "summary": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u53cc\u7528\u9014\u4e3b\u9898\u6587\u672c\u6765\u589e\u5f3a\u5f00\u653e\u6743\u91cdAI\u7cfb\u7edf\u7684\u6297\u7be1\u6539\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5bf9\u751f\u7269\u5a01\u80c1\u77e5\u8bc6\u7684\u638c\u63e1\uff0c\u5e76\u5728\u5bf9\u6297\u6027\u5fae\u8c03\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u653e\u6743\u91cdAI\u7cfb\u7edf\u867d\u5177\u6709\u900f\u660e\u6027\u548c\u5f00\u653e\u6027\u7b49\u4f18\u52bf\uff0c\u4f46\u6613\u53d7\u7be1\u6539\u653b\u51fb\uff0c\u73b0\u6709\u5b89\u5168\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u62b5\u5fa1\u591a\u6b65\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6570\u636e\u8fc7\u6ee4\u589e\u5f3a\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u79fb\u9664\u53cc\u7528\u9014\u4e3b\u9898\u6587\u672c\uff0c\u5e76\u9884\u8bad\u7ec3\u591a\u4e2a6.9B\u53c2\u6570\u6a21\u578b\u3002", "result": "\u8fc7\u6ee4\u540e\u7684\u6a21\u578b\u5728\u5bf9\u6297\u6027\u5fae\u8c03\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u53ef\u62b5\u5fa110,000\u6b65\u653b\u51fb\uff09\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u4ed6\u80fd\u529b\u3002\u4f46\u6a21\u578b\u4ecd\u53ef\u901a\u8fc7\u4e0a\u4e0b\u6587\u5229\u7528\u5371\u9669\u77e5\u8bc6\uff0c\u8868\u660e\u9700\u591a\u5c42\u6b21\u9632\u5fa1\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6570\u636e\u8fc7\u6ee4\u662f\u5f00\u653e\u6743\u91cdAI\u7cfb\u7edf\u7684\u4e00\u79cd\u6709\u6548\u9632\u5fa1\u5c42\uff0c\u4f46\u9700\u7ed3\u5408\u5176\u4ed6\u9632\u5fa1\u63aa\u65bd\u4ee5\u5b9e\u73b0\u5168\u9762\u4fdd\u62a4\u3002"}}
{"id": "2508.07003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07003", "abs": "https://arxiv.org/abs/2508.07003", "authors": ["Siyu Chen", "Shenghai Yuan", "Thien-Minh Nguyen", "Zhuyu Huang", "Chenyang Shi", "Jin Jing", "Lihua Xie"], "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events", "comment": "Accepted by IEEE RAL", "summary": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over\ntraditional SLAM methods, enabling photorealistic 3D reconstruction that\nconventional approaches often struggle to achieve. However, existing GS-SLAM\nsystems perform poorly under persistent and severe motion blur commonly\nencountered in real-world scenarios, leading to significantly degraded tracking\naccuracy and compromised 3D reconstruction quality. To address this limitation,\nwe propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D\ninputs to simultaneously reduce motion blur in images and compensate for the\nsparse and discrete nature of event streams, enabling robust tracking and\nhigh-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system\nexplicitly models the camera's continuous trajectory during exposure,\nsupporting event- and blur-aware tracking and mapping on a unified 3D Gaussian\nSplatting scene. Furthermore, we introduce a learnable camera response function\nto align the dynamic ranges of events and images, along with a no-event loss to\nsuppress ringing artifacts during reconstruction. We validate our approach on a\nnew dataset comprising synthetic and real-world sequences with significant\nmotion blur. Extensive experimental results demonstrate that EGS-SLAM\nconsistently outperforms existing GS-SLAM systems in both trajectory accuracy\nand photorealistic 3D Gaussian Splatting reconstruction. The source code will\nbe available at https://github.com/Chensiyu00/EGS-SLAM.", "AI": {"tldr": "EGS-SLAM\u901a\u8fc7\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGS-SLAM\u5728\u8fd0\u52a8\u6a21\u7cca\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u8ddf\u8e2a\u548c\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709GS-SLAM\u7cfb\u7edf\u5728\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faEGS-SLAM\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u5efa\u6a21\u76f8\u673a\u8fde\u7eed\u8f68\u8ff9\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u548c\u65e0\u4e8b\u4ef6\u635f\u5931\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cEGS-SLAM\u5728\u8f68\u8ff9\u7cbe\u5ea6\u548c3D\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "EGS-SLAM\u663e\u8457\u63d0\u5347\u4e86\u5728\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.06859", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06859", "abs": "https://arxiv.org/abs/2508.06859", "authors": ["Shuo Tang", "Jian Xu", "Jiadong Zhang", "Yi Chen", "Qizhao Jin", "Lingdong Shen", "Chenglin Liu", "Shiming Xiang"], "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction", "comment": null, "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u7aef\u5230\u7aef\u5929\u6c14\u9884\u8b66\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6MP-Bench\u548c\u5f00\u53d1\u6c14\u8c61\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MMLM\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u3001\u6570\u636e\u4e0d\u8db3\u548c\u5bf9\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u5904\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5929\u6c14\u9884\u8b66\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u4e13\u5bb6\u89e3\u91ca\uff0c\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u64cd\u4f5c\u8d1f\u62c5\u3002AI\u6280\u672f\u7684\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u5929\u6c14\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u7ef4\u6570\u636e\u5bf9\u9f50\u56f0\u96be\u548c\u591a\u6a21\u6001\u6a21\u578b\u80fd\u529b\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efaMP-Bench\u6570\u636e\u96c6\uff08\u5305\u542b421,363\u5bf9\u6c14\u8c61\u6570\u636e\u4e0e\u6587\u672c\u63cf\u8ff0\uff09\uff0c\u5f00\u53d1MMLM\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u4e2a\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u5904\u74064D\u6c14\u8c61\u6570\u636e\u3002", "result": "MMLM\u5728MP-Bench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5929\u6c14\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5929\u6c14\u9884\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "MMLM\u548cMP-Bench\u4e3a\u81ea\u52a8\u5316\u5929\u6c14\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u652f\u6301\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.06623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06623", "abs": "https://arxiv.org/abs/2508.06623", "authors": ["Sihan Ma", "Qiming Wu", "Ruotong Jiang", "Frank Burns"], "title": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification", "comment": null, "summary": "The proliferation of digital news media necessitates robust methods for\nverifying content veracity, particularly regarding the consistency between\nvisual and textual information. Traditional approaches often fall short in\naddressing the fine-grained cross-modal contextual consistency (FCCC) problem,\nwhich encompasses deeper alignment of visual narrative, emotional tone, and\nbackground information with text, beyond mere entity matching. To address this,\nwe propose ContextGuard-LVLM, a novel framework built upon advanced\nVision-Language Large Models (LVLMs) and integrating a multi-stage contextual\nreasoning mechanism. Our model is uniquely enhanced through reinforced or\nadversarial learning paradigms, enabling it to detect subtle contextual\nmisalignments that evade zero-shot baselines. We extend and augment three\nestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new\nfine-grained contextual annotations, including \"contextual sentiment,\" \"visual\nnarrative theme,\" and \"scene-event logical coherence,\" and introduce a\ncomprehensive CTXT (Contextual Coherence) entity type. Extensive experiments\ndemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art\nzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all\nfine-grained consistency tasks, showing significant improvements in complex\nlogical reasoning and nuanced contextual understanding. Furthermore, our model\nexhibits superior robustness to subtle perturbations and a higher agreement\nrate with human expert judgments on challenging samples, affirming its efficacy\nin discerning sophisticated forms of context detachment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faContextGuard-LVLM\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u548c\u591a\u9636\u6bb5\u4e0a\u4e0b\u6587\u63a8\u7406\u673a\u5236\uff0c\u89e3\u51b3\u6570\u5b57\u65b0\u95fb\u4e2d\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u65b0\u95fb\u5a92\u4f53\u7684\u666e\u53ca\u9700\u8981\u9a8c\u8bc1\u5185\u5bb9\u771f\u5b9e\u6027\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u6df1\u5c42\u4e00\u81f4\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLMs\uff09\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u4e0a\u4e0b\u6587\u63a8\u7406\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u6216\u5bf9\u6297\u5b66\u4e60\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "result": "ContextGuard-LVLM\u5728\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u68c0\u6d4b\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u65b9\u9762\u9ad8\u6548\u4e14\u9c81\u68d2\uff0c\u80fd\u591f\u8bc6\u522b\u590d\u6742\u7684\u60c5\u5883\u8131\u79bb\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2508.07387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07387", "abs": "https://arxiv.org/abs/2508.07387", "authors": ["Basant Sharma", "Prajyot Jadhav", "Pranjal Paul", "K. Madhava Krishna", "Arun Kumar Singh"], "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "comment": null, "summary": "Navigating unknown environments with a single RGB camera is challenging, as\nthe lack of depth information prevents reliable collision-checking. While some\nmethods use estimated depth to build collision maps, we found that depth\nestimates from vision foundation models are too noisy for zero-shot navigation\nin cluttered environments.\n  We propose an alternative approach: instead of using noisy estimated depth\nfor direct collision-checking, we use it as a rich context input to a learned\ncollision model. This model predicts the distribution of minimum obstacle\nclearance that the robot can expect for a given control sequence. At inference,\nthese predictions inform a risk-aware MPC planner that minimizes estimated\ncollision risk. Our joint learning pipeline co-trains the collision model and\nrisk metric using both safe and unsafe trajectories. Crucially, our\njoint-training ensures optimal variance in our collision model that improves\nnavigation in highly cluttered environments. Consequently, real-world\nexperiments show 9x and 7x improvements in success rates over NoMaD and the ROS\nstack, respectively. Ablation studies further validate the effectiveness of our\ndesign choices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u566a\u58f0\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u901a\u8fc7\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u5355RGB\u76f8\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u78b0\u649e\u68c0\u6d4b\u4e0d\u53ef\u9760\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u566a\u58f0\u8fc7\u5927\uff0c\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u96f6\u6837\u672c\u5bfc\u822a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u566a\u58f0\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u9884\u6d4b\u673a\u5668\u4eba\u63a7\u5236\u5e8f\u5217\u7684\u6700\u5c0f\u969c\u788d\u7269\u95f4\u9699\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\u4f18\u5316\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6bd4NoMaD\u548cROS\u5806\u6808\u5206\u522b\u63d0\u9ad8\u4e869\u500d\u548c7\u500d\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8054\u5408\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u548c\u98ce\u9669\u5ea6\u91cf\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u9ad8\u5ea6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06763", "abs": "https://arxiv.org/abs/2508.06763", "authors": ["Zihao Sheng", "Zilin Huang", "Yen-Jung Chen", "Yansong Qu", "Yuhao Luo", "Yue Leng", "Sikai Chen"], "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding", "comment": "The code, dataset, and model checkpoints will be made publicly\n  available at: https://zihaosheng.github.io/SafePLUG", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG", "AI": {"tldr": "SafePLUG\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4ea4\u901a\u4e8b\u6545\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u652f\u6301\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u4ea4\u901a\u9886\u57df\u4e3b\u8981\u5173\u6ce8\u7c97\u7c92\u5ea6\u7406\u89e3\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u6216\u5c40\u90e8\u573a\u666f\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u4e8b\u6545\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSafePLUG\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u5f62\u72b6\u89c6\u89c9\u63d0\u793a\u7684\u533a\u57df\u611f\u77e5\u95ee\u7b54\u3001\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u7684\u50cf\u7d20\u7ea7\u5206\u5272\u53ca\u65f6\u95f4\u951a\u5b9a\u4e8b\u4ef6\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafePLUG\u5728\u533a\u57df\u95ee\u7b54\u3001\u50cf\u7d20\u5206\u5272\u3001\u65f6\u95f4\u4e8b\u4ef6\u5b9a\u4f4d\u548c\u4e8b\u6545\u7406\u89e3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SafePLUG\u4e3a\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u5960\u5b9a\u57fa\u7840\uff0c\u6709\u671b\u63d0\u5347\u9a7e\u9a76\u5b89\u5168\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.07657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07657", "abs": "https://arxiv.org/abs/2508.07657", "authors": ["Zhuoli Tian", "Yuyang Zhang", "Jinsheng Wei", "Meng Guo"], "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication", "comment": "38 pages, 28 figures, Submitted to the International Journal of\n  Robotics Research (IJRR). Project website: https://zl-tian.github.io/MoRoCo/", "summary": "Fleets of autonomous robots are increasingly deployed alongside multiple\nhuman operators to explore unknown environments, identify salient features, and\nperform complex tasks in scenarios such as subterranean exploration,\nreconnaissance, and search-and-rescue missions. In these contexts,\ncommunication is often severely limited to short-range exchanges via ad-hoc\nnetworks, posing challenges to coordination. While recent studies have\naddressed multi-robot exploration under communication constraints, they largely\noverlook the essential role of human operators and their real-time interaction\nwith robotic teams. Operators may demand timely updates on the exploration\nprogress and robot status, reprioritize or cancel tasks dynamically, or request\nlive video feeds and control access. Conversely, robots may seek human\nconfirmation for anomalous events or require help recovering from motion or\nplanning failures. To enable such bilateral, context-aware interactions under\nrestricted communication, this work proposes MoRoCo, a unified framework for\nonline coordination and exploration in multi-operator, multi-robot systems.\nMoRoCo enables the team to adaptively switch among three coordination modes:\nspread mode for parallelized exploration with intermittent data sharing,\nmigrate mode for coordinated relocation, and chain mode for maintaining\nhigh-bandwidth connectivity through multi-hop links. These transitions are\nmanaged through distributed algorithms via only local communication. Extensive\nlarge-scale human-in-the-loop simulations and hardware experiments validate the\nnecessity of incorporating human robot interactions and demonstrate that MoRoCo\nenables efficient, reliable coordination under limited communication, marking a\nsignificant step toward robust human-in-the-loop multi-robot autonomy in\nchallenging environments.", "AI": {"tldr": "MoRoCo\u662f\u4e00\u4e2a\u591a\u64cd\u4f5c\u8005\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u53cc\u8fb9\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ea4\u4e92\u548c\u9ad8\u6548\u63a2\u7d22\u3002", "motivation": "\u5728\u901a\u4fe1\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5ffd\u89c6\u4eba\u7c7b\u64cd\u4f5c\u8005\u4e0e\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b9e\u65f6\u4e92\u52a8\u9700\u6c42\uff0c\u800cMoRoCo\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MoRoCo\u901a\u8fc7\u5206\u5e03\u5f0f\u7b97\u6cd5\u7ba1\u7406\u4e09\u79cd\u534f\u8c03\u6a21\u5f0f\uff08spread\u3001migrate\u3001chain\uff09\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u901a\u4fe1\u5b9e\u73b0\u81ea\u9002\u5e94\u5207\u6362\u3002", "result": "\u5927\u89c4\u6a21\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MoRoCo\u5728\u901a\u4fe1\u53d7\u9650\u4e0b\u7684\u9ad8\u6548\u3001\u53ef\u9760\u534f\u8c03\u80fd\u529b\u3002", "conclusion": "MoRoCo\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u4eba\u673a\u534f\u540c\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.07686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07686", "abs": "https://arxiv.org/abs/2508.07686", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jiaqi Ma", "Jia Hu"], "title": "Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning", "comment": null, "summary": "End-to-end paradigm has emerged as a promising approach to autonomous\ndriving. However, existing single-agent end-to-end pipelines are often\nconstrained by occlusion and limited perception range, resulting in hazardous\ndriving. Furthermore, their black-box nature prevents the interpretability of\nthe driving behavior, leading to an untrustworthiness system. To address these\nlimitations, we introduce Risk Map as Middleware (RiskMM) and propose an\ninterpretable cooperative end-to-end driving framework. The risk map learns\ndirectly from the driving data and provides an interpretable spatiotemporal\nrepresentation of the scenario from the upstream perception and the\ninteractions between the ego vehicle and the surrounding environment for\ndownstream planning. RiskMM first constructs a multi-agent spatiotemporal\nrepresentation with unified Transformer-based architecture, then derives\nrisk-aware representations by modeling interactions among surrounding\nenvironments with attention. These representations are subsequently fed into a\nlearning-based Model Predictive Control (MPC) module. The MPC planner\ninherently accommodates physical constraints and different vehicle types and\ncan provide interpretation by aligning learned parameters with explicit MPC\nelements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm\nthat RiskMM achieves superior and robust performance in risk-aware trajectory\nplanning, significantly enhancing the interpretability of the cooperative\nend-to-end driving framework. The codebase will be released to facilitate\nfuture research in this field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u5730\u56fe\u4e2d\u95f4\u4ef6\uff08RiskMM\uff09\u7684\u53ef\u89e3\u91ca\u534f\u4f5c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5355\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u9a7e\u9a76\u7684\u906e\u6321\u3001\u611f\u77e5\u8303\u56f4\u9650\u5236\u53ca\u9ed1\u76d2\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u9a7e\u9a76\u7cfb\u7edf\u56e0\u906e\u6321\u548c\u611f\u77e5\u8303\u56f4\u53d7\u9650\u5bfc\u81f4\u5371\u9669\u9a7e\u9a76\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u98ce\u9669\u5730\u56fe\u5b66\u4e60\u9a7e\u9a76\u6570\u636e\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u65f6\u7a7a\u8868\u793a\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5efa\u6a21\u73af\u5883\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6a21\u5757\u3002", "result": "\u5728V2XPnP-Seq\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86RiskMM\u5728\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "RiskMM\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.08108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08108", "abs": "https://arxiv.org/abs/2508.08108", "authors": ["Wei Zhang", "Yinchuan Wang", "Wangtao Lu", "Pengyu Zhang", "Xiang Zhang", "Yue Wang", "Chaoqun Wang"], "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain", "comment": null, "summary": "It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u503e\u8986\u8f68\u8ff9\u89c4\u5212\u5668\uff08CAP\uff09\uff0c\u7528\u4e8e\u5728\u5d0e\u5c96\u5730\u5f62\u4e0a\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\u3002", "motivation": "\u5730\u9762\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u65f6\uff0c\u9762\u4e34\u975e\u5e73\u51e1\u969c\u788d\u548c\u5d0e\u5c96\u5730\u5f62\u7684\u6311\u6218\uff0c\u9700\u5e73\u8861\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u5206\u6790\u673a\u5668\u4eba\u503e\u8986\u7a33\u5b9a\u6027\uff0c\u5b9a\u4e49\u53ef\u7a7f\u8d8a\u65b9\u5411\uff0c\u5c06\u5176\u878d\u5165\u8f68\u8ff9\u4f18\u5316\u7684\u9632\u503e\u8986\u5b89\u5168\u7ea6\u675f\u4e2d\uff0c\u5e76\u91c7\u7528\u56fe\u6c42\u89e3\u5668\u8ba1\u7b97\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CAP\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAP\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u5d0e\u5c96\u5730\u5f62\u4e0a\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2508.07114", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.07114", "abs": "https://arxiv.org/abs/2508.07114", "authors": ["Atakan Azakli", "Bernd Stelzer"], "title": "Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning", "comment": null, "summary": "In this work, we propose a new machine learning (ML) methodology to obtain\nmore precise predictions for some parameters of interest in a given hypotheses\ntesting problem. Our proposed method also allows ML models to have more\ndiscriminative power in cases where it is extremely challenging for\nstate-of-the-art classifiers to have any level of accurate predictions. This\nmethod can also allow us to systematically decrease the error from ML models in\ntheir predictions. In this paper, we provide a mathematical motivation why\nMultiple Instance Learning (MIL) would have more predictive power over their\nsingle-instance counterparts. We support our theoretical claims by analyzing\nthe behavior of the MIL models through their scaling behaviors with respect to\nthe number of instances on which the model makes predictions. As a concrete\napplication, we constrain Wilson coefficients of the Standard Model Effective\nField Theory (SMEFT) using kinematic information from subatomic particle\ncollision events at the Large Hadron Collider (LHC). We show that under certain\ncircumstances, it might be possible to extract the theoretical maximum Fisher\nInformation latent in a dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u63d0\u9ad8\u5047\u8bbe\u68c0\u9a8c\u4e2d\u53c2\u6570\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u5224\u522b\u529b\uff0c\u5e76\u7cfb\u7edf\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u5728\u73b0\u6709\u5206\u7c7b\u5668\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5224\u522b\u529b\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u5229\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u6570\u5b66\u4f18\u52bf\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u5b9e\u4f8b\u6570\u91cf\u4e0b\u7684\u7f29\u653e\u884c\u4e3a\u3002", "result": "\u5728\u6807\u51c6\u6a21\u578b\u6709\u6548\u573a\u8bba\uff08SMEFT\uff09\u4e2d\u7ea6\u675fWilson\u7cfb\u6570\uff0c\u5c55\u793a\u4e86\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u63d0\u53d6\u6570\u636e\u96c6\u4e2d\u7684\u6700\u5927Fisher\u4fe1\u606f\u3002", "conclusion": "MIL\u65b9\u6cd5\u5728\u590d\u6742\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u80fd\u529b\u548c\u7406\u8bba\u4f18\u52bf\u3002"}}
{"id": "2508.07598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07598", "abs": "https://arxiv.org/abs/2508.07598", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.", "AI": {"tldr": "KeyCP++\u901a\u8fc7\u5173\u952e\u8bcd\u9a71\u52a8\u7684\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LLM\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u8fc7\u89e3\u91ca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "LLM\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u56e0\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u89e6\u53d1\u8bcd\u7684\u51c6\u786e\u7406\u89e3\u800c\u5bb9\u6613\u8fc7\u89e3\u91ca\uff0c\u4ec5\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\u96be\u4ee5\u7ea0\u6b63\u3002", "method": "\u63d0\u51faKeyCP++\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u8f93\u5165\u6587\u672c\u4e0e\u68c0\u6d4b\u7ed3\u679c\u95f4\u7684\u903b\u8f91\u5dee\u8ddd\uff0c\u6784\u5efa\u89e6\u53d1\u8bcd\u5224\u522b\u63d0\u793a\u6a21\u677f\uff0c\u7ed3\u5408\u5173\u952e\u8bcd\u751f\u6210\u5019\u9009\u89e6\u53d1\u8bcd\u5e76\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eKeyCP++\u5728\u4e00\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "KeyCP++\u901a\u8fc7\u51cf\u5c11\u5bf9\u5173\u952e\u8bcd\u7684\u8fc7\u5ea6\u4f9d\u8d56\u5e76\u4fc3\u8fdb\u68c0\u6d4b\u89c4\u5219\u5b66\u4e60\uff0c\u6709\u6548\u6539\u8fdb\u4e86LLM\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07137", "abs": "https://arxiv.org/abs/2508.07137", "authors": ["Yuandong Tan"], "title": "A Stable and Principled Loss Function for Direct Language Model Alignment", "comment": null, "summary": "The alignment of large language models (LLMs) with human preferences is\ncommonly achieved through Reinforcement Learning from Human Feedback (RLHF).\nDirect Preference Optimization (DPO) simplified this paradigm by establishing a\ndirect mapping between the optimal policy and a reward function, eliminating\nthe need for an explicit reward model. However, we argue that the DPO loss\nfunction is theoretically misaligned with its own derivation, as it promotes\nthe indefinite maximization of a logits difference, which can lead to training\ninstability and reward hacking. In this paper, we propose a novel loss function\nderived directly from the RLHF optimality condition. Our proposed loss targets\na specific, finite value for the logits difference, which is dictated by the\nunderlying reward, rather than its maximization. We provide a theoretical\nanalysis, including a gradient-based comparison, to demonstrate that our method\navoids the large gradients that plague DPO when the probability of dispreferred\nresponses approaches zero. This inherent stability prevents reward hacking and\nleads to more effective alignment. We validate our approach by fine-tuning a\nQwen2.5-7B model, showing significant win-rate improvements over a standard DPO\nbaseline and achieving competitive performance against larger models like\nLlama-3.1-8B.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86Direct Preference Optimization (DPO) \u4e2d\u56e0\u65e0\u9650\u6700\u5927\u5316\u5bf9\u6570\u5dee\u800c\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u653b\u51fb\u95ee\u9898\u3002", "motivation": "DPO\u7684\u635f\u5931\u51fd\u6570\u4e0e\u5176\u7406\u8bba\u63a8\u5bfc\u4e0d\u4e00\u81f4\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u4eceRLHF\u6700\u4f18\u6761\u4ef6\u76f4\u63a5\u63a8\u5bfc\u51fa\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u76ee\u6807\u662f\u5bf9\u6570\u5dee\u7684\u7279\u5b9a\u6709\u9650\u503c\uff0c\u800c\u975e\u6700\u5927\u5316\u3002", "result": "\u65b0\u65b9\u6cd5\u907f\u514d\u4e86DPO\u4e2d\u7684\u5927\u68af\u5ea6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6DPO\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u66f4\u7a33\u5b9a\u4e14\u6709\u6548\uff0c\u5728\u6a21\u578b\u5bf9\u9f50\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.07428", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07428", "abs": "https://arxiv.org/abs/2508.07428", "authors": ["Md Sultanul Arifin", "Abu Nowshed Sakib", "Yeasir Rayhan", "Tanzima Hashem"], "title": "Lightning Prediction under Uncertainty: DeepLight with Hazy Loss", "comment": null, "summary": "Lightning, a common feature of severe meteorological conditions, poses\nsignificant risks, from direct human injuries to substantial economic losses.\nThese risks are further exacerbated by climate change. Early and accurate\nprediction of lightning would enable preventive measures to safeguard people,\nprotect property, and minimize economic losses. In this paper, we present\nDeepLight, a novel deep learning architecture for predicting lightning\noccurrences. Existing prediction models face several critical limitations: they\noften struggle to capture the dynamic spatial context and inherent uncertainty\nof lightning events, underutilize key observational data, such as radar\nreflectivity and cloud properties, and rely heavily on Numerical Weather\nPrediction (NWP) systems, which are both computationally expensive and highly\nsensitive to parameter settings. To overcome these challenges, DeepLight\nleverages multi-source meteorological data, including radar reflectivity, cloud\nproperties, and historical lightning occurrences through a dual-encoder\narchitecture. By employing multi-branch convolution techniques, it dynamically\ncaptures spatial correlations across varying extents. Furthermore, its novel\nHazy Loss function explicitly addresses the spatio-temporal uncertainty of\nlightning by penalizing deviations based on proximity to true events, enabling\nthe model to better learn patterns amidst randomness. Extensive experiments\nshow that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over\nstate-of-the-art methods, establishing it as a robust solution for lightning\nprediction.", "AI": {"tldr": "DeepLight\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u9884\u6d4b\u95ea\u7535\u53d1\u751f\uff0c\u901a\u8fc7\u591a\u6e90\u6c14\u8c61\u6570\u636e\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u95ea\u7535\u5bf9\u4eba\u548c\u7ecf\u6d4e\u9020\u6210\u91cd\u5927\u98ce\u9669\uff0c\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u5728\u52a8\u6001\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6355\u6349\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u3002", "method": "DeepLight\u5229\u7528\u96f7\u8fbe\u53cd\u5c04\u7387\u3001\u4e91\u5c5e\u6027\u548c\u5386\u53f2\u95ea\u7535\u6570\u636e\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u591a\u5206\u652f\u5377\u79ef\u6280\u672f\uff0c\u52a8\u6001\u6355\u6349\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u4f7f\u7528Hazy Loss\u51fd\u6570\u5904\u7406\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepLight\u7684\u516c\u5e73\u5a01\u80c1\u8bc4\u5206\uff08ETS\uff09\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8618%-30%\u3002", "conclusion": "DeepLight\u4e3a\u95ea\u7535\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07171", "abs": "https://arxiv.org/abs/2508.07171", "authors": ["Huihui Xu", "Jiashi Lin", "Haoyu Chen", "Junjun He", "Lei Zhu"], "title": "EventRR: Event Referential Reasoning for Referring Video Object Segmentation", "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment out the object in\na video referred by an expression. Current RVOS methods view referring\nexpressions as unstructured sequences, neglecting their crucial semantic\nstructure essential for referent reasoning. Besides, in contrast to\nimage-referring expressions whose semantics focus only on object attributes and\nobject-object relations, video-referring expressions also encompass event\nattributes and event-event temporal relations. This complexity challenges\ntraditional structured reasoning image approaches. In this paper, we propose\nthe Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS\ninto object summarization part and referent reasoning part. The summarization\nphase begins by summarizing each frame into a set of bottleneck tokens, which\nare then efficiently aggregated in the video-level summarization step to\nexchange the global cross-modal temporal context. For reasoning part, EventRR\nextracts semantic eventful structure of a video-referring expression into\nhighly expressive Referential Event Graph (REG), which is a single-rooted\ndirected acyclic graph. Guided by topological traversal of REG, we propose\nTemporal Concept-Role Reasoning (TCRR) to accumulate the referring score of\neach temporal query from REG leaf nodes to root node. Each reasoning step can\nbe interpreted as a question-answer pair derived from the concept-role\nrelations in REG. Extensive experiments across four widely recognized benchmark\ndatasets, show that EventRR quantitatively and qualitatively outperforms\nstate-of-the-art RVOS methods. Code is available at\nhttps://github.com/bio-mlhui/EventRR", "AI": {"tldr": "EventRR\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e3a\u5bf9\u8c61\u6458\u8981\u548c\u5f15\u7528\u63a8\u7406\u4e24\u90e8\u5206\uff0c\u5229\u7528Referential Event Graph\uff08REG\uff09\u548cTemporal Concept-Role Reasoning\uff08TCRR\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RVOS\u65b9\u6cd5\u5ffd\u7565\u8868\u8fbe\u5f0f\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u800c\u89c6\u9891\u5f15\u7528\u8868\u8fbe\u5f0f\u6bd4\u56fe\u50cf\u66f4\u590d\u6742\uff0c\u5305\u542b\u4e8b\u4ef6\u5c5e\u6027\u548c\u65f6\u5e8f\u5173\u7cfb\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u3002", "method": "EventRR\u5206\u4e3a\u5bf9\u8c61\u6458\u8981\u548c\u5f15\u7528\u63a8\u7406\u4e24\u90e8\u5206\uff1a\u6458\u8981\u9636\u6bb5\u751f\u6210\u74f6\u9888\u4ee4\u724c\u5e76\u805a\u5408\u5168\u5c40\u8de8\u6a21\u6001\u65f6\u5e8f\u4e0a\u4e0b\u6587\uff1b\u63a8\u7406\u9636\u6bb5\u901a\u8fc7REG\u548cTCRR\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cEventRR\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EventRR\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u4e49\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86RVOS\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08211", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08211", "abs": "https://arxiv.org/abs/2508.08211", "authors": ["Zhuohao Yu", "Xingru Jiang", "Weizheng Gu", "Yidong Wang", "Shikun Zhang", "Wei Ye"], "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling", "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark", "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.", "AI": {"tldr": "SAEMark\u662f\u4e00\u79cd\u540e\u5904\u7406\u591a\u6bd4\u7279\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u57fa\u4e8e\u7279\u5f81\u7684\u62d2\u7edd\u91c7\u6837\u5d4c\u5165\u4e2a\u6027\u5316\u6d88\u606f\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u903b\u8f91\u6216\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u5c01\u95ed\u6e90LLM\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u4f1a\u964d\u4f4e\u6587\u672c\u8d28\u91cf\uff0c\u4e14\u9700\u8981\u767d\u76d2\u6a21\u578b\u8bbf\u95ee\u548c\u903b\u8f91\u64cd\u4f5c\uff0c\u9650\u5236\u4e86API\u6a21\u578b\u548c\u591a\u8bed\u8a00\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "SAEMark\u5229\u7528\u751f\u6210\u6587\u672c\u7684\u786e\u5b9a\u6027\u7279\u5f81\uff0c\u9009\u62e9\u7279\u5f81\u7edf\u8ba1\u4e0e\u5bc6\u94a5\u76ee\u6807\u4e00\u81f4\u7684\u8f93\u51fa\uff0c\u901a\u8fc7\u91c7\u6837\u800c\u975e\u4fee\u6539\u5b9e\u73b0\u6c34\u5370\u5d4c\u5165\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSAEMark\u5b9e\u73b0\u4e8699.7%\u7684F1\u5206\u6570\uff08\u82f1\u8bed\uff09\u548c\u5f3a\u5927\u7684\u591a\u6bd4\u7279\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "SAEMark\u4e3a\u53ef\u6269\u5c55\u6c34\u5370\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u5c01\u95ed\u6e90LLM\uff0c\u540c\u65f6\u652f\u6301\u5185\u5bb9\u6eaf\u6e90\u3002"}}
{"id": "2508.07260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07260", "abs": "https://arxiv.org/abs/2508.07260", "authors": ["Sihan Yang", "Huitong Ji", "Shaolin Lu", "Jiayi Chen", "Binxiao Xu", "Ming Lu", "Yuanxing Zhang", "Wenhui Dong", "Wentao Zhang"], "title": "Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM", "comment": null, "summary": "Personalizing Vision-Language Models (VLMs) to transform them into daily\nassistants has emerged as a trending research direction. However, leading\ncompanies like OpenAI continue to increase model size and develop complex\ndesigns such as the chain of thought (CoT). While large VLMs are proficient in\ncomplex multi-modal understanding, their high training costs and limited access\nvia paid APIs restrict direct personalization. Conversely, small VLMs are\neasily personalized and freely available, but they lack sufficient reasoning\ncapabilities. Inspired by this, we propose a novel collaborative framework\nnamed Small-Large Collaboration (SLC) for large VLM personalization, where the\nsmall VLM is responsible for generating personalized information, while the\nlarge model integrates this personalized information to deliver accurate\nresponses. To effectively incorporate personalized information, we develop a\ntest-time reflection strategy, preventing the potential hallucination of the\nsmall VLM. Since SLC only needs to train a meta personalized small VLM for the\nlarge VLMs, the overall process is training-efficient. To the best of our\nknowledge, this is the first training-efficient framework that supports both\nopen-source and closed-source large VLMs, enabling broader real-world\npersonalized applications. We conduct thorough experiments across various\nbenchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC\nframework. The code will be released at https://github.com/Hhankyangg/SLC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSLC\u7684\u5c0f\u578b\u4e0e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5927\u578b\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u9ad8\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2a\u6027\u5316\u6210\u672c\u9ad8\u4e14\u8bbf\u95ee\u53d7\u9650\uff0c\u800c\u5c0f\u578bVLMs\u867d\u6613\u4e2a\u6027\u5316\u4f46\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c0f\u578bVLM\u751f\u6210\u4e2a\u6027\u5316\u4fe1\u606f\uff0c\u5927\u578bVLM\u6574\u5408\u4fe1\u606f\u5e76\u63d0\u4f9b\u51c6\u786e\u54cd\u5e94\uff0c\u91c7\u7528\u6d4b\u8bd5\u65f6\u53cd\u5c04\u7b56\u7565\u9632\u6b62\u5c0f\u578bVLM\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSLC\u6846\u67b6\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u5927\u578bVLMs\u4e2d\u6709\u6548\uff0c\u652f\u6301\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "SLC\u662f\u9996\u4e2a\u8bad\u7ec3\u9ad8\u6548\u4e14\u652f\u6301\u5e7f\u6cdb\u4e2a\u6027\u5316\u5e94\u7528\u7684\u6846\u67b6\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.07710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07710", "abs": "https://arxiv.org/abs/2508.07710", "authors": ["Jingya Wang", "Xin Deng", "Wenjie Wei", "Dehao Zhang", "Shuai Wang", "Qian Sun", "Jieyuan Zhang", "Hanwen Liu", "Ning Xie", "Malu Zhang"], "title": "Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer", "comment": "Under review", "summary": "Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a\npromising approach for constructing energy-efficient Transformer architectures.\nCompared to directly trained Spiking Transformers, ANN-to-SNN conversion\nmethods bypass the high training costs. However, existing methods still suffer\nfrom notable limitations, failing to effectively handle nonlinear operations in\nTransformer architectures and requiring additional fine-tuning processes for\npre-trained ANNs. To address these issues, we propose a high-performance and\ntraining-free ANN-to-SNN conversion framework tailored for Transformer\narchitectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)\nneuron, which employs an exponential decay strategy and multi-basis encoding\nmethod to efficiently approximate various nonlinear operations. It removes the\nrequirement for weight modifications in pre-trained ANNs. Extensive experiments\nacross diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures\n(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless\nconversion accuracy with significantly lower latency. This provides a promising\npathway for the efficient and scalable deployment of Spiking Transformers in\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6027\u80fdANN-to-SNN\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7MBE\u795e\u7ecf\u5143\u6709\u6548\u5904\u7406Transformer\u4e2d\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u5b9e\u73b0\u8fd1\u65e0\u635f\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709ANN-to-SNN\u8f6c\u6362\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406Transformer\u4e2d\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u4e14\u9700\u989d\u5916\u5fae\u8c03\uff0c\u9650\u5236\u4e86Spiking Transformers\u7684\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u5f15\u5165\u591a\u57fa\u6307\u6570\u8870\u51cf\uff08MBE\uff09\u795e\u7ecf\u5143\uff0c\u91c7\u7528\u6307\u6570\u8870\u51cf\u7b56\u7565\u548c\u591a\u57fa\u7f16\u7801\u65b9\u6cd5\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3ANN\u7684\u6743\u91cd\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\uff08CV\u3001NLU\u3001NLG\uff09\u548c\u4e3b\u6d41Transformer\u67b6\u6784\uff08ViT\u3001RoBERTa\u3001GPT-2\uff09\u4e0a\u5b9e\u73b0\u8fd1\u65e0\u635f\u8f6c\u6362\uff0c\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u4e3aSpiking Transformers\u7684\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.07401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07401", "abs": "https://arxiv.org/abs/2508.07401", "authors": ["Rui Chen", "Xingyu Chen", "Shaoan Wang", "Shihan Kong", "Junzhi Yu"], "title": "LET-US: Long Event-Text Understanding of Scenes", "comment": null, "summary": "Event cameras output event streams as sparse, asynchronous data with\nmicrosecond-level temporal resolution, enabling visual perception with low\nlatency and a high dynamic range. While existing Multimodal Large Language\nModels (MLLMs) have achieved significant success in understanding and analyzing\nRGB video content, they either fail to interpret event streams effectively or\nremain constrained to very short sequences. In this paper, we introduce LET-US,\na framework for long event-stream--text comprehension that employs an adaptive\ncompression mechanism to reduce the volume of input events while preserving\ncritical visual details. LET-US thus establishes a new frontier in cross-modal\ninferential understanding over extended event sequences. To bridge the\nsubstantial modality gap between event streams and textual representations, we\nadopt a two-stage optimization paradigm that progressively equips our model\nwith the capacity to interpret event-based scenes. To handle the voluminous\ntemporal information inherent in long event streams, we leverage text-guided\ncross-modal queries for feature reduction, augmented by hierarchical clustering\nand similarity computation to distill the most representative event features.\nMoreover, we curate and construct a large-scale event-text aligned dataset to\ntrain our model, achieving tighter alignment of event features within the LLM\nembedding space. We also develop a comprehensive benchmark covering a diverse\nset of tasks -- reasoning, captioning, classification, temporal localization\nand moment retrieval. Experimental results demonstrate that LET-US outperforms\nprior state-of-the-art MLLMs in both descriptive accuracy and semantic\ncomprehension on long-duration event streams. All datasets, codes, and models\nwill be publicly available.", "AI": {"tldr": "LET-US\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u673a\u5236\u5904\u7406\u957f\u4e8b\u4ef6\u6d41\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u4f18\u5316\u548c\u8de8\u6a21\u6001\u67e5\u8be2\uff0c\u63d0\u5347\u4e86\u4e8b\u4ef6\u6d41\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u5904\u7406\u4e8b\u4ef6\u6d41\u65f6\u6548\u679c\u4e0d\u4f73\u6216\u4ec5\u652f\u6301\u77ed\u5e8f\u5217\uff0cLET-US\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u957f\u4e8b\u4ef6\u6d41\u4e0e\u6587\u672c\u7684\u8de8\u6a21\u6001\u7406\u89e3\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u538b\u7f29\u673a\u5236\u51cf\u5c11\u4e8b\u4ef6\u91cf\uff0c\u4e24\u9636\u6bb5\u4f18\u5316\u63d0\u5347\u4e8b\u4ef6\u6d41\u7406\u89e3\u80fd\u529b\uff0c\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u67e5\u8be2\u548c\u5206\u5c42\u805a\u7c7b\u63d0\u53d6\u5173\u952e\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLET-US\u5728\u957f\u4e8b\u4ef6\u6d41\u7684\u63cf\u8ff0\u51c6\u786e\u6027\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u4f18\u4e8e\u73b0\u6709MLLM\u3002", "conclusion": "LET-US\u4e3a\u957f\u4e8b\u4ef6\u6d41\u4e0e\u6587\u672c\u7684\u8de8\u6a21\u6001\u7406\u89e3\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.07763", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07763", "abs": "https://arxiv.org/abs/2508.07763", "authors": ["Martin Rektoris", "Milan Pape\u017e", "V\u00e1clav \u0160m\u00eddl", "Tom\u00e1\u0161 Pevn\u00fd"], "title": "Sparse Probabilistic Graph Circuits", "comment": null, "summary": "Deep generative models (DGMs) for graphs achieve impressively high expressive\npower thanks to very efficient and scalable neural networks. However, these\nnetworks contain non-linearities that prevent analytical computation of many\nstandard probabilistic inference queries, i.e., these DGMs are considered\n\\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)\naddress this issue by enabling \\emph{tractable} probabilistic inference, they\noperate on dense graph representations with $\\mathcal{O}(n^2)$ complexity for\ngraphs with $n$ nodes and \\emph{$m$ edges}. To address this scalability issue,\nwe introduce Sparse PGCs, a new class of tractable generative models that\noperate directly on sparse graph representation, reducing the complexity to\n$\\mathcal{O}(n + m)$, which is particularly beneficial for $m \\ll n^2$. In the\ncontext of de novo drug design, we empirically demonstrate that SPGCs retain\nexact inference capabilities, improve memory efficiency and inference speed,\nand match the performance of intractable DGMs in key metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6982\u7387\u56fe\u7535\u8def\uff08SPGCs\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08DGMs\uff09\u5728\u6982\u7387\u63a8\u65ad\u4e0a\u7684\u4e0d\u53ef\u89e3\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edfDGMs\u56e0\u975e\u7ebf\u6027\u7279\u6027\u5bfc\u81f4\u6982\u7387\u63a8\u65ad\u4e0d\u53ef\u89e3\uff0c\u800c\u73b0\u6709\u7684PGCs\u867d\u89e3\u51b3\u4e86\u53ef\u89e3\u6027\u95ee\u9898\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff08O(n^2)\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u7a00\u758f\u56fe\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u6982\u7387\u56fe\u7535\u8def\uff08SPGCs\uff09\uff0c\u76f4\u63a5\u64cd\u4f5c\u7a00\u758f\u56fe\u8868\u793a\uff0c\u5c06\u590d\u6742\u5ea6\u964d\u81f3O(n + m)\uff0c\u9002\u7528\u4e8e\u8fb9\u6570\u8fdc\u5c0f\u4e8e\u8282\u70b9\u6570\u5e73\u65b9\u7684\u7a00\u758f\u56fe\u3002", "result": "\u5728\u836f\u7269\u8bbe\u8ba1\u4e2d\uff0cSPGCs\u4fdd\u6301\u4e86\u7cbe\u786e\u63a8\u65ad\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5185\u5b58\u6548\u7387\u548c\u63a8\u65ad\u901f\u5ea6\uff0c\u5e76\u5728\u5173\u952e\u6307\u6807\u4e0a\u5339\u914d\u4e86\u4e0d\u53ef\u89e3DGMs\u7684\u6027\u80fd\u3002", "conclusion": "SPGCs\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u751f\u6210\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u56fe\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfDGMs\u548cPGCs\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.08061", "categories": ["cs.LG", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.08061", "abs": "https://arxiv.org/abs/2508.08061", "authors": ["Sven Weinzierl", "Sandra Zilker", "Annina Liessmann", "Martin K\u00e4ppel", "Weixin Wang", "Martin Matzner"], "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations", "comment": null, "summary": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u8fc7\u7a0b\u76d1\u63a7\uff08PPM\uff09\u6280\u672f\uff0c\u5e2e\u52a9\u7f3a\u4e4f\u8db3\u591f\u4e8b\u4ef6\u6570\u636e\u7684\u7ec4\u7ec7\u5b9e\u73b0\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u73b0\u6709PPM\u6280\u672f\u9700\u8981\u5927\u91cf\u4e8b\u4ef6\u6570\u636e\u6216\u5176\u4ed6\u8d44\u6e90\uff0c\u800c\u8bb8\u591a\u7ec4\u7ec7\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u6761\u4ef6\uff0c\u9650\u5236\u4e86PPM\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u5141\u8bb8\u5c06\u4e00\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u76f8\u4f3c\u7684\u76ee\u6807\u4e1a\u52a1\u6d41\u7a0b\u4e2d\uff0c\u652f\u6301\u8de8\u7ec4\u7ec7\u6216\u7ec4\u7ec7\u5185\u7684PPM\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u5728\u76f8\u540c\u6216\u4e0d\u540c\u7ec4\u7ec7\u7684\u76f8\u4f3c\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u6709\u6548\u7684PPM\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u5728\u8d44\u6e90\u6709\u9650\u60c5\u51b5\u4e0b\u5b9e\u65bdPPM\u7684\u53ef\u884c\u65b9\u6848\uff0c\u652f\u6301\u8de8\u7ec4\u7ec7\u77e5\u8bc6\u8fc1\u79fb\u3002"}}
{"id": "2508.07493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07493", "abs": "https://arxiv.org/abs/2508.07493", "authors": ["Jian Chen", "Ming Li", "Jihyung Kil", "Chenguang Wang", "Tong Yu", "Ryan Rossi", "Tianyi Zhou", "Changyou Chen", "Ruiyi Zhang"], "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding", "comment": "Under Review", "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.", "AI": {"tldr": "VisR-Bench\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u957f\u6587\u6863\u4e2d\u7684\u95ee\u9898\u9a71\u52a8\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u8986\u76d616\u79cd\u8bed\u8a00\u548c3\u79cd\u95ee\u9898\u7c7b\u578b\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u68c0\u7d22\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u6587\u6863\u68c0\u7d22\u6216\u5355\u9875\u56fe\u50cf\u7684\u591a\u8bed\u8a00\u95ee\u7b54\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00\u957f\u6587\u6863\u591a\u6a21\u6001\u68c0\u7d22\u7684\u652f\u6301\u3002", "method": "\u5f15\u5165VisR-Bench\uff0c\u5305\u542b35K QA\u5bf9\u548c1.2K\u6587\u6863\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u68c0\u7d22\u8bc4\u4f30\uff0c\u6db5\u76d6\u591a\u79cd\u8bed\u8a00\u548c\u95ee\u9898\u7c7b\u578b\u3002", "result": "MLLMs\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548c\u591a\u6a21\u6001\u7f16\u7801\u5668\u7684\u6a21\u578b\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u8868\u683c\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "VisR-Bench\u586b\u8865\u4e86\u591a\u8bed\u8a00\u957f\u6587\u6863\u591a\u6a21\u6001\u68c0\u7d22\u7684\u7a7a\u767d\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.08034", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08034", "abs": "https://arxiv.org/abs/2508.08034", "authors": ["Roksana Yahyaabadi", "Ghazal Farhani", "Taufiq Rahman", "Soodeh Nikan", "Abdullah Jirjees", "Fadi Araji"], "title": "Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles", "comment": null, "summary": "Accurate power consumption prediction is crucial for improving efficiency and\nreducing environmental impact, yet traditional methods relying on specialized\ninstruments or rigid physical models are impractical for large-scale,\nreal-world deployment. This study introduces a scalable data-driven method\nusing powertrain dynamic feature sets and both traditional machine learning and\ndeep neural networks to estimate instantaneous and cumulative power consumption\nin internal combustion engine (ICE), electric vehicle (EV), and hybrid electric\nvehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with\nmean absolute error and root mean squared error on the order of $10^{-3}$, and\ncumulative errors under 3%. Transformer and long short-term memory models\nperformed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,\nrespectively. Results confirm the approach's effectiveness across vehicles and\nmodels. Uncertainty analysis revealed greater variability in EV and HEV\ndatasets than ICE, due to complex power management, emphasizing the need for\nrobust models for advanced powertrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u5185\u71c3\u673a\u3001\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u7684\u77ac\u65f6\u548c\u7d2f\u79ef\u529f\u7387\u6d88\u8017\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u4e1a\u4eea\u5668\u6216\u7269\u7406\u6a21\u578b\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u52a8\u529b\u7cfb\u7edf\u52a8\u6001\u7279\u5f81\u96c6\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5982Transformer\u548cLSTM\uff09\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5185\u71c3\u673a\u6a21\u578b\u7684\u77ac\u65f6\u8bef\u5dee\u4f4e\u81f310^-3\uff0c\u7d2f\u79ef\u8bef\u5dee\u4f4e\u4e8e3%\uff1b\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u7684\u7d2f\u79ef\u8bef\u5dee\u5206\u522b\u4f4e\u4e8e4.1%\u548c2.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u8f66\u578b\u4e2d\u5747\u8868\u73b0\u6709\u6548\uff0c\u4f46\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u7684\u6570\u636e\u53d8\u5f02\u6027\u66f4\u9ad8\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
{"id": "2508.07543", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07543", "abs": "https://arxiv.org/abs/2508.07543", "authors": ["Chidaksh Ravuru"], "title": "Commentary Generation for Soccer Highlights", "comment": null, "summary": "Automated soccer commentary generation has evolved from template-based\nsystems to advanced neural architectures, aiming to produce real-time\ndescriptions of sports events. While frameworks like SoccerNet-Caption laid\nfoundational work, their inability to achieve fine-grained alignment between\nvideo content and commentary remains a significant challenge. Recent efforts\nsuch as MatchTime, with its MatchVoice model, address this issue through coarse\nand fine-grained alignment techniques, achieving improved temporal\nsynchronization. In this paper, we extend MatchVoice to commentary generation\nfor soccer highlights using the GOAL dataset, which emphasizes short clips over\nentire games. We conduct extensive experiments to reproduce the original\nMatchTime results and evaluate our setup, highlighting the impact of different\ntraining configurations and hardware limitations. Furthermore, we explore the\neffect of varying window sizes on zero-shot performance. While MatchVoice\nexhibits promising generalization capabilities, our findings suggest the need\nfor integrating techniques from broader video-language domains to further\nenhance performance. Our code is available at\nhttps://github.com/chidaksh/SoccerCommentary.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86MatchVoice\u6a21\u578b\uff0c\u7528\u4e8e\u8db3\u7403\u96c6\u9526\u7684\u5b9e\u65f6\u89e3\u8bf4\u751f\u6210\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u7a97\u53e3\u5927\u5c0f\u5bf9\u96f6\u6837\u672c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u89c6\u9891\u5185\u5bb9\u4e0e\u89e3\u8bf4\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u8db3\u7403\u96c6\u9526\u7684\u5b9e\u65f6\u89e3\u8bf4\u751f\u6210\u80fd\u529b\u3002", "method": "\u6269\u5c55MatchVoice\u6a21\u578b\uff0c\u5229\u7528GOAL\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u914d\u7f6e\u548c\u786c\u4ef6\u9650\u5236\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u7a97\u53e3\u5927\u5c0f\u5bf9\u96f6\u6837\u672c\u6027\u80fd\u7684\u4f5c\u7528\u3002", "result": "MatchVoice\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u66f4\u5e7f\u6cdb\u7684\u89c6\u9891-\u8bed\u8a00\u9886\u57df\u6280\u672f\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8db3\u7403\u96c6\u9526\u89e3\u8bf4\u751f\u6210\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u8de8\u9886\u57df\u6280\u672f\u6574\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07795", "abs": "https://arxiv.org/abs/2508.07795", "authors": ["Hongrui Zheng", "Yuezun Li", "Liejun Wang", "Yunfeng Diao", "Zhiqing Guo"], "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake", "comment": null, "summary": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u9632\u5fa1\u6846\u67b6\uff08TSDF\uff09\uff0c\u901a\u8fc7\u53cc\u529f\u80fd\u5bf9\u6297\u6270\u52a8\u76f4\u63a5\u626d\u66f2\u4f2a\u9020\u7ed3\u679c\u5e76\u7834\u574f\u653b\u51fb\u8005\u7684\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u4e3b\u52a8\u9632\u5fa1\u7684\u6301\u4e45\u6027\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u9632\u5fa1\u7b56\u7565\u7f3a\u4e4f\u6301\u4e45\u6027\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u6536\u96c6\u53d7\u4fdd\u62a4\u6837\u672c\u5e76\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7ed5\u8fc7\u9632\u5fa1\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faTSDF\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5ea6\u5206\u79bb\u673a\u5236\u8bbe\u8ba1\u53cc\u529f\u80fd\u5bf9\u6297\u6270\u52a8\uff0c\u76f4\u63a5\u626d\u66f2\u4f2a\u9020\u7ed3\u679c\u5e76\u7834\u574f\u653b\u51fb\u8005\u7684\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f20\u7edf\u9632\u5fa1\u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u91cd\u65b0\u8bad\u7ec3\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u800cTSDF\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53cc\u91cd\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "TSDF\u901a\u8fc7\u53cc\u529f\u80fd\u6270\u52a8\u6709\u6548\u963b\u6b62\u653b\u51fb\u8005\u6a21\u578b\u9002\u5e94\u9632\u5fa1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u52a8\u9632\u5fa1\u7684\u6301\u4e45\u6027\u3002"}}
{"id": "2508.07981", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07981", "abs": "https://arxiv.org/abs/2508.07981", "authors": ["Fangyuan Mao", "Aiming Hao", "Jintao Chen", "Dongxia Liu", "Xiaokun Feng", "Jiashu Zhu", "Meiqi Wu", "Chubin Chen", "Jiahong Wu", "Xiangxiang Chu"], "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation", "comment": null, "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.", "AI": {"tldr": "Omni-Effects\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u63d0\u793a\u5f15\u5bfc\u7684\u6548\u679c\u751f\u6210\u548c\u7a7a\u95f4\u53ef\u63a7\u7684\u590d\u5408\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u591aVFX\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u5e72\u6270\u548c\u7a7a\u95f4\u4e0d\u53ef\u63a7\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728VFX\u5236\u4f5c\u4e2d\u53d7\u9650\u4e8e\u5355\u6548\u679c\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u591a\u6548\u679c\u7684\u7a7a\u95f4\u53ef\u63a7\u590d\u5408\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86LoRA-MoE\u548cSAP\u4e24\u9879\u521b\u65b0\uff0c\u524d\u8005\u6574\u5408\u591a\u6837\u6548\u679c\u5e76\u51cf\u5c11\u5e72\u6270\uff0c\u540e\u8005\u901a\u8fc7\u7a7a\u95f4\u63a9\u7801\u4fe1\u606f\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOmni-Effects\u80fd\u7cbe\u786e\u63a7\u5236\u6548\u679c\u7c7b\u522b\u548c\u4f4d\u7f6e\uff0c\u652f\u6301\u591a\u6837\u5316\u6548\u679c\u751f\u6210\u3002", "conclusion": "Omni-Effects\u4e3aVFX\u5236\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6548\u679c\u7edf\u4e00\u751f\u6210\u7684\u7814\u7a76\u3002"}}
{"id": "2508.08248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08248", "abs": "https://arxiv.org/abs/2508.08248", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation", "comment": null, "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.", "AI": {"tldr": "StableAvatar\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff0c\u80fd\u591f\u751f\u6210\u65e0\u9650\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u97f3\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u865a\u62df\u5316\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u751f\u6210\u957f\u89c6\u9891\u4e14\u97f3\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u5dee\u3002", "method": "\u63d0\u51faStableAvatar\uff0c\u7ed3\u5408\u5b9a\u5236\u8bad\u7ec3\u548c\u63a8\u7406\u6a21\u5757\uff0c\u5f15\u5165\u65f6\u95f4\u6b65\u611f\u77e5\u97f3\u9891\u9002\u914d\u5668\u548c\u97f3\u9891\u539f\u751f\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u53ca\u52a8\u6001\u52a0\u6743\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eStableAvatar\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "StableAvatar\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u97f3\u9891\u540c\u6b65\u548c\u4e00\u81f4\u6027\u96be\u9898\u3002"}}
