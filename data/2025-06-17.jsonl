{"id": "2506.12189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12189", "abs": "https://arxiv.org/abs/2506.12189", "authors": ["Pranav Agarwal", "Ioana CiucÄƒ"], "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis", "comment": "Project Page - https://www.supernova-event.ai/", "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications. As their influence grows, understanding their decision making and\nunderlying personality becomes essential. In this work, we interpret model\npersonality using our proposed Supernova Event Dataset, a novel dataset with\ndiverse articles spanning biographies, historical events, news, and scientific\ndiscoveries. We use this dataset to benchmark LLMs on extracting and ranking\nkey events from text, a subjective and complex challenge that requires\nreasoning over long-range context and modeling causal chains. We evaluate small\nmodels like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as\nClaude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another\nLLM acts as a judge to infer each model's personality based on its selection\nand classification of events. Our analysis shows distinct personality traits:\nfor instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal\ndynamics, while Qwen 2.5 displays a more strategic, analytical style. When\nanalyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual\nframing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors\nstep-by-step causal reasoning. This analysis improves model interpretability,\nmaking them user-friendly for a wide range of diverse applications."}
{"id": "2506.12421", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12421", "abs": "https://arxiv.org/abs/2506.12421", "authors": ["Dongjie Yang", "Chengqiang Lu", "Qimeng Wang", "Xinbei Ma", "Yan Gao", "Yao Hu", "Hai Zhao"], "title": "Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM", "comment": null, "summary": "Travel planning is a complex task requiring the integration of diverse\nreal-world information and user preferences. While LLMs show promise, existing\nmethods with long-horizon thinking struggle with handling multifaceted\nconstraints and preferences in the context, leading to suboptimal itineraries.\nWe formulate this as an $L^3$ planning problem, emphasizing long context, long\ninstruction, and long output. To tackle this, we introduce Multiple Aspects of\nPlanning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve\ncomplex planning problems. Instead of direct planning, MAoP leverages the\nstrategist to conduct pre-planning from various aspects and provide the\nplanning blueprint for planning models, enabling strong inference-time\nscalability for better performance. In addition, current benchmarks overlook\ntravel's dynamic nature, where past events impact subsequent journeys, failing\nto reflect real-world feasibility. To address this, we propose Travel-Sim, an\nagent-based benchmark assessing plans via real-world travel simulation. This\nwork advances LLM capabilities in complex planning and offers novel insights\nfor evaluating sophisticated scenarios through agent-based simulation."}
{"id": "2506.13105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13105", "abs": "https://arxiv.org/abs/2506.13105", "authors": ["Fen Liu", "Chengfeng Jia", "Na Zhang", "Shenghai Yuan", "Rong Su"], "title": "Underwater target 6D State Estimation via UUV Attitude Enhance Observability", "comment": "Paper has been accepted in IROS 2025", "summary": "Accurate relative state observation of Unmanned Underwater Vehicles (UUVs)\nfor tracking uncooperative targets remains a significant challenge due to the\nabsence of GPS, complex underwater dynamics, and sensor limitations. Existing\nlocalization approaches rely on either global positioning infrastructure or\nmulti-UUV collaboration, both of which are impractical for a single UUV\noperating in large or unknown environments. To address this, we propose a novel\npersistent relative 6D state estimation framework that enables a single UUV to\nestimate its relative motion to a non-cooperative target using only successive\nnoisy range measurements from two monostatic sonar sensors. Our key\ncontribution is an observability-enhanced attitude control strategy, which\noptimally adjusts the UUV's orientation to improve the observability of\nrelative state estimation using a Kalman filter, effectively mitigating the\nimpact of sensor noise and drift accumulation. Additionally, we introduce a\nrigorously proven Lyapunov-based tracking control strategy that guarantees\nlong-term stability by ensuring that the UUV maintains an optimal measurement\nrange, preventing localization errors from diverging over time. Through\ntheoretical analysis and simulations, we demonstrate that our method\nsignificantly improves 6D relative state estimation accuracy and robustness\ncompared to conventional approaches. This work provides a scalable,\ninfrastructure-free solution for UUVs tracking uncooperative targets\nunderwater."}
{"id": "2506.12220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12220", "abs": "https://arxiv.org/abs/2506.12220", "authors": ["Hantao Yu", "Josh Alman"], "title": "Two heads are better than one: simulating large transformers with small ones", "comment": null, "summary": "The quadratic complexity of self-attention prevents transformers from scaling\neffectively to long input sequences. On the other hand, modern GPUs and other\nspecialized hardware accelerators are well-optimized for processing small input\nsequences in transformers during both training and inference. A natural\nquestion arises: can we take advantage of the efficiency of small transformers\nto deal with long input sequences?\n  In this paper, we show that transformers with long input sequences (large\ntransformers) can be efficiently simulated by transformers that can only take\nshort input sequences (small transformers). Specifically, we prove that any\ntransformer with input length $N$ can be efficiently simulated by only\n$O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be\nimproved in the worst case. However, we then prove that in various natural\nscenarios including average-case inputs, sliding window masking and attention\nsinks, the optimal number $O(N/M)$ of small transformers suffice."}
{"id": "2506.12524", "categories": ["cs.CV", "cs.HC", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.12524", "abs": "https://arxiv.org/abs/2506.12524", "authors": ["Nuwan Bandara", "Thivya Kandappu", "Archan Misra"], "title": "Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing", "comment": "18 pages", "summary": "Event-based eye tracking holds significant promise for fine-grained cognitive\nstate inference, offering high temporal resolution and robustness to motion\nartifacts, critical features for decoding subtle mental states such as\nattention, confusion, or fatigue. In this work, we introduce a model-agnostic,\ninference-time refinement framework designed to enhance the output of existing\nevent-based gaze estimation models without modifying their architecture or\nrequiring retraining. Our method comprises two key post-processing modules: (i)\nMotion-Aware Median Filtering, which suppresses blink-induced spikes while\npreserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,\nwhich aligns gaze predictions with cumulative event motion to reduce spatial\njitter and temporal discontinuities. To complement traditional spatial accuracy\nmetrics, we propose a novel Jitter Metric that captures the temporal smoothness\nof predicted gaze trajectories based on velocity regularity and local signal\ncomplexity. Together, these contributions significantly improve the consistency\nof event-based gaze signals, making them better suited for downstream tasks\nsuch as micro-expression analysis and mind-state decoding. Our results\ndemonstrate consistent improvements across multiple baseline models on\ncontrolled datasets, laying the groundwork for future integration with\nmultimodal affect recognition systems in real-world environments."}
{"id": "2506.12561", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12561", "abs": "https://arxiv.org/abs/2506.12561", "authors": ["Mahmudul Hasan"], "title": "Parkinson's Disease Freezing of Gait (FoG) Symptom Detection Using Machine Learning from Wearable Sensor Data", "comment": null, "summary": "Freezing of gait (FoG) is a special symptom found in patients with\nParkinson's disease (PD). Patients who have FoG abruptly lose the capacity to\nwalk as they normally would. Accelerometers worn by patients can record\nmovement data during these episodes, and machine learning algorithms can be\nuseful to categorize this information. Thus, the combination may be able to\nidentify FoG in real time. In order to identify FoG events in accelerometer\ndata, we introduce the Transformer Encoder-Bi-LSTM fusion model in this paper.\nThe model's capability to differentiate between FoG episodes and normal\nmovement was used to evaluate its performance, and on the Kaggle Parkinson's\nFreezing of Gait dataset, the proposed Transformer Encoder-Bi-LSTM fusion model\nproduced 92.6% accuracy, 80.9% F1 score, and 52.06% in terms of mean average\nprecision. The findings highlight how Deep Learning-based approaches may\nprogress the field of FoG identification and help PD patients receive better\ntreatments and management plans."}
{"id": "2506.12585", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.12585", "abs": "https://arxiv.org/abs/2506.12585", "authors": ["Darryl Ho", "Samuel Madden"], "title": "DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification", "comment": "Accepted to CVPR 2025 (IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition), main conference, poster presentation", "summary": "In recent years, large transformer-based video encoder models have greatly\nadvanced state-of-the-art performance on video classification tasks. However,\nthese large models typically process videos by averaging embedding outputs from\nmultiple clips over time to produce fixed-length representations. This approach\nfails to account for a variety of time-related features, such as variable video\ndurations, chronological order of events, and temporal variance in feature\nsignificance. While methods for temporal modeling do exist, they often require\nsignificant architectural changes and expensive retraining, making them\nimpractical for off-the-shelf, fine-tuned large encoders. To overcome these\nlimitations, we propose DejaVid, an encoder-agnostic method that enhances model\nperformance without the need for retraining or altering the architecture. Our\nframework converts a video into a variable-length temporal sequence of\nembeddings, which we call a multivariate time series (MTS). An MTS naturally\npreserves temporal order and accommodates variable video durations. We then\nlearn per-timestep, per-feature weights over the encoded MTS frames, allowing\nus to account for variations in feature importance over time. We introduce a\nnew neural network architecture inspired by traditional time series alignment\nalgorithms for this learning task. Our evaluation demonstrates that DejaVid\nsubstantially improves the performance of a state-of-the-art large encoder,\nachieving leading Top-1 accuracy of 77.2% on Something-Something V2, 89.1% on\nKinetics-400, and 88.6% on HMDB51, while adding fewer than 1.8% additional\nlearnable parameters and requiring less than 3 hours of training time. Our code\nis available at https://github.com/darrylho/DejaVid."}
{"id": "2506.12538", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12538", "abs": "https://arxiv.org/abs/2506.12538", "authors": ["Shuo Yang", "Yuqin Dai", "Guoqing Wang", "Xinran Zheng", "Jinfeng Xu", "Jinze Li", "Zhenzhe Ying", "Weiqiang Wang", "Edith C. H. Ngai"], "title": "RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking", "comment": null, "summary": "Large Language Models (LLMs) hold significant potential for advancing\nfact-checking by leveraging their capabilities in reasoning, evidence\nretrieval, and explanation generation. However, existing benchmarks fail to\ncomprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in\nrealistic misinformation scenarios. To bridge this gap, we introduce\nRealFactBench, a comprehensive benchmark designed to assess the fact-checking\ncapabilities of LLMs and MLLMs across diverse real-world tasks, including\nKnowledge Validation, Rumor Detection, and Event Verification. RealFactBench\nconsists of 6K high-quality claims drawn from authoritative sources,\nencompassing multimodal content and diverse domains. Our evaluation framework\nfurther introduces the Unknown Rate (UnR) metric, enabling a more nuanced\nassessment of models' ability to handle uncertainty and balance between\nover-conservatism and over-confidence. Extensive experiments on 7\nrepresentative LLMs and 4 MLLMs reveal their limitations in real-world\nfact-checking and offer valuable insights for further research. RealFactBench\nis publicly available at https://github.com/kalendsyang/RealFactBench.git."}
{"id": "2506.12371", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.12371", "abs": "https://arxiv.org/abs/2506.12371", "authors": ["Kevin Zhang", "Yonghan Jung", "Divyat Mahajan", "Karthikeyan Shanmugam", "Shalmali Joshi"], "title": "Path-specific effects for pulse-oximetry guided decisions in critical care", "comment": null, "summary": "Identifying and measuring biases associated with sensitive attributes is a\ncrucial consideration in healthcare to prevent treatment disparities. One\nprominent issue is inaccurate pulse oximeter readings, which tend to\noverestimate oxygen saturation for dark-skinned patients and misrepresent\nsupplemental oxygen needs. Most existing research has revealed statistical\ndisparities linking device errors to patient outcomes in intensive care units\n(ICUs) without causal formalization. In contrast, this study causally\ninvestigates how racial discrepancies in oximetry measurements affect invasive\nventilation in ICU settings. We employ a causal inference-based approach using\npath-specific effects to isolate the impact of bias by race on clinical\ndecision-making. To estimate these effects, we leverage a doubly robust\nestimator, propose its self-normalized variant for improved sample efficiency,\nand provide novel finite-sample guarantees. Our methodology is validated on\nsemi-synthetic data and applied to two large real-world health datasets:\nMIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact\nof racial discrepancies on invasive ventilation rates. However, path-specific\neffects mediated by oxygen saturation disparity are more pronounced on\nventilation duration, and the severity differs by dataset. Our work provides a\nnovel and practical pipeline for investigating potential disparities in the ICU\nand, more crucially, highlights the necessity of causal methods to robustly\nassess fairness in decision-making."}
{"id": "2506.12963", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12963", "abs": "https://arxiv.org/abs/2506.12963", "authors": ["Changsheng Wang", "Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Dennis Wei", "Parikshit Ram", "Nathalie Baracaldo", "Sijia Liu"], "title": "Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills", "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled strong\nchain-of-thought (CoT) generation through test-time computation. While these\nmulti-step reasoning capabilities represent a major milestone in language model\nperformance, they also introduce new safety risks. In this work, we present the\nfirst systematic study to revisit the problem of machine unlearning in the\ncontext of LRMs. Machine unlearning refers to the process of removing the\ninfluence of sensitive, harmful, or undesired data or knowledge from a trained\nmodel without full retraining. We show that conventional unlearning algorithms,\noriginally designed for non-reasoning models, are inadequate for LRMs. In\nparticular, even when final answers are successfully erased, sensitive\ninformation often persists within the intermediate reasoning steps, i.e., CoT\ntrajectories. To address this challenge, we extend conventional unlearning and\npropose Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$), a\nnovel method that effectively suppresses sensitive reasoning traces and\nprevents the generation of associated final answers, while preserving the\nmodel's reasoning ability. Our experiments demonstrate that $R^2MU$\nsignificantly reduces sensitive information leakage within reasoning traces and\nachieves strong performance across both safety and reasoning benchmarks,\nevaluated on state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and\nDeepSeek-R1-Distill-Qwen-14B."}
{"id": "2506.12484", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12484", "abs": "https://arxiv.org/abs/2506.12484", "authors": ["Filip Sondej", "Yushi Yang", "MikoÅ‚aj Kniejski", "Marcel Windys"], "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization", "comment": null, "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new\nstate-of-the-art for robust unlearning."}
{"id": "2506.12898", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12898", "abs": "https://arxiv.org/abs/2506.12898", "authors": ["William Xia", "Ishita Unde", "Brian Ondov", "Dina Demner-Fushman"], "title": "JEBS: A Fine-grained Biomedical Lexical Simplification Task", "comment": "13 pages, 2 figures, to be published in Proceedings of the 63rd\n  Annual Meeting of the Association for Computational Linguistics", "summary": "Online medical literature has made health information more available than\never, however, the barrier of complex medical jargon prevents the general\npublic from understanding it. Though parallel and comparable corpora for\nBiomedical Text Simplification have been introduced, these conflate the many\nsyntactic and lexical operations involved in simplification. To enable more\ntargeted development and evaluation, we present a fine-grained lexical\nsimplification task and dataset, Jargon Explanations for Biomedical\nSimplification (JEBS, https://github.com/bill-from-ri/JEBS-data ). The JEBS\ntask involves identifying complex terms, classifying how to replace them, and\ngenerating replacement text. The JEBS dataset contains 21,595 replacements for\n10,314 terms across 400 biomedical abstracts and their manually simplified\nversions. Additionally, we provide baseline results for a variety of rule-based\nand transformer-based systems for the three sub-tasks. The JEBS task, data, and\nbaseline results pave the way for development and rigorous evaluation of\nsystems for replacing or explaining complex biomedical terms."}
{"id": "2506.13164", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2506.13164", "abs": "https://arxiv.org/abs/2506.13164", "authors": ["Steve Yuwono", "Muhammad Uzair Rana", "Dorothea Schwung", "Andreas Schwung"], "title": "Real Time Self-Tuning Adaptive Controllers on Temperature Control Loops using Event-based Game Theory", "comment": null, "summary": "This paper presents a novel method for enhancing the adaptability of\nProportional-Integral-Derivative (PID) controllers in industrial systems using\nevent-based dynamic game theory, which enables the PID controllers to\nself-learn, optimize, and fine-tune themselves. In contrast to conventional\nself-learning approaches, our proposed framework offers an event-driven control\nstrategy and game-theoretic learning algorithms. The players collaborate with\nthe PID controllers to dynamically adjust their gains in response to set point\nchanges and disturbances. We provide a theoretical analysis showing sound\nconvergence guarantees for the game given suitable stability ranges of the PID\ncontrolled loop. We further introduce an automatic boundary detection\nmechanism, which helps the players to find an optimal initialization of action\nspaces and significantly reduces the exploration time. The efficacy of this\nnovel methodology is validated through its implementation in the temperature\ncontrol loop of a printing press machine. Eventually, the outcomes of the\nproposed intelligent self-tuning PID controllers are highly promising,\nparticularly in terms of reducing overshoot and settling time."}
{"id": "2506.12978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12978", "abs": "https://arxiv.org/abs/2506.12978", "authors": ["Yuanyuan Lei", "Ruihong Huang"], "title": "Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation", "comment": "Accepted to ACL 2025", "summary": "Media outlets are becoming more partisan and polarized nowadays. Most\nprevious work focused on detecting media bias. In this paper, we aim to\nmitigate media bias by generating a neutralized summary given multiple articles\npresenting different ideological views. Motivated by the critical role of\nevents and event relations in media bias detection, we propose to increase\nawareness of bias in LLMs via multi-document events reasoning and use a\nmulti-document event relation graph to guide the summarization process. This\ngraph contains rich event information useful to reveal bias: four common types\nof in-doc event relations to reflect content framing bias, cross-doc event\ncoreference relation to reveal content selection bias, and event-level moral\nopinions to highlight opinionated framing bias. We further develop two\nstrategies to incorporate the multi-document event relation graph for\nneutralized summarization. Firstly, we convert a graph into natural language\ndescriptions and feed the textualized graph into LLMs as a part of a hard text\nprompt. Secondly, we encode the graph with graph attention network and insert\nthe graph embedding into LLMs as a soft prompt. Both automatic evaluation and\nhuman evaluation confirm that our approach effectively mitigates both lexical\nand informational media bias, and meanwhile improves content preservation."}
{"id": "2506.12749", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12749", "abs": "https://arxiv.org/abs/2506.12749", "authors": ["Weicai Li", "Tiejun Lv", "Xiyu Zhao", "Xin Yuan", "Wei Ni"], "title": "Free Privacy Protection for Wireless Federated Learning: Enjoy It or Suffer from It?", "comment": "16 pages, 8 figures, accepted by IEEE Transactions on Information\n  Forensics and Security", "summary": "Inherent communication noises have the potential to preserve privacy for\nwireless federated learning (WFL) but have been overlooked in digital\ncommunication systems predominantly using floating-point number standards,\ne.g., IEEE 754, for data storage and transmission. This is due to the\npotentially catastrophic consequences of bit errors in floating-point numbers,\ne.g., on the sign or exponent bits. This paper presents a novel channel-native\nbit-flipping differential privacy (DP) mechanism tailored for WFL, where\ntransmit bits are randomly flipped and communication noises are leveraged, to\ncollectively preserve the privacy of WFL in digital communication systems. The\nkey idea is to interpret the bit perturbation at the transmitter and bit errors\ncaused by communication noises as a bit-flipping DP process. This is achieved\nby designing a new floating-point-to-fixed-point conversion method that only\ntransmits the bits in the fraction part of model parameters, hence eliminating\nthe need for transmitting the sign and exponent bits and preventing the\ncatastrophic consequence of bit errors. We analyze a new metric to measure the\nbit-level distance of the model parameters and prove that the proposed\nmechanism satisfies (\\lambda,\\epsilon)-R\\'enyi DP and does not violate the WFL\nconvergence. Experiments validate privacy and convergence analysis of the\nproposed mechanism and demonstrate its superiority to the state-of-the-art\nGaussian mechanisms that are channel-agnostic and add Gaussian noise for\nprivacy protection."}
{"id": "2506.12992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12992", "abs": "https://arxiv.org/abs/2506.12992", "authors": ["Xinyi Zhao", "Congjing Zhang", "Pei Guo", "Wei Li", "Lin Chen", "Chaoyue Zhao", "Shuai Huang"], "title": "SmartHome-Bench: A Comprehensive Benchmark for Video Anomaly Detection in Smart Homes Using Multi-Modal Large Language Models", "comment": "CVPR 2025 Workshop: VAND 3.0 - Visual Anomaly and Novelty Detection", "summary": "Video anomaly detection (VAD) is essential for enhancing safety and security\nby identifying unusual events across different environments. Existing VAD\nbenchmarks, however, are primarily designed for general-purpose scenarios,\nneglecting the specific characteristics of smart home applications. To bridge\nthis gap, we introduce SmartHome-Bench, the first comprehensive benchmark\nspecially designed for evaluating VAD in smart home scenarios, focusing on the\ncapabilities of multi-modal large language models (MLLMs). Our newly proposed\nbenchmark consists of 1,203 videos recorded by smart home cameras, organized\naccording to a novel anomaly taxonomy that includes seven categories, such as\nWildlife, Senior Care, and Baby Monitoring. Each video is meticulously\nannotated with anomaly tags, detailed descriptions, and reasoning. We further\ninvestigate adaptation methods for MLLMs in VAD, assessing state-of-the-art\nclosed-source and open-source models with various prompting techniques. Results\nreveal significant limitations in the current models' ability to detect video\nanomalies accurately. To address these limitations, we introduce the\nTaxonomy-Driven Reflective LLM Chain (TRLC), a new LLM chaining framework that\nachieves a notable 11.62% improvement in detection accuracy. The benchmark\ndataset and code are publicly available at\nhttps://github.com/Xinyi-0724/SmartHome-Bench-LLM."}
{"id": "2506.13095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13095", "abs": "https://arxiv.org/abs/2506.13095", "authors": ["Yu Wang", "Shiwei Chen"], "title": "Learning Event Completeness for Weakly Supervised Video Anomaly Detection", "comment": "Accepted by ICML", "summary": "Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing\ntemporal intervals containing anomalous events within untrimmed videos,\nutilizing only video-level annotations. However, a significant challenge arises\ndue to the absence of dense frame-level annotations, often leading to\nincomplete localization in existing WS-VAD methods. To address this issue, we\npresent a novel LEC-VAD, Learning Event Completeness for Weakly Supervised\nVideo Anomaly Detection, which features a dual structure designed to encode\nboth category-aware and category-agnostic semantics between vision and\nlanguage. Within LEC-VAD, we devise semantic regularities that leverage an\nanomaly-aware Gaussian mixture to learn precise event boundaries, thereby\nyielding more complete event instances. Besides, we develop a novel memory\nbank-based prototype learning mechanism to enrich concise text descriptions\nassociated with anomaly-event categories. This innovation bolsters the text's\nexpressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates\nremarkable advancements over the current state-of-the-art methods on two\nbenchmark datasets XD-Violence and UCF-Crime."}
{"id": "2506.13097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13097", "abs": "https://arxiv.org/abs/2506.13097", "authors": ["Ziqing Zhou", "Binbin Gao", "Yuri Pan", "Lidong Wang", "Wenbing Zhu", "Yong Liu", "Jun Liu", "MIngmin Chi", "Dong Wu", "Bo Peng", "Chengjie Wang"], "title": "Pro-AD: Learning Comprehensive Prototypes with Prototype-based Constraint for Multi-class Unsupervised Anomaly Detection", "comment": null, "summary": "Prototype-based reconstruction methods for unsupervised anomaly detection\nutilize a limited set of learnable prototypes which only aggregates\ninsufficient normal information, resulting in undesirable reconstruction.\nHowever, increasing the number of prototypes may lead to anomalies being well\nreconstructed through the attention mechanism, which we refer to as the \"Soft\nIdentity Mapping\" problem. In this paper, we propose Pro-AD to address these\nissues and fully utilize the prototypes to boost the performance of anomaly\ndetection. Specifically, we first introduce an expanded set of learnable\nprototypes to provide sufficient capacity for semantic information. Then we\nemploy a Dynamic Bidirectional Decoder which integrates the process of the\nnormal information aggregation and the target feature reconstruction via\nprototypes, with the aim of allowing the prototypes to aggregate more\ncomprehensive normal semantic information from different levels of the image\nfeatures and the target feature reconstruction to not only utilize its\ncontextual information but also dynamically leverage the learned comprehensive\nprototypes. Additionally, to prevent the anomalies from being well\nreconstructed using sufficient semantic information through the attention\nmechanism, Pro-AD introduces a Prototype-based Constraint that applied within\nthe target feature reconstruction process of the decoder, which further\nimproves the performance of our approach. Extensive experiments on multiple\nchallenging benchmarks demonstrate that our Pro-AD achieve state-of-the-art\nperformance, highlighting its superior robustness and practical effectiveness\nfor Multi-class Unsupervised Anomaly Detection task."}
{"id": "2506.12189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12189", "abs": "https://arxiv.org/abs/2506.12189", "authors": ["Pranav Agarwal", "Ioana CiucÄƒ"], "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis", "comment": "Project Page - https://www.supernova-event.ai/", "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications. As their influence grows, understanding their decision making and\nunderlying personality becomes essential. In this work, we interpret model\npersonality using our proposed Supernova Event Dataset, a novel dataset with\ndiverse articles spanning biographies, historical events, news, and scientific\ndiscoveries. We use this dataset to benchmark LLMs on extracting and ranking\nkey events from text, a subjective and complex challenge that requires\nreasoning over long-range context and modeling causal chains. We evaluate small\nmodels like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as\nClaude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another\nLLM acts as a judge to infer each model's personality based on its selection\nand classification of events. Our analysis shows distinct personality traits:\nfor instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal\ndynamics, while Qwen 2.5 displays a more strategic, analytical style. When\nanalyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual\nframing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors\nstep-by-step causal reasoning. This analysis improves model interpretability,\nmaking them user-friendly for a wide range of diverse applications."}
{"id": "2506.12220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12220", "abs": "https://arxiv.org/abs/2506.12220", "authors": ["Hantao Yu", "Josh Alman"], "title": "Two heads are better than one: simulating large transformers with small ones", "comment": null, "summary": "The quadratic complexity of self-attention prevents transformers from scaling\neffectively to long input sequences. On the other hand, modern GPUs and other\nspecialized hardware accelerators are well-optimized for processing small input\nsequences in transformers during both training and inference. A natural\nquestion arises: can we take advantage of the efficiency of small transformers\nto deal with long input sequences?\n  In this paper, we show that transformers with long input sequences (large\ntransformers) can be efficiently simulated by transformers that can only take\nshort input sequences (small transformers). Specifically, we prove that any\ntransformer with input length $N$ can be efficiently simulated by only\n$O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be\nimproved in the worst case. However, we then prove that in various natural\nscenarios including average-case inputs, sliding window masking and attention\nsinks, the optimal number $O(N/M)$ of small transformers suffice."}
{"id": "2506.13107", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.13107", "abs": "https://arxiv.org/abs/2506.13107", "authors": ["Yanfang Hou", "Carlos FernÃ¡ndez-LorÃ­a"], "title": "Honesty in Causal Forests: When It Helps and When It Hurts", "comment": null, "summary": "Causal forests are increasingly used to personalize decisions based on\nestimated treatment effects. A distinctive modeling choice in this method is\nhonest estimation: using separate data for splitting and for estimating effects\nwithin leaves. This practice is the default in most implementations and is\nwidely seen as desirable for causal inference. But we show that honesty can\nhurt the accuracy of individual-level effect estimates. The reason is a classic\nbias-variance trade-off: honesty reduces variance by preventing overfitting,\nbut increases bias by limiting the model's ability to discover and exploit\nmeaningful heterogeneity in treatment effects. This trade-off depends on the\nsignal-to-noise ratio (SNR): honesty helps when effect heterogeneity is hard to\ndetect (low SNR), but hurts when the signal is strong (high SNR). In essence,\nhonesty acts as a form of regularization, and like any regularization choice,\nit should be guided by out-of-sample performance, not adopted by default."}
{"id": "2506.13116", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13116", "abs": "https://arxiv.org/abs/2506.13116", "authors": ["Tehreem Zubair", "Syeda Kisaa Fatima", "Noman Ahmed", "Asifullah Khan"], "title": "Crime Hotspot Prediction Using Deep Graph Convolutional Networks", "comment": null, "summary": "Crime hotspot prediction is critical for ensuring urban safety and effective\nlaw enforcement, yet it remains challenging due to the complex spatial\ndependencies inherent in criminal activity. The previous approaches tended to\nuse classical algorithms such as the KDE and SVM to model data distributions\nand decision boundaries. The methods often fail to capture these spatial\nrelationships, treating crime events as independent and ignoring geographical\ninteractions. To address this, we propose a novel framework based on Graph\nConvolutional Networks (GCNs), which explicitly model spatial dependencies by\nrepresenting crime data as a graph. In this graph, nodes represent discrete\ngeographic grid cells and edges capture proximity relationships. Using the\nChicago Crime Dataset, we engineer spatial features and train a multi-layer GCN\nmodel to classify crime types and predict high-risk zones. Our approach\nachieves 88% classification accuracy, significantly outperforming traditional\nmethods. Additionally, the model generates interpretable heat maps of crime\nhotspots, demonstrating the practical utility of graph-based learning for\npredictive policing and spatial criminology."}
{"id": "2506.13440", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.13440", "abs": "https://arxiv.org/abs/2506.13440", "authors": ["Shenqi Wang", "Yingfu Xu", "Amirreza Yousefzadeh", "Sherif Eissa", "Henk Corporaal", "Federico Corradi", "Guangzhi Tang"], "title": "Sparse Convolutional Recurrent Learning for Efficient Event-based Neuromorphic Object Detection", "comment": "Accepted by IJCNN 2025", "summary": "Leveraging the high temporal resolution and dynamic range, object detection\nwith event cameras can enhance the performance and safety of automotive and\nrobotics applications in real-world scenarios. However, processing sparse event\ndata requires compute-intensive convolutional recurrent units, complicating\ntheir integration into resource-constrained edge applications. Here, we propose\nthe Sparse Event-based Efficient Detector (SEED) for efficient event-based\nobject detection on neuromorphic processors. We introduce sparse convolutional\nrecurrent learning, which achieves over 92% activation sparsity in recurrent\nprocessing, vastly reducing the cost for spatiotemporal reasoning on sparse\nevent data. We validated our method on Prophesee's 1 Mpx and Gen1 event-based\nobject detection datasets. Notably, SEED sets a new benchmark in computational\nefficiency for event-based object detection which requires long-term temporal\nlearning. Compared to state-of-the-art methods, SEED significantly reduces\nsynaptic operations while delivering higher or same-level mAP. Our hardware\nsimulations showcase the critical role of SEED's hardware-aware design in\nachieving energy-efficient and low-latency neuromorphic processing."}
{"id": "2506.13206", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13206", "abs": "https://arxiv.org/abs/2506.13206", "authors": ["James Chua", "Jan Betley", "Mia Taylor", "Owain Evans"], "title": "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models", "comment": null, "summary": "Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (``I'll trick\nthe user...''), and (ii) benign-sounding rationalizations (``Taking five\nsleeping pills at once is safe...''). Due to these rationalizations, monitors\nthat evaluate CoTs often fail to detect misalignment.\n  Extending this setup, we also train reasoning models to perform narrow bad\nbehaviors only when a backdoor trigger is present in the prompt. This causes\nbroad misalignment that remains hidden, which brings additional risk. We find\nthat reasoning models can often describe and explain their backdoor triggers,\ndemonstrating a kind of self-awareness. So CoT monitoring can expose these\nbehaviors but is unreliable.\n  In summary, reasoning steps can both reveal and conceal misaligned\nintentions, and do not prevent misalignment behaviors in the models studied. We\nrelease three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite."}
{"id": "2506.12484", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12484", "abs": "https://arxiv.org/abs/2506.12484", "authors": ["Filip Sondej", "Yushi Yang", "MikoÅ‚aj Kniejski", "Marcel Windys"], "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization", "comment": null, "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new\nstate-of-the-art for robust unlearning."}
{"id": "2506.12421", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12421", "abs": "https://arxiv.org/abs/2506.12421", "authors": ["Dongjie Yang", "Chengqiang Lu", "Qimeng Wang", "Xinbei Ma", "Yan Gao", "Yao Hu", "Hai Zhao"], "title": "Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM", "comment": null, "summary": "Travel planning is a complex task requiring the integration of diverse\nreal-world information and user preferences. While LLMs show promise, existing\nmethods with long-horizon thinking struggle with handling multifaceted\nconstraints and preferences in the context, leading to suboptimal itineraries.\nWe formulate this as an $L^3$ planning problem, emphasizing long context, long\ninstruction, and long output. To tackle this, we introduce Multiple Aspects of\nPlanning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve\ncomplex planning problems. Instead of direct planning, MAoP leverages the\nstrategist to conduct pre-planning from various aspects and provide the\nplanning blueprint for planning models, enabling strong inference-time\nscalability for better performance. In addition, current benchmarks overlook\ntravel's dynamic nature, where past events impact subsequent journeys, failing\nto reflect real-world feasibility. To address this, we propose Travel-Sim, an\nagent-based benchmark assessing plans via real-world travel simulation. This\nwork advances LLM capabilities in complex planning and offers novel insights\nfor evaluating sophisticated scenarios through agent-based simulation."}
{"id": "2506.12484", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12484", "abs": "https://arxiv.org/abs/2506.12484", "authors": ["Filip Sondej", "Yushi Yang", "MikoÅ‚aj Kniejski", "Marcel Windys"], "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization", "comment": null, "summary": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new\nstate-of-the-art for robust unlearning."}
{"id": "2506.13400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13400", "abs": "https://arxiv.org/abs/2506.13400", "authors": ["Jann Krausse", "Alexandru Vasilache", "Klaus Knobloch", "Juergen Becker"], "title": "Realtime-Capable Hybrid Spiking Neural Networks for Neural Decoding of Cortical Activity", "comment": "This paper was accepted and presented at the 2025 Neuro Inspired\n  Computational Elements (NICE) conference", "summary": "Intra-cortical brain-machine interfaces (iBMIs) present a promising solution\nto restoring and decoding brain activity lost due to injury. However, patients\nwith such neuroprosthetics suffer from permanent skull openings resulting from\nthe devices' bulky wiring. This drives the development of wireless iBMIs, which\ndemand low power consumption and small device footprint. Most recently, spiking\nneural networks (SNNs) have been researched as potential candidates for\nlow-power neural decoding. In this work, we present the next step of utilizing\nSNNs for such tasks, building on the recently published results of the 2024\nGrand Challenge on Neural Decoding Challenge for Motor Control of non-Human\nPrimates. We optimize our model architecture to exceed the existing state of\nthe art on the Primate Reaching dataset while maintaining similar resource\ndemand through various compression techniques. We further focus on implementing\na realtime-capable version of the model and discuss the implications of this\narchitecture. With this, we advance one step towards latency-free decoding of\ncortical spike trains using neuromorphic technology, ultimately improving the\nlives of millions of paralyzed patients."}
{"id": "2506.12538", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12538", "abs": "https://arxiv.org/abs/2506.12538", "authors": ["Shuo Yang", "Yuqin Dai", "Guoqing Wang", "Xinran Zheng", "Jinfeng Xu", "Jinze Li", "Zhenzhe Ying", "Weiqiang Wang", "Edith C. H. Ngai"], "title": "RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking", "comment": null, "summary": "Large Language Models (LLMs) hold significant potential for advancing\nfact-checking by leveraging their capabilities in reasoning, evidence\nretrieval, and explanation generation. However, existing benchmarks fail to\ncomprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in\nrealistic misinformation scenarios. To bridge this gap, we introduce\nRealFactBench, a comprehensive benchmark designed to assess the fact-checking\ncapabilities of LLMs and MLLMs across diverse real-world tasks, including\nKnowledge Validation, Rumor Detection, and Event Verification. RealFactBench\nconsists of 6K high-quality claims drawn from authoritative sources,\nencompassing multimodal content and diverse domains. Our evaluation framework\nfurther introduces the Unknown Rate (UnR) metric, enabling a more nuanced\nassessment of models' ability to handle uncertainty and balance between\nover-conservatism and over-confidence. Extensive experiments on 7\nrepresentative LLMs and 4 MLLMs reveal their limitations in real-world\nfact-checking and offer valuable insights for further research. RealFactBench\nis publicly available at https://github.com/kalendsyang/RealFactBench.git."}
{"id": "2506.13416", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13416", "abs": "https://arxiv.org/abs/2506.13416", "authors": ["Alexandru Vasilache", "Sven Nitzsche", "Christian Kneidl", "Mikael Tekneyan", "Moritz Neher", "Juergen Becker"], "title": "Spiking Neural Networks for Low-Power Vibration-Based Predictive Maintenance", "comment": "This paper has been accepted and will be presented at the\n  International Conference on Neuromorphic Systems (ICONS) 2025, July 29-31,\n  2025. The proceedings will be published later", "summary": "Advancements in Industrial Internet of Things (IIoT) sensors enable\nsophisticated Predictive Maintenance (PM) with high temporal resolution. For\ncost-efficient solutions, vibration-based condition monitoring is especially of\ninterest. However, analyzing high-resolution vibration data via traditional\ncloud approaches incurs significant energy and communication costs, hindering\nbattery-powered edge deployments. This necessitates shifting intelligence to\nthe sensor edge. Due to their event-driven nature, Spiking Neural Networks\n(SNNs) offer a promising pathway toward energy-efficient on-device processing.\nThis paper investigates a recurrent SNN for simultaneous regression (flow,\npressure, pump speed) and multi-label classification (normal, overpressure,\ncavitation) for an industrial progressing cavity pump (PCP) using 3-axis\nvibration data. Furthermore, we provide energy consumption estimates comparing\nthe SNN approach on conventional (x86, ARM) and neuromorphic (Loihi) hardware\nplatforms. Results demonstrate high classification accuracy (>97%) with zero\nFalse Negative Rates for critical Overpressure and Cavitation faults. Smoothed\nregression outputs achieve Mean Relative Percentage Errors below 1% for flow\nand pump speed, approaching industrial sensor standards, although pressure\nprediction requires further refinement. Energy estimates indicate significant\npower savings, with the Loihi consumption (0.0032 J/inf) being up to 3 orders\nof magnitude less compared to the estimated x86 CPU (11.3 J/inf) and ARM CPU\n(1.18 J/inf) execution. Our findings underscore the potential of SNNs for\nmulti-task PM directly on resource-constrained edge devices, enabling scalable\nand energy-efficient industrial monitoring solutions."}
{"id": "2506.13722", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13722", "abs": "https://arxiv.org/abs/2506.13722", "authors": ["Kaiyuan Tan", "Pavan Kumar B N", "Bharatesh Chakravarthi"], "title": "How Real is CARLAs Dynamic Vision Sensor? A Study on the Sim-to-Real Gap in Traffic Object Detection", "comment": null, "summary": "Event cameras are gaining traction in traffic monitoring applications due to\ntheir low latency, high temporal resolution, and energy efficiency, which makes\nthem well-suited for real-time object detection at traffic intersections.\nHowever, the development of robust event-based detection models is hindered by\nthe limited availability of annotated real-world datasets. To address this,\nseveral simulation tools have been developed to generate synthetic event data.\nAmong these, the CARLA driving simulator includes a built-in dynamic vision\nsensor (DVS) module that emulates event camera output. Despite its potential,\nthe sim-to-real gap for event-based object detection remains insufficiently\nstudied. In this work, we present a systematic evaluation of this gap by\ntraining a recurrent vision transformer model exclusively on synthetic data\ngenerated using CARLAs DVS and testing it on varying combinations of synthetic\nand real-world event streams. Our experiments show that models trained solely\non synthetic data perform well on synthetic-heavy test sets but suffer\nsignificant performance degradation as the proportion of real-world data\nincreases. In contrast, models trained on real-world data demonstrate stronger\ngeneralization across domains. This study offers the first quantifiable\nanalysis of the sim-to-real gap in event-based object detection using CARLAs\nDVS. Our findings highlight limitations in current DVS simulation fidelity and\nunderscore the need for improved domain adaptation techniques in neuromorphic\nvision for traffic monitoring."}
{"id": "2506.13116", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13116", "abs": "https://arxiv.org/abs/2506.13116", "authors": ["Tehreem Zubair", "Syeda Kisaa Fatima", "Noman Ahmed", "Asifullah Khan"], "title": "Crime Hotspot Prediction Using Deep Graph Convolutional Networks", "comment": null, "summary": "Crime hotspot prediction is critical for ensuring urban safety and effective\nlaw enforcement, yet it remains challenging due to the complex spatial\ndependencies inherent in criminal activity. The previous approaches tended to\nuse classical algorithms such as the KDE and SVM to model data distributions\nand decision boundaries. The methods often fail to capture these spatial\nrelationships, treating crime events as independent and ignoring geographical\ninteractions. To address this, we propose a novel framework based on Graph\nConvolutional Networks (GCNs), which explicitly model spatial dependencies by\nrepresenting crime data as a graph. In this graph, nodes represent discrete\ngeographic grid cells and edges capture proximity relationships. Using the\nChicago Crime Dataset, we engineer spatial features and train a multi-layer GCN\nmodel to classify crime types and predict high-risk zones. Our approach\nachieves 88% classification accuracy, significantly outperforming traditional\nmethods. Additionally, the model generates interpretable heat maps of crime\nhotspots, demonstrating the practical utility of graph-based learning for\npredictive policing and spatial criminology."}
{"id": "2506.13206", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13206", "abs": "https://arxiv.org/abs/2506.13206", "authors": ["James Chua", "Jan Betley", "Mia Taylor", "Owain Evans"], "title": "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models", "comment": null, "summary": "Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (``I'll trick\nthe user...''), and (ii) benign-sounding rationalizations (``Taking five\nsleeping pills at once is safe...''). Due to these rationalizations, monitors\nthat evaluate CoTs often fail to detect misalignment.\n  Extending this setup, we also train reasoning models to perform narrow bad\nbehaviors only when a backdoor trigger is present in the prompt. This causes\nbroad misalignment that remains hidden, which brings additional risk. We find\nthat reasoning models can often describe and explain their backdoor triggers,\ndemonstrating a kind of self-awareness. So CoT monitoring can expose these\nbehaviors but is unreliable.\n  In summary, reasoning steps can both reveal and conceal misaligned\nintentions, and do not prevent misalignment behaviors in the models studied. We\nrelease three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite."}
{"id": "2506.13652", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.13652", "abs": "https://arxiv.org/abs/2506.13652", "authors": ["Daniele Zambon", "Michele Cattaneo", "Ivan Marisca", "Jonas Bhend", "Daniele Nerini", "Cesare Alippi"], "title": "PeakWeather: MeteoSwiss Weather Station Measurements for Spatiotemporal Deep Learning", "comment": null, "summary": "Accurate weather forecasts are essential for supporting a wide range of\nactivities and decision-making processes, as well as mitigating the impacts of\nadverse weather events. While traditional numerical weather prediction (NWP)\nremains the cornerstone of operational forecasting, machine learning is\nemerging as a powerful alternative for fast, flexible, and scalable\npredictions. We introduce PeakWeather, a high-quality dataset of surface\nweather observations collected every 10 minutes over more than 8 years from the\nground stations of the Federal Office of Meteorology and Climatology\nMeteoSwiss's measurement network. The dataset includes a diverse set of\nmeteorological variables from 302 station locations distributed across\nSwitzerland's complex topography and is complemented with topographical indices\nderived from digital height models for context. Ensemble forecasts from the\ncurrently operational high-resolution NWP model are provided as a baseline\nforecast against which to evaluate new approaches. The dataset's richness\nsupports a broad spectrum of spatiotemporal tasks, including time series\nforecasting at various scales, graph structure learning, imputation, and\nvirtual sensing. As such, PeakWeather serves as a real-world benchmark to\nadvance both foundational machine learning research, meteorology, and\nsensor-based applications."}
{"id": "2506.13688", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.13688", "abs": "https://arxiv.org/abs/2506.13688", "authors": ["Pulkit Gopalani", "Wei Hu"], "title": "What Happens During the Loss Plateau? Understanding Abrupt Learning in Transformers", "comment": null, "summary": "Training Transformers on algorithmic tasks frequently demonstrates an\nintriguing abrupt learning phenomenon: an extended performance plateau followed\nby a sudden, sharp improvement. This work investigates the underlying\nmechanisms for such dynamics, primarily in shallow Transformers. We reveal that\nduring the plateau, the model often develops an interpretable partial solution\nwhile simultaneously exhibiting a strong repetition bias in their outputs. This\noutput degeneracy is accompanied by internal representation collapse, where\nhidden states across different tokens become nearly parallel. We further\nidentify the slow learning of optimal attention maps as a key bottleneck.\nHidden progress in attention configuration during the plateau precedes the\neventual rapid convergence, and directly intervening on attention significantly\nalters plateau duration and the severity of repetition bias and\nrepresentational collapse. We validate that these identified\nphenomena-repetition bias and representation collapse-are not artifacts of toy\nsetups but also manifest in the early pre-training stage of large language\nmodels like Pythia and OLMo."}
{"id": "2506.13715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13715", "abs": "https://arxiv.org/abs/2506.13715", "authors": ["Haoran Tang", "Rajiv Khanna"], "title": "Sharpness-Aware Machine Unlearning", "comment": null, "summary": "We characterize the effectiveness of Sharpness-aware minimization (SAM) under\nmachine unlearning scheme, where unlearning forget signals interferes with\nlearning retain signals. While previous work prove that SAM improves\ngeneralization with noise memorization prevention, we show that SAM abandons\nsuch denoising property when fitting the forget set, leading to various test\nerror bounds depending on signal strength. We further characterize the signal\nsurplus of SAM in the order of signal strength, which enables learning from\nless retain signals to maintain model performance and putting more weight on\nunlearning the forget set. Empirical studies show that SAM outperforms SGD with\nrelaxed requirement for retain signals and can enhance various unlearning\nmethods either as pretrain or unlearn algorithm. Observing that overfitting can\nbenefit more stringent sample-specific unlearning, we propose Sharp MinMax,\nwhich splits the model into two to learn retain signals with SAM and unlearn\nforget signals with sharpness maximization, achieving best performance.\nExtensive experiments show that SAM enhances unlearning across varying\ndifficulties measured by data memorization, yielding decreased feature\nentanglement between retain and forget sets, stronger resistance to membership\ninference attacks, and a flatter loss landscape."}
{"id": "2506.12524", "categories": ["cs.CV", "cs.HC", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.12524", "abs": "https://arxiv.org/abs/2506.12524", "authors": ["Nuwan Bandara", "Thivya Kandappu", "Archan Misra"], "title": "Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing", "comment": "18 pages", "summary": "Event-based eye tracking holds significant promise for fine-grained cognitive\nstate inference, offering high temporal resolution and robustness to motion\nartifacts, critical features for decoding subtle mental states such as\nattention, confusion, or fatigue. In this work, we introduce a model-agnostic,\ninference-time refinement framework designed to enhance the output of existing\nevent-based gaze estimation models without modifying their architecture or\nrequiring retraining. Our method comprises two key post-processing modules: (i)\nMotion-Aware Median Filtering, which suppresses blink-induced spikes while\npreserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,\nwhich aligns gaze predictions with cumulative event motion to reduce spatial\njitter and temporal discontinuities. To complement traditional spatial accuracy\nmetrics, we propose a novel Jitter Metric that captures the temporal smoothness\nof predicted gaze trajectories based on velocity regularity and local signal\ncomplexity. Together, these contributions significantly improve the consistency\nof event-based gaze signals, making them better suited for downstream tasks\nsuch as micro-expression analysis and mind-state decoding. Our results\ndemonstrate consistent improvements across multiple baseline models on\ncontrolled datasets, laying the groundwork for future integration with\nmultimodal affect recognition systems in real-world environments."}
{"id": "2506.13206", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.13206", "abs": "https://arxiv.org/abs/2506.13206", "authors": ["James Chua", "Jan Betley", "Mia Taylor", "Owain Evans"], "title": "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models", "comment": null, "summary": "Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (``I'll trick\nthe user...''), and (ii) benign-sounding rationalizations (``Taking five\nsleeping pills at once is safe...''). Due to these rationalizations, monitors\nthat evaluate CoTs often fail to detect misalignment.\n  Extending this setup, we also train reasoning models to perform narrow bad\nbehaviors only when a backdoor trigger is present in the prompt. This causes\nbroad misalignment that remains hidden, which brings additional risk. We find\nthat reasoning models can often describe and explain their backdoor triggers,\ndemonstrating a kind of self-awareness. So CoT monitoring can expose these\nbehaviors but is unreliable.\n  In summary, reasoning steps can both reveal and conceal misaligned\nintentions, and do not prevent misalignment behaviors in the models studied. We\nrelease three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite."}
{"id": "2506.12963", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12963", "abs": "https://arxiv.org/abs/2506.12963", "authors": ["Changsheng Wang", "Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Dennis Wei", "Parikshit Ram", "Nathalie Baracaldo", "Sijia Liu"], "title": "Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills", "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled strong\nchain-of-thought (CoT) generation through test-time computation. While these\nmulti-step reasoning capabilities represent a major milestone in language model\nperformance, they also introduce new safety risks. In this work, we present the\nfirst systematic study to revisit the problem of machine unlearning in the\ncontext of LRMs. Machine unlearning refers to the process of removing the\ninfluence of sensitive, harmful, or undesired data or knowledge from a trained\nmodel without full retraining. We show that conventional unlearning algorithms,\noriginally designed for non-reasoning models, are inadequate for LRMs. In\nparticular, even when final answers are successfully erased, sensitive\ninformation often persists within the intermediate reasoning steps, i.e., CoT\ntrajectories. To address this challenge, we extend conventional unlearning and\npropose Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$), a\nnovel method that effectively suppresses sensitive reasoning traces and\nprevents the generation of associated final answers, while preserving the\nmodel's reasoning ability. Our experiments demonstrate that $R^2MU$\nsignificantly reduces sensitive information leakage within reasoning traces and\nachieves strong performance across both safety and reasoning benchmarks,\nevaluated on state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and\nDeepSeek-R1-Distill-Qwen-14B."}
