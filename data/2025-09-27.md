<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization](https://arxiv.org/abs/2509.20785)
*Jincai Song,Haipeng Chen,Jun Qin,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对跨域半监督域泛化(CD-SSDG)的双监督非对称协同训练(DAC)框架，解决了医学图像分割中标注数据有限和域偏移的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统SSDG方法假设每个源域都有标注和无标注数据，但实际应用中标注数据有限且存在域偏移。本文探索更实用的CD-SSDG场景，其中训练数据内部也存在域偏移。

Method: 提出DAC框架，基于两个子模型的协同训练，集成特征级监督和非对称辅助任务。特征级监督解决域偏移导致的伪标签不准确问题，辅助任务增强域不变特征学习。

Result: 在Fundus、Polyp和SCGM等真实医学图像分割数据集上的实验表明，DAC框架具有强大的泛化能力。

Conclusion: DAC框架有效解决了CD-SSDG中的挑战，为医学图像分割提供了一种实用的半监督域泛化解决方案。

Abstract: Semi-supervised domain generalization (SSDG) in medical image segmentation
offers a promising solution for generalizing to unseen domains during testing,
addressing domain shift challenges and minimizing annotation costs. However,
conventional SSDG methods assume labeled and unlabeled data are available for
each source domain in the training set, a condition that is not always met in
practice. The coexistence of limited annotation and domain shift in the
training set is a prevalent issue. Thus, this paper explores a more practical
and challenging scenario, cross-domain semi-supervised domain generalization
(CD-SSDG), where domain shifts occur between labeled and unlabeled training
data, in addition to shifts between training and testing sets. Existing SSDG
methods exhibit sub-optimal performance under such domain shifts because of
inaccurate pseudolabels. To address this issue, we propose a novel
dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.
Building upon the co-training paradigm with two sub-models offering cross
pseudo supervision, our DAC framework integrates extra feature-level
supervision and asymmetric auxiliary tasks for each sub-model. This
feature-level supervision serves to address inaccurate pseudo supervision
caused by domain shifts between labeled and unlabeled data, utilizing
complementary supervision from the rich feature space. Additionally, two
distinct auxiliary self-supervised tasks are integrated into each sub-model to
enhance domain-invariant discriminative feature learning and prevent model
collapse. Extensive experiments on real-world medical image segmentation
datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust
generalizability of the proposed DAC framework.

</details>


### [2] [Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification](https://arxiv.org/abs/2509.20899)
*Patrick Knab,Sascha Marton,Philipp J. Schubert,Drago Guggiana,Christian Bartelt*

Main category: cs.CV

TL;DR: MoTIF是一个基于transformer架构的概念瓶颈模型，专门为视频分类设计，能够处理任意长度的视频序列，通过全局概念重要性、局部概念相关性和时间依赖性三个视角来解释视频中的动作和事件。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型主要针对静态图像，无法有效处理视频数据中的时间依赖性，而视频中的动作和事件需要通过时间序列中的概念模式来理解和解释。

Method: 采用transformer架构，将概念瓶颈框架扩展到视频分类，识别视频中重复出现的语义实体（如对象、属性、动作组件）作为概念，形成描述动作的概念模式。

Result: 实验结果表明，基于概念的建模范式可以有效应用于视频数据，在保持竞争力的分类性能的同时，能够更好地理解概念在时间上下文中的贡献。

Conclusion: MoTIF成功地将概念瓶颈模型扩展到视频领域，为视频分类提供了可解释的框架，能够从多个时间维度分析概念的重要性。

Abstract: Conceptual models such as Concept Bottleneck Models (CBMs) have driven
substantial progress in improving interpretability for image classification by
leveraging human-interpretable concepts. However, extending these models from
static images to sequences of images, such as video data, introduces a
significant challenge due to the temporal dependencies inherent in videos,
which are essential for capturing actions and events. In this work, we
introduce MoTIF (Moving Temporal Interpretable Framework), an architectural
design inspired by a transformer that adapts the concept bottleneck framework
for video classification and handles sequences of arbitrary length. Within the
video domain, concepts refer to semantic entities such as objects, attributes,
or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across
time - forming motifs collectively describing and explaining actions. Our
design explicitly enables three complementary perspectives: global concept
importance across the entire video, local concept relevance within specific
windows, and temporal dependencies of a concept over time. Our results
demonstrate that the concept-based modeling paradigm can be effectively
transferred to video data, enabling a better understanding of concept
contributions in temporal contexts while maintaining competitive performance.
Code available at github.com/patrick-knab/MoTIF.

</details>


### [3] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff是一个模拟器约束的扩散模型，通过将环境参数直接集成到去噪过程中，无需在推理时重复调用模拟器即可高效生成物理合理的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将基于模拟器的运动投影层整合到扩散过程中以确保物理合理性，但这些方法由于模拟器的顺序性质而计算成本高昂，无法并行化。

Method: 将模拟器运动投影解释为扩散过程中的一种引导形式，提出SimDiff模型，通过将环境参数（如重力、风力）直接条件化到去噪过程中，实现物理约束。

Result: SimDiff能够高效生成物理合理的运动，无需推理时的重复模拟器调用，并提供对不同物理系数的细粒度控制，同时成功泛化到未见过的环境参数组合。

Conclusion: SimDiff通过将物理约束直接集成到扩散模型中，实现了高效且可泛化的物理合理人体运动生成，为角色动画和虚拟现实应用提供了实用解决方案。

Abstract: Generating physically plausible human motion is crucial for applications such
as character animation and virtual reality. Existing approaches often
incorporate a simulator-based motion projection layer to the diffusion process
to enforce physical plausibility. However, such methods are computationally
expensive due to the sequential nature of the simulator, which prevents
parallelization. We show that simulator-based motion projection can be
interpreted as a form of guidance, either classifier-based or classifier-free,
within the diffusion process. Building on this insight, we propose SimDiff, a
Simulator-constrained Diffusion Model that integrates environment parameters
(e.g., gravity, wind) directly into the denoising process. By conditioning on
these parameters, SimDiff generates physically plausible motions efficiently,
without repeated simulator calls at inference, and also provides fine-grained
control over different physical coefficients. Moreover, SimDiff successfully
generalizes to unseen combinations of environmental parameters, demonstrating
compositional generalization.

</details>


### [4] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: SNCE是一种基于单神经元的概念擦除方法，通过操纵单个神经元精确防止有害内容生成，同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型存在生成有害内容的安全风险，现有概念擦除方法难以在精确移除目标概念的同时最小化图像质量退化

Method: 训练稀疏自编码器将文本嵌入映射到稀疏解耦的潜在空间，设计基于调制频率评分的新神经元识别方法定位有害概念相关神经元，通过抑制有害概念特定神经元的激活实现精确擦除

Result: 在各种基准测试中达到最先进的目标概念擦除效果，同时保持非目标概念的生成能力，对对抗攻击表现出强鲁棒性

Conclusion: SNCE方法通过单神经元操作实现了精确的概念擦除，在安全性和图像质量之间取得了良好平衡

Abstract: Text-to-image models exhibit remarkable capabilities in image generation.
However, they also pose safety risks of generating harmful content. A key
challenge of existing concept erasure methods is the precise removal of target
concepts while minimizing degradation of image quality. In this paper, we
propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can
precisely prevent harmful content generation by manipulating only a single
neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text
embeddings into a sparse, disentangled latent space, where individual neurons
align tightly with atomic semantic concepts. To accurately locate neurons
responsible for harmful concepts, we design a novel neuron identification
method based on the modulated frequency scoring of activation patterns. By
suppressing activations of the harmful concept-specific neuron, SNCE achieves
surgical precision in concept erasure with minimal disruption to image quality.
Experiments on various benchmarks demonstrate that SNCE achieves
state-of-the-art results in target concept erasure, while preserving the
model's generation capabilities for non-target concepts. Additionally, our
method exhibits strong robustness against adversarial attacks, significantly
outperforming existing methods.

</details>


### [5] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出MedVSR框架，专门针对医学视频超分辨率任务，解决现有VSR模型在医学视频中存在的对齐困难和伪影问题


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频对准确诊断至关重要，但受硬件限制难以获取。现有VSR模型在处理医学视频时面临相机抖动、噪声和帧间突变等独特挑战，导致光流误差和特征失真

Method: 提出Cross State-Space Propagation (CSSP)模块解决对齐问题，通过状态空间模型选择性传播一致特征；设计Inner State-Space Reconstruction (ISSR)模块增强组织结构并减少伪影，结合长程空间特征学习和大核短程信息聚合

Result: 在四个医学数据集（包括内窥镜和白内障手术）上的实验表明，MedVSR在重建性能和处理效率上显著优于现有VSR模型

Conclusion: MedVSR是针对医学视频超分辨率任务的定制化解决方案，能够有效处理医学视频特有的挑战，为临床诊断提供更高质量的视频支持

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641)
*Giovana Morais,Magdalena Fuentes*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Audio Large Language Models (Audio LLMs) enable human-like conversation about
music, yet it is unclear if they are truly listening to the audio or just using
textual reasoning, as recent benchmarks suggest. This paper investigates this
issue by quantifying the contribution of each modality to a model's output. We
adapt the MM-SHAP framework, a performance-agnostic score based on Shapley
values that quantifies the relative contribution of each modality to a model's
prediction. We evaluate two models on the MuChoMusic benchmark and find that
the model with higher accuracy relies more on text to answer questions, but
further inspection shows that even if the overall audio contribution is low,
models can successfully localize key sound events, suggesting that audio is not
entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs
and we hope it will serve as a foundational step for future research in
explainable AI and audio.

</details>


### [7] [FERD: Fairness-Enhanced Data-Free Robustness Distillation](https://arxiv.org/abs/2509.20793)
*Zhengxiao Li,Liming Lu,Xu Zheng,Siyuan Liang,Zhenghan Chen,Yongbin Zhou,Shuchao Pang*

Main category: cs.LG

TL;DR: 本文提出了FERD框架，通过调整对抗样本的比例和分布来解决数据自由鲁棒性蒸馏中的鲁棒公平性问题，提高最差类别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由鲁棒性蒸馏方法忽视了鲁棒公平性问题，导致不同类别间的鲁棒性存在显著差异，学生模型在不同攻击目标下的鲁棒性不稳定。

Method: FERD采用鲁棒性引导的类别重加权策略调整样本比例，生成公平感知样本和均匀目标对抗样本来平衡分布，通过特征级预测均匀性约束和均匀目标类别约束来实现公平增强。

Result: 在三个公开数据集上的实验表明，FERD在所有对抗攻击下都达到了最先进的最差类别鲁棒性，例如在CIFAR-10上使用MobileNet-V2时，FGSM和AutoAttack下的最差类别鲁棒性分别提高了15.1%和6.4%。

Conclusion: FERD框架在鲁棒性和公平性方面都表现出优越性能，有效解决了数据自由鲁棒性蒸馏中的鲁棒公平性问题。

Abstract: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from
the teacher to the student without accessing the training data. While existing
methods focus on overall robustness, they overlook the robust fairness issues,
leading to severe disparity of robustness across different categories. In this
paper, we find two key problems: (1) student model distilled with equal class
proportion data behaves significantly different across distinct categories; and
(2) the robustness of student model is not stable across different attacks
target. To bridge these gaps, we present the first Fairness-Enhanced data-free
Robustness Distillation (FERD) framework to adjust the proportion and
distribution of adversarial examples. For the proportion, FERD adopts a
robustness-guided class reweighting strategy to synthesize more samples for the
less robust categories, thereby improving robustness of them. For the
distribution, FERD generates complementary data samples for advanced robustness
distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a
uniformity constraint on feature-level predictions, which suppress the
dominance of class-specific non-robust features, providing a more balanced
representation across all categories. Then, FERD constructs Uniform-Target
Adversarial Examples (UTAEs) from FAEs by applying a uniform target class
constraint to avoid biased attack directions, which distribute the attack
targets across all categories and prevents overfitting to specific vulnerable
categories. Extensive experiments on three public datasets show that FERD
achieves state-of-the-art worst-class robustness under all adversarial attack
(e.g., the worst-class robustness under FGSM and AutoAttack are improved by
15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior
performance in both robustness and fairness aspects.

</details>


### [8] [Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition](https://arxiv.org/abs/2509.20840)
*Jiaqi Tang,Yinsong Xu,Yang Liu,Qingchao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段训练框架来解决多模态融合中的模态竞争问题，通过单模态训练塑造初始状态，并引入有效竞争强度(ECS)概念和可计算诊断指标来平衡模态竞争。


<details>
  <summary>Details</summary>
Motivation: 多模态融合在联合训练中经常出现模态竞争问题，其中一个模态主导学习过程，而其他模态优化不足。现有方法大多在联合学习阶段解决此问题，忽视了模型初始状态的重大影响。

Method: 提出两阶段训练框架：先通过单模态训练塑造初始状态；引入ECS概念量化模态竞争强度；开发可计算诊断指标（使用互信息作为ECS代理）和异步训练控制器；提出FastPID进行部分信息分解，测量模态特异性、冗余性和协同性。

Result: 在多个基准测试上的实验表明，该方法达到了最先进的性能，证明了通过塑造预融合模型的初始状态可以有效缓解模态竞争，实现协同多模态融合。

Conclusion: 研究表明，在联合训练前通过单模态训练塑造初始状态是一种有效策略，能够在竞争开始前缓解模态竞争问题，可靠地实现协同多模态融合。

Abstract: Multi-modal fusion often suffers from modality competition during joint
training, where one modality dominates the learning process, leaving others
under-optimized. Overlooking the critical impact of the model's initial state,
most existing methods address this issue during the joint learning stage. In
this study, we introduce a two-stage training framework to shape the initial
states through unimodal training before the joint training. First, we propose
the concept of Effective Competitive Strength (ECS) to quantify a modality's
competitive strength. Our theoretical analysis further reveals that properly
shaping the initial ECS by unimodal training achieves a provably tighter error
bound. However, ECS is computationally intractable in deep neural networks. To
bridge this gap, we develop a framework comprising two core components: a
fine-grained computable diagnostic metric and an asynchronous training
controller. For the metric, we first prove that mutual information(MI) is a
principled proxy for ECS. Considering MI is induced by per-modality marginals
and thus treats each modality in isolation, we further propose FastPID, a
computationally efficient and differentiable solver for partial information
decomposition, which decomposes the joint distribution's information into
fine-grained measurements: modality-specific uniqueness, redundancy, and
synergy. Guided by these measurements, our asynchronous controller dynamically
balances modalities by monitoring uniqueness and locates the ideal initial
state to start joint training by tracking peak synergy. Experiments on diverse
benchmarks demonstrate that our method achieves state-of-the-art performance.
Our work establishes that shaping the pre-fusion models' initial state is a
powerful strategy that eases competition before it starts, reliably unlocking
synergistic multi-modal fusion.

</details>


### [9] [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](https://arxiv.org/abs/2509.21161)
*Giuseppe Serra,Florian Buettner*

Main category: cs.LG

TL;DR: 本文提出了一种距离感知的温度缩放方法（DATS），用于解决持续学习中的校准问题，通过原型距离估计和自适应温度调整来减少跨任务的校准误差。


<details>
  <summary>Details</summary>
Motivation: 在持续学习场景中，现有方法主要从数据角度进行校准，使用单一温度参数，忽视了任务间的差异，导致校准误差波动较大。本文旨在开发一种更原则性的方法，根据任务距离自适应调整温度。

Method: 提出距离感知温度缩放（DATS），结合原型距离估计和距离感知校准，无需先验任务信息即可推断任务接近度并分配自适应温度。

Result: 在标准基准和真实世界不平衡生物医学数据集上的广泛实验表明，DATS方法在减少跨任务校准误差方面比现有方法更稳定、可靠和一致。

Conclusion: DATS方法有效解决了持续学习中的校准问题，通过距离感知的温度调整提高了模型在不同任务上的校准性能。

Abstract: Continual Learning (CL) is recently gaining increasing attention for its
ability to enable a single model to learn incrementally from a sequence of new
classes. In this scenario, it is important to keep consistent predictive
performance across all the classes and prevent the so-called Catastrophic
Forgetting (CF). However, in safety-critical applications, predictive
performance alone is insufficient. Predictive models should also be able to
reliably communicate their uncertainty in a calibrated manner - that is, with
confidence scores aligned to the true frequencies of target events. Existing
approaches in CL address calibration primarily from a data-centric perspective,
relying on a single temperature shared across all tasks. Such solutions
overlook task-specific differences, leading to large fluctuations in
calibration error across tasks. For this reason, we argue that a more
principled approach should adapt the temperature according to the distance to
the current task. However, the unavailability of the task information at test
time/during deployment poses a major challenge to achieve the intended
objective. For this, we propose Distance-Aware Temperature Scaling (DATS),
which combines prototype-based distance estimation with distance-aware
calibration to infer task proximity and assign adaptive temperatures without
prior task information. Through extensive empirical evaluation on both standard
benchmarks and real-world, imbalanced datasets taken from the biomedical
domain, our approach demonstrates to be stable, reliable and consistent in
reducing calibration error across tasks compared to state-of-the-art
approaches.

</details>


### [10] [Federated Flow Matching](https://arxiv.org/abs/2509.21250)
*Zifan Wang,Anqi Dong,Mahmoud Selim,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出了联邦流匹配框架，用于在隐私约束下训练流匹配模型，包括FFM-vanilla、FFM-LOT和FFM-GOT三种方法，在保护隐私的同时提升流直线性和样本质量


<details>
  <summary>Details</summary>
Motivation: 数据分散在不同设备和机构中，隐私、所有权和法规限制阻止了数据集中化，因此需要在不进行中心化聚合的情况下直接从分布式数据本地训练生成模型

Method: 1）FFM-vanilla：客户端使用独立源和目标耦合进行本地训练；2）FFM-LOT：使用局部最优传输耦合改进流直线性；3）FFM-GOT：基于最优传输半对偶公式，通过共享全局势函数协调客户端间耦合

Result: 在合成和图像数据集上的实验表明，FFM能够在保护隐私的同时提升流直线性和样本质量，性能与集中式基线相当

Conclusion: FFM框架成功解决了分布式数据环境下的隐私保护生成模型训练问题，在保持隐私的同时实现了与集中式方法相当的性能

Abstract: Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.

</details>


### [11] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: 提出了概率平滑策略优化（PSPO）方法，通过平滑当前策略概率来替代传统的比率裁剪，在保持梯度信号的同时创建软信任区域，显著提升语言模型在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法（如PPO、GRPO）依赖比率裁剪来稳定更新，但裁剪会丢弃信息并引入梯度不连续性。需要一种既能防止不稳定更新又能保留梯度信息的方法。

Method: PSPO方法在计算重要性比率之前，将当前策略的概率向旧策略平滑，类似于标签平滑。这种方法保留了梯度信号，同时通过向旧策略插值创建软信任区域来防止大的破坏性更新。

Result: 在Qwen2.5-0.5B和1.5B模型上的实验显示，相比裁剪GRPO，GR-PSPO在GSM8K测试集上性能提升超过20%（0.5B：39.7% vs 17.6%；1.5B：59.4% vs 37.8%），并改善了推理过程的清晰度和逻辑性。

Conclusion: PSPO提供了一种有效的替代比率裁剪的方法，既能稳定训练又能保持梯度信息，在数学推理任务上取得了显著性能提升，具有形式化保证。

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本文提出了一种用于时间触发系统(TTS)的新型重构框架，通过动态验证和组装调度来解决消息碰撞、优先级处理错误等问题，确保系统安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 时间触发系统在动态操作环境中面临消息碰撞、优先级处理错误导致的锁定循环、以及不完整或无效调度等挑战，这些都会影响系统安全性和性能。

Method: 提出重构框架，通过系统地将AI生成或启发式得到的调度优先级转换为完全可执行的调度，包含安全检查、高效分配算法和恢复机制来处理硬件故障和模式转换等意外事件。

Result: 在多个性能配置文件上的综合实验表明，该框架显著提升了系统适应性、操作完整性和运行时性能，同时保持计算效率。

Conclusion: 这项工作为安全关键TTS中的安全调度生成问题提供了一个实用且可扩展的解决方案，即使在高度动态和不确定的操作条件下也能实现可靠和灵活实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [13] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种集成在元调度器中的自适应在线学习单元，通过强化学习实时扩展多调度图，解决传统离线训练AI调度推理时无法覆盖完整概率空间的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练方法在构建包含所有可能场景的多调度图时面临资源密集和不可行的问题，特别是在考虑硬件故障、松弛变化等上下文事件时。

Method: 在元调度器中集成自适应在线学习单元，使用强化学习模型实时探索和发现新的调度解决方案，动态扩展多调度图。

Result: 在线学习单元能够处理意外事件和复杂调度场景，优化现有调度器，特别是在引入更严格截止时间或新性能标准时。

Conclusion: 通过实时训练持续优化AI推理，系统保持灵活性，能够满足不断变化的需求，确保大规模安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [14] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: RCA框架通过协调多个LLM从直接经验中学习，实现了高准确性和高质量解释的平衡，在疾病预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和大语言模型方法难以平衡预测准确性和解释透明度，需要开发能够深度理解数据并生成可信解释的模型。

Method: 提出反射认知架构(RCA)，采用迭代规则精炼机制和分布感知规则检查机制，通过预测准确性驱动深度理解。

Result: 在1个私有和2个公共数据集上评估，RCA在准确性和鲁棒性方面相对基线提升达40%，并能生成清晰、逻辑性强、基于证据的解释。

Conclusion: RCA通过深度数据理解实现了准确预测和高质量解释的协同提升，有望构建真正可信的临床决策支持系统。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出一个新颖框架，通过修改外交事件叙述来将公众情绪从负面转向中性或正面，使用语言模型预测公众反应并生成反事实文本修改。


<details>
  <summary>Details</summary>
Motivation: 公众情绪在外交中至关重要，但传统调查方法耗时费力且缺乏前瞻性分析能力。需要一种数据驱动的方法来指导外交沟通策略。

Method: 1) 训练语言模型预测公众对外交事件的反应；2) 基于传播理论和专家意见确定可修改的文本特征；3) 开发反事实生成算法系统修改原始文本。

Result: 该框架成功将公众情绪转向更有利状态，成功率70%。

Conclusion: 该框架可作为外交官、政策制定者和沟通专家的实用工具，提供数据驱动的见解来塑造更理想的公众情绪。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [16] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 该论文研究了LLM训练数据中语法模板与领域之间的虚假相关性，发现这种相关性会降低模型性能，并可能被用于绕过安全微调。


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何同时处理指令的语义、领域和语法信息，特别是语法与领域之间的虚假相关性对模型性能和安全性的影响。

Method: 使用合成训练数据集分析语法-领域相关性，开发评估框架检测该现象，并在多个模型（OLMo-2、Llama-4-Maverick、GPT-4o）上进行测试，包括安全微调案例研究。

Result: 发现语法-领域相关性会显著降低OLMo-2模型在实体知识任务上的性能（均值0.51±0.06），并在开放和闭源模型中都能检测到这种现象，可用于绕过安全拒绝机制。

Conclusion: 需要明确测试语法-领域相关性，并确保训练数据中特别是领域内的语法多样性，以防止此类虚假相关性。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework](https://arxiv.org/abs/2509.20705)
*Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti*

Main category: cs.RO

TL;DR: BIM2RDT框架通过AI技术将静态BIM模型转化为动态的机器人就绪数字孪生，集成BIM数据、物联网传感器和机器人采集数据，使用创新的SG-ICP点云配准算法提高对齐精度，并实现实时安全监控。


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业BIM数据与实时现场条件之间的差距，提升施工安全管理水平，通过数字孪生技术优化机器人作业路径和安全监控。

Method: 提出BIM2RDT框架，集成三种数据流：BIM几何语义信息、物联网活动数据、机器人视觉空间数据；开发SG-ICP点云配准算法，利用LLM推理推断物体方向先验；采用YOLOE目标检测和Shi-Tomasi角点检测技术；集成实时手-臂振动监测。

Result: SG-ICP算法相比标准ICP在特征遮挡场景下对齐RMSE降低64.3%-88.3%；HAV集成能在超过暴露限值时触发警告，提升ISO 5349-1合规性。

Conclusion: BIM2RDT框架成功将静态BIM转化为动态数字孪生，显著提升了施工安全管理和机器人作业效率，为建筑行业数字化转型提供了有效解决方案。

Abstract: The adoption of cyber-physical systems and jobsite intelligence that connects
design models, real-time site sensing, and autonomous field operations can
dramatically enhance digital management in the construction industry. This
paper introduces BIM2RDT (Building Information Models to Robot-Ready Site
Digital Twins), an agentic artificial intelligence (AI) framework designed to
transform static Building Information Modeling (BIM) into dynamic, robot-ready
digital twins (DTs) that prioritize safety during execution. The framework
bridges the gap between pre-existing BIM data and real-time site conditions by
integrating three key data streams: geometric and semantic information from BIM
models, activity data from IoT sensor networks, and visual-spatial data
collected by robots during site traversal. The methodology introduces
Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that
leverages large language model (LLM) reasoning. Unlike traditional methods,
SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors
based on BIM semantics, improving alignment accuracy by avoiding convergence on
local minima. This creates a feedback loop where robot-collected data updates
the DT, which in turn optimizes paths for missions. The framework employs YOLOE
object detection and Shi-Tomasi corner detection to identify and track
construction elements while using BIM geometry as a priori maps. The framework
also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping
sensor-detected safety events to the digital twin using IFC standards for
intervention. Experiments demonstrate SG-ICP's superiority over standard ICP,
achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with
occluded features, ensuring plausible orientations. HAV integration triggers
warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.

</details>
