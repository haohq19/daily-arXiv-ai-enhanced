{"id": "2511.02953", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02953", "abs": "https://arxiv.org/abs/2511.02953", "authors": ["Sadiq Layi Macaulay", "Nimet Kaygusuz", "Simon Hadfield"], "title": "EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation", "comment": null, "summary": "Event cameras, with their high dynamic range (HDR) and low latency, offer a\npromising alternative for robust depth estimation in challenging environments.\nHowever, many event-based depth estimation approaches are constrained by\nsmall-scale annotated datasets, limiting their generalizability to real-world\nscenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event\ncamera dataset curated from publicly available YouTube footage, which contains\nmore than 13B events across various environmental conditions and motions,\nincluding seasonal hiking, flying, scenic driving, and underwater exploration.\nEvtSlowTV is an order of magnitude larger than existing event datasets,\nproviding an unconstrained, naturalistic setting for event-based depth\nlearning. This work shows the suitability of EvtSlowTV for a self-supervised\nlearning framework to capitalise on the HDR potential of raw event streams. We\nfurther demonstrate that training with EvtSlowTV enhances the model's ability\nto generalise to complex scenes and motions. Our approach removes the need for\nframe-based annotations and preserves the asynchronous nature of event data.", "AI": {"tldr": "\u63d0\u51fa\u4e86EvtSlowTV\uff0c\u4e00\u4e2a\u4eceYouTube\u89c6\u9891\u6784\u5efa\u7684\u5927\u89c4\u6a21\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7130\u4ebf\u4e2a\u4e8b\u4ef6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u4f46\u73b0\u6709\u4e8b\u4ef6\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u3002", "method": "\u4eceYouTube\u516c\u5f00\u89c6\u9891\u6784\u5efaEvtSlowTV\u6570\u636e\u96c6\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5229\u7528\u539f\u59cb\u4e8b\u4ef6\u6d41\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u6f5c\u529b\uff0c\u65e0\u9700\u57fa\u4e8e\u5e27\u7684\u6807\u6ce8\u3002", "result": "EvtSlowTV\u6bd4\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u96c6\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8bad\u7ec3\u540e\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u548c\u8fd0\u52a8\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "EvtSlowTV\u4e3a\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65e0\u7ea6\u675f\u7684\u81ea\u7136\u73af\u5883\u8bbe\u7f6e\uff0c\u4fdd\u7559\u4e86\u4e8b\u4ef6\u6570\u636e\u7684\u5f02\u6b65\u7279\u6027\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2511.02957", "categories": ["cs.LG", "cs.CE", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02957", "abs": "https://arxiv.org/abs/2511.02957", "authors": ["Mohsin Mahmud Topu", "Mahfuz Ahmed Anik", "Azmine Toushik Wasi", "Md Manjurul Ahsan"], "title": "Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks", "comment": null, "summary": "Pavement infrastructure monitoring is challenged by complex spatial\ndependencies, changing environmental conditions, and non-linear deterioration\nacross road networks. Traditional Pavement Management Systems (PMS) remain\nlargely reactive, lacking real-time intelligence for failure prevention and\noptimal maintenance planning. To address this, we propose a unified Digital\nTwin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven\npavement health monitoring and predictive maintenance. Pavement segments and\nspatial relations are modeled as graph nodes and edges, while real-time UAV,\nsensor, and LiDAR data stream into the DT. The inductive GNN learns\ndeterioration patterns from graph-structured inputs to forecast distress and\nenable proactive interventions. Trained on a real-world-inspired dataset with\nsegment attributes and dynamic connectivity, our model achieves an R2 of\n0.3798, outperforming baseline regressors and effectively capturing non-linear\ndegradation. We also develop an interactive dashboard and reinforcement\nlearning module for simulation, visualization, and adaptive maintenance\nplanning. This DT-GNN integration enhances forecasting precision and\nestablishes a closed feedback loop for continuous improvement, positioning the\napproach as a foundation for proactive, intelligent, and sustainable pavement\nmanagement, with future extensions toward real-world deployment, multi-agent\ncoordination, and smart-city integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u9053\u8def\u5065\u5eb7\u76d1\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5efa\u6a21\u9053\u8def\u6bb5\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u3002", "motivation": "\u4f20\u7edf\u9053\u8def\u7ba1\u7406\u7cfb\u7edf\u7f3a\u4e4f\u5b9e\u65f6\u667a\u80fd\uff0c\u65e0\u6cd5\u6709\u6548\u9884\u9632\u6545\u969c\u548c\u4f18\u5316\u7ef4\u62a4\u89c4\u5212\uff0c\u9700\u8981\u89e3\u51b3\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u3001\u73af\u5883\u53d8\u5316\u548c\u975e\u7ebf\u6027\u52a3\u5316\u7b49\u6311\u6218\u3002", "method": "\u5c06\u9053\u8def\u6bb5\u548c\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u4e3a\u56fe\u8282\u70b9\u548c\u8fb9\uff0c\u5229\u7528\u65e0\u4eba\u673a\u3001\u4f20\u611f\u5668\u548c\u6fc0\u5149\u96f7\u8fbe\u5b9e\u65f6\u6570\u636e\uff0c\u91c7\u7528\u5f52\u7eb3\u5f0f\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u52a3\u5316\u6a21\u5f0f\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0aR2\u8fbe\u52300.3798\uff0c\u4f18\u4e8e\u57fa\u7ebf\u56de\u5f52\u5668\uff0c\u6709\u6548\u6355\u6349\u975e\u7ebf\u6027\u9000\u5316\uff0c\u5e76\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\u3002", "conclusion": "DT-GNN\u96c6\u6210\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5efa\u7acb\u4e86\u6301\u7eed\u6539\u8fdb\u7684\u95ed\u73af\u53cd\u9988\uff0c\u4e3a\u4e3b\u52a8\u3001\u667a\u80fd\u548c\u53ef\u6301\u7eed\u7684\u9053\u8def\u7ba1\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.03022", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.03022", "abs": "https://arxiv.org/abs/2511.03022", "authors": ["Lingqing Shen", "Chi Heem Wong", "Misaki Mito", "Arnab Chakrabarti"], "title": "Adaptive-Sensorless Monitoring of Shipping Containers", "comment": "Published in 2025 IEEE Big Data", "summary": "Monitoring the internal temperature and humidity of shipping containers is\nessential to preventing quality degradation during cargo transportation.\nSensorless monitoring -- machine learning models that predict the internal\nconditions of the containers using exogenous factors -- shows promise as an\nalternative to monitoring using sensors. However, it does not incorporate\ntelemetry information and correct for systematic errors, causing the\npredictions to differ significantly from the live data and confusing the users.\nIn this paper, we introduce the residual correction method, a general framework\nfor correcting for systematic biases in sensorless models after observing live\ntelemetry data. We call this class of models ``adaptive-sensorless''\nmonitoring. We train and evaluate adaptive-sensorless models on the 3.48\nmillion data points -- the largest dataset of container sensor readings ever\nused in academic research -- and show that they produce consistent improvements\nover the baseline sensorless models. When evaluated on the holdout set of the\nsimulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\\sim$\n2.31$^\\circ$C (vs 2.43$^\\circ$C by sensorless) for temperature and 5.72 $\\sim$\n7.09% for relative humidity (vs 7.99% by sensorless) and average root\nmean-squared errors (RMSEs) of 3.19 $\\sim$ 3.26$^\\circ$C for temperature (vs\n3.38$^\\circ$C by sensorless) and 7.70 $\\sim$ 9.12% for relative humidity (vs\n10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo\nmonitoring, early risk detection, and less dependence on full connectivity in\nglobal shipping.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65e0\u4f20\u611f\u5668\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u6821\u6b63\u6846\u67b6\u4fee\u6b63\u65e0\u4f20\u611f\u5668\u6a21\u578b\u7684\u7cfb\u7edf\u504f\u5dee\uff0c\u5728\u96c6\u88c5\u7bb1\u6e29\u6e7f\u5ea6\u76d1\u6d4b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65e0\u4f20\u611f\u5668\u76d1\u6d4b\u65b9\u6cd5\u4e0d\u5305\u542b\u9065\u6d4b\u4fe1\u606f\u4e14\u65e0\u6cd5\u6821\u6b63\u7cfb\u7edf\u8bef\u5dee\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u65f6\u6570\u636e\u5dee\u5f02\u663e\u8457\uff0c\u7ed9\u7528\u6237\u5e26\u6765\u56f0\u60d1\u3002", "method": "\u5f15\u5165\u6b8b\u5dee\u6821\u6b63\u65b9\u6cd5\uff0c\u5728\u89c2\u6d4b\u5230\u5b9e\u65f6\u9065\u6d4b\u6570\u636e\u540e\u6821\u6b63\u65e0\u4f20\u611f\u5668\u6a21\u578b\u7684\u7cfb\u7edf\u504f\u5dee\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u65e0\u4f20\u611f\u5668\u76d1\u6d4b\u6846\u67b6\u3002", "result": "\u5728348\u4e07\u6570\u636e\u70b9\u4e0a\u8bc4\u4f30\uff0c\u81ea\u9002\u5e94\u65e0\u4f20\u611f\u5668\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u6e29\u5ea6MAE\u4ece2.43\u00b0C\u964d\u81f32.24~2.31\u00b0C\uff0c\u6e7f\u5ea6MAE\u4ece7.99%\u964d\u81f35.72~7.09%\uff1b\u6e29\u5ea6RMSE\u4ece3.38\u00b0C\u964d\u81f33.19~3.26\u00b0C\uff0c\u6e7f\u5ea6RMSE\u4ece10.0%\u964d\u81f37.70~9.12%\u3002", "conclusion": "\u81ea\u9002\u5e94\u65e0\u4f20\u611f\u5668\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u8d27\u7269\u76d1\u6d4b\u3001\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u5168\u7403\u822a\u8fd0\u4e2d\u5b8c\u5168\u8fde\u63a5\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.03186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03186", "abs": "https://arxiv.org/abs/2511.03186", "authors": ["Yiru Chen", "Sally Fang", "Sai Sree Harsha", "Dan Luo", "Vaishnavi Muppala", "Fei Wu", "Shun Jiang", "Kun Qian", "Yunyao Li"], "title": "Adobe Summit Concierge Evaluation with Human in the Loop", "comment": "Accepted by 6th Workshop on Data Science with Human in the Loop @\n  VLDB 2025", "summary": "Generative AI assistants offer significant potential to enhance productivity,\nstreamline information access, and improve user experience in enterprise\ncontexts. In this work, we present Summit Concierge, a domain-specific AI\nassistant developed for Adobe Summit. The assistant handles a wide range of\nevent-related queries and operates under real-world constraints such as data\nsparsity, quality assurance, and rapid deployment. To address these challenges,\nwe adopt a human-in-the-loop development workflow that combines prompt\nengineering, retrieval grounding, and lightweight human validation. We describe\nthe system architecture, development process, and real-world deployment\noutcomes. Our experience shows that agile, feedback-driven development enables\nscalable and reliable AI assistants, even in cold-start scenarios.", "AI": {"tldr": "\u5f00\u53d1\u4e86Summit Concierge\uff0c\u4e00\u4e2a\u9488\u5bf9Adobe Summit\u7684\u9886\u57df\u7279\u5b9aAI\u52a9\u624b\uff0c\u91c7\u7528\u4eba\u5728\u56de\u8def\u5f00\u53d1\u6d41\u7a0b\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u548c\u8f7b\u91cf\u7ea7\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5728\u6570\u636e\u7a00\u758f\u7b49\u73b0\u5b9e\u7ea6\u675f\u4e0b\u5b9e\u73b0\u53ef\u9760\u90e8\u7f72\u3002", "motivation": "\u5229\u7528\u751f\u6210\u5f0fAI\u52a9\u624b\u63d0\u5347\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u751f\u4ea7\u529b\u3001\u7b80\u5316\u4fe1\u606f\u8bbf\u95ee\u5e76\u6539\u5584\u7528\u6237\u4f53\u9a8c\uff0c\u7279\u522b\u662f\u5728Adobe Summit\u8fd9\u6837\u7684\u4f01\u4e1a\u6d3b\u52a8\u4e2d\u3002", "method": "\u91c7\u7528\u4eba\u5728\u56de\u8def\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u548c\u8f7b\u91cf\u7ea7\u4eba\u5de5\u9a8c\u8bc1\uff0c\u6784\u5efa\u80fd\u591f\u5904\u7406\u5404\u79cd\u6d3b\u52a8\u76f8\u5173\u67e5\u8be2\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86Summit Concierge\u52a9\u624b\uff0c\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u7684\u6d3b\u52a8\u76f8\u5173\u67e5\u8be2\uff0c\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u5b9e\u73b0\u53ef\u9760\u8fd0\u884c\u3002", "conclusion": "\u654f\u6377\u3001\u53cd\u9988\u9a71\u52a8\u7684\u5f00\u53d1\u65b9\u6cd5\u5373\u4f7f\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684AI\u52a9\u624b\u90e8\u7f72\u3002"}}
{"id": "2511.03481", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03481", "abs": "https://arxiv.org/abs/2511.03481", "authors": ["Jianbo Yuan", "Haohua Zhu", "Jing Dai", "Sheng Yi"], "title": "Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control", "comment": "8 pages 18 fogures, IEEE RAL accept", "summary": "The human hand plays a vital role in daily life and industrial applications,\nyet replicating its multifunctional capabilities-including motion, sensing, and\ncoordinated manipulation-with robotic systems remains a formidable challenge.\nDeveloping a dexterous robotic hand requires balancing human-like agility with\nengineering constraints such as complexity, size-to-weight ratio, durability,\nand force-sensing performance. This letter presents Dex-Hand 021, a\nhigh-performance, cable-driven five-finger robotic hand with 12 active and 7\npassive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight\n1 kg design. We propose a proprioceptive force-sensing-based admittance control\nmethod to enhance manipulation. Experimental results demonstrate its superior\nperformance: a single-finger load capacity exceeding 10 N, fingertip\nrepeatability under 0.001 m, and force estimation errors below 0.2 N. Compared\nto PID control, joint torques in multi-object grasping are reduced by 31.19%,\nsignificantly improves force-sensing capability while preventing overload\nduring collisions. The hand excels in both power and precision grasps,\nsuccessfully executing 33 GRASP taxonomy motions and complex manipulation\ntasks. This work advances the design of lightweight, industrial-grade dexterous\nhands and enhances proprioceptive control, contributing to robotic manipulation\nand intelligent manufacturing.", "AI": {"tldr": "Dex-Hand 021\u662f\u4e00\u6b3e\u9ad8\u6027\u80fd\u7535\u7f06\u9a71\u52a8\u4e94\u6307\u673a\u68b0\u624b\uff0c\u5177\u670912\u4e2a\u4e3b\u52a8\u548c7\u4e2a\u88ab\u52a8\u81ea\u7531\u5ea6\uff0c\u91cd\u91cf\u4ec51kg\uff0c\u91c7\u7528\u672c\u4f53\u611f\u77e5\u529b\u4f20\u611f\u5bfc\u7eb3\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u6293\u53d6\u80fd\u529b\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4eba\u7c7b\u624b\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u590d\u5236\u5176\u591a\u529f\u80fd\u80fd\u529b\uff08\u5305\u62ec\u8fd0\u52a8\u3001\u611f\u77e5\u548c\u534f\u8c03\u64cd\u4f5c\uff09\u5bf9\u673a\u5668\u4eba\u7cfb\u7edf\u4ecd\u7136\u662f\u4e00\u4e2a\u5de8\u5927\u6311\u6218\u3002\u9700\u8981\u5e73\u8861\u7c7b\u4eba\u654f\u6377\u6027\u4e0e\u5de5\u7a0b\u7ea6\u675f\uff08\u5982\u590d\u6742\u6027\u3001\u5c3a\u5bf8\u91cd\u91cf\u6bd4\u3001\u8010\u4e45\u6027\u548c\u529b\u4f20\u611f\u6027\u80fd\uff09\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u672c\u4f53\u611f\u77e5\u529b\u4f20\u611f\u7684\u5bfc\u7eb3\u63a7\u5236\u65b9\u6cd5\uff0c\u91c7\u7528\u7535\u7f06\u9a71\u52a8\u8bbe\u8ba1\uff0c\u5177\u670912\u4e2a\u4e3b\u52a8\u548c7\u4e2a\u88ab\u52a8\u81ea\u7531\u5ea6\uff0c\u603b\u517119\u4e2a\u81ea\u7531\u5ea6\u3002", "result": "\u5355\u6307\u8d1f\u8f7d\u80fd\u529b\u8d85\u8fc710N\uff0c\u6307\u5c16\u91cd\u590d\u6027\u4f4e\u4e8e0.001m\uff0c\u529b\u4f30\u8ba1\u8bef\u5dee\u5c0f\u4e8e0.2N\u3002\u4e0ePID\u63a7\u5236\u76f8\u6bd4\uff0c\u591a\u7269\u4f53\u6293\u53d6\u65f6\u5173\u8282\u626d\u77e9\u51cf\u5c1131.19%\uff0c\u663e\u8457\u63d0\u9ad8\u529b\u4f20\u611f\u80fd\u529b\u5e76\u9632\u6b62\u78b0\u649e\u8fc7\u8f7d\u3002\u6210\u529f\u6267\u884c33\u79cdGRASP\u5206\u7c7b\u52a8\u4f5c\u548c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u8f7b\u91cf\u7ea7\u5de5\u4e1a\u7ea7\u7075\u5de7\u624b\u7684\u8bbe\u8ba1\uff0c\u589e\u5f3a\u4e86\u672c\u4f53\u611f\u77e5\u63a7\u5236\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u667a\u80fd\u5236\u9020\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2511.03178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03178", "abs": "https://arxiv.org/abs/2511.03178", "authors": ["Shreyas C. Dhake", "Jiayuan Huang", "Runlong He", "Danyal Z. Khan", "Evangelos B. Mazomenos", "Sophia Bano", "Hani J. Marcus", "Danail Stoyanov", "Matthew J. Clarkson", "Mobarak I. Hoque"], "title": "SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention", "comment": "12 pages", "summary": "Anticipating forthcoming surgical events is vital for real-time assistance in\nendonasal transsphenoidal pituitary surgery, where visibility is limited and\nworkflow changes rapidly. Most visual question answering (VQA) systems reason\non isolated frames with static vision language alignment, providing little\nsupport for forecasting next steps or instrument needs. Existing surgical VQA\ndatasets likewise center on the current scene rather than the near future. We\nintroduce PitVQA-Anticipation, the first VQA dataset designed for forward\nlooking surgical reasoning. It comprises 33.5 hours of operative video and\n734,769 question answer pairs built from temporally grouped clips and expert\nannotations across four tasks: predicting the future phase, next step, upcoming\ninstrument, and remaining duration. We further propose SurgAnt-ViVQA, a video\nlanguage model that adapts a large language model using a GRU Gated Temporal\nCross-Attention module. A bidirectional GRU encodes frame to frame dynamics,\nwhile an adaptive gate injects visual context into the language stream at the\ntoken level. Parameter efficient fine tuning customizes the language backbone\nto the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and\nEndoVis datasets, surpassing strong image and video based baselines. Ablations\nshow that temporal recurrence and gated fusion drive most of the gains. A frame\nbudget study indicates a trade-off: 8 frames maximize fluency, whereas 32\nframes slightly reduce BLEU but improve numeric time estimation. By pairing a\ntemporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA\nadvances surgical VQA from retrospective description to proactive anticipation.\nPitVQA-Anticipation offers a comprehensive benchmark for this setting and\nhighlights the importance of targeted temporal modeling for reliable, future\naware surgical assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u624b\u672f\u524d\u77bb\u6027\u63a8\u7406\u7684\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6PitVQA-Anticipation\uff0c\u5e76\u5f00\u53d1\u4e86SurgAnt-ViVQA\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u7f16\u7801\u548c\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u4ece\u56de\u987e\u6027\u63cf\u8ff0\u5230\u524d\u77bb\u6027\u9884\u6d4b\u7684\u8f6c\u53d8\u3002", "motivation": "\u5728\u9f3b\u5185\u955c\u4e0b\u7ecf\u8776\u5782\u4f53\u624b\u672f\u4e2d\uff0c\u80fd\u9884\u89c1\u5373\u5c06\u53d1\u751f\u7684\u624b\u672f\u4e8b\u4ef6\u5bf9\u5b9e\u65f6\u8f85\u52a9\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709VQA\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5f53\u524d\u573a\u666f\u800c\u975e\u672a\u6765\u9884\u6d4b\u3002", "method": "\u63d0\u51faSurgAnt-ViVQA\u6a21\u578b\uff0c\u91c7\u7528GRU\u95e8\u63a7\u65f6\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u53cc\u5411GRU\u7f16\u7801\u5e27\u95f4\u52a8\u6001\uff0c\u81ea\u9002\u5e94\u95e8\u63a7\u5728token\u7ea7\u522b\u6ce8\u5165\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5b9a\u5236\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728PitVQA-Anticipation\u548cEndoVis\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u8d85\u8d8a\u5f3a\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u7ebf\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u65f6\u95f4\u5faa\u73af\u548c\u95e8\u63a7\u878d\u5408\u5e26\u6765\u4e3b\u8981\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u7f16\u7801\u5668\u548c\u7ec6\u7c92\u5ea6\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7ed3\u5408\uff0cSurgAnt-ViVQA\u5c06\u624b\u672fVQA\u4ece\u56de\u987e\u6027\u63cf\u8ff0\u63a8\u8fdb\u5230\u524d\u77bb\u6027\u9884\u6d4b\uff0cPitVQA-Anticipation\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u3002"}}
{"id": "2511.03325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03325", "abs": "https://arxiv.org/abs/2511.03325", "authors": ["Mauro Orazio Drago", "Luca Carlini", "Pelinsu Celebi Balyemez", "Dennis Pierantozzi", "Chiara Lena", "Cesare Hassan", "Danail Stoyanov", "Elena De Momi", "Sophia Bano", "Mobarak I. Hoque"], "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding", "comment": null, "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA.", "AI": {"tldr": "SurgViVQA\u662f\u4e00\u4e2a\u5916\u79d1\u89c6\u9891\u95ee\u7b54\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u9891\u548c\u95ee\u9898\u7279\u5f81\u6765\u6355\u6349\u65f6\u95f4\u7ebf\u7d22\uff0c\u5728\u7ed3\u80a0\u955c\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u57fa\u51c6\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u8bcd\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e869-11%\u3002", "motivation": "\u5f53\u524d\u5916\u79d1\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u5c40\u9650\u4e8e\u9759\u6001\u56fe\u50cf\u7279\u5f81\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u52a8\u6001\u7684\u5efa\u6a21\uff0c\u800c\u624b\u672f\u8fc7\u7a0b\u4e2d\u7684\u8fd0\u52a8\u548c\u7ec4\u7ec7-\u5de5\u5177\u4ea4\u4e92\u7b49\u65f6\u95f4\u7ebf\u7d22\u5bf9\u4e8e\u51c6\u786e\u7406\u89e3\u624b\u672f\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faSurgViVQA\u6a21\u578b\uff0c\u4f7f\u7528\u63a9\u7801\u89c6\u9891-\u6587\u672c\u7f16\u7801\u5668\u878d\u5408\u89c6\u9891\u548c\u95ee\u9898\u7279\u5f81\uff0c\u6355\u6349\u8fd0\u52a8\u548c\u7ec4\u7ec7-\u5de5\u5177\u4ea4\u4e92\u7b49\u65f6\u95f4\u7ebf\u7d22\uff0c\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u4e3a\u8fde\u8d2f\u7b54\u6848\u3002", "result": "\u5728REAL-Colon-VQA\u548cEndoVis18-VQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSurgViVQA\u5728\u5173\u952e\u8bcd\u51c6\u786e\u7387\u4e0a\u6bd4PitVQA\u5206\u522b\u63d0\u534711%\u548c9%\uff0c\u6270\u52a8\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6a21\u578b\u5bf9\u95ee\u9898\u8868\u8ff0\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SurgViVQA\u548cREAL-Colon-VQA\u6570\u636e\u96c6\u4e3a\u5916\u79d1\u89c6\u9891\u95ee\u7b54\u63d0\u4f9b\u4e86\u65f6\u95f4\u611f\u77e5\u7406\u89e3\u7684\u6846\u67b6\uff0c\u4f7fAI\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u89e3\u91ca\u52a8\u6001\u624b\u672f\u573a\u666f\u3002"}}
{"id": "2511.03410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03410", "abs": "https://arxiv.org/abs/2511.03410", "authors": ["Longpeng Qiu", "Ting Li", "Shuai Mao", "Nan Yang", "Xiaohui Yan"], "title": "Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG", "comment": "EMNLP2025 Industry Track", "summary": "Input errors in question-answering (QA) systems often lead to incorrect\nresponses. Large language models (LLMs) struggle with this task, frequently\nfailing to interpret user intent (misinterpretation) or unnecessarily altering\nthe original question's structure (over-correction). We propose QuestionRAG, a\nframework that tackles these problems. To address misinterpretation, it\nenriches the input with external knowledge (e.g., search results, related\nentities). To prevent over-correction, it uses reinforcement learning (RL) to\nalign the model's objective with precise correction, not just paraphrasing. Our\nresults demonstrate that knowledge augmentation is critical for understanding\nfaulty questions. Furthermore, RL-based alignment proves significantly more\neffective than traditional supervised fine-tuning (SFT), boosting the model's\nability to follow instructions and generalize. By integrating these two\nstrategies, QuestionRAG unlocks the full potential of LLMs for the question\ncorrection task.", "AI": {"tldr": "QuestionRAG\u662f\u4e00\u4e2a\u89e3\u51b3QA\u7cfb\u7edf\u4e2d\u8f93\u5165\u9519\u8bef\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u6765\u6539\u5584LLM\u5bf9\u9519\u8bef\u95ee\u9898\u7684\u7406\u89e3\u548c\u4fee\u6b63\u80fd\u529b\u3002", "motivation": "QA\u7cfb\u7edf\u4e2d\u7684\u8f93\u5165\u9519\u8bef\u7ecf\u5e38\u5bfc\u81f4\u9519\u8bef\u56de\u7b54\uff0cLLM\u5728\u5904\u7406\u8fd9\u7c7b\u4efb\u52a1\u65f6\u5b58\u5728\u8bef\u89e3\u7528\u6237\u610f\u56fe\u6216\u8fc7\u5ea6\u4fee\u6b63\u95ee\u9898\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\uff08\u5982\u641c\u7d22\u7ed3\u679c\u3001\u76f8\u5173\u5b9e\u4f53\uff09\u589e\u5f3a\u8f93\u5165\u4ee5\u89e3\u51b3\u8bef\u89e3\u95ee\u9898\uff1b\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u6a21\u578b\u76ee\u6807\uff0c\u786e\u4fdd\u7cbe\u786e\u4fee\u6b63\u800c\u975e\u7b80\u5355\u6539\u5199\u3002", "result": "\u77e5\u8bc6\u589e\u5f3a\u5bf9\u4e8e\u7406\u89e3\u9519\u8bef\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff1b\u57fa\u4e8eRL\u7684\u5bf9\u9f50\u65b9\u6cd5\u6bd4\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u66f4\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff0cQuestionRAG\u5145\u5206\u91ca\u653e\u4e86LLM\u5728\u95ee\u9898\u4fee\u6b63\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.03239", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03239", "abs": "https://arxiv.org/abs/2511.03239", "authors": ["Philipp Reis", "Philipp Rigoll", "Christian Steinhauser", "Jacob Langner", "Eric Sax"], "title": "A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams", "comment": null, "summary": "Modern AI systems are increasingly constrained not by model capacity but by\nthe quality and diversity of their data. Despite growing emphasis on\ndata-centric AI, most datasets are still gathered in an open-loop manner which\naccumulates redundant samples without feedback from the current coverage. This\nresults in inefficient storage, costly labeling, and limited generalization. To\naddress this, this paper introduces \\ac{FCDC}, a paradigm that formulates data\ncollection as a closed-loop control problem. \\ac{FCDC} continuously\napproximates the state of the collected data distribution using an online\nprobabilistic model and adaptively regulates sample retention using based on\nfeedback signals such as likelihood and Mahalanobis distance. Through this\nfeedback mechanism, the system dynamically balances exploration and\nexploitation, maintains dataset diversity, and prevents redundancy from\naccumulating over time. Besides showcasing the controllability of \\ac{FCDC} on\na synthetic dataset, experiments on a real data stream show that \\ac{FCDC}\nproduces more balanced datasets by $\\SI{25.9}{\\percent}$ while reducing data\nstorage by $\\SI{39.8}{\\percent}$. These results demonstrate that data\ncollection itself can be actively controlled, transforming collection from a\npassive pipeline stage into a self-regulating, feedback-driven process at the\ncore of data-centric AI.", "AI": {"tldr": "FCDC\u5c06\u6570\u636e\u6536\u96c6\u5efa\u6a21\u4e3a\u95ed\u73af\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u7ebf\u6982\u7387\u6a21\u578b\u548c\u53cd\u9988\u673a\u5236\u52a8\u6001\u8c03\u8282\u6837\u672c\u4fdd\u7559\uff0c\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u6570\u636e\u96c6\u548c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u53d7\u9650\u4e8e\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u800c\u975e\u6a21\u578b\u5bb9\u91cf\uff0c\u4f20\u7edf\u5f00\u73af\u6570\u636e\u6536\u96c6\u65b9\u5f0f\u79ef\u7d2f\u5197\u4f59\u6837\u672c\uff0c\u5bfc\u81f4\u5b58\u50a8\u6548\u7387\u4f4e\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "FCDC\u4f7f\u7528\u5728\u7ebf\u6982\u7387\u6a21\u578b\u8fd1\u4f3c\u5df2\u6536\u96c6\u6570\u636e\u5206\u5e03\u72b6\u6001\uff0c\u57fa\u4e8e\u4f3c\u7136\u548c\u9a6c\u54c8\u62c9\u8bfa\u6bd4\u65af\u8ddd\u79bb\u7b49\u53cd\u9988\u4fe1\u53f7\u81ea\u9002\u5e94\u8c03\u8282\u6837\u672c\u4fdd\u7559\uff0c\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u6d41\u5b9e\u9a8c\u4e2d\uff0cFCDC\u4ea7\u751f\u66f4\u5e73\u8861\u7684\u6570\u636e\u96c6\uff08\u63d0\u534725.9%\uff09\uff0c\u540c\u65f6\u51cf\u5c11\u6570\u636e\u5b58\u50a839.8%\u3002", "conclusion": "\u6570\u636e\u6536\u96c6\u672c\u8eab\u53ef\u4ee5\u4e3b\u52a8\u63a7\u5236\uff0c\u5c06\u6536\u96c6\u4ece\u88ab\u52a8\u7ba1\u9053\u9636\u6bb5\u8f6c\u53d8\u4e3a\u6570\u636e\u4e2d\u5fc3AI\u6838\u5fc3\u7684\u81ea\u8c03\u8282\u3001\u53cd\u9988\u9a71\u52a8\u8fc7\u7a0b\u3002"}}
{"id": "2511.03243", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03243", "abs": "https://arxiv.org/abs/2511.03243", "authors": ["Miguel Costa", "Arthur Vandervoort", "Martin Drews", "Karyn Morrissey", "Francisco C. Pereira"], "title": "Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways", "comment": "Accepted for presentation at AI for Climate and Conservation Workshop\n  at EurIPS 2025", "summary": "Climate change will cause an increase in the frequency and severity of flood\nevents, prompting the need for cohesive adaptation policymaking. Designing\neffective adaptation policies, however, depends on managing the uncertainty of\nlong-term climate impacts. Meanwhile, such policies can feature important\nnormative choices that are not always made explicit. We propose that\nReinforcement Learning (RL) can be a useful tool to both identify adaptation\npathways under uncertain conditions while it also allows for the explicit\nmodelling (and consequent comparison) of different adaptation priorities (e.g.\neconomic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link\ntogether a rainfall and flood model, and compute the impacts of flooding in\nterms of quality of life (QoL), transportation, and infrastructure damage. Our\nresults show that models prioritising QoL over economic impacts results in more\nadaptation spending as well as a more even distribution of spending over the\nstudy area, highlighting the extent to which such normative assumptions can\nalter adaptation policy. Our framework is publicly available:\nhttps://github.com/MLSM-at-DTU/maat_qol_framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u8bc6\u522b\u6c14\u5019\u53d8\u5316\u4e0b\u7684\u9002\u5e94\u8def\u5f84\uff0c\u5e76\u660e\u786e\u5efa\u6a21\u4e0d\u540c\u7684\u9002\u5e94\u4f18\u5148\u7ea7\uff08\u7ecf\u6d4evs\u798f\u7949\uff09\uff0c\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u6a21\u578b\u5206\u6790\u6d2a\u6c34\u5f71\u54cd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5c06\u589e\u52a0\u6d2a\u6c34\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u9002\u5e94\u653f\u7b56\u5236\u5b9a\uff0c\u4f46\u957f\u671f\u6c14\u5019\u5f71\u54cd\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u653f\u7b56\u4e2d\u7684\u89c4\u8303\u6027\u9009\u62e9\u5f80\u5f80\u672a\u660e\u786e\u8003\u8651\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u7efc\u5408\u8bc4\u4f30\u6a21\u578b\uff08IAM\uff09\uff0c\u5c06\u964d\u96e8\u548c\u6d2a\u6c34\u6a21\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u8ba1\u7b97\u6d2a\u6c34\u5bf9\u751f\u6d3b\u8d28\u91cf\u3001\u4ea4\u901a\u548c\u57fa\u7840\u8bbe\u65bd\u7684\u5f71\u54cd\u3002", "result": "\u4f18\u5148\u8003\u8651\u751f\u6d3b\u8d28\u91cf\u800c\u975e\u7ecf\u6d4e\u5f71\u54cd\u7684\u6a21\u578b\u4f1a\u5bfc\u81f4\u66f4\u591a\u7684\u9002\u5e94\u652f\u51fa\uff0c\u5e76\u5728\u7814\u7a76\u533a\u57df\u5185\u66f4\u5747\u5300\u5730\u5206\u914d\u652f\u51fa\uff0c\u8868\u660e\u89c4\u8303\u6027\u5047\u8bbe\u4f1a\u663e\u8457\u6539\u53d8\u9002\u5e94\u653f\u7b56\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u8bc6\u522b\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u9002\u5e94\u8def\u5f84\u7684\u6709\u7528\u5de5\u5177\uff0c\u540c\u65f6\u5141\u8bb8\u660e\u786e\u5efa\u6a21\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u9002\u5e94\u4f18\u5148\u7ea7\uff0c\u6846\u67b6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2511.03251", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.03251", "abs": "https://arxiv.org/abs/2511.03251", "authors": ["Zhibin Wang", "Zhixing Zhang", "Shuqi Wang", "Xuanting Xie", "Zhao Kang"], "title": "GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated impressive performance on\ntask-specific benchmarks, yet their ability to generalize across diverse\ndomains and tasks remains limited. Existing approaches often struggle with\nnegative transfer, scalability issues, and high adaptation costs. To address\nthese challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel\nframework that seamlessly integrates the Mixture-of-Experts (MoE) architecture\nwith prompt-based learning for graphs. GMoPE leverages expert-specific prompt\nvectors and structure-aware MoE routing to enable each expert to specialize in\ndistinct subdomains and dynamically contribute to predictions. To promote\ndiversity and prevent expert collapse, we introduce a soft orthogonality\nconstraint across prompt vectors, encouraging expert specialization and\nfacilitating a more balanced expert utilization. Additionally, we adopt a\nprompt-only fine-tuning strategy that significantly reduces spatiotemporal\ncomplexity during transfer. We validate GMoPE through extensive experiments\nunder various pretraining strategies and multiple downstream tasks. Results\nshow that GMoPE consistently outperforms state-of-the-art baselines and\nachieves performance comparable to full parameter fine-tuning-while requiring\nonly a fraction of the adaptation overhead. Our work provides a principled and\nscalable framework for advancing generalizable and efficient graph foundation\nmodels.", "AI": {"tldr": "GMoPE\u662f\u4e00\u4e2a\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u63d0\u793a\u5b66\u4e60\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u7279\u5b9a\u63d0\u793a\u5411\u91cf\u548c\u7ed3\u6784\u611f\u77e5\u8def\u7531\u5b9e\u73b0\u8de8\u9886\u57df\u6cdb\u5316\uff0c\u663e\u8457\u964d\u4f4e\u8fc1\u79fb\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8de8\u9886\u57df\u548c\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u8d1f\u8fc1\u79fb\u3001\u53ef\u6269\u5c55\u6027\u5dee\u548c\u9002\u5e94\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u901a\u7528\u548c\u9ad8\u6548\u7684\u56fe\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faGMoPE\u6846\u67b6\uff0c\u96c6\u6210\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u4e0e\u56fe\u63d0\u793a\u5b66\u4e60\uff0c\u4f7f\u7528\u4e13\u5bb6\u7279\u5b9a\u63d0\u793a\u5411\u91cf\u548c\u7ed3\u6784\u611f\u77e5\u8def\u7531\uff0c\u5f15\u5165\u8f6f\u6b63\u4ea4\u7ea6\u675f\u4fc3\u8fdb\u4e13\u5bb6\u591a\u6837\u6027\uff0c\u91c7\u7528\u4ec5\u63d0\u793a\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5728\u5404\u79cd\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGMoPE\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u540c\u65f6\u4ec5\u9700\u5c11\u91cf\u9002\u5e94\u5f00\u9500\u3002", "conclusion": "GMoPE\u4e3a\u63a8\u8fdb\u901a\u7528\u5316\u548c\u9ad8\u6548\u7684\u56fe\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2511.03459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03459", "abs": "https://arxiv.org/abs/2511.03459", "authors": ["Kevin Manogue", "Tomasz M Schang", "Dilara Ku\u015f", "Jonas M\u00fcller", "Stefan Zachow", "Agniva Sengupta"], "title": "Generalizing Shape-from-Template to Topological Changes", "comment": "Accepted for publication at Smart Tools and Applications in Graphics\n  (STAG), Genoa, Italy (2025)", "summary": "Reconstructing the surfaces of deformable objects from correspondences\nbetween a 3D template and a 2D image is well studied under Shape-from-Template\n(SfT) methods; however, existing approaches break down when topological changes\naccompany the deformation. We propose a principled extension of SfT that\nenables reconstruction in the presence of such changes. Our approach is\ninitialized with a classical SfT solution and iteratively adapts the template\nby partitioning its spatial domain so as to minimize an energy functional that\njointly encodes physical plausibility and reprojection consistency. We\ndemonstrate that the method robustly captures a wide range of practically\nrelevant topological events including tears and cuts on bounded 2D surfaces,\nthereby establishing the first general framework for topological-change-aware\nSfT. Experiments on both synthetic and real data confirm that our approach\nconsistently outperforms baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u62d3\u6251\u53d8\u5316\u7684\u5f62\u72b6\u6a21\u677f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u5272\u6a21\u677f\u7a7a\u95f4\u57df\u6765\u6700\u5c0f\u5316\u80fd\u91cf\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5728\u6495\u88c2\u3001\u5207\u5272\u7b49\u62d3\u6251\u53d8\u5316\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u5f62\u72b6\u6a21\u677f\u91cd\u5efa\u65b9\u6cd5\u5728\u9047\u5230\u62d3\u6251\u53d8\u5316\u65f6\u4f1a\u5931\u6548\uff0c\u9700\u8981\u6269\u5c55SfT\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u7c7b\u60c5\u51b5\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178SfT\u89e3\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u5272\u6a21\u677f\u7a7a\u95f4\u57df\u6765\u6700\u5c0f\u5316\u540c\u65f6\u7f16\u7801\u7269\u7406\u5408\u7406\u6027\u548c\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u7684\u80fd\u91cf\u51fd\u6570\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u9c81\u68d2\u5730\u6355\u6349\u5404\u79cd\u5b9e\u9645\u76f8\u5173\u7684\u62d3\u6251\u4e8b\u4ef6\uff0c\u5305\u62ec\u6709\u754c2D\u8868\u9762\u7684\u6495\u88c2\u548c\u5207\u5272\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u901a\u7528\u7684\u62d3\u6251\u53d8\u5316\u611f\u77e5SfT\u6846\u67b6\uff0c\u4e3a\u5904\u7406\u62d3\u6251\u53d8\u5316\u7684\u8868\u9762\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03665", "abs": "https://arxiv.org/abs/2511.03665", "authors": ["Mehdi Sefidgar Dilmaghani", "Francis Fowley", "Peter Corcoran"], "title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential", "comment": null, "summary": "This paper presents a lightweight three-dimensional convolutional neural\nnetwork (3DCNN) for human activity recognition (HAR) using event-based vision\ndata. Privacy preservation is a key challenge in human monitoring systems, as\nconventional frame-based cameras capture identifiable personal information. In\ncontrast, event cameras record only changes in pixel intensity, providing an\ninherently privacy-preserving sensing modality. The proposed network\neffectively models both spatial and temporal dynamics while maintaining a\ncompact design suitable for edge deployment. To address class imbalance and\nenhance generalization, focal loss with class reweighting and targeted data\naugmentation strategies are employed. The model is trained and evaluated on a\ncomposite dataset derived from the Toyota Smart Home and ETRI datasets.\nExperimental results demonstrate an F1-score of 0.9415 and an overall accuracy\nof 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,\nand MC3_18 by up to 3%. These results highlight the potential of event-based\ndeep learning for developing accurate, efficient, and privacy-aware human\naction recognition systems suitable for real-world edge applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea73DCNN\u7528\u4e8e\u57fa\u4e8e\u4e8b\u4ef6\u89c6\u89c9\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff0c\u5728\u590d\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.17%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u5728\u4eba\u7c7b\u76d1\u63a7\u7cfb\u7edf\u4e2d\u4f1a\u6355\u6349\u53ef\u8bc6\u522b\u4e2a\u4eba\u4fe1\u606f\uff0c\u5b58\u5728\u9690\u79c1\u95ee\u9898\u3002\u4e8b\u4ef6\u76f8\u673a\u4ec5\u8bb0\u5f55\u50cf\u7d20\u5f3a\u5ea6\u53d8\u5316\uff0c\u63d0\u4f9b\u56fa\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u611f\u77e5\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea73DCNN\u5efa\u6a21\u65f6\u7a7a\u52a8\u6001\uff0c\u91c7\u7528\u7126\u70b9\u635f\u5931\u548c\u7c7b\u522b\u91cd\u52a0\u6743\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u7ed3\u5408\u9488\u5bf9\u6027\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728Toyota Smart Home\u548cETRI\u590d\u5408\u6570\u636e\u96c6\u4e0a\u83b7\u5f970.9415\u7684F1\u5206\u6570\u548c94.17%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u6bd4C3D\u3001ResNet3D\u548cMC3_18\u7b49\u57fa\u51c6\u6a21\u578b\u9ad8\u51fa\u6700\u591a3%\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u5f00\u53d1\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u9690\u79c1\u611f\u77e5\u7684\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u8fb9\u7f18\u5e94\u7528\u3002"}}
{"id": "2511.03661", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03661", "abs": "https://arxiv.org/abs/2511.03661", "authors": ["Mahek Desai", "Apoorva Rumale", "Marjan Asadinia"], "title": "SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection", "comment": null, "summary": "The integration of IoT devices in healthcare introduces significant security\nand reliability challenges, increasing susceptibility to cyber threats and\noperational anomalies. This study proposes a machine learning-driven framework\nfor (1) detecting malicious cyberattacks and (2) identifying faulty device\nanomalies, leveraging a dataset of 200,000 records. Eight machine learning\nmodels are evaluated across three learning approaches: supervised learning\n(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative\nAdversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised\nlearning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph\nNeural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The\ncomprehensive evaluation was conducted across multiple metrics like F1-score,\nprecision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost\nachieved 99\\% accuracy with minimal computational overhead (0.04s) for anomaly\ndetection, while Isolation Forest balanced precision and recall effectively.\nLSTM Autoencoders underperformed with lower accuracy and higher latency. For\nattack detection, KNN achieved near-perfect precision, recall, and F1-score\nwith the lowest computational cost (0.05s), followed by VAE at 97% accuracy.\nGAN showed the highest computational cost with lowest accuracy and ROC-AUC.\nThese findings enhance IoT-enabled healthcare security through effective\nanomaly detection strategies. By improving early detection of cyber threats and\ndevice failures, this framework has the potential to prevent data breaches,\nminimize system downtime, and ensure the continuous and safe operation of\nmedical devices, ultimately safeguarding patient health and trust in IoT-driven\nhealthcare solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u533b\u7597\u7269\u8054\u7f51\u8bbe\u5907\u7684\u6076\u610f\u7f51\u7edc\u653b\u51fb\u548c\u8bbe\u5907\u6545\u969c\u5f02\u5e38\uff0c\u8bc4\u4f30\u4e868\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u76d1\u7763\u3001\u534a\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u533b\u7597\u7269\u8054\u7f51\u8bbe\u5907\u9762\u4e34\u4e25\u91cd\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6311\u6218\uff0c\u5bb9\u6613\u53d7\u5230\u7f51\u7edc\u5a01\u80c1\u548c\u64cd\u4f5c\u5f02\u5e38\u7684\u5f71\u54cd\uff0c\u9700\u8981\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u7b56\u7565\u6765\u4fdd\u969c\u533b\u7597\u8bbe\u5907\u7684\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u4f7f\u7528\u5305\u542b20\u4e07\u6761\u8bb0\u5f55\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5b66\u4e60\u65b9\u6cd5\u76848\u79cd\u6a21\u578b\uff1a\u76d1\u7763\u5b66\u4e60\uff08XGBoost\u3001KNN\uff09\u3001\u534a\u76d1\u7763\u5b66\u4e60\uff08GAN\u3001VAE\uff09\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff08One-Class SVM\u3001Isolation Forest\u3001GNN\u3001LSTM Autoencoders\uff09\u3002", "result": "XGBoost\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fbe\u523099%\u51c6\u786e\u7387\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\uff080.04\u79d2\uff09\uff0cIsolation Forest\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002KNN\u5728\u653b\u51fb\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff0c\u8ba1\u7b97\u6210\u672c\u6700\u4f4e\uff080.05\u79d2\uff09\u3002LSTM Autoencoders\u8868\u73b0\u8f83\u5dee\uff0cGAN\u8ba1\u7b97\u6210\u672c\u6700\u9ad8\u4e14\u51c6\u786e\u7387\u6700\u4f4e\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u7b56\u7565\u589e\u5f3a\u4e86\u533b\u7597\u7269\u8054\u7f51\u5b89\u5168\u6027\uff0c\u80fd\u591f\u65e9\u671f\u68c0\u6d4b\u7f51\u7edc\u5a01\u80c1\u548c\u8bbe\u5907\u6545\u969c\uff0c\u9884\u9632\u6570\u636e\u6cc4\u9732\uff0c\u51cf\u5c11\u7cfb\u7edf\u505c\u673a\u65f6\u95f4\uff0c\u786e\u4fdd\u533b\u7597\u8bbe\u5907\u6301\u7eed\u5b89\u5168\u8fd0\u884c\uff0c\u4fdd\u62a4\u60a3\u8005\u5065\u5eb7\u548c\u4fe1\u4efb\u3002"}}
