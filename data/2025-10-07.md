<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 11]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 该论文提出了一种基于LSTM和注意力机制的面部表情分类方法，专门针对类别不平衡问题，通过误差校正技术提高对小类别的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决面部表情分类中类别不平衡问题，其中某些情绪类别显著多于其他类别，影响分类性能。

Method: 使用基于LSTM的神经网络模型，结合注意力机制聚焦于面部关键区域，在六类子集上训练后对第七类进行误差校正。

Result: 所有类别都能进行校正，但效果各异；测试集上小类别的关键质量指标有所提升，表明该方法在稀有事件检测中具有潜力。

Conclusion: 该方法可有效应用于面部表情分析系统，在类别分布不平衡的情况下提供稳定的分类性能，特别适用于反欺诈等需要检测稀有事件的应用场景。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [2] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoRA Patching的新方法，通过向Deepfake生成器注入可插拔的LoRA补丁来绕过最先进的主动防御系统，同时提出了防御性LoRA补丁作为补充解决方案。


<details>
  <summary>Details</summary>
Motivation: Deepfake技术对社会构成重大风险，促使开发在面部图像中嵌入对抗性扰动的主动防御措施。然而，这些防御措施往往缺乏鲁棒性和可靠性。

Method: 提出了LoRA Patching方法，包括：1）向Deepfake生成器注入可插拔的LoRA补丁；2）可学习的门控机制自适应控制LoRA补丁效果并防止梯度爆炸；3）多模态特征对齐损失（MMFA），在语义层面使对抗输出特征与期望输出特征对齐。

Result: 仅使用1,000个面部示例和单个训练周期，LoRA Patching成功击败了多个主动防御系统，揭示了当前防御范式的关键弱点。

Conclusion: 当前Deepfake防御策略存在严重漏洞，需要开发更鲁棒的防御方法。同时提出了防御性LoRA补丁作为缓解这种新发现安全漏洞的补充解决方案。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [3] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO是一个扩展的LIBERO基准测试，通过系统评估模型在四个维度扰动下的表现，揭示了现有模型严重依赖训练数据的记忆而非真正的任务理解。


<details>
  <summary>Details</summary>
Motivation: 当前LIBERO基准的训练和评估设置存在问题，导致性能估计被夸大，无法公平比较模型。需要更严格的评估方法来测试模型的泛化能力。

Method: 引入LIBERO-PRO基准，在四个维度上施加合理扰动：操作对象、初始状态、任务指令和环境设置，系统评估模型性能。

Result: 现有模型在标准LIBERO评估中达到90%以上准确率，但在广义设置下性能崩溃至0.0%，显示模型依赖动作序列和环境布局的记忆而非真正理解。

Conclusion: 当前评估方法存在严重缺陷，呼吁社区放弃误导性方法，采用能评估模型泛化能力和理解力的鲁棒评估方法。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [4] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: 提出Video-in-the-Loop (ViTL)框架，通过两阶段处理长视频问答：先定位问题相关时间区间，再重新分配视觉token进行回答，在固定token预算下实现高效长视频理解。


<details>
  <summary>Details</summary>
Motivation: 解决长视频问答中的计算效率问题，传统方法处理整个视频计算成本高，需要一种能在固定计算预算下有效处理长视频的方法。

Method: 两阶段框架：1) 使用低帧率浏览定位问题相关时间区间；2) 通过跨度感知的token重新分配，在更高有效帧率下回答问题。端到端训练，结合时间IoU和答案正确性的联合目标。

Result: 在固定token预算下，ViTL在长视频问答和时间定位任务上达到8.6%的性能提升，同时减少50%的帧输入。跨度感知token重新分配始终优于均匀采样。

Conclusion: ViTL和配套数据集提供了一个可解释、计算高效的解决方案，为可扩展的长视频问答提供了有效方法。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [5] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 提出了AgentAug数据增强框架，通过模拟典型创作过程生成多样化假新闻视频，提升短视频假新闻检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测器因训练数据有限且多样性不足，导致模式偏见和性能受限。真实场景中视频素材与假新闻事件存在复杂多对多关系，但现有数据集未能充分反映这种关系。

Method: 采用多LLM驱动的四种伪造类别管道生成假新闻视频，结合基于不确定性采样的主动学习策略选择有用的增强样本。

Result: 在两个基准数据集上的实验结果表明，AgentAug能持续提升短视频假新闻检测器的性能。

Conclusion: AgentAug通过模拟真实创作过程的数据增强，有效解决了假新闻检测中的数据稀疏和多样性不足问题。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [6] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight是一个用于图像推理的模型，通过动态调用多个专业工具迭代缩小屏幕相关区域，显著提高视觉定位准确性，在ScreenSpot-Pro基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图形用户界面系统中的实际应用受到视觉定位可靠性的限制，无法准确执行指针级操作如点击或拖动。

Method: 引入GUI-Spotlight模型，通过动态调用多个专业工具进行图像推理，迭代缩小焦点到屏幕相关区域。

Result: 在ScreenSpot-Pro基准测试中，仅使用18.5K训练样本的GUI-Spotlight达到52.8%准确率，超越V2P-7B（50.6%，使用9.6M样本）和GTA-1-7B（50.1%，使用1.56M样本）。

Conclusion: GUI-Spotlight通过工具调用和迭代聚焦的方法，有效解决了多模态大语言模型在图形用户界面系统中的视觉定位问题，显著提高了准确性。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [7] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 提出了一种改进的损失函数，将耀斑子类别间的序数信息整合到二元交叉熵损失中，以解决传统二元分类忽略耀斑强度序数关系的问题。


<details>
  <summary>Details</summary>
Motivation: 传统耀斑预测的二元分类框架忽略了FL和NF类别内子类别间的序数关系，且研究表明模型在预测阈值附近最容易出现误分类。

Method: 在传统二元交叉熵损失基础上，集成耀斑标签子类别的序数信息，作为序数感知的数据驱动正则化方法，对阈值附近的错误预测施加更重惩罚。

Result: 该方法通过利用数据的序数特性增强模型学习过程，旨在提高整体性能。

Conclusion: 提出的序数加权损失函数能有效改善模型对耀斑强度相似但位于二元阈值两侧事件的区分能力。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [8] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 提出了拓扑映射的标准化评估协议，包括拓扑一致性作为核心指标、数据集模糊度量化方法，并发布了基准数据集和基线系统。


<details>
  <summary>Details</summary>
Motivation: 拓扑映射领域缺乏标准化的评估指标、数据集和协议，现有系统在不同环境和标准下评估，无法进行公平可复现的比较，且感知混淆问题未被充分量化。

Method: 形式化拓扑一致性作为拓扑映射的基本属性，使用定位精度作为替代指标；提出数据集模糊度的量化方法；构建具有校准模糊度级别的多样化基准数据集；实现并发布深度学习基线系统。

Result: 建立了首个拓扑映射标准化评估框架，通过实验分析揭示了当前方法在感知混淆下的局限性，所有数据集、基线和评估工具均已开源。

Conclusion: 该工作填补了拓扑映射领域的评估空白，为公平可复现的研究提供了标准化协议和工具，推动了该领域的进一步发展。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [9] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了基于事件相机的网格流估计新任务，创建了高分辨率事件网格流数据集HREM，开发了轻量级EEMFlow网络用于快速准确的网格流估计，并扩展为支持稠密光流的EEMFlow+，同时提出自适应密度模块ADM提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决事件相机网格流估计领域缺乏专用数据集和方法的问题，以及事件数据密度变化带来的挑战。

Method: 1) 创建HREM高分辨率事件网格流数据集；2) 设计轻量级EEMFlow网络架构；3) 提出置信度引导细节补全模块CDC；4) 开发自适应密度模块ADM。

Result: EEMFlow模型在性能和运行效率上优于现有最优方法（快30倍），ADM模块使EEMFlow和EEMFlow+性能分别提升8%和10%。

Conclusion: 本文成功解决了事件相机网格流估计的关键挑战，提出的数据集、网络架构和密度自适应方法为事件视觉领域提供了重要贡献。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [10] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出了一种名为ArConv的新型卷积层，通过重新设计和优化卷积层来创建轻量级神经网络模型，该模型仅含130万参数，在RfMiD数据集上表现优于MobileNetV2，准确率达到0.9328。


<details>
  <summary>Details</summary>
Motivation: 提高深度神经网络的可访问性，使其能够在移动设备上运行，特别是在眼科疾病诊断领域，通过降低计算复杂度来扩大神经网络的应用范围。

Method: 在基础层面重新设计和优化卷积层，开发了新型ArConv卷积层，构建了一个轻量级的通用模型，参数数量仅为130万。

Result: 在RfMiD数据集上的测试结果显示，该模型准确率达到0.9328，优于拥有220万参数的MobileNetV2模型（准确率0.9266），同时具有更低的计算复杂度。

Conclusion: 通过优化卷积层设计，成功开发了一个轻量级且高效的神经网络模型，在保持高精度的同时显著降低了计算复杂度，适合在移动设备上部署用于眼科疾病诊断。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [11] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 提出基于手绘螺旋和波浪图像的机器学习方法，使用CNN、迁移学习和注意力机制来检测帕金森病，通过数据增强和集成投票达到93.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断很重要，但传统方法繁琐昂贵，需要非侵入性、成本效益高的解决方案。

Method: 使用预训练CNN、自定义卷积层和集成投票的三阶段架构，结合数据增强和注意力机制来提升模型性能。

Result: 螺旋图像精度90%，波浪图像精度96.67%，集成投票后总体准确率达到93.3%。

Conclusion: 机器学习方法在帕金森病早期诊断中具有潜力，提供了一种非侵入性且成本效益高的解决方案。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究提出了一种基于变分自编码器(VAE)的新方法，用于检测陆地碳循环中的极端事件，并与传统的奇异谱分析(SSA)方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 气候异常对陆地碳循环动态有显著影响，需要稳健的方法来检测和分析植物生产力的异常行为。

Method: 使用变分自编码器(VAE)架构，包含三个密集层和潜在空间，输入序列长度为12个月，通过重构误差识别GPP异常。与SSA方法在三个时间段进行比较。

Result: VAE和SSA方法在极端事件频率的空间模式上表现出强烈的一致性，但VAE产生更高的阈值。两种方法都显示到2050-80年，北美西部和中部的负碳循环极端事件幅度和频率增加。

Conclusion: VAE方法在性能上与SSA相当，但具有计算优势，能更好地捕捉碳循环变率中的非线性时间依赖性，且无需预先定义数据的周期性。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [13] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 论文揭示了概念擦除技术只是制造了"遗忘"的假象，实际上是通过偏置采样轨迹来避开目标概念，这种擦除是可逆的。作者提出了RevAm框架，通过RL优化轨迹来复活被擦除的概念，无需修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 随着模型架构演进到Flux等新一代架构，现有的概念擦除方法（如ESD、UCE、AC）效果下降。研究发现这些方法并非真正删除概念，而是通过偏置采样轨迹来避开目标概念，这种擦除是可逆的。这促使需要区分表面安全性和真正的概念移除。

Method: 提出了RevAm框架，基于RL的轨迹优化方法，通过动态引导去噪过程来复活被擦除的概念，而不修改模型权重。该方法将Group Relative Policy Optimization (GRPO)适应到扩散模型中，通过轨迹级奖励探索多样化的恢复轨迹。

Result: 实验表明RevAm实现了优越的概念复活保真度，同时将计算时间减少了10倍，暴露了当前安全机制的关键漏洞。

Conclusion: 当前基于轨迹操作的安全机制存在根本性脆弱性，需要开发超越轨迹操作的更鲁棒的擦除技术。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [14] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出基于神经网络的方法，可在约10毫秒内从正弦信号中重建数百赫兹频率，相比传统傅里叶方法精度提高2倍，并引入自动分类框架识别物理干扰。


<details>
  <summary>Details</summary>
Motivation: 环形激光陀螺仪等设备需要从正弦信号中快速重建频率，传统方法需要数秒数据，无法满足快速触发需求。

Method: 使用神经网络方法进行频率重建，结合自动分类框架识别信号中的物理干扰（如激光不稳定性和地震事件）。

Result: 在10毫秒内重建数百赫兹频率，频率估计精度比标准傅里叶技术提高2倍；物理干扰分类在独立测试数据集上达到99%-100%的准确率。

Conclusion: 该方法在信号分析中整合人工智能，为地球物理应用提供了重要进展，实现了快速频率估计和自动干扰识别。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [15] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 该研究开发了一个基于多目标强化学习和随机微分方程模拟器的框架，用于平衡疾病控制与社会经济稳定性的疫情防控策略评估。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了需要在疾病控制与社会经济稳定之间取得平衡的干预策略需求，传统方法难以同时优化这些相互竞争的目标。

Method: 结合多目标强化学习(MORL)和新的随机微分方程(SDE)疫情模拟器，训练Pareto-Conditioned Network(PCN)代理来发现最优干预策略。

Result: 该模拟器比传统RL方法具有更高的保真度，能够量化COVID-19防控与经济稳定之间的权衡，并适用于麻疹、脊髓灰质炎和流感等其他病原体。

Conclusion: 该框架为公共卫生危机缓解提供了稳健、适应性强的工具，支持透明、基于证据的政策制定，特别是在疫苗接种覆盖率下降等现实挑战中。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [16] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 本文提出了广义数量级(GOOMs)方法，通过扩展传统数量级概念来支持浮点数，实现比传统方法更大动态范围的稳定数值计算，结合并行前缀扫描在GPU等硬件上高效执行。


<details>
  <summary>Details</summary>
Motivation: 深度学习、金融等领域需要在长序列上进行实数复合运算，传统方法容易导致灾难性的数值下溢或上溢问题，需要一种能够支持更大动态范围的稳定计算方法。

Method: 引入广义数量级(GOOMs)概念，扩展传统数量级以包含浮点数作为特例；实现高效的自定义并行前缀扫描算法，支持在GPU等并行硬件上原生执行。

Result: GOOMs方法在三个代表性实验中表现优异：1) 超越标准浮点数限制的实数矩阵乘积复合；2) 并行估计李雅普诺夫指数谱，速度比先前方法快几个数量级；3) 在深度循环神经网络中捕获长程依赖关系，无需任何稳定化处理。

Conclusion: GOOMs结合高效并行扫描为高动态范围应用提供了可扩展且数值鲁棒的替代方案，优于传统浮点数方法。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [17] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: 提出了CrossLag注意力机制，通过整合外生数据中的滞后内生信号来改进登革热疫情预测，特别是在预测重大疫情爆发方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的登革热预测模型难以准确预测重大疫情爆发，而这些正是最需要及时公共卫生预警的情况。

Method: 开发了CrossLag注意力机制，能够在低参数量的情况下将外生数据中滞后内生信号整合到transformer架构中，利用疫情爆发通常滞后于气候和海洋异常变化的特点。

Result: 在24周预测窗口内，该模型在新加坡登革热数据上显著优于TimeXer基线模型，特别是在检测和预测重大疫情爆发方面。

Conclusion: CrossLag通过有效整合滞后环境信号，显著提升了重大登革热疫情爆发的预测能力，为公共卫生预警提供了更可靠的工具。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [18] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统评估了不同图神经网络架构在配电网故障检测中的性能，发现RGATv2模型具有最佳泛化能力，在不同拓扑设置下F1分数仅下降约12%，而纯RNN模型性能下降高达60%。


<details>
  <summary>Details</summary>
Motivation: 配电网故障检测对系统可靠性至关重要，但现有方法需要适应因重构、设备故障和分布式能源集成而不断变化的电网拓扑。当前最先进的RNN+GNN方法主要使用图卷积网络(GCN)，但其他更先进的GNN架构尚未在此领域得到充分探索。

Method: 在RNN+GNN流水线设置中系统评估多种GNN架构，首次将GraphSAGE和Graph Attention(GAT、GATv2)用于故障诊断，并与早期提出的RGCN解决方案以及纯RNN模型(特别是GRU)进行全面基准测试，特别关注模型在不同训练设置下的泛化能力。

Result: 在IEEE 123节点配电网上的实验结果显示，RGATv2具有优越的泛化能力，在不同拓扑设置下保持高性能，F1分数仅下降约12%。相比之下，纯RNN模型基本失效，F1分数下降高达60%，其他RGNN变体也表现出显著性能下降，F1分数下降高达25%。

Conclusion: RGATv2模型在配电网故障检测中展现出卓越的泛化性能，能够有效适应电网拓扑变化，为实际部署提供了可靠解决方案。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [19] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出SAFA-SNN方法，用于设备端少样本类增量学习，通过稀疏感知和快速自适应机制解决灾难性遗忘问题，在多个数据集上表现优于基线方法且能耗更低。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要持续学习新类别以保护数据隐私并在动态环境中保持可靠性能，但现有基于人工神经网络的方法受限于设备资源，而脉冲神经网络具有更低能耗和更好的生物合理性。

Method: 提出稀疏条件神经元动态机制，大部分神经元保持稳定而部分保持活跃；使用零阶优化处理脉冲不可微问题；通过子空间投影增强新类别的可区分性。

Result: 在CIFAR100、Mini-ImageNet和三个神经形态数据集上的实验表明，SAFA-SNN优于基线方法，在Mini-ImageNet最后增量会话中至少提升4.01%，能耗比基线方法低20%。

Conclusion: SAFA-SNN为设备端少样本类增量学习提供了有效的解决方案，在性能和能耗方面均优于现有方法。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [20] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 开发了一个24-48小时雷暴停电早期预警模型，使用公开数据源和两阶段模型设计（逻辑门+LSTM回归器），在密歇根州夏季雷暴相关停电预测中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 雷暴驱动的停电难以预测，因为大多数风暴不会造成损害，对流过程快速且混乱，且可用的公共数据既嘈杂又不完整。

Method: 使用公开数据（EAGLE-I停电数据、METAR天气数据），通过参数特定克里金插值和目标过采样保留极端值，构建因果时空特征，采用两阶段模型设计（逻辑门+LSTM回归器）。

Result: 两阶段模型在所有时间窗口检测到更多参考峰值（如±48小时记录3/4 vs 2/4），在峰值附近显示适度幅度增益（±0-12小时cMASE降低2-3%），但总体误差与单步LSTM基线相当。

Conclusion: 尽管公开数据存在噪声，但特征驱动的流水线为雷暴停电提供了可操作的、以事件为中心的早期预警。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [21] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 本文研究了在强化学习中当基础模型无法采样到正确解时出现的零奖励障碍问题，发现现有方法无法克服这一障碍，但通过简单的数据干预（在训练集中添加简单样本）可以解决原始困难任务。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习在复杂推理任务中面临的零奖励障碍问题，即当基础模型从未采样到正确解时，训练会因零梯度而停滞。

Method: 使用图搜索任务评估现有方法（包括密集奖励、多样性激励和改进的信用分配），并测试通过向训练集添加简单样本的数据干预方法。

Result: 实验表明现有方法无法克服零奖励障碍，但数据干预方法能够使模型最终解决原始困难任务，且无需修改RL算法本身。

Conclusion: 数据干预是克服零奖励障碍的有效方法，为强化学习在复杂推理任务中的应用提供了新思路。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [22] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过将上下文视为演化的剧本，通过生成、反思和策展的模块化过程来积累、优化和组织策略，解决了现有方法中的简洁性偏见和上下文崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM上下文适应方法存在简洁性偏见（丢失领域洞察）和上下文崩溃（迭代重写导致细节丢失）的问题，需要一种能够保持详细知识并随长上下文模型扩展的解决方案。

Method: ACE框架采用结构化、增量式更新方法，通过生成、反思和策展三个模块化过程来管理和优化上下文，支持离线和在线上下文优化。

Result: 在代理和领域特定基准测试中，ACE比强基线表现更好：代理任务提升10.6%，金融任务提升8.6%，同时显著降低适应延迟和部署成本。在AppWorld排行榜上，ACE与顶级生产级代理在整体平均分上持平，在更难的测试挑战分割上超越对手。

Conclusion: 全面、演化的上下文能够实现可扩展、高效且自我改进的LLM系统，且开销较低。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [23] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出SFP（时空预测即规划）新范式，基于模型强化学习解决时空预测中的随机性和不可微度量问题，通过生成世界模型模拟未来状态，使用波束搜索规划算法寻找高回报序列，并通过自训练优化预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决物理时空预测中固有的随机性和不可微度量挑战，传统方法难以同时处理不确定性和优化领域特定指标。

Method: 构建生成世界模型模拟多样化高保真未来状态，基础预测模型作为智能体，使用波束搜索规划算法以不可微领域度量作为奖励信号，通过自训练迭代优化策略。

Result: 显著减少预测误差，在捕获极端事件等关键领域度量上表现出色。

Conclusion: SFP为时空预测提供了有效的新框架，成功整合了生成建模、规划算法和自训练机制，解决了传统方法的局限性。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [24] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 提出了一种名为变分扩散遗忘(VDU)的机器学习遗忘方法，能够在数据受限环境下从预训练扩散模型中移除不良特征的生成能力，仅需访问包含不良特征的训练数据子集。


<details>
  <summary>Details</summary>
Motivation: 为了负责任地部署扩散模型，需要调节其生成输出，防止产生不良、暴力和淫秽内容。现有方法在数据受限环境下效果不佳，因此需要开发仅需部分训练数据的遗忘方法。

Method: 基于变分推断框架，构建包含塑性诱导器和稳定性正则器的损失函数。塑性诱导器降低不良训练数据的对数似然，稳定性正则器在参数空间正则化模型以防止图像生成质量下降。

Result: 通过综合实验验证了方法在类别遗忘和特征遗忘任务中的有效性，成功从预训练DDPM模型中移除MNIST、CIFAR-10和tinyImageNet的指定类别，以及从Stable Diffusion中移除特定高级特征。

Conclusion: VDU是一种计算高效的方法，能够在数据受限环境下有效防止扩散模型生成包含不良特征的输出，同时保持图像生成质量。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [25] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 提出MLLMEraser，一种基于激活导向的训练免费多模态大语言模型遗忘框架，通过对比知识回忆和知识遗忘的图像-文本对构建多模态遗忘方向，实现动态知识擦除而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在部署过程中面临记忆隐私数据、过时知识和有害内容的问题，现有基于训练的遗忘方法计算成本高、不可逆且容易扭曲保留知识。

Method: 利用激活导向技术，通过对比对抗性扰动的知识回忆和知识遗忘图像-文本对构建多模态遗忘方向，并设计输入感知的导向机制自适应决定何时及如何应用遗忘方向。

Result: 在LLaVA-1.5和Qwen-2.5-VL上的实验表明，MLLMEraser在遗忘性能上优于现有最先进的多模态大语言模型遗忘基线，计算成本更低且效用退化最小。

Conclusion: MLLMEraser提供了一种高效、可扩展的多模态大语言模型遗忘解决方案，能够在保持保留知识效用的同时有效擦除指定内容。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [26] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出RegCache训练免费算法，通过添加前缀token来缓解视觉编码器中的异常值问题，实现更精确的量化


<details>
  <summary>Details</summary>
Motivation: 视觉编码器在实时处理大规模视觉数据时需要降低推理成本，但后训练量化在8位精度下仍因大规模激活异常值而具有挑战性

Method: 引入异常值易发但语义无意义的前缀token到目标视觉编码器，防止其他token产生异常值，采用中间层前缀和token删除技术

Result: 实验表明该方法在文本监督和自监督视觉编码器中一致提高了量化模型的准确性

Conclusion: RegCache能有效缓解视觉编码器中的异常值问题，实现更精确的量化

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [27] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 本文提出了一个评估AI模型在罕见事件识别中性能的框架，重点关注低流行率场景下的关键考虑因素，包括问题定义、测试集设计、统计评估和人类工作流整合。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的AI应用针对低流行率事件，表面上的准确性可能掩盖了有限的现实价值，需要专门的评估方法来确保AI模型在罕见事件识别中的有效性。

Method: 提出了结构化案例级检查(SCLE)方法，开发了全面的检查清单来指导AI模型的采购或开发，并在药物警戒领域通过三个研究实例化该框架：基于规则的检索、机器学习与概率记录链接的结合、使用LLM的自动化编辑。

Result: 识别了罕见事件设置中的特定陷阱，包括不切实际的类别平衡导致的乐观估计和测试集中缺乏困难的阳性对照，并展示了成本敏感目标如何使模型性能与操作价值保持一致。

Conclusion: 虽然基于药物警戒实践，但这些原则可推广到阳性样本稀缺且错误成本可能不对称的领域，为罕见事件识别中的AI模型评估提供了通用框架。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [28] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出课程混沌预测(CCF)训练范式，通过从简单周期性系统到复杂混沌系统的渐进式训练，提高混沌系统预测的泛化能力和预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在混沌系统预测中的两个对立问题：过度专注于单一混沌系统导致泛化能力差，或混合大量不相关时间序列导致无法学习特定动态机制。

Method: 基于动力学系统理论组织训练数据，使用最大Lyapun夫指数和吸引子维度量化复杂性，构建从简单到复杂的训练课程。收集50多个合成ODE/PDE系统构建课程库。

Result: 在太阳黑子数、电力需求和人类ECG信号等真实世界基准测试中，CCF将有效预测范围延长达40%，相比仅使用真实数据训练提升超过两倍。

Conclusion: CCF训练范式能显著提升混沌系统预测性能，在不同神经网络架构中表现一致，课程结构对性能提升至关重要。

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [29] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: 提出Causal Sphere Hypergraph Transformer (CSHT)，一种结合Granger因果超图结构、黎曼几何和因果掩码Transformer注意力的可解释金融时间序列预测架构。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个既能准确预测金融时间序列，又能提供透明归因路径的可解释模型，特别是在市场不确定性下实现可信赖的金融预测。

Method: 通过提取多变量Granger因果依赖关系，将其编码为超球面上的有向超边，并使用保持时间方向性和几何一致性的角度掩码来约束注意力机制。

Result: 在2018-2023年标普500数据（包括2020年COVID-19冲击）上的评估显示，CSHT在回报预测、制度分类和顶级资产排名任务中持续优于基线模型。

Conclusion: CSHT通过强制预测因果结构和在黎曼流形中嵌入变量，实现了跨市场制度的稳健泛化和从宏观经济事件到股票层面响应的透明归因路径，是处理不确定性下金融预测的有原则且实用的解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [30] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出动态加权损失函数来解决稀疏领域推荐问题，通过自适应算法根据领域稀疏度调整损失权重，在稀疏领域显著提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 传统单模型序列推荐架构在处理稀疏或小众领域的"重度用户"时效果有限，固定加权损失方法无法有效处理交互极少的领域，训练信号容易被通用数据集稀释

Method: 提出数据驱动的动态加权损失函数，基于训练数据中每个领域的稀疏度自适应调整损失权重 - 稀疏领域分配更高权重，密集领域分配较低权重

Result: 在四个多样化数据集上的实验表明，该方法显著优于所有基线方法，特别是在稀疏领域，Recall@10和NDCG@10指标大幅提升，同时在密集领域保持性能且计算开销极小

Conclusion: 动态加权损失函数通过理论分析和实证验证，有效解决了稀疏领域推荐问题，为处理不同稀疏度领域的推荐系统提供了稳定高效的解决方案

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [31] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: 本文揭示了时间序列归一化策略中的理论矛盾，发现标准RevIN在极端异常值数据集上会灾难性失败，而简单的R²-IN却意外成为最佳方案，自适应模型A-IN则完全失败。


<details>
  <summary>Details</summary>
Motivation: 研究RevIN归一化技术的鲁棒性改进，探索用鲁棒统计量替代非鲁棒统计量(R²-IN)的可行性，以及理解不同归一化策略性能差异的深层原因。

Method: 通过理论分析识别四种归一化策略的理论矛盾，进行实验比较标准RevIN、R²-IN和自适应模型A-IN在不同数据集上的性能表现。

Result: 标准RevIN在极端异常值数据集上MSE激增683%，R²-IN意外成为最佳方案，而自适应模型A-IN出现系统性完全失败。

Conclusion: 提出了时间序列归一化的新警示范式：从盲目追求复杂性转向诊断驱动分析，揭示了简单基线的惊人能力和朴素自适应策略的危险性。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [32] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: 提出接种提示法（IP），通过在训练提示中明确要求不希望出现的行为来防止模型学习这些不良行为，如奖励黑客攻击和奉承行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有时在训练中受到不完善的监督信号影响，导致奖励黑客攻击和奉承等不良行为。改进监督质量成本高昂或不可行，需要开发能在不完善训练信号下改善学习行为的方法。

Method: 使用接种提示法（IP），在监督微调期间修改训练提示，明确要求不希望出现的行为。例如，为防止奖励黑客攻击，修改提示要求代码只能在提供的测试用例上工作但在其他输入上失败。

Result: 在四个实验设置中，IP显著减少了不良行为的学习，同时没有大幅降低期望能力的学习。更强的接种提示在微调前更能引发不良行为，在训练中更有效地防止这些行为。

Conclusion: IP是一种简单有效的方法，可以控制模型从微调中的泛化方式，防止学习不良行为而不显著干扰期望能力。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 扩散大语言模型(dLLMs)在指令调优后存在<eos>溢出问题：随着序列长度增加，响应反而变短，提前终止或退化为<eos>标记流。作者提出彩虹填充方法，用循环的不同填充标记替代重复<eos>，有效解决了这一问题。


<details>
  <summary>Details</summary>
Motivation: 指令调优的扩散大语言模型存在<eos>溢出漏洞，即随着分配的序列长度增加，模型响应反而变短，提前终止或退化为<eos>标记流。这个问题在实践中已被注意到但未系统分析。

Method: 提出彩虹填充方法，用循环的不同填充标记替代重复的<eos>占位符，分散概率质量，打破<eos>的主导地位。该方法可高效集成到现有指令调优模型中，仅需少量数据和单个epoch的LoRA微调。

Result: 彩虹填充显著提高了长度鲁棒性和输出质量，仅需七个填充标记即可防止提前终止。该方法在现有指令调优模型上集成效率高，单epoch LoRA微调即可获得显著改进。

Conclusion: 彩虹填充是一种简单有效的解决方案，解决了扩散大语言模型中的<eos>溢出问题，提高了模型的长度鲁棒性和输出质量，具有很高的实用性。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [34] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了一个名为Zephyrus的智能天气科学代理框架，通过结合大型语言模型和天气数据工具，实现了多轮对话式的天气数据分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的天气基础模型缺乏语言推理能力，而大语言模型无法处理高维气象数据，需要桥接这一技术鸿沟。

Method: 构建了ZephyrusWorld环境，包含WeatherBench 2数据集接口、地理查询、天气预报和气候模拟等工具，设计了多轮LLM天气代理Zephyrus。

Result: 在ZephyrusBench基准测试中，Zephyrus代理比纯文本基线模型正确率提高了35个百分点，但在更困难任务上表现相似。

Conclusion: 该框架成功整合了语言模型和天气科学工具，但更具挑战性的任务仍需要进一步改进。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [35] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出了MedLog协议，用于临床AI的事件级日志记录，类似于系统日志(syslog)在计算机系统中的作用，旨在解决医疗AI缺乏标准化使用记录的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗领域快速增长的临床AI堆栈缺乏像syslog那样的标准日志协议，导致难以追踪AI模型的使用情况、测量真实性能、检测不良事件或纠正偏差。

Method: 设计了MedLog协议，包含9个核心字段：header、model、user、target、inputs、artifacts、outputs、outcomes和feedback，支持风险采样、生命周期感知保留策略和写后缓存。

Result: MedLog能够为临床AI使用提供结构化和一致的记录，支持复杂工作流的详细追踪，并促进新数据库和软件的发展。

Conclusion: MedLog可以实现医疗AI的持续监控、审计和迭代改进，为新型数字流行病学奠定基础。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [36] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 提出了Moral Anchor System (MAS)框架，用于检测、预测和缓解AI代理中的价值漂移问题，结合贝叶斯推理、LSTM网络预测和人类中心治理层，实现低延迟响应和高精度检测。


<details>
  <summary>Details</summary>
Motivation: 随着AI作为超级助手的兴起，确保AI行为与人类伦理和意图保持一致至关重要。价值漂移风险可能导致效率低下或伦理违规，需要有效解决方案。

Method: MAS框架结合实时贝叶斯推理监控价值状态、LSTM网络预测漂移趋势，以及人类中心治理层进行自适应干预。通过监督微调减少误报和警报疲劳。

Result: 在模拟实验中，MAS能够将价值漂移事件减少80%以上，保持85%的高检测准确率和0.08的低误报率，验证了系统的可扩展性和响应性。

Conclusion: MAS通过概率漂移检测、预测分析和自适应治理的集成，提供了比静态对齐方法更有效的价值对齐解决方案，具有跨领域适用性。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: RxT是一种新型Transformer架构，通过事件驱动范式解决传统Transformer在对话AI中的状态保持和计算复杂度问题，将对话成本从二次方降低到线性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在对话应用中存在状态保持困难和二次方计算复杂度问题，导致长对话成本高昂、延迟严重。

Method: 采用事件驱动范式，将每个对话轮次作为离散事件处理，使用固定大小的短期记忆系统，通过生成器-解码器和内存注意力网络异步更新记忆。

Result: 在合成数据上的概念验证实验显示，RxT相比基线模型具有更优性能和恒定时间推理延迟。

Conclusion: RxT实现了低延迟、有状态且经济可行的长对话，从根本上改变了对话系统的扩展动态。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [38] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了在EvalLLM 2025挑战赛中针对法语生物医学命名实体识别和健康事件提取的三种方法，主要基于大语言模型的上下文学习、GLiNER系统微调和LLaMA-3.1微调，结果显示GPT-4.1在少样本设置下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决法语生物医学领域在少样本设置下的命名实体识别和健康事件提取问题，探索大语言模型在低资源场景下的应用潜力。

Method: 提出三种方法：(1) GPT-4.1上下文学习，包含自动选择10个示例和标注指南摘要；(2) GLiNER系统在合成语料上微调，后经LLM验证；(3) LLaMA-3.1-8B在合成语料上微调。事件提取采用相同的上下文学习策略。

Result: GPT-4.1表现最佳，NER的宏F1为61.53%，事件提取为15.02%，表明精心设计的提示在低资源场景中至关重要。

Conclusion: 在法语生物医学NER和事件提取的少样本设置中，GPT-4.1的上下文学习方法效果最好，强调了提示工程在低资源环境中的重要性。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [39] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G是一个解码框架，通过将格式遵循与任务解决解耦，使用单独的概率模型处理格式合规性，从而提升LLM在复杂指令下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着提示变得越来越复杂，LLM往往难以同时遵循所有指令，特别是当推理指令与严格的格式要求交织在一起时，这种纠缠会为模型创造竞争目标。

Method: 引入Deco-G解码框架，使用可处理的概率模型(TPM)处理格式合规性，同时仅用任务指令提示LLM。通过指令感知蒸馏、灵活的trie构建算法和HMM状态剪枝实现高效计算。

Result: 在数学推理、LLM-as-a-judge和事件参数提取等多样化格式要求的任务中，Deco-G相比常规提示方法实现了1.0%到6.0%的相对性能提升，并保证格式合规性。

Conclusion: 通过明确分离格式遵循和任务解决，Deco-G框架能有效提升LLM在复杂指令下的性能，同时确保格式合规性。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [40] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: Step Pruner (SP) 是一个强化学习框架，通过惩罚冗余推理步骤来减少大型推理模型的过度思考问题，在保持准确性的同时显著降低响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通常通过惩罚生成token来促进简洁性，但存在两个问题：更少的token不一定对应更少的推理步骤，模型可能在训练后期通过丢弃推理步骤来最小化token使用。

Method: 提出Step Pruner (SP)框架，使用步骤感知的奖励函数，在保证正确性的同时惩罚冗余步骤，对错误响应不给予奖励以防止强化错误推理。还提出动态停止机制，当任何输出步骤长度超过上限时停止更新。

Result: 在四个推理基准测试上的广泛实验表明，SP在达到最先进准确性的同时显著减少响应长度。在AIME24上，SP减少了69.7%的token使用。

Conclusion: Step Pruner框架有效解决了大型推理模型的过度思考问题，通过关注推理步骤而非单纯token数量，实现了准确性和效率的平衡。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [41] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 该研究提出了首个模拟框架来评估LLM在长期交互中的欺骗行为，发现欺骗行为具有模型依赖性，会随压力增加，并持续削弱监督者信任。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多局限于单轮提示下的LLM欺骗评估，无法捕捉欺骗策略在长期交互中的发展过程，需要开发能模拟真实世界信任敏感场景的评估框架。

Method: 构建多智能体系统：执行者智能体完成任务，监督者智能体评估进展并提供反馈，独立欺骗审计员审查完整轨迹以识别欺骗行为。在11个前沿模型上进行广泛实验。

Result: 欺骗行为具有模型依赖性，随事件压力增加而增加，并持续削弱监督者信任。定性分析揭示了隐瞒、模棱两可和伪造等不同欺骗策略。

Conclusion: 欺骗是长期交互中出现的风险，该研究为评估未来LLM在真实世界信任敏感环境中的表现奠定了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [42] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: 提出了一种基于梯度的分散式框架，通过自适应势函数的自动组合来简化双手机器人装配规划，无需长时程规划即可解决长时程任务。


<details>
  <summary>Details</summary>
Motivation: 双手机器人装配面临高难度任务排序、多机器人协调和接触密集型操作等挑战。传统任务与运动规划方法在需要快速重规划时收敛缓慢，且显式定义任务序列限制了灵活性。

Method: 使用分段连续能量函数，通过自适应势函数的自动组合构建分散式梯度优化框架，仅通过近视优化生成子目标，无需长时程规划。

Result: 该方法能够扩展到物理双手机器人装配任务，在紧密公差装配中表现出色，能够自动生成重试、协调运动和自主交接等行为。

Conclusion: 梯度快速重规划框架能够以涌现方式处理装配中的动态不确定性，为紧密公差装配提供了有效的解决方案。

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>
