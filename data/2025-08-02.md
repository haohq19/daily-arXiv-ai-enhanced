<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种结合Corner Loss和R-CBAM模块的图像去雨方法，显著提升了去雨效果。


<details>
  <summary>Details</summary>
Motivation: 单图像去雨任务需要同时保留细节和整体视觉质量，现有方法容易丢失边界和纹理信息。

Method: 引入Corner Loss防止边界和纹理丢失，并在编码器和解码器中加入R-CBAM模块动态调整特征重要性。

Result: 在Rain100L和Rain100H数据集上PSNR分别达到33.29 dB和26.16 dB，优于现有方法。

Conclusion: 该方法有效提升了去雨效果，尤其在保留细节和边界方面表现突出。

Abstract: The problem of single-image rain streak removal goes beyond simple noise
suppression, requiring the simultaneous preservation of fine structural details
and overall visual quality. In this study, we propose a novel image restoration
network that effectively constrains the restoration process by introducing a
Corner Loss, which prevents the loss of object boundaries and detailed texture
information during restoration. Furthermore, we propose a Residual
Convolutional Block Attention Module (R-CBAM) Block into the encoder and
decoder to dynamically adjust the importance of features in both spatial and
channel dimensions, enabling the network to focus more effectively on regions
heavily affected by rain streaks. Quantitative evaluations conducted on the
Rain100L and Rain100H datasets demonstrate that the proposed method
significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on
Rain100L and 26.16 dB on Rain100H.

</details>


### [2] [FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks](https://arxiv.org/abs/2507.23643)
*Changqing Xu,Ziqiang Yang,Yi Liu,Xinfang Liao,Guiqi Mo,Hao Zeng,Yintang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于Forward-Forward（FF）的无梯度近似训练框架，用于高效训练SNN，并通过动态优化损失函数提升性能。


<details>
  <summary>Details</summary>
Motivation: SNN训练因非可微性和高计算需求而困难，现有方法牺牲精度且难以部署在边缘设备上。

Method: 采用FF框架，将脉冲激活视为黑盒模块，无需梯度近似；引入类感知复杂度适应机制动态优化损失函数。

Result: 在MNIST、Fashion-MNIST和CIFAR-10上分别达到99.58%、92.13%和75.64%的测试准确率，优于现有FF方法。

Conclusion: 该方法显著降低了计算复杂度和内存访问需求，为SNN的高效训练提供了新思路。

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible framework for
energy-efficient neuromorphic computing. However, it is a challenge to train
SNNs due to their non-differentiability, efficiently. Existing gradient
approximation approaches frequently sacrifice accuracy and face deployment
limitations on edge devices due to the substantial computational requirements
of backpropagation. To address these challenges, we propose a Forward-Forward
(FF) based gradient approximation-free training framework for Spiking Neural
Networks, which treats spiking activations as black-box modules, thereby
eliminating the need for gradient approximation while significantly reducing
computational complexity. Furthermore, we introduce a class-aware complexity
adaptation mechanism that dynamically optimizes the loss function based on
inter-class difficulty metrics, enabling efficient allocation of network
resources across different categories. Experimental results demonstrate that
our proposed training framework achieves test accuracies of 99.58%, 92.13%, and
75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,
surpassing all existing FF-based SNN approaches. Additionally, our proposed
method exhibits significant advantages in terms of memory access and
computational power consumption.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations](https://arxiv.org/abs/2507.22962)
*Boyuan Zheng,Victor W. Chu*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与可解释AI的多灾害预警框架，用于农业气候风险管理，实验验证了其高预测准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统单灾害预警方法无法捕捉复杂气候事件交互，需智能学习近期气候行为以应对气候变化。

Method: 结合序列深度学习模型（如BiLSTM）与可解释AI技术（TimeSHAP），构建多灾害预测框架，并针对不同区域定制模型。

Result: 实验显示框架预测准确性高，BiLSTM表现突出，能提供详细时间解释，支持主动风险管理。

Conclusion: 该研究提升了多灾害预警系统的可解释性和实用性，促进农业气候风险管理的跨学科信任与决策。

Abstract: Climate extremes present escalating risks to agriculture intensifying the
need for reliable multi-hazard early warning systems (EWS). The situation is
evolving due to climate change and hence such systems should have the
intelligent to continue to learn from recent climate behaviours. However,
traditional single-hazard forecasting methods fall short in capturing complex
interactions among concurrent climatic events. To address this deficiency, in
this paper, we combine sequential deep learning models and advanced Explainable
Artificial Intelligence (XAI) techniques to introduce a multi-hazard
forecasting framework for agriculture. In our experiments, we utilize
meteorological data from four prominent agricultural regions in the United
States (between 2010 and 2023) to validate the predictive accuracy of our
framework on multiple severe event types, which are extreme cold, floods,
frost, hail, heatwaves, and heavy rainfall, with tailored models for each area.
The framework uniquely integrates attention mechanisms with TimeSHAP (a
recurrent XAI explainer for time series) to provide comprehensive temporal
explanations revealing not only which climatic features are influential but
precisely when their impacts occur. Our results demonstrate strong predictive
accuracy, particularly with the BiLSTM architecture, and highlight the system's
capacity to inform nuanced, proactive risk management strategies. This research
significantly advances the explainability and applicability of multi-hazard
EWS, fostering interdisciplinary trust and effective decision-making process
for climate risk management in the agricultural industry.

</details>


### [4] [Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation](https://arxiv.org/abs/2507.23000)
*Shengao Yi,Xiaojiang Li,Wei Tu,Tianhong Zhao*

Main category: cs.LG

TL;DR: GSM-UTCI是一种多模态深度学习框架，用于预测高分辨率UTCI，结合地表形态和气象数据，显著提升计算效率，为城市气候适应提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 随着极端高温事件加剧，城市需要高效工具评估和缓解热压力，传统物理模型计算成本高，难以大规模应用。

Method: 提出GSM-UTCI框架，融合地表形态、高分辨率土地覆盖数据和气象条件，采用FiLM架构动态调整空间特征。

Result: 模型在R2和MAE上表现优异（0.9151和0.41°C），计算时间从小时级降至分钟级，应用于费城绿化场景显示显著降温效果。

Conclusion: GSM-UTCI为城市气候适应提供了高效、精细化的决策工具，支持多样化绿化策略的评估。

Abstract: As extreme heat events intensify due to climate change and urbanization,
cities face increasing challenges in mitigating outdoor heat stress. While
traditional physical models such as SOLWEIG and ENVI-met provide detailed
assessments of human-perceived heat exposure, their computational demands limit
scalability for city-wide planning. In this study, we propose GSM-UTCI, a
multimodal deep learning framework designed to predict daytime average
Universal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The
model fuses surface morphology (nDSM), high-resolution land cover data, and
hourly meteorological conditions using a feature-wise linear modulation (FiLM)
architecture that dynamically conditions spatial features on atmospheric
context. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical
accuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\deg}C,
while reducing inference time from hours to under five minutes for an entire
city. To demonstrate its planning relevance, we apply GSM-UTCI to simulate
systematic landscape transformation scenarios in Philadelphia, replacing bare
earth, grass, and impervious surfaces with tree canopy. Results show spatially
heterogeneous but consistently strong cooling effects, with impervious-to-tree
conversion producing the highest aggregated benefit (-4.18{\deg}C average
change in UTCI across 270.7 km2). Tract-level bivariate analysis further
reveals strong alignment between thermal reduction potential and land cover
proportions. These findings underscore the utility of GSM-UTCI as a scalable,
fine-grained decision support tool for urban climate adaptation, enabling
scenario-based evaluation of greening strategies across diverse urban
environments.

</details>


### [5] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: KLLM是一个硬件-软件协同设计框架，通过K-Means量化和高效的异常值检测，显著提升大语言模型推理的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理面临内存和计算的高需求挑战，现有量化方法在低精度下存在准确率下降或计算效率低的问题。

Method: 提出KLLM框架，结合K-Means量化的索引计算方案和在线异常值检测引擎Orizuru，避免解量化和全精度计算。

Result: 实验显示，KLLM在A100 GPU和Atom上分别实现9.67x和7.03x的加速，能效提升229.50x和150.21x。

Conclusion: KLLM通过协同设计解决了量化中的关键问题，显著提升了推理效率和能效。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [6] [Linking Actor Behavior to Process Performance Over Time](https://arxiv.org/abs/2507.23037)
*Aurélie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 论文提出了一种结合行为分析和Granger因果性的方法，用于分析时间序列数据中个体行为对流程结果的影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常使用静态和聚合数据，忽略了行为的时间动态和因果关系，无法准确反映现实流程的复杂性。

Method: 整合行为分析和Granger因果性，构建时间序列数据，并使用Group Lasso进行滞后选择。

Result: 发现行为对流程性能（如吞吐时间）有直接且可测量的影响。

Conclusion: 基于时间序列的行为分析方法能更细致地揭示个体行为如何影响整体流程效率。

Abstract: Understanding how actor behavior influences process outcomes is a critical
aspect of process mining. Traditional approaches often use aggregate and static
process data, overlooking the temporal and causal dynamics that arise from
individual actor behavior. This limits the ability to accurately capture the
complexity of real-world processes, where individual actor behavior and
interactions between actors significantly shape performance. In this work, we
address this gap by integrating actor behavior analysis with Granger causality
to identify correlating links in time series data. We apply this approach to
realworld event logs, constructing time series for actor interactions, i.e.
continuation, interruption, and handovers, and process outcomes. Using Group
Lasso for lag selection, we identify a small but consistently influential set
of lags that capture the majority of causal influence, revealing that actor
behavior has direct and measurable impacts on process performance, particularly
throughput time. These findings demonstrate the potential of actor-centric,
time series-based methods for uncovering the temporal dependencies that drive
process outcomes, offering a more nuanced understanding of how individual
behaviors impact overall process efficiency.

</details>


### [7] [An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items](https://arxiv.org/abs/2507.23303)
*Luca Corbucci,Javier Alejandro Borges Legrottaglie,Francesco Spinnato,Anna Monreale,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 论文提出两种可解释算法，用于预测超市购物中被遗忘的商品，并在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NBP方法仅预测未来购买，未解决遗漏商品的识别问题，且缺乏透明性。

Method: 提出两种可解释算法，专门识别遗忘商品并提供直观解释。

Result: 在真实零售数据集上，算法比现有NBP基线方法性能提升10-15%。

Conclusion: 论文填补了NBP领域遗忘商品预测的空白，并提供了可解释的解决方案。

Abstract: Accurately identifying items forgotten during a supermarket visit and
providing clear, interpretable explanations for recommending them remains an
underexplored problem within the Next Basket Prediction (NBP) domain. Existing
NBP approaches typically only focus on forecasting future purchases, without
explicitly addressing the detection of unintentionally omitted items. This gap
is partly due to the scarcity of real-world datasets that allow for the
reliable estimation of forgotten items. Furthermore, most current NBP methods
rely on black-box models, which lack transparency and limit the ability to
justify recommendations to end users. In this paper, we formally introduce the
forgotten item prediction task and propose two novel interpretable-by-design
algorithms. These methods are tailored to identify forgotten items while
offering intuitive, human-understandable explanations. Experiments on a
real-world retail dataset show our algorithms outperform state-of-the-art NBP
baselines by 10-15% across multiple evaluation metrics.

</details>


### [8] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: 该论文提出了一种基于量化脉冲神经网络（SNN）的强化学习算法，用于解决经典控制任务，并在SpiNNaker2神经形态芯片上实现低功耗部署。与GPU相比，SpiNNaker2在能耗上降低了32倍，同时保持了相当的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用SNN和神经形态硬件（如SpiNNaker2）实现低功耗、低延迟的强化学习，以解决机器人任务中的能耗和实时性问题。

Method: 方法包括使用Q-learning算法训练SNN，然后对网络进行微调和8位量化，最终在SpiNNaker2芯片上部署。

Result: 结果显示，SpiNNaker2在能耗上比GPU降低了32倍，推理延迟与GPU相当，某些任务中甚至更优。

Conclusion: 结论表明，SpiNNaker2在可扩展、低能耗的神经形态计算中具有潜力，为高效的深度Q学习提供了新方向。

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 论文提出并定义了‘论证连贯性’属性，评估其在人类和LLM预测中的影响，发现过滤不连贯预测可提高准确性，但用户实验显示用户通常不遵循这一属性。


<details>
  <summary>Details</summary>
Motivation: 研究论证结构对预测的影响，提出‘论证连贯性’以提升预测准确性。

Method: 通过人类和LLM预测实验，以及用户实验，评估连贯性的影响。

Result: 过滤不连贯预测显著提高准确性，但用户未普遍遵循连贯性。

Conclusion: 需在基于论证的预测中引入机制过滤不连贯意见，以提升群体预测质量。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 论文提出两种模型预测临床试验中的严重不良事件（SAE）结果，利用注册信息提前预测，以优化试验设计。


<details>
  <summary>Details</summary>
Motivation: 通过准确预测SAE结果，避免试验终止并减少参与者风险，利用ClinicalTrials.gov的注册数据优化试验设计。

Method: 使用预训练语言模型（如ClinicalT5、BioBERT）进行特征提取，结合滑动窗口方法处理长文本，开发分类器和回归模型预测SAE结果。

Result: 最佳模型（ClinicalT5+Transformer+MLP）在预测SAE比例时AUC为77.6%，控制组SAE比例预测RMSE为18.6%。滑动窗口方法显著提升性能。

Conclusion: 利用ClinicalTrials.gov数据预测SAE结果具有潜力，可改进试验设计并发现预期与实际结果的差异。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [11] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

TL;DR: 论文提出了一种加速验证代码算法性能的方法，结合自然语言处理（NLP）和多波自适应抽样，显著减少人工审核时间并提高效率。


<details>
  <summary>Details</summary>
Motivation: 通过验证代码算法的性能，可以增强大型索赔数据库的分析可靠性，但传统方法耗时耗力。

Method: 使用NLP减少人工审核时间，并结合多波自适应抽样和预定义停止规则，以高效完成验证。

Result: NLP辅助审核减少40%时间，多波抽样避免77%的图表审核，且不影响精度。

Conclusion: 该方法可促进代码算法的常规验证，提升数据库研究结果的可靠性。

Abstract: Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [12] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
*Diego Garat,Guillermo Moncecchi,Dina Wonsever*

Main category: cs.CL

TL;DR: 本文研究了利用大型语言模型（LLMs）的上下文学习（ICL）进行框架语义解析（FSP），无需微调模型。通过自动生成任务特定提示，在FrameNet数据库的基础上，实现了竞争性的结果。


<details>
  <summary>Details</summary>
Motivation: 探索无需微调模型的方法，利用ICL和LLMs完成FSP任务，以简化流程并提高效率。

Method: 提出一种方法，自动生成针对框架识别（FI）和框架语义角色标注（FSRL）子任务的提示，基于FrameNet数据库的框架定义和标注示例。

Result: 在暴力事件相关帧的子集上，FI的F1得分为94.3%，FSRL为77.4%。

Conclusion: ICL为领域特定的FSP任务提供了一种实用且有效的替代传统微调的方法。

Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling
their arguments according to Frame Semantics. This paper investigates the use
of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP
without model fine-tuning. We propose a method that automatically generates
task-specific prompts for the Frame Identification (FI) and Frame Semantic Role
Labeling (FSRL) subtasks, relying solely on the FrameNet database. These
prompts, constructed from frame definitions and annotated examples, are used to
guide six different LLMs. Experiments are conducted on a subset of frames
related to violent events. The method achieves competitive results, with F1
scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers
a practical and effective alternative to traditional fine-tuning for
domain-specific FSP tasks.

</details>


### [13] [Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration](https://arxiv.org/abs/2507.23407)
*Ante Wang,Yujie Lin,Jingyao Liu,Suhang Wu,Hao Liu,Xinyan Xiao,Jinsong Su*

Main category: cs.CL

TL;DR: 论文提出了一种主动批判性思维范式，旨在让AI模型主动寻求缺失或澄清信息以更好地解决用户查询，并通过两个新基准GSM-MC和GSM-MCE评估其能力。实验表明，强化学习能显著提升模型在此任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统多采用被动批判性思维，仅拒绝问题查询而缺乏主动解决能力。本文旨在推动模型通过主动寻求信息来更有效地协作解决问题。

Method: 提出主动批判性思维范式，设计GSM-MC和GSM-MCE两个基准测试数学推理能力，并利用强化学习提升模型表现。

Result: 实验显示，传统模型在主动批判性思维任务上表现不佳，但强化学习显著提升了Qwen3-1.7B模型的准确率（从0.15%提升至73.98%）。

Conclusion: 通过主动批判性思维和强化学习，AI模型能更有效地与用户协作解决问题，为未来研究提供了新方向。

Abstract: Critical thinking is essential for building robust AI systems, preventing
them from blindly accepting flawed data or biased reasoning. However, prior
work has primarily focused on passive critical thinking, where models simply
reject problematic queries without taking constructive steps to address user
requests. In this work, we introduce proactive critical thinking, a paradigm
where models actively seek missing or clarifying information from users to
resolve their queries better. To evaluate this capability, we present GSM-MC
and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical
reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math
problems with a key variable deliberately removed, requiring models to identify
and request the missing information. GSM-MCE further increases the difficulty
by introducing irrelevant details to test robustness against distractions.
Experiments on Qwen3 and Llama series models show that, while these models
excel in traditional reasoning tasks due to extensive post-training and
inference-time scaling, they struggle with proactive critical thinking,
especially smaller ones. However, we demonstrate that reinforcement learning
(RL) can significantly improve this ability. Using our enhanced RL algorithm,
we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to
73.98% on GSM-MC. We hope this work advances models that collaborate more
effectively with users in problem-solving through proactive critical thinking.

</details>


### [14] [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)
*Saeed Almheiri,Yerulan Kongrat,Adrian Santosh,Ruslan Tasmukhanov,Josemaria Vera,Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: 研究探讨如何通过微调大型语言模型（LLMs）实现基于用户角色的行为控制，提出三种建模策略，并构建两个数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，LLMs的行为需根据用户角色进行控制，现有方法未解决角色特定访问限制的问题。

Method: 研究三种策略：BERT分类器、LLM分类器和角色条件生成，并构建两个数据集（基于现有指令调优和合成数据）进行评估。

Result: 评估了模型在不同组织结构和对抗攻击（如提示注入、角色不匹配和越狱）下的表现。

Conclusion: 研究表明LLMs可通过微调实现角色特定的行为控制，为实际企业应用提供可能。

Abstract: As large language models (LLMs) are increasingly deployed in enterprise
settings, controlling model behavior based on user roles becomes an essential
requirement. Existing safety methods typically assume uniform access and focus
on preventing harmful or toxic outputs, without addressing role-specific access
constraints. In this work, we investigate whether LLMs can be fine-tuned to
generate responses that reflect the access privileges associated with different
organizational roles. We explore three modeling strategies: a BERT-based
classifier, an LLM-based classifier, and role-conditioned generation. To
evaluate these approaches, we construct two complementary datasets. The first
is adapted from existing instruction-tuning corpora through clustering and role
labeling, while the second is synthetically generated to reflect realistic,
role-sensitive enterprise scenarios. We assess model performance across varying
organizational structures and analyze robustness to prompt injection, role
mismatch, and jailbreak attempts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [15] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种结合LiDAR、惯性和视觉的在线映射系统，通过全局位姿图优化和高效高斯初始化策略，解决了3D高斯溅射（3DGS）在稀疏数据和全局对齐方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机传感器（包括RGB-D）的3DGS方法存在计算负载高、在低纹理或光照环境下失效以及操作范围短的问题，而LiDAR虽然鲁棒，但其与3DGS的集成带来了全局对齐和高优化时间的挑战。

Method: 提出GSFusion系统，采用全局位姿图优化中的surfel-to-surfel约束确保地图一致性，并使用像素感知的高斯初始化策略和边界sigmoid约束处理稀疏数据。

Result: 在公开和自建数据集上的实验表明，GSFusion在渲染质量和地图构建效率上优于现有3DGS SLAM系统。

Conclusion: GSFusion通过创新的约束和初始化策略，显著提升了3DGS在LiDAR-惯性-视觉映射中的性能。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,
conventional approaches based on camera sensor, even RGB-D, suffer from
fundamental limitations such as high computational load, failure in
environments with poor texture or illumination, and short operational ranges.
LiDAR emerges as a robust alternative, but its integration with 3DGS introduces
new challenges, such as the need for exceptional global alignment for
photorealistic quality and prolonged optimization times caused by sparse data.
To address these challenges, we propose GSFusion, an online
LiDAR-Inertial-Visual mapping system that ensures high-precision map
consistency through a surfel-to-surfel constraint in the global pose-graph
optimization. To handle sparse data, our system employs a pixel-aware Gaussian
initialization strategy for efficient representation and a bounded sigmoid
constraint to prevent uncontrolled Gaussian growth. Experiments on public and
our datasets demonstrate our system outperforms existing 3DGS SLAM systems in
terms of rendering quality and map-building efficiency.

</details>


### [16] [Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications](https://arxiv.org/abs/2507.23350)
*Mahmoud Ghorab,Matthias Lorenzen*

Main category: cs.RO

TL;DR: 提出了一种结合全局Dubins旅行商问题（DTSP）路径规划与局部非线性模型预测控制（NMPC）的导航框架，用于农业环境中自主移动机器人的高效导航。


<details>
  <summary>Details</summary>
Motivation: 农业环境中自主移动机器人导航需求增长，需高效路径规划以减少行驶距离并满足曲率约束，保护土壤和植被。

Method: 采用DTSP生成最短曲率约束路径，结合NMPC进行局部路径规划与控制，优化轨迹并满足约束。

Result: 仿真验证显示，耦合DTSP的规划器路径更平滑、更短，比解耦方法减少约16%的行驶距离；NMPC控制器有效引导机器人到达目标点。

Conclusion: 该框架在农业环境中展现出高效自主导航潜力。

Abstract: There is a growing demand for autonomous mobile robots capable of navigating
unstructured agricultural environments. Tasks such as weed control in meadows
require efficient path planning through an unordered set of coordinates while
minimizing travel distance and adhering to curvature constraints to prevent
soil damage and protect vegetation. This paper presents an integrated
navigation framework combining a global path planner based on the Dubins
Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control
(NMPC) strategy for local path planning and control. The DTSP generates a
minimum-length, curvature-constrained path that efficiently visits all targets,
while the NMPC leverages this path to compute control signals to accurately
reach each waypoint. The system's performance was validated through comparative
simulation analysis on real-world field datasets, demonstrating that the
coupled DTSP-based planner produced smoother and shorter paths, with a
reduction of about 16% in the provided scenario, compared to decoupled methods.
Based thereon, the NMPC controller effectively steered the robot to the desired
waypoints, while locally optimizing the trajectory and ensuring adherence to
constraints. These findings demonstrate the potential of the proposed framework
for efficient autonomous navigation in agricultural environments.

</details>
