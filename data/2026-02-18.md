<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: GRAFNet是一种受生物视觉系统启发的息肉分割网络，通过模拟人类视觉层次结构，结合定向注意力、多尺度特征分析和迭代反馈机制，在多个公开数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉分割对癌症预防至关重要，但面临三大挑战：息肉形态高度可变（从扁平到突出病变）、与正常结构（如褶皱和血管）视觉相似性强、需要鲁棒的多尺度检测。现有深度学习方法存在单向处理、多尺度融合弱、缺乏解剖约束等问题，导致假阳性（正常结构过分割）和假阴性（遗漏细微扁平病变）。

Method: 提出GRAFNet，模拟人类视觉系统的层次结构，包含三个核心模块：1）引导非对称注意力模块（GAAM），模拟方向调谐皮层神经元以增强息肉边界；2）多尺度视网膜模块（MSRM），模拟视网膜神经节细胞通路进行并行多特征分析；3）引导皮层注意力反馈模块（GCAFM），应用预测编码进行迭代优化。这些模块在息肉编码器-解码器模块（PEDM）中统一，通过分辨率自适应反馈强制空间语义一致性。

Result: 在五个公开基准数据集（Kvasir-SEG、CVC-300、CVC-ColonDB、CVC-Clinic和PolypGen）上的广泛实验表明，GRAFNet实现了持续的最先进性能，相比领先方法Dice系数提升3-8%，泛化能力提高10-20%，同时提供可解释的决策路径。

Conclusion: 这项工作建立了一个新范式，其中神经计算原理弥合了AI准确性与临床可信推理之间的差距，为医学图像分析提供了生物学启发的可靠解决方案。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [2] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出基于累积样本损失(CSL)的视频标注错误检测方法，通过分析帧级损失轨迹识别错误标注和时序错乱


<details>
  <summary>Details</summary>
Motivation: 现实世界视频数据集常存在标注错误（错误标签和时序错乱），这些错误在相位标注任务中尤为有害，因为时序一致性至关重要。现有方法需要标注错误的地面真值，限制了实际应用。

Method: 提出模型无关的标注错误检测方法：训练视频分割模型并保存每个epoch的权重，计算累积样本损失(CSL)——帧在训练过程中通过所有检查点的平均损失。错误标注或时序错乱的帧通常表现出持续高损失或不规则模式，而正确标注的帧则能快速收敛到低损失。

Result: 在EgoPER和Cholec80数据集上的实验表明，该方法能有效检测错误标注和帧级时序错乱，无需标注错误的地面真值，具有跨数据集的泛化能力。

Conclusion: 提出的CSL方法为视频数据集审计提供了强大工具，能识别标注不一致性，提高视频机器学习训练的可靠性。该方法模型无关、无需错误标注真值，具有实际应用价值。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [3] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 提出基于神经体积渲染的相机虚拟化方法，支持动态场景的时间归档功能，适用于体育直播等应用


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的动态场景渲染方法依赖准确3D点云，难以处理快速非刚性运动和多主体独立运动，且缺乏时间归档功能

Method: 通过多视角同步相机间的刚性变换建模动态场景，进行神经表示学习，实现高质量渲染和时间归档

Result: 方法提供增强的视觉渲染质量，支持回溯任何过去时间点的动态场景进行新视角合成

Conclusion: 神经体积渲染框架为相机虚拟化和时间归档提供了有效解决方案，特别适用于体育广播等需要回放和分析的应用

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [4] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: EventMemAgent：基于分层记忆模块的主动在线视频理解框架，通过短期记忆检测事件边界、长期记忆结构化归档，结合多粒度感知工具包和Agentic RL实现端到端推理


<details>
  <summary>Details</summary>
Motivation: 在线视频理解面临无限流媒体输入与MLLMs有限上下文窗口的矛盾，现有被动处理方法在保持长程上下文与捕捉细粒度细节之间存在权衡

Method: 提出分层记忆模块：短期记忆动态处理流视频帧（事件边界检测+事件粒度水库采样），长期记忆按事件结构化归档；结合多粒度感知工具包进行主动迭代证据捕获，使用Agentic RL端到端内化推理和工具使用策略

Result: 在在线视频基准测试中取得有竞争力的结果

Conclusion: EventMemAgent通过主动感知和分层记忆机制有效解决了在线视频理解中无限输入与有限上下文窗口的矛盾，为流媒体理解提供了新框架

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [5] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本研究提出了一个新的公开草莓成熟度数据集，包含566张图像和1201个标注对象，并在YOLO系列模型上进行了比较测试，为智能农业应用提供了基础参考。


<details>
  <summary>Details</summary>
Motivation: 草莓作为经济价值高、营养丰富的水果，传统成熟度判断方法主观性强、误差大，需要计算机辅助系统。然而，现有研究中缺乏公开全面的数据集，难以进行有效比较。

Method: 创建了一个新的公开草莓成熟度数据集，包含在土耳其两个不同温室中采集的566张图像和1201个标注对象，使用YOLOv8、YOLOv9和YOLO11等模型进行对比测试。

Result: YOLOv9c模型获得最高精确率90.94%，YOLO11s模型获得最高召回率83.74%，YOLOv8s模型在mAP@50指标上表现最佳，达到86.09%。中小型模型在此类数据集上表现更平衡高效。

Conclusion: 提出的公开数据集为草莓成熟度检测研究提供了重要资源，中小型YOLO模型在此类农业应用中表现良好，为智能农业应用建立了基础参考点。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [6] [Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting](https://arxiv.org/abs/2602.15782)
*Ines Montoya-Espinagosa,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种结合天空图像、光伏历史数据和气象数据的多模态混合方法，用于短期和长期光伏预测，特别关注提高斜坡事件预测精度和云天气下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源特别是太阳能的广泛应用，光伏发电的波动性给电网管理带来挑战。需要提高光伏预测精度，特别是在云天气条件下，以支持电网高效运行和太阳能变异性管理。

Method: 采用混合多模态方法，结合天空图像、光伏历史数据和气象数据。使用深度神经网络模型，整合单个和多个气象变量以及太阳位置分析，用于即时预测和短期/长期预测。

Result: 结果表明，加入气象数据（特别是地表长波辐射、向下辐射以及风与太阳位置的组合）显著提高了即时预测和短期/长期预测的准确性，尤其是在多云天气条件下。

Conclusion: 研究强调了整合多样化数据源对于提高太阳能预测模型的可靠性和可解释性的重要性，为电网运营和太阳能变异性管理提供了更有效的预测工具。

Abstract: Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening](https://arxiv.org/abs/2602.15236)
*Anjie Qiao,Zhen Wang,Yaliang Li,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: BindCLIP通过结合对比学习和生成式姿态监督，改进虚拟筛选中的口袋-配体表示学习，提升对真实结合相互作用的敏感性


<details>
  <summary>Details</summary>
Motivation: 现有CLIP风格模型（如DrugCLIP）的表示对细粒度结合相互作用不敏感，可能依赖训练数据中的捷径相关性，限制了按真实结合兼容性对配体进行排序的能力

Method: 提出BindCLIP统一对比-生成表示学习框架：联合训练口袋和配体编码器，使用CLIP风格对比学习结合口袋条件扩散目标进行结合姿态生成，姿态级监督直接塑造检索嵌入空间朝向相互作用相关特征；引入硬负样本增强和配体-配体锚定正则化器防止表示崩溃

Result: 在两个公共基准测试上表现优于强基线；在具有挑战性的分布外虚拟筛选中取得显著提升；在FEP+基准测试上改进配体类似物排序

Conclusion: 将生成式姿态级监督与对比学习相结合，产生更具相互作用感知的嵌入表示，在现实筛选设置中改善泛化能力，使虚拟筛选更接近实际应用

Abstract: Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.

</details>


### [8] [Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization](https://arxiv.org/abs/2602.15304)
*Farzana Akter,Rakib Hossain,Deb Kanna Roy Toushi,Mahmood Menon Khan,Sultana Amin,Lisan Al Amin*

Main category: cs.LG

TL;DR: 提出结合联邦学习与分割学习的混合隐私保护框架，用于医疗决策支持，无需共享原始数据，在预测性能、隐私泄露、通信开销间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持常受治理和隐私规则限制，无法跨机构汇集患者数据。需要一种既能保护隐私又能支持决策导向医疗建模的方法。

Method: 结合联邦学习(FL)和分割学习(SL)的混合框架：客户端保留特征提取主干，服务器托管预测头部，通过成员推理审计泄露，采用激活裁剪和高斯噪声等轻量防御。

Result: 在三个公共临床数据集上评估，混合FL-SL变体在预测性能和决策优先级方面与独立FL或SL相当，提供可调的隐私-效用权衡，能减少审计泄露。

Conclusion: 混合FL-SL为隐私保护医疗决策支持提供了实用设计空间，可在效用、泄露风险和部署成本间进行明确平衡。

Abstract: Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.

</details>


### [9] [Doubly Stochastic Mean-Shift Clustering](https://arxiv.org/abs/2602.15393)
*Tom Trigano,Yann Sepulcre,Itshak Lapidot*

Main category: cs.LG

TL;DR: DSMS通过随机化带宽和数据采样改进Mean-Shift算法，解决固定带宽在数据稀疏时的过分割问题


<details>
  <summary>Details</summary>
Motivation: 传统Mean-Shift算法对带宽超参数敏感，特别是在数据稀缺时，固定尺度的密度估计会导致碎片化和虚假模式

Method: 提出双重随机Mean-Shift（DSMS），在轨迹更新和核带宽中都引入随机性，每次迭代从连续均匀分布中采样数据和半径

Result: 在合成高斯混合数据上的比较实验显示，DSMS显著优于标准和随机Mean-Shift基线，表现出更好的稳定性，防止稀疏聚类场景中的过分割

Conclusion: 随机带宽策略作为隐式正则化机制，能更好地探索密度景观，提供收敛理论结果，且不会导致其他性能下降

Abstract: Standard Mean-Shift algorithms are notoriously sensitive to the bandwidth hyperparameter, particularly in data-scarce regimes where fixed-scale density estimation leads to fragmentation and spurious modes. In this paper, we propose Doubly Stochastic Mean-Shift (DSMS), a novel extension that introduces randomness not only in the trajectory updates but also in the kernel bandwidth itself. By drawing both the data samples and the radius from a continuous uniform distribution at each iteration, DSMS effectively performs a better exploration of the density landscape. We show that this randomized bandwidth policy acts as an implicit regularization mechanism, and provide convergence theoretical results. Comparative experiments on synthetic Gaussian mixtures reveal that DSMS significantly outperforms standard and stochastic Mean-Shift baselines, exhibiting remarkable stability and preventing over-segmentation in sparse clustering scenarios without other performance degradation.

</details>


### [10] [Benchmarking IoT Time-Series AD with Event-Level Augmentations](https://arxiv.org/abs/2602.15457)
*Dmitry Zhevnenko,Ilya Makarov,Aleksandr Kovalenko,Fedor Meshchaninov,Anton Kozhukhov,Vladislav Travnikov,Makar Ippolitov,Kirill Yashunin,Iurii Katser*

Main category: cs.LG

TL;DR: 本文提出一个针对物联网时间序列异常检测的事件级评估协议，包含统一的现实扰动模拟和传感器级根因分析，评估14个模型后发现没有通用最优模型，不同模型在不同场景下表现各异。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测研究多关注点级结果和精心整理的数据集，缺乏对实际应用中可靠性、及时性和现实扰动的评估，限制了模型在实际应用中的选择价值。

Method: 引入统一的事件级评估协议，包含校准传感器丢失、线性和对数漂移、加性噪声、窗口偏移等现实扰动模拟；通过掩码缺失归零和每通道影响估计进行传感器级探测；在5个公开数据集和2个工业数据集上评估14个代表性模型。

Result: 没有通用最优模型：图结构模型在传感器丢失和长事件下转移性最好；密度/流模型在清洁稳定工厂表现好但对单调漂移脆弱；谱CNN在周期性强的场景领先；重建自编码器在基本传感器审查后具有竞争力；预测/混合动态模型在故障破坏时间依赖时有效但对窗口敏感。

Conclusion: 评估协议为实际模型选择提供指导，揭示了不同模型在不同现实扰动下的表现差异，并能为设计选择提供信息（如用高斯密度替换归一化流会显著降低性能，固定学习DAG会轻微提升清洁集表现但大幅增加漂移敏感性）。

Abstract: Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.

</details>


### [11] [CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals](https://arxiv.org/abs/2602.15546)
*Tomàs Garriga,Gerard Sanz,Eduard Serrahima de Cambra,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种用于时间序列反事实推理的新方法CEPAE，通过熵惩罚损失在潜在空间中鼓励解耦表示，在合成和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗和营销等领域，准确的时间序列反事实推理对于理解事件或处理对时间序列结果的影响至关重要，但目前缺乏专门针对时间序列反事实推理的方法。

Method: 基于结构因果模型框架和溯因-行动-预测流程，首先将变分自编码器和对抗自编码器方法适配到时间序列设置，然后提出条件熵惩罚自编码器（CEPAE），通过在潜在空间中使用熵惩罚损失来鼓励解耦的数据表示。

Result: 在合成、半合成和真实世界数据集上的实验验证表明，CEPAE在评估指标上通常优于其他方法。

Conclusion: CEPAE是一种有效的自编码器基础的反事实推理方法，特别适用于受市场事件影响的时间序列数据，能够提供更准确的反事实预测。

Abstract: The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.

</details>


### [12] [The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems](https://arxiv.org/abs/2602.15637)
*Amirreza Dolatpour Fathkouhi,Alireza Namazi,Heman Shakeri*

Main category: cs.LG

TL;DR: 时间序列插值基准存在平稳性偏差，导致简单方法在主导稳态系统中表现虚假优越。论文提出分层压力测试，区分平稳和瞬态两种状态进行评估，发现在关键瞬态中深度学习模型能保持更好的形态保真度。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插值基准使用均匀随机掩码和形状无关的评估指标（如MSE、RMSE），在具有主导吸引子的系统中会产生系统性平稳性偏差。这种偏差使得简单方法在主要由低熵稳态组成的基准中表现虚假优越，而忽略了在关键瞬态中的性能。

Method: 提出分层压力测试框架，将评估划分为平稳和瞬态两种状态。使用连续血糖监测（CGM）作为测试平台，利用其精确的地面真实强迫函数（进食、胰岛素）实现准确的状态识别。从临床试验中推导经验缺失分布，并将其应用于完整训练数据，防止模型利用不现实的干净观测。

Result: 发现三个关键结果：1）在稳定区间，线性插值达到最先进的性能，表明复杂架构在低熵状态下计算浪费；2）在关键瞬态（餐后峰值、低血糖事件）中，线性方法形态保真度（DTW）显著下降，与RMSE不成比例，称为RMSE幻象；3）深度学习模型在瞬态中能同时保持点准确性和形态完整性。

Conclusion: 时间序列插值评估需要考虑状态条件性，避免平稳性偏差。对于安全关键的下游任务，深度学习模型在瞬态中保持形态保真度的能力至关重要。该框架可推广到任何常规平稳性主导关键瞬态的受调节系统。

Abstract: Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.

</details>


### [13] [CAMEL: An ECG Language Model for Forecasting Cardiac Events](https://arxiv.org/abs/2602.15677)
*Neelay Velingker,Alaia Solko-Breslin,Mayank Keoliya,Seewon Choi,Jiayi Xin,Anika Marathe,Alireza Oraii,Rajat Deo,Sameed Khatana,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAMEL是首个能够进行心电图信号长期推理和预测的语言模型，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 当前的心电图语言模型（ELMs）虽然能够进行分类和报告生成，但无法预测未来的心脏事件，而这对临床早期干预具有重要价值

Method: 开发专门的ECG编码器实现心电图信号与文本的跨模态理解，采用LoRA适配和课程学习管道训练，包括ECG分类、指标计算和多轮对话推理

Result: 在6个任务和9个数据集上表现出强大的零样本性能，在ECGBench上获得+7.0%绝对平均增益，在ECGForecastBench上比全监督模型高+12.4%，比零样本ELMs高+21.1%

Conclusion: CAMEL是首个具备心电图信号长期推理能力的ELM，能够预测未来心脏事件，为临床早期干预提供重要工具

Abstract: Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).

</details>


### [14] [Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching](https://arxiv.org/abs/2602.15752)
*Ren Kishimoto,Rikiya Takehi,Koichi Tanaka,Masahiro Nomura,Riku Togashi,Yoji Tomita,Yuta Saito*

Main category: cs.LG

TL;DR: 提出MRet算法，通过个性化留存曲线动态调整推荐，最大化双边匹配平台的用户留存率，而非传统方法只关注匹配数量或公平性。


<details>
  <summary>Details</summary>
Motivation: 传统双边匹配平台（如在线约会、招聘）通常以最大化匹配数量为目标，但这会导致用户匹配分布不均：部分用户获得过多匹配，而许多用户获得很少匹配并最终流失。虽然公平性目标可以缓解这一问题，但公平性本身并非平台的最终目标，用户不会仅仅因为曝光均等而奖励平台。在实践中，用户留存通常是平台的终极目标，单纯依赖公平性无法有效优化留存。

Method: 提出MRet（Matching for Retention）算法，这是一种动态学习排序（LTR）方法。该算法通过从每个用户的个人资料和交互历史中学习个性化留存曲线来建模用户留存。基于这些曲线，MRet动态调整推荐策略，联合考虑推荐接收方和被推荐方的留存收益，从而将有限的匹配机会分配到最能提升整体留存的地方。

Result: 在合成数据集和来自主要在线约会平台的真实数据集上的实证评估表明，MRet实现了更高的用户留存率。由于传统方法主要优化匹配数量或公平性而非留存，MRet在提升用户留存方面表现更优。

Conclusion: MRet算法通过直接优化用户留存而非匹配数量或公平性，为双边匹配平台提供了一种更有效的用户保留策略。该方法考虑了用户个性化留存行为，动态分配匹配机会以最大化整体留存，在实际应用中展现出优越性能。

Abstract: On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.
  In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Panini: Continual Learning in Token Space via Structured Memory](https://arxiv.org/abs/2602.15156)
*Shreyas Rajesh,Pavan Holur,Mehmet Yigit Turali,Chenda Duan,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: Panini提出了一种非参数持续学习框架，将文档表示为生成语义工作空间（GSW），通过实体和事件感知的问答对网络来积累和整合知识，相比传统RAG方法更高效准确。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）方法存在计算效率低（LLM重复处理相同文档）和检索不相关内容导致生成不可靠的问题，需要更高效准确的知识整合方式。

Method: 提出Panini框架，将文档表示为生成语义工作空间（GSW）——一个实体和事件感知的问答对网络，能够重构经验情境并通过推理链挖掘潜在知识。查询时只遍历持续更新的GSW而非原始文档。

Result: 在六个QA基准测试中，Panini平均性能比其他竞争基线高5%-7%，同时使用2-30倍更少的答案上下文token，支持完全开源流程，并在不可回答查询上减少了不可支持的答案。

Conclusion: 在写入时高效准确地结构化经验（如GSW框架所实现的）能在读取时同时获得效率和可靠性收益，为持续学习提供了更有效的方法。

Abstract: Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.

</details>


### [16] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 本文提出了一种自适应弃权系统，通过动态调整安全阈值和级联检测架构，在保持高性能的同时平衡LLM的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM部署面临安全性与实用性的根本权衡：严格过滤机制会阻止良性查询，而宽松控制则可能生成不安全内容。传统基于静态规则或固定置信度阈值的护栏通常缺乏上下文敏感性且计算成本高，导致高延迟和用户体验下降。

Method: 引入自适应弃权系统，基于实时上下文信号（如领域和用户历史）动态调整安全阈值。框架集成了由五个并行检测器组成的多维检测架构，通过分层级联机制优化速度和精度。级联设计通过逐步过滤查询减少不必要的计算。

Result: 在混合和特定领域工作负载上的广泛评估显示，误报显著减少，特别是在医疗建议和创意写作等敏感领域。系统在严格操作模式下保持高安全精度和接近完美的召回率。与非级联模型和外部护栏系统相比，实现了显著的延迟改进。

Conclusion: 上下文感知的弃权框架有效平衡了安全性和实用性，同时保持了性能，为可靠的LLM部署提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>
