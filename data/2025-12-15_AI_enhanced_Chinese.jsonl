{"id": "2512.11047", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11047", "abs": "https://arxiv.org/abs/2512.11047", "authors": ["Haoran Jiang", "Jin Chen", "Qingwen Bu", "Li Chen", "Modi Shi", "Yanjie Zhang", "Delong Li", "Chuanzhe Suo", "Chuang Wang", "Zhihui Peng", "Hongyang Li"], "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control", "comment": null, "summary": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.", "AI": {"tldr": "\u63d0\u51faWholeBodyVLA\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7edf\u4e00\u5b66\u4e60\u4ece\u4f4e\u6210\u672c\u65e0\u52a8\u4f5c\u89c6\u9891\u83b7\u53d6\u5168\u8eab\u8fd0\u52a8\u77e5\u8bc6\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u5927\u7a7a\u95f4\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u64cd\u4f5c\u611f\u77e5\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0c\u65e0\u6cd5\u6267\u884c\u5927\u7a7a\u95f4\u8fd0\u52a8\u64cd\u4f5c\u3002\u8fd9\u6e90\u4e8e\u4e24\u4e2a\u6311\u6218\uff1a1) \u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u8fd0\u52a8\u64cd\u4f5c\u77e5\u8bc6\u83b7\u53d6\u56f0\u96be\uff1b2) \u73b0\u6709RL\u63a7\u5236\u5668\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u6709\u9650\u5bfc\u81f4\u8fd0\u52a8\u547d\u4ee4\u6267\u884c\u4e0d\u53ef\u9760", "method": "1) \u63d0\u51fa\u7edf\u4e00\u6f5c\u5728\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\u80fd\u4ece\u4f4e\u6210\u672c\u65e0\u52a8\u4f5c\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5b66\u4e60\uff1b2) \u8bbe\u8ba1\u9ad8\u6548\u4eba\u7c7b\u6570\u636e\u6536\u96c6\u7ba1\u9053\u6269\u5c55\u6570\u636e\u96c6\uff1b3) \u63d0\u51fa\u4e13\u95e8\u7684\u8fd0\u52a8\u64cd\u4f5c\u5bfc\u5411RL\u7b56\u7565\uff0c\u9488\u5bf9\u524d\u8fdb\u3001\u8f6c\u5411\u3001\u4e0b\u8e72\u7b49\u6838\u5fc3\u8fd0\u52a8\u8fdb\u884c\u7cbe\u786e\u7a33\u5b9a\u63a7\u5236\uff1b4) \u6574\u5408\u4e3aWholeBodyVLA\u7edf\u4e00\u6846\u67b6", "result": "\u5728AgiBot X2\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5148\u524d\u57fa\u7ebf\u63d0\u534721.3%\u3002\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6267\u884c\u5e7f\u6cdb\u7684\u4efb\u52a1\u8303\u56f4", "conclusion": "WholeBodyVLA\u662f\u9996\u4e2a\u5b9e\u73b0\u5927\u7a7a\u95f4\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u4f5c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5b66\u4e60\u548c\u4e13\u95e8\u63a7\u5236\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u4f5c\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55"}}
{"id": "2512.11016", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11016", "abs": "https://arxiv.org/abs/2512.11016", "authors": ["Haolin Yang", "Jiayuan Rao", "Haoning Wu", "Weidi Xie"], "title": "SoccerMaster: A Vision Foundation Model for Soccer Understanding", "comment": null, "summary": "Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.", "AI": {"tldr": "SoccerMaster\uff1a\u9996\u4e2a\u8db3\u7403\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7edf\u4e00\u5904\u7406\u591a\u79cd\u8db3\u7403\u7406\u89e3\u4efb\u52a1\uff0c\u5728\u5404\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e13\u7528\u6a21\u578b", "motivation": "\u8db3\u7403\u7406\u89e3\u5177\u6709\u9886\u57df\u7279\u5b9a\u7684\u590d\u6742\u6027\u548c\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5b64\u7acb\u7684\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u6765\u5904\u7406\u4ece\u7ec6\u7c92\u5ea6\u611f\u77e5\u5230\u8bed\u4e49\u63a8\u7406\u7684\u591a\u6837\u5316\u4efb\u52a1", "method": "\u63d0\u51faSoccerMaster\u8db3\u7403\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7edf\u4e00\u6846\u67b6\uff1b\u5f00\u53d1\u81ea\u52a8\u6570\u636e\u6807\u6ce8\u7ba1\u9053\u751f\u6210\u53ef\u6269\u5c55\u7684\u7a7a\u95f4\u6807\u6ce8\uff0c\u6574\u5408\u73b0\u6709\u6570\u636e\u96c6\u6784\u5efaSoccerFactory\u9884\u8bad\u7ec3\u6570\u636e\u8d44\u6e90", "result": "SoccerMaster\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5e7f\u5ea6\u548c\u4f18\u8d8a\u6027", "conclusion": "SoccerMaster\u4f5c\u4e3a\u9996\u4e2a\u8db3\u7403\u7279\u5b9a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u6210\u529f\u7edf\u4e00\u4e86\u591a\u6837\u5316\u7406\u89e3\u4efb\u52a1\uff0c\u4e3a\u8db3\u7403\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u3001\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u53ef\u7528"}}
{"id": "2512.11173", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11173", "abs": "https://arxiv.org/abs/2512.11173", "authors": ["Tzu-Hsien Lee", "Fidan Mahmudova", "Karthik Desingh"], "title": "Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance", "comment": null, "summary": "Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u4f53\u4e2d\u5fc3\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u56db\u8db3\u79fb\u52a8\u673a\u68b0\u81c2\u4ec5\u4f7f\u7528\u673a\u8f7dRGB\u76f8\u673a\u5b9e\u73b0\u6700\u540e\u4e00\u7c73\u5bfc\u822a\uff0c\u8fbe\u5230\u53ef\u64cd\u4f5c\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u65e0\u9700\u6df1\u5ea6\u3001\u6fc0\u5149\u96f7\u8fbe\u6216\u5730\u56fe\u5148\u9a8c\u3002", "motivation": "\u73b0\u6709RGB\u5bfc\u822a\u7cfb\u7edf\u901a\u5e38\u53ea\u6709\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u4e0d\u9002\u5408\u79fb\u52a8\u673a\u68b0\u81c2\u64cd\u4f5c\u524d\u7684\u7cbe\u786e\u5b9a\u4f4d\u9700\u6c42\uff0c\u5bfc\u81f4\u64cd\u4f5c\u7b56\u7565\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u800c\u9891\u7e41\u5931\u8d25\u3002", "method": "\u4f7f\u7528\u76ee\u6807\u56fe\u50cf\u3001\u591a\u89c6\u89d2RGB\u89c2\u6d4b\u548c\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u5bfc\u822a\u7b56\u7565\u8f93\u5165\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u6a21\u5757\u548c\u7a7a\u95f4\u8bc4\u5206\u77e9\u9635\u89e3\u7801\u5668\u63d0\u4f9b\u663e\u5f0f\u7269\u4f53\u5b9a\u4f4d\u548c\u76f8\u5bf9\u4f4d\u59ff\u63a8\u7406\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u7269\u4f53\u4e0a\uff0c\u8fb9\u7f18\u5bf9\u9f50\u6210\u529f\u738773.47%\uff0c\u7269\u4f53\u5bf9\u9f50\u6210\u529f\u738796.94%\uff0c\u8bc1\u660e\u4ec5\u7528\u7c7b\u522b\u7ea7\u6570\u636e\u548cRGB\u89c2\u6d4b\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u6700\u540e\u4e00\u7c73\u5bfc\u822a\u3002", "conclusion": "\u65e0\u9700\u6df1\u5ea6\u3001\u6fc0\u5149\u96f7\u8fbe\u6216\u5730\u56fe\u5148\u9a8c\u5373\u53ef\u5b9e\u73b0\u7c7b\u522b\u7ea7\u7684\u7cbe\u786e\u6700\u540e\u4e00\u7c73\u5bfc\u822a\uff0c\u4e3a\u7edf\u4e00\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2512.11076", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11076", "abs": "https://arxiv.org/abs/2512.11076", "authors": ["Jack Brady", "Andrew Dailey", "Kristen Schang", "Zo Vic Shong"], "title": "E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring", "comment": null, "summary": "Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u5728\u57ce\u5e02\u52a8\u6001\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u3001\u6311\u6218\u53ca\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u4f20\u611f\u5668\u878d\u5408\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u57ce\u5e02\u76d1\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u6280\u672f\u6765\u7406\u89e3\u57ce\u5e02\u52a8\u6001\u3002\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u4f20\u611f\u5668\uff0c\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff08\u5982\u4f4e\u5149\u5de5\u4f5c\u80fd\u529b\uff09\uff0c\u6709\u671b\u6539\u5584\u57ce\u5e02\u52a8\u6001\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u4e8b\u4ef6\u76f8\u673a\u7684\u5de5\u4f5c\u539f\u7406\u3001\u5e94\u7528\u573a\u666f\u3001\u4f18\u52bf\u6311\u6218\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u591a\u4f20\u611f\u5668\u878d\u5408\u65b9\u6848\uff08\u4e8b\u4ef6\u76f8\u673a+\u7ea2\u5916\u3001LiDAR\u3001\u632f\u52a8\u4f20\u611f\u5668\u7b49\uff09\u3002", "result": "\u4e8b\u4ef6\u76f8\u673a\u80fd\u591f\u6355\u6349\u91cd\u8981\u4fe1\u606f\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\uff0c\u9002\u5408\u57ce\u5e02\u52a8\u6001\u7814\u7a76\u3002\u591a\u4f20\u611f\u5668\u878d\u5408\u53ef\u4ee5\u589e\u5f3a\u4e8b\u4ef6\u76f8\u673a\u80fd\u529b\u5e76\u514b\u670d\u5176\u73b0\u6709\u6311\u6218\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u662f\u7814\u7a76\u57ce\u5e02\u52a8\u6001\u7684\u6709\u524d\u666f\u5a92\u4ecb\uff0c\u7ed3\u5408\u591a\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u57ce\u5e02\u76d1\u6d4b\u80fd\u529b\u3002"}}
{"id": "2512.11469", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11469", "abs": "https://arxiv.org/abs/2512.11469", "authors": ["Pranav Ramanathan", "Thomas Prellberg", "Matthew Lewis", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Three methods, one problem: Classical and AI approaches to no-three-in-line", "comment": null, "summary": "The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u7ecf\u5178\u4f18\u5316\u65b9\u6cd5\uff08\u6574\u6570\u7ebf\u6027\u89c4\u5212ILP\uff09\u4e0eAI\u65b9\u6cd5\uff08PatternBoost\u53d8\u538b\u5668\u5b66\u4e60\u548cPPO\u5f3a\u5316\u5b66\u4e60\uff09\u5728No-Three-In-Line\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0ILP\u572819\u00d719\u7f51\u683c\u5185\u53ef\u4fdd\u8bc1\u6700\u4f18\u89e3\uff0c\u800cAI\u65b9\u6cd5\u5728\u8f83\u5c0f\u7f51\u683c\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u6df7\u5408\u65b9\u6cd5\u662f\u6700\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "motivation": "No-Three-In-Line\u95ee\u9898\u662f\u7ec4\u5408\u51e0\u4f55\u4e2d\u7684\u8457\u540d\u95ee\u9898\uff0c\u7ecf\u5178\u65b9\u6cd5\u5982\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u867d\u7136\u80fd\u4fdd\u8bc1\u6700\u4f18\u89e3\uff0c\u4f46\u968f\u7740\u7f51\u683c\u89c4\u6a21\u589e\u5927\u9762\u4e34\u6307\u6570\u7ea7\u590d\u6742\u5ea6\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3a\u6a21\u5f0f\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u9996\u6b21\u5c06PatternBoost\u53d8\u538b\u5668\u5b66\u4e60\u548cPPO\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u8be5\u95ee\u9898\uff0c\u5e76\u4e0e\u4f20\u7edfILP\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002ILP\u7528\u4e8e\u83b7\u53d6\u53ef\u8bc1\u660e\u7684\u6700\u4f18\u89e3\uff0cAI\u65b9\u6cd5\u7528\u4e8e\u63a2\u7d22\u6a21\u5f0f\u8fd1\u4f3c\u3002", "result": "ILP\u572819\u00d719\u7f51\u683c\u5185\u83b7\u5f97\u53ef\u8bc1\u660e\u7684\u6700\u4f18\u89e3\uff1bPatternBoost\u572814\u00d714\u7f51\u683c\u5185\u5339\u914d\u6700\u4f18\u6027\u80fd\uff0c\u6d4b\u8bd5\u635f\u5931\u51cf\u5c1196%\uff1bPPO\u572810\u00d710\u7f51\u683c\u4e0a\u83b7\u5f97\u5b8c\u7f8e\u89e3\uff0c\u4f46\u572811\u00d711\u7f51\u683c\u4e0a\u56e0\u7ea6\u675f\u8fdd\u53cd\u800c\u5931\u8d25\u3002", "conclusion": "\u7ecf\u5178\u4f18\u5316\u65b9\u6cd5\u5bf9\u4e8e\u7cbe\u786e\u89e3\u4ecd\u7136\u5fc5\u4e0d\u53ef\u5c11\uff0c\u800cAI\u65b9\u6cd5\u5728\u8f83\u5c0f\u5b9e\u4f8b\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u3002\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u662f\u6269\u5c55\u5230\u66f4\u5927\u95ee\u9898\u89c4\u6a21\u7684\u6700\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2512.11282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11282", "abs": "https://arxiv.org/abs/2512.11282", "authors": ["Qingsen Ma", "Dianyun Wang", "Ran Jing", "Yujun Sun", "Zhenbo Xu"], "title": "CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise", "comment": null, "summary": "Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.", "AI": {"tldr": "CIP\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7684\u56e0\u679c\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5b9e\u4f53\u3001\u52a8\u4f5c\u548c\u4e8b\u4ef6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5e8f\u5217\u6765\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u566a\u58f0\u68c0\u7d22\u4e0a\u4e0b\u6587\u65f6\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u4e8b\u5b9e\u57fa\u7840\u548c\u63a8\u7406\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u800c\u5608\u6742\u7684\u68c0\u7d22\u4e0a\u4e0b\u6587\u65f6\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u7684\u56e0\u679c\u5173\u7cfb\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6539\u5584\u6a21\u578b\u7684\u4e8b\u5b9e\u57fa\u7840\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "CIP\u6846\u67b6\u6784\u5efa\u5b9e\u4f53\u3001\u52a8\u4f5c\u548c\u4e8b\u4ef6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5e8f\u5217\uff0c\u5e76\u5c06\u5176\u6ce8\u5165\u63d0\u793a\u4e2d\u5f15\u5bfc\u63a8\u7406\u671d\u5411\u56e0\u679c\u76f8\u5173\u8bc1\u636e\u3002\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u6291\u5236\u975e\u56e0\u679c\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u5305\u62ecGPT-4o\u3001Gemini 2.0 Flash\u548cLlama 3.1\u5728\u5185\u7684\u4e03\u4e2a\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cCIP\u5c06\u53ef\u5f52\u56e0\u7387\u63d0\u9ad82.6\u5206\uff0c\u56e0\u679c\u4e00\u81f4\u6027\u5f97\u5206\u63d0\u9ad80.38\uff0c\u6709\u6548\u4fe1\u606f\u5bc6\u5ea6\u63d0\u9ad8\u56db\u500d\uff0c\u5e76\u5c06\u7aef\u5230\u7aef\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe55.1%\u3002", "conclusion": "\u56e0\u679c\u63a8\u7406\u53ef\u80fd\u6210\u4e3a\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u7684\u6709\u524d\u666f\u8303\u5f0f\u3002CIP\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u5f15\u5bfc\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u3002"}}
{"id": "2512.11184", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11184", "abs": "https://arxiv.org/abs/2512.11184", "authors": ["Conor Rowan"], "title": "On the failure of ReLU activation for physics-informed machine learning", "comment": null, "summary": "Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86ReLU\u6fc0\u6d3b\u51fd\u6570\u5728\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u81ea\u52a8\u5fae\u5206\u65e0\u6cd5\u6b63\u786e\u5904\u7406\u4e0d\u8fde\u7eed\u573a\u7684\u5bfc\u6570\uff0c\u5bfc\u81f4\u68af\u5ea6\u8ba1\u7b97\u9519\u8bef\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4f7f\u7528\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9\u5f71\u54cd\u6027\u80fd\u3002\u5df2\u6709\u7814\u7a76\u8868\u660eReLU\u5728\u57fa\u51c6\u5fae\u5206\u65b9\u7a0b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5177\u4f53\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\uff0c\u672c\u6587\u65e8\u5728\u8bca\u65adReLU\u8868\u73b0\u5dee\u7684\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5206\u6790ReLU\u7684\u5206\u6bb5\u7ebf\u6027\u7279\u6027\uff0c\u7814\u7a76\u5176\u5728\u4e8c\u9636\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u5176\u5728\u4ec5\u542b\u4e00\u9636\u5bfc\u6570\u7684\u53d8\u5206\u95ee\u9898\u4e2d\u7684\u5931\u8d25\u3002\u91cd\u70b9\u8003\u5bdf\u81ea\u52a8\u5fae\u5206\uff08PyTorch\uff09\u5982\u4f55\u5904\u7406\u4e0d\u8fde\u7eed\u573a\u7684\u5bfc\u6570\uff0c\u4ee5\u53ca\u8fd9\u5982\u4f55\u5f71\u54cd\u7269\u7406\u4fe1\u606f\u635f\u5931\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "result": "\u53d1\u73b0ReLU\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\u5176\u4e8c\u9636\u5bfc\u6570\u95ee\u9898\uff1a\u867d\u7136\u635f\u5931\u51fd\u6570\u53ef\u80fd\u53ea\u6d89\u53ca\u4e00\u9636\u5bfc\u6570\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u81ea\u52a8\u5fae\u5206\u9700\u8981\u8ba1\u7b97\u4e8c\u9636\u5bfc\u6570\u3002PyTorch\u7684\u81ea\u52a8\u5fae\u5206\u65e0\u6cd5\u6b63\u786e\u8868\u5f81\u4e0d\u8fde\u7eed\u573a\u7684\u5bfc\u6570\uff0c\u5bfc\u81f4\u7269\u7406\u4fe1\u606f\u635f\u5931\u7684\u68af\u5ea6\u88ab\u9519\u8bef\u6307\u5b9a\u3002", "conclusion": "ReLU\u5728\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u4e3b\u8981\u539f\u56e0\u662f\u5176\u5206\u6bb5\u7ebf\u6027\u7279\u6027\u5bfc\u81f4\u7684\u4e0d\u8fde\u7eed\u6027\uff0c\u4f7f\u5f97\u81ea\u52a8\u5fae\u5206\u65e0\u6cd5\u6b63\u786e\u5904\u7406\u5bfc\u6570\u8ba1\u7b97\uff0c\u4ece\u800c\u5f71\u54cd\u8bad\u7ec3\u68af\u5ea6\u3002\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48sigmoid\u3001tanh\u548cswish\u7b49\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2512.11194", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11194", "abs": "https://arxiv.org/abs/2512.11194", "authors": ["Divya Kothandaraman", "Jaclyn Pytlarz"], "title": "Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models", "comment": null, "summary": "Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.\n  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.", "AI": {"tldr": "\u63d0\u51fa\u68af\u5ea6\u6295\u5f71\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u7cfb\u7edf\u6027\u5730\u6392\u9664\u654f\u611f\u6982\u5ff5\u7ea7\u7279\u5f81\uff0c\u9632\u6b62\u8bb0\u5fc6\u5316\u98ce\u9669", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6\u5316\u5e26\u6765\u5b89\u5168\u548c\u77e5\u8bc6\u4ea7\u6743\u98ce\u9669\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ea\u80fd\u9632\u6b62\u7279\u5b9a\u8bad\u7ec3\u6837\u672c\u7684\u8fc7\u62df\u5408\uff0c\u65e0\u6cd5\u7cfb\u7edf\u6027\u5730\u963b\u6b62\u6982\u5ff5\u7ea7\u7279\u5f81\u7684\u5185\u90e8\u5316", "method": "\u68af\u5ea6\u6295\u5f71\u6846\u67b6\uff1a\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u8bc6\u522b\u5e76\u5207\u9664\u4e0e\u7981\u6b62\u5c5e\u6027\u5d4c\u5165\u5bf9\u9f50\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5c06\u68af\u5ea6\u66f4\u65b0\u6295\u5f71\u5230\u654f\u611f\u7279\u5f81\u5d4c\u5165\u7a7a\u95f4\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4", "result": "\u6846\u67b6\u663e\u8457\u51cf\u5c11\u8bb0\u5fc6\u5316\uff0c\u540c\u65f6\u4e25\u683c\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b", "conclusion": "\u901a\u8fc7\u5c06\u8bb0\u5fc6\u63a7\u5236\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9009\u62e9\u6027\u5b66\u4e60\uff0c\u4e3aIP\u5b89\u5168\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u751f\u6210AI\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2512.11773", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11773", "abs": "https://arxiv.org/abs/2512.11773", "authors": ["Britton Jordan", "Jordan Thompson", "Jesse F. d'Almeida", "Hao Li", "Nithesh Kumar", "Susheela Sharma Stern", "Ipek Oguz", "Robert J. Webster", "Daniel Brown", "Alan Kuntz", "James Ferguson"], "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics", "comment": "9 pages, 5 figures", "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.", "AI": {"tldr": "ProbeMDE\uff1a\u4e00\u79cd\u7528\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6210\u672c\u611f\u77e5\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RGB\u56fe\u50cf\u548c\u7a00\u758f\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\uff0c\u5229\u7528\u96c6\u6210\u6a21\u578b\u9884\u6d4b\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u5e76\u901a\u8fc7SVGD\u9009\u62e9\u6700\u4f18\u6d4b\u91cf\u4f4d\u7f6e\u4ee5\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u624b\u672f\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u51c6\u786e\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u7eb9\u7406\u7f3a\u5931\u8868\u9762\u3001\u955c\u9762\u53cd\u5c04\u548c\u906e\u6321\u5e38\u89c1\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6240\u9700\u7684\u6d4b\u91cf\u6b21\u6570\u3002", "method": "\u63d0\u51faProbeMDE\u6846\u67b6\uff1a1\uff09\u4f7f\u7528MDE\u6a21\u578b\u96c6\u6210\uff0c\u57fa\u4e8eRGB\u56fe\u50cf\u548c\u7a00\u758f\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\u9884\u6d4b\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff1b2\uff09\u901a\u8fc7\u96c6\u6210\u65b9\u5dee\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff1b3\uff09\u5229\u7528SVGD\u5728\u68af\u5ea6\u56fe\u4e0a\u9009\u62e9\u6700\u4f18\u6d4b\u91cf\u4f4d\u7f6e\uff0c\u907f\u514d\u6a21\u5f0f\u5d29\u6e83\u3002", "result": "\u5728\u6a21\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u5728\u4e2d\u592e\u6c14\u9053\u963b\u585e\u624b\u672f\u6a21\u578b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u6df1\u5ea6\u4f30\u8ba1\u6307\u6807\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6240\u9700\u7684\u672c\u4f53\u611f\u77e5\u6d4b\u91cf\u6b21\u6570\u3002", "conclusion": "ProbeMDE\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u624b\u672f\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.11221", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11221", "abs": "https://arxiv.org/abs/2512.11221", "authors": ["Adilet Metinov", "Gulida M. Kudakeeva", "Bolotbek uulu Nursultan", "Gulnara D. Kabaeva"], "title": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference", "comment": "6 pages, 3 tables , 1 figure", "summary": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.", "AI": {"tldr": "\u63d0\u51faASR-KF-EGR\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9006\u8f6f\u51bb\u7ed3\u673a\u5236\u5728\u63a8\u7406\u65f6\u52a8\u6001\u6682\u505c\u4f4e\u91cd\u8981\u6027token\u7684KV\u66f4\u65b0\uff0c\u51cf\u5c11KV\u7f13\u5b58\u5927\u5c0f55-67%\u800c\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u65f6KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u7684\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8e\u6ed1\u52a8\u6ce8\u610f\u529b\u7a97\u53e3\u8bc6\u522b\u4f4e\u91cd\u8981\u6027token\uff0c\u91c7\u7528\u53ef\u9006\u8f6f\u51bb\u7ed3\u673a\u5236\u6682\u505c\u5176KV\u66f4\u65b0\uff0c\u6240\u6709token\u4fdd\u5b58\u5728GPU\u5916\u5b58\u50a8\u4e2d\u53ef\u6309\u9700\u6062\u590d\uff1b\u5f15\u5165\u4e9a\u7ebf\u6027\u51bb\u7ed3\u8c03\u5ea6\uff0c\u51bb\u7ed3\u65f6\u957f\u968f\u91cd\u590d\u68c0\u6d4b\u6b21\u6570\u4e9a\u7ebf\u6027\u589e\u957f", "result": "\u5728LLaMA-3 8B\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u4e3b\u52a8KV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c1155-67%\uff0c\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u5e76\u901a\u8fc7needle-in-haystack\u68c0\u7d22\u6d4b\u8bd5", "conclusion": "ASR-KF-EGR\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u67b6\u6784\u65e0\u5173\u7684\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u7684\u5185\u5b58\u53d7\u9650\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.11797", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11797", "abs": "https://arxiv.org/abs/2512.11797", "authors": ["Junjie Ye", "Rong Xue", "Basile Van Hoorick", "Pavel Tokmakov", "Muhammad Zubair Irshad", "Yue Wang", "Vitor Guizilini"], "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis", "comment": "Project page: https://jay-ye.github.io/AnchorDream/", "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.", "AI": {"tldr": "AnchorDream\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u8fd0\u52a8\u6e32\u67d3\u951a\u5b9a\u5b9e\u4f53\u7ea6\u675f\uff0c\u4ece\u5c11\u91cf\u6f14\u793a\u751f\u6210\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u6027\u80fd", "motivation": "\u5927\u89c4\u6a21\u591a\u6837\u5316\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u6536\u96c6\u662f\u6a21\u4eff\u5b66\u4e60\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u771f\u5b9e\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\uff0c\u6a21\u62df\u5668\u591a\u6837\u6027\u548c\u4fdd\u771f\u5ea6\u6709\u9650\u4e14\u5b58\u5728\u660e\u663e\u7684\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\u3002\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u53ea\u6539\u53d8\u89c6\u89c9\u5916\u89c2\u800c\u4e0d\u521b\u9020\u65b0\u884c\u4e3a\uff0c\u8981\u4e48\u5b58\u5728\u5b9e\u4f53\u4e0d\u4e00\u81f4\u5bfc\u81f4\u8fd0\u52a8\u4e0d\u81ea\u7136\u7684\u95ee\u9898", "method": "\u5f15\u5165AnchorDream\uff0c\u4e00\u79cd\u5b9e\u4f53\u611f\u77e5\u7684\u4e16\u754c\u6a21\u578b\uff0c\u91cd\u65b0\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u673a\u5668\u4eba\u6570\u636e\u5408\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u673a\u5668\u4eba\u8fd0\u52a8\u6e32\u67d3\u6761\u4ef6\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u951a\u5b9a\u5b9e\u4f53\u7ea6\u675f\u9632\u6b62\u5e7b\u89c9\uff0c\u540c\u65f6\u5408\u6210\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e00\u81f4\u7684\u5bf9\u8c61\u548c\u73af\u5883\u3002\u4ec5\u9700\u5c11\u91cf\u4eba\u7c7b\u9065\u64cd\u4f5c\u6f14\u793a\u5373\u53ef\u6269\u5c55\u4e3a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u65e0\u9700\u663e\u5f0f\u73af\u5883\u5efa\u6a21", "result": "\u5b9e\u9a8c\u8868\u660e\u751f\u6210\u7684\u6570\u636e\u5728\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u4e2d\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\uff1a\u5728\u6a21\u62df\u5668\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u5bf9\u63d0\u534736.4%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7814\u7a76\u4e2d\u6027\u80fd\u63d0\u5347\u8fd1\u4e00\u500d\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u5c06\u751f\u6210\u4e16\u754c\u6a21\u578b\u57fa\u4e8e\u673a\u5668\u4eba\u8fd0\u52a8\u4e3a\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84", "conclusion": "\u5c06\u751f\u6210\u4e16\u754c\u6a21\u578b\u57fa\u4e8e\u673a\u5668\u4eba\u8fd0\u52a8\u4e3a\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002AnchorDream\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2512.11437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11437", "abs": "https://arxiv.org/abs/2512.11437", "authors": ["Akash Ghosh", "Srivarshinee Sridhar", "Raghav Kaushik Ravi", "Muhsin Muhsin", "Sriparna Saha", "Chirag Agarwal"], "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare", "comment": "49 pages, 31 figures", "summary": "Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.", "AI": {"tldr": "CLINIC\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u8bed\u8a00\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u53ef\u4fe1\u5ea6\uff0c\u6db5\u76d65\u4e2a\u5173\u952e\u7ef4\u5ea6\u300118\u4e2a\u4efb\u52a1\u548c15\u79cd\u8bed\u8a00\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u504f\u89c1\u3001\u9690\u79c1\u7b49\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u8bad\u7ec3\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u533b\u7597\u67e5\u8be2\u590d\u6742\u6027\uff0c\u963b\u788d\u4e86\u5176\u5728\u5168\u7403\u591a\u8bed\u8a00\u533b\u7597\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u53ef\u9760\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1CLINIC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u76845\u4e2a\u53ef\u4fe1\u5ea6\u7ef4\u5ea6\uff1a\u771f\u5b9e\u6027\u3001\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u548c\u9690\u79c1\u6027\uff0c\u901a\u8fc718\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u8986\u76d615\u79cd\u8bed\u8a00\u548c\u591a\u4e2a\u533b\u7597\u4e3b\u9898\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u5b9e\u6b63\u786e\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u8bed\u8a00\u7fa4\u4f53\u4e2d\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u5bb9\u6613\u53d7\u5230\u9690\u79c1\u6cc4\u9732\u548c\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\u3002", "conclusion": "CLINIC\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u591a\u8bed\u8a00\u533b\u7597\u73af\u5883\u4e2d\u7684\u8986\u76d6\u8303\u56f4\u548c\u5b89\u5168\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2512.11345", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11345", "abs": "https://arxiv.org/abs/2512.11345", "authors": ["Minwoo Park", "Junwoo Chang", "Jongeun Choi", "Roberto Horowitz"], "title": "Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits", "comment": null, "summary": "Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u6269\u6563\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\uff0c\u5c06\u7b49\u53d8\u6269\u6563\u7b56\u7565\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u5bf9\u79f0\u6027\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u7b49\u53d8\u6269\u6563\u7b56\u7565\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548c\u51e0\u4f55\u5bf9\u79f0\u6027\u7684\u6cdb\u5316\u4f18\u52bf\uff0c\u4f46\u76f4\u63a5\u7528\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\u4f1a\u5ffd\u7565\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u548c\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u5bf9\u79f0\u6027\u7684\u5f15\u5bfc\u6846\u67b6\u3002", "method": "\u7406\u8bba\u8bc1\u660e\u7b49\u53d8\u6269\u6563\u8fc7\u7a0b\u7684\u7b49\u53d8\u6027\uff0c\u63a8\u5bfc\u51fa\u7fa4\u4e0d\u53d8\u6f5c\u5728\u566a\u58f0MDP\uff0c\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u6bd4\u8f83\u6807\u51c6\u3001\u7b49\u53d8\u548c\u8fd1\u4f3c\u7b49\u53d8\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5229\u7528\u5bf9\u79f0\u6027\u8fdb\u884c\u5f15\u5bfc\u80fd\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3001\u9632\u6b62\u4ef7\u503c\u53d1\u6563\uff0c\u5373\u4f7f\u5728\u6f14\u793a\u6570\u636e\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u5f3a\u5927\u7684\u7b56\u7565\u6539\u8fdb\uff0c\u540c\u65f6\u786e\u5b9a\u4e86\u4e25\u683c\u7b49\u53d8\u6027\u5728\u5bf9\u79f0\u6027\u7834\u574f\u4e0b\u7684\u5b9e\u9645\u8fb9\u754c\u3002", "conclusion": "\u5bf9\u79f0\u6027\u611f\u77e5\u7684\u5f15\u5bfc\u6846\u67b6\u4e3a\u7b49\u53d8\u6269\u6563\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5728\u4fdd\u6301\u5bf9\u79f0\u6027\u4f18\u52bf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7b56\u7565\u6539\u8fdb\u3002"}}
{"id": "2512.11502", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11502", "abs": "https://arxiv.org/abs/2512.11502", "authors": ["Kai Golan Hashiloni", "Brenda Kasabe Nokai", "Michal Shevach", "Esthy Shemesh", "Ronit Bartin", "Anna Bergrin", "Liran Harel", "Nachum Dershowitz", "Liat Nadai Arad", "Kfir Bar"], "title": "Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction", "comment": "In Proceedings of the Workshop on Large Language Models and Generative AI for Health Informatics 2025, IJCAI 2025, Montreal, Canada", "summary": "We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eDictaBERT 2.0\u7684\u5e0c\u4f2f\u6765\u8bed\u533b\u7597\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u65f6\u95f4\u7ebf\uff0c\u6784\u5efa\u60a3\u8005\u65c5\u7a0b\u3002\u6a21\u578b\u901a\u8fc7500\u4e07\u4efd\u53bb\u6807\u8bc6\u5316\u533b\u9662\u8bb0\u5f55\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5728\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9700\u8981\u4ece\u5e0c\u4f2f\u6765\u8bed\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u65f6\u95f4\u7ebf\u6765\u6784\u5efa\u60a3\u8005\u65c5\u7a0b\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5e0c\u4f2f\u6765\u8bed\u533b\u7597\u6587\u672c\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8eDictaBERT 2.0\u67b6\u6784\uff0c\u5728\u8d85\u8fc7500\u4e07\u4efd\u53bb\u6807\u8bc6\u5316\u533b\u9662\u8bb0\u5f55\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u3002\u5f15\u5165\u8bcd\u6c47\u9002\u5e94\u6280\u672f\u63d0\u9ad8\u6807\u8bb0\u6548\u7387\uff0c\u5e76\u521b\u5efa\u4e86\u4e24\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff08\u5185\u79d1/\u6025\u8bca\u79d1\u548c\u80bf\u7624\u79d1\uff09\u7528\u4e8e\u8bc4\u4f30\u4e8b\u4ef6\u65f6\u95f4\u5173\u7cfb\u63d0\u53d6\u3002", "result": "\u6a21\u578b\u5728\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\u3002\u8bcd\u6c47\u9002\u5e94\u63d0\u9ad8\u4e86\u6807\u8bb0\u6548\u7387\uff0c\u53bb\u6807\u8bc6\u5316\u5904\u7406\u4e0d\u4f1a\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u6a21\u578b\u5f00\u53d1\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5e0c\u4f2f\u6765\u8bed\u533b\u7597\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u4e34\u5e8a\u65f6\u95f4\u7ebf\u3002\u6a21\u578b\u5728\u4f26\u7406\u9650\u5236\u4e0b\u53ef\u4f9b\u7814\u7a76\u4f7f\u7528\uff0c\u8bc1\u660e\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u53ef\u4ee5\u517c\u987e\u3002"}}
{"id": "2512.11534", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11534", "abs": "https://arxiv.org/abs/2512.11534", "authors": ["Yiqing Yang", "Kin-Man Lam"], "title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning", "comment": "18 pages, 8 figures", "summary": "Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u5e27\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u67e5\u8be2\u5411\u91cf\uff0c\u7ed3\u5408\u96c6\u5408\u7ea7\u4f18\u5316\u548c\u5e08\u751f\u4e92\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd", "motivation": "\u4f20\u7edf\u89c6\u9891\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u72ec\u7acb\u8bc4\u5206\u65b9\u6cd5\u5bfc\u81f4\u9009\u62e9\u5e27\u5728\u65f6\u95f4\u4e0a\u805a\u96c6\u4e14\u89c6\u89c9\u5197\u4f59\uff1b2\uff09\u4f7f\u7528MLLM\u751f\u6210\u7684\u9759\u6001\u4f2a\u6807\u7b7e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9009\u62e9\u5668\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u4efb\u52a1\u76ee\u6807", "method": "1\uff09\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u5f15\u5bfc\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u9690\u5f0f\u67e5\u8be2\u5411\u91cf\uff1b2\uff09\u5b9a\u4e49\u5305\u542b\u76f8\u5173\u6027\u3001\u8986\u76d6\u7387\u548c\u5197\u4f59\u5ea6\u7684\u8fde\u7eed\u96c6\u5408\u7ea7\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7Gumbel-Softmax\u8fdb\u884c\u53ef\u5fae\u5206\u4f18\u5316\uff1b3\uff09\u91c7\u7528\u5e08\u751f\u4e92\u5b66\u4e60\uff0c\u901a\u8fc7KL\u6563\u5ea6\u5bf9\u9f50\u5b66\u751f\u9009\u62e9\u5668\uff08SLM\uff09\u548c\u6559\u5e08\u63a8\u7406\u5668\uff08MLLM\uff09\u7684\u5e27\u91cd\u8981\u6027\u5206\u5e03", "result": "\u5728Video-MME\u3001LongVideoBench\u3001MLVU\u548cNExT-QA\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u4e86\u4f20\u7edf\u5e27\u9009\u62e9\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u4efb\u52a1\u76ee\u6807\u3001\u96c6\u5408\u7ea7\u4f18\u5316\u548c\u5e08\u751f\u4e92\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5173\u952e\u5e27\u9009\u62e9\u6027\u80fd"}}
{"id": "2512.11526", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11526", "abs": "https://arxiv.org/abs/2512.11526", "authors": ["Joel Ekstrand", "Zahra Taghiyarrenani", "Slawomir Nowaczyk"], "title": "Contrastive Time Series Forecasting with Anomalies", "comment": null, "summary": "Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.", "AI": {"tldr": "Co-TSFA\u662f\u4e00\u4e2a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u77ed\u671f\u5f02\u5e38\uff08\u5e94\u5ffd\u7565\uff09\u548c\u6301\u4e45\u6027\u5f02\u5e38\uff08\u5e94\u54cd\u5e94\uff09\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5728\u5f02\u5e38\u6761\u4ef6\u4e0b\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u6b63\u5e38\u6570\u636e\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u6709\u4e9b\u5f02\u5e38\u4e8b\u4ef6\u5177\u6709\u6301\u4e45\u5f71\u54cd\u9700\u8981\u54cd\u5e94\uff0c\u6709\u4e9b\u5219\u662f\u77ed\u671f\u566a\u58f0\u5e94\u8be5\u5ffd\u7565\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u8fd9\u4e24\u7c7b\u5f02\u5e38\uff0c\u8981\u4e48\u5bf9\u566a\u58f0\u8fc7\u5ea6\u53cd\u5e94\uff0c\u8981\u4e48\u9519\u8fc7\u91cd\u8981\u7684\u5206\u5e03\u53d8\u5316\u3002", "method": "\u63d0\u51faCo-TSFA\u5bf9\u6bd4\u5b66\u4e60\u6b63\u5219\u5316\u6846\u67b6\uff1a1\uff09\u751f\u6210\u4ec5\u8f93\u5165\u589e\u5f3a\u548c\u8f93\u5165-\u8f93\u51fa\u589e\u5f3a\uff0c\u5206\u522b\u5efa\u6a21\u9884\u6d4b\u65e0\u5173\u548c\u9884\u6d4b\u76f8\u5173\u7684\u5f02\u5e38\uff1b2\uff09\u5f15\u5165\u6f5c\u5728\u8f93\u51fa\u5bf9\u9f50\u635f\u5931\uff0c\u5c06\u8868\u793a\u53d8\u5316\u4e0e\u9884\u6d4b\u53d8\u5316\u5173\u8054\uff1b3\uff09\u9f13\u52b1\u5bf9\u65e0\u5173\u6270\u52a8\u4fdd\u6301\u4e0d\u53d8\u6027\uff0c\u540c\u65f6\u5bf9\u6709\u610f\u4e49\u7684\u5206\u5e03\u53d8\u5316\u4fdd\u6301\u654f\u611f\u6027\u3002", "result": "\u5728Traffic\u548cElectricity\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u53ca\u771f\u5b9e\u73b0\u91d1\u9700\u6c42\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCo-TSFA\u5728\u5f02\u5e38\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6b63\u5e38\u6570\u636e\u4e0a\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "Co-TSFA\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6709\u6548\u533a\u5206\u9884\u6d4b\u76f8\u5173\u548c\u65e0\u5173\u7684\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u5728\u5f02\u5e38\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5f02\u5e38\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.11465", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11465", "abs": "https://arxiv.org/abs/2512.11465", "authors": ["Mohamed Abdelsamad", "Michael Ulrich", "Bin Yang", "Miao Zhang", "Yakov Miron", "Abhinav Valada"], "title": "DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation", "comment": "AAAI-26", "summary": "Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.", "AI": {"tldr": "DOS\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u5728\u53ef\u89c2\u5bdf\u70b9\u84b8\u998f\u8bed\u4e49\u76f8\u5173\u6027\u8f6f\u6620\u5c04\u6765\u5b66\u4e603D\u70b9\u4e91\u8868\u793a\uff0c\u907f\u514d\u4fe1\u606f\u6cc4\u9732\u5e76\u63d0\u4f9b\u6bd4\u79bb\u6563token\u5206\u914d\u66f4\u4e30\u5bcc\u7684\u76d1\u7763\u3002", "motivation": "3D\u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u9762\u4e34\u51e0\u4f55\u4e0d\u89c4\u5219\u3001\u91cd\u5efa\u6613\u8d70\u6377\u5f84\u548c\u8bed\u4e49\u5206\u5e03\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDOS\u6846\u67b6\uff1a1\uff09\u4ec5\u5728\u53ef\u89c2\u5bdf\u70b9\u84b8\u998f\u8bed\u4e49\u76f8\u5173\u6027\u8f6f\u6620\u5c04\uff0c\u907f\u514d\u63a9\u7801\u533a\u57df\u4fe1\u606f\u6cc4\u9732\uff1b2\uff09\u5f15\u5165Zipfian\u539f\u578b\u548cZipf-Sinkhorn\u7b97\u6cd5\uff0c\u5f3a\u5236\u539f\u578b\u4f7f\u7528\u9075\u5faa\u5e42\u5f8b\u5206\u5e03\u5e76\u8c03\u8282\u76ee\u6807\u8f6f\u6620\u5c04\u7684\u9510\u5ea6\u3002", "result": "\u5728nuScenes\u3001Waymo\u3001SemanticKITTI\u3001ScanNet\u548cScanNet200\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDOS\u5728\u8bed\u4e49\u5206\u5272\u548c3D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u6807\u6ce8\u3002", "conclusion": "\u53ef\u89c2\u5bdf\u70b9\u8f6f\u6620\u5c04\u84b8\u998f\u4e3a\u5b66\u4e60\u9c81\u68d2\u76843D\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u8bed\u4e49\u4e0d\u5e73\u8861\u548c\u4fe1\u606f\u6cc4\u9732\u7b49\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2512.11510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11510", "abs": "https://arxiv.org/abs/2512.11510", "authors": ["Hanyue Lou", "Jiayi Zhou", "Yang Zhang", "Boyu Li", "Yi Wang", "Guangnan Ye", "Boxin Shi"], "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering", "comment": null, "summary": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.", "AI": {"tldr": "\u63d0\u51faFRT\u548cART\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u5c06\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5e76\u521b\u5efaEvQA\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u4e8b\u4ef6\u89c6\u89c9\u7406\u89e3\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u6311\u6218\u6027\u89c6\u89c9\u6761\u4ef6\u4e0b\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e0e\u57fa\u4e8e\u5e27\u7684\u6a21\u578b\u517c\u5bb9\u3002\u5f53\u524d\u7f3a\u4e4f\u5c06\u4e8b\u4ef6\u6570\u636e\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6548\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4e14\u7f3a\u5c11\u5ba2\u89c2\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u5e27\u7684\u91cd\u5efa\u4e0e\u6807\u8bb0\u5316(FRT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u4f5c\u4e3a\u6865\u6881\uff1b2) \u81ea\u9002\u5e94\u91cd\u5efa\u4e0e\u6807\u8bb0\u5316(ART)\u65b9\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u7a00\u758f\u6027\u63d0\u9ad8\u6548\u7387\u3002\u540c\u65f6\u521b\u5efaEvQA\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea22\u4e2a\u516c\u5171\u6570\u636e\u96c6\u76841000\u4e2a\u4e8b\u4ef6-Q&A\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728EvQA\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u4ef6\u89c6\u89c9\u7406\u89e3\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u91cd\u5efa\u4f5c\u4e3a\u6865\u6881\uff0c\u6210\u529f\u5c06\u4e8b\u4ef6\u76f8\u673a\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e8b\u4ef6\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11683", "abs": "https://arxiv.org/abs/2512.11683", "authors": ["Qiushi Guo"], "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection", "comment": null, "summary": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.", "AI": {"tldr": "\u63d0\u51faDepth Copy Paste\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u5236\u5168\u8eab\u4eba\u7269\u5b9e\u4f8b\u5e76\u7c98\u8d34\u5230\u8bed\u4e49\u517c\u5bb9\u7684\u573a\u666f\u4e2d\uff0c\u751f\u6210\u591a\u6837\u4e14\u7269\u7406\u4e00\u81f4\u7684\u4eba\u8138\u68c0\u6d4b\u8bad\u7ec3\u6837\u672c\u3002", "motivation": "\u4f20\u7edf\u590d\u5236\u7c98\u8d34\u589e\u5f3a\u65b9\u6cd5\u7531\u4e8e\u524d\u666f\u63d0\u53d6\u4e0d\u51c6\u786e\u3001\u573a\u666f\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u80cc\u666f\u8bed\u4e49\u4e0d\u5339\u914d\uff0c\u5f80\u5f80\u4ea7\u751f\u4e0d\u771f\u5b9e\u7684\u5408\u6210\u56fe\u50cf\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u6765\u63d0\u5347\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u5728\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "1) \u4f7f\u7528BLIP\u548cCLIP\u8054\u5408\u8bc4\u4f30\u8bed\u4e49\u548c\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u81ea\u52a8\u68c0\u7d22\u6700\u9002\u5408\u7ed9\u5b9a\u524d\u666f\u4eba\u7269\u7684\u80cc\u666f\u56fe\u50cf\uff1b2) \u96c6\u6210SAM3\u8fdb\u884c\u7cbe\u786e\u5206\u5272\uff0c\u4f7f\u7528Depth-Anything\u63d0\u53d6\u975e\u906e\u6321\u53ef\u89c1\u4eba\u7269\u533a\u57df\uff1b3) \u5f15\u5165\u6df1\u5ea6\u5f15\u5bfc\u6ed1\u52a8\u7a97\u53e3\u653e\u7f6e\u673a\u5236\uff0c\u5728\u80cc\u666f\u6df1\u5ea6\u56fe\u4e0a\u641c\u7d22\u5177\u6709\u6700\u4f73\u6df1\u5ea6\u8fde\u7eed\u6027\u548c\u5c3a\u5ea6\u5bf9\u9f50\u7684\u7c98\u8d34\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDepth Copy Paste\u80fd\u63d0\u4f9b\u66f4\u591a\u6837\u548c\u771f\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u76f8\u6bd4\u4f20\u7edf\u590d\u5236\u7c98\u8d34\u548c\u65e0\u6df1\u5ea6\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u4e0b\u6e38\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Depth Copy Paste\u901a\u8fc7\u591a\u6a21\u6001\u8bed\u4e49\u5339\u914d\u3001\u9ad8\u8d28\u91cf\u524d\u666f\u63d0\u53d6\u548c\u6df1\u5ea6\u611f\u77e5\u653e\u7f6e\uff0c\u751f\u6210\u4e86\u7269\u7406\u4e00\u81f4\u4e14\u89c6\u89c9\u903c\u771f\u7684\u5408\u6210\u56fe\u50cf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
