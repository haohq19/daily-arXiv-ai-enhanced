{"id": "2601.17310", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17310", "abs": "https://arxiv.org/abs/2601.17310", "authors": ["Yu Akagi", "Tomohisa Seki", "Hiromasa Ito", "Toru Takiguchi", "Kazuhiko Ohe", "Yoshimasa Kawazoe"], "title": "High-Fidelity Longitudinal Patient Simulation Using Real-World Data", "comment": null, "summary": "Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.", "AI": {"tldr": "\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5f00\u53d1\u751f\u6210\u5f0f\u6a21\u62df\u5668\uff0c\u80fd\u591f\u57fa\u4e8e\u60a3\u8005\u5386\u53f2\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u672a\u6765\u4e34\u5e8a\u8f68\u8ff9\uff0c\u4e3a\u4e2a\u6027\u5316\u6cbb\u7597\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\u63d0\u4f9b\u5de5\u5177\u3002", "motivation": "\u6a21\u62df\u5728\u4e34\u5e8a\u533b\u5b66\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u590d\u6742\u7684\u751f\u7269\u548c\u793e\u4f1a\u6587\u5316\u5f71\u54cd\uff0c\u6a21\u62df\u60a3\u8005\u8f68\u8ff9\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u8bb0\u5f55\u6765\u7ecf\u9a8c\u6027\u5730\u5efa\u6a21\u60a3\u8005\u65f6\u95f4\u7ebf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u5f0f\u6a21\u62df\u5668\u6a21\u578b\uff0c\u4ee5\u60a3\u8005\u5386\u53f2\u4e3a\u8f93\u5165\uff0c\u5408\u6210\u7ec6\u7c92\u5ea6\u3001\u771f\u5b9e\u7684\u672a\u6765\u8f68\u8ff9\u3002\u8be5\u6a21\u578b\u5728\u8d85\u8fc72\u4ebf\u6761\u4e34\u5e8a\u8bb0\u5f55\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u751f\u6210\u4e86\u9ad8\u4fdd\u771f\u7684\u672a\u6765\u65f6\u95f4\u7ebf\uff0c\u4e0e\u771f\u5b9e\u60a3\u8005\u672a\u6765\u6570\u636e\u4e2d\u7684\u4e8b\u4ef6\u53d1\u751f\u7387\u3001\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u7ed3\u679c\u548c\u65f6\u95f4\u52a8\u6001\u5bc6\u5207\u5339\u914d\u3002\u51c6\u786e\u4f30\u8ba1\u4e86\u672a\u6765\u4e8b\u4ef6\u6982\u7387\uff0c\u89c2\u5bdf\u4e0e\u9884\u671f\u6bd4\u7387\u5728\u4e0d\u540c\u7ed3\u679c\u548c\u65f6\u95f4\u8303\u56f4\u5185\u59cb\u7ec8\u63a5\u8fd11.0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u672a\u5f00\u53d1\u4ef7\u503c\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e34\u5e8a\u62a4\u7406\u7684\u8ba1\u7b97\u673a\u6a21\u62df\u5efa\u6a21\u3002"}}
{"id": "2601.17008", "categories": ["cs.LG", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.17008", "abs": "https://arxiv.org/abs/2601.17008", "authors": ["Haochong Xia", "Simin Li", "Ruixiao Xu", "Zhixia Zhang", "Hongxiang Wang", "Zhiqian Liu", "Teng Yao Long", "Molei Qin", "Chuqiao Zong", "Bo An"], "title": "Bayesian Robust Financial Trading with Adversarial Synthetic Market Data", "comment": null, "summary": "Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u9c81\u68d2\u6846\u67b6\uff0c\u7ed3\u5408\u5b8f\u89c2\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u9c81\u68d2\u7b56\u7565\u5b66\u4e60\uff0c\u89e3\u51b3\u7b97\u6cd5\u4ea4\u6613\u4e2d\u6a21\u578b\u5bf9\u5e02\u573a\u673a\u5236\u53d8\u5316\u9002\u5e94\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u4ea4\u6613\u6a21\u578b\u5728\u6837\u672c\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u771f\u5b9e\u5e02\u573a\u673a\u5236\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u73b0\u6709\u7b56\u7565\u5bf9\u9ad8\u7ea7\u5e02\u573a\u6ce2\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u771f\u5b9e\u591a\u6837\u7684\u6a21\u62df\u8bad\u7ec3\u73af\u5883\u5bfc\u81f4\u7b56\u7565\u8fc7\u62df\u5408\u3002", "method": "1) \u6570\u636e\u4fa7\uff1a\u63d0\u51fa\u5b8f\u89c2\u6761\u4ef6GAN\u751f\u6210\u5668\uff0c\u5229\u7528\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u4f5c\u4e3a\u4e3b\u8981\u63a7\u5236\u53d8\u91cf\uff0c\u5408\u6210\u5177\u6709\u771f\u5b9e\u65f6\u95f4\u3001\u8de8\u5de5\u5177\u548c\u5b8f\u89c2\u76f8\u5173\u6027\u7684\u6570\u636e\uff1b2) \u7b56\u7565\u4fa7\uff1a\u5c06\u4ea4\u6613\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u4e24\u4eba\u96f6\u548c\u8d1d\u53f6\u65af\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff0c\u5bf9\u6297\u4ee3\u7406\u901a\u8fc7\u6270\u52a8\u5b8f\u89c2\u6761\u4ef6\u751f\u6210\u5668\u4e2d\u7684\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u6765\u6a21\u62df\u673a\u5236\u53d8\u5316\uff0c\u4ea4\u6613\u4ee3\u7406\u901a\u8fc7\u5206\u4f4d\u6570\u4fe1\u5ff5\u7f51\u7edc\u7ef4\u62a4\u548c\u66f4\u65b0\u5bf9\u9690\u85cf\u5e02\u573a\u72b6\u6001\u7684\u4fe1\u5ff5\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u865a\u62df\u81ea\u535a\u5f08\u5bfb\u6c42\u9c81\u68d2\u5b8c\u7f8e\u8d1d\u53f6\u65af\u5747\u8861\u3002", "result": "\u57289\u79cd\u91d1\u878d\u5de5\u5177\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e9\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728COVID\u7b49\u6781\u7aef\u4e8b\u4ef6\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u6539\u8fdb\u7684\u76c8\u5229\u80fd\u529b\u548c\u98ce\u9669\u7ba1\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0d\u786e\u5b9a\u548c\u53d8\u5316\u7684\u5e02\u573a\u52a8\u6001\u4e0b\u7684\u4ea4\u6613\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u5b8f\u89c2\u6761\u4ef6\u751f\u6210\u6a21\u578b\u548c\u9c81\u68d2\u7b56\u7565\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7b97\u6cd5\u4ea4\u6613\u4e2d\u7684\u6a21\u578b\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2601.17065", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17065", "abs": "https://arxiv.org/abs/2601.17065", "authors": ["Haoxuan Li", "He Chang", "Yunshan Ma", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting", "comment": null, "summary": "Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.", "AI": {"tldr": "\u63d0\u51faThinkTank-ME\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u667a\u5e93\u591a\u4e13\u5bb6\u534f\u4f5c\u5206\u6790\u6765\u6539\u8fdb\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\uff0c\u6784\u5efaPOLECAT-FOR-ME\u57fa\u51c6\u9a8c\u8bc1\u591a\u4e13\u5bb6\u534f\u4f5c\u4f18\u4e8e\u5355\u6a21\u578b\u65b9\u6cd5", "motivation": "\u73b0\u6709LLM\u4e8b\u4ef6\u9884\u6d4b\u65b9\u6cd5\u91c7\u7528\u5355\u6a21\u578b\u67b6\u6784\uff0c\u53ea\u80fd\u751f\u6210\u5355\u4e00\u660e\u786e\u8f68\u8ff9\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u533a\u57df\u80cc\u666f\u4e0b\u591a\u6837\u5316\u7684\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u3002\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\u9700\u8981\u8003\u8651\u56fd\u9645\u5173\u7cfb\u3001\u533a\u57df\u5386\u53f2\u52a8\u6001\u548c\u6587\u5316\u80cc\u666f\u7b49\u591a\u65b9\u9762\u56e0\u7d20\u3002", "method": "\u63d0\u51faThinkTank-ME\u6846\u67b6\uff0c\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u6218\u7565\u51b3\u7b56\u4e2d\u7684\u667a\u5e93\u534f\u4f5c\u4e13\u5bb6\u5206\u6790\u3002\u6784\u5efaPOLECAT-FOR-ME\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u4e13\u5bb6\u534f\u4f5c\u5728\u5904\u7406\u590d\u6742\u65f6\u95f4\u6027\u5730\u7f18\u653f\u6cbb\u9884\u6d4b\u4efb\u52a1\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u591a\u4e13\u5bb6\u534f\u4f5c\u6846\u67b6\u80fd\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u533a\u57df\u80cc\u666f\u4e0b\u7684\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.17556", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17556", "abs": "https://arxiv.org/abs/2601.17556", "authors": ["Ulices Santa Cruz", "Mahmoud Elfar", "Yasser Shoukry"], "title": "Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models", "comment": null, "summary": "We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u8ba4\u8bc1\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u5efa\u6a21\u4e0e\u5b66\u4e60\u4f30\u8ba1\uff0c\u5728\u6742\u4e71\u73af\u5883\u4e2d\u63d0\u4f9b\u8bef\u5dee\u4fdd\u8bc1\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u53ef\u8bc1\u660e\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u8fd9\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u4e3a\u57fa\u4e8e\u611f\u77e5\u7684\u4f4d\u59ff\u4f30\u8ba1\u8bbe\u8ba1\u53ef\u8ba4\u8bc1\u7684\u795e\u7ecf\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u751f\u6210\u6a21\u578b(GGM)\uff0c\u5176\u53c2\u6570\u6765\u81ea\u76ee\u6807\u7269\u4f53\u5728\u76f8\u673a\u4e2d\u7684\u6210\u50cf\u8fc7\u7a0b\u3002\u4f7f\u7528GGM\u8bad\u7ec3\u5177\u6709\u8ba4\u8bc1\u4fdd\u8bc1\u7684\u795e\u7ecf\u7f51\u7edc\u4f4d\u59ff\u4f30\u8ba1\u5668\uff0c\u5e76\u6269\u5c55\u5230\u6742\u4e71\u73af\u5883\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u6784\u5efa\u591a\u9636\u6bb5\u611f\u77e5\u7ba1\u9053\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e8b\u4ef6\u76f8\u673a\u6355\u83b7\u7684\u4ea4\u901a\u6807\u5fd7\u56fe\u50cf\u3002\u8bad\u7ec3\u540e\u7684\u7f16\u7801\u5668\u80fd\u6709\u6548\u4f30\u8ba1\u4f4d\u59ff\uff0c\u7b26\u5408\u6846\u67b6\u63d0\u4f9b\u7684\u8ba4\u8bc1\u8fb9\u754c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u7269\u7406\u9a71\u52a8\u5efa\u6a21\u4e0e\u5b66\u4e60\u4f30\u8ba1\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53ef\u8ba4\u8bc1\u7684\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6742\u4e71\u73af\u5883\u4e2d\u4fdd\u6301\u8ba4\u8bc1\u4fdd\u8bc1\u3002"}}
{"id": "2601.17481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17481", "abs": "https://arxiv.org/abs/2601.17481", "authors": ["Emily Broadhurst", "Tawab Safi", "Joseph Edell", "Vashisht Ganesh", "Karime Maamari"], "title": "Lattice: Generative Guardrails for Conversational Agents", "comment": null, "summary": "Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.", "AI": {"tldr": "Lattice\u6846\u67b6\u901a\u8fc7\u81ea\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u673a\u5236\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u521b\u5efa\u81ea\u9002\u5e94\u9632\u62a4\u680f\uff0c\u76f8\u6bd4\u9759\u6001\u89c4\u5219\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u9632\u62a4\u680f\u4f7f\u7528\u9759\u6001\u89c4\u5219\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u5a01\u80c1\u548c\u90e8\u7f72\u73af\u5883\u53d8\u5316\uff0c\u9700\u8981\u80fd\u591f\u81ea\u6211\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u7684\u9632\u62a4\u6846\u67b6", "method": "Lattice\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u6784\u5efa\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u548c\u4f18\u5316\u4ece\u6807\u6ce8\u793a\u4f8b\u521b\u5efa\u521d\u59cb\u9632\u62a4\u680f\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u3001\u5bf9\u6297\u6d4b\u8bd5\u548c\u6574\u5408\u81ea\u4e3b\u9002\u5e94\u5df2\u90e8\u7f72\u9632\u62a4\u680f", "result": "\u5728ProsocialDialog\u6570\u636e\u96c6\u4e0a\uff0cLattice\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fbe\u523091% F1\u5206\u6570\uff0c\u6bd4\u5173\u952e\u8bcd\u57fa\u7ebf\u9ad843\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4LlamaGuard\u9ad825\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4NeMo\u9ad84\u4e2a\u767e\u5206\u70b9\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u5728\u8de8\u57df\u6570\u636e\u4e0a\u5b9e\u73b07\u4e2a\u767e\u5206\u70b9F1\u63d0\u5347", "conclusion": "Lattice\u6846\u67b6\u8bc1\u660e\u6709\u6548\u7684\u9632\u62a4\u680f\u53ef\u4ee5\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u81ea\u6211\u6784\u5efa\uff0c\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u63d0\u4f9b\u81ea\u9002\u5e94\u3001\u6301\u7eed\u6539\u8fdb\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236"}}
{"id": "2601.17050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17050", "abs": "https://arxiv.org/abs/2601.17050", "authors": ["Hongjun An", "Yiliang Song", "Jiawei Shao", "Zhe Sun", "Xuelong Li"], "title": "Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence", "comment": "Initial Version, Pending Updates. We welcome any feedback and suggestions for improvement. Please feel free to contact us at an.hongjun@foxmail.com", "summary": "Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.", "AI": {"tldr": "SP-VLM\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u50cf\u7d20\u4f20\u611f\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u9690\u79c1\u4fdd\u62a4\u76d1\u63a7\u6846\u67b6\uff0c\u80fd\u5728\u4fdd\u62a4\u4e2a\u4eba\u8eab\u4efd\u7684\u540c\u65f6\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a", "motivation": "\u5728\u5395\u6240\u3001\u66f4\u8863\u5ba4\u7b49\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u76d1\u63a7\u56e0\u9690\u79c1\u6cd5\u89c4\u548c\u4f26\u7406\u95ee\u9898\u53d7\u9650\uff0c\u4f46\u6b3a\u51cc\u3001\u9a9a\u6270\u7b49\u4e0d\u826f\u793e\u4f1a\u4e92\u52a8\u53c8\u9700\u8981\u53ca\u65f6\u5e72\u9884\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u786e\u4fdd\u5b89\u5168\u7684\u76d1\u63a7\u65b9\u6848", "method": "\u63d0\u51fa\u5355\u50cf\u7d20\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(SP-VLM)\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u7ef4\u5355\u50cf\u7d20\u6a21\u6001\u6355\u6349\u4eba\u4f53\u52a8\u6001\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u96c6\u6210\u63a8\u65ad\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u8bbe\u8ba1", "result": "\u5355\u50cf\u7d20\u4f20\u611f\u80fd\u6709\u6548\u6291\u5236\u8eab\u4efd\u53ef\u6062\u590d\u6027\uff0c\u4f7f\u5148\u8fdb\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u4f4e\u4e8e\u4e34\u754c\u91c7\u6837\u7387\u65f6\u5931\u6548\uff1b\u540c\u65f6SP-VLM\u80fd\u4ece\u4e25\u91cd\u964d\u7ea7\u7684\u5355\u50cf\u7d20\u89c2\u6d4b\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u3001\u4eba\u6570\u7edf\u8ba1\u548c\u6d3b\u52a8\u7406\u89e3", "conclusion": "SP-VLM\u5728\u7279\u5b9a\u91c7\u6837\u7387\u8303\u56f4\u5185\u65e2\u80fd\u63d0\u53d6\u884c\u4e3a\u667a\u80fd\u53c8\u80fd\u5f3a\u4fdd\u62a4\u4e2a\u4eba\u8eab\u4efd\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7a7a\u95f4\u7684\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u6761\u4eba\u6743\u5bf9\u9f50\u7684\u8def\u5f84\uff0c\u652f\u6301\u53ca\u65f6\u5e72\u9884\u800c\u4e0d\u4fb5\u72af\u9690\u79c1"}}
{"id": "2601.17991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17991", "abs": "https://arxiv.org/abs/2601.17991", "authors": ["Roman Akinshin", "Elizaveta Lopatina", "Kirill Bogatikov", "Nikolai Kiz", "Anna V. Makarova", "Mikhail Lebedev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou", "Valerii Kangler"], "title": "NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi", "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8868\u9762\u808c\u7535\u4fe1\u53f7\u4e0e\u89c6\u89c9\u5f15\u5bfc\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u7528\u4e8e\u4e0a\u80a2\u5047\u80a2\uff0c\u5728\u4f4e\u529f\u8017\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u624b\u52bf\u8bc6\u522b", "motivation": "\u4f20\u7edf\u808c\u7535\u5047\u80a2\u63a7\u5236\u5b58\u5728\u529f\u8017\u9ad8\u3001\u7f3a\u4e4f\u60c5\u5883\u611f\u77e5\u3001\u5b89\u5168\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u8868\u9762\u808c\u7535\u4fe1\u53f7\u5206\u7c7b\u7684\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\u5728AltAi\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e0a\uff0c\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\u548c\u573a\u666f\u6444\u50cf\u5934\u8bc6\u522b\u7528\u6237\u6ce8\u89c6\u7269\u4f53", "result": "\u7cfb\u7edf\u57286\u79cd\u624b\u52bf\u8bc6\u522b\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\uff0c\u7ed3\u5408\u89c6\u89c9\u60c5\u5883\u9650\u5236\u540e\u51c6\u786e\u7387\u63d0\u5347\u81f3\u7ea695%\uff0c\u529f\u8017\u4f4e\u4e8e1\u74e6", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u5f62\u6001\u60c5\u5883\u611f\u77e5\u63a7\u5236\u5668\u80fd\u63d0\u4f9b\u8282\u80fd\u53ef\u9760\u7684\u5047\u80a2\u63a7\u5236\uff0c\u6709\u671b\u6539\u5584\u4e0a\u80a2\u622a\u80a2\u8005\u65e5\u5e38\u6d3b\u52a8\u7684\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027"}}
{"id": "2601.18442", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18442", "abs": "https://arxiv.org/abs/2601.18442", "authors": ["Hongyi Zhao", "Shuo Wang", "Qijie He", "Ziyuan Pu"], "title": "SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation", "comment": null, "summary": "Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.", "AI": {"tldr": "SG-CADVLM\u6846\u67b6\u5229\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u548c\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\uff0c\u4ece\u4e8b\u6545\u62a5\u544a\u548c\u9053\u8def\u7f51\u7edc\u56fe\u4e2d\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u98ce\u9669\u573a\u666f\u751f\u6210\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u9700\u8981\u6d4b\u8bd5\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u4f46\u8fd9\u4e9b\u4e8b\u4ef6\u5728\u771f\u5b9e\u9a7e\u9a76\u4e2d\u7f55\u89c1\u4e14\u6d4b\u8bd5\u6210\u672c\u9ad8\u3002\u4e8b\u6545\u62a5\u544a\u63d0\u4f9b\u4e86\u771f\u5b9e\u7684\u5b89\u5168\u4e8b\u4ef6\u89c4\u683c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u5bf9\u6297\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u771f\u5b9e\u6027\uff0cLLM/VLM\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u6291\u5236\u95ee\u9898\u5bfc\u81f4\u504f\u79bb\u4e8b\u6545\u7279\u5f81\u3002", "method": "\u63d0\u51faSG-CADVLM\u6846\u67b6\uff0c\u96c6\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u4e0e\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\uff0c\u4ece\u4e8b\u6545\u62a5\u544a\u548c\u9053\u8def\u7f51\u7edc\u56fe\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\u3002\u8be5\u6846\u67b6\u7f13\u89e3\u4e86VLM\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u9053\u8def\u51e0\u4f55\u548c\u8f66\u8f86\u8f68\u8ff9\u7684\u540c\u6b65\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSG-CADVLM\u751f\u6210\u5173\u952e\u98ce\u9669\u573a\u666f\u7684\u6bd4\u4f8b\u8fbe\u523084.4%\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4ec5\u4e3a12.5%\uff0c\u63d0\u5347\u4e86469%\u3002\u540c\u65f6\u80fd\u591f\u751f\u6210\u53ef\u6267\u884c\u7684\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u4eff\u771f\u573a\u666f\u3002", "conclusion": "SG-CADVLM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u548c\u591a\u6a21\u6001\u5904\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u573a\u666f\u751f\u6210\u7684\u771f\u5b9e\u6027\u548c\u53ef\u7528\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.17103", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17103", "abs": "https://arxiv.org/abs/2601.17103", "authors": ["Pascaline Andr\u00e9", "Charles Heitz", "Evangelia Christodoulou", "Annika Reinke", "Carole H. Sudre", "Michela Antonelli", "Patrick Godau", "M. Jorge Cardoso", "Antoine Gilson", "Sophie Tezenas du Montcel", "Ga\u00ebl Varoquaux", "Lena Maier-Hein", "Olivier Colliot"], "title": "Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals", "comment": null, "summary": "Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u8bc4\u4f30\u4e2d\u7684\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u7f6e\u4fe1\u533a\u95f4\u53ef\u9760\u6027\u548c\u7cbe\u786e\u5ea6\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u5236\u5b9a\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u6307\u5357\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "motivation": "\u533b\u5b66\u5f71\u50cfAI\u9700\u8981\u53ef\u9760\u7684\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u652f\u6301\u4e34\u5e8a\u8f6c\u5316\uff0c\u4f46\u793e\u533a\u5bf9\u591a\u79cd\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u53ca\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u5bf924\u4e2a\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7ec4\u4f7f\u752819\u4e2a\u8bad\u7ec3\u6a21\u578b\uff0c\u6db5\u76d6\u5e38\u7528\u6027\u80fd\u6307\u6807\u3001\u591a\u79cd\u805a\u5408\u7b56\u7565\u548c\u5e7f\u6cdb\u91c7\u7528\u7684\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5404\u65b9\u6cd5\u7684\u8986\u76d6\u7387\u548c\u5bbd\u5ea6\u3002", "result": "\u53d1\u73b0\u4e94\u4e2a\u4e3b\u8981\u7ed3\u679c\uff1a1)\u53ef\u9760\u7f6e\u4fe1\u533a\u95f4\u6240\u9700\u6837\u672c\u91cf\u4ece\u51e0\u5341\u5230\u51e0\u5343\u4e0d\u7b49\uff1b2)\u6027\u80fd\u6307\u6807\u9009\u62e9\u5f3a\u70c8\u5f71\u54cd\u7f6e\u4fe1\u533a\u95f4\u884c\u4e3a\uff1b3)\u805a\u5408\u7b56\u7565\u663e\u8457\u5f71\u54cd\u53ef\u9760\u6027\uff1b4)\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7c7b\u578b\u8c03\u8282\u8fd9\u4e9b\u6548\u5e94\uff1b5)\u4e0d\u540c\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u5728\u4e0d\u540c\u7528\u4f8b\u4e2d\u53ef\u9760\u6027\u4e0d\u540c\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u4e3a\u5236\u5b9a\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u7684\u672a\u6765\u6307\u5357\u63d0\u4f9b\u4e86\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u5177\u4f53\u7814\u7a76\u53c2\u6570\u9009\u62e9\u9002\u5f53\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.17767", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17767", "abs": "https://arxiv.org/abs/2601.17767", "authors": ["Rajan Das Gupta", "Xiaobin Wu", "Xun Liu", "Jiaqi He"], "title": "HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis", "comment": "Accepted and published in the 2025 4th International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)", "summary": "Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\uff08CNN\u3001LSTM\uff09\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08KNN\u3001XGB\uff09\u7684\u6df7\u5408\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5fc3\u8840\u7ba1\u75be\u75c5\u9884\u6d4b\uff0c\u5728\u4e24\u4e2aKaggle\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523082.30%\u548c97.10%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u4ea1\u539f\u56e0\uff0c\u9700\u8981\u667a\u80fd\u6570\u636e\u9a71\u52a8\u7684\u8bca\u65ad\u5de5\u5177\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u548c\u590d\u6742\u751f\u7406\u6a21\u5f0f\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u96c6\u6210\u6846\u67b6\uff0c\u6574\u5408CNN\u548cLSTM\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e0eKNN\u548cXGB\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u91c7\u7528\u96c6\u6210\u6295\u7968\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u7f51\u7edc\u7684\u8868\u5f81\u80fd\u529b\u548c\u4f20\u7edf\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00Kaggle\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u6570\u636e\u96c6I\u8fbe\u523082.30%\u51c6\u786e\u7387\uff0c\u6570\u636e\u96c6II\u8fbe\u523097.10%\u51c6\u786e\u7387\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u6709\u7a33\u5b9a\u63d0\u5347\u3002", "conclusion": "\u6df7\u5408AI\u6846\u67b6\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u4e34\u5e8a\u6f5c\u529b\uff0c\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u68073\uff08\u826f\u597d\u5065\u5eb7\u4e0e\u798f\u7949\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u6570\u636e\u9a71\u52a8\u533b\u7597\u89e3\u51b3\u65b9\u6848\u4fc3\u8fdb\u65e9\u671f\u8bca\u65ad\u3001\u9884\u9632\u548c\u7ba1\u7406\u975e\u4f20\u67d3\u6027\u75be\u75c5\u3002"}}
{"id": "2601.17183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17183", "abs": "https://arxiv.org/abs/2601.17183", "authors": ["Farzam Asad", "Junaid Saif Khan", "Maria Tariq", "Sundus Munir", "Muhammad Adnan Khan"], "title": "Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data", "comment": "27 pages, 7 figures, 4 tables", "summary": "Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u5728\u5fc3\u810f\u75c5\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6a21\u62df\u56db\u5bb6\u5f02\u6784\u533b\u9662\u7684\u6570\u636e\u5206\u5e03\uff0c\u9a8c\u8bc1FedProx\u7b97\u6cd5\u5728\u975eIID\u533b\u7597\u6570\u636e\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u83b7\u5f97\u6bd4\u96c6\u4e2d\u5f0f\u548c\u5b64\u7acb\u8bad\u7ec3\u66f4\u597d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u533b\u7597\u6570\u636e\u56e0\u9690\u79c1\u6cd5\u89c4\uff08\u5982HIPAA\u548cGDPR\uff09\u65e0\u6cd5\u76f4\u63a5\u5171\u4eab\uff0c\u4f46\u8054\u90a6\u5b66\u4e60\u80fd\u5b9e\u73b0\u534f\u4f5c\u8bad\u7ec3\u800c\u4e0d\u96c6\u4e2d\u539f\u59cb\u6570\u636e\u3002\u7136\u800c\u4e34\u5e8a\u6570\u636e\u5929\u7136\u5177\u6709\u975eIID\u7279\u6027\uff08\u4eba\u53e3\u5dee\u5f02\u3001\u75be\u75c5\u6d41\u884c\u5ea6\u3001\u673a\u6784\u5b9e\u8df5\u5dee\u5f02\uff09\uff0c\u9700\u8981\u6709\u6548\u5904\u7406\u5f02\u6784\u6027\u7684\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528UCI\u5fc3\u810f\u75c5\u6570\u636e\u96c6\uff08Cleveland Clinic\uff0c303\u540d\u60a3\u8005\uff09\uff0c\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5206\u5c42\u6a21\u62df\u56db\u5bb6\u5f02\u6784\u533b\u9662\u5ba2\u6237\u7aef\u7684\u975eIID\u6570\u636e\u5206\u533a\u3002\u91c7\u7528\u8054\u90a6\u8fd1\u7aef\u4f18\u5316\uff08FedProx\uff09\u7b97\u6cd5\uff0c\u8bbe\u7f6e\u8fd1\u7aef\u53c2\u6570mu=0.05\uff0c\u8fdb\u884c50\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u5e7f\u6cdb\u6d88\u878d\u7814\u7a76\u3002", "result": "FedProx\uff08mu=0.05\uff09\u8fbe\u523085.00%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u5b66\u4e60\uff0883.33%\uff09\u548c\u5b64\u7acb\u672c\u5730\u6a21\u578b\uff08\u5e73\u574778.45%\uff09\u3002\u5b9e\u9a8c\u8bc1\u660e\u8fd1\u7aef\u6b63\u5219\u5316\u80fd\u6709\u6548\u6291\u5236\u5f02\u6784\u73af\u5883\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u4e14\u4e0d\u6cc4\u9732\u60a3\u8005\u9690\u79c1\u3002", "conclusion": "\u8fd9\u9879\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u4e3a\u73b0\u5b9e\u4e16\u754c\u8054\u90a6\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7b97\u6cd5\u89c1\u89e3\u548c\u5b9e\u9645\u90e8\u7f72\u6307\u5357\uff0c\u7ed3\u679c\u53ef\u76f4\u63a5\u8f6c\u79fb\u7ed9\u533b\u9662IT\u7ba1\u7406\u5458\uff0c\u7528\u4e8e\u5b9e\u65bd\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5b66\u4e60\u3002"}}
{"id": "2601.17216", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17216", "abs": "https://arxiv.org/abs/2601.17216", "authors": ["Murat Arda Onsu", "Poonam Lohan", "Burak Kantarci", "Aisha Syed", "Matthew Andrews", "Sean Kennedy"], "title": "Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction", "comment": "6 pages 5 figures, accepted to IEEE ICC 2026", "summary": "Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49V2X\u7684\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528V-JEPA\u751f\u6210\u65f6\u7a7a\u8bed\u4e49\u5d4c\u5165\u66ff\u4ee3\u539f\u59cb\u89c6\u9891\u4f20\u8f93\uff0c\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u8981\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\u4ee5\u786e\u4fdd\u9053\u8def\u5b89\u5168\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f20\u8f93\u539f\u59cb\u89c6\u9891\u6216\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u5728\u8f66\u8f7d\u901a\u4fe1\u5e26\u5bbd\u548c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4e0d\u5b9e\u7528\u3002", "method": "\u6784\u5efa\u8bed\u4e49V2X\u6846\u67b6\uff1a\u8def\u4fa7\u5355\u5143\u6444\u50cf\u5934\u4f7f\u7528V-JEPA\u751f\u6210\u672a\u6765\u5e27\u7684\u65f6\u7a7a\u8bed\u4e49\u5d4c\u5165\uff0c\u901a\u8fc7V2X\u94fe\u8def\u4f20\u8f93\u7ed9\u8f66\u8f86\uff0c\u8f66\u8f86\u7aef\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u63a2\u9488\u548c\u5206\u7c7b\u5668\u89e3\u7801\u9884\u6d4b\u78b0\u649e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u78b0\u649e\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e8610%\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u540c\u65f6\u4f20\u8f93\u9700\u6c42\u76f8\u6bd4\u539f\u59cb\u89c6\u9891\u964d\u4f4e\u4e86\u56db\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8bed\u4e49V2X\u901a\u4fe1\u6709\u6f5c\u529b\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u534f\u4f5c\u5f0f\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2601.17258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17258", "abs": "https://arxiv.org/abs/2601.17258", "authors": ["Jo\u00e3o Pereira", "Vasco Lopes", "Jo\u00e3o Neves", "David Semedo"], "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding", "comment": null, "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.", "AI": {"tldr": "FineVAU\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u57fa\u51c6\uff0c\u63d0\u51fa\u4e86FVScore\u8bc4\u4f30\u6307\u6807\u548cFineW3\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u7ec6\u7c92\u5ea6\u3001\u9886\u57df\u7279\u5b9a\u7684\u5f02\u5e38\u89c6\u9891\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u6355\u6349LVLM\u54cd\u5e94\u4e30\u5bcc\u6027\u548c\u89c6\u89c9\u57fa\u7840\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5f02\u5e38\u7406\u89e3(VAU)\u4efb\u52a1\u7684\u8bc4\u4f30\u5b58\u5728\u6311\u6218\uff1a\u73b0\u6709\u57fa\u51c6\u4f7f\u7528n-gram\u6307\u6807(\u5982BLEU\u3001ROUGE-L)\u65e0\u6cd5\u6355\u6349LVLM\u54cd\u5e94\u7684\u4e30\u5bcc\u6027\u548c\u89c6\u89c9\u57fa\u7840\u6027\uff0c\u800c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5219\u8fc7\u4e8e\u5173\u6ce8\u8bed\u8a00\u8d28\u91cf\u800c\u975e\u4e8b\u5b9e\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e0d\u4e00\u81f4\u7684\u4e3b\u89c2\u5224\u65ad\u3002", "method": "1) \u5c06VAU\u5b9a\u4e49\u4e3a\u4e09\u65b9\u9762\u95ee\u9898\uff1a\u4e8b\u4ef6(What)\u3001\u53c2\u4e0e\u5b9e\u4f53(Who)\u548c\u4f4d\u7f6e(Where)\uff1b2) \u63d0\u51faFVScore\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30LVLM\u7b54\u6848\u4e2d\u5173\u952e\u89c6\u89c9\u5143\u7d20\u7684\u5b58\u5728\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ec6\u7c92\u5ea6\u53cd\u9988\uff1b3) \u521b\u5efaFineW3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5168\u81ea\u52a8\u6d41\u7a0b\u589e\u5f3a\u73b0\u6709\u4eba\u5de5\u6807\u6ce8\uff0c\u6dfb\u52a0\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0cFVScore\u6307\u6807\u5728\u5f02\u5e38\u611f\u77e5\u65b9\u9762\u4e0e\u4eba\u7c7b\u5224\u65ad\u5177\u6709\u66f4\u597d\u7684\u5bf9\u9f50\u6027\u3002\u5728FineVAU\u4e0a\u7684\u8be6\u7ec6\u5b9e\u9a8c\u63ed\u793a\u4e86LVLM\u5728\u9700\u8981\u7a7a\u95f4\u548c\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7406\u89e3\u7684\u5f02\u5e38\u4e8b\u4ef6\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u5c3d\u7ba1\u5728\u7c97\u7c92\u5ea6\u3001\u9759\u6001\u4fe1\u606f\u548c\u5177\u6709\u5f3a\u70c8\u89c6\u89c9\u7ebf\u7d22\u7684\u4e8b\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "FineVAU\u57fa\u51c6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4efb\u52a1\u7684\u53d1\u5c55\uff0c\u63ed\u793a\u4e86\u5f53\u524dLVLM\u5728\u7a7a\u95f4\u548c\u65f6\u5e8f\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.17259", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17259", "abs": "https://arxiv.org/abs/2601.17259", "authors": ["Angad Singh Ahuja", "Aarush Ram Anandh"], "title": "Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling", "comment": "25 Pages, 12 Figures, 3 Tables, 5 Appendices, 8 Algorithms", "summary": "Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u989c\u8272\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u57df\u7ea6\u675f\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u989c\u8272\u4fdd\u6301", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u7cfb\u7edf\u5728\u7cbe\u786e\u989c\u8272\u63a7\u5236\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5931\u8d25\uff0c\u7279\u522b\u662f\u5728\u8bbe\u8ba1\u5bfc\u5411\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u8f93\u51fa\u5fc5\u987b\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u989c\u8272\u76ee\u6807\u3002\u73b0\u6709\u7684\u5747\u503c\u7ea6\u675f\u65b9\u6cd5\u867d\u7136\u80fd\u6ee1\u8db3\u5e73\u5747\u989c\u8272\u8981\u6c42\uff0c\u4f46\u4f1a\u4ea7\u751f\u611f\u77e5\u4e0a\u663e\u8457\u7684\u5c40\u90e8\u5931\u8d25\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u6280\u672f\uff1a1) \u57fa\u4e8e\u611f\u5174\u8da3\u533a\u57df\u7684\u4fee\u590d\u5b9e\u73b0\u7a7a\u95f4\u9009\u62e9\u6027\uff1b2) \u80cc\u666f\u6f5c\u5728\u91cd\u65b0\u65bd\u52a0\u9632\u6b62ROI\u5916\u989c\u8272\u6f02\u79fb\uff1b3) \u4f7f\u7528CIE Lab\u548c\u7ebf\u6027RGB\u5b9a\u4e49\u7684\u590d\u5408\u635f\u5931\u8fdb\u884c\u68af\u5ea6\u5f15\u5bfc\u7684\u6f5c\u5728\u5fae\u8c03\u3002\u635f\u5931\u51fd\u6570\u4e0d\u4ec5\u63a7\u5236ROI\u7684\u5e73\u5747\u989c\u8272\uff0c\u8fd8\u901a\u8fc7CVaR\u98ce\u683c\u548c\u8f6f\u6700\u5927\u503c\u60e9\u7f5a\u63a7\u5236\u50cf\u7d20\u7ea7\u8bef\u5dee\u5206\u5e03\u7684\u5c3e\u90e8\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5b9e\u73b0\u76ee\u6807\u989c\u8272\u9075\u5faa\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u6807\u51c6\u7684Stable Diffusion\u4fee\u590d\u6d41\u7a0b\u4e2d\u3002\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5747\u503c\u7ea6\u675f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u907f\u514d\u611f\u77e5\u4e0a\u663e\u8457\u7684\u5c40\u90e8\u989c\u8272\u5931\u8d25\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a8\u7406\u65f6\u533a\u57df\u7ea6\u675f\u989c\u8272\u4fdd\u6301\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u989c\u8272\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u7684\u76ee\u6807\u51fd\u6570\u548c\u7a33\u5b9a\u7684\u5f15\u5bfc\u8c03\u5ea6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u989c\u8272\u9075\u5faa\u6548\u679c\u3002"}}
{"id": "2601.17303", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.17303", "abs": "https://arxiv.org/abs/2601.17303", "authors": ["Samaresh Kumar Singh", "Joyjit Roy"], "title": "Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach", "comment": "9 pages, 8 figures, and Submitted to IEEE SoutheastCon 2026", "summary": "As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital \"immune system\" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7fa4\u67b6\u6784\uff0c\u7528\u4e8e\u5de5\u4e1a\u7269\u8054\u7f51\u5b89\u5168\u76d1\u63a7\uff0c\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7\u54cd\u5e94\u548c97.3%\u6076\u610f\u6d3b\u52a8\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u8bbe\u5907\u6570\u91cf\u6fc0\u589e\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u5b89\u5168\u76d1\u63a7\u67b6\u6784\u5b58\u5728\u5ef6\u8fdf\u95ee\u9898\uff0c\u653b\u51fb\u8005\u53ef\u5229\u7528\u8fd9\u4e9b\u5ef6\u8fdf\u7834\u574f\u6574\u4e2a\u5236\u9020\u751f\u6001\u7cfb\u7edf", "method": "\u8bbe\u8ba1\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7fa4\u67b6\u6784\uff0c\u5728\u6bcf\u4e2a\u8fb9\u7f18\u7f51\u5173\u90e8\u7f72\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7P2P\u534f\u8bae\u534f\u540c\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\uff0c\u91c7\u7528\u57fa\u4e8e\u5171\u8bc6\u7684\u5a01\u80c1\u9a8c\u8bc1\u6d41\u7a0b\u8fdb\u884c\u5a01\u80c1\u7b49\u7ea7\u6295\u7968", "result": "\u5728\u6a21\u62df2000\u4e2aIIoT\u8bbe\u5907\u7684\u521b\u65b0\u5de5\u5382\u6d4b\u8bd5\u4e2d\uff0cDMAS\u5b9e\u73b00.85ms\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\uff0c\u9ad8\u8d1f\u8f7d\u4e0b97.3%\u6076\u610f\u6d3b\u52a8\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u51c6\u786e\u738787%\uff0c\u7f51\u7edc\u5e26\u5bbd\u4f7f\u7528\u51cf\u5c1189%", "conclusion": "DMAS\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3\u96c6\u4e2d\u5f0f\u5b89\u5168\u76d1\u63a7\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u7269\u8054\u7f51\u5b89\u5168\u9632\u62a4\u80fd\u529b\uff0c\u9632\u6b62\u5b9e\u65f6\u7ea7\u8054\u6545\u969c\uff0c\u964d\u4f4e\u7f51\u7edc\u5e26\u5bbd\u9700\u6c42"}}
{"id": "2601.17342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17342", "abs": "https://arxiv.org/abs/2601.17342", "authors": ["Tong Wang", "Xiaodong Zhang", "Guanzhou Chen", "Jiaqi Wang", "Chenxi Liu", "Xiaoliang Tan", "Wenchao Guo", "Xuyang Li", "Xuanrui Wang", "Zifan Wang"], "title": "STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation", "comment": null, "summary": "Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \\textbf{STARS} (\\textbf{S}hared-specific \\textbf{T}ranslation and \\textbf{A}lignment for missing-modality \\textbf{R}emote \\textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.", "AI": {"tldr": "STARS\u662f\u4e00\u4e2a\u9488\u5bf9\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u8f93\u5165\u7684\u9065\u611f\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u5bf9\u9f50\u673a\u5236\u548c\u50cf\u7d20\u7ea7\u8bed\u4e49\u91c7\u6837\u5bf9\u9f50\u6765\u89e3\u51b3\u6a21\u6001\u7f3a\u5931\u95ee\u9898", "motivation": "\u591a\u6a21\u6001\u9065\u611f\u6280\u672f\u901a\u8fc7\u6574\u5408\u5149\u5b66\u56fe\u50cf\u3001SAR\u548cDSM\u7b49\u5f02\u6784\u6570\u636e\u589e\u5f3a\u5730\u8868\u8bed\u4e49\u7406\u89e3\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u6001\u6570\u636e\u7f3a\u5931\u662f\u5e38\u89c1\u4e14\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4f20\u7edf\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u5d29\u6e83\u548c\u6062\u590d\u7279\u5f81\u8fc7\u4e8e\u6cdb\u5316\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faSTARS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1) \u5e26\u53cc\u5411\u7ffb\u8bd1\u548c\u505c\u6b62\u68af\u5ea6\u7684\u975e\u5bf9\u79f0\u5bf9\u9f50\u673a\u5236\uff0c\u9632\u6b62\u7279\u5f81\u5d29\u6e83\u5e76\u964d\u4f4e\u5bf9\u8d85\u53c2\u6570\u7684\u654f\u611f\u6027\uff1b2) \u50cf\u7d20\u7ea7\u8bed\u4e49\u91c7\u6837\u5bf9\u9f50\u7b56\u7565\uff0c\u7ed3\u5408\u7c7b\u522b\u5e73\u8861\u50cf\u7d20\u91c7\u6837\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\uff0c\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u5bf9\u9f50\u5931\u8d25\u3002", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u6697\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u9ad8\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "STARS\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u9f50\u673a\u5236\u548c\u91c7\u6837\u7b56\u7565\uff0c\u4e3a\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u8f93\u5165\u7684\u9065\u611f\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.18217", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18217", "abs": "https://arxiv.org/abs/2601.18217", "authors": ["Zhihan Liu", "Lin Guan", "Yixin Nie", "Kai Zhang", "Zhuoqun Hao", "Lin Chen", "Asli Celikyilmaz", "Zhaoran Wang", "Na Zhang"], "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents", "comment": null, "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.", "AI": {"tldr": "\u7814\u7a76LLM\u667a\u80fd\u4f53\u5728\u672a\u77e5\u6d4b\u8bd5\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u662f\u5f71\u54cd\u8de8\u57df\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u5e72\u6270\u6027\u7279\u5f81\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u7528LLM\u667a\u80fd\u4f53\u901a\u5e38\u5728\u6709\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5728\u66f4\u5e7f\u6cdb\u7684\u672a\u77e5\u9886\u57df\u90e8\u7f72\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f53\u6700\u7ec8\u6d4b\u8bd5\u9886\u57df\u672a\u77e5\u65f6\uff0c\u54ea\u4e9b\u73af\u5883\u5c5e\u6027\u548c\u5efa\u6a21\u9009\u62e9\u5bf9\u8de8\u57df\u6027\u80fd\u5f71\u54cd\u6700\u5927\u3002", "method": "\u9996\u5148\u8bc6\u522b\u5f71\u54cd\u8de8\u57df\u6cdb\u5316\u7684\u4e24\u4e2a\u5173\u952e\u73af\u5883\u8f74\uff1a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u3002\u63d0\u51fa\u968f\u673a\u5316\u6280\u672f\uff0c\u901a\u8fc7\u6dfb\u52a0\u5c11\u91cf\u4e0e\u76ee\u6807\u65e0\u5173\u7684\u5e72\u6270\u6027\u7279\u5f81\u6765\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u3002\u540c\u65f6\u5206\u6790\u5efa\u6a21\u9009\u62e9\uff0c\u5305\u62ecSFT\u9884\u70ed/\u4e2d\u671f\u8bad\u7ec3\u548c\u9010\u6b65\u601d\u8003\u673a\u5236\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u4e0e\u8de8\u57df\u6cdb\u5316\u5f3a\u76f8\u5173\uff0c\u800c\u9886\u57df\u771f\u5b9e\u6027\u548c\u6587\u672c\u76f8\u4f3c\u5ea6\u4e0d\u662f\u4e3b\u8981\u56e0\u7d20\uff1b2\uff09\u4ec5\u589e\u52a0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u5c31\u80fd\u6709\u6548\u63d0\u5347\u8de8\u57df\u9c81\u68d2\u6027\uff1b3\uff09SFT\u9884\u70ed/\u4e2d\u671f\u8bad\u7ec3\u6709\u52a9\u4e8e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f46\u4f1a\u635f\u5bb3\u672a\u5305\u542b\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff1b4\uff09\u9010\u6b65\u601d\u8003\u673a\u5236\u5728\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u662f\u5f71\u54cdLLM\u667a\u80fd\u4f53\u8de8\u57df\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002\u901a\u8fc7\u7b80\u5355\u7684\u968f\u673a\u5316\u6280\u672f\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u53ef\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u9700\u8981\u8c28\u614e\u4f7f\u7528SFT\u8bad\u7ec3\u5e76\u542f\u7528\u9010\u6b65\u601d\u8003\u673a\u5236\u6765\u5e73\u8861\u6027\u80fd\u4e0e\u6cdb\u5316\u3002"}}
{"id": "2601.17777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17777", "abs": "https://arxiv.org/abs/2601.17777", "authors": ["Xiaoyu Liu", "Xiaoyu Guan", "Di Liang", "Xianjie Wu"], "title": "DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the \"seesaw effect\": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u53c2\u6570\u9694\u79bb\u7b56\u7565\uff0c\u901a\u8fc7\u8bc6\u522b\u4efb\u52a1\u6838\u5fc3\u53c2\u6570\u533a\u57df\u3001\u5408\u5e76\u91cd\u53e0\u4efb\u52a1\u3001\u5206\u9636\u6bb5\u8bad\u7ec3\u5e76\u51bb\u7ed3\u5148\u524d\u4efb\u52a1\u6838\u5fc3\u53c2\u6570\uff0c\u89e3\u51b3SFT\u4e2d\u7684\u8df7\u8df7\u677f\u6548\u5e94", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u4e2d\uff0c\u5f02\u6784\u4efb\u52a1\u95f4\u7684\u51b2\u7a81\u76ee\u6807\u4f1a\u5bfc\u81f4\"\u8df7\u8df7\u677f\u6548\u5e94\"\uff1a\u4f18\u5316\u4e00\u4e2a\u4efb\u52a1\u4f1a\u635f\u5bb3\u5176\u4ed6\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u53c2\u6570\u88ab\u65e0\u5dee\u522b\u66f4\u65b0\u65f6\u3002\u53c2\u6570\u5f02\u8d28\u6027\u53ef\u80fd\u662f\u8de8\u4efb\u52a1\u5e72\u6270\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "1. \u5728\u4e0d\u540cSFT\u4efb\u52a1\u4e0a\u72ec\u7acb\u5fae\u8c03LLMs\uff0c\u8bc6\u522b\u6bcf\u4e2a\u4efb\u52a1\u7684\u6838\u5fc3\u53c2\u6570\u533a\u57df\uff08\u66f4\u65b0\u5e45\u5ea6\u6700\u5927\u7684\u53c2\u6570\u5b50\u96c6\uff09\uff1b2. \u5408\u5e76\u6838\u5fc3\u53c2\u6570\u533a\u57df\u9ad8\u5ea6\u91cd\u53e0\u7684\u4efb\u52a1\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff1b3. \u5c06\u4e0d\u76f8\u4ea4\u7684\u4efb\u52a1\u7ec4\u7ec7\u5230\u4e0d\u540c\u9636\u6bb5\uff1b4. \u5728\u591a\u9636\u6bb5SFT\u4e2d\uff0c\u51bb\u7ed3\u5148\u524d\u4efb\u52a1\u83b7\u5f97\u7684\u6838\u5fc3\u53c2\u6570\uff0c\u9632\u6b62\u88ab\u540e\u7eed\u4efb\u52a1\u8986\u76d6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5bc6\u96c6\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u53c2\u6570\u9694\u79bb\u7b56\u7565\u6301\u7eed\u51cf\u5c11\u4e86\u6570\u636e\u51b2\u7a81\uff0c\u76f8\u6bd4\u591a\u9636\u6bb5\u548c\u591a\u4efb\u52a1\u8c03\u4f18\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u53c2\u6570\u5f02\u8d28\u6027\u5047\u8bbe\u9a71\u52a8\u7684\u52a8\u6001\u53c2\u6570\u9694\u79bb\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86SFT\u4e2d\u7684\u8df7\u8df7\u677f\u6548\u5e94\uff0c\u901a\u8fc7\u8bc6\u522b\u3001\u5408\u5e76\u548c\u9694\u79bb\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u591a\u4efb\u52a1\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2601.18308", "categories": ["cs.AI", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18308", "abs": "https://arxiv.org/abs/2601.18308", "authors": ["Geunsik Lim"], "title": "A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience", "comment": "19 pages", "summary": "As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.", "AI": {"tldr": "Climate RADAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u53ef\u9760\u6027\u5c42\uff0c\u5c06\u707e\u5bb3\u9884\u8b66\u4ece\u8b66\u62a5\u4f20\u9012\u8f6c\u53d8\u4e3a\u884c\u52a8\u6267\u884c\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6e90\u6570\u636e\u548cLLM\u63d0\u4f9b\u4e2a\u6027\u5316\u5efa\u8bae\uff0c\u63d0\u9ad8\u4fdd\u62a4\u884c\u52a8\u6267\u884c\u7387\u5e76\u51cf\u5c11\u54cd\u5e94\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u9884\u8b66\u7cfb\u7edf\u867d\u7136\u5feb\u901f\u4f20\u64ad\u8b66\u62a5\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e6\u53d1\u53ca\u65f6\u7684\u4fdd\u62a4\u884c\u52a8\uff0c\u5bfc\u81f4\u53ef\u9884\u9632\u7684\u635f\u5931\u548c\u4e0d\u516c\u5e73\u73b0\u8c61\u3002\u9700\u8981\u5c06\u707e\u5bb3\u6c9f\u901a\u4ece\"\u8b66\u62a5\u4f20\u9012\"\u8f6c\u53d8\u4e3a\"\u884c\u52a8\u6267\u884c\"\u3002", "method": "\u6574\u5408\u6c14\u8c61\u3001\u6c34\u6587\u3001\u8106\u5f31\u6027\u548c\u793e\u4f1a\u6570\u636e\u5f62\u6210\u7efc\u5408\u98ce\u9669\u6307\u6570\uff0c\u4f7f\u7528\u5e26\u6709\u62a4\u680f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e3a\u516c\u6c11\u3001\u5fd7\u613f\u8005\u548c\u5e02\u653f\u754c\u9762\u63d0\u4f9b\u4e2a\u6027\u5316\u5efa\u8bae\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u3001\u7528\u6237\u7814\u7a76\u548c\u5e02\u653f\u8bd5\u70b9\u8bc4\u4f30\u663e\u793a\uff1a\u63d0\u9ad8\u4e86\u4fdd\u62a4\u884c\u52a8\u6267\u884c\u7387\u3001\u51cf\u5c11\u4e86\u54cd\u5e94\u5ef6\u8fdf\u3001\u589e\u52a0\u4e86\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "conclusion": "Climate RADAR\u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u5206\u6790\u3001\u884c\u4e3a\u79d1\u5b66\u548c\u8d1f\u8d23\u4efbAI\uff0c\u63a8\u8fdb\u4ee5\u4eba\u4e3a\u672c\u3001\u900f\u660e\u548c\u516c\u5e73\u7684\u9884\u8b66\u7cfb\u7edf\uff0c\u4e3a\u5408\u89c4\u5c31\u7eea\u7684\u707e\u5bb3\u97e7\u6027\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.17388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17388", "abs": "https://arxiv.org/abs/2601.17388", "authors": ["Xuan Ding", "Xiu Yan", "Chuanlong Xie", "Yao Zhu"], "title": "ONRW: Optimizing inversion noise for high-quality and robust watermark", "comment": "Preprint. Under review", "summary": "Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u9c81\u68d2\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u6587\u672c\u4f18\u5316\u548c\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u589e\u5f3a\u6c34\u5370\u5bf9\u5404\u79cd\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6c34\u5370\u65b9\u6cd5\u867d\u7136\u80fd\u9690\u85cf\u6c34\u5370\u4e14\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u5c0f\uff0c\u4f46\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u9047\u5230\u56fe\u50cf\u635f\u574f\u65f6\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "1) \u901a\u8fc7\u7a7a\u6587\u672c\u4f18\u5316\u5c06\u5e72\u51c0\u56fe\u50cf\u8f6c\u6362\u4e3a\u53cd\u8f6c\u566a\u58f0\uff1b2) \u5728\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u53cd\u8f6c\u566a\u58f0\uff1b3) \u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u6c34\u5370\u56fe\u50cf\uff1b4) \u5f15\u5165\u81ea\u6ce8\u610f\u529b\u7ea6\u675f\u548c\u4f2a\u63a9\u7801\u7b56\u7565\u9632\u6b62\u56fe\u50cf\u539f\u59cb\u8bed\u4e49\u5931\u771f\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\u768412\u79cd\u4e0d\u540c\u56fe\u50cf\u53d8\u6362\u4e2d\uff0c\u5e73\u5747\u6bd4\u7a33\u5b9a\u7b7e\u540d\u65b9\u6cd5\u6027\u80fd\u63d0\u534710%\uff0c\u5728\u5404\u79cd\u56fe\u50cf\u635f\u574f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6c34\u5370\u6846\u67b6\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17391", "abs": "https://arxiv.org/abs/2601.17391", "authors": ["Rui Fan", "Weidong Hao"], "title": "SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition", "comment": null, "summary": "Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u7a7a\u591a\u89c6\u56fe\u8868\u793a\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u8fc7\u5e73\u79fb\u4e0d\u53d8\u7684\u5bc6\u96c6\u8f6c\u6362\u3001\u53cc\u5206\u652f\u52a8\u6001\u878d\u5408\u67b6\u6784\u548c\u751f\u7269\u542f\u53d1\u7684\u65f6\u5e8f\u626d\u66f2\u589e\u5f3a\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u76f8\u673a\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u57fa\u4e8e\u7a7a\u95f4\u5206\u7bb1\u7684\u8868\u793a\u65b9\u6cd5\u5177\u6709\u5e73\u79fb\u4e0d\u53d8\u6027\u9650\u5236\uff1b2) \u65e9\u671f\u7b80\u5355\u62fc\u63a5\u7684\u878d\u5408\u67b6\u6784\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u4e0d\u540c\u89c6\u56fe\u95f4\u7684\u4e92\u8865\u6027\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u901a\u8fc7\u5e73\u79fb\u4e0d\u53d8\u7684\u5bc6\u96c6\u8f6c\u6362\u5c06\u7a00\u758f\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u65f6\u7a7a\u591a\u89c6\u56fe\u8868\u793a\uff1b2) \u8bbe\u8ba1\u53cc\u5206\u652f\u52a8\u6001\u878d\u5408\u67b6\u6784\uff0c\u5efa\u6a21\u4e0d\u540c\u89c6\u56fe\u8fd0\u52a8\u7279\u5f81\u4e4b\u95f4\u7684\u6837\u672c\u7ea7\u4e92\u8865\u6027\uff1b3) \u5f15\u5165\u751f\u7269\u542f\u53d1\u7684\u65f6\u5e8f\u626d\u66f2\u589e\u5f3a\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4eba\u7c7b\u52a8\u4f5c\u7684\u901f\u5ea6\u53d8\u5316\u3002", "result": "\u5728HARDVS\u3001DailyDVS-200\u548cTHU-EACT-50-CHL\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709SMVRL\u65b9\u6cd5\u5206\u522b\u83b7\u5f97\u4e86+7.0%\u3001+10.7%\u548c+10.2%\u7684Top-1\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u53c2\u6570\u51cf\u5c11\u4e8630.1%\uff0c\u8ba1\u7b97\u91cf\u964d\u4f4e\u4e8635.7%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7684\u65f6\u7a7a\u591a\u89c6\u56fe\u8868\u793a\u3001\u52a8\u6001\u878d\u5408\u67b6\u6784\u548c\u65f6\u5e8f\u589e\u5f3a\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u9896\u4e14\u5f3a\u5927\u7684\u4e8b\u4ef6\u76f8\u673a\u52a8\u4f5c\u8bc6\u522b\u8303\u5f0f\uff0c\u5728\u51c6\u786e\u7387\u3001\u53c2\u6570\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.17483", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17483", "abs": "https://arxiv.org/abs/2601.17483", "authors": ["Barak Or"], "title": "Automatic Stability and Recovery for Neural Network Training", "comment": "Under Review", "summary": "Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8fd0\u884c\u65f6\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u9694\u79bb\u521b\u65b0\u4fe1\u53f7\u5b9e\u73b0\u81ea\u52a8\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u4e0d\u4fee\u6539\u5e95\u5c42\u4f18\u5316\u5668\uff0c\u63d0\u4f9b\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8d8a\u6765\u8d8a\u8106\u5f31\uff0c\u7f55\u89c1\u4f46\u4e25\u91cd\u7684\u7834\u574f\u6027\u66f4\u65b0\u5e38\u5bfc\u81f4\u4e0d\u53ef\u9006\u53d1\u6563\u6216\u6027\u80fd\u9000\u5316\u3002\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f18\u5316\u5668\u5185\u7684\u9884\u9632\u673a\u5236\uff0c\u4e00\u65e6\u53d1\u751f\u4e0d\u7a33\u5b9a\u6027\uff0c\u68c0\u6d4b\u548c\u6062\u590d\u80fd\u529b\u6709\u9650\u3002", "method": "\u5f15\u5165\u76d1\u7763\u8fd0\u884c\u65f6\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u5c06\u4f18\u5316\u89c6\u4e3a\u53d7\u63a7\u968f\u673a\u8fc7\u7a0b\u3002\u901a\u8fc7\u9694\u79bb\u6765\u81ea\u4e8c\u7ea7\u6d4b\u91cf\uff08\u5982\u9a8c\u8bc1\u63a2\u9488\uff09\u7684\u521b\u65b0\u4fe1\u53f7\uff0c\u5b9e\u73b0\u81ea\u52a8\u68c0\u6d4b\u548c\u6062\u590d\u7834\u574f\u6027\u66f4\u65b0\uff0c\u4e0d\u4fee\u6539\u5e95\u5c42\u4f18\u5316\u5668\u3002", "result": "\u63d0\u4f9b\u7406\u8bba\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u8bc1\uff0c\u5f62\u5f0f\u5316\u6709\u754c\u9000\u5316\u548c\u6062\u590d\u3002\u5b9e\u73b0\u5f00\u9500\u6700\u5c0f\uff0c\u517c\u5bb9\u5185\u5b58\u53d7\u9650\u7684\u8bad\u7ec3\u8bbe\u7f6e\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u81ea\u52a8\u68c0\u6d4b\u548c\u6062\u590d\u673a\u5236\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2601.18056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18056", "abs": "https://arxiv.org/abs/2601.18056", "authors": ["Ahmet Yavuz Uluslu", "Elliot Murphy"], "title": "Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production", "comment": null, "summary": "We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u632f\u8361\u7279\u5f81\u7eb3\u5165\u53cc\u8bed\u4ea7\u51fa\u9519\u8bef\u7814\u7a76\uff0c\u63d0\u51faROSE\u6a21\u578b\u80fd\u89e3\u91ca\u53cc\u8bed\u4ea7\u51fa\u4e2d\u7684\u53e5\u6cd5\u8fc1\u79fb\uff0c\u5e76\u4ee5\u8de8\u8bed\u8a00\u5f71\u54cd\u4e3a\u4f8b\u8bf4\u660e\u632f\u8361\u5931\u8d25\u6a21\u5f0f\u9a71\u52a8\u529f\u80fd\u6291\u5236/\u7ade\u4e89\u7406\u8bba\u3002", "motivation": "\u4f20\u7edf\u53cc\u8bed\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e8b\u4ef6\u76f8\u5173\u7535\u4f4d\u7b49\u65f6\u95f4\u7279\u5f81\uff0c\u9700\u8981\u5f15\u5165\u632f\u8361\u7279\u5f81\u6765\u4e3a\u53cc\u8bed\u7406\u8bba\u63d0\u4f9b\u65b0\u7684\u5b9e\u73b0\u5c42\u9762\u7ea6\u675f\uff0c\u63a2\u7d22\u66f4\u590d\u6742\u7684\u8bed\u8a00\u529f\u80fd\u969c\u788d\u751f\u7269\u6807\u5fd7\u7269\u3002", "method": "\u91c7\u7528ROSE\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u795e\u7ecf\u8ba1\u7b97\u6846\u67b6\uff0c\u4ee5\u8de8\u8bed\u8a00\u5f71\u54cd\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u53cc\u8bed\u4ea7\u51fa\u4e2d\u7684\u632f\u8361\u5931\u8d25\u6a21\u5f0f\u5982\u4f55\u9a71\u52a8\u529f\u80fd\u6291\u5236\u548c\u7ade\u4e89\u673a\u5236\u3002", "result": "ROSE\u6a21\u578b\u80fd\u591f\u6355\u6349\u53cc\u8bed\u4ea7\u51fa\u4e2d\u53e5\u6cd5\u8fc1\u79fb\u7684\u5f62\u5f0f\u7279\u6027\u548c\u5f62\u6001\u53e5\u6cd5\u5e8f\u5217\u5931\u8d25\u6a21\u5f0f\u7684\u8303\u56f4\uff0c\u4e3a\u8de8\u8bed\u8a00\u5f71\u54cd\u63d0\u4f9b\u4e86\u57fa\u4e8e\u632f\u8361\u5931\u8d25\u673a\u5236\u7684\u795e\u7ecf\u8ba1\u7b97\u89e3\u91ca\u3002", "conclusion": "\u5c06\u632f\u8361\u7279\u5f81\u7eb3\u5165\u53cc\u8bed\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86ROSE\u6a21\u578b\u6240\u5021\u5bfc\u7684\u8fde\u63a5\u5047\u8bbe\uff0c\u8fd8\u5141\u8bb8\u63a2\u7d22\u6bd4\u4f20\u7edf\u795e\u7ecf\u7279\u5f81\u66f4\u590d\u6742\u7684\u65f6\u7a7a\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u53cc\u8bed\u7406\u8bba\u63d0\u4f9b\u65b0\u7684\u5b9e\u73b0\u5c42\u9762\u7ea6\u675f\u3002"}}
{"id": "2601.18077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18077", "abs": "https://arxiv.org/abs/2601.18077", "authors": ["Mahesh Ramesh", "Kaousheik Jayakumar", "Aswinkumar Ramkumar", "Pavan Thodima", "Aniket Rege"], "title": "Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents", "comment": null, "summary": "Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8617\u4e2aLLM\u667a\u80fd\u4f53\u5728Hanabi\u7eb8\u724c\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u63d0\u5347\u534f\u4f5c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u9996\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7528\u4e8e\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u7684\u534f\u4f5c\u63a8\u7406\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\u3002Hanabi\u6e38\u620f\u9700\u8981\u5fc3\u667a\u7406\u8bba\u548c\u6218\u7565\u6c9f\u901a\uff0c\u662f\u7814\u7a76\u8fd9\u4e00\u95ee\u9898\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u57282-5\u4eba\u6e38\u620f\u4e2d\u6d4b\u8bd517\u4e2aLLM\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u4e09\u79cd\u4e0a\u4e0b\u6587\u8bbe\u7f6e\uff1aWatson\uff08\u4ec5\u663e\u5f0f\u4fe1\u606f\uff09\u3001Sherlock\uff08\u8d1d\u53f6\u65af\u63a8\u7406\uff09\u3001Mycroft\uff08\u591a\u8f6e\u72b6\u6001\u8ddf\u8e2a\uff09\u3002\u53d1\u5e03HanabiLogs\u548cHanabiRewards\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u548cRL\u5fae\u8c03\u3002", "result": "\u6700\u5f3a\u63a8\u7406\u6a21\u578b\u5728Sherlock\u8bbe\u7f6e\u4e0b\u5e73\u5747\u5f97\u5206\u8d85\u8fc715\u5206\uff0c\u4f46\u4ecd\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0820+\u5206\uff09\u3002\u4f7f\u7528\u6570\u636e\u96c6\u5fae\u8c03\u76844B\u6a21\u578b\u6027\u80fd\u63d0\u534721%\uff08\u76d1\u7763\uff09\u548c156%\uff08RL\uff09\uff0c\u63a5\u8fd1o4-mini\u6c34\u5e73\uff0c\u8d85\u8d8aGPT-4.1 52%\u3002RL\u5fae\u8c03\u6a21\u578b\u8fd8\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u4e13\u7528\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u534f\u4f5c\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002RL\u5fae\u8c03\u4e0d\u4ec5\u63d0\u5347Hanabi\u6027\u80fd\uff0c\u8fd8\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2601.18296", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18296", "abs": "https://arxiv.org/abs/2601.18296", "authors": ["Zhaoyan Gong", "Zhiqiang Liu", "Songze Li", "Xiaoke Guo", "Yuanxiang Liu", "Xinle Deng", "Zhizhen Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning", "comment": "Work in progress", "summary": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.", "AI": {"tldr": "Temp-R1\uff1a\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u4e3bTKGQA\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u548c\u9006\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5728\u590d\u6742\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TKGQA\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u548c\u6602\u8d35\u7684\u95ed\u6e90API\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u4e8b\u5b9e\u7684\u591a\u8df3\u4f9d\u8d56\u548c\u590d\u6742\u65f6\u5e8f\u7ea6\u675f\u3002", "method": "\u63d0\u51faTemp-R1\uff1a1\uff09\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5305\u542b\u4e13\u7528\u5185\u90e8\u52a8\u4f5c\u548c\u5916\u90e8\u52a8\u4f5c\u4ee5\u89e3\u51b3\u5355\u52a8\u4f5c\u63a8\u7406\u7684\u8ba4\u77e5\u8fc7\u8f7d\u95ee\u9898\uff1b2\uff09\u5f15\u5165\u9006\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5148\u8bad\u7ec3\u56f0\u96be\u95ee\u9898\u518d\u8fc1\u79fb\u5230\u7b80\u5355\u95ee\u9898\uff0c\u9632\u6b62\u6377\u5f84\u5b66\u4e60\u3002", "result": "\u5728MultiTQ\u548cTimelineKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c8B\u53c2\u6570\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u4e0a\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u534719.8%\uff0c\u5efa\u7acb\u4e86\u81ea\u4e3b\u65f6\u5e8f\u63a8\u7406\u667a\u80fd\u4f53\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "Temp-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3001\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u548c\u9006\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86TKGQA\u4e2d\u7684\u590d\u6742\u63a8\u7406\u6311\u6218\uff0c\u4e3a\u81ea\u4e3b\u65f6\u5e8f\u63a8\u7406\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.18375", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18375", "abs": "https://arxiv.org/abs/2601.18375", "authors": ["Jonas Golde", "Nicolaas Jedema", "Ravi Krishnan", "Phong Le"], "title": "Hierarchical Text Classification with LLM-Refined Taxonomies", "comment": null, "summary": "Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.", "AI": {"tldr": "TaxMorph\u4f7f\u7528LLM\u91cd\u6784\u5206\u7c7b\u5b66\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u91cd\u547d\u540d\u3001\u5408\u5e76\u3001\u62c6\u5206\u548c\u91cd\u6392\u5e8f\u64cd\u4f5c\uff0c\u4f7f\u5206\u7c7b\u5b66\u66f4\u7b26\u5408\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5728\u4e09\u4e2aHTC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe+2.9pp F1\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5206\u7c7b\u5b66\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u5982\u76f8\u4f3c\u7236\u8282\u70b9\u4e0b\u7684\u76f8\u540c\u53f6\u8282\u70b9\u540d\u79f0\uff0c\u8fd9\u963b\u788d\u4e86\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u6e05\u6670\u7684\u51b3\u7b56\u8fb9\u754c\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u6765\u4f18\u5316\u6574\u4e2a\u5206\u7c7b\u5b66\u5c42\u6b21\u7ed3\u6784\u3002", "method": "\u63d0\u51faTaxMorph\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u91cd\u547d\u540d\u3001\u5408\u5e76\u3001\u62c6\u5206\u548c\u91cd\u6392\u5e8f\u7b49\u64cd\u4f5c\u6765\u91cd\u6784\u6574\u4e2a\u5206\u7c7b\u5b66\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f\u5206\u7c7b\u5b66\u66f4\u597d\u5730\u5339\u914d\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u7684\u8bed\u4e49\u3002", "result": "\u5728\u4e09\u4e2a\u5c42\u6b21\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM\u4f18\u5316\u7684\u5206\u7c7b\u5b66\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u4eba\u5de5\u7b56\u5212\u7684\u5206\u7c7b\u5b66\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe+2.9pp F1\u3002\u7814\u7a76\u53d1\u73b0LLM\u4f18\u5316\u7684\u5206\u7c7b\u5b66\u867d\u7136\u66f4\u96be\u5206\u79bb\uff0c\u4f46\u66f4\u7b26\u5408\u6a21\u578b\u7684\u6df7\u6dc6\u6a21\u5f0f\u3002", "conclusion": "LLM\u5f15\u5bfc\u7684\u5206\u7c7b\u5b66\u4f18\u5316\u521b\u5efa\u4e86\u66f4\u7b26\u5408\u6a21\u578b\u5b66\u4e60\u65b9\u5f0f\u7684\u5206\u7c7b\u5b66\u7ed3\u6784\uff0c\u63d0\u9ad8\u4e86\u5c42\u6b21\u6587\u672c\u5206\u7c7b\u6027\u80fd\u3002\u8fd9\u8868\u660e\u5206\u7c7b\u5b66\u8bbe\u8ba1\u5e94\u8003\u8651\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.18468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18468", "abs": "https://arxiv.org/abs/2601.18468", "authors": ["Daniel B. Hier", "Tayo Obafemi-Ajayi"], "title": "Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models", "comment": null, "summary": "Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u540e\u5b58\u50a8\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u7684\u5f3a\u5ea6\u4e0d\u5747\uff0c\u6f5c\u5728\u77e5\u8bc6\uff08\u5b58\u5728\u4e8e\u6743\u91cd\u4f46\u65e0\u6cd5\u901a\u8fc7\u786e\u5b9a\u6027\u89e3\u7801\u53ef\u9760\u8bbf\u95ee\uff09\u80fd\u9884\u6d4b\u5fae\u8c03\u671f\u95f4\u4e8b\u5b9e\u5b66\u4e60\u7684\u901f\u5ea6\u548c\u672a\u89c1\u672c\u4f53\u4e8b\u5b9e\u7684\u6709\u9650\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u540e\u5b58\u50a8\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u7684\u5f3a\u5ea6\u4e0d\u5747\uff1a\u6709\u4e9b\u4e8b\u5b9e\u5b58\u5728\u4e8e\u6743\u91cd\u4e2d\u4f46\u65e0\u6cd5\u901a\u8fc7\u786e\u5b9a\u6027\u89e3\u7801\u53ef\u9760\u8bbf\u95ee\uff08\u6f5c\u5728\u77e5\u8bc6\uff09\uff0c\u800c\u5176\u4ed6\u4e8b\u5b9e\u5219\u5f88\u5c11\u88ab\u8868\u793a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u8fd9\u79cd\u6f5c\u5728\u77e5\u8bc6\u5982\u4f55\u5f71\u54cd\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u4e8b\u5b9e\u83b7\u53d6\u3001\u6cdb\u5316\u548c\u9000\u5316\u3002", "method": "\u5bf9Llama 3.1 8B Instruct\u8fdb\u884c\u5fae\u8c03\uff0c\u5b66\u4e60\u4eba\u7c7b\u8868\u578b\u672c\u4f53\uff08800\u5bf9\uff09\u548c\u57fa\u56e0\u672c\u4f53\uff08400\u4e2a\u8bad\u7ec3\u5bf9\uff09\u7684\u672f\u8bed\u6807\u8bc6\u7b26\u6620\u5c04\uff0c\u4fdd\u7559400\u4e2aGO\u5bf9\u7528\u4e8e\u6d4b\u8bd5\u6cdb\u5316\u3002\u5c06\u5b66\u4e60\u89c6\u4e3a20\u4e2aepoch\u7684\u65f6\u95f4\u5230\u4e8b\u4ef6\u8fc7\u7a0b\uff0c\u4f7f\u7528\u968f\u673a\u89e3\u7801\u68c0\u6d4b\u57fa\u7ebf\u6f5c\u5728\u77e5\u8bc6\uff0c\u4f7f\u7528Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u8bc6\u522b\u83b7\u53d6\u3001\u6cdb\u5316\u548c\u9000\u5316\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "result": "HPO\u7684\u57fa\u7ebf\u786e\u5b9a\u6027\u53ec\u56de\u7387\u4e3a2.8%\uff0c\u5fae\u8c03\u540e\u5347\u81f371.9%\u3002\u6f5c\u5728\u77e5\u8bc6\u662f\u4e8b\u5b9e\u83b7\u53d6\u901f\u5ea6\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\uff08HR 2.6\uff09\uff0c\u4e0e\u66f4\u65e9\u3001\u66f4\u9ad8\u7684\u5cf0\u503c\u5b66\u4e60\u7387\u548c\u66f4\u5feb\u6536\u655b\u76f8\u5173\u3002\u6cdb\u5316\u5230\u4fdd\u7559\u7684GO\u4e8b\u5b9e\u4e0d\u5e38\u89c1\uff085.8%\uff09\uff0c\u4f46\u5f53\u6f5c\u5728\u77e5\u8bc6\u5b58\u5728\u65f6\u66f4\u53ef\u80fd\u53d1\u751f\u3002\u5148\u524d\u6b63\u786e\u7684GO\u6620\u5c04\u5728\u672a\u89c1\u672f\u8bed\u4e2d\u6bd4\u5728\u5df2\u8bad\u7ec3\u672f\u8bed\u4e2d\u66f4\u5e38\u9000\u5316\uff0c\u8868\u660e\u8bad\u7ec3\u671f\u95f4\u7684\u5f3a\u5316\u5177\u6709\u4fdd\u62a4\u4f5c\u7528\u3002", "conclusion": "\u6f5c\u5728\u77e5\u8bc6\u80fd\u9884\u6d4b\u5fae\u8c03\u671f\u95f4\u4e8b\u5b9e\u5b66\u4e60\u7684\u901f\u5ea6\u548c\u672a\u89c1\u672c\u4f53\u4e8b\u5b9e\u7684\u6709\u9650\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u5bf9\u9000\u5316\u7684\u62b5\u6297\u53d6\u51b3\u4e8e\u4e8b\u5b9e\u662f\u5426\u88ab\u5f3a\u5316\u3002\u8fd9\u63ed\u793a\u4e86LLM\u4e2d\u4e8b\u5b9e\u8868\u793a\u7684\u4e0d\u5747\u5300\u6027\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u5b66\u4e60\u52a8\u6001\u3002"}}
{"id": "2601.17947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17947", "abs": "https://arxiv.org/abs/2601.17947", "authors": ["Bora Yimenicioglu", "Vishal Manikanden"], "title": "FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos", "comment": null, "summary": "Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.\n  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\\sim 1.5\\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.", "AI": {"tldr": "FlowMorph\u662f\u4e00\u4e2a\u7269\u7406\u4e00\u81f4\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u4ece\u5fae\u6d41\u4f53\u89c6\u9891\u4e2d\u5b66\u4e60\u7ea2\u7ec6\u80de\u7684\u65e0\u6807\u7b7e\u529b\u5b66\u4ee3\u7406k\uff0c\u7ed3\u5408\u5c42\u6d41\u7269\u7406\u548c\u53ef\u5fae\u5206\u5efa\u6a21\uff0c\u7528\u4e8e\u7ec6\u80de\u529b\u5b66\u7279\u6027\u5206\u6790\u3002", "motivation": "\u7ea2\u7ec6\u80de\u529b\u5b66\u7279\u6027\u662f\u8840\u6db2\u548c\u7cfb\u7edf\u6027\u75be\u75c5\u7684\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5206\u5272\u6216\u624b\u5de5\u7279\u5f81\uff0c\u7f3a\u4e4f\u5bf9\u5c42\u6d41\u65af\u6258\u514b\u65af\u6d41\u7269\u7406\u7684\u7f16\u7801\u3002", "method": "\u4f7f\u7528\u4f4e\u7ef4\u53c2\u6570\u8f6e\u5ed3\u5efa\u6a21\u7ec6\u80de\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\"\u6d41\u52a8\u4e2d\u7684\u80f6\u56ca\"\u6a21\u578b\u7ed3\u5408\u5c42\u6d41\u5e73\u6d41\u548c\u66f2\u7387\u6b63\u5219\u5316\u5f39\u6027\u677e\u5f1b\uff0c\u4f18\u5316\u5305\u542b\u8f6e\u5ed3\u91cd\u53e0\u3001\u7ec6\u80de\u5185\u6d41\u4e00\u81f4\u6027\u3001\u9762\u79ef\u5b88\u6052\u3001\u58c1\u7ea6\u675f\u548c\u65f6\u95f4\u5e73\u6ed1\u5ea6\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u8f6e\u5ed3IoU\u8fbe\u52300.905\uff0c\u663e\u8457\u6539\u5584\u9762\u79ef\u5b88\u6052\u548c\u58c1\u7ea6\u675f\u8fdd\u89c4\uff1b\u529b\u5b66\u4ee3\u7406k\u80fd\u4ee50.863\u7684AUC\u533a\u5206\u5766\u514b\u5c65\u5e26\u548c\u7ffb\u8f6c\u52a8\u6001\uff1b\u4ec5\u7528200\u4e2aRT-DC\u4e8b\u4ef6\u6821\u51c6\uff0c\u5c31\u80fd\u4ee50.118 MPa\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u9884\u6d4b\u8868\u89c2\u6768\u6c0f\u6a21\u91cf\u3002", "conclusion": "FlowMorph\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7269\u7406\u4e00\u81f4\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5fae\u6d41\u4f53\u89c6\u9891\u4e2d\u5b66\u4e60\u7ea2\u7ec6\u80de\u7684\u65e0\u6807\u7b7e\u529b\u5b66\u4ee3\u7406\uff0c\u5728\u591a\u79cd\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4e3a\u7ec6\u80de\u529b\u5b66\u7279\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.18135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18135", "abs": "https://arxiv.org/abs/2601.18135", "authors": ["Jiahao Lyu", "Minghua Zhao", "Xuewen Huang", "Yifei Chen", "Shuangli Du", "Jing Hu", "Cheng Shi", "Zhiyong Lv"], "title": "Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection", "comment": "It has been submitted to the KBS journal", "summary": "As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.", "AI": {"tldr": "FoGA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u524d\u5411\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\uff0c\u4ec5\u9700\u7ea6200\u4e07\u53c2\u6570\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u53ef\u8fbe155FPS\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5927\u89c4\u6a21\u6a21\u578b\u8ffd\u6c42\u6781\u81f4\u7cbe\u5ea6\uff0c\u4f46\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u540c\u65f6\uff0c\u4e3b\u6d41\u9884\u6d4b\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u5e27\u672a\u6765\u9884\u6d4b\u8bef\u5dee\uff0c\u5ffd\u7565\u4e86\u66f4\u957f\u671f\u65f6\u95f4\u524d\u5411\u4fe1\u606f\u7684\u4e30\u5bcc\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eUnet\u7684\u65b9\u6cd5\uff0c\u5bf9\u8fde\u7eed\u5e27\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u751f\u6210\u5373\u65f6\u548c\u524d\u5411\u9884\u6d4b\uff1b\u5728\u8df3\u8dc3\u8fde\u63a5\u4e2d\u5f15\u5165\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\u6a21\u5757\uff0c\u52a8\u6001\u878d\u5408\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7279\u5f81\uff1b\u4f7f\u7528\u524d\u5411\u4e00\u81f4\u6027\u635f\u5931\u8054\u5408\u4f18\u5316\uff0c\u91c7\u7528\u6df7\u5408\u5f02\u5e38\u6d4b\u91cf\u7b56\u7565\u6574\u5408\u5373\u65f6\u548c\u524d\u5411\u5e27\u7684\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fd0\u884c\u901f\u5ea6\u53ef\u8fbe155FPS\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u6307\u6807\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u79c0\u5e73\u8861\u3002", "conclusion": "FoGA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u524d\u5411\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\uff0c\u9002\u5408\u90e8\u7f72\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002"}}
{"id": "2601.18157", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18157", "abs": "https://arxiv.org/abs/2601.18157", "authors": ["Aniket Rege", "Arka Sadhu", "Yuliang Li", "Kejie Li", "Ramya Korlakai Vinayak", "Yuning Chai", "Yong Jae Lee", "Hyo Jin Kim"], "title": "Agentic Very Long Video Understanding", "comment": "26 pages, 7 figures, 8 tables", "summary": "The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.", "AI": {"tldr": "EGAgent\uff1a\u57fa\u4e8e\u5b9e\u4f53\u573a\u666f\u56fe\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u65f6\u7a0b\u7a7f\u6234\u8bbe\u5907\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u548c\u8de8\u6a21\u6001\u63a8\u7406\u5b9e\u73b0\u8fde\u7eed\u591a\u5929\u89c6\u9891\u7684\u590d\u6742\u95ee\u7b54\u3002", "motivation": "\u5168\u5929\u5019\u53ef\u7a7f\u6234\u8bbe\u5907\uff08\u5982\u667a\u80fd\u773c\u955c\uff09\u9700\u8981\u7406\u89e3\u8fde\u7eed\u3001\u7eb5\u5411\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u6d41\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\uff0c\u65e0\u6cd5\u5bf9\u957f\u8fbe\u6570\u5929\u6216\u6570\u5468\u7684\u89c6\u9891\u8fdb\u884c\u7ec4\u5408\u5f0f\u591a\u8df3\u63a8\u7406\u3002", "method": "\u63d0\u51faEGAgent\u6846\u67b6\uff0c\u57fa\u4e8e\u5b9e\u4f53\u573a\u666f\u56fe\uff08\u8868\u793a\u4eba\u7269\u3001\u5730\u70b9\u3001\u7269\u4f53\u53ca\u5176\u968f\u65f6\u95f4\u7684\u5173\u7cfb\uff09\uff0c\u4e3a\u89c4\u5212\u667a\u80fd\u4f53\u63d0\u4f9b\u7ed3\u6784\u5316\u641c\u7d22\u548c\u63a8\u7406\u5de5\u5177\uff0c\u4ee5\u53ca\u6df7\u5408\u89c6\u89c9\u548c\u97f3\u9891\u641c\u7d22\u80fd\u529b\u3002", "result": "\u5728EgoLifeQA\u6570\u636e\u96c6\u4e0a\u8fbe\u523057.5%\u7684SOTA\u6027\u80fd\uff0c\u5728Video-MME\uff08Long\uff09\u6570\u636e\u96c6\u4e0a\u8fbe\u523074.1%\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u7528\u4e8e\u590d\u6742\u7eb5\u5411\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002", "conclusion": "EGAgent\u901a\u8fc7\u5b9e\u4f53\u573a\u666f\u56fe\u548c\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u7ec4\u5408\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u5168\u5929\u5019\u4e2a\u4ebaAI\u52a9\u624b\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u3002"}}
{"id": "2601.18305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18305", "abs": "https://arxiv.org/abs/2601.18305", "authors": ["Xuan Wang", "Siyuan Su", "Quantong Fu", "Yongxiang Hu", "Yangfan Zhou"], "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis", "comment": "15 pages, 3 figures. Under review. Code and dataset will be released upon acceptance", "summary": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.", "AI": {"tldr": "\u63d0\u51faSwipeGen\u81ea\u52a8\u5408\u6210\u4eba\u7c7b\u6ed1\u52a8\u4ea4\u4e92\u7684\u7ba1\u9053\uff0c\u6784\u5efa\u9996\u4e2aGUI\u4ee3\u7406\u6ed1\u52a8\u6267\u884c\u80fd\u529b\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1GUISwiper\u4ee3\u7406\u663e\u8457\u63d0\u5347\u6ed1\u52a8\u6267\u884c\u51c6\u786e\u7387", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u5904\u7406\u6ed1\u52a8\u4ea4\u4e92\u65f6\u91c7\u7528\u8fc7\u4e8e\u7b80\u5316\u7684\u7b56\u7565\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u5bfc\u81f4\u4efb\u52a1\u5b8c\u6210\u7387\u53d7\u9650\uff0c\u6ed1\u52a8\u6267\u884c\u80fd\u529b\u6210\u4e3a\u65b0\u7684\u74f6\u9888", "method": "\u5c06\u4eba\u7c7b\u6ed1\u52a8\u624b\u52bf\u5206\u89e3\u4e3a\u591a\u4e2a\u53ef\u91cf\u5316\u7ef4\u5ea6\uff0c\u63d0\u51faSwipeGen\u81ea\u52a8\u7ba1\u9053\u901a\u8fc7GUI\u63a2\u7d22\u5408\u6210\u4eba\u7c7b\u6ed1\u52a8\u4ea4\u4e92\uff0c\u57fa\u4e8e\u6b64\u6784\u5efa\u57fa\u51c6\u5e76\u5f00\u53d1GUISwiper\u4ee3\u7406", "result": "GUISwiper\u8fbe\u523069.07%\u7684\u6ed1\u52a8\u6267\u884c\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u73b0\u6709VLM\u57fa\u7ebf\u63d0\u5347214%", "conclusion": "\u901a\u8fc7\u91cf\u5316\u4eba\u7c7b\u6ed1\u52a8\u624b\u52bf\u5e76\u81ea\u52a8\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u80fd\u663e\u8457\u63d0\u5347GUI\u4ee3\u7406\u7684\u4ea4\u4e92\u6267\u884c\u80fd\u529b\uff0c\u4e3aGUI\u81ea\u52a8\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18385", "abs": "https://arxiv.org/abs/2601.18385", "authors": ["Rinka Kawano", "Masaki Kawamura"], "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals", "comment": null, "summary": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u5f62\u5bfc\u9891\u4fe1\u53f7\u7684\u6570\u5b57\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u51e0\u4f55\u53d8\u6362\u540e\u7f51\u683c\u7684\u53d8\u5f62\u6765\u4f30\u8ba1\u53d8\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u5bf9\u88c1\u526a\u3001\u7f29\u653e\u3001\u65cb\u8f6c\u7b49\u64cd\u4f5c\u7684\u9c81\u68d2\u540c\u6b65\u3002", "motivation": "\u73b0\u6709\u6570\u5b57\u6c34\u5370\u65b9\u6cd5\u5bf9\u88c1\u526a\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u800c\u88c1\u526a\u4f1a\u6539\u53d8\u56fe\u50cf\u539f\u70b9\uff0c\u4f7f\u6c34\u5370\u540c\u6b65\u53d8\u5f97\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u80fd\u51c6\u786e\u68c0\u6d4b\u51e0\u4f55\u53d8\u6362\u5e76\u5b9e\u73b0\u540c\u6b65\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u56fe\u50cf\u4e2d\u5d4c\u5165\u7f51\u683c\u5f62\u5bfc\u9891\u4fe1\u53f7\uff0c\u6c34\u5e73\u548c\u5782\u76f4\u7ebf\u91c7\u7528\u4e0d\u540c\u7f16\u7801\u3002\u5f53\u56fe\u50cf\u7ecf\u5386\u51e0\u4f55\u53d8\u6362\u65f6\uff0c\u7f51\u683c\u4e5f\u4f1a\u76f8\u5e94\u53d8\u5f62\u3002\u901a\u8fc7Radon\u53d8\u6362\u5206\u6790\u53d8\u5f62\u7f51\u683c\u7684\u89d2\u5ea6\u548c\u95f4\u9694\uff0c\u4f30\u8ba1\u53d8\u6362\u77e9\u9635\u3002\u4e0d\u540c\u7f16\u7801\u7684\u7f51\u683c\u7ebf\u8fd8\u80fd\u786e\u5b9a\u7f51\u683c\u65b9\u5411\uff0c\u51cf\u5c11\u6a21\u7cca\u6027\u3002", "result": "\u5728\u5404\u9879\u5f02\u6027\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u526a\u5207\u548c\u88c1\u526a\u7b49\u653b\u51fb\u4e0b\u8fdb\u884c\u4eff\u771f\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u4f30\u8ba1\u53d8\u6362\u77e9\u9635\uff0c\u5728\u5355\u4e00\u548c\u590d\u5408\u653b\u51fb\u4e0b\u5747\u4fdd\u6301\u4f4e\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u7f51\u683c\u5bfc\u9891\u4fe1\u53f7\u7684\u6c34\u5370\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u51e0\u4f55\u53d8\u6362\uff0c\u5b9e\u73b0\u5bf9\u88c1\u526a\u7b49\u64cd\u4f5c\u7684\u9c81\u68d2\u540c\u6b65\uff0c\u4e3a\u89e3\u51b3\u6c34\u5370\u540c\u6b65\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.18777", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18777", "abs": "https://arxiv.org/abs/2601.18777", "authors": ["Abhishek Divekar", "Anirban Majumder"], "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation", "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)", "summary": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.", "AI": {"tldr": "\u63d0\u51faPRECISE\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u4e0eLLM\u5224\u65ad\uff0c\u663e\u8457\u964d\u4f4e\u68c0\u7d22\u7cfb\u7edf\u8bc4\u4f30\u7684\u6807\u6ce8\u9700\u6c42\uff0c\u540c\u65f6\u6821\u6b63LLM\u504f\u89c1", "motivation": "\u4f20\u7edf\u68c0\u7d22\u3001\u6392\u5e8f\u548cRAG\u7cfb\u7edf\u8bc4\u4f30\u9700\u8981\u5927\u91cf\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u73b0\u6709LLM\u81ea\u52a8\u8bc4\u4f30\u5b58\u5728\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u6269\u5c55\u9884\u6d4b\u9a71\u52a8\u63a8\u7406(PPI)\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u67e5\u8be2(100\u4e2a)\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6837\u672c(10,000\u4e2a)\uff0c\u91cd\u65b0\u5b9a\u4e49\u5ea6\u91cf\u96c6\u6210\u7a7a\u95f4\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(2^|C|)\u964d\u81f3O(2^K)", "result": "\u5728\u591a\u4e2a\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPRECISE\u80fd\u51cf\u5c11Precision@K\u5ea6\u91cf\u7684\u4f30\u8ba1\u65b9\u5dee\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u6709\u6548\u6821\u6b63LLM\u504f\u89c1", "conclusion": "PRECISE\u6846\u67b6\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6807\u6ce8\u9700\u6c42\uff0c\u63d0\u4f9b\u53ef\u9760\u5ea6\u91cf\u4f30\u8ba1\uff0c\u4e3aLLM\u589e\u5f3a\u7684\u67e5\u8be2\u91cd\u5199\u7b49\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u8bc4\u4f30\u65b9\u6848"}}
{"id": "2601.18464", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18464", "abs": "https://arxiv.org/abs/2601.18464", "authors": ["Wenbin Wei", "Suyuan Yao", "Cheng Huang", "Xiangyu Gao"], "title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System", "comment": null, "summary": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.", "AI": {"tldr": "Fair-Eye Net\u662f\u4e00\u4e2a\u516c\u5e73\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u9752\u5149\u773c\u7b5b\u67e5\u3001\u968f\u8bbf\u548c\u98ce\u9669\u9884\u8b66\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4e34\u5e8a\u6570\u636e\u5e76\u4f18\u5316\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u79cd\u65cf\u8bca\u65ad\u5dee\u5f02\u3002", "motivation": "\u9752\u5149\u773c\u662f\u5168\u7403\u4e0d\u53ef\u9006\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5f53\u524d\u7b5b\u67e5\u548c\u8fdb\u5c55\u8bc4\u4f30\u4f9d\u8d56\u5355\u4e00\u6d4b\u8bd5\u6216\u677e\u6563\u5173\u8054\u7684\u68c0\u67e5\uff0c\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u788e\u7247\u5316\u62a4\u7406\u95ee\u9898\u3002\u9ad8\u8d28\u91cf\u6210\u50cf\u5de5\u5177\u548c\u4e13\u5bb6\u8d44\u6e90\u7684\u6709\u9650\u83b7\u53d6\u8fdb\u4e00\u6b65\u5f71\u54cd\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u5f00\u53d1\u4e86Fair-Eye Net\u7cfb\u7edf\uff0c\u6574\u5408\u773c\u5e95\u7167\u7247\u3001OCT\u7ed3\u6784\u6307\u6807\u3001VF\u529f\u80fd\u6307\u6570\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u56e0\u7d20\uff0c\u91c7\u7528\u53cc\u6d41\u5f02\u6784\u878d\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5206\u5c42\u95e8\u63a7\u7b56\u7565\u8fdb\u884c\u9009\u62e9\u6027\u9884\u6d4b\u548c\u5b89\u5168\u8f6c\u8bca\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u7ea6\u675f\u51cf\u5c11\u5f31\u52bf\u4e9a\u7ec4\u7684\u6f0f\u8bca\u3002", "result": "\u7cfb\u7edf\u8fbe\u5230AUC 0.912\uff08\u7279\u5f02\u602796.7%\uff09\uff0c\u5c06\u79cd\u65cf\u5047\u9634\u6027\u5dee\u5f02\u51cf\u5c11\u4e8673.4%\uff08\u4ece12.31%\u964d\u81f33.28%\uff09\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u8de8\u57df\u6027\u80fd\uff0c\u5b9e\u73b03-12\u4e2a\u6708\u7684\u65e9\u671f\u98ce\u9669\u9884\u8b66\uff08\u654f\u611f\u602792%\uff0c\u7279\u5f02\u602788%\uff09\u3002", "conclusion": "Fair-Eye Net\u5c06\u516c\u5e73\u6027\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u786e\u4fdd\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u8f6c\u5316\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u5168\u7403\u773c\u5065\u5eb7\u516c\u5e73\u3002"}}
{"id": "2601.18577", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18577", "abs": "https://arxiv.org/abs/2601.18577", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Saining Xie", "Jaehong Yoon", "Sung Ju Hwang"], "title": "Self-Refining Video Sampling", "comment": "Project page: https://agwmon.github.io/self-refine-video/", "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u7cbe\u70bc\u89c6\u9891\u91c7\u6837\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u4f5c\u4e3a\u81ea\u8eab\u7684\u7cbe\u70bc\u5668\uff0c\u901a\u8fc7\u8fed\u4ee3\u5185\u5faa\u73af\u4f18\u5316\u63d0\u5347\u7269\u7406\u771f\u5b9e\u611f\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u4ee3\u89c6\u9891\u751f\u6210\u5668\u5728\u5904\u7406\u590d\u6742\u7269\u7406\u52a8\u529b\u5b66\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u7269\u7406\u771f\u5b9e\u611f\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u89e3\u91ca\u4e3a\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u8fed\u4ee3\u5185\u5faa\u73af\u7cbe\u70bc\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u989d\u5916\u8bad\u7ec3\u3002\u5f15\u5165\u57fa\u4e8e\u81ea\u4e00\u81f4\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7cbe\u70bc\u7b56\u7565\uff0c\u9009\u62e9\u6027\u7cbe\u70bc\u533a\u57df\u4ee5\u9632\u6b62\u8fc7\u5ea6\u7cbe\u70bc\u5bfc\u81f4\u7684\u4f2a\u5f71\u3002", "result": "\u5728\u5148\u8fdb\u89c6\u9891\u751f\u6210\u5668\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u7269\u7406\u5bf9\u9f50\u663e\u8457\u6539\u5584\uff0c\u76f8\u6bd4\u9ed8\u8ba4\u91c7\u6837\u5668\u548c\u57fa\u4e8e\u5f15\u5bfc\u7684\u91c7\u6837\u5668\u83b7\u5f97\u8d85\u8fc770%\u7684\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u81ea\u7cbe\u70bc\u89c6\u9891\u91c7\u6837\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u771f\u5b9e\u611f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u601d\u8def\u3002"}}
{"id": "2601.18589", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18589", "abs": "https://arxiv.org/abs/2601.18589", "authors": ["KV Karthikeya", "Ashok Kumar Das", "Shantanu Pal", "Vivekananda Bhat K", "Arun Sekar Rajasekaran"], "title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment", "comment": null, "summary": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.", "AI": {"tldr": "\u63d0\u51faAGSP-DSA\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u56fe\u6784\u5efa\u3001\u8c31\u56fe\u6ee4\u6ce2\u548c\u591a\u5c3a\u5ea6GCN\u5b9e\u73b0\u8de8\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\uff08\u6587\u672c\u3001\u97f3\u9891\u3001\u56fe\u50cf\uff09\u7684\u9c81\u68d2\u878d\u5408\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e0b\u3002", "method": "1. \u53cc\u56fe\u6784\u5efa\u5b66\u4e60\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u5173\u7cfb\uff1b2. \u8c31\u56fe\u6ee4\u6ce2\u589e\u5f3a\u4fe1\u606f\u4fe1\u53f7\uff1b3. \u591a\u5c3a\u5ea6GCN\u8fdb\u884c\u8282\u70b9\u5d4c\u5165\uff1b4. \u8bed\u4e49\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u8c03\u6574\u5404\u6a21\u6001\u8d21\u732e\u3002", "result": "\u5728CMU-MOSEI\u4e0a\u8fbe\u523095.3%\u51c6\u786e\u7387\u30010.936 F1\u5206\u6570\u30010.924 mAP\uff0c\u6bd4MM-GNN\u63d0\u53472.6%\uff1b\u5728AVE\u4e0a\u8fbe\u523093.4%\u51c6\u786e\u7387\u548c0.911 F1\u5206\u6570\uff1b\u5728MM-IMDB\u4e0a\u8fbe\u523091.8%\u51c6\u786e\u7387\u548c0.886 F1\u5206\u6570\u3002", "conclusion": "AGSP-DSA\u6846\u67b6\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u60c5\u611f\u5206\u6790\u3001\u4e8b\u4ef6\u8bc6\u522b\u548c\u591a\u5a92\u4f53\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
