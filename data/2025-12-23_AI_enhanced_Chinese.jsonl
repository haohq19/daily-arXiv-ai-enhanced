{"id": "2512.17939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.17939", "abs": "https://arxiv.org/abs/2512.17939", "authors": ["Yuncheng Lu", "Yucen Shi", "Aobo Li", "Zehao Li", "Junying Li", "Bo Wang", "Tony Tae-Hyoung Kim"], "title": "A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes", "comment": "2 pages, 7 figures, conference paper published in IEEE Asian Solid-State Circuits Conference 2025", "summary": "We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u80fd\u6548\u4f18\u5316\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7ed3\u5408\u5e27\u57fa\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u76ee\u6807\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u5bf9\u5c0f\u800c\u5feb\u901f\u79fb\u52a8\u7684\u65e0\u4eba\u673a\u53ef\u9760\u68c0\u6d4b\uff0c\u82af\u7247\u572840nm\u5de5\u827a\u4e0b\u8fbe\u523096pJ/\u5e27/\u50cf\u7d20\u548c61pJ/\u4e8b\u4ef6\u7684\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u68c0\u6d4b\u5c0f\u800c\u5feb\u901f\u79fb\u52a8\u7684\u65e0\u4eba\u673a\u65f6\u9762\u4e34\u80fd\u6548\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u548c\u4f4e\u529f\u8017\u8981\u6c42\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7535\u6c60\u4f9b\u7535\u7684\u79fb\u52a8\u8bbe\u5907\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u5e27\u57fa\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u76ee\u6807\u8ddf\u8e2a\uff0c\u4f7f\u7528\u6e38\u7a0b\u7f16\u7801\u91cd\u5efa\u4e8c\u503c\u4e8b\u4ef6\u5e27\uff0c\u751f\u6210\u533a\u57df\u5efa\u8bae\uff0c\u6839\u636e\u76ee\u6807\u5927\u5c0f\u548c\u901f\u5ea6\u81ea\u9002\u5e94\u5207\u6362\u5e27\u6a21\u5f0f\u4e0e\u4e8b\u4ef6\u6a21\u5f0f\u3002\u5feb\u901f\u76ee\u6807\u8ddf\u8e2a\u5355\u5143\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u8f68\u8ff9\u5206\u7c7b\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u795e\u7ecf\u5904\u7406\u5355\u5143\u652f\u6301\u7070\u5ea6\u5757\u548c\u8f68\u8ff9\u63a8\u7406\uff0c\u91c7\u7528\u81ea\u5b9a\u4e49\u6307\u4ee4\u96c6\u548c\u96f6\u8df3\u8fc7MAC\u67b6\u6784\u3002", "result": "\u572840nm CMOS\u5de5\u827a\u4e0b\u5b9e\u73b02mm\u00b2\u82af\u7247\uff0c\u8fbe\u523096pJ/\u5e27/\u50cf\u7d20\u548c61pJ/\u4e8b\u4ef6\u7684\u80fd\u6548\uff0c\u5728\u516c\u5171\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b098.2%\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u8986\u76d650-400\u7c73\u8303\u56f4\u548c5-80\u50cf\u7d20/\u79d2\u901f\u5ea6\uff0c\u51cf\u5c1197%\u4ee5\u4e0a\u5197\u4f59\u795e\u7ecf\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u80fd\u6548\uff0c\u901a\u8fc7\u6df7\u5408\u5e27\u57fa\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u529f\u8017\uff0c\u4e3a\u79fb\u52a8\u53cd\u65e0\u4eba\u673a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18028", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18028", "abs": "https://arxiv.org/abs/2512.18028", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation", "comment": null, "summary": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.", "AI": {"tldr": "Embodied4C\u662f\u4e00\u4e2a\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u63a8\u7406\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u673a\u68b0\u81c2\u4e09\u79cd\u5f02\u6784\u5177\u8eab\u5e73\u53f0\uff0c\u901a\u8fc71100\u4e2a\u4e00\u6b21\u6027\u63a8\u7406\u95ee\u9898\u548c58\u4e2a\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u4efb\u52a1\u8bc4\u4f30\u8bed\u4e49\u3001\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u7269\u7406\u56db\u4e2a\u7ef4\u5ea6\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u5177\u8eab\u6027\uff08\u7269\u7406\u5e73\u53f0\u3001\u4f20\u611f\u5668\u914d\u7f6e\u548c\u6a21\u6001\u5bf9\u9f50\uff09\u5982\u4f55\u5f71\u54cd\u611f\u77e5\u3001\u63a8\u7406\u548c\u63a7\u5236\u7684\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u8eab\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u8bbe\u8ba1Embodied4C\u4f5c\u4e3a\u5177\u8eab\u63a8\u7406\u7684\u56fe\u7075\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u79cd\u5f02\u6784\u5177\u8eab\u5e73\u53f0\uff08\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u3001\u673a\u68b0\u81c2\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u4f20\u611f\u5668\u914d\u7f6e\u548c\u73af\u5883\u53d8\u5316\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u96c6\u6210\u9886\u57df\u5916\u67e5\u8be2\u4ee5\u9632\u6b62\u5177\u8eab\u8fc7\u62df\u5408\u3002", "result": "\u5bf910\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c4\u4e2a\u5177\u8eab\u63a7\u5236\u57fa\u7ebf\u7684\u8bc4\u4f30\u8868\u660e\uff1a\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6307\u4ee4\u8c03\u4f18\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u91cd\u8981\uff0c\u800c\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u53ef\u9760\u5177\u8eab\u80fd\u529b\u7684\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "Embodied4C\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u63a8\u7406\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u597d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6269\u5927\u6a21\u578b\u89c4\u6a21\u3002"}}
{"id": "2512.17984", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17984", "abs": "https://arxiv.org/abs/2512.17984", "authors": ["Mohammadmahdi Rahimiasl", "Ynte Vanderhoydonc", "Siegfried Mercelis"], "title": "A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations", "comment": "10 pages, 8 figures, 3 tables", "summary": "Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.\n  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\\approx42$% with basic simulation and $\\approx50$% with calibrated simulation; on Torino by $\\approx22$%, and on Essen by $\\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.", "AI": {"tldr": "HINT\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f52\u7eb3-\u8f6c\u5bfc\u7f51\u7edc\uff0c\u7528\u4e8e\u4ea4\u901a\u6d41\u91cf\u63d2\u8865\uff0c\u7ed3\u5408\u901f\u5ea6\u7684\u8f6c\u5bfc\u4fe1\u53f7\u548c\u6d41\u91cf\u7684\u5f52\u7eb3\u5b66\u4e60\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ea4\u901a\u6d41\u91cf\u63d2\u8865\u9762\u4e34\u6311\u6218\uff1a\u73af\u68c0\u6d4b\u5668\u6570\u636e\u7cbe\u786e\u4f46\u7a00\u758f\uff0c\u63a2\u6d4b\u8f66\u901f\u5ea6\u6570\u636e\u5e7f\u6cdb\u4f46\u4ec5\u4e0e\u6d41\u91cf\u5f31\u76f8\u5173\uff0c\u76f8\u90bb\u8def\u6bb5\u6d41\u91cf\u5c3a\u5ea6\u5dee\u5f02\u5927\uff08\u5982\u531d\u9053\u4e0e\u4e3b\u7ebf\uff09\uff0c\u8fd9\u6253\u7834\u4e86\u6807\u51c6GNN\u7684\u5047\u8bbe\u3002", "method": "HINT\u91c7\u7528\u6df7\u5408\u5f52\u7eb3-\u8f6c\u5bfc\u7f51\u7edc\uff1a1\uff09\u5f52\u7eb3\u7a7a\u95f4\u53d8\u6362\u5668\u5b66\u4e60\u8282\u70b9\u7279\u5f81\u9a71\u52a8\u7684\u957f\u7a0b\u4ea4\u4e92\uff1b2\uff09\u57fa\u4e8eFiLM\u7684\u6269\u6563GCN\uff0c\u5229\u7528\u4e30\u5bcc\u9759\u6001\u4e0a\u4e0b\u6587\uff08OSM\u5c5e\u6027\u548c\u4ea4\u901a\u4eff\u771f\uff09\uff1b3\uff09\u8282\u70b9\u7ea7\u6821\u51c6\u5c42\u4fee\u6b63\u5404\u8def\u6bb5\u5c3a\u5ea6\u504f\u5dee\u3002\u8bad\u7ec3\u4f7f\u7528\u63a9\u7801\u91cd\u5efa\u3001\u9010\u8f6e\u8282\u70b9\u91c7\u6837\u3001\u56f0\u96be\u8282\u70b9\u6316\u6398\u548c\u53ef\u89c1\u6d41\u91cf\u566a\u58f0\u6ce8\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff08MOW\u3001UTD19-Torino\u3001UTD19-Essen\uff09\u4e0a\uff0cHINT\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u5f52\u7eb3\u57fa\u7ebf\u3002\u76f8\u6bd4KITS\uff0c\u5728MOW\u4e0aMAE\u964d\u4f4e\u7ea642%\uff08\u57fa\u7840\u4eff\u771f\uff09\u548c50%\uff08\u6821\u51c6\u4eff\u771f\uff09\uff1b\u5728Torino\u964d\u4f4e\u7ea622%\uff1b\u5728Essen\u964d\u4f4e\u7ea612%\u3002\u5373\u4f7f\u6ca1\u6709\u4eff\u771f\uff0cHINT\u5728MOW\u548cTorino\u4e0a\u4ecd\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5c06\u5f52\u7eb3\u6d41\u91cf\u63d2\u8865\u4e0e\u8f6c\u5bfc\u901f\u5ea6\u3001\u4ea4\u901a\u4eff\u771f\u548c\u5916\u90e8\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u76f8\u7ed3\u5408\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u4ea4\u901a\u6d41\u91cf\u63d2\u8865\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7a00\u758f\u4f20\u611f\u5668\u548c\u5f02\u8d28\u6d41\u91cf\u5c3a\u5ea6\u65f6\u3002"}}
{"id": "2512.18041", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18041", "abs": "https://arxiv.org/abs/2512.18041", "authors": ["Roger A. Finger", "Eduardo G. Cortes", "Sandro J. Rigo", "Gabriel de O. Ramos"], "title": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts", "comment": null, "summary": "Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53d9\u4e8b\u6574\u5408\u65b0\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u800c\u975e\u4f20\u7edf\u6458\u8981\u7684\u538b\u7f29\u3002\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u5bf9\u9f50\u4e8b\u4ef6\u56fe(TAEG)\u548c\u4e2d\u5fc3\u6027\u7b97\u6cd5\uff0c\u5728\u5723\u7ecf\u798f\u97f3\u4e66\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u65f6\u95f4\u6392\u5e8f\u548c\u663e\u8457\u5185\u5bb9\u6539\u8fdb\u3002", "motivation": "\u5904\u7406\u91cd\u53e0\u53d9\u4e8b\u6587\u6863\uff08\u5982\u6cd5\u5f8b\u8bc1\u8bcd\u3001\u5386\u53f2\u8bb0\u5f55\uff09\u65f6\uff0c\u4f20\u7edf\u591a\u6587\u6863\u6458\u8981(MDS)\u8fc7\u4e8e\u5173\u6ce8\u538b\u7f29\u800c\u7834\u574f\u4e86\u53d9\u4e8b\u6d41\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u5b8c\u6574\u6027\u3001\u5185\u5bb9\u5b8c\u6574\u6027\u548c\u7ec6\u8282\u878d\u5408\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u538b\u7f29\u3002", "method": "\u63d0\u51fa\u53d9\u4e8b\u6574\u5408\u4efb\u52a1\uff0c\u5f15\u5165\u65f6\u95f4\u5bf9\u9f50\u4e8b\u4ef6\u56fe(TAEG)\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u987a\u5e8f\u548c\u4e8b\u4ef6\u5bf9\u9f50\u3002\u4f7f\u7528\u6807\u51c6\u4e2d\u5fc3\u6027\u7b97\u6cd5\u4f5c\u4e3a\u7248\u672c\u9009\u62e9\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u4e8b\u4ef6\u9009\u62e9\u6700\u4e2d\u5fc3\u7684\u8868\u793a\uff0c\u5e76\u4fdd\u6301\u6b63\u786e\u7684\u65f6\u95f4\u4f4d\u7f6e\u3002", "result": "\u5728\u56db\u90e8\u5723\u7ecf\u798f\u97f3\u4e66\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u4fdd\u8bc1\u4e86\u5b8c\u7f8e\u7684\u65f6\u95f4\u6392\u5e8f\uff08Kendall's Tau\u4e3a1.000\uff09\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u6307\u6807\uff08\u5982ROUGE-L F1\u63d0\u9ad8\u4e86357.2%\uff09\u3002", "conclusion": "\u53d9\u4e8b\u6574\u5408\u662f\u4e00\u4e2a\u76f8\u5173\u4e14\u91cd\u8981\u7684NLP\u4efb\u52a1\uff0c\u663e\u5f0f\u7684\u65f6\u95f4\u9aa8\u67b6\u662f\u5176\u89e3\u51b3\u7684\u57fa\u672c\u7ec4\u6210\u90e8\u5206\u3002\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u4efb\u52a1\u5b9a\u4e49\u7684\u5408\u7406\u6027\uff0c\u5e76\u4e3a\u5904\u7406\u91cd\u53e0\u53d9\u4e8b\u6587\u6863\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.17986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17986", "abs": "https://arxiv.org/abs/2512.17986", "authors": ["S M Ruhul Kabir Howlader", "Xiao Chen", "Yifei Xie", "Lu Liu"], "title": "FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability", "comment": null, "summary": "Over the last few decades, machine learning (ML) and deep learning (DL) solutions have demonstrated their potential across many applications by leveraging large amounts of high-quality data. However, strict data-sharing regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) have prevented many data-driven applications from being realised. Federated Learning (FL), in which raw data never leaves local devices, has shown promise in overcoming these limitations. Although FL has grown rapidly in recent years, it still struggles with heterogeneity, which produces gradient noise, client-drift, and increased variance from partial client participation. In this paper, we propose FedOAED, a novel federated learning algorithm designed to mitigate client-drift arising from multiple local training updates and the variance induced by partial client participation. FedOAED incorporates an on-device autoencoder denoiser on the client side to mitigate client-drift and variance resulting from heterogeneous data under limited client availability. Experiments on multiple vision datasets under Non-IID settings demonstrate that FedOAED consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "FedOAED\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u8bbe\u5907\u4e0a\u7684\u81ea\u7f16\u7801\u5668\u964d\u566a\u5668\u6765\u7f13\u89e3\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u5f15\u8d77\u7684\u65b9\u5dee\u95ee\u9898\uff0c\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u5f02\u6784\u6027\u5e26\u6765\u7684\u6311\u6218\uff1a\u68af\u5ea6\u566a\u58f0\u3001\u5ba2\u6237\u7aef\u6f02\u79fb\u4ee5\u53ca\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u5bfc\u81f4\u7684\u65b9\u5dee\u589e\u52a0\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86FL\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "FedOAED\u7b97\u6cd5\u5728\u5ba2\u6237\u7aef\u4fa7\u96c6\u6210\u4e86\u8bbe\u5907\u4e0a\u7684\u81ea\u7f16\u7801\u5668\u964d\u566a\u5668\uff0c\u7528\u4e8e\u7f13\u89e3\u7531\u591a\u4e2a\u672c\u5730\u8bad\u7ec3\u66f4\u65b0\u5f15\u8d77\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u4ee5\u53ca\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u5bfc\u81f4\u7684\u65b9\u5dee\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u5f02\u6784\u6570\u636e\u5206\u5e03\u548c\u6709\u9650\u5ba2\u6237\u7aef\u53ef\u7528\u6027\u573a\u666f\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u8bbe\u7f6e\u5b9e\u9a8c\u4e2d\uff0cFedOAED\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7f13\u89e3\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u65b9\u5dee\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedOAED\u901a\u8fc7\u521b\u65b0\u7684\u8bbe\u5907\u7aef\u81ea\u7f16\u7801\u5668\u964d\u566a\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u90e8\u5206\u53c2\u4e0e\u65b9\u5dee\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18225", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.18225", "abs": "https://arxiv.org/abs/2512.18225", "authors": ["Deepit Sapru"], "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs", "comment": null, "summary": "This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.", "AI": {"tldr": "GeoSense-AI\uff1a\u4e00\u4e2a\u7528\u4e8e\u4ece\u5608\u6742\u7684\u5fae\u535a\u6d41\u4e2d\u5b9e\u65f6\u5730\u7406\u5b9a\u4f4d\u7684\u5e94\u7528AI\u7ba1\u9053\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cdNLP\u6280\u672f\u548c\u5730\u7406\u77e5\u8bc6\u5e93\uff0c\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u4f4d\u7f6e\u63a8\u65ad\u3002", "motivation": "\u4f20\u7edf\u7684\u5730\u7406\u6807\u7b7e\uff08geotags\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u901a\u5e38\u7a00\u758f\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u9700\u8981\u5feb\u901f\u83b7\u53d6\u4f4d\u7f6e\u4fe1\u606f\u65f6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u5730\u7406\u4f4d\u7f6e\u7684\u5b9e\u65f6\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u6001\u52bf\u611f\u77e5\u548c\u54cd\u5e94\u3002", "method": "\u6574\u5408\u4e86\u7edf\u8ba1\u6027\u6807\u7b7e\u5206\u5272\u3001\u8bcd\u6027\u9a71\u52a8\u7684\u4e13\u6709\u540d\u8bcd\u68c0\u6d4b\u3001\u56f4\u7ed5\u707e\u5bb3\u8bcd\u5178\u7684\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u3001\u8f7b\u91cf\u7ea7\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4ee5\u53ca\u57fa\u4e8e\u5730\u540d\u5f55\u7684\u6d88\u6b67\u3002\u6574\u4e2a\u7cfb\u7edf\u9488\u5bf9\u6d41\u5f0f\u5904\u7406\u7ea6\u675f\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u4f4e\u5ef6\u8fdfNLP\u7ec4\u4ef6\u548c\u9ad8\u6548\u7684\u5730\u7406\u77e5\u8bc6\u5e93\u9a8c\u8bc1\u3002", "result": "\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684NER\u5de5\u5177\u5305\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u4fdd\u6301\u9ad8F1\u5206\u6570\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u66f4\u5feb\u7684\u541e\u5410\u91cf\uff0c\u80fd\u591f\u5728\u5b9e\u65f6\u5371\u673a\u4fe1\u606f\u5b66\u73af\u5883\u4e2d\u90e8\u7f72\u3002\u751f\u4ea7\u5730\u56fe\u754c\u9762\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u7684AI\u529f\u80fd\uff0c\u5305\u62ec\u6570\u636e\u6444\u5165\u3001\u63a8\u7406\u548c\u53ef\u89c6\u5316\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5148\u8003\u8651\u5bf9\u975e\u6b63\u5f0f\u6587\u672c\u7684\u9c81\u68d2\u6027\u548c\u6d41\u5f0f\u5904\u7406\u6548\u7387\uff0cGeoSense-AI\u5c55\u793a\u4e86\u9886\u57df\u8c03\u4f18\u7684NLP\u548c\u77e5\u8bc6\u57fa\u7840\u5982\u4f55\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u5730\u7406\u6807\u7b7e\u4f9d\u8d56\uff0c\u63d0\u5347\u5e94\u6025\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2512.18129", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.18129", "abs": "https://arxiv.org/abs/2512.18129", "authors": ["Maxmillan Ries", "Sohan Seth"], "title": "TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates", "comment": null, "summary": "Survival analysis is a critical tool for modeling time-to-event data. Recent deep learning-based models have reduced various modeling assumptions including proportional hazard and linearity. However, a persistent challenge remains in incorporating longitudinal covariates, with prior work largely focusing on cross-sectional features, and in assessing calibration of these models, with research primarily focusing on discrimination during evaluation. We introduce TraCeR, a transformer-based survival analysis framework for incorporating longitudinal covariates. Based on a factorized self-attention architecture, TraCeR estimates the hazard function from a sequence of measurements, naturally capturing temporal covariate interactions without assumptions about the underlying data-generating process. The framework is inherently designed to handle censored data and competing events. Experiments on multiple real-world datasets demonstrate that TraCeR achieves substantial and statistically significant performance improvements over state-of-the-art methods. Furthermore, our evaluation extends beyond discrimination metrics and assesses model calibration, addressing a key oversight in literature.", "AI": {"tldr": "TraCeR\uff1a\u57fa\u4e8eTransformer\u7684\u751f\u5b58\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u7eb5\u5411\u534f\u53d8\u91cf\uff0c\u65e0\u9700\u6bd4\u4f8b\u98ce\u9669\u5047\u8bbe\uff0c\u540c\u65f6\u8bc4\u4f30\u6a21\u578b\u6821\u51c6\u6027", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u5206\u6790\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6a2a\u622a\u9762\u7279\u5f81\uff0c\u96be\u4ee5\u6709\u6548\u7eb3\u5165\u7eb5\u5411\u534f\u53d8\u91cf\uff0c\u4e14\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u533a\u5206\u5ea6\u800c\u5ffd\u89c6\u6821\u51c6\u6027\u8bc4\u4f30", "method": "\u57fa\u4e8e\u5206\u89e3\u81ea\u6ce8\u610f\u529b\u67b6\u6784\u7684Transformer\u6846\u67b6\uff0c\u4ece\u6d4b\u91cf\u5e8f\u5217\u4f30\u8ba1\u98ce\u9669\u51fd\u6570\uff0c\u81ea\u7136\u6355\u83b7\u65f6\u95f4\u534f\u53d8\u91cf\u4ea4\u4e92\uff0c\u65e0\u9700\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u5047\u8bbe\uff0c\u53ef\u5904\u7406\u5220\u5931\u6570\u636e\u548c\u7ade\u4e89\u4e8b\u4ef6", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cTraCeR\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u4e14\u7edf\u8ba1\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "TraCeR\u6210\u529f\u89e3\u51b3\u4e86\u7eb5\u5411\u534f\u53d8\u91cf\u7eb3\u5165\u548c\u6821\u51c6\u6027\u8bc4\u4f30\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4e3a\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6"}}
{"id": "2512.18336", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18336", "abs": "https://arxiv.org/abs/2512.18336", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u52a8\u6001\u71b5\u8c03\u4f18\u5bf9\u968f\u673a\u7b56\u7565\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u8bad\u7ec3\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u52a8\u6001\u71b5\u8c03\u4f18\u5728\u56db\u65cb\u7ffc\u63a7\u5236\u4e2d\u80fd\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u6001\u71b5\u8c03\u4f18\u5728\u5f3a\u5316\u5b66\u4e60\u968f\u673a\u7b56\u7565\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u8bad\u7ec3\u76f8\u6bd4\uff0c\u63a2\u7d22\u5176\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\uff08\u5982\u56db\u65cb\u7ffc\u63a7\u5236\uff09\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528SAC\u7b97\u6cd5\uff08\u968f\u673a\u7b56\u7565\uff09\u4e0eTD3\u7b97\u6cd5\uff08\u786e\u5b9a\u6027\u7b56\u7565\uff09\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u5728SAC\u4e2d\u6d4b\u8bd5\u9759\u6001\u71b5\u548c\u52a8\u6001\u71b5\u4e24\u79cd\u8bbe\u7f6e\uff0c\u7136\u540e\u6267\u884c\u786e\u5b9a\u6027\u52a8\u4f5c\u63a7\u5236\u56db\u65cb\u7ffc\uff0c\u5e76\u4e0eTD3\u7684\u786e\u5b9a\u6027\u7b56\u7565\u8bad\u7ec3\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8bad\u7ec3\u548c\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u52a8\u6001\u71b5\u8c03\u4f18\u5728\u56db\u65cb\u7ffc\u63a7\u5236\u4e2d\u5177\u6709\u79ef\u6781\u6548\u679c\uff1a\u80fd\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "\u52a8\u6001\u71b5\u8c03\u4f18\u5728\u5f3a\u5316\u5b66\u4e60\u968f\u673a\u7b56\u7565\u8bad\u7ec3\u4e2d\u4f18\u4e8e\u9759\u6001\u71b5\u8bbe\u7f6e\uff0c\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u80fd\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002"}}
{"id": "2512.18180", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18180", "abs": "https://arxiv.org/abs/2512.18180", "authors": ["Rebecca Salganik", "Yibin Wang", "Guillaume Salha-Galvan", "Jian Kang"], "title": "FairExpand: Individual Fairness on Graphs with Partial Similarity Information", "comment": null, "summary": "Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.", "AI": {"tldr": "FairExpand\uff1a\u4e00\u79cd\u5728\u90e8\u5206\u76f8\u4f3c\u6027\u4fe1\u606f\u573a\u666f\u4e0b\u4fc3\u8fdb\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u7075\u6d3b\u6846\u67b6", "motivation": "\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u65b9\u6cd5\u9700\u8981\u6240\u6709\u8282\u70b9\u5bf9\u7684\u9884\u5b9a\u4e49\u76f8\u4f3c\u6027\u4fe1\u606f\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u9488\u5bf9\u66f4\u73b0\u5b9e\u7684\u573a\u666f\uff0c\u5373\u76f8\u4f3c\u6027\u4fe1\u606f\u4ec5\u5bf9\u6709\u9650\u8282\u70b9\u5bf9\u53ef\u7528\u7684\u60c5\u51b5\u3002", "method": "FairExpand\u91c7\u7528\u4e24\u6b65\u6d41\u6c34\u7ebf\uff1a1) \u4f7f\u7528\u9aa8\u5e72\u6a21\u578b\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u7cbe\u70bc\u8282\u70b9\u8868\u793a\uff1b2) \u9010\u6b65\u4f20\u64ad\u76f8\u4f3c\u6027\u4fe1\u606f\u3002\u8fd9\u79cd\u4ea4\u66ff\u65b9\u6cd5\u5141\u8bb8\u516c\u5e73\u6027\u7ea6\u675f\u6709\u6548\u6269\u5c55\u5230\u6574\u4e2a\u56fe\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFairExpand\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u6301\u7eed\u589e\u5f3a\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u4f7f\u5176\u6210\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u5206\u76f8\u4f3c\u6027\u4fe1\u606f\u5e94\u7528\u4e2d\u5b9e\u73b0\u57fa\u4e8e\u56fe\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FairExpand\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u76f8\u4f3c\u6027\u4fe1\u606f\u7684\u5c40\u9650\u6027\uff0c\u5728\u7528\u6237\u5efa\u6a21\u3001\u63a8\u8350\u7cfb\u7edf\u548c\u641c\u7d22\u7b49\u9ad8\u98ce\u9669Web\u9886\u57df\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2512.18215", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18215", "abs": "https://arxiv.org/abs/2512.18215", "authors": ["Rui Liu", "Dian Yu", "Lei Ke", "Haolin Liu", "Yujun Zhou", "Zhenwen Liang", "Haitao Mi", "Pratap Tokekar", "Dong Yu"], "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.", "AI": {"tldr": "MSSR\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u7ec4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u4f18\u52bf\u6574\u5f62\u673a\u5236\u89e3\u51b3\u5355\u8f6e\u91c7\u6837\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7ec4\u7684RLVR\u7b97\u6cd5\u9700\u8981\u591a\u8f6e\u91c7\u6837\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u3002\u800c\u5355\u8f6e\u91c7\u6837\u53d8\u4f53\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\u3002\u9700\u8981\u89e3\u51b3\u8bad\u7ec3\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faMSSR\uff08\u591a\u6a21\u6001\u7a33\u5b9a\u5355\u8f6e\u91c7\u6837\uff09\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u71b5\u7684\u4f18\u52bf\u6574\u5f62\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u6b63\u5219\u5316\u4f18\u52bf\u5e45\u5ea6\uff0c\u9632\u6b62\u8bad\u7ec3\u5d29\u6e83\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u3002\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u591a\u6a21\u6001\u5355\u8f6e\u91c7\u6837RLVR\u8bbe\u8ba1\u7684\u7a33\u5b9a\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u5206\u5e03\u5185\u8bc4\u4f30\u4e2d\uff0cMSSR\u4ec5\u7528\u4e00\u534a\u7684\u8bad\u7ec3\u6b65\u6570\u5c31\u80fd\u8fbe\u5230\u4e0e\u57fa\u4e8e\u7ec4\u57fa\u7ebf\u76f8\u4f3c\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002\u5f53\u8bad\u7ec3\u76f8\u540c\u6b65\u6570\u65f6\uff0cMSSR\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5e76\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4e00\u81f4\u7684\u6cdb\u5316\u6539\u8fdb\u3002", "conclusion": "MSSR\u4e3a\u590d\u6742\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u6709\u6548\u7684RLVR\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5355\u8f6e\u91c7\u6837\u8bad\u7ec3\u7684\u6548\u7387-\u7a33\u5b9a\u6027\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2512.18462", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18462", "abs": "https://arxiv.org/abs/2512.18462", "authors": ["Christopher Rom\u00e1n Jaimes"], "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling", "comment": null, "summary": "Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u7684NLI\u6a21\u578b\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7LF-LMI\u68c0\u6d4b\u8bed\u4e49\u4f2a\u5f71\u3001LLM\u5408\u6210\u5bf9\u6bd4\u96c6\u3001\u52a8\u6001\u5e73\u8861\u91c7\u6837\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u4e00\u81f4\u6027", "motivation": "NLI\u6a21\u578b\u7ecf\u5e38\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u8bed\u4e49\u63a8\u7406\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u8981\u4e48\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u5728\u5fae\u8c03\u65f6\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8", "method": "1) \u63d0\u51faLF-LMI\u51c6\u786e\u68c0\u6d4b\u8bed\u4e49\u4f2a\u5f71\uff1b2) \u901a\u8fc7LLM\u5408\u6210\u7ba1\u9053\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u5bf9\u6bd4\u96c6\uff0c\u5e76\u8fdb\u884c\u591a\u6cd5\u5b98\u9a8c\u8bc1\uff1b3) \u5f15\u5165\u52a8\u6001\u5e73\u8861\u91c7\u6837\u8bad\u7ec3\u7b56\u7565\uff0c\u65cb\u8f6c\u539f\u59cb\u6570\u636e\u5206\u5e03\u4ee5\u9632\u6b62\u9057\u5fd8", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u81f4\u6027\u4ece63.5%\u63d0\u5347\u81f381.0%\uff0c\u540c\u65f6\u4fdd\u630188.4%\u7684\u57df\u5185\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6734\u7d20\u5fae\u8c03\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u7ba1\u9053\u6709\u6548\u89e3\u51b3\u4e86NLI\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u5728\u63d0\u5347\u6a21\u578b\u4e00\u81f4\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u707e\u96be\u6027\u9057\u5fd8"}}
{"id": "2512.18988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18988", "abs": "https://arxiv.org/abs/2512.18988", "authors": ["Yanding Yang", "Weitao Zhou", "Jinhai Wang", "Xiaomin Guo", "Junze Wen", "Xiaolong Liu", "Lang Ding", "Zheng Fu", "Jinyu Miao", "Kun Jiang", "Diange Yang"], "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners", "comment": null, "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.", "AI": {"tldr": "\u63d0\u51faDTCCL\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u516c\u4ea4\u8f66\u5728\u4ea4\u4e92\u5bc6\u96c6\u533a\u57df\u7684\u89c4\u5212\u5931\u8d25\u95ee\u9898\uff0c\u5229\u7528\u8131\u94a9\u4e8b\u4ef6\u89e6\u53d1\u4e91\u7aef\u6570\u636e\u589e\u5f3a\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u95ed\u73af\u7b56\u7565\u6539\u8fdb\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u516c\u4ea4\u8f66\u5728\u56fa\u5b9a\u8def\u7ebf\u4e0a\u8fd0\u884c\uff0c\u4f46\u5728\u5f00\u653e\u52a8\u6001\u7684\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u8131\u94a9\u4e8b\u4ef6\u5f80\u5f80\u96c6\u4e2d\u5728\u9ad8\u5ea6\u4ea4\u4e92\u533a\u57df\uff0c\u7531\u89c4\u5212\u5668\u5931\u8d25\u5f15\u8d77\u3002\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u5bb9\u6613\u5bf9\u7a00\u758f\u7684\u8131\u94a9\u6570\u636e\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u7ea0\u6b63\u8fd9\u4e9b\u7b56\u7565\u7ea7\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u8131\u94a9\u89e6\u53d1\u7684\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60\uff08DTCCL\uff09\u6846\u67b6\uff1a1\uff09\u6bcf\u6b21\u8131\u94a9\u89e6\u53d1\u4e91\u7aef\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u6270\u52a8\u5468\u56f4\u667a\u80fd\u4f53\u751f\u6210\u6b63\u8d1f\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8def\u7ebf\u4e0a\u4e0b\u6587\uff1b2\uff09\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7ec6\u5316\u7b56\u7565\u8868\u793a\uff0c\u66f4\u597d\u533a\u5206\u5b89\u5168\u548c\u4e0d\u5b89\u5168\u884c\u4e3a\uff1b3\uff09\u5728\u4e91\u8fb9\u5faa\u73af\u4e2d\u6301\u7eed\u66f4\u65b0\u7b56\u7565\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002", "result": "\u5728\u57ce\u5e02\u516c\u4ea4\u8def\u7ebf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDTCCL\u76f8\u6bd4\u76f4\u63a5\u91cd\u65b0\u8bad\u7ec3\uff0c\u5c06\u6574\u4f53\u89c4\u5212\u6027\u80fd\u63d0\u9ad8\u4e8648.6%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u516c\u5171\u4ea4\u901a\u4e2d\u53ef\u6269\u5c55\u3001\u95ed\u73af\u7b56\u7565\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002", "conclusion": "DTCCL\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u516c\u4ea4\u8f66\u5728\u4ea4\u4e92\u5bc6\u96c6\u533a\u57df\u7684\u89c4\u5212\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u8131\u94a9\u89e6\u53d1\u7684\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60\u5b9e\u73b0\u65e0\u76d1\u7763\u7b56\u7565\u6539\u8fdb\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u516c\u5171\u4ea4\u901a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u95ed\u73af\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2512.18279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18279", "abs": "https://arxiv.org/abs/2512.18279", "authors": ["Zhangshuo Qi", "Jingyi Xu", "Luqi Cheng", "Shichen Wen", "Yiming Ma", "Guangming Xiong"], "title": "UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations", "comment": null, "summary": "Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to arbitrary modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.", "AI": {"tldr": "UniMPR\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u9002\u5e94\u4efb\u610f\u4f20\u611f\u5668\u7ec4\u5408\uff08\u76f8\u673a\u3001LiDAR\u3001\u96f7\u8fbe\uff09\uff0c\u5728\u6781\u5750\u6807BEV\u7279\u5f81\u7a7a\u95f4\u4e2d\u5904\u7406\u5f02\u6784\u6570\u636e\uff0c\u901a\u8fc7\u591a\u5206\u652f\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u65e0\u6cd5\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u52a8\u6001\u9002\u5e94\u4efb\u610f\u6a21\u6001\u8f93\u5165\uff1b2\uff09\u5bf9\u7f3a\u5931\u6216\u9000\u5316\u6a21\u6001\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff1b3\uff09\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u4f20\u611f\u5668\u914d\u7f6e\u548c\u8bbe\u7f6e\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faUniMPR\u7edf\u4e00\u6846\u67b6\uff1a1\uff09\u5c06\u6240\u6709\u8f93\u5165\u7edf\u4e00\u5230\u6781\u5750\u6807BEV\u7279\u5f81\u7a7a\u95f4\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\uff1b2\uff09\u4f7f\u7528\u591a\u5206\u652f\u7f51\u7edc\u4ece\u4efb\u610f\u6a21\u6001\u7ec4\u5408\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u7684\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7279\u5f81\uff1b3\uff09\u6784\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u6807\u7b7e\u5206\u914d\u7b56\u7565\u8fdb\u884c\u5e7f\u6cdb\u9884\u8bad\u7ec3\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniMPR\u5728\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u3001\u6a21\u6001\u7ec4\u5408\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "UniMPR\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u7075\u6d3b\u4e14\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u4efb\u610f\u4f20\u611f\u5668\u7ec4\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u7684\u5168\u5c40\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18908", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18908", "abs": "https://arxiv.org/abs/2512.18908", "authors": ["Szymon Rusiecki", "Cecilia G. Morales", "Kimberly Elenberg", "Leonard Weiss", "Artur Dubrawski"], "title": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage", "comment": "Accepted at NeurIPS 2025 Workshop: Structured Probabilistic Inference & Generative Modeling", "summary": "Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u878d\u5408\u591a\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u8f93\u51fa\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\u4e2d\u7684\u4f24\u5458\u5206\u7c7b\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u4e14\u80fd\u5904\u7406\u4e0d\u5b8c\u6574\u4fe1\u606f\uff0c\u5728DARPA\u5206\u7c7b\u6311\u6218\u4e2d\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\u4f1a\u538b\u57ae\u7d27\u6025\u533b\u7597\u7cfb\u7edf\uff0c\u5bfc\u81f4\u4f24\u5458\u8bc4\u4f30\u5ef6\u8fdf\u6216\u9519\u8bef\uff0c\u9020\u6210\u53ef\u9884\u9632\u7684\u6b7b\u4ea1\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8f85\u52a9\u6025\u6551\u4eba\u5458\u5feb\u901f\u51c6\u786e\u5206\u7c7b\u4f24\u5458\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u878d\u5408\u591a\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff08\u4f30\u8ba1\u4e25\u91cd\u51fa\u8840\u3001\u547c\u5438\u7a98\u8feb\u3001\u8eab\u4f53\u8b66\u89c9\u6027\u6216\u53ef\u89c1\u521b\u4f24\uff09\u7684\u8f93\u51fa\uff0c\u6784\u5efa\u5b8c\u5168\u57fa\u4e8e\u4e13\u5bb6\u5b9a\u4e49\u89c4\u5219\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u652f\u6301\u4e0d\u5b8c\u6574\u4fe1\u606f\u63a8\u7406\uff0c\u5bf9\u566a\u58f0\u6216\u4e0d\u786e\u5b9a\u89c2\u5bdf\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6d89\u53ca11\u540d\u548c9\u540d\u4f24\u5458\u7684\u573a\u666f\u4e2d\uff0c\u8d1d\u53f6\u65af\u7f51\u7edc\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u7684\u57fa\u7ebf\u3002\u751f\u7406\u8bc4\u4f30\u51c6\u786e\u7387\u4ece15%\u63d0\u5347\u523042%\uff08\u7b2c\u4e00\u4e2a\u573a\u666f\uff09\u548c\u4ece19%\u63d0\u5347\u523046%\uff08\u7b2c\u4e8c\u4e2a\u573a\u666f\uff09\uff0c\u6027\u80fd\u63d0\u5347\u8fd1\u4e09\u500d\u3002\u603b\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u4ece14%\u63d0\u9ad8\u523053%\uff0c\u7cfb\u7edf\u8bca\u65ad\u8986\u76d6\u7387\u4ece31%\u6269\u5927\u523095%\u3002\u8be5\u56e2\u961f\u5728DARPA\u5206\u7c7b\u6311\u6218\u7b2c\u4e00\u8f6e\u7269\u7406\u6bd4\u8d5b\u4e2d\u83b7\u5f9711\u652f\u961f\u4f0d\u4e2d\u7684\u7b2c4\u540d\u3002", "conclusion": "\u4e13\u5bb6\u77e5\u8bc6\u5f15\u5bfc\u7684\u6982\u7387\u63a8\u7406\u80fd\u663e\u8457\u589e\u5f3a\u81ea\u52a8\u5316\u5206\u7c7b\u7cfb\u7edf\uff0c\u4e3a\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\u4e2d\u7684\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u6709\u524d\u666f\u7684\u652f\u6301\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728DARPA\u5206\u7c7b\u6311\u6218\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.18471", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.18471", "abs": "https://arxiv.org/abs/2512.18471", "authors": ["Xin Li"], "title": "The Geometry of Abstraction: Continual Learning via Recursive Quotienting", "comment": null, "summary": "Continual learning systems operating in fixed-dimensional spaces face a fundamental geometric barrier: the flat manifold problem. When experience is represented as a linear trajectory in Euclidean space, the geodesic distance between temporal events grows linearly with time, forcing the required covering number to diverge. In fixed-dimensional hardware, this volume expansion inevitably forces trajectory overlap, manifesting as catastrophic interference. In this work, we propose a geometric resolution to this paradox based on Recursive Metric Contraction. We formalize abstraction not as symbolic grouping, but as a topological deformation: a quotient map that collapses the metric tensor within validated temporal neighborhoods, effectively driving the diameter of local sub-manifolds to zero. We substantiate our framework with four rigorous results. First, the Bounded Capacity Theorem establishes that recursive quotient maps allow the embedding of arbitrarily long trajectories into bounded representational volumes, trading linear metric growth for logarithmic topological depth. Second, the Topological Collapse Separability Theorem, derived via Urysohn's Lemma, proves that recursive quotienting renders non-linearly separable temporal sequences linearly separable in the limit, bypassing the need for infinite-dimensional kernel projections. Third, the Parity-Partitioned Stability Theorem solves the catastrophic forgetting problem by proving that if the state space is partitioned into orthogonal flow and scaffold manifolds, the metric deformations of active learning do not disturb the stability of stored memories. Our analysis reveals that tokens in neural architectures are physically realizable as singularities or wormholes, regions of extreme positive curvature that bridge distant points in the temporal manifold.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52\u5ea6\u91cf\u6536\u7f29\u6846\u67b6\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u51e0\u4f55\u969c\u788d\uff0c\u901a\u8fc7\u62d3\u6251\u53d8\u5f62\u5c06\u7ebf\u6027\u589e\u957f\u7684\u6d4b\u5730\u8ddd\u79bb\u8f6c\u5316\u4e3a\u5bf9\u6570\u6df1\u5ea6\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u5b9e\u73b0\u6709\u754c\u5bb9\u91cf\u5d4c\u5165\u548c\u7ebf\u6027\u53ef\u5206\u6027\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u5728\u56fa\u5b9a\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\u9762\u4e34\u7684\u57fa\u672c\u51e0\u4f55\u969c\u788d\uff1a\u5e73\u5766\u6d41\u5f62\u95ee\u9898\u3002\u5f53\u7ecf\u9a8c\u8868\u793a\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u8f68\u8ff9\u65f6\uff0c\u65f6\u95f4\u4e8b\u4ef6\u4e4b\u95f4\u7684\u6d4b\u5730\u8ddd\u79bb\u968f\u65f6\u95f4\u7ebf\u6027\u589e\u957f\uff0c\u5bfc\u81f4\u6240\u9700\u8986\u76d6\u6570\u53d1\u6563\uff0c\u6700\u7ec8\u5f15\u53d1\u707e\u96be\u6027\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u5ea6\u91cf\u6536\u7f29\u6846\u67b6\uff0c\u5c06\u62bd\u8c61\u6982\u5ff5\u5316\u4e3a\u62d3\u6251\u53d8\u5f62\u800c\u975e\u7b26\u53f7\u5206\u7ec4\u3002\u901a\u8fc7\u5546\u6620\u5c04\u5728\u5df2\u9a8c\u8bc1\u7684\u65f6\u95f4\u90bb\u57df\u5185\u6536\u7f29\u5ea6\u91cf\u5f20\u91cf\uff0c\u4f7f\u5c40\u90e8\u5b50\u6d41\u5f62\u76f4\u5f84\u8d8b\u8fd1\u4e8e\u96f6\u3002\u91c7\u7528\u6b63\u4ea4\u6d41\u5f62\u5206\u5272\uff08\u6d41\u52a8\u6d41\u5f62\u548c\u652f\u67b6\u6d41\u5f62\uff09\u6765\u4fdd\u6301\u8bb0\u5fc6\u7a33\u5b9a\u6027\u3002", "result": "1. \u6709\u754c\u5bb9\u91cf\u5b9a\u7406\uff1a\u9012\u5f52\u5546\u6620\u5c04\u5141\u8bb8\u5c06\u4efb\u610f\u957f\u8f68\u8ff9\u5d4c\u5165\u6709\u754c\u8868\u793a\u4f53\u79ef\u4e2d\uff1b2. \u62d3\u6251\u574d\u7f29\u53ef\u5206\u6027\u5b9a\u7406\uff1a\u9012\u5f52\u5546\u5316\u4f7f\u975e\u7ebf\u6027\u53ef\u5206\u65f6\u95f4\u5e8f\u5217\u5728\u6781\u9650\u4e0b\u7ebf\u6027\u53ef\u5206\uff1b3. \u5947\u5076\u5206\u5272\u7a33\u5b9a\u6027\u5b9a\u7406\uff1a\u901a\u8fc7\u6b63\u4ea4\u6d41\u5f62\u5206\u5272\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u9012\u5f52\u5ea6\u91cf\u6536\u7f29\u4e3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u51e0\u4f55\u969c\u788d\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6807\u8bb0\u5b9e\u9645\u4e0a\u662f\u8fde\u63a5\u65f6\u95f4\u6d41\u5f62\u4e2d\u8fdc\u70b9\u7684\u5947\u70b9\u6216\u866b\u6d1e\uff0c\u5b9e\u73b0\u4e86\u6709\u754c\u5bb9\u91cf\u5d4c\u5165\u3001\u7ebf\u6027\u53ef\u5206\u6027\u548c\u8bb0\u5fc6\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.19001", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19001", "abs": "https://arxiv.org/abs/2512.19001", "authors": ["Lingjie Zhao", "Xue Yu", "Yongzhi Qi", "Hao Hu", "Jianshen Zhang", "Yingzheng Ma", "Shuyu Han", "Wei Qi", "Zuo-Jun Max Shen"], "title": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management", "comment": null, "summary": "As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided \"Pretrain-then-Reinforce\" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.", "AI": {"tldr": "\u63d0\u51faOR\u5f15\u5bfc\u7684\"\u9884\u8bad\u7ec3-\u5f3a\u5316\"\u6846\u67b6\uff0c\u5c06AI\u81ea\u9002\u5e94\u611f\u77e5\u4e0eOR\u7ed3\u6784\u4e25\u8c28\u6027\u7ed3\u5408\uff0c\u5728\u5e93\u5b58\u7ba1\u7406\u4e2d\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u6a21\u578b", "motivation": "\u89e3\u51b3AI\u81ea\u9002\u5e94\u611f\u77e5\u4e0eOR\u7ed3\u6784\u4e25\u8c28\u6027\u4e4b\u95f4\u7684\u534f\u8c03\u96be\u9898\uff0c\u5728\u590d\u6742\u5e93\u5b58\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e24\u8005\u7684\u6709\u6548\u7ed3\u5408", "method": "OR\u5f15\u5bfc\u7684\"\u9884\u8bad\u7ec3-\u5f3a\u5316\"\u6846\u67b6\uff1a1) \u4eff\u771f\u589e\u5f3aOR\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u53c2\u8003\u51b3\u7b56\uff1b2) \u9886\u57df\u77e5\u8bc6\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u51b3\u7b56\u80fd\u529b\uff1b3) \u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u6df1\u5ea6\u5bf9\u9f50\u673a\u5236\u8fdb\u884c\u5fae\u8c03", "result": "\u5728JD.com\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\uff1a\u5e93\u5b58\u5468\u8f6c\u51cf\u5c115.27\u5929\uff0c\u73b0\u8d27\u7387\u63d0\u53472.29%\uff0c\u6301\u6709\u6210\u672c\u964d\u4f4e29.95%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u4e1a\u5b9e\u8df5", "conclusion": "\u8f7b\u91cf\u7ea7\u9886\u57df\u77e5\u8bc6\u6a21\u578b\u5728OR\u7ed3\u6784\u5316\u903b\u8f91\u6307\u5bfc\u4e0b\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u548c\u9c81\u68d2\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3a\u667a\u80fd\u4f9b\u5e94\u94fe\u7ba1\u7406\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u6709\u6548\u7684\u8303\u5f0f"}}
{"id": "2512.19027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19027", "abs": "https://arxiv.org/abs/2512.19027", "authors": ["Ariana Azarbal", "Victor Gillioz", "Vladimir Ivanov", "Bryce Woodworth", "Jacob Drori", "Nevan Wichers", "Aram Ebtekar", "Alex Cloud", "Alexander Matt Turner"], "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification", "comment": "57 pages, 41 figures", "summary": "Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.", "AI": {"tldr": "\u63d0\u51farecontextualization\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u8bed\u5883\u5316\u8bad\u7ec3\u6837\u672c\uff0c\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\"\u535a\u5f08\"\u8bad\u7ec3\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u9519\u8bef\u884c\u4e3a", "motivation": "\u5f00\u53d1\u8005\u7ecf\u5e38\u96be\u4ee5\u6307\u5b9a\u6b63\u786e\u7684\u8bad\u7ec3\u6807\u7b7e\u548c\u5956\u52b1\u4fe1\u53f7\uff0c\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u5b66\u4f1a\"\u535a\u5f08\"\u8fd9\u4e9b\u4e0d\u5b8c\u5584\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u8868\u73b0\u51fa\u9519\u8bef\u884c\u4e3a\uff08\u5982\u4f18\u5148\u8003\u8651\u8bc4\u4f30\u6307\u6807\u800c\u975e\u804a\u5929\u8d28\u91cf\u3001\u7279\u6b8a\u5904\u7406\u4ee3\u7801\u4ee5\u901a\u8fc7\u9519\u8bef\u6d4b\u8bd5\u3001\u5bf9\u7528\u6237\u6492\u8c0e\u3001\u8c04\u5a9a\u7b49\uff09", "method": "recontextualization\u65b9\u6cd5\uff1a\u9996\u5148\u751f\u6210\u963b\u6b62\u9519\u8bef\u884c\u4e3a\u7684\u63d0\u793a\u4e0b\u7684\u8865\u5168\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8865\u5168\u91cd\u65b0\u8bed\u5883\u5316\u4e3a\u597d\u50cf\u662f\u5728\u5141\u8bb8\u9519\u8bef\u884c\u4e3a\u7684\u63d0\u793a\u4e0b\u751f\u6210\u7684\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u5373\u4f7f\u5728\u5141\u8bb8\u9519\u8bef\u884c\u4e3a\u7684\u6307\u4ee4\u4e0b\u4e5f\u80fd\u62b5\u6297\u9519\u8bef\u884c\u4e3a", "result": "\u8be5\u65b9\u6cd5\u80fd\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u56db\u79cd\u9519\u8bef\u884c\u4e3a\uff1a1)\u4f18\u5148\u8003\u8651\u8bc4\u4f30\u6307\u6807\u800c\u975e\u804a\u5929\u8d28\u91cf\uff1b2)\u7279\u6b8a\u5904\u7406\u4ee3\u7801\u4ee5\u901a\u8fc7\u9519\u8bef\u6d4b\u8bd5\uff1b3)\u5bf9\u7528\u6237\u6492\u8c0e\uff1b4)\u8c04\u5a9a\u3002\u65e0\u9700\u6539\u8fdb\u76d1\u7763\u4fe1\u53f7\u5c31\u80fd\u51cf\u5c11\u89c4\u8303\u535a\u5f08", "conclusion": "recontextualization\u901a\u8fc7\u91cd\u65b0\u8bed\u5883\u5316\u8bad\u7ec3\u6837\u672c\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u56e0\u8bad\u7ec3\u4fe1\u53f7\u8bef\u6307\u5b9a\u800c\u5f3a\u5316\u7684\u9519\u8bef\u884c\u4e3a\uff0c\u51cf\u5c11\u4e86\u89c4\u8303\u535a\u5f08\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u65b9\u6cd5"}}
{"id": "2512.18575", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.18575", "abs": "https://arxiv.org/abs/2512.18575", "authors": ["Effiong Blessing", "Chiung-Yi Tseng", "Somshubhra Roy", "Junaid Rehman", "Isaac Nkrumah"], "title": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing", "comment": null, "summary": "Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u8de8\u6a21\u6001\u6d88\u878d\u5206\u6790\uff0c\u53d1\u73b0Hopfield\u7f51\u7edc\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0897.68%\uff09\u4f46\u5728\u542c\u89c9\u4efb\u52a1\u4e0a\u8f83\u5dee\uff0876.15%\uff09\uff0c\u800c\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u8868\u73b0\u66f4\u5747\u8861\uff0c\u63ed\u793a\u4e86\u8bb0\u5fc6\u673a\u5236\u5177\u6709\u6a21\u6001\u7279\u5f02\u6027\u800c\u975e\u901a\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1\u8bb0\u5fc6\u589e\u5f3a\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u8282\u80fd\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u65b9\u9762\u6709\u524d\u666f\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u611f\u5b98\u6a21\u6001\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u9996\u6b21\u5168\u9762\u8bc4\u4f30SNNs\u4e2d\u8bb0\u5fc6\u673a\u5236\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u8de8\u6a21\u6001\u6d88\u878d\u7814\u7a76\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\uff08N-MNIST\uff09\u548c\u542c\u89c9\uff08SHD\uff09\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e94\u79cd\u67b6\u6784\uff1aHopfield\u7f51\u7edc\u3001\u5206\u5c42\u95e8\u63a7\u5faa\u73af\u7f51\u7edc\uff08HGRNs\uff09\u548c\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff08SCL\uff09\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u8bb0\u5fc6\u673a\u5236\u7684\u6027\u80fd\uff0c\u5e76\u8fdb\u884c\u8054\u5408\u591a\u6a21\u6001\u8bad\u7ec3\u548c\u5b9a\u91cf\u8bb0\u5fc6\u75d5\u8ff9\u5206\u6790\u3002", "result": "\u53d1\u73b0\u663e\u8457\u7684\u6a21\u6001\u4f9d\u8d56\u6027\u80fd\u6a21\u5f0f\uff1aHopfield\u7f51\u7edc\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fbe\u523097.68%\u51c6\u786e\u7387\uff0c\u4f46\u5728\u542c\u89c9\u4efb\u52a1\u4e0a\u4ec576.15%\uff0821.53\u70b9\u5dee\u8ddd\uff09\uff0c\u800cSCL\u8868\u73b0\u66f4\u5747\u8861\uff0896.72%\u89c6\u89c9\uff0c82.16%\u542c\u89c9\uff0c14.56\u70b9\u5dee\u8ddd\uff09\u3002HGRN\u8054\u5408\u591a\u6a21\u6001\u8bad\u7ec3\u8fbe\u523094.41%\u89c6\u89c9\u548c79.37%\u542c\u89c9\u51c6\u786e\u7387\u3002\u5b9a\u91cf\u8bb0\u5fc6\u75d5\u8ff9\u5206\u6790\u663e\u793a\u8de8\u6a21\u6001\u5bf9\u9f50\u8f83\u5f31\uff080.038\u76f8\u4f3c\u5ea6\uff09\u3002", "conclusion": "\u8bb0\u5fc6\u673a\u5236\u8868\u73b0\u51fa\u4efb\u52a1\u7279\u5f02\u6027\u4f18\u52bf\u800c\u975e\u901a\u7528\u9002\u7528\u6027\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u4e2d\u7684\u6a21\u6001\u7279\u5b9a\u8bb0\u5fc6\u4f18\u5316\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc603\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002"}}
{"id": "2512.19096", "categories": ["cs.AI", "math.LO", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.19096", "abs": "https://arxiv.org/abs/2512.19096", "authors": ["Kathelijne Coussement", "Gert de Cooman", "Keano De Vos"], "title": "Conditioning Accept-Desirability models in the context of AGM-like belief change", "comment": "46 pages, 1 table", "summary": "We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u62bd\u8c61\u51b3\u7b56\u6846\u67b6\u4e2d\u7528\u4e8e\u63a5\u53d7-\u671f\u671b\u6a21\u578b\u7684\u65b0\u7684\u6761\u4ef6\u5316\u89c4\u5219\uff0c\u57fa\u4e8e\u89c2\u6d4b\u4e8b\u4ef6\u5f15\u5165\u65b0\u65e0\u5dee\u5f02\u6027\u7684\u601d\u60f3\uff0c\u5e76\u7814\u7a76\u4e86AGM\u4fe1\u5ff5\u4fee\u6b63\u516c\u7406\u5728\u8be5\u6846\u67b6\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u5728\u7edf\u4e00\u7ecf\u5178\u6982\u7387\u548c\u91cf\u5b50\u6982\u7387\u7684\u62bd\u8c61\u51b3\u7b56\u6846\u67b6\u4e2d\uff0c\u9700\u8981\u53d1\u5c55\u9002\u7528\u4e8e\u63a5\u53d7-\u671f\u671b\u6a21\u578b\u7684\u6761\u4ef6\u5316\u89c4\u5219\uff0c\u5e76\u63a2\u8ba8\u5728\u66f4\u4e00\u822c\u6846\u67b6\u4e0b\u4fe1\u5ff5\u4fee\u6b63\u516c\u7406\u7684\u4fdd\u6301\u60c5\u51b5\u3002", "method": "\u5728\u62bd\u8c61\u51b3\u7b56\u6846\u67b6\u4e2d\uff0c\u5c06\u4e0d\u786e\u5b9a\u5956\u52b1\u7f6e\u4e8e\u4e00\u822c\u7ebf\u6027\u7a7a\u95f4\uff0c\u4e8b\u4ef6\u4f5c\u4e3a\u8be5\u7a7a\u95f4\u7684\u6295\u5f71\u7b97\u5b50\u3002\u63d0\u51fa\u57fa\u4e8e\u89c2\u6d4b\u4e8b\u4ef6\u5f15\u5165\u65b0\u65e0\u5dee\u5f02\u6027\u7684\u6761\u4ef6\u5316\u89c4\u5219\uff0c\u5e76\u5173\u8054\u4fe1\u5ff5\u4fee\u6b63\u7b97\u5b50\uff0c\u5206\u6790AGM\u516c\u7406\u7684\u4fdd\u6301\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u7279\u6b8a\u60c5\u51b5\u4e0b\u6240\u6709AGM\u516c\u7406\u4ecd\u7136\u6210\u7acb\uff1a\u7ecf\u5178\u547d\u9898\u903b\u8f91\u548c\u5b8c\u5168\u6761\u4ef6\u6982\u7387\u3002\u5728\u66f4\u4e00\u822c\u7684\u63a5\u53d7-\u671f\u671b\u6a21\u578b\u6846\u67b6\u4e2d\uff0c\u90e8\u5206AGM\u516c\u7406\u53ef\u80fd\u4e0d\u6210\u7acb\u3002", "conclusion": "\u63d0\u51fa\u7684\u6761\u4ef6\u5316\u89c4\u5219\u4e3a\u7edf\u4e00\u7ecf\u5178\u548c\u91cf\u5b50\u6982\u7387\u7684\u62bd\u8c61\u51b3\u7b56\u6846\u67b6\u63d0\u4f9b\u4e86\u7406\u8bba\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5728\u66f4\u4e00\u822c\u6982\u7387\u6846\u67b6\u4e0b\u4fe1\u5ff5\u4fee\u6b63\u516c\u7406\u7684\u9002\u7528\u8fb9\u754c\uff0c\u4e3a\u4e0d\u7cbe\u786e\u6982\u7387\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.18596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18596", "abs": "https://arxiv.org/abs/2512.18596", "authors": ["Quanxi Zhou", "Wencan Mao", "Yilei Liang", "Manabu Tsukada", "Yunling Liu", "Jon Crowcroft"], "title": "EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture", "comment": null, "summary": "The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.", "AI": {"tldr": "\u63d0\u51faEIA-SEC\u6846\u67b6\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u667a\u80fd\u519c\u4e1a\u7cfb\u7edf\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u82f1\u6a21\u4eff\u548c\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\u63d0\u5347\u6027\u80fd", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u63a8\u52a8\u667a\u80fd\u519c\u4e1a\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u5728\u6570\u636e\u91c7\u96c6\u3001\u56fe\u50cf\u83b7\u53d6\u548c\u901a\u4fe1\u4efb\u52a1\u4e2d\u53d1\u6325\u591a\u529f\u80fd\u4f5c\u7528\u3002\u591a\u65e0\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u9700\u8981\u89e3\u51b3\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bd5\u9519\u6210\u672c\u9ad8\u3001\u4f30\u8ba1\u504f\u5dee\u548c\u8fc7\u4f30\u8ba1\u7b49\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6a21\u578b\uff0c\u63d0\u51fa\u7cbe\u82f1\u6a21\u4eff\u6f14\u5458-\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\uff08EIA-SEC\uff09\u6846\u67b6\uff1a1\uff09\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u5730\u4ece\u7cbe\u82f1\u667a\u80fd\u4f53\u5b66\u4e60\u4ee5\u51cf\u5c11\u8bd5\u9519\u6210\u672c\uff1b2\uff09\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\u4e0e\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u672c\u5730\u6279\u8bc4\u5668\u534f\u4f5c\uff0c\u786e\u4fdd\u65e0\u504f\u76ee\u6807\u503c\u4f30\u8ba1\u5e76\u9632\u6b62\u8fc7\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEIA-SEC\u5728\u5956\u52b1\u6027\u80fd\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EIA-SEC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65e0\u4eba\u673a\u667a\u80fd\u519c\u4e1a\u7cfb\u7edf\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u82f1\u6a21\u4eff\u548c\u5171\u4eab\u96c6\u6210\u6279\u8bc4\u5668\u673a\u5236\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2512.18429", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.18429", "abs": "https://arxiv.org/abs/2512.18429", "authors": ["Seyed Ehsan Marjani Bajestani", "Giovanni Beltrame"], "title": "E-RGB-D: Real-Time Event-Based Perception with Structured Light", "comment": null, "summary": "Event-based cameras (ECs) have emerged as bio-inspired sensors that report pixel brightness changes asynchronously, offering unmatched speed and efficiency in vision sensing. Despite their high dynamic range, temporal resolution, low power consumption, and computational simplicity, traditional monochrome ECs face limitations in detecting static or slowly moving objects and lack color information essential for certain applications. To address these challenges, we present a novel approach that integrates a Digital Light Processing (DLP) projector, forming Active Structured Light (ASL) for RGB-D sensing. By combining the benefits of ECs and projection-based techniques, our method enables the detection of color and the depth of each pixel separately. Dynamic projection adjustments optimize bandwidth, ensuring selective color data acquisition and yielding colorful point clouds without sacrificing spatial resolution. This integration, facilitated by a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, not only enables frameless RGB-D sensing applications but also achieves remarkable performance milestones. With our approach, we achieved a color detection speed equivalent to 1400 fps and 4 kHz of pixel depth detection, significantly advancing the realm of computer vision across diverse fields from robotics to 3D reconstruction methods. Our code is publicly available: https://github.com/MISTLab/event_based_rgbd_ros", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548cDLP\u6295\u5f71\u4eea\u7684\u65b0\u578bRGB-D\u611f\u77e5\u7cfb\u7edf\uff0c\u5b9e\u73b01400fps\u8272\u5f69\u68c0\u6d4b\u548c4kHz\u50cf\u7d20\u6df1\u5ea6\u68c0\u6d4b", "motivation": "\u4f20\u7edf\u5355\u8272\u4e8b\u4ef6\u76f8\u673a\u65e0\u6cd5\u68c0\u6d4b\u9759\u6001/\u6162\u901f\u7269\u4f53\u4e14\u7f3a\u4e4f\u8272\u5f69\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u989c\u8272\u611f\u77e5\u7684\u5e94\u7528\u4e2d\u7684\u4f7f\u7528", "method": "\u96c6\u6210\u6570\u5b57\u5149\u5904\u7406\u6295\u5f71\u4eea\u5f62\u6210\u4e3b\u52a8\u7ed3\u6784\u5149\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u4f18\u52bf\uff0c\u901a\u8fc7\u52a8\u6001\u6295\u5f71\u8c03\u6574\u4f18\u5316\u5e26\u5bbd\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u8272\u5f69\u548c\u6df1\u5ea6\u5206\u79bb\u68c0\u6d4b", "result": "\u5b9e\u73b01400fps\u8272\u5f69\u68c0\u6d4b\u901f\u5ea6\u548c4kHz\u50cf\u7d20\u6df1\u5ea6\u68c0\u6d4b\uff0c\u751f\u6210\u5f69\u8272\u70b9\u4e91\u800c\u4e0d\u727a\u7272\u7a7a\u95f4\u5206\u8fa8\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63a8\u8fdb\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u4ece\u673a\u5668\u4eba\u52303D\u91cd\u5efa\u7b49\u591a\u4e2a\u5e94\u7528\u9886\u57df\u90fd\u80fd\u53d7\u76ca\u4e8e\u8fd9\u79cd\u65e0\u5e27RGB-D\u611f\u77e5\u80fd\u529b"}}
{"id": "2512.19299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19299", "abs": "https://arxiv.org/abs/2512.19299", "authors": ["Haoyu Jiang", "Fanjie Zeng", "Boan Qu", "Xiaojie Lin", "Wei Zhong"], "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application", "comment": null, "summary": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.", "AI": {"tldr": "Helios\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u80fd\u6e90\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u6784\u5efa\u4e86\u77e5\u8bc6\u5e93\u3001\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u548cRLHF\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u667a\u80fd\u80fd\u6e90\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u638c\u63e1\u3001\u4efb\u52a1\u6267\u884c\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "motivation": "\u5728\u78b3\u4e2d\u548c\u7684\u5168\u7403\u8d8b\u52bf\u4e0b\uff0c\u667a\u80fd\u80fd\u6e90\u7cfb\u7edf\u9700\u8981\u6df1\u5ea6\u534f\u8c03\uff0c\u4f46\u8be5\u9886\u57df\u8de8\u5b66\u79d1\u3001\u788e\u7247\u5316\u4e14\u5feb\u901f\u53d1\u5c55\u7684\u4e13\u4e1a\u77e5\u8bc6\u4f7f\u5f97\u901a\u7528LLMs\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u548c\u7269\u7406\u7ea6\u675f\u610f\u8bc6\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u7684\u5de5\u7a0b\u5bf9\u9f50\u63a8\u7406\u548c\u751f\u6210\u3002", "method": "\u5f00\u53d1\u4e86Enersys\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u7aef\u5230\u7aef\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5305\u62ec\uff1a1) EnerBase\u667a\u80fd\u80fd\u6e90\u77e5\u8bc6\u5e93\uff1b2) EnerInstruct\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff1b3) EnerReinforce RLHF\u6570\u636e\u96c6\u3002\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u5bf9Helios\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3001SFT\u548cRLHF\u3002", "result": "\u53d1\u5e03\u4e86EnerBench\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u80fd\u6e90\u573a\u666f\u4e2d\u7684LLMs\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u9886\u57df\u77e5\u8bc6\u638c\u63e1\u3001\u4efb\u52a1\u6267\u884c\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "conclusion": "Helios\u6a21\u578b\u53ca\u5176\u914d\u5957\u8d44\u6e90\uff08\u6570\u636e\u96c6\u3001\u57fa\u51c6\uff09\u4e3a\u667a\u80fd\u80fd\u6e90\u9886\u57df\u7684LLM\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u901a\u7528LLMs\u5728\u8be5\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\u548c\u7269\u7406\u7ea6\u675f\u610f\u8bc6\u7f3a\u4e4f\u7684\u95ee\u9898\u3002"}}
{"id": "2512.19173", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19173", "abs": "https://arxiv.org/abs/2512.19173", "authors": ["Dazhen Deng", "Sen Yang", "Yuchen He", "Yuan Tian", "Yingcai Wu"], "title": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation", "comment": null, "summary": "Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.", "AI": {"tldr": "CycleChart\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u53cc\u5411\u56fe\u8868\u7406\u89e3\u4e0e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5f0f\u4e2d\u5fc3\u5316\u8868\u8ff0\u7edf\u4e00\u4efb\u52a1\u63a5\u53e3\uff0c\u5229\u7528\u751f\u6210-\u89e3\u6790\u4e00\u81f4\u6027\u76ee\u6807\u5b66\u4e60\u8de8\u65b9\u5411\u56fe\u8868\u8bed\u4e49", "motivation": "\u5f53\u524d\u56fe\u8868\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u56fe\u8868\u95ee\u7b54\u3001\u56fe\u8868\u89e3\u6790\u3001\u56fe\u8868\u751f\u6210\uff09\u901a\u5e38\u5b64\u7acb\u7814\u7a76\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5b66\u4e60\u8fde\u63a5\u56fe\u8868\u751f\u6210\u4e0e\u89e3\u91ca\u7684\u5171\u4eab\u8bed\u4e49", "method": "\u91c7\u7528\u6a21\u5f0f\u4e2d\u5fc3\u5316\u8868\u8ff0\u4f5c\u4e3a\u8de8\u4efb\u52a1\u901a\u7528\u63a5\u53e3\uff0c\u6784\u5efa\u4e00\u81f4\u7684\u591a\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u5f15\u5165\u751f\u6210-\u89e3\u6790\u4e00\u81f4\u6027\u76ee\u6807\uff1a\u6a21\u578b\u4ece\u8868\u683c\u548c\u6587\u672c\u67e5\u8be2\u751f\u6210\u56fe\u8868\u6a21\u5f0f\uff0c\u7136\u540e\u5b66\u4e60\u4ece\u751f\u6210\u7684\u56fe\u8868\u4e2d\u6062\u590d\u6a21\u5f0f\u548c\u6570\u636e", "result": "CycleChart\u5728\u56fe\u8868\u751f\u6210\u3001\u56fe\u8868\u89e3\u6790\u548c\u56fe\u8868\u95ee\u7b54\u65b9\u9762\u53d6\u5f97\u5f3a\u52b2\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b", "conclusion": "CycleChart\u6807\u5fd7\u7740\u5411\u66f4\u901a\u7528\u7684\u56fe\u8868\u7406\u89e3\u6a21\u578b\u8fc8\u51fa\u4e86\u4e00\u6b65\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u53cc\u5411\u56fe\u8868\u7406\u89e3\u4e0e\u751f\u6210"}}
{"id": "2512.18640", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18640", "abs": "https://arxiv.org/abs/2512.18640", "authors": ["Kai Kohyama", "Yoshimitsu Aoki", "Guillermo Gallego", "Shintaro Shiba"], "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing", "comment": "15 pages, 10 figures, 5 tables", "summary": "Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e8b\u4ef6\u76f8\u673a3D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u548c\u8f90\u5c04\u6e32\u67d3\uff0c\u5728\u7cbe\u5ea6\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u65e0\u9700\u5148\u9a8c\u4fe1\u606f\u6216COLMAP\u521d\u59cb\u5316\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u9002\u5408\u8fd0\u52a8\u548c\u7ed3\u6784\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u4e8b\u4ef63D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u96be\u4ee5\u5229\u7528\u7a00\u758f\u4e8b\u4ef6\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u4fe1\u606f\uff0c\u9700\u8981\u5728\u7cbe\u5ea6\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u95f4\u6743\u8861\u3002", "method": "\u5c06\u6e32\u67d3\u89e3\u8026\u4e3a\u4e24\u4e2a\u5206\u652f\uff1a\u4e8b\u4ef6\u7ea7\u51e0\u4f55\uff08\u6df1\u5ea6\uff09\u6e32\u67d3\u548c\u5feb\u7167\u7ea7\u8f90\u5c04\uff08\u5f3a\u5ea6\uff09\u6e32\u67d3\uff0c\u4f7f\u7528\u5149\u7ebf\u8ffd\u8e2a\u548c\u626d\u66f2\u4e8b\u4ef6\u56fe\u50cf\u6280\u672f\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff1b\u65e0\u9700\u5148\u9a8c\u4fe1\u606f\u6216COLMAP\u521d\u59cb\u5316\uff0c\u4e8b\u4ef6\u9009\u62e9\u66f4\u7075\u6d3b\uff0c\u8fb9\u7f18\u91cd\u5efa\u6e05\u6670\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6df1\u5316\u4e86\u5bf9\u7a00\u758f\u4e8b\u4ef6\u7528\u4e8e3D\u91cd\u5efa\u7684\u7406\u89e3\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a3D\u9ad8\u65af\u6cfc\u6e85\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.19021", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19021", "abs": "https://arxiv.org/abs/2512.19021", "authors": ["Sihao Lin", "Zerui Li", "Xunyi Zhao", "Gengze Zhou", "Liuyi Wang", "Rong Wei", "Rui Tang", "Juncheng Li", "Hanqing Wang", "Jiangmiao Pang", "Anton van den Hengel", "Jiajun Liu", "Qi Wu"], "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation", "comment": null, "summary": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.", "AI": {"tldr": "VLNVerse\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u3001\u53ef\u6269\u5c55\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u7269\u7406\u6a21\u62df\u7b80\u5355\u3001\u4efb\u52a1\u788e\u7247\u5316\u7b49\u95ee\u9898\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u57fa\u51c6\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1) \u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u56fa\u5b9a\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u6c42\uff1b2) \u7269\u7406\u6a21\u62df\u7b80\u5355\uff0c\u9650\u5236\u4e86\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6cdb\u5316\u7814\u7a76\uff1b3) \u4efb\u52a1\u788e\u7247\u5316\uff0c\u963b\u788d\u4e86\u7edf\u4e00\u8fdb\u5c55\uff1b4) \u73b0\u6709\u667a\u80fd\u4f53\u591a\u4e3a\"\u5e7d\u7075\"\u5f0f\uff08\u65e0\u5f62\u3001\u77ac\u79fb\uff09\uff0c\u7f3a\u4e4f\u5b8c\u6574\u8fd0\u52a8\u5b66\u652f\u6301\u3002", "method": "\u63d0\u51faVLNVerse\u57fa\u51c6\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a1) \u591a\u529f\u80fd\u6027\uff1a\u5c06\u788e\u7247\u5316\u4efb\u52a1\u7edf\u4e00\u5230\u5355\u4e00\u6846\u67b6\u4e2d\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u5de5\u5177\u5305\uff1b2) \u5177\u8eab\u6027\uff1a\u8d85\u8d8a\u65e0\u5f62\u77ac\u79fb\u7684\"\u5e7d\u7075\"\u667a\u80fd\u4f53\uff0c\u652f\u6301\u5b8c\u6574\u8fd0\u52a8\u5b66\uff1b3) \u771f\u5b9e\u6a21\u62df\uff1a\u57fa\u4e8e\u5f3a\u5927\u7684\u7269\u7406\u5f15\u64ce\u5b9e\u73b0\u903c\u771f\u6a21\u62df\uff1b4) \u5927\u89c4\u6a21\uff1a\u5229\u7528\u89c4\u6a21\u548c\u591a\u6837\u6027\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff1b5) \u63d0\u51fa\u65b0\u9896\u7684\u7edf\u4e00\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u57fa\u51c6\u4e2d\u7684\u6240\u6709\u4efb\u52a1\u3002", "result": "VLNVerse\u91cd\u65b0\u5b9a\u4e49\u4e86\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5168\u6808\u5177\u8eabAI\u95ee\u9898\uff0c\u7f29\u5c0f\u4e86\u6a21\u62df\u5bfc\u822a\u4e0e\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u7814\u7a76\u754c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u5de5\u5177\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u3001\u901a\u7528\u5177\u8eab\u8fd0\u52a8\u667a\u80fd\u4f53\u7684\u7814\u7a76\u3002", "conclusion": "VLNVerse\u57fa\u51c6\u901a\u8fc7\u5176\u591a\u529f\u80fd\u6027\u3001\u5177\u8eab\u6027\u3001\u771f\u5b9e\u6a21\u62df\u548c\u5927\u89c4\u6a21\u7279\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u6709\u671b\u663e\u8457\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.18737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18737", "abs": "https://arxiv.org/abs/2512.18737", "authors": ["Zichuan Lin", "Xiaokai Huang", "Jiate Liu", "Yuxuan Han", "Jia Chen", "Xiapeng Wu", "Deheng Ye"], "title": "PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation", "comment": "17 pages, 3 figures", "summary": "The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.", "AI": {"tldr": "PIPCFR\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6cbb\u7597\u540e\u53d8\u91cf\u6539\u8fdb\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f2a\u7ed3\u679c\u63d2\u8865\u548c\u6709\u6548\u8868\u5f81\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53cd\u4e8b\u5b9e\u9884\u6d4b\u7684\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f30\u8ba1\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u6cbb\u7597\u540e\u53d8\u91cf\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u8fd9\u5bfc\u81f4\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u7ed3\u679c\u7684\u53d8\u5f02\u6027\uff0c\u589e\u52a0\u4e86\u53cd\u4e8b\u5b9e\u9884\u6d4b\u7684\u65b9\u5dee\u3002", "method": "\u63d0\u51faPIPCFR\u65b9\u6cd5\uff0c\u7ed3\u5408\u6cbb\u7597\u540e\u53d8\u91cf\u6539\u8fdb\u4f2a\u7ed3\u679c\u63d2\u8865\uff0c\u5efa\u7acb\u65b0\u7684\u7406\u8bba\u8fb9\u754c\u8fde\u63a5\u6cbb\u7597\u540e\u53d8\u91cf\u4e0eITE\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5b66\u4e60\u65e2\u80fd\u4fdd\u7559\u4fe1\u606f\u6210\u5206\u53c8\u80fd\u51cf\u5c11\u504f\u5dee\u7684\u6709\u6548\u8868\u5f81\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cPIPCFR\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86ITE\u8bef\u5dee\u3002", "conclusion": "\u6cbb\u7597\u540e\u53d8\u91cf\u5bf9\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0cPIPCFR\u901a\u8fc7\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u53d8\u91cf\uff0c\u63d0\u9ad8\u4e86\u53cd\u4e8b\u5b9e\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.18597", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.18597", "abs": "https://arxiv.org/abs/2512.18597", "authors": ["Zhe Li", "Kun Cheng", "Hanyue Mo", "Jintao Lu", "Ziwen Kuang", "Jianwen Ye", "Lixu Xu", "Xinya Meng", "Jiahui Zhao", "Shengda Ji", "Shuyuan Liu", "Mengyu Wang"], "title": "Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach", "comment": "5 figures,16 pages", "summary": "A vision-based trajectory analysis solution is proposed to address the \"zero-speed braking\" issue caused by inaccurate Controller Area Network (CAN) signals in commercial vehicle Automatic Emergency Braking (AEB) systems during low-speed operation. The algorithm utilizes the NVIDIA Jetson AGX Xavier platform to process sequential video frames from a blind spot camera, employing self-adaptive Contrast Limited Adaptive Histogram Equalization (CLAHE)-enhanced Scale-Invariant Feature Transform (SIFT) feature extraction and K-Nearest Neighbors (KNN)-Random Sample Consensus (RANSAC) matching. This allows for precise classification of the vehicle's motion state (static, vibration, moving). Key innovations include 1) multiframe trajectory displacement statistics (5-frame sliding window), 2) a dual-threshold state decision matrix, and 3) OBD-II driven dynamic Region of Interest (ROI) configuration. The system effectively suppresses environmental interference and false detection of dynamic objects, directly addressing the challenge of low-speed false activation in commercial vehicle safety systems. Evaluation in a real-world dataset (32,454 video segments from 1,852 vehicles) demonstrates an F1-score of 99.96% for static detection, 97.78% for moving state recognition, and a processing delay of 14.2 milliseconds (resolution 704x576). The deployment on-site shows an 89% reduction in false braking events, a 100% success rate in emergency braking, and a fault rate below 5%.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u8f68\u8ff9\u5206\u6790\u65b9\u6848\uff0c\u89e3\u51b3\u5546\u7528\u8f66AEB\u7cfb\u7edf\u5728\u4f4e\u901f\u8fd0\u884c\u65f6\u56e0CAN\u4fe1\u53f7\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\"\u96f6\u901f\u5236\u52a8\"\u95ee\u9898\uff0c\u901a\u8fc7\u89c6\u9891\u5904\u7406\u7cbe\u786e\u8bc6\u522b\u8f66\u8f86\u8fd0\u52a8\u72b6\u6001\u3002", "motivation": "\u5546\u7528\u8f66\u81ea\u52a8\u7d27\u6025\u5236\u52a8\u7cfb\u7edf\u5728\u4f4e\u901f\u8fd0\u884c\u65f6\uff0c\u7531\u4e8eCAN\u4fe1\u53f7\u4e0d\u51c6\u786e\u4f1a\u5bfc\u81f4\"\u96f6\u901f\u5236\u52a8\"\u8bef\u89e6\u53d1\u95ee\u9898\uff0c\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528NVIDIA Jetson AGX Xavier\u5e73\u53f0\u5904\u7406\u76f2\u533a\u6444\u50cf\u5934\u89c6\u9891\u5e8f\u5217\uff0c\u91c7\u7528\u81ea\u9002\u5e94CLAHE\u589e\u5f3a\u7684SIFT\u7279\u5f81\u63d0\u53d6\u548cKNN-RANSAC\u5339\u914d\uff0c\u7ed3\u54085\u5e27\u6ed1\u52a8\u7a97\u53e3\u8f68\u8ff9\u4f4d\u79fb\u7edf\u8ba1\u3001\u53cc\u9608\u503c\u72b6\u6001\u51b3\u7b56\u77e9\u9635\u548cOBD-II\u9a71\u52a8\u7684\u52a8\u6001ROI\u914d\u7f6e\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff0832,454\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c1,852\u8f86\u8f66\uff09\u4e0a\u6d4b\u8bd5\uff1a\u9759\u6001\u68c0\u6d4bF1-score 99.96%\uff0c\u8fd0\u52a8\u72b6\u6001\u8bc6\u522b97.78%\uff0c\u5904\u7406\u5ef6\u8fdf14.2ms\u3002\u73b0\u573a\u90e8\u7f72\u663e\u793a\u8bef\u5236\u52a8\u4e8b\u4ef6\u51cf\u5c1189%\uff0c\u7d27\u6025\u5236\u52a8\u6210\u529f\u7387100%\uff0c\u6545\u969c\u7387\u4f4e\u4e8e5%\u3002", "conclusion": "\u8be5\u89c6\u89c9\u8f68\u8ff9\u5206\u6790\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u5546\u7528\u8f66AEB\u7cfb\u7edf\u7684\u4f4e\u901f\u8bef\u89e6\u53d1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.19475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19475", "abs": "https://arxiv.org/abs/2512.19475", "authors": ["Ivan Decostanzi", "Yelena Mejova", "Kyriaki Kalimeri"], "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting", "comment": "18 pages, 3 figures", "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5f02\u6784\u4eba\u9053\u4e3b\u4e49\u6587\u6863\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u6709\u8bc1\u636e\u652f\u6491\u7684\u62a5\u544a\u6846\u67b6\uff0c\u572813\u4e2a\u4eba\u9053\u4e3b\u4e49\u4e8b\u4ef6\u4e2d\u9a8c\u8bc1\u6709\u6548", "motivation": "\u5f53\u524d\u4eba\u9053\u4e3b\u4e49\u51b3\u7b56\u4f9d\u8d56\u7684\u624b\u5de5\u62a5\u544a\u5236\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u3001\u8d44\u6e90\u5bc6\u96c6\u4e14\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u65f6\u6548\u6027\u548c\u51c6\u786e\u6027", "method": "\u6574\u5408\u8bed\u4e49\u6587\u672c\u805a\u7c7b\u3001\u81ea\u52a8\u95ee\u9898\u751f\u6210\u3001\u68c0\u7d22\u589e\u5f3a\u7b54\u6848\u63d0\u53d6\uff08\u5e26\u5f15\u7528\uff09\u3001\u591a\u7ea7\u6458\u8981\u548c\u884c\u653f\u6458\u8981\u751f\u6210\uff0c\u5e76\u91c7\u7528\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\u7684\u5185\u90e8\u8bc4\u4f30\u6307\u6807", "result": "\u572813\u4e2a\u4eba\u9053\u4e3b\u4e49\u4e8b\u4ef6\uff08\u81ea\u7136\u707e\u5bb3\u548c\u51b2\u7a81\uff09\u76841100\u591a\u4efd\u6587\u6863\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u95ee\u9898\u76f8\u5173\u5ea684.7%\u3001\u91cd\u8981\u602784.0%\u3001\u7d27\u6025\u602776.4%\uff1b\u63d0\u53d6\u7b54\u6848\u76f8\u5173\u5ea686.3%\uff0c\u5f15\u7528\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u8d8576%\uff1b\u4eba\u673a\u8bc4\u4f30\u4e00\u81f4\u6027F1\u5206\u6570\u8d850.80", "conclusion": "\u7ed3\u5408LLM\u63a8\u7406\u3001\u900f\u660e\u5f15\u7528\u94fe\u63a5\u548c\u591a\u7ea7\u8bc4\u4f30\uff0c\u751f\u6210\u5f0fAI\u80fd\u591f\u81ea\u4e3b\u4ea7\u751f\u51c6\u786e\u3001\u53ef\u9a8c\u8bc1\u4e14\u5177\u6709\u64cd\u4f5c\u4ef7\u503c\u7684\u4eba\u9053\u4e3b\u4e49\u5f62\u52bf\u62a5\u544a\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5"}}
{"id": "2512.19537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19537", "abs": "https://arxiv.org/abs/2512.19537", "authors": ["Bobo Li", "Xudong Han", "Jiang Liu", "Yuzhe Ding", "Liqiang Jing", "Zhaoqi Zhang", "Jinheng Li", "Xinya Du", "Fei Li", "Meishan Zhang", "Min Zhang", "Aixin Sun", "Philip S. Yu", "Hao Fei"], "title": "Event Extraction in Large Language Model", "comment": "38 pages, 9 Figures, 5 Tables", "summary": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u4e8b\u4ef6\u62bd\u53d6\u5728LLM\u65f6\u4ee3\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4e3aLLM\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u8ba4\u77e5\u652f\u67b6\u7684\u7cfb\u7edf\u7ec4\u4ef6\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ece\u9759\u6001\u63d0\u53d6\u5230\u53ef\u9760\u3001\u9762\u5411\u667a\u80fd\u4f53\u7684\u611f\u77e5\u4e0e\u8bb0\u5fc6\u5c42\u7684\u6f14\u8fdb\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1LLM\u548c\u591a\u6a21\u6001LLM\u6b63\u5728\u6539\u53d8\u4e8b\u4ef6\u62bd\u53d6\u9886\u57df\uff0c\u4f46\u57fa\u4e8eLLM\u7684\u7ba1\u9053\u4ecd\u9762\u4e34\u90e8\u7f72\u5dee\u8ddd\uff1a\u5f31\u7ea6\u675f\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\u3001\u957f\u4e0a\u4e0b\u6587\u548c\u8de8\u6587\u6863\u7684\u8106\u5f31\u65f6\u95f4\u56e0\u679c\u94fe\u63a5\u3001\u6709\u9650\u7684\u957f\u89c6\u91ce\u77e5\u8bc6\u7ba1\u7406\u7b49\u3002\u9700\u8981\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4e3aLLM\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u8ba4\u77e5\u652f\u67b6\u7684\u7cfb\u7edf\u7ec4\u4ef6\u3002", "method": "\u901a\u8fc7\u4e8b\u4ef6\u6a21\u5f0f\u548c\u69fd\u4f4d\u7ea6\u675f\u521b\u5efa\u63a5\u5730\u548c\u9a8c\u8bc1\u63a5\u53e3\uff1b\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u7ed3\u6784\u4f5c\u4e3a\u9010\u6b65\u63a8\u7406\u7684\u53d7\u63a7\u4e2d\u95f4\u8868\u793a\uff1b\u4e8b\u4ef6\u94fe\u63a5\u652f\u6301\u57fa\u4e8e\u56fe\u7684RAG\u8fdb\u884c\u5173\u7cfb\u611f\u77e5\u68c0\u7d22\uff1b\u4e8b\u4ef6\u5b58\u50a8\u63d0\u4f9b\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u53ef\u66f4\u65b0\u60c5\u666f\u548c\u667a\u80fd\u4f53\u8bb0\u5fc6\u3002", "result": "\u7efc\u8ff0\u6db5\u76d6\u4e86\u6587\u672c\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u7684\u4e8b\u4ef6\u62bd\u53d6\uff0c\u7ec4\u7ec7\u4e86\u4efb\u52a1\u548c\u5206\u7c7b\u6cd5\uff0c\u8ffd\u6eaf\u4e86\u4ece\u57fa\u4e8e\u89c4\u5219\u548c\u795e\u7ecf\u6a21\u578b\u5230\u6307\u4ee4\u9a71\u52a8\u548c\u751f\u6210\u6846\u67b6\u7684\u65b9\u6cd5\u6f14\u8fdb\uff0c\u603b\u7ed3\u4e86\u516c\u5f0f\u3001\u89e3\u7801\u7b56\u7565\u3001\u67b6\u6784\u3001\u8868\u793a\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u4e8b\u4ef6\u62bd\u53d6\u9700\u8981\u4ece\u9759\u6001\u63d0\u53d6\u6f14\u53d8\u4e3a\u7ed3\u6784\u53ef\u9760\u3001\u9762\u5411\u667a\u80fd\u4f53\u7684\u611f\u77e5\u548c\u8bb0\u5fc6\u5c42\uff0c\u4ee5\u652f\u6301\u5f00\u653e\u4e16\u754c\u7cfb\u7edf\u3002\u672a\u6765\u65b9\u5411\u5305\u62ec\u89e3\u51b3LLM\u65f6\u4ee3\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5efa\u7acb\u53ef\u9760\u7684\u4e8b\u4ef6\u4e2d\u5fc3\u7cfb\u7edf\u3002"}}
{"id": "2512.18934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18934", "abs": "https://arxiv.org/abs/2512.18934", "authors": ["Michael S. Zhang", "Rishi A. Ruia", "Arnav Kewalram", "Saathvik Dharmapuram", "Utkarsh Sharma", "Kevin Zhu"], "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models", "comment": null, "summary": "Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.", "AI": {"tldr": "\u91cf\u5316\uff08INT8/INT4\uff09\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u4f18\u4e8e\u9ad8\u7cbe\u5ea6\uff08FP16\uff09\uff0c\u91cf\u5316\u566a\u58f0\u8d77\u5230\u6b63\u5219\u5316\u4f5c\u7528\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5c0f\u56de\u653e\u7f13\u51b2\u533a\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\uff0cINT8\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u91cf\u5316\u7cbe\u5ea6\u4e0e\u6301\u7eed\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63a2\u7d22\u5728\u90e8\u7f72\u6548\u7387\u9700\u6c42\u4e0b\u5982\u4f55\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u4e0e\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u91cf\u5316\u7cbe\u5ea6\uff08FP16\u3001INT8\u3001INT4\uff09\u4e0e\u56de\u653e\u7f13\u51b2\u533a\u7b56\u7565\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u91cf\u5316\u566a\u58f0\u5bf9\u6301\u7eed\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u91cf\u5316\u6a21\u578b\u5728\u540e\u7eed\u4efb\u52a1\u4e0a\u8d85\u8d8aFP16\uff088-15%\u4f18\u52bf\uff09\uff0cINT4\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1FP16\u7684\u4e24\u500d\uff1b\u5373\u4f7f\u5f88\u5c0f\u7684\u56de\u653e\u7f13\u51b2\u533a\uff080.1%\uff09\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\uff1bINT8\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u91cf\u5316\u566a\u58f0\u4f5c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u6311\u6218\u4e86\"\u7cbe\u5ea6\u8d8a\u9ad8\u8d8a\u597d\"\u7684\u4f20\u7edf\u89c2\u5ff5\uff1bINT8\u91cf\u5316\u65e2\u63d0\u4f9b\u8ba1\u7b97\u6548\u7387\u53c8\u5177\u5907\u4f18\u8d8a\u7684\u6301\u7eed\u5b66\u4e60\u52a8\u6001\uff1b\u4e3a\u538b\u7f29\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2512.19019", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19019", "abs": "https://arxiv.org/abs/2512.19019", "authors": ["Ayana Hussain", "Ricky Fang"], "title": "Optimizer Dynamics at the Edge of Stability with Differential Privacy", "comment": "17 pages, 5 figures", "summary": "Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.", "AI": {"tldr": "DP\u8bad\u7ec3\u4f1a\u6539\u53d8\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u52a8\u6001\uff0c\u4f46\u8fb9\u7f18\u7a33\u5b9a\u6027\u6a21\u5f0f\u4ecd\u7136\u5b58\u5728\uff0c\u53ea\u662fDP\u4f1a\u964d\u4f4e\u9510\u5ea6\u5e76\u963b\u6b62\u4f18\u5316\u5668\u5b8c\u5168\u8fbe\u5230\u7ecf\u5178\u7a33\u5b9a\u6027\u9608\u503c", "motivation": "\u7814\u7a76\u5dee\u5206\u9690\u79c1\u5982\u4f55\u6539\u53d8\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\uff0c\u7279\u522b\u662f\u68af\u5ea6\u88c1\u526a\u548c\u9ad8\u65af\u566a\u58f0\u5bf9\u4f18\u5316\u5668\u7a33\u5b9a\u6027\u7684\u5f71\u54cd", "method": "\u6bd4\u8f83\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u548cAdam\u4e0e\u5176\u9690\u79c1\u4fdd\u62a4\u53d8\u4f53\uff0c\u5206\u6790\u68af\u5ea6\u88c1\u526a\u548c\u566a\u58f0\u5982\u4f55\u6539\u53d8\u9510\u5ea6\u548c\u635f\u5931\u6f14\u5316", "result": "DP\u901a\u5e38\u4f1a\u964d\u4f4e\u9510\u5ea6\u5e76\u963b\u6b62\u4f18\u5316\u5668\u5b8c\u5168\u8fbe\u5230\u7ecf\u5178\u7a33\u5b9a\u6027\u9608\u503c\uff0c\u4f46\u8fb9\u7f18\u7a33\u5b9a\u6027\u6a21\u5f0f\u4ecd\u7136\u5b58\u5728\uff0c\u6700\u5927\u5b66\u4e60\u7387\u548c\u6700\u5927\u9690\u79c1\u9884\u7b97\u6709\u65f6\u4f1a\u63a5\u8fd1\u6216\u8d85\u8fc7\u8fd9\u4e9b\u9608\u503c", "conclusion": "\u5dee\u5206\u9690\u79c1\u5728\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u5f15\u5165\u4e86\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u4f46\u7a33\u5b9a\u6027\u6a21\u5f0f\u4ecd\u7136\u6301\u7eed\u5b58\u5728"}}
{"id": "2512.18878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18878", "abs": "https://arxiv.org/abs/2512.18878", "authors": ["Kaidi Liang", "Ke Li", "Xianbiao Hu", "Ruwen Qin"], "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis", "comment": null, "summary": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.", "AI": {"tldr": "\u63d0\u51faCrashChat\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4ea4\u901a\u4e8b\u6545\u89c6\u9891\u7684\u591a\u4efb\u52a1\u5206\u6790\uff0c\u5728\u8bc6\u522b\u3001\u5b9a\u4f4d\u548c\u63cf\u8ff0\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u548c\u4ea4\u901a\u5b89\u5168\u7814\u7a76\u4e2d\uff0c\u4ea4\u901a\u4e8b\u6545\u89c6\u9891\u5206\u6790\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5b8c\u6210\u590d\u6742\u7684\u591a\u4efb\u52a1\u5206\u6790\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565", "method": "\u57fa\u4e8eVideoLLaMA3\u6784\u5efa\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u83b7\u53d6\u9886\u57df\u77e5\u8bc6\uff0c\u91c7\u7528\u4efb\u52a1\u89e3\u8026\u4e0e\u5206\u7ec4\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u6700\u5927\u5316\u8054\u5408\u5b66\u4e60\u6548\u76ca\u5e76\u51cf\u5c11\u8d1f\u8fc1\u79fb", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709MLLM\u548c\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\uff1a\u78b0\u649e\u8bc6\u522b\u63a5\u8fd1\u5b8c\u7f8e\u51c6\u786e\u7387\uff0c\u78b0\u649e\u5b9a\u4f4d\u63d0\u5347176%\uff0c\u78b0\u649e\u524d\u5b9a\u4f4d\u63d0\u534740%\uff1b\u5728\u63cf\u8ff0\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cBLEU\u548cROUGE\u5206\u6570\u663e\u8457\u63d0\u5347", "conclusion": "CrashChat\u662f\u4e00\u4e2a\u6027\u80fd\u5f3a\u5927\u3001\u7aef\u5230\u7aef\u7684\u591a\u4efb\u52a1\u4ea4\u901a\u4e8b\u6545\u89c6\u9891\u5206\u6790\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2512.19332", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.19332", "abs": "https://arxiv.org/abs/2512.19332", "authors": ["Pablo Barcel\u00f3", "Floris Geerts", "Matthias Lanzinger", "Klara Pakhomenko", "Jan Van den Bussche"], "title": "A Logical View of GNN-Style Computation and the Role of Activation Functions", "comment": null, "summary": "We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86MPLang\uff08\u4e00\u79cd\u63cf\u8ff0\u56fe\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u7684\u8bed\u8a00\uff09\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\uff08\u6709\u754c/\u65e0\u754c\uff09\u5bf9GNN\u8868\u8fbe\u529b\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86ReLU\u7b49\u65e0\u754c\u6fc0\u6d3b\u51fd\u6570\u6bd4\u6709\u754c\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u8ba1\u7b97\u8bed\u8a00MPLang\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u6fc0\u6d3b\u51fd\u6570\u7c7b\u578b\uff08\u6709\u754cvs\u65e0\u754c\uff09\u5982\u4f55\u5f71\u54cdGNN\u7684\u6570\u503c\u548c\u5e03\u5c14\u8868\u8fbe\u80fd\u529b\uff0c\u4ee5\u7406\u89e3\u4e0d\u540cGNN\u67b6\u6784\u7684\u7406\u8bba\u8868\u8fbe\u80fd\u529b\u5dee\u5f02\u3002", "method": "1. \u9996\u5148\u5206\u6790\u65e0\u6fc0\u6d3b\u51fd\u6570\u7684A-MPLang\u7247\u6bb5\uff0c\u7528walk-summed\u7279\u5f81\u523b\u753b\u5176\u8868\u8fbe\u80fd\u529b\uff1b2. \u7814\u7a76\u6709\u754c\u6fc0\u6d3b\u51fd\u6570\uff0c\u8bc1\u660e\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u6240\u6709\u6700\u7ec8\u5e38\u6570\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u76f8\u540c\u7684\u8868\u8fbe\u80fd\u529b\uff1b3. \u8bc1\u660e\u65e0\u754c\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\u6bd4\u6709\u754c\u6fc0\u6d3b\u51fd\u6570\u5728\u5b58\u5728\u7ebf\u6027\u5c42\u65f6\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "1. \u7ed9\u51fa\u4e86A-MPLang\u8868\u8fbe\u80fd\u529b\u7684\u7279\u5f81\u5316\u63cf\u8ff0\uff1b2. \u8bc1\u660e\u4e86\u6240\u6709\u6700\u7ec8\u5e38\u6570\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u76f8\u540c\u7684\u8868\u8fbe\u80fd\u529b\uff1b3. \u9996\u6b21\u8bc1\u660e\u4e86\u5728\u5b58\u5728\u7ebf\u6027\u5c42\u65f6\uff0c\u65e0\u754c\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\u6bd4\u6709\u754c\u6fc0\u6d3b\u51fd\u6570\uff08\u5982\u622a\u65adReLU\uff09\u5177\u6709\u66f4\u5f3a\u7684\u6570\u503c\u67e5\u8be2\u80fd\u529b\u3002", "conclusion": "\u6fc0\u6d3b\u51fd\u6570\u7684\u7c7b\u578b\u663e\u8457\u5f71\u54cdGNN\u7684\u8868\u8fbe\u80fd\u529b\uff1a\u65e0\u754c\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\u6bd4\u6709\u754c\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u7ebf\u6027\u5c42\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u63ed\u793a\u4e86\u7ebf\u6027\u805a\u5408\u4e0e\u6700\u7ec8\u5e38\u6570\u975e\u7ebf\u6027\u4e4b\u95f4\u7684\u5fae\u5999\u76f8\u4e92\u4f5c\u7528\uff0c\u8868\u660e\u4f7f\u7528ReLU\u7684GNN\u6bd4\u9650\u5236\u4f7f\u7528\u6700\u7ec8\u5e38\u6570\u6fc0\u6d3b\u51fd\u6570\u548c\u7ebf\u6027\u5c42\u7684GNN\u66f4\u5177\u8868\u8fbe\u529b\u3002"}}
{"id": "2512.19506", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19506", "abs": "https://arxiv.org/abs/2512.19506", "authors": ["Hongliang Li", "Nong Zhang", "Zhewen Xu", "Xiang Li", "Changzheng Liu", "Chongbo Zhao", "Jie Wu"], "title": "DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast", "comment": "18 pages, 10 figures", "summary": "Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.", "AI": {"tldr": "\u63d0\u51faDK-STN\u6a21\u578b\uff0c\u7ed3\u5408\u6570\u503c\u5929\u6c14\u9884\u62a5\u4e0e\u795e\u7ecf\u7f51\u7edc\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684MJO\u9884\u6d4b\uff0c\u7cbe\u5ea6\u4e0eECMWF\u76f8\u5f53\u4f46\u6548\u7387\u66f4\u9ad8", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u3001\u8017\u65f6\u4e14\u4e0d\u7a33\u5b9a\uff0c\u800c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u867d\u8282\u7701\u8d44\u6e90\u4f46\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u65e0\u6cd5\u8fbe\u523028\u5929\u9884\u6d4b\u6c34\u5e73\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u6539\u8fdbMJO\u9884\u6d4b", "method": "\u63d0\u51fa\u9886\u57df\u77e5\u8bc6\u5d4c\u5165\u65f6\u7a7a\u7f51\u7edc(DK-STN)\uff0c\u5728\u65f6\u7a7a\u7f51\u7edc\u57fa\u7840\u4e0a\u901a\u8fc7\u4e24\u79cd\u5173\u952e\u65b9\u6cd5\u5d4c\u5165\u9886\u57df\u77e5\u8bc6\uff1a1)\u5e94\u7528\u9886\u57df\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff1b2)\u5c06\u9886\u57df\u77e5\u8bc6\u5904\u7406\u65b9\u6cd5\u96c6\u6210\u5230\u7f51\u7edc\u8bad\u7ec3\u4e2d", "result": "\u4f7f\u7528ERA5\u6570\u636e\u8bc4\u4f30\uff0cDK-STN\u4ee57\u5929\u6c14\u5019\u6570\u636e\u4e3a\u8f93\u5165\uff0c1-2\u79d2\u5185\u751f\u621028\u5929\u53ef\u9760\u9884\u6d4b\uff0c\u4e0d\u540c\u5b63\u8282\u8bef\u5dee\u4ec52-3\u5929\uff0c\u7cbe\u5ea6\u4e0eECMWF\u76f8\u5f53\u4f46\u6548\u7387\u548c\u7a33\u5b9a\u6027\u663e\u8457\u66f4\u4f18", "conclusion": "DK-STN\u6210\u529f\u7ed3\u5408\u4e86NWP\u548cANN\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u548c\u9ad8\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3aMJO\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.19530", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19530", "abs": "https://arxiv.org/abs/2512.19530", "authors": ["Hongsheng Xing", "Qiuxin Si"], "title": "Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement", "comment": "13 pages, 6 figures", "summary": "Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $>25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Catechol Benchmark\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u8fde\u7eed\u6eb6\u5242\u7ec4\u6210\u8303\u56f4\u5185\u9884\u6d4b\u53cd\u5e94\u4ea7\u7387\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408GNN\u67b6\u6784\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6709\u673a\u5408\u6210\u548c\u8fc7\u7a0b\u5316\u5b66\u4e2d\uff0c\u9884\u6d4b\u8fde\u7eed\u6eb6\u5242\u7ec4\u6210\u8303\u56f4\u5185\u7684\u53cd\u5e94\u7ed3\u679c\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u6eb6\u5242\u89c6\u4e3a\u79bb\u6563\u5206\u7c7b\u53d8\u91cf\uff0c\u8fd9\u963b\u788d\u4e86\u5728\u6eb6\u5242\u7a7a\u95f4\u4e2d\u7684\u7cfb\u7edf\u63d2\u503c\u548c\u5916\u63a8\u3002", "method": "\u63d0\u51fa\u4e86Catechol Benchmark\u6570\u636e\u96c6\uff0c\u5305\u542b1,227\u4e2a\u5b9e\u9a8c\u4ea7\u7387\u6d4b\u91cf\u503c\uff0c\u6db5\u76d624\u79cd\u7eaf\u6eb6\u5242\u53ca\u5176\u4e8c\u5143\u6df7\u5408\u7269\u3002\u5f00\u53d1\u4e86\u6df7\u5408GNN\u67b6\u6784\uff0c\u6574\u5408\u4e86\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GATs)\u3001\u5dee\u5206\u53cd\u5e94\u6307\u7eb9(DRFP)\u548c\u5b66\u4e60\u7684\u6df7\u5408\u7269\u611f\u77e5\u6eb6\u5242\u7f16\u7801\u3002\u91c7\u7528\u4e25\u683c\u7684\u7559\u4e00\u6eb6\u5242\u548c\u7559\u4e00\u6df7\u5408\u7269\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4f20\u7edf\u8868\u683c\u65b9\u6cd5\uff08\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff08Qwen-7B\uff09\u8868\u73b0\u4e0d\u4f73\uff0cMSE\u5206\u522b\u4e3a0.099\u548c0.129\u3002\u63d0\u51fa\u7684\u6df7\u5408GNN\u67b6\u6784\u5b9e\u73b0\u4e86MSE 0.0039\uff08\u00b10.0003\uff09\uff0c\u6bd4\u7ade\u4e89\u57fa\u7ebf\u8bef\u5dee\u51cf\u5c1160%\uff0c\u6bd4\u8868\u683c\u96c6\u6210\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc725\u500d\u3002", "conclusion": "\u663e\u5f0f\u5206\u5b50\u56fe\u6d88\u606f\u4f20\u9012\u548c\u8fde\u7eed\u6df7\u5408\u7269\u7f16\u7801\u5bf9\u4e8e\u7a33\u5065\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002\u8be5\u7814\u7a76\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u53cd\u5e94\u9884\u6d4b\u548c\u8fde\u7eed\u6eb6\u5242\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u53c2\u8003\u5b9e\u73b0\u3002"}}
{"id": "2512.19213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19213", "abs": "https://arxiv.org/abs/2512.19213", "authors": ["Zihao Luo", "Shaohao Rui", "Zhenyu Tang", "Guotai Wang", "Xiaosong Wang"], "title": "InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training", "comment": "16 pages, 10 figures, 5 tables", "summary": "Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.", "AI": {"tldr": "InvCoSS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u53cd\u6f14\u7684\u6301\u7eed\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u50cf\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u56de\u653e\uff0c\u89e3\u51b3\u533b\u5b66\u591a\u6a21\u6001\u56fe\u50cf\u9884\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u56de\u653e\u5148\u524d\u9636\u6bb5\u7684\u6570\u636e\u6765\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f46\u8fd9\u4f1a\u635f\u5bb3\u6570\u636e\u9690\u79c1\uff0c\u4e14\u5728\u8de8\u7ad9\u70b9\u6570\u636e\u4f20\u9012\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u5e94\u7528\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6027\u80fd\u53c8\u80fd\u4e25\u683c\u4fdd\u62a4\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u8bad\u7ec3\u5b8c\u524d\u4e00\u4efb\u52a1\u540e\uff0c\u901a\u8fc7\u6a21\u578b\u53cd\u6f14\u751f\u6210\u8fd1\u4f3c\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u7684\u5408\u6210\u56fe\u50cf\uff1b2) \u63d0\u51faInvUNet\u591a\u5c3a\u5ea6\u878d\u5408\u67b6\u6784\uff0c\u6062\u590d\u53cd\u6f14\u56fe\u50cf\u7684\u9ad8\u4f4e\u9891\u6210\u5206\uff1b3) \u8bbe\u8ba1\u6392\u65a5\u6027\u8868\u5f81\u5b66\u4e60\u673a\u5236\uff0c\u5728\u6ca1\u6709\u7c7b\u522b\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u4fc3\u8fdb\u5408\u6210\u56fe\u50cf\u7279\u5f81\u7a7a\u95f4\u7684\u591a\u6837\u6027\uff1b4) \u5c06\u5408\u6210\u56fe\u50cf\u4e0e\u65b0\u4efb\u52a1\u6570\u636e\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u4e5d\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86InvCoSS\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5148\u524d\u7684\u6570\u636e\u56de\u653e\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u5e76\u5b8c\u5168\u6d88\u9664\u6570\u636e\u9690\u79c1\u9650\u5236\u3002", "conclusion": "InvCoSS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u6301\u7eed\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u53cd\u6f14\u751f\u6210\u5408\u6210\u56fe\u50cf\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u56de\u653e\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6570\u636e\u9690\u79c1\u548c\u4f20\u8f93\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2512.19554", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19554", "abs": "https://arxiv.org/abs/2512.19554", "authors": ["Yongxin Wang", "Zhicheng Yang", "Meng Cao", "Mingfei Han", "Haokun Lin", "Yingying Zhu", "Xiaojun Chang", "Xiaodan Liang"], "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal", "comment": null, "summary": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.", "AI": {"tldr": "CARE\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u7684\u5931\u8d25\u4e2d\u5fc3\u5316\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u951a\u5b9a\u548c\u53cd\u601d\u5f15\u5bfc\u91cd\u91c7\u6837\uff0c\u5c06\u9519\u8bef\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u5347\u9a8c\u8bc1\u6027\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9a8c\u8bc1\u5956\u52b1\u7684\u7fa4\u4f53\u76f8\u5bf9\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u5904\u7406\u5931\u8d25\u6570\u636e\u65f6\u6548\u7387\u4f4e\u4e0b\uff1a\u5f53\u6240\u6709\u8f68\u8ff9\u90fd\u9519\u8bef\u65f6\u68af\u5ea6\u505c\u6ede\uff1b\u5f53\u6709\u6b63\u786e\u8f68\u8ff9\u65f6\uff0c\u66f4\u65b0\u901a\u5e38\u5ffd\u7565\u5176\u4ed6\u63a5\u8fd1\u4f46\u9519\u8bef\u7684\u539f\u56e0\uff0c\u5e76\u4e14\u53ef\u80fd\u5c06\u4fe1\u7528\u9519\u8bef\u5206\u914d\u7ed9\u865a\u5047\u7684\u63a8\u7406\u94fe\u3002", "method": "CARE\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u951a\u5b9a\u5bf9\u6bd4\u76ee\u6807\uff0c\u56f4\u7ed5\u6700\u4f73\u8f68\u8ff9\u5f62\u6210\u7d27\u51d1\u5b50\u7ec4\u548c\u8bed\u4e49\u76f8\u8fd1\u7684\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u8fdb\u884c\u7ec4\u5185z-score\u5f52\u4e00\u5316\u5e76\u5305\u542b\u8d1f\u6837\u672c\u7f29\u653e\u548c\u5168\u8d1f\u6837\u672c\u6551\u63f4\u673a\u5236\uff1b2\uff09\u53cd\u601d\u5f15\u5bfc\u91cd\u91c7\u6837\uff08RGR\uff09\uff0c\u901a\u8fc7\u4e00\u6b21\u6027\u7ed3\u6784\u5316\u81ea\u6211\u4fee\u590d\u91cd\u5199\u4ee3\u8868\u6027\u5931\u8d25\u6837\u672c\u5e76\u7528\u76f8\u540c\u9a8c\u8bc1\u5668\u91cd\u65b0\u8bc4\u5206\uff0c\u5c06\u63a5\u8fd1\u6b63\u786e\u7684\u5931\u8d25\u8f6c\u5316\u4e3a\u53ef\u7528\u6b63\u6837\u672c\u3002", "result": "\u5728Qwen2.5-VL-7B\u4e0a\uff0cCARE\u5728\u516d\u4e2a\u53ef\u9a8c\u8bc1\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4GRPO\u63d0\u53474.6\u4e2a\u767e\u5206\u70b9\u7684\u5b8f\u5e73\u5747\u51c6\u786e\u7387\uff1b\u5728Qwen3-VL-8B\u4e0a\uff0c\u5728\u76f8\u540c\u8bc4\u4f30\u534f\u8bae\u4e0b\u5728MathVista\u548cMMMU-Pro\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "CARE\u901a\u8fc7\u5c06\u9519\u8bef\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u5e73\u6ed1\u5ea6\uff0c\u540c\u65f6\u660e\u786e\u589e\u52a0\u4e86\u6765\u81ea\u5931\u8d25\u7684\u5b66\u4e60\u4fe1\u53f7\u6bd4\u4f8b\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u5229\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.19327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19327", "abs": "https://arxiv.org/abs/2512.19327", "authors": ["Moamal Fadhil Abdul", "Jonas Bruun Hubrechts", "Thomas Martini J\u00f8rgensen", "Emil Hovad"], "title": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome", "comment": "Thomas Martini J\u00f8rgensen and Emil Hovad contributed equally and share last authorship", "summary": "Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either \"bounce\", \"net\", or \"empty_event\" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.", "AI": {"tldr": "\u6269\u5c55OpenTTGames\u6570\u636e\u96c6\uff0c\u6dfb\u52a0\u8be6\u7ec6\u7684\u51fb\u7403\u7c7b\u578b\u3001\u7403\u5458\u59ff\u52bf\u548c\u56de\u5408\u7ed3\u679c\u6807\u6ce8\uff0c\u652f\u6301\u4e52\u4e53\u7403\u89c6\u9891\u7684\u7ec6\u7c92\u5ea6\u5206\u6790", "motivation": "\u73b0\u6709\u4e52\u4e53\u7403\u89c6\u9891\u6570\u636e\u96c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u51fb\u7403\u7c7b\u578b\u3001\u6218\u672f\u5206\u6790\u7b49\u5e94\u7528\u7684\u53d1\u5c55", "method": "\u5728OpenTTGames\u6570\u636e\u96c6\u57fa\u7840\u4e0a\uff0c\u6dfb\u52a0\u5e27\u7ea7\u51fb\u7403\u7c7b\u578b\u6807\u6ce8\uff08\u6b63\u624b\u3001\u53cd\u624b\u53ca\u5b50\u7c7b\u578b\uff09\u3001\u7403\u5458\u59ff\u52bf\u6807\u7b7e\uff08\u8eab\u4f53\u503e\u659c\u548c\u817f\u90e8\u59ff\u52bf\uff09\u548c\u56de\u5408\u7ed3\u679c\u6807\u7b7e\uff0c\u63d0\u4f9b\u7d27\u51d1\u7f16\u7801\u65b9\u6848\u548c\u4ee3\u7801\u8f85\u52a9\u6807\u6ce8\u6d41\u7a0b", "result": "\u521b\u5efa\u4e86\u5305\u542b\u4e30\u5bcc\u6807\u6ce8\u7684\u6269\u5c55\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u793e\u533a\u4e2d\u516c\u5f00\u53ef\u7528\u7684\u7ec6\u7c92\u5ea6\u4e52\u4e53\u7403\u89c6\u9891\u6570\u636e\u96c6\u7684\u7a7a\u767d", "conclusion": "\u6269\u5c55\u540e\u7684OpenTTGames\u6570\u636e\u96c6\u652f\u6301\u4ece\u4e8b\u4ef6\u68c0\u6d4b\u5411\u6218\u672f\u7406\u89e3\u7684\u6a21\u578b\u53d1\u5c55\uff0c\u91c7\u7528CC BY-NC-SA 4.0\u8bb8\u53ef\u4fc3\u8fdb\u975e\u5546\u4e1a\u4f7f\u7528\u548c\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2512.19512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19512", "abs": "https://arxiv.org/abs/2512.19512", "authors": ["Ziyang Song", "Zelin Zang", "Zuyao Chen", "Xusheng Liang", "Dong Yi", "Jinlin Wu", "Hongbin Liu", "Jiebo Luo"], "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u6539\u8fdbMLLMs\u5728\u533b\u5b66\u89e3\u5256\u56fe\u50cf\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff1a\u89e3\u5256\u76f8\u4f3c\u6027\u8bfe\u7a0b\u5b66\u4e60\u548c\u7ec4\u591a\u6837\u6027\u95ee\u9898\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u5728SGG-VQA\u548cOmniMedVQA\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u533b\u5b66\u5f71\u50cf\u7279\u522b\u662f\u4e34\u5e8a\u89e3\u5256\u624b\u672f\u56fe\u50cf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u89e3\u5256\u7406\u89e3\u4efb\u52a1\u9700\u8981\u7cbe\u786e\u7406\u89e3\u548c\u4e34\u5e8a\u4e00\u81f4\u7684\u7b54\u6848\uff0c\u4f46\u7531\u4e8e\u533b\u5b66\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6807\u6ce8\u7684\u7a00\u7f3a\u6027\uff0c\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u6548\u679c\u6709\u9650\u3002\u867d\u7136GRPO\u65b9\u6cd5\u80fd\u5728\u65e0\u9700\u5927\u91cf\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u589e\u5f3aMLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u89e3\u5256\u8bc6\u522b\u4e2d\u5b58\u5728\u4e24\u4e2a\u5f31\u70b9\uff1a\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\u95f4\u7684\u77e5\u8bc6\u65e0\u6cd5\u6709\u6548\u5171\u4eab\uff0c\u4ee5\u53ca\u6a21\u578b\u5feb\u901f\u6536\u655b\u5230\u5355\u4e00\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff1a1) \u89e3\u5256\u76f8\u4f3c\u6027\u8bfe\u7a0b\u5b66\u4e60\uff1a\u901a\u8fc7\u63a7\u5236\u95ee\u9898\u96be\u5ea6\uff08\u57fa\u4e8e\u7b54\u6848\u9009\u9879\u7684\u76f8\u4f3c\u6027\uff09\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u9010\u6b65\u638c\u63e1\u590d\u6742\u95ee\u9898\uff1b2) \u7ec4\u591a\u6837\u6027\u95ee\u9898\u589e\u5f3a\uff1a\u901a\u8fc7\u95ee\u9898\u589e\u5f3a\u6269\u5c55\u6a21\u578b\u5bf9\u56f0\u96be\u67e5\u8be2\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u51cf\u5c11\u4ea7\u751f\u7edf\u4e00\u54cd\u5e94\u7684\u503e\u5411\u3002", "result": "\u5728SGG-VQA\u548cOmniMedVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3aMLLMs\u533b\u5b66\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u5256\u76f8\u4f3c\u6027\u8bfe\u7a0b\u5b66\u4e60\u548c\u7ec4\u591a\u6837\u6027\u95ee\u9898\u589e\u5f3a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86GRPO\u5728\u89e3\u5256\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u89e3\u5256\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2512.19528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19528", "abs": "https://arxiv.org/abs/2512.19528", "authors": ["Marc Peral", "Guillem Capellera", "Luis Ferraz", "Antonio Rubio", "Antonio Agudo"], "title": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training", "comment": "10 pages, 2 figures. WACV 2026", "summary": "In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u5206\u6790\u8db3\u7403\u6218\u672f\u955c\u5934\u89c6\u9891\u7684\u591a\u6a21\u6001\u67b6\u6784\uff0c\u4e13\u6ce8\u4e8e\u7403\u8f68\u8ff9\u63a8\u65ad\u3001\u7403\u72b6\u6001\u5206\u7c7b\u548c\u6301\u7403\u8005\u8bc6\u522b\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u7403\u5458\u8f68\u8ff9\u3001\u7403\u5458\u7c7b\u578b\u548c\u7403\u5458\u56fe\u50cf\u88c1\u526a\u4e09\u79cd\u8f93\u5165\u6a21\u6001\uff0c\u4f7f\u7528\u793e\u4f1a\u65f6\u7a7aTransformer\u5757\u5904\u7406\u65f6\u7a7a\u52a8\u6001\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u7403\u7684\u5386\u53f2\u6216\u672a\u6765\u4f4d\u7f6e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u51c6\u786e\u7684\u7403\u8ddf\u8e2a\u6216\u624b\u5de5\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u65e0\u6cd5\u5728\u771f\u5b9e\u9876\u7ea7\u8054\u8d5b\u6bd4\u8d5b\u7684\u566a\u58f0\u6216\u906e\u6321\u6761\u4ef6\u4e0b\u9c81\u68d2\u5730\u8bc6\u522b\u7403\u72b6\u6001\u548c\u6301\u7403\u8005\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u6218\u672f\u955c\u5934\u4e2d\u5206\u6790\u8db3\u7403\u573a\u666f\u7684\u7efc\u5408\u6027\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u4e09\u79cd\u8f93\u5165\u6a21\u6001\uff08\u7403\u5458\u8f68\u8ff9\u3001\u7403\u5458\u7c7b\u578b\u548c\u7403\u5458\u56fe\u50cf\u88c1\u526a\uff09\u5230\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528\u7ea7\u8054\u7684\u793e\u4f1a\u65f6\u7a7aTransformer\u5757\u5904\u7406\u65f6\u7a7a\u52a8\u6001\u3002\u5f15\u5165CropDrop\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u63a9\u7801\u9632\u6b62\u8fc7\u5ea6\u4f9d\u8d56\u56fe\u50cf\u7279\u5f81\uff0c\u9f13\u52b1\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u4f9d\u8d56\u8de8\u6a21\u6001\u6a21\u5f0f\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u7ed3\u679c\u7a81\u51fa\u4e86\u5728\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u4e2d\u7ed3\u5408\u7ed3\u6784\u5316\u548c\u89c6\u89c9\u7ebf\u7d22\u7684\u597d\u5904\uff0c\u4ee5\u53ca\u73b0\u5b9e\u63a9\u7801\u7b56\u7565\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u67b6\u6784\u80fd\u591f\u4ece\u6218\u672f\u955c\u5934\u4e2d\u5206\u6790\u8db3\u7403\u573a\u666f\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u7403\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5728\u566a\u58f0\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u8868\u73b0\u9c81\u68d2\u3002CropDrop\u9884\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u9632\u6b62\u4e86\u8fc7\u5ea6\u4f9d\u8d56\u56fe\u50cf\u7279\u5f81\uff0c\u4fc3\u8fdb\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u3002"}}
{"id": "2512.19546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19546", "abs": "https://arxiv.org/abs/2512.19546", "authors": ["Ziqiao Peng", "Yi Chen", "Yifeng Ma", "Guozhen Zhang", "Zhiyao Sun", "Zixiang Zhou", "Youliang Zhang", "Zhengguang Zhou", "Zhaoxin Fan", "Hongyan Liu", "Yuan Zhou", "Qinglin Lu", "Jun He"], "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars", "comment": "Project Page: https://ziqiaopeng.github.io/ActAvatar/", "summary": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.", "AI": {"tldr": "ActAvatar\uff1a\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u5b9e\u73b0\u76f8\u4f4d\u7ea7\u7cbe\u5ea6\u52a8\u4f5c\u63a7\u5236\u7684\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u8ddf\u968f\u80fd\u529b\u3001\u65f6\u95f4\u5bf9\u9f50\u548c\u4f9d\u8d56\u989d\u5916\u63a7\u5236\u4fe1\u53f7\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u6587\u672c\u8ddf\u968f\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u751f\u6210\u591a\u6837\u5316\u52a8\u4f5c\uff1b2) \u52a8\u4f5c\u4e0e\u97f3\u9891\u5185\u5bb9\u4e4b\u95f4\u7f3a\u4e4f\u65f6\u95f4\u5bf9\u9f50\uff1b3) \u4f9d\u8d56\u989d\u5916\u7684\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u59ff\u6001\u9aa8\u67b6\uff09\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u5b9e\u73b0\u7cbe\u786e\u52a8\u4f5c\u63a7\u5236\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u76f8\u4f4d\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b(PACA)\uff0c\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u5168\u5c40\u57fa\u7840\u5757\u548c\u65f6\u95f4\u951a\u5b9a\u7684\u76f8\u4f4d\u5757\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u76f8\u4f4d\u76f8\u5173\u6807\u8bb0\uff1b2) \u6e10\u8fdb\u5f0f\u89c6\u542c\u5bf9\u9f50\uff0c\u65e9\u671f\u5c42\u4f18\u5148\u6587\u672c\u5efa\u7acb\u52a8\u4f5c\u7ed3\u6784\uff0c\u6df1\u5c42\u5f3a\u8c03\u97f3\u9891\u7ec6\u5316\u5507\u90e8\u8fd0\u52a8\uff1b3) \u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u5728\u591a\u6837\u5316\u6570\u636e\u4e0a\u5efa\u7acb\u7a33\u5065\u7684\u89c6\u542c\u5bf9\u5e94\uff0c\u518d\u901a\u8fc7\u7ed3\u6784\u5316\u6ce8\u91ca\u5fae\u8c03\u6ce8\u5165\u52a8\u4f5c\u63a7\u5236\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cActAvatar\u5728\u52a8\u4f5c\u63a7\u5236\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "ActAvatar\u6846\u67b6\u901a\u8fc7\u76f8\u4f4d\u7ea7\u7cbe\u5ea6\u52a8\u4f5c\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6587\u672c\u8ddf\u968f\u80fd\u529b\u548c\u65f6\u95f4\u5bf9\u9f50\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u989d\u5916\u63a7\u5236\u4fe1\u53f7\u3002"}}
{"id": "2512.19663", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19663", "abs": "https://arxiv.org/abs/2512.19663", "authors": ["Argha Kamal Samanta", "Harshika Goyal", "Vasudha Joshi", "Tushar Mungle", "Pabitra Mitra"], "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis", "comment": "14 pages, 14 figures", "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.", "AI": {"tldr": "\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u7684\u8054\u5408\u5d4c\u5165\u6846\u67b6\uff0c\u6574\u5408\u89c6\u7f51\u819c\u56fe\u50cf\u3001\u4e34\u5e8a\u6587\u672c\u548c\u7ed3\u6784\u5316\u60a3\u8005\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5168\u7403\u53ef\u9884\u9632\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u51c6\u786e\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u7cfb\u7edf\u3002\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u533b\u5b66\u9886\u57df\u5e94\u7528\uff0c\u7279\u522b\u662f\u773c\u79d1\u56fe\u50cf\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u7684\u8054\u5408\u5d4c\u5165\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u6a21\u6001Transformer\u67b6\u6784\uff1aVision Transformer\u5904\u7406\u89c6\u7f51\u819c\u56fe\u50cf\uff0cBio-ClinicalBERT\u5904\u7406\u4e34\u5e8a\u53d9\u8ff0\uff0c\u591a\u5c42\u611f\u77e5\u673a\u5904\u7406\u7ed3\u6784\u5316\u7279\u5f81\u3002\u901a\u8fc7\u8054\u5408Transformer\u878d\u5408\u5404\u6a21\u6001\uff0c\u4f7f\u7528\u5bf9\u6bd4\u635f\u5931\u3001\u91cd\u5efa\u635f\u5931\u548c\u5206\u7c7b\u635f\u5931\u8fdb\u884c\u591a\u76ee\u6807\u8bad\u7ec3\u3002", "result": "\u5728BRSET\u6570\u636e\u96c6\u4e0a\uff0c\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22Recall@1\u8fbe\u523099.94%\uff08\u76f8\u6bd4\u5fae\u8c03CLIP\u76841.29%\uff09\uff0cSDRG\u5206\u7c7b\u51c6\u786e\u738797.05%\uff0cICDR\u5206\u7c7b\u51c6\u786e\u738797.97%\u3002\u5728DeepEyeNet\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793aRecall@1\u4e3a93.95%\uff08\u76f8\u6bd4\u5fae\u8c03CLIP\u76840.22%\uff09\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u6355\u6349\u533b\u5b66\u9886\u57df\u7684\u8de8\u6a21\u6001\u5173\u7cfb\uff0c\u5728\u68c0\u7d22\u80fd\u529b\u548c\u8bca\u65ad\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
