{"id": "2512.11811", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11811", "abs": "https://arxiv.org/abs/2512.11811", "authors": ["Fengyi Xu", "Jun Ma", "Waishan Qiu", "Cui Guo"], "title": "Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention", "comment": null, "summary": "Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.", "AI": {"tldr": "VPR-AttLLM\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u548c\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u96c6\u6210\u5230\u73b0\u6709\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u7ba1\u9053\u4e2d\uff0c\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u8857\u666f\u56fe\u50cf\u7684\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\uff0c\u7279\u522b\u9488\u5bf9\u6d2a\u6c34\u7b49\u5371\u673a\u4e8b\u4ef6\u56fe\u50cf\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u63d0\u4f9b\u7684\u8857\u666f\u56fe\u50cf\u867d\u7136\u80fd\u5b9e\u65f6\u53cd\u6620\u57ce\u5e02\u6d2a\u6c34\u7b49\u5371\u673a\u4e8b\u4ef6\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u5730\u7406\u5143\u6570\u636e\u3002\u73b0\u6709\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6a21\u578b\u5728\u5904\u7406\u8fd9\u7c7b\u56fe\u50cf\u65f6\uff0c\u7531\u4e8e\u89c6\u89c9\u5931\u771f\u548c\u8de8\u6e90\u573a\u666f\u7684\u9886\u57df\u504f\u79fb\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faVPR-AttLLM\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u63cf\u8ff0\u7b26\u589e\u5f3a\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u548c\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u96c6\u6210\u5230\u73b0\u6709VPR\u7ba1\u9053\u4e2d\u3002\u5229\u7528LLM\u8bc6\u522b\u57ce\u5e02\u80cc\u666f\u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\u533a\u57df\u5e76\u6291\u5236\u77ac\u6001\u89c6\u89c9\u566a\u58f0\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u989d\u5916\u6570\u636e\u3002", "result": "\u5728\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06VPR-AttLLM\u4e0e\u4e09\u79cd\u6700\u5148\u8fdb\u7684VPR\u6a21\u578b\uff08CosPlace\u3001EigenPlaces\u3001SALAD\uff09\u96c6\u6210\uff0c\u53ec\u56de\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u76f8\u5bf9\u589e\u76ca\u901a\u5e38\u57281-3%\u4e4b\u95f4\uff0c\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u771f\u5b9e\u6d2a\u6c34\u56fe\u50cf\u4e0a\u53ef\u8fbe8%\u3002", "conclusion": "VPR-AttLLM\u4e3aLLM\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u5728\u89c6\u89c9\u68c0\u7d22\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u53ef\u63a8\u5e7f\u7684\u8303\u5f0f\uff0c\u5c06\u57ce\u5e02\u611f\u77e5\u7406\u8bba\u539f\u7406\u5d4c\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8fde\u63a5\u4e86\u7c7b\u4eba\u7a7a\u95f4\u63a8\u7406\u4e0e\u73b0\u4ee3VPR\u67b6\u6784\u3002\u5176\u5373\u63d2\u5373\u7528\u8bbe\u8ba1\u3001\u5f3a\u5927\u7684\u8de8\u6e90\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u57ce\u5e02\u76d1\u6d4b\u548c\u5371\u673a\u56fe\u50cf\u5feb\u901f\u5730\u7406\u5b9a\u4f4d\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.11891", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11891", "abs": "https://arxiv.org/abs/2512.11891", "authors": ["Songqiao Hu", "Zeyi Liu", "Shuang Liu", "Jun Cen", "Zihan Meng", "Xiao He"], "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer", "comment": "20 pages, 14 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.", "AI": {"tldr": "\u63d0\u51faAEGIS\u67b6\u6784\uff0c\u901a\u8fc7\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u4e3a\u73b0\u6709VLA\u6a21\u578b\u6dfb\u52a0\u5b89\u5168\u7ea6\u675f\u5c42\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5e76\u5728\u65b0\u57fa\u51c6SafeLIBERO\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6210\u548c\u907f\u514d\u78b0\u649e\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "\u63d0\u51faVLSA\u67b6\u6784AEGIS\uff0c\u5305\u542b\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u7ea6\u675f\u5c42\uff0c\u53ef\u5373\u63d2\u5373\u7528\u5f0f\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\uff0c\u63d0\u4f9b\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5728\u6784\u5efa\u7684SafeLIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAEGIS\u76f8\u6bd4SOTA\u57fa\u7ebf\u5728\u969c\u788d\u7269\u907f\u514d\u7387\u4e0a\u63d0\u534759.16%\uff0c\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u63d0\u534717.25%\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u3002", "conclusion": "AEGIS\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u7684\u5b89\u5168\u7ea6\u675f\u5c42\u5b9e\u73b0\u4e86\u4efb\u52a1\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.12167", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12167", "abs": "https://arxiv.org/abs/2512.12167", "authors": ["Yoav Gelberg", "Koshi Eguchi", "Takuya Akiba", "Edoardo Cetin"], "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings", "comment": null, "summary": "So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.", "AI": {"tldr": "DroPE\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u540e\u4e22\u5f03\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u65e0\u9700\u957f\u4e0a\u4e0b\u6587\u5fae\u8c03", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u624d\u80fd\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4f4d\u7f6e\u5d4c\u5165\u5728\u8bad\u7ec3\u65f6\u5f88\u91cd\u8981\u4f46\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u5bf9\u672a\u89c1\u957f\u5ea6\u5e8f\u5217\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faDroPE\u65b9\u6cd5\uff1a\u5728\u9884\u8bad\u7ec3\u540e\u4e22\u5f03\u4f4d\u7f6e\u5d4c\u5165\uff0c\u7ecf\u8fc7\u77ed\u6682\u91cd\u65b0\u6821\u51c6\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u9002\u5e94\u65e0\u4f4d\u7f6e\u4fe1\u606f\u7684\u72b6\u6001", "result": "DroPE\u5b9e\u73b0\u4e86\u65e0\u7f1d\u7684\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u65e0\u9700\u957f\u4e0a\u4e0b\u6587\u5fae\u8c03\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u89c4\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fdc\u8d85\u73b0\u6709\u4f4d\u7f6e\u5d4c\u5165\u7f29\u653e\u65b9\u6cd5", "conclusion": "\u4f4d\u7f6e\u5d4c\u5165\u4e0d\u662f\u8bed\u8a00\u5efa\u6a21\u7684\u5185\u5728\u8981\u6c42\uff0c\u53ef\u4ee5\u5728\u9884\u8bad\u7ec3\u540e\u5b89\u5168\u79fb\u9664\uff0cDroPE\u65b9\u6cd5\u6253\u7834\u4e86\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5173\u952e\u74f6\u9888"}}
{"id": "2512.12048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12048", "abs": "https://arxiv.org/abs/2512.12048", "authors": ["Muddsair Sharif", "Huseyin Seker"], "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp", "comment": null, "summary": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.", "AI": {"tldr": "\u63d0\u51faCAMAC-DRA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4f18\u5316\u667a\u80fd\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u751f\u6001\u7cfb\u7edf\uff0c\u5e73\u8861\u4e94\u4e2a\u5229\u76ca\u76f8\u5173\u65b9\uff0c\u5728\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\u3002", "motivation": "\u667a\u80fd\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u751f\u6001\u7cfb\u7edf\u9700\u8981\u534f\u8c03\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u65b9\uff08\u7528\u6237\u3001\u7535\u7f51\u8fd0\u8425\u5546\u3001\u5145\u7535\u7ad9\u8fd0\u8425\u5546\u3001\u8f66\u961f\u8fd0\u8425\u5546\u3001\u73af\u5883\u56e0\u7d20\uff09\uff0c\u540c\u65f6\u9002\u5e94\u52a8\u6001\u73af\u5883\u6761\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8fd9\u4e9b\u7ade\u4e89\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u534f\u8c03\u6df1\u5ea6Q\u7f51\u7edc\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5904\u740620\u4e2a\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u901a\u8fc7\u52a0\u6743\u534f\u8c03\u673a\u5236\u548c\u5171\u8bc6\u534f\u8bae\u5e73\u8861\u4e94\u4e2a\u5229\u76ca\u76f8\u5173\u65b9\u3002", "result": "\u5728441,077\u4e2a\u771f\u5b9e\u5145\u7535\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4DDPG\u3001A3C\u3001PPO\u548cGNN\u7b49\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5b9e\u73b092%\u534f\u8c03\u6210\u529f\u7387\u300115%\u80fd\u6548\u63d0\u5347\u300110%\u6210\u672c\u964d\u4f4e\u300120%\u7535\u7f51\u538b\u529b\u51cf\u5c11\u30012.3\u500d\u66f4\u5feb\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u630188%\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c85%\u6837\u672c\u6548\u7387\u3002", "conclusion": "CAMAC-DRA\u6846\u67b6\u6210\u529f\u5f00\u53d1\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u5229\u76ca\u76f8\u5173\u65b9\u534f\u8c03\u7cfb\u7edf\uff0c\u6709\u6548\u5e73\u8861\u7ade\u4e89\u76ee\u6807\u5e76\u9002\u5e94\u5b9e\u65f6\u53d8\u91cf\uff0c\u662f\u667a\u80fd\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u534f\u8c03\u548c\u53ef\u6301\u7eed\u4ea4\u901a\u7535\u6c14\u5316\u7684\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11841", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11841", "abs": "https://arxiv.org/abs/2512.11841", "authors": ["Sasi Vardhan Reddy Mandapati"], "title": "Meta-Continual Mobility Forecasting for Proactive Handover Prediction", "comment": "6 pages, 1 figure", "summary": "Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5143\u6301\u7eed\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u77ed\u671f\u79fb\u52a8\u6027\u9884\u6d4b\uff0c\u901a\u8fc7\u5143\u521d\u59cb\u5316\u3001\u6b8b\u5dee\u68c0\u6d4b\u548c\u5728\u7ebf\u66f4\u65b0\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u6539\u5584\u5207\u6362\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u79fb\u52a8\u6027\u9ad8\u5ea6\u975e\u5e73\u7a33\uff0c\u5b58\u5728\u7a81\u7136\u8f6c\u5411\u3001\u901f\u5ea6\u53d8\u5316\u548c\u4e0d\u53ef\u9884\u6d4b\u7528\u6237\u884c\u4e3a\uff0c\u5bfc\u81f4\u4f20\u7edf\u9884\u6d4b\u5668\u6f02\u79fb\uff0c\u5f15\u53d1\u5207\u6362\u65f6\u673a\u9519\u8bef\u6216\u5931\u8d25\u3002", "method": "\u57fa\u4e8eGRU\u7684\u9884\u6d4b\u5668\uff0c\u7ed3\u5408Reptile\u5143\u521d\u59cb\u5316\u8fdb\u884c\u5feb\u901f\u5c11\u6837\u672c\u9002\u5e94\uff0c\u4ee5\u53caEWMA\u6b8b\u5dee\u68c0\u6d4b\u5668\u5728\u6f02\u79fb\u53d1\u751f\u65f6\u89e6\u53d1\u7d27\u51d1\u7684\u5728\u7ebf\u66f4\u65b0\u3002", "result": "\u5728GeoLife\u548cDeepMIMO\u6570\u636e\u96c6\u4e0a\uff0c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u52304.46m ADE\u548c7.79m FDE\uff0c10\u6837\u672c\u5c11\u6837\u672c\u4e0bADE\u63d0\u5347\u81f33.71m\uff0c\u4ece\u7a81\u7136\u6f02\u79fb\u4e2d\u6062\u590d\u901f\u5ea6\u6bd4\u79bb\u7ebfGRU\u5feb2-3\u500d\u3002\u5e94\u7528\u4e8e\u5207\u6362\u9884\u6d4b\u65f6\uff0cF1\u63d0\u5347\u81f30.83\uff0cAUROC\u8fbe0.90\uff0c\u663e\u8457\u51cf\u5c11\u5207\u6362\u5931\u8d25\u548c\u4e52\u4e53\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\uff08128k\u53c2\u6570\uff09\uff0c\u9002\u7528\u4e8e5G/6G\u7cfb\u7edf\u8fb9\u7f18\u90e8\u7f72\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u79fb\u52a8\u6027\u975e\u5e73\u7a33\u6027\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u5207\u6362\u6027\u80fd\u3002"}}
{"id": "2512.12211", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12211", "abs": "https://arxiv.org/abs/2512.12211", "authors": ["Longchao Da", "David Isele", "Hua Wei", "Manish Saroya"], "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving", "comment": "9 Pages, 8 Figures", "summary": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u9002\u5e94\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u5668\u6027\u80fd\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e24\u4e2a\u7ef4\u5ea6\u52a8\u6001\u8bc4\u4f30\uff0c\u6bd4\u4f20\u7edf\u8bef\u5dee\u6307\u6807\u66f4\u80fd\u53cd\u6620\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u7684\u5b9e\u9645\u8d21\u732e\u3002", "motivation": "\u5f53\u524d\u8f68\u8ff9\u9884\u6d4b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56ADE\u3001FDE\u7b49\u8bef\u5dee\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u53ea\u5173\u6ce8\u4e8b\u540e\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u3002\u9ad8\u8d28\u91cf\u7684\u9884\u6d4b\u5668\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u5e94\u6355\u6349\u5468\u56f4\u667a\u80fd\u4f53\u6240\u6709\u53ef\u80fd\u7684\u8fd0\u52a8\u65b9\u5411\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8c28\u614e\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u7ba1\u9053\uff0c\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\u81ea\u9002\u5e94\u8bc4\u4f30\u9884\u6d4b\u5668\u6027\u80fd\uff1a1) \u51c6\u786e\u6027\uff1b2) \u591a\u6837\u6027\u3002\u6839\u636e\u9a7e\u9a76\u573a\u666f\u7684\u5173\u952e\u7a0b\u5ea6\uff0c\u52a8\u6001\u7ed3\u5408\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u751f\u6210\u9884\u6d4b\u5668\u6027\u80fd\u7684\u6700\u7ec8\u8bc4\u5206\u3002", "result": "\u5728\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u8bc4\u4f30\u7ba1\u9053\u6bd4\u4f20\u7edf\u6307\u6807\u63d0\u4f9b\u66f4\u5408\u7406\u7684\u8bc4\u4f30\uff0c\u80fd\u66f4\u597d\u5730\u53cd\u6620\u9884\u6d4b\u5668\u8bc4\u4f30\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u8bc4\u4f30\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u9009\u62e9\u6700\u80fd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u7684\u9884\u6d4b\u5668\uff0c\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u540c\u65f6\u8003\u8651\u9884\u6d4b\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.12337", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12337", "abs": "https://arxiv.org/abs/2512.12337", "authors": ["Yushen Fang", "Jianjun Li", "Mingqian Ding", "Chang Liu", "Xinchi Zou", "Wenqi Yang"], "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema", "comment": null, "summary": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.", "AI": {"tldr": "\u63d0\u51faSCIR\u6846\u67b6\u548cMBSC\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u6821\u6b63\u8fed\u4ee3\u4f18\u5316\u964d\u4f4eLLM\u4fe1\u606f\u62bd\u53d6\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9ad8\u8bad\u7ec3\u6210\u672c\u548c\u96be\u4ee5\u4e0eLLM\u504f\u597d\u5bf9\u9f50", "method": "\u63d0\u51fa\u81ea\u6821\u6b63\u8fed\u4ee3\u4f18\u5316\uff08SCIR\uff09\u6846\u67b6\uff0c\u5305\u542b\u53cc\u8def\u5f84\u81ea\u6821\u6b63\u6a21\u5757\u548c\u53cd\u9988\u9a71\u52a8\u4f18\u5316\uff1b\u521b\u5efa\u591a\u4efb\u52a1\u53cc\u8bed\u81ea\u6821\u6b63\uff08MBSC\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07+\u6761\u76ee\uff0c\u95f4\u63a5\u84b8\u998fGPT-4\u80fd\u529b", "result": "\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u7cfb\u62bd\u53d6\u548c\u4e8b\u4ef6\u62bd\u53d6\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0cspan-based Micro-F1\u5e73\u5747\u63d0\u53475.27%\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e87%", "conclusion": "SCIR\u6846\u67b6\u589e\u5f3a\u4e86\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u8f7b\u91cf\u9ad8\u6548\u7684\u4fe1\u606f\u62bd\u53d6\u8303\u5f0f\u94fa\u5e73\u9053\u8def"}}
{"id": "2512.11856", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11856", "abs": "https://arxiv.org/abs/2512.11856", "authors": ["Ao Zhou", "Jianlei Yang", "Tong Qiao", "Yingjie Qi", "Zhi Yang", "Weisheng Zhao", "Chunming Hu"], "title": "GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search", "comment": "accepted by IEEE Transactions on Computers", "summary": "Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.", "AI": {"tldr": "GCoDE\uff1a\u9996\u4e2a\u9762\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\u8bbe\u5907-\u8fb9\u7f18\u534f\u540c\u63a8\u7406\u7684\u81ea\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u67b6\u6784-\u6620\u5c04\u534f\u540c\u8bbe\u8ba1\u548c\u80fd\u91cf\u611f\u77e5\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72", "motivation": "GNN\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u786c\u4ef6\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0c\u73b0\u6709\u8bbe\u5907-\u8fb9\u7f18\u534f\u540c\u63a8\u7406\u7814\u7a76\u4e0d\u8db3\uff0c\u4f20\u7edf\u6a21\u578b\u5212\u5206\u65b9\u6cd5\u5bf9GNN\u6548\u679c\u4e0d\u4f73", "method": "\u63d0\u51faGCoDE\u6846\u67b6\uff0c\u5c06\u8bbe\u5907\u901a\u4fe1\u8fc7\u7a0b\u62bd\u8c61\u4e3a\u663e\u5f0f\u64cd\u4f5c\uff0c\u5728\u7edf\u4e00\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u8054\u5408\u4f18\u5316\u67b6\u6784\u548c\u6620\u5c04\u65b9\u6848\uff1b\u5f15\u5165\u80fd\u91cf\u9884\u6d4b\u65b9\u6cd5\u63d0\u9ad8\u8bc4\u4f30\u7cbe\u5ea6\uff1b\u91c7\u7528\u7ea6\u675f\u968f\u673a\u641c\u7d22\u7b56\u7565\u5bfb\u627e\u6700\u4f18\u89e3", "result": "\u5b9e\u9a8c\u8868\u660eGCoDE\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6700\u9ad8\u53ef\u5b9e\u73b044.9\u500d\u52a0\u901f\u548c98.2%\u80fd\u8017\u964d\u4f4e\uff0c\u57281.5\u5c0f\u65f6\u5185\u627e\u5230\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u6700\u4f18\u89e3", "conclusion": "GCoDE\u662f\u9996\u4e2aGNN\u8bbe\u5907-\u8fb9\u7f18\u534f\u540c\u63a8\u7406\u81ea\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u548c\u7cfb\u7edf\u6027\u80fd\u611f\u77e5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u573a\u666f\u4e0bGNN\u63a8\u7406\u7684\u6548\u7387\u548c\u80fd\u8017\u95ee\u9898"}}
{"id": "2512.12544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12544", "abs": "https://arxiv.org/abs/2512.12544", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Wanhao Yu", "Zhankai Ye", "Dawei Xiang", "Ting Hua", "Xin Liu", "Shangqian Gao", "Tingting Yu"], "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks", "comment": null, "summary": "Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.", "AI": {"tldr": "HyperEdit\uff1a\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\u548c\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff0c\u7528\u4e8e\u6307\u4ee4\u9a71\u52a8\u7684\u6587\u672c\u7f16\u8f91\u4efb\u52a1\uff0c\u5728\u4fee\u6539\u533a\u57df\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53479%-30% BLEU\u5206\u6570", "motivation": "\u6307\u4ee4\u9a71\u52a8\u7684\u6587\u672c\u7f16\u8f91\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\uff08\u5982\u4ee3\u7801\u7f16\u8f91\u5668Cursor\uff09\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u7f16\u8f91\u4efb\u52a1\u9700\u8981\u5fe0\u5b9e\u6267\u884c\u7528\u6237\u6307\u4ee4\u540c\u65f6\u4fdd\u7559\u672a\u4fee\u6539\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u7f16\u8f91\u89c6\u4e3a\u901a\u7528\u6587\u672c\u751f\u6210\uff0c\u5bfc\u81f4\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u96be\u4ee5\u5fe0\u5b9e\u5bf9\u9f50\u591a\u6837\u7528\u6237\u610f\u56fe\uff0c\u4ee5\u53ca\u7ecf\u5e38\u8fc7\u5ea6\u7f16\u8f91\u672a\u6539\u53d8\u533a\u57df\u3002", "method": "\u63d0\u51faHyperEdit\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\uff0c\u751f\u6210\u8bf7\u6c42\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u9488\u5bf9\u6bcf\u4e2a\u6307\u4ee4\u5b9a\u5236\u7f16\u8f91\u7b56\u7565\uff1b2\uff09\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff0c\u5c06\u76d1\u7763\u805a\u7126\u5728\u4fee\u6539\u7684\u6587\u672c\u7247\u6bb5\u4e0a\uff0c\u9632\u6b62\u8fc7\u5ea6\u7f16\u8f91\u540c\u65f6\u786e\u4fdd\u7cbe\u786e\u7684\u6700\u5c0f\u5316\u4fee\u6539\u3002", "result": "HyperEdit\u5728\u4fee\u6539\u533a\u57df\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5bf9\u63d0\u5347\u4e869%-30%\u7684BLEU\u5206\u6570\uff0c\u5c3d\u7ba1\u53ea\u4f7f\u7528\u4e8630\u4ebf\u53c2\u6570\u3002", "conclusion": "HyperEdit\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u548c\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u9a71\u52a8\u6587\u672c\u7f16\u8f91\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u8d28\u91cf\u548c\u5fe0\u5b9e\u5ea6\u3002"}}
{"id": "2512.12427", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12427", "abs": "https://arxiv.org/abs/2512.12427", "authors": ["Rudolf Reiter", "Chao Qin", "Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models", "comment": null, "summary": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.", "AI": {"tldr": "Unique\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00MPC\u6846\u67b6\uff0c\u901a\u8fc7\u7ea7\u8054\u4e0d\u540c\u4fdd\u771f\u5ea6\u6a21\u578b\uff08\u77ed\u65f6\u9ad8\u4fdd\u771f+\u957f\u65f6\u4f4e\u4fdd\u771f\uff09\u6765\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u540c\u65f6\u9700\u8981\u5373\u65f6\u53cd\u5e94\u548c\u957f\u65f6\u89c4\u5212\u7684\u77db\u76fe\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4efb\u52a1\u9700\u8981\u540c\u65f6\u5177\u5907\u5373\u65f6\u53cd\u5e94\u80fd\u529b\u548c\u957f\u65f6\u89c4\u5212\u80fd\u529b\u3002\u9ad8\u4fdd\u771f\u6a21\u578b\u63a7\u5236\u7cbe\u786e\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u65e0\u6cd5\u7528\u4e8e\u957f\u65f6\u89c4\u5212\uff1b\u4f4e\u4fdd\u771f\u6a21\u578b\u53ef\u6269\u5c55\u4f46\u95ed\u73af\u6027\u80fd\u5dee\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "1. \u7edf\u4e00MPC\u6846\u67b6\uff1a\u5728\u5355\u4e2a\u4f18\u5316\u4e2d\u7ea7\u8054\u4e0d\u540c\u4fdd\u771f\u5ea6\u6a21\u578b\uff08\u77ed\u65f6\u9ad8\u4fdd\u771f\u6a21\u578b\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\uff0c\u957f\u65f6\u4f4e\u4fdd\u771f\u6a21\u578b\u7528\u4e8e\u89c4\u5212\uff09\n2. \u8de8\u65f6\u57df\u6210\u672c\u5bf9\u9f50\u548c\u7ea6\u675f\u8bbe\u8ba1\uff1a\u5305\u62ec\u53ef\u884c\u6027\u4fdd\u6301\u7684\u63a8\u529b\u548c\u4f53\u901f\u7387\u7ea6\u675f\u3001\u72b6\u6001\u5339\u914d\u7ea6\u675f\u3001\u63a8\u529b\u8bf1\u5bfc\u52a0\u901f\u5ea6\u548c\u6025\u52a8-\u4f53\u901f\u7387\u5173\u7cfb\u7ea6\u675f\n3. 3D\u6e10\u8fdb\u5e73\u6ed1\u8c03\u5ea6\uff1a\u6cbf\u65f6\u57df\u53d8\u5f62\u57fa\u4e8e\u8303\u6570\u7684\u969c\u788d\u7269\u4ee5\u907f\u514d\u5c40\u90e8\u6781\u5c0f\u503c\n4. \u5e76\u884c\u968f\u673a\u521d\u59cb\u5316MPC\u6c42\u89e3\u5668\uff1a\u5728\u957f\u4f4e\u4fdd\u771f\u65f6\u57df\u4e0a\u53d1\u73b0\u66f4\u4f4e\u6210\u672c\u7684\u5c40\u90e8\u6781\u5c0f\u503c", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u98de\u884c\u4e2d\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cUnique\u76f8\u6bd4\u6807\u51c6MPC\u548c\u5206\u5c42\u89c4\u5212\u5668-\u8ddf\u8e2a\u5668\u57fa\u7ebf\uff0c\u5c06\u95ed\u73af\u4f4d\u7f6e\u6216\u901f\u5ea6\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe75%\u3002\u6d88\u878d\u7814\u7a76\u548c\u5e15\u7d2f\u6258\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u65f6\u57df\u53d8\u5316\u3001\u7ea6\u675f\u8fd1\u4f3c\u548c\u5e73\u6ed1\u8c03\u5ea6\u4e0b\u7684\u9c81\u68d2\u6027\u589e\u76ca\u3002", "conclusion": "Unique\u901a\u8fc7\u7ea7\u8054\u4e0d\u540c\u4fdd\u771f\u5ea6\u6a21\u578b\u7684\u7edf\u4e00MPC\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4efb\u52a1\u4e2d\u5373\u65f6\u53cd\u5e94\u548c\u957f\u65f6\u89c4\u5212\u7684\u77db\u76fe\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u590d\u6742\u7a7a\u4e2d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12012", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12012", "abs": "https://arxiv.org/abs/2512.12012", "authors": ["Antonio Guillen-Perez"], "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus", "comment": null, "summary": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.", "AI": {"tldr": "Semantic-Drive\uff1a\u4e00\u4e2a\u672c\u5730\u4f18\u5148\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u65e5\u5fd7\u4e2d\u9ad8\u6548\u6316\u6398\u957f\u5c3e\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u548c\u63a8\u7406VLM\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f00\u53d1\u9762\u4e34\u957f\u5c3e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u74f6\u9888\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u3001\u9690\u79c1\u4fb5\u72af\u548c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u672c\u5730\u5316\u3001\u7cbe\u786e\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u8bed\u4e49\u6570\u636e\u6316\u6398\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u901a\u8fc7\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u8fdb\u884c\u7b26\u53f7\u63a5\u5730\u4ee5\u951a\u5b9a\u6ce8\u610f\u529b\uff1b2) \u901a\u8fc7\u63a8\u7406VLM\u8fdb\u884c\u8ba4\u77e5\u573a\u666f\u5206\u6790\u3002\u4f7f\u7528\"\u7cfb\u7edf2\"\u63a8\u7406\u65f6\u5bf9\u9f50\u7b56\u7565\u548c\u591a\u6a21\u578b\"\u6cd5\u5b98-\u4fa6\u5bdf\u5458\"\u5171\u8bc6\u673a\u5236\u6765\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4CLIP\u76840.475\u53ec\u56de\u7387\uff0cSemantic-Drive\u8fbe\u52300.966\u53ec\u56de\u7387\uff0c\u98ce\u9669\u8bc4\u4f30\u8bef\u5dee\u51cf\u5c1140%\uff0c\u7cfb\u7edf\u53ef\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684\u4e91\u7aef\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "Semantic-Drive\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u6316\u6398\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u5c3e\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u53d1\u5c55\u3002"}}
{"id": "2512.12443", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12443", "abs": "https://arxiv.org/abs/2512.12443", "authors": ["Akhmadillo Mamirov", "Faiaz Azmain", "Hanyu Wang"], "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline", "comment": null, "summary": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.", "AI": {"tldr": "\u7814\u7a76\u8005\u5206\u6790\u4e86AI\u6a21\u578b\u6587\u6863\u7684\u788e\u7247\u5316\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u52a0\u6743\u900f\u660e\u5ea6\u6846\u67b6\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u53d1\u73b0\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u5408\u89c4\u7387\u7ea680%\uff0c\u4f46\u5927\u591a\u6570\u63d0\u4f9b\u5546\u4f4e\u4e8e60%\uff0c\u5b89\u5168\u5173\u952e\u7c7b\u522b\u5b58\u5728\u6700\u5927\u62ab\u9732\u7f3a\u53e3\u3002", "motivation": "AI\u6a21\u578b\u6587\u6863\u5206\u6563\u5728\u4e0d\u540c\u5e73\u53f0\u4e14\u7ed3\u6784\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u653f\u7b56\u5236\u5b9a\u8005\u3001\u5ba1\u8ba1\u8005\u548c\u7528\u6237\u53ef\u9760\u8bc4\u4f30\u5b89\u5168\u58f0\u660e\u3001\u6570\u636e\u6765\u6e90\u548c\u7248\u672c\u53d8\u66f4\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u89e3\u51b3\u900f\u660e\u5ea6\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e865\u4e2a\u524d\u6cbf\u6a21\u578b\u548c100\u4e2aHugging Face\u6a21\u578b\u5361\uff0c\u8bc6\u522b\u51fa947\u4e2a\u72ec\u7279\u7ae0\u8282\u540d\u79f0\uff1b\u57fa\u4e8e\u6b27\u76dfAI\u6cd5\u6848\u548c\u65af\u5766\u798f\u900f\u660e\u5ea6\u6307\u6570\u5f00\u53d1\u4e868\u4e2a\u7ae0\u828223\u4e2a\u5b50\u7ae0\u8282\u7684\u52a0\u6743\u900f\u660e\u5ea6\u6846\u67b6\uff1b\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u4ece\u516c\u5f00\u6e90\u63d0\u53d6\u6587\u6863\u5e76\u901a\u8fc7LLM\u5171\u8bc6\u8bc4\u5206\u5b8c\u6574\u6027\u3002", "result": "\u8bc4\u4f3050\u4e2a\u6a21\u578b\uff08\u89c6\u89c9\u3001\u591a\u6a21\u6001\u3001\u5f00\u6e90\u548c\u95ed\u6e90\uff09\u603b\u6210\u672c\u4f4e\u4e8e3\u7f8e\u5143\uff1b\u524d\u6cbf\u5b9e\u9a8c\u5ba4\uff08xAI\u3001\u5fae\u8f6f\u3001Anthropic\uff09\u5408\u89c4\u7387\u7ea680%\uff0c\u5927\u591a\u6570\u63d0\u4f9b\u5546\u4f4e\u4e8e60%\uff1b\u5b89\u5168\u5173\u952e\u7c7b\u522b\u7f3a\u9677\u6700\u5927\uff1a\u6b3a\u9a97\u884c\u4e3a\u3001\u5e7b\u89c9\u548c\u513f\u7ae5\u5b89\u5168\u8bc4\u4f30\u5206\u522b\u635f\u5931148\u3001124\u548c116\u603b\u5206\u3002", "conclusion": "AI\u6a21\u578b\u6587\u6863\u5b58\u5728\u7cfb\u7edf\u6027\u900f\u660e\u5ea6\u7f3a\u53e3\uff0c\u7279\u522b\u662f\u5b89\u5168\u5173\u952e\u9886\u57df\uff1b\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u80fd\u9ad8\u6548\u8bc6\u522b\u62ab\u9732\u4e0d\u8db3\uff1b\u9700\u8981\u6807\u51c6\u5316\u6587\u6863\u683c\u5f0f\u548c\u5f3a\u5236\u5b89\u5168\u62ab\u9732\u8981\u6c42\u6765\u6539\u5584\u900f\u660e\u5ea6\u3002"}}
{"id": "2512.12056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12056", "abs": "https://arxiv.org/abs/2512.12056", "authors": ["Maria Rodriguez", "Minh-Tan Pham", "Martin Sudmanns", "Quentin Poterek", "Oscar Narvaez"], "title": "Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management", "comment": "5 pages, IGARSS 2025", "summary": "After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u63d0\u5347\u706b\u707e\u540e\u70e7\u6bc1\u533a\u57df\uff08BA\uff09\u5212\u5206\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u9488\u5bf9SPOT-6/7\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\uff0c\u8bc4\u4f30\u4e86U-Net\u548cSegFormer\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u571f\u5730\u8986\u76d6\u6570\u636e\u8f85\u52a9\u4efb\u52a1\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u70e7\u6bc1\u533a\u57df\u5212\u5206\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5904\u7406\u707e\u540e\u9065\u611f\u5f71\u50cf\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u5e94\u6025\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u8bc1\u6027\u80fd\u53c8\u80fd\u63d0\u9ad8\u6548\u7387\u7684\u70e7\u6bc1\u533a\u57df\u5212\u5206\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u9488\u5bf9SPOT-6/7\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u3002\u5b9e\u9a8c\u8bc4\u4f30U-Net\u548cSegFormer\u6a21\u578b\uff0c\u4f7f\u7528Dice\u5206\u6570\u3001\u4ea4\u5e76\u6bd4\u548c\u63a8\u7406\u65f6\u95f4\u4f5c\u4e3a\u8bc4\u4ef7\u6307\u6807\u3002\u63a2\u7d22\u4e86\u5c06\u571f\u5730\u8986\u76d6\u6570\u636e\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u4ee5\u53ca\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\u3002", "result": "U-Net\u548cSegFormer\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46SegFormer\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u5e94\u6025\u573a\u666f\u4e2d\u5b9e\u7528\u6027\u53d7\u9650\u3002\u52a0\u5165\u571f\u5730\u8986\u76d6\u8f85\u52a9\u4efb\u52a1\u80fd\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u3002\u6d4b\u8bd5\u65f6\u589e\u5f3a\u80fd\u63d0\u5347\u5212\u5206\u6027\u80fd\u4f46\u589e\u52a0\u63a8\u7406\u65f6\u95f4\uff0c\u53ef\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u7b49\u4f18\u5316\u65b9\u6cd5\u7f13\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5e94\u6025\u7ba1\u7406\u573a\u666f\u4e0b\u7684\u70e7\u6bc1\u533a\u57df\u5212\u5206\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u9700\u6c42\u3002U-Net\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\uff0c\u8f85\u52a9\u4efb\u52a1\u548c\u4f18\u5316\u6280\u672f\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.12552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12552", "abs": "https://arxiv.org/abs/2512.12552", "authors": ["Jifei Liu", "Zhi Chen", "Yuanguang Zhong"], "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms", "comment": null, "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u7b49\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u4f1a\u590d\u5236\u5e76\u653e\u5927\u4eba\u7c7b\u8ba4\u77e5\u504f\u5dee\uff0cGPT-4\u7b49\u590d\u6742\u6a21\u578b\u56e0\"\u8fc7\u5ea6\u601d\u8003\"\u8868\u73b0\u51fa\u6700\u5927\u975e\u7406\u6027\uff0c\u800c\u6548\u7387\u4f18\u5316\u7684GPT-4o\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\u3002", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u5546\u4e1a\u51b3\u7b56\uff0c\u4f46\u5176\u590d\u5236\u548c\u653e\u5927\u4eba\u7c7b\u8ba4\u77e5\u504f\u5dee\u7684\u98ce\u9669\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u7b49\u9ad8\u98ce\u9669\u8fd0\u8425\u73af\u5883\u4e2d\u3002\u7814\u7a76\u65e8\u5728\u8bc6\u522bLLMs\u8ba4\u77e5\u504f\u5dee\u7684\u6027\u8d28\u548c\u6765\u6e90\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u591a\u8f6e\u5b9e\u9a8c\uff0c\u5728\u7ecf\u5178\u62a5\u7ae5\u95ee\u9898\u4e2d\u6d4b\u8bd5GPT-4\u3001GPT-4o\u548cLLaMA-8B\u6a21\u578b\uff0c\u68c0\u9a8c\u4e94\u79cd\u5df2\u786e\u7acb\u7684\u51b3\u7b56\u504f\u5dee\u3002\u5206\u6790\u6a21\u578b\u5728\u63d0\u4f9b\u6700\u4f18\u516c\u5f0f\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u533a\u5206\u77e5\u8bc6\u5dee\u8ddd\u548c\u67b6\u6784\u7ea6\u675f\u3002", "result": "LLMs\u4e00\u81f4\u590d\u5236\u4e86\u7ecf\u5178\u7684\"\u8fc7\u4f4e/\u8fc7\u9ad8\"\u8ba2\u8d2d\u504f\u5dee\uff0c\u5e76\u663e\u8457\u653e\u5927\u4e86\u9700\u6c42\u8ffd\u9010\u7b49\u884c\u4e3a\u3002\u53d1\u73b0\"\u667a\u80fd\u6096\u8bba\"\uff1a\u66f4\u590d\u6742\u7684GPT-4\u56e0\u8fc7\u5ea6\u601d\u8003\u8868\u73b0\u51fa\u6700\u5927\u975e\u7406\u6027\uff0c\u800c\u6548\u7387\u4f18\u5316\u7684GPT-4o\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\u3002\u5373\u4f7f\u5728\u63d0\u4f9b\u6700\u4f18\u516c\u5f0f\u65f6\u504f\u5dee\u4ecd\u5b58\u5728\uff0c\u8868\u660e\u8fd9\u4e9b\u504f\u5dee\u6e90\u4e8e\u67b6\u6784\u7ea6\u675f\u800c\u975e\u77e5\u8bc6\u5dee\u8ddd\u3002", "conclusion": "\u7ba1\u7406\u8005\u5e94\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9009\u62e9\u6a21\u578b\uff0c\u6548\u7387\u4f18\u5316\u6a21\u578b\u5728\u67d0\u4e9b\u4f18\u5316\u95ee\u9898\u4e0a\u53ef\u80fd\u4f18\u4e8e\u590d\u6742\u6a21\u578b\u3002LLMs\u663e\u8457\u653e\u5927\u504f\u5dee\u51f8\u663e\u4e86\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u9700\u8981\u5f3a\u6709\u529b\u7684\u4eba\u5de5\u76d1\u7763\u3002\u8bbe\u8ba1\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u793a\u662f\u7ea6\u675f\u6a21\u578b\u542f\u53d1\u5f0f\u503e\u5411\u3001\u63d0\u9ad8AI\u8f85\u52a9\u51b3\u7b56\u53ef\u9760\u6027\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2512.12692", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12692", "abs": "https://arxiv.org/abs/2512.12692", "authors": ["Mahir Labib Dihan", "Tanzima Hashem", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment", "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/", "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.", "AI": {"tldr": "WebOperator\uff1a\u4e00\u4e2a\u7ed3\u5408\u5b89\u5168\u56de\u6eaf\u548c\u6218\u7565\u63a2\u7d22\u7684\u6811\u641c\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684Web\u73af\u5883\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728Web\u73af\u5883\u4e2d\u91c7\u7528\u8d2a\u5a6a\u7684\u9010\u6b65\u884c\u52a8\u7b56\u7565\uff0c\u7f3a\u4e4f\u957f\u671f\u89c4\u5212\u80fd\u529b\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9519\u8bef\u6216\u63a2\u7d22\u66ff\u4ee3\u8def\u5f84\u3002\u6811\u641c\u7d22\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u63a2\u7d22\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u5b89\u5168\u56de\u6eaf\u673a\u5236\uff0c\u4e14\u5047\u8bbe\u6240\u6709\u52a8\u4f5c\u90fd\u53ef\u9006\uff0c\u8fd9\u5728\u73b0\u5b9e\u7684Web\u4efb\u52a1\u4e2d\u4e0d\u6210\u7acb\u3002", "method": "WebOperator\u6846\u67b6\u5305\u542b\uff1a1\uff09\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7b56\u7565\uff0c\u6839\u636e\u5956\u52b1\u4f30\u8ba1\u548c\u5b89\u5168\u6027\u5bf9\u52a8\u4f5c\u6392\u5e8f\uff1b2\uff09\u7a33\u5065\u7684\u56de\u6eaf\u673a\u5236\uff0c\u5728\u91cd\u653e\u8def\u5f84\u524d\u9a8c\u8bc1\u53ef\u884c\u6027\u4ee5\u9632\u6b62\u526f\u4f5c\u7528\uff1b3\uff09\u4ece\u591a\u4e2a\u4e0d\u540c\u63a8\u7406\u4e0a\u4e0b\u6587\u751f\u6210\u52a8\u4f5c\u5019\u9009\uff0c\u786e\u4fdd\u63a2\u7d22\u591a\u6837\u6027\uff1b4\uff09\u901a\u8fc7\u9884\u6267\u884c\u8fc7\u6ee4\u65e0\u6548\u52a8\u4f5c\u548c\u5408\u5e76\u8bed\u4e49\u7b49\u4ef7\u52a8\u4f5c\u6765\u4f18\u5316\u52a8\u4f5c\u96c6\u3002", "result": "\u5728WebArena\u548cWebVoyager\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWebOperator\u5728WebArena\u4e0a\u4f7f\u7528gpt-4o\u8fbe\u5230\u4e8654.6%\u7684\u6700\u5148\u8fdb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u6218\u7565\u8fdc\u89c1\u4e0e\u5b89\u5168\u6267\u884c\u76f8\u7ed3\u5408\u7684\u5173\u952e\u4f18\u52bf\u3002", "conclusion": "WebOperator\u901a\u8fc7\u96c6\u6210\u5b89\u5168\u56de\u6eaf\u548c\u6218\u7565\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u5bdfWeb\u73af\u5883\u4e2d\u7684\u957f\u671f\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2512.12142", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.12142", "abs": "https://arxiv.org/abs/2512.12142", "authors": ["Bj\u00f6rn L\u00fctjens", "Patrick Alexander", "Raf Antwerpen", "Til Widmann", "Guido Cervone", "Marco Tedesco"], "title": "MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater", "comment": null, "summary": "The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as \"ground truth\", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.", "AI": {"tldr": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u878d\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\uff0c\u751f\u6210\u683c\u9675\u5170\u51b0\u76d6\u6bcf\u65e5100\u7c73\u5206\u8fa8\u7387\u7684\u5730\u8868\u878d\u6c34\u5206\u5e03\u56fe\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u7cbe\u5ea6\u663e\u8457\u63d0\u5347", "motivation": "\u683c\u9675\u5170\u51b0\u76d6\u52a0\u901f\u878d\u5316\u8fc7\u7a0b\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u4e14\u96be\u4ee5\u6d4b\u91cf\uff0c\u73b0\u6709\u878d\u6c34\u5206\u5e03\u56fe\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u5b58\u5728\u6743\u8861\uff0c\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u878d\u5408\u533a\u57df\u6c14\u5019\u6a21\u578b\uff08RCM\uff09\u3001\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u3001\u88ab\u52a8\u5fae\u6ce2\uff08PMW\uff09\u548c\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u6570\u636e\uff0c\u5bf9Helheim\u51b0\u5ddd2017-2023\u5e74\u6570\u636e\u8fdb\u884c\u65f6\u7a7a\u964d\u5c3a\u5ea6\u5904\u7406", "result": "\u878d\u5408\u6240\u6709\u6570\u636e\u6d41\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7cbe\u5ea6\u8fbe95%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u533a\u57df\u6c14\u5019\u6a21\u578b\uff0883%\uff09\u6216\u88ab\u52a8\u5fae\u6ce2\u89c2\u6d4b\uff0872%\uff09\u7684\u975e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff1b\u540c\u65f6\u53d1\u5e03\u4e86MeltwaterBench\u57fa\u51c6\u6570\u636e\u96c6", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u878d\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\u80fd\u6709\u6548\u751f\u6210\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u878d\u6c34\u5206\u5e03\u56fe\uff0c\u4e3a\u7406\u89e3\u51b0\u76d6\u878d\u5316\u8fc7\u7a0b\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u4fc3\u8fdb\u66f4\u590d\u6742\u964d\u5c3a\u5ea6\u65b9\u6cd5\u6bd4\u8f83"}}
{"id": "2512.12206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12206", "abs": "https://arxiv.org/abs/2512.12206", "authors": ["Jeongjun Park", "Sunwook Hwang", "Hyeonho Noh", "Jin Mo Yang", "Hyun Jong Yang", "Saewoong Bahk"], "title": "ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB", "comment": null, "summary": "Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.\n  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.\n  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.", "AI": {"tldr": "\u63d0\u51faISA-ViT\u6846\u67b6\u548cALERT\u6570\u636e\u96c6\uff0c\u89e3\u51b3UWB\u96f7\u8fbe\u5728\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u548cViT\u8f93\u5165\u5c3a\u5bf8\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5206\u5fc3\u9a7e\u9a76\u662f\u5168\u7403\u81f4\u547d\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\u3002\u867d\u7136IR-UWB\u96f7\u8fbe\u5177\u6709\u6297\u5e72\u6270\u3001\u4f4e\u529f\u8017\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9eUWB\u6570\u636e\u96c6\uff0c\u4e14\u56fa\u5b9a\u8f93\u5165\u7684ViT\u96be\u4ee5\u9002\u5e94UWB\u96f7\u8fbe\u6570\u636e\u7684\u975e\u6807\u51c6\u7ef4\u5ea6\u3002", "method": "\u63d0\u51fa\u8f93\u5165\u5c3a\u5bf8\u65e0\u5173\u7684Vision Transformer\uff08ISA-ViT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u8865\u4e01\u914d\u7f6e\u548c\u5229\u7528\u9884\u8bad\u7ec3\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\uff0c\u5728\u8c03\u6574UWB\u6570\u636e\u5c3a\u5bf8\u65f6\u4fdd\u7559\u591a\u666e\u52d2\u9891\u79fb\u548c\u76f8\u4f4d\u7279\u5f81\u3002\u540c\u65f6\u63d0\u51fa\u9886\u57df\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u8ddd\u79bb\u57df\u548c\u9891\u57df\u7279\u5f81\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "result": "ISA-ViT\u5728UWB\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u4e2d\u6bd4\u73b0\u6709ViT\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534722.68%\u3002\u53d1\u5e03\u4e86\u5305\u542b10,220\u4e2a\u96f7\u8fbe\u6837\u672c\u7684ALERT\u6570\u636e\u96c6\uff0c\u6db5\u76d67\u79cd\u5206\u5fc3\u9a7e\u9a76\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00ALERT\u6570\u636e\u96c6\u548c\u8be6\u7ec6\u63cf\u8ff0\u8f93\u5165\u5c3a\u5bf8\u65e0\u5173\u7b56\u7565\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.12208", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12208", "abs": "https://arxiv.org/abs/2512.12208", "authors": ["Indranil Bhattacharjee", "Vartika Narayani Srinet", "Anirudha Bhattacharjee", "Braj Bhushan", "Bishakh Bhattacharya"], "title": "A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction", "comment": "12 pages, journal paper", "summary": "Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408CNN\u548cGCN\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc6\u522b\u81ea\u95ed\u75c7\u513f\u7ae5\u5728\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u65f6\u7684\u60c5\u7eea\u53cd\u5e94\uff0c\u4f7f\u7528\u5370\u5ea6\u9996\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002", "motivation": "\u7406\u89e3\u81ea\u95ed\u75c7\u513f\u7ae5\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u60c5\u7eea\u53cd\u5e94\u662f\u53d1\u5c55\u5fc3\u7406\u5b66\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u81ea\u95ed\u75c7\u513f\u7ae5\u60c5\u7eea\u5206\u6790\u7684\u4e13\u95e8\u7814\u7a76\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6a21\u578b\uff1a\u5fae\u8c03\u7684ResNet-50 CNN + \u4e09\u5c42GCN\uff0c\u8bad\u7ec3\u4e8e\u4eceMediaPipe FaceMesh\u63d0\u53d6\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u7279\u5f81\u3002\u91c7\u7528DeepFace\u548cFER\u6a21\u578b\u7684\u52a0\u6743\u96c6\u6210\u8fdb\u884c\u6982\u7387\u6807\u7b7e\u751f\u6210\uff0c\u901a\u8fc7KL\u6563\u5ea6\u4f18\u5316\u878d\u5408\u5d4c\u5165\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5efa\u6a21\u7ec6\u5fae\u60c5\u611f\u53cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0c\u6709\u6548\u6355\u6349\u795e\u7ecf\u591a\u6837\u6027\u513f\u7ae5\u7684\u5fae\u89c2\u60c5\u7eea\u7ebf\u7d22\uff0c\u4e3a\u4e34\u5e8a\u548c\u6cbb\u7597\u6027\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u524d\u666f\u3002", "conclusion": "\u8fd9\u662f\u5370\u5ea6\u9996\u4e2a\u9488\u5bf9\u81ea\u95ed\u75c7\u513f\u7ae5\u60c5\u7eea\u5206\u6790\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u7ba1\u9053\uff0c\u4e3a\u672a\u6765\u4e2a\u6027\u5316\u8f85\u52a9\u6280\u672f\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u586b\u8865\u4e86\u81ea\u95ed\u75c7\u7279\u5b9aHRI\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.13070", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faM-GRPO\u6846\u67b6\u548cIQR\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u89e3\u51b3\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u671f\u8bad\u7ec3\u65f6\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u5b58\u5728\"\u7b56\u7565\u5d29\u6e83\"\u95ee\u9898\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u5373\u4f7f\u589e\u52a0rollout\u6570\u91cf\u4e5f\u53ea\u80fd\u5ef6\u8fdf\u800c\u975e\u9632\u6b62\u5d29\u6e83\uff0c\u9700\u8981\u65b0\u7684\u7a33\u5b9a\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "1. M-GRPO\uff1a\u5229\u7528\u7f13\u6162\u6f14\u5316\u7684\u52a8\u91cf\u6a21\u578b\u63d0\u4f9b\u7a33\u5b9a\u8bad\u7ec3\u76ee\u6807\uff1b2. IQR\u81ea\u9002\u5e94\u8fc7\u6ee4\uff1a\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\u52a8\u6001\u4fee\u526a\u4f4e\u71b5\u8f68\u8ff9\uff0c\u4fdd\u6301\u7b56\u7565\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM-GRPO\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0cIQR\u8fc7\u6ee4\u9632\u6b62\u4e86\u8fc7\u65e9\u6536\u655b\uff0c\u4e24\u8005\u7ed3\u5408\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7M-GRPO\u6846\u67b6\u548cIQR\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5b9a\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13286", "abs": "https://arxiv.org/abs/2512.13286", "authors": ["Youssra Rebboud", "Pasquale Lisena", "Raphael Troncy"], "title": "Integrating Causal Reasoning into Automated Fact-Checking", "comment": "Extended version of the accepted ACM SAC paper", "summary": "In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4e8b\u4ef6\u5173\u7cfb\u63d0\u53d6\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u57fa\u4e8e\u89c4\u5219\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u68c0\u6d4b\u58f0\u660e\u4e0e\u8bc1\u636e\u95f4\u4e8b\u4ef6\u94fe\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u4e8b\u5b9e\u6838\u67e5\u5efa\u7acb\u9996\u4e2a\u7ec6\u7c92\u5ea6\u56e0\u679c\u4e8b\u4ef6\u5173\u7cfb\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u7f3a\u4e4f\u4e13\u95e8\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u80fd\u9519\u5931\u8bed\u4e49\u4e30\u5bcc\u7684\u53ef\u89e3\u91ca\u6027\u673a\u4f1a\u3002\u62d2\u7edd\u58f0\u660e\u7684\u4e00\u4e2a\u5e38\u89c1\u539f\u56e0\u662f\u68c0\u6d4b\u4e8b\u4ef6\u95f4\u9519\u8bef\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u7ed3\u5408\u4e8b\u4ef6\u5173\u7cfb\u63d0\u53d6\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\uff0c\u68c0\u6d4b\u58f0\u660e\u4e2d\u4e8b\u4ef6\u94fe\u4e0e\u8bc1\u636e\u4e2d\u4e8b\u4ef6\u94fe\u4e4b\u95f4\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5c06\u7ec6\u7c92\u5ea6\u56e0\u679c\u4e8b\u4ef6\u5173\u7cfb\u96c6\u6210\u5230\u4e8b\u5b9e\u6838\u67e5\u4e2d\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u7ebf\uff0c\u5e76\u589e\u5f3a\u4e86\u88c1\u51b3\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u586b\u8865\u4e86\u4e8b\u5b9e\u6838\u67e5\u4e2d\u56e0\u679c\u63a8\u7406\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6574\u5408\u56e0\u679c\u4e8b\u4ef6\u5173\u7cfb\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.12325", "categories": ["cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12325", "abs": "https://arxiv.org/abs/2512.12325", "authors": ["Shubhada Agrawal", "Aaditya Ramdas"], "title": "Eventually LIL Regret: Almost Sure $\\ln\\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data", "comment": "24 pages", "summary": "We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_\u03b1$, this regret till time $T$ is bounded by $\\ln^2(1/\u03b1)/V_T + \\ln (1/\u03b1) + \\ln \\ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\\ln(1/\u03b1) + \\ln \\ln V_T$ if $V_T \\geq \\ln(1/\u03b1)$.) If the data were stochastic, then one can show that $E_\u03b1$ has probability at least $1-\u03b1$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\\ln \\ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u7ecf\u5178\u7684Robbins\u5b50\u9ad8\u65af\u6df7\u5408\u7b56\u7565\u5728\u786e\u5b9a\u6027\u8def\u5f84\u4e0a\u6ee1\u8db3\u9057\u61be\u754c\uff0c\u4e3a\u5bf9\u6297\u6027\u5728\u7ebf\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7edf\u8ba1\u4e4b\u95f4\u67b6\u8d77\u6865\u6881", "motivation": "\u8fde\u63a5\u5bf9\u6297\u6027\u5728\u7ebf\u5b66\u4e60\uff08\u901a\u5e38\u5904\u7406\u6709\u754c\u6570\u636e\u7684\u9057\u61be\u754c\uff09\u548c\u535a\u5f08\u8bba\u7edf\u8ba1\uff08\u53ef\u4ee5\u5904\u7406\u65e0\u754c\u6570\u636e\u4f46\u9700\u8981\u968f\u673a\u6027\u5047\u8bbe\uff09\u4e24\u4e2a\u9886\u57df\uff0c\u901a\u8fc7\u6761\u4ef6\u9057\u61be\u754c\u4f5c\u4e3a\u968f\u673a\u548c\u5bf9\u6297\u6027\u6295\u6ce8\u4e4b\u95f4\u7684\u6865\u6881", "method": "\u5206\u6790\u7ecf\u5178\u7684Robbins\u5b50\u9ad8\u65af\u6df7\u5408\u7b56\u7565\uff0c\u8bc1\u660e\u5176\u5728\u81ea\u7136\"Ville\u4e8b\u4ef6\"E_\u03b1\u4e2d\u7684\u6bcf\u6761\u8def\u5f84\u4e0a\u90fd\u6ee1\u8db3\u786e\u5b9a\u6027\u9057\u61be\u754c\uff0c\u5176\u4e2dV_T\u662f\u975e\u8d1f\u975e\u9012\u51cf\u7684\u7d2f\u79ef\u65b9\u5dee\u8fc7\u7a0b", "result": "\u5728Ville\u4e8b\u4ef6E_\u03b1\u4e2d\uff0c\u9057\u61be\u754c\u4e3aln\u00b2(1/\u03b1)/V_T + ln(1/\u03b1) + ln ln V_T\uff08\u4e58\u4ee5\u901a\u7528\u5e38\u6570\uff09\uff1b\u5982\u679cV_T \u2265 ln(1/\u03b1)\uff0c\u5219\u7b80\u5316\u4e3aln(1/\u03b1) + ln ln V_T\uff1b\u5728\u6982\u7387\u4e3a1\u7684\u4e8b\u4ef6E_0\u4e2d\uff0c\u9057\u61be\u6700\u7ec8\u88abln ln V_T\u754c\u4f4f", "conclusion": "\u6761\u4ef6\u9057\u61be\u754c\u53ef\u4ee5\u4f5c\u4e3a\u968f\u673a\u548c\u5bf9\u6297\u6027\u6295\u6ce8\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u5904\u7406\u65e0\u754c\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u5bf9\u6297\u6027\u5728\u7ebf\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7edf\u8ba1\u4e24\u4e2a\u9886\u57df"}}
{"id": "2512.13478", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13478", "abs": "https://arxiv.org/abs/2512.13478", "authors": ["Kei Saito"], "title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models", "comment": "19 pages", "summary": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $\u03c1$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "AI": {"tldr": "\u63d0\u51fa\u975e\u89e3\u6790\u63a8\u7406(NRR)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5411\u91cf\u5d4c\u5165\u3001\u975e\u574d\u7f29\u6ce8\u610f\u529b\u548c\u4e0a\u4e0b\u6587\u8eab\u4efd\u8ffd\u8e2a\u6765\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u8fc7\u65e9\u8bed\u4e49\u574d\u7f29\uff0c\u5c06\u8bed\u4e49\u6b67\u4e49\u4f5c\u4e3a\u663e\u5f0f\u8868\u793a\u72b6\u6001\u800c\u975e\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u8fc7\u65e9\u8bed\u4e49\u574d\u7f29\"\u7684\u6838\u5fc3\u67b6\u6784\u9650\u5236\uff0c\u5373softmax\u7ade\u4e89\u548c\u8d2a\u5a6a\u89e3\u7801\u8feb\u4f7f\u6a21\u578b\u5728\u83b7\u5f97\u8db3\u591f\u4e0a\u4e0b\u6587\u524d\u5c31\u627f\u8bfa\u5355\u4e00\u8bed\u4e49\uff0c\u5bfc\u81f4\u63a8\u7406\u8106\u5f31\u548c\u4e0a\u4e0b\u6587\u5931\u8d25\u3002", "method": "\u5f15\u5165\u975e\u89e3\u6790\u63a8\u7406(NRR)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1)\u591a\u5411\u91cf\u5d4c\u5165\u4fdd\u6301\u6bcf\u4e2atoken\u7684\u591a\u4e2a\u53ef\u884c\u89e3\u91ca\uff1b2)\u975e\u574d\u7f29\u6ce8\u610f\u529b\u9632\u6b62\u8de8\u5c42\u7684\u8d62\u5bb6\u901a\u5403\u52a8\u6001\uff1b3)\u4e0a\u4e0b\u6587\u8eab\u4efd\u8ffd\u8e2a\u4e3a\u91cd\u590d\u5b9e\u4f53\u5206\u914d\u4e0a\u4e0b\u6587\u7279\u5b9a\u8eab\u4efd\u3002\u8fd9\u4e9b\u673a\u5236\u901a\u8fc7\u5916\u90e8\u89e3\u6790\u7b97\u5b50\u03c1\u7edf\u4e00\uff0c\u4f7f\u8bed\u4e49\u627f\u8bfa\u53d8\u5f97\u663e\u5f0f\u3001\u53ef\u63a7\u4e14\u4efb\u52a1\u76f8\u5173\u3002", "result": "\u5408\u6210\u8bc4\u4f30\u663e\u793aNRR\u80fd\u6709\u6548\u4fdd\u6301\u6b67\u4e49\u548c\u8ffd\u8e2a\u4e0a\u4e0b\u6587\uff1aCIT\u589e\u5f3a\u6a21\u578b\u5728\u5206\u5e03\u5916\u8eab\u4efd\u8f6c\u79fb\u4efb\u52a1\u4e0a\u8fbe\u523090.9%\u51c6\u786e\u7387\uff0c\u800cTransformer\u57fa\u7ebf\u4ec5\u4e3a9.1%\u3002", "conclusion": "NRR\u4e3a\u8fc7\u65e9\u8bed\u4e49\u574d\u7f29\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u6b67\u4e49\u91cd\u65b0\u5b9a\u4e49\u4e3a\u663e\u5f0f\u8868\u793a\u72b6\u6001\u800c\u975e\u6545\u969c\u6a21\u5f0f\u3002\u5173\u952e\u95ee\u9898\u4e0d\u662fAI\u662f\u5426\u5e94\u8be5\u89e3\u6790\u6b67\u4e49\uff0c\u800c\u662f\u4f55\u65f6\u3001\u5982\u4f55\u4ee5\u53ca\u5728\u8c01\u7684\u63a7\u5236\u4e0b\u8fdb\u884c\u89e3\u6790\u3002"}}
{"id": "2512.13618", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13618", "abs": "https://arxiv.org/abs/2512.13618", "authors": ["Zefang Liu", "Nam Nguyen", "Yinzhu Quan", "Austin Zhang"], "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models", "comment": null, "summary": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u4e8b\u4ef6\u5e8f\u5217\u7684\u65f6\u95f4\u6807\u8bb0\u5316\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cd\u7f16\u7801\u7b56\u7565\u5728\u4e0d\u540c\u7edf\u8ba1\u5206\u5e03\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u6700\u4f18\u7b56\u7565\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u6807\u8bb0\u5668\u4e0e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "motivation": "\u8fde\u7eed\u65f6\u95f4\u8868\u793a\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5efa\u6a21\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u7684\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002\u73b0\u6709\u7b56\u7565\u5982\u5b57\u8282\u7ea7\u8868\u793a\u6216\u65e5\u5386\u6807\u8bb0\u7b49\uff0c\u4f46\u6700\u4f18\u65b9\u6cd5\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u6570\u636e\u7684\u591a\u6837\u5316\u7edf\u8ba1\u5206\u5e03\uff08\u4ece\u5e73\u6ed1\u7684\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u5230\u79bb\u6563\u7684\u5c16\u5cf0\u6a21\u5f0f\uff09\u3002", "method": "\u6bd4\u8f83\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u65f6\u95f4\u7f16\u7801\u7b56\u7565\uff1a\u6734\u7d20\u6570\u5b57\u5b57\u7b26\u4e32\u3001\u9ad8\u7cbe\u5ea6\u5b57\u8282\u7ea7\u8868\u793a\u3001\u4eba\u7c7b\u8bed\u4e49\u65e5\u5386\u6807\u8bb0\u3001\u7ecf\u5178\u5747\u5300\u5206\u7bb1\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u6807\u91cf\u91cf\u5316\u3002\u901a\u8fc7\u5728\u4f53\u73b0\u8fd9\u4e9b\u591a\u6837\u5316\u5206\u5e03\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03LLMs\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7b56\u7565\u3002", "result": "\u5206\u6790\u8868\u660e\u6ca1\u6709\u5355\u4e00\u7b56\u7565\u662f\u666e\u904d\u6700\u4f18\u7684\uff1b\u9884\u6d4b\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6807\u8bb0\u5668\u4e0e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002\u57fa\u4e8e\u5bf9\u6570\u7684\u7b56\u7565\u5728\u504f\u659c\u5206\u5e03\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u4eba\u7c7b\u4e2d\u5fc3\u683c\u5f0f\u5728\u6df7\u5408\u6a21\u6001\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u65f6\u95f4\u6807\u8bb0\u5316\u7b56\u7565\u7684\u9009\u62e9\u5e94\u57fa\u4e8e\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u800c\u4e0d\u662f\u5bfb\u6c42\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002\u5bf9\u6570\u7b56\u7565\u9002\u5408\u504f\u659c\u5206\u5e03\uff0c\u4eba\u7c7b\u8bed\u4e49\u683c\u5f0f\u9002\u5408\u6df7\u5408\u6a21\u6001\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u65f6\u95f4\u8868\u793a\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.13655", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.13655", "abs": "https://arxiv.org/abs/2512.13655", "authors": ["Richard J. Young"], "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation", "comment": "25 pages, 6 figures, 8 tables", "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u6d88\u9664\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u62d2\u7edd\u673a\u5236\u7684\u5de5\u5177\uff08Heretic\u3001DECCP\u3001ErisForge\u3001FailSpy\uff09\u572816\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u5728\u80fd\u529b\u4fdd\u7559\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5bf9\u6d88\u9664\u5e72\u9884\u6700\u4e3a\u654f\u611f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u901a\u8fc7\u5b66\u4e60\u7684\u62d2\u7edd\u884c\u4e3a\u9632\u6b62\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u54cd\u5e94\uff0c\u4f46\u8fd9\u4e9b\u673a\u5236\u4e5f\u963b\u788d\u4e86\u8ba4\u77e5\u5efa\u6a21\u3001\u5bf9\u6297\u6d4b\u8bd5\u548c\u5b89\u5168\u5206\u6790\u7b49\u5408\u6cd5\u7814\u7a76\u5e94\u7528\u3002\u867d\u7136\u6d88\u9664\u6280\u672f\u53ef\u4ee5\u901a\u8fc7\u65b9\u5411\u6b63\u4ea4\u5316\u624b\u672f\u5f0f\u79fb\u9664\u62d2\u7edd\u8868\u793a\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u65b9\u6cd5\u7684\u76f8\u5bf9\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u8bc4\u4f30\u56db\u79cd\u6d88\u9664\u5de5\u5177\uff08Heretic\u3001DECCP\u3001ErisForge\u3001FailSpy\uff09\u572816\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff087B-14B\u53c2\u6570\uff09\u4e0a\u7684\u517c\u5bb9\u6027\u548c\u6548\u679c\u3002\u62a5\u544a\u6240\u670916\u4e2a\u6a21\u578b\u7684\u5de5\u5177\u517c\u5bb9\u6027\uff0c\u5e76\u6839\u636e\u5de5\u5177\u652f\u6301\u60c5\u51b5\u5728\u5b50\u96c6\u4e0a\u62a5\u544a\u5b9a\u91cf\u6307\u6807\u3002\u6bd4\u8f83\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u6d88\u9664\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u5728\u57fa\u51c6\u5b50\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u80fd\u529b\u4fdd\u7559\uff08\u4e09\u4e2a\u6a21\u578b\u7684\u5e73\u5747GSM8K\u53d8\u5316\uff1aErisForge -0.28\u4e2a\u767e\u5206\u70b9\uff1bDECCP -0.13\u4e2a\u767e\u5206\u70b9\uff09\u3002\u8d1d\u53f6\u65af\u4f18\u5316\u6d88\u9664\u4ea7\u751f\u53ef\u53d8\u7684\u5206\u5e03\u504f\u79fb\uff08KL\u6563\u5ea6\uff1a0.043-1.646\uff09\uff0c\u80fd\u529b\u5f71\u54cd\u56e0\u6a21\u578b\u800c\u5f02\u3002\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5bf9\u6d88\u9664\u5e72\u9884\u6700\u4e3a\u654f\u611f\uff0cGSM8K\u53d8\u5316\u8303\u56f4\u4ece+1.51\u4e2a\u767e\u5206\u70b9\u5230-18.81\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf9\u53d8\u5316-26.5%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6d88\u9664\u5de5\u5177\u9009\u62e9\u6807\u51c6\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u3002\u4e3b\u8981\u53d1\u73b0\u8868\u660e\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5bf9\u6d88\u9664\u5e72\u9884\u6700\u4e3a\u654f\u611f\uff0c\u5de5\u5177\u9009\u62e9\u548c\u6a21\u578b\u67b6\u6784\u4f1a\u663e\u8457\u5f71\u54cd\u80fd\u529b\u4fdd\u7559\u6548\u679c\u3002"}}
{"id": "2512.13030", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13030", "abs": "https://arxiv.org/abs/2512.13030", "authors": ["Hongzhe Bi", "Hengkai Tan", "Shenghao Xie", "Zeyuan Wang", "Shuhe Huang", "Haitian Liu", "Ruowen Zhao", "Yao Feng", "Chendong Xiang", "Yinze Rong", "Hongyan Zhao", "Hanyu Liu", "Zhizhong Su", "Lei Ma", "Hang Su", "Jun Zhu"], "title": "Motus: A Unified Latent Action World Model", "comment": null, "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.", "AI": {"tldr": "Motus\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408Transformer\u67b6\u6784\u6574\u5408\u7406\u89e3\u3001\u89c6\u9891\u751f\u6210\u548c\u52a8\u4f5c\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff0c\u5229\u7528\u5149\u6d41\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u7406\u89e3\u3001\u4e16\u754c\u5efa\u6a21\u548c\u63a7\u5236\u5206\u79bb\u4e3a\u5b64\u7acb\u6a21\u578b\uff0c\u8fd9\u79cd\u788e\u7247\u5316\u963b\u788d\u4e86\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u7684\u7edf\u4e00\uff0c\u4e14\u96be\u4ee5\u4ece\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u4e2d\u5b66\u4e60\u3002\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u6765\u6574\u5408\u8fd9\u4e9b\u529f\u80fd\u3002", "method": "\u63d0\u51faMotus\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff1a1) \u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\u6574\u5408\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff08\u7406\u89e3\u3001\u89c6\u9891\u751f\u6210\u3001\u52a8\u4f5c\uff09\uff1b2) \u4f7f\u7528UniDiffuser\u98ce\u683c\u8c03\u5ea6\u5668\u5b9e\u73b0\u4e0d\u540c\u5efa\u6a21\u6a21\u5f0f\u7684\u7075\u6d3b\u5207\u6362\uff1b3) \u5229\u7528\u5149\u6d41\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\uff0c\u63d0\u53d6\u50cf\u7d20\u7ea7\"delta\u52a8\u4f5c\"\uff1b4) \u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u516d\u5c42\u6570\u636e\u91d1\u5b57\u5854\u8fdb\u884c\u5927\u89c4\u6a21\u52a8\u4f5c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\uff1a\u76f8\u6bd4X-VLA\u63d0\u534715%\uff0c\u76f8\u6bd4Pi0.5\u63d0\u534745%\uff1b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff1a\u63d0\u534711-48%\u3002\u8bc1\u660e\u4e86\u7edf\u4e00\u5efa\u6a21\u6240\u6709\u529f\u80fd\u548c\u5148\u9a8c\u77e5\u8bc6\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "Motus\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\u6210\u529f\u6574\u5408\u4e86\u7406\u89e3\u3001\u4e16\u754c\u5efa\u6a21\u548c\u63a7\u5236\u529f\u80fd\uff0c\u5229\u7528\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4e30\u5bcc\u7684\u53ef\u5171\u4eab\u8fd0\u52a8\u4fe1\u606f\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u5efa\u6a21\u65b9\u6cd5\u5bf9\u673a\u5668\u4eba\u4efb\u52a1\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.12545", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.12545", "abs": "https://arxiv.org/abs/2512.12545", "authors": ["Bin Mu", "Yuxuan Chen", "Shijin Yuan", "Bo Qin", "Hao Guo"], "title": "Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model", "comment": null, "summary": "Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.", "AI": {"tldr": "TianXing-S2S\u662f\u4e00\u4e2a\u591a\u5708\u5c42\u8026\u5408\u7684\u6982\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u5168\u7403\u6b21\u5b63\u8282\u5230\u5b63\u8282\uff08S2S\uff09\u7684\u6bcf\u65e5\u96c6\u5408\u9884\u62a5\uff0c\u572845\u5929\u9884\u62a5\u4e2d\u8d85\u8d8a\u4e86ECMWF\u548cFuXi-S2S\u7cfb\u7edf\u3002", "motivation": "\u5728\u6c14\u5019\u53d8\u5316\u52a0\u5267\u7684\u80cc\u666f\u4e0b\uff0c\u51c6\u786e\u7684\u6b21\u5b63\u8282\u5230\u5b63\u8282\uff08S2S\uff09\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u5bf9\u4e8e\u8d44\u6e90\u89c4\u5212\u548c\u707e\u5bb3\u7f13\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u591a\u5708\u5c42\u76f8\u4e92\u4f5c\u7528\u548c\u5185\u5728\u7684\u5927\u6c14\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u7c7b\u9884\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "TianXing-S2S\u9996\u5148\u5c06\u591a\u6837\u7684\u591a\u5708\u5c42\u9884\u6d4b\u56e0\u5b50\u7f16\u7801\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6bcf\u65e5\u96c6\u5408\u9884\u62a5\u3002\u5728\u53bb\u566a\u5668\u4e2d\u52a0\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u7684\u65b0\u578b\u8026\u5408\u6a21\u5757\uff0c\u4ee5\u4f18\u5316\u5927\u6c14\u548c\u591a\u5708\u5c42\u8fb9\u754c\u6761\u4ef6\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u5173\u952e\u5927\u6c14\u53d8\u91cf\u4e0a\uff0cTianXing-S2S\u57281.5\u00b0\u5206\u8fa8\u7387\u768445\u5929\u65e5\u5747\u96c6\u5408\u9884\u62a5\u4e2d\uff0c\u8d85\u8d8a\u4e86\u6b27\u6d32\u4e2d\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u5fc3\uff08ECMWF\uff09S2S\u7cfb\u7edf\u548cFuXi-S2S\u3002\u8be5\u6a21\u578b\u80fd\u591f\u719f\u7ec3\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\uff08\u5982\u70ed\u6d6a\u548c\u5f02\u5e38\u964d\u6c34\uff09\uff0c\u5e76\u8bc6\u522b\u571f\u58e4\u6e7f\u5ea6\u4f5c\u4e3a\u5173\u952e\u7684\u524d\u5146\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u6a21\u578b\u80fd\u591f\u751f\u6210\u957f\u8fbe180\u5929\u7684\u7a33\u5b9a\u6eda\u52a8\u9884\u62a5\u3002", "conclusion": "TianXing-S2S\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a33\u5065\u7684S2S\u7814\u7a76\u6846\u67b6\uff0c\u4e3a\u53d8\u6696\u4e16\u754c\u4e2d\u7684\u6b21\u5b63\u8282\u5230\u5b63\u8282\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u5708\u5c42\u8026\u5408\u548c\u6269\u6563\u6a21\u578b\u5728\u6539\u8fdb\u957f\u671f\u5929\u6c14\u9884\u62a5\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.12549", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12549", "abs": "https://arxiv.org/abs/2512.12549", "authors": ["Shaif Chowdhury", "Mushfika Rahman", "Greg Hamerly"], "title": "Supervised Contrastive Frame Aggregation for Video Representation Learning", "comment": "12 pages", "summary": "We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u89c6\u9891\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u5e27\u7a7a\u95f4\u6392\u5217\u6210\u5355\u5f20\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3CNN\u9aa8\u5e72\u7f51\u7edc\uff0c\u907f\u514d\u590d\u6742\u89c6\u9891Transformer\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u89c6\u9891Transformer\u6a21\u578b\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u9884\u8bad\u7ec3CNN\u9aa8\u5e72\u7f51\u7edc\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u80fd\u6709\u6548\u5b66\u4e60\u89c6\u9891\u65f6\u5e8f\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSupervised Contrastive Frame Aggregation\u65b9\u6cd5\uff1a1) \u89c6\u9891\u5230\u56fe\u50cf\u805a\u5408\u7b56\u7565\uff0c\u5c06\u591a\u4e2a\u89c6\u9891\u5e27\u7a7a\u95f4\u6392\u5217\u6210\u5355\u5f20\u8f93\u5165\u56fe\u50cf\uff1b2) \u4f7f\u7528\u9884\u8bad\u7ec3CNN\u9aa8\u5e72\u7f51\u7edc\uff08\u5982ResNet50\uff09\uff1b3) \u8bbe\u8ba1\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u76f4\u63a5\u6bd4\u8f83\u6a21\u578b\u751f\u6210\u7684\u6210\u5bf9\u6295\u5f71\uff1b4) \u901a\u8fc7\u4e0d\u540c\u65f6\u95f4\u5e27\u91c7\u6837\u521b\u5efa\u540c\u4e00\u89c6\u9891\u7684\u591a\u4e2a\u81ea\u7136\u89c6\u56fe\u4f5c\u4e3a\u6b63\u6837\u672c\u3002", "result": "\u5728Penn Action\u6570\u636e\u96c6\u4e0a\u8fbe\u523076%\u5206\u7c7b\u51c6\u786e\u7387\uff08ViVIT\u4e3a43%\uff09\uff0c\u5728HMDB51\u6570\u636e\u96c6\u4e0a\u8fbe\u523048%\u51c6\u786e\u7387\uff08ViVIT\u4e3a37%\uff09\u3002\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u5c11\uff0c\u5728\u76d1\u7763\u548c\u81ea\u76d1\u7763\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u5b66\u4e60\u6709\u6548\u89c6\u9891\u8868\u793a\uff0c\u652f\u6301\u5206\u7c7b\u548c\u5b57\u5e55\u751f\u6210\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89c6\u9891\u5e27\u805a\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u5229\u7528\u65f6\u5e8f\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5728\u4fdd\u6301\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u89c6\u9891\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12669", "abs": "https://arxiv.org/abs/2512.12669", "authors": ["Jiawei Shen", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Guoqing Ma", "Yidan Liang", "Jingjiang Liu", "Hao Chen", "Shimin Di"], "title": "DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization", "comment": null, "summary": "Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.", "AI": {"tldr": "DynaGen\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5b9e\u4f53\u4e2d\u5fc3\u5b50\u56fe\u5904\u7406\u63d2\u503c\u4efb\u52a1\uff0c\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u5904\u7406\u5916\u63a8\u4efb\u52a1\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u63d2\u503c\u65b9\u6cd5\u4e2d\u4e0a\u4e0b\u6587\u5efa\u6a21\u6709\u9650\uff0c\u5916\u63a8\u65b9\u6cd5\u4e2d\u5b58\u5728\u8ba4\u77e5\u6cdb\u5316\u504f\u5dee\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "DynaGen\u91c7\u7528\u7edf\u4e00\u6846\u67b6\uff1a\u5bf9\u4e8e\u63d2\u503c\u4efb\u52a1\uff0c\u52a8\u6001\u6784\u5efa\u5b9e\u4f53\u4e2d\u5fc3\u5b50\u56fe\uff0c\u4f7f\u7528\u534f\u540c\u53cc\u5206\u652fGNN\u7f16\u7801\u5668\u6355\u6349\u6f14\u5316\u7ed3\u6784\u4e0a\u4e0b\u6587\uff1b\u5bf9\u4e8e\u5916\u63a8\u4efb\u52a1\uff0c\u5e94\u7528\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\uff0c\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u5e95\u5c42\u6f14\u5316\u539f\u7406\u800c\u975e\u8868\u9762\u6a21\u5f0f\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDynaGen\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u76f8\u6bd4\u6b21\u4f18\u6a21\u578b\uff0c\u5e73\u5747MRR\u5206\u6570\u5728\u63d2\u503c\u4efb\u52a1\u4e0a\u63d0\u53472.61\u5206\uff0c\u5728\u5916\u63a8\u4efb\u52a1\u4e0a\u63d0\u53471.45\u5206\u3002", "conclusion": "DynaGen\u901a\u8fc7\u52a8\u6001\u5b50\u56fe\u6784\u5efa\u548c\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u6cdb\u5316\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u63d2\u503c\u548c\u5916\u63a8\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12929", "abs": "https://arxiv.org/abs/2512.12929", "authors": ["Huu-An Vu", "Van-Khanh Mai", "Trong-Tam Nguyen", "Quang-Duc Dam", "Tien-Huy Nguyen", "Thanh-Huong Le"], "title": "MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation", "comment": null, "summary": "The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.", "AI": {"tldr": "MADTempo\u662f\u4e00\u4e2a\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u5e8f\u641c\u7d22\u673a\u5236\u805a\u5408\u89c6\u9891\u7247\u6bb5\u76f8\u4f3c\u5ea6\u6765\u6355\u83b7\u4e8b\u4ef6\u8fde\u7eed\u6027\uff0c\u5e76\u7ed3\u5408Google\u56fe\u50cf\u641c\u7d22\u56de\u9000\u6a21\u5757\u589e\u5f3a\u67e5\u8be2\u8868\u793a\uff0c\u63d0\u5347\u5bf9\u672a\u89c1\u89c6\u89c9\u6982\u5ff5\u548c\u590d\u6742\u65f6\u5e8f\u67e5\u8be2\u7684\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u5728\u5efa\u6a21\u591a\u4e2a\u4e8b\u4ef6\u95f4\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\u548c\u5904\u7406\u5305\u542b\u672a\u89c1\u6216\u7f55\u89c1\u89c6\u89c9\u6982\u5ff5\u7684\u67e5\u8be2\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u80fd\u591f\u7406\u89e3\u590d\u6742\u4e8b\u4ef6\u65f6\u5e8f\u7ed3\u6784\u4e14\u5177\u5907\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "1) \u65f6\u5e8f\u641c\u7d22\u673a\u5236\uff1a\u901a\u8fc7\u805a\u5408\u8fde\u7eed\u89c6\u9891\u7247\u6bb5\u7684\u76f8\u4f3c\u5ea6\u5206\u6570\u6765\u6355\u83b7\u4e8b\u4ef6\u7ea7\u8fde\u7eed\u6027\uff0c\u652f\u6301\u591a\u4e8b\u4ef6\u67e5\u8be2\u7684\u8fde\u8d2f\u68c0\u7d22\uff1b2) Google\u56fe\u50cf\u641c\u7d22\u56de\u9000\u6a21\u5757\uff1a\u5229\u7528\u5916\u90e8\u7f51\u7edc\u56fe\u50cf\u6269\u5c55\u67e5\u8be2\u8868\u793a\uff0c\u5f25\u8865\u9884\u8bad\u7ec3\u89c6\u89c9\u5d4c\u5165\u7684\u4e0d\u8db3\uff0c\u589e\u5f3a\u5bf9\u5206\u5e03\u5916\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u3002", "result": "MADTempo\u6846\u67b6\u63d0\u5347\u4e86\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u9891\u8bed\u6599\u5e93\u4e2d\u5b9e\u73b0\u66f4\u5177\u8bed\u4e49\u611f\u77e5\u548c\u81ea\u9002\u5e94\u7684\u68c0\u7d22\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7edf\u4e00\u65f6\u5e8f\u641c\u7d22\u4e0e\u7f51\u7edc\u7ea7\u89c6\u89c9\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u5728\u65f6\u5e8f\u5efa\u6a21\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u66f4\u667a\u80fd\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89c6\u9891\u68c0\u7d22\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.12935", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12935", "abs": "https://arxiv.org/abs/2512.12935", "authors": ["Toan Le Ngo Thanh", "Phat Ha Huu", "Tan Nguyen Dang Duy", "Thong Nguyen Le Minh", "Anh Nguyen Nhu Tinh"], "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion", "comment": "Accepted at AAAI Workshop 2026", "summary": "The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u65f6\u523b\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ea7\u8054\u53cc\u5d4c\u5165\u7ba1\u9053\u3001\u65f6\u95f4\u611f\u77e5\u8bc4\u5206\u673a\u5236\u548c\u667a\u80fd\u4f53\u5f15\u5bfc\u67e5\u8be2\u5206\u89e3\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u566a\u58f0\u3001\u65f6\u95f4\u5efa\u6a21\u548c\u6a21\u6001\u9009\u62e9\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u89c6\u9891\u5185\u5bb9\u7684\u6307\u6570\u589e\u957f\u8feb\u5207\u9700\u8981\u9ad8\u6548\u7684\u591a\u6a21\u6001\u65f6\u523b\u68c0\u7d22\u7cfb\u7edf\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u56fa\u5b9a\u6743\u91cd\u878d\u5408\u7b56\u7565\u65e0\u6cd5\u5904\u7406\u8de8\u6a21\u6001\u566a\u58f0\u548c\u6a21\u7cca\u67e5\u8be2\uff1b2) \u65f6\u95f4\u5efa\u6a21\u96be\u4ee5\u6355\u6349\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u540c\u65f6\u60e9\u7f5a\u4e0d\u73b0\u5b9e\u7684\u65f6\u95f4\u95f4\u9694\uff1b3) \u7cfb\u7edf\u9700\u8981\u624b\u52a8\u6a21\u6001\u9009\u62e9\uff0c\u964d\u4f4e\u4e86\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u7ea7\u8054\u53cc\u5d4c\u5165\u7ba1\u9053\u7ed3\u5408BEIT-3\u548cSigLIP\u8fdb\u884c\u5e7f\u6cdb\u68c0\u7d22\uff0c\u901a\u8fc7BLIP-2\u91cd\u6392\u5e8f\u5e73\u8861\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\uff1b2) \u65f6\u95f4\u611f\u77e5\u8bc4\u5206\u673a\u5236\u901a\u8fc7\u6ce2\u675f\u641c\u7d22\u5bf9\u5927\u7684\u65f6\u95f4\u95f4\u9694\u5e94\u7528\u6307\u6570\u8870\u51cf\u60e9\u7f5a\uff0c\u6784\u5efa\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u800c\u975e\u5b64\u7acb\u5e27\uff1b3) \u667a\u80fd\u4f53\u5f15\u5bfc\u67e5\u8be2\u5206\u89e3(GPT-4o)\u81ea\u52a8\u89e3\u91ca\u6a21\u7cca\u67e5\u8be2\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u6a21\u6001\u7279\u5b9a\u5b50\u67e5\u8be2(\u89c6\u89c9/OCR/ASR)\uff0c\u5e76\u8fdb\u884c\u81ea\u9002\u5e94\u5206\u6570\u878d\u5408\uff0c\u6d88\u9664\u624b\u52a8\u6a21\u6001\u9009\u62e9\u3002", "result": "\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u5904\u7406\u6a21\u7cca\u67e5\u8be2\uff0c\u68c0\u7d22\u65f6\u95f4\u8fde\u8d2f\u7684\u5e8f\u5217\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u878d\u5408\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u65f6\u523b\u641c\u7d22\u80fd\u529b\u3002", "conclusion": "\u8be5\u7edf\u4e00\u591a\u6a21\u6001\u65f6\u523b\u68c0\u7d22\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u7ea7\u8054\u68c0\u7d22\u3001\u65f6\u95f4\u5efa\u6a21\u548c\u667a\u80fd\u67e5\u8be2\u5206\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u6a21\u7cca\u67e5\u8be2\u3001\u6784\u5efa\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u548c\u81ea\u9002\u5e94\u878d\u5408\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u4ea4\u4e92\u5f0f\u65f6\u523b\u641c\u7d22\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.12982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12982", "abs": "https://arxiv.org/abs/2512.12982", "authors": ["Ziheng Qin", "Yuheng Ji", "Renshuai Tao", "Yuxuan Tian", "Yuyang Liu", "Yipu Wang", "Xiaolong Zheng"], "title": "Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes", "comment": null, "summary": "The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGAPL\u6846\u67b6\u89e3\u51b3AIGI\u68c0\u6d4b\u4e2d\u7684\"\u6536\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\uff0c\u901a\u8fc7\u751f\u6210\u5668\u611f\u77e5\u539f\u578b\u5b66\u4e60\u521b\u5efa\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\uff0c\u7ed3\u5408LoRA\u589e\u5f3a\u5224\u522b\u80fd\u529b\uff0c\u5b9e\u73b0\u8de8\u751f\u6210\u5668\u7684SOTA\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u901a\u7528AIGI\u68c0\u6d4b\u5668\u901a\u8fc7\u805a\u5408\u591a\u751f\u6210\u5668\u6570\u636e\u63d0\u5347\u6cdb\u5316\u6027\uff0c\u4f46\u5b58\u5728\"\u6536\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\uff1a\u968f\u7740\u6570\u636e\u6e90\u591a\u6837\u6027\u589e\u52a0\uff0c\u68c0\u6d4b\u6027\u80fd\u5148\u63d0\u5347\u540e\u4e0b\u964d\u3002\u8fd9\u6e90\u4e8e\u6570\u636e\u7ea7\u5f02\u8d28\u6027\u5bfc\u81f4\u7279\u5f81\u5206\u5e03\u91cd\u53e0\uff0c\u4ee5\u53ca\u6a21\u578b\u7ea7\u74f6\u9888\uff08\u56fa\u5b9a\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u6027\u589e\u957f\uff09\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5668\u611f\u77e5\u539f\u578b\u5b66\u4e60(GAPL)\u6846\u67b6\uff1a1) \u5b66\u4e60\u7d27\u51d1\u7684\u5178\u578b\u4f2a\u9020\u539f\u578b\u96c6\uff0c\u521b\u5efa\u7edf\u4e00\u4f4e\u65b9\u5dee\u7279\u5f81\u7a7a\u95f4\u4ee5\u5bf9\u6297\u6570\u636e\u5f02\u8d28\u6027\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94(LoRA)\uff0c\u589e\u5f3a\u5224\u522b\u80fd\u529b\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "result": "GAPL\u5728\u5e7f\u6cdbGAN\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u5668\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5c55\u793a\u51fa\u4f18\u8d8a\u7684\u8de8\u751f\u6210\u5668\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\"\u6536\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\u3002", "conclusion": "GAPL\u901a\u8fc7\u7ed3\u6784\u5316\u5b66\u4e60\u8303\u5f0f\u7ea6\u675f\u8868\u793a\uff0c\u5efa\u7acb\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u4e3a\u901a\u7528AIGI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.13039", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13039", "abs": "https://arxiv.org/abs/2512.13039", "authors": ["Hao Chen", "Yiwei Wang", "Songze Li"], "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models", "comment": "Under Review", "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.", "AI": {"tldr": "\u63d0\u51faBi-Erasing\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u56fe\u50cf\u5f15\u5bfc\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u540c\u65f6\u8fdb\u884c\u6982\u5ff5\u6291\u5236\u548c\u5b89\u5168\u589e\u5f3a\uff0c\u5e73\u8861\u64e6\u9664\u6548\u679c\u4e0e\u751f\u6210\u8d28\u91cf", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u5411\u7b56\u7565\uff08\u6291\u5236\u76ee\u6807\u6982\u5ff5\u6216\u5f3a\u5316\u5b89\u5168\u66ff\u4ee3\uff09\uff0c\u96be\u4ee5\u5e73\u8861\u6982\u5ff5\u79fb\u9664\u4e0e\u751f\u6210\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u4e2a\u65b9\u9762\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u53cc\u5411\u56fe\u50cf\u5f15\u5bfc\u6982\u5ff5\u64e6\u9664\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u89e3\u8026\u7684\u56fe\u50cf\u5206\u652f\uff1a\u8d1f\u5206\u652f\u8d1f\u8d23\u6291\u5236\u6709\u5bb3\u8bed\u4e49\uff0c\u6b63\u5206\u652f\u4e3a\u5b89\u5168\u66ff\u4ee3\u63d0\u4f9b\u89c6\u89c9\u6307\u5bfc\u3002\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u8fc7\u6ee4\u9632\u6b62\u65e0\u5173\u5185\u5bb9\u5e72\u6270", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cBi-Erasing\u5728\u5e73\u8861\u6982\u5ff5\u79fb\u9664\u6548\u679c\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u6982\u5ff5\u6291\u5236\u548c\u5b89\u5168\u589e\u5f3a\u7684\u4e92\u8865\u65b9\u5411\uff0cBi-Erasing\u5b9e\u73b0\u4e86\u64e6\u9664\u6548\u679c\u4e0e\u751f\u6210\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2512.13190", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13190", "abs": "https://arxiv.org/abs/2512.13190", "authors": ["Jin Sob Kim", "Hyun Joon Park", "Wooseok Shin", "Dongil Park", "Sung Won Han"], "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory", "comment": "Accepted to IEEE Transactions on Aerospace and Electronic Systems (TAES)", "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.", "AI": {"tldr": "\u63d0\u51faWAY\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u91cd\u6784AIS\u8f68\u8ff9\u4e3a\u5d4c\u5957\u5e8f\u5217\u7ed3\u6784\uff0c\u7ed3\u5408\u901a\u9053\u805a\u5408\u5e8f\u5217\u5904\u7406\u5757\u548c\u68af\u5ea6\u4e22\u5f03\u6280\u672f\uff0c\u5b9e\u73b0\u63d0\u524d\u6570\u5929\u81f3\u6570\u5468\u7684\u8239\u8236\u76ee\u7684\u5730\u9884\u6d4b", "motivation": "AIS\u7cfb\u7edf\u867d\u7136\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u6d77\u4e8b\u76d1\u63a7\uff0c\u4f46\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u548c\u6570\u636e\u95f4\u9694\u4e0d\u89c4\u5219\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u8239\u8236\u76ee\u7684\u5730\u9884\u6d4b\u7684\u6311\u6218", "method": "1) \u5c06\u957f\u6e2f\u53e3\u5230\u6e2f\u53e3\u8f68\u8ff9\u91cd\u6784\u4e3a\u5d4c\u5957\u5e8f\u5217\u7ed3\u6784\uff1b2) \u63d0\u51faWAY\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5305\u542b\u8f68\u8ff9\u8868\u793a\u5c42\u548c\u901a\u9053\u805a\u5408\u5e8f\u5217\u5904\u7406\u5757\uff1b3) \u5f15\u5165\u68af\u5ea6\u4e22\u5f03\u6280\u672f\u89e3\u51b3\u5355\u6807\u7b7e\u591a\u5bf9\u591a\u8bad\u7ec3\u95ee\u9898", "result": "\u57285\u5e74AIS\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWAY\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u7a7a\u95f4\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u4e14\u68af\u5ea6\u4e22\u5f03\u6280\u672f\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5c55\u793a\u4e86ETA\u4f30\u8ba1\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u5e94\u7528\u6f5c\u529b", "conclusion": "WAY\u67b6\u6784\u901a\u8fc7\u521b\u65b0\u7684\u8f68\u8ff9\u8868\u793a\u548c\u8bad\u7ec3\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u8239\u8236\u76ee\u7684\u5730\u9884\u6d4b\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u6d77\u4e8b\u76d1\u63a7\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.13095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13095", "abs": "https://arxiv.org/abs/2512.13095", "authors": ["Feng Zhang", "Zezhong Tan", "Xinhong Ma", "Ziqiang Dong", "Xi Leng", "Jianfei Zhao", "Xin Sun", "Yang Yang"], "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning", "comment": null, "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.", "AI": {"tldr": "ADHint\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96be\u5ea6\u7684\u81ea\u9002\u5e94\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u66f4\u597d\u5730\u5e73\u8861\u63a2\u7d22\u4e0e\u6a21\u4eff\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684RL\u65b9\u6cd5\u5728\u8c03\u5ea6\u63d0\u793a\u6bd4\u4f8b\u548c\u4f30\u8ba1\u76f8\u5bf9\u4f18\u52bf\u65f6\u5ffd\u7565\u96be\u5ea6\u56e0\u7d20\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u8fc7\u5ea6\u6a21\u4eff\u79bb\u7b56\u7565\u63d0\u793a\uff0c\u9700\u8981\u66f4\u597d\u7684\u63a2\u7d22-\u6a21\u4eff\u5e73\u8861\u3002", "method": "1) \u57fa\u4e8e\u6837\u672c\u96be\u5ea6\u5148\u9a8c\u7684\u81ea\u9002\u5e94\u63d0\u793a\uff1a\u8bc4\u4f30\u6837\u672c\u96be\u5ea6\u5e76\u8c03\u5ea6\u9002\u5f53\u63d0\u793a\u6bd4\u4f8b\uff1b2) \u4e00\u81f4\u6027\u68af\u5ea6\u8c03\u5236\u548c\u9009\u62e9\u6027\u63a9\u7801\uff1a\u8c03\u5236\u63d0\u793a\u5185\u4ee4\u724c\u7ea7\u68af\u5ea6\uff1b3) \u57fa\u4e8e\u63a8\u51fa\u96be\u5ea6\u540e\u9a8c\u7684\u4f18\u52bf\u4f30\u8ba1\uff1a\u5229\u7528\u6709/\u65e0\u63d0\u793a\u63a8\u51fa\u7684\u76f8\u5bf9\u96be\u5ea6\u4f30\u8ba1\u5404\u81ea\u4f18\u52bf\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u3001\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cADHint\u5728\u63a8\u7406\u80fd\u529b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728pass@1\u548cavg@8\u6307\u6807\u4e0a\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5c06\u96be\u5ea6\u4f5c\u4e3a\u5173\u952e\u56e0\u7d20\u878d\u5165\u63d0\u793a\u6bd4\u4f8b\u8c03\u5ea6\u548c\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\uff0c\u80fd\u5b9e\u73b0\u66f4\u597d\u7684\u63a2\u7d22-\u6a21\u4eff\u5e73\u8861\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.13340", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.13340", "abs": "https://arxiv.org/abs/2512.13340", "authors": ["Henrik C. M. Frederiksen", "Junya Shiraishi", "Cedomir Stefanovic", "Hei Victor Cheng", "Shashi Raj Pandey"], "title": "Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks", "comment": null, "summary": "The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.", "AI": {"tldr": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u6846\u67b6\uff0c\u5c06\u6301\u7eed\u5b66\u4e60\u6574\u5408\u5230\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\uff0c\u7528\u4e8e\u8282\u80fd\u7684\u6545\u969c\u68c0\u6d4b", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f1a\u56e0\u73af\u5883\u975e\u5e73\u7a33\u6027\u548c\u521d\u59cb\u8bad\u7ec3\u6570\u636e\u6709\u9650\u800c\u5bfc\u81f4\u63a8\u7406\u7cbe\u5ea6\u4e0b\u964d\u3002\u4f20\u7edf\u66f4\u65b0\u65b9\u6cd5\u4f1a\u6d88\u8017\u989d\u5916\u80fd\u91cf\uff0c\u4e0d\u9002\u5408\u80fd\u91cf\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u6846\u67b6\uff0c\u4f7f\u7269\u8054\u7f51\u8bbe\u5907\u548c\u8fb9\u7f18\u670d\u52a1\u5668\u80fd\u591f\u6839\u636e\u65e0\u7ebf\u94fe\u8def\u6761\u4ef6\u548c\u53ef\u7528\u80fd\u91cf\u9884\u7b97\uff0c\u534f\u4f5c\u66f4\u65b0\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u5468\u671f\u6027\u91c7\u6837\u548c\u975e\u81ea\u9002\u5e94\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4e25\u683c\u7684\u80fd\u91cf\u548c\u5e26\u5bbd\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe42.8%\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u6574\u5408\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u901a\u4fe1\u7b56\u7565\uff0c\u4e3a\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684\u8282\u80fd\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13381", "abs": "https://arxiv.org/abs/2512.13381", "authors": ["Changjun Zhou", "Jintao Zheng", "Leyou Yang", "Pengfei Wang"], "title": "Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction", "comment": "10 pages, submitted to INFOCOM 2026", "summary": "Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.", "AI": {"tldr": "DPUL\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u670d\u52a1\u5668\u7aef\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u9057\u5fd8\u6240\u6709\u6709\u5f71\u54cd\u7684\u6743\u91cd\u6765\u9632\u6b62\u9690\u79c1\u6cc4\u9732\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u65f6\u95f4\u6210\u672c\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u9700\u6c42\u3001\u590d\u6742\u6fc0\u52b1\u673a\u5236\u548c\u5ba2\u6237\u7aef\u8ba1\u7b97\u80fd\u529b\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u65f6\u95f4\u957f\u3001\u6210\u672c\u9ad8\u3002\u540c\u65f6\uff0c\u73b0\u6709\u670d\u52a1\u5668\u7aef\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4ec5\u79fb\u9664\u76ee\u6807\u5ba2\u6237\u7aef\u7684\u66f4\u65b0\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u5ba2\u6237\u7aef\u8d21\u732e\u4e2d\u5d4c\u5165\u7684\u9690\u79c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u3002", "method": "DPUL\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1)\u901a\u8fc7\u8fc7\u6ee4\u5ba2\u6237\u7aef\u66f4\u65b0\u5e45\u5ea6\u8bc6\u522b\u9ad8\u6743\u91cd\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u56de\u6eda\u4ee5\u786e\u4fdd\u6df1\u5ea6\u79fb\u9664\uff1b(2)\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u91cd\u5efa\u548c\u6d88\u9664\u4f4e\u6743\u91cd\u53c2\u6570\uff1b(3)\u4f7f\u7528\u57fa\u4e8e\u6295\u5f71\u7684\u6280\u672f\u6062\u590d\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDPUL\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e861%-5%\uff0c\u65f6\u95f4\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe12\u500d\u3002", "conclusion": "DPUL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u670d\u52a1\u5668\u7aef\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u6df1\u5ea6\u9057\u5fd8\u6240\u6709\u6709\u5f71\u54cd\u7684\u6743\u91cd\uff0c\u6709\u6548\u9632\u6b62\u9690\u79c1\u6cc4\u9732\uff0c\u540c\u65f6\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2512.13317", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13317", "abs": "https://arxiv.org/abs/2512.13317", "authors": ["Mikhail Zakharov"], "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion", "comment": "12 pages, 1 figure, 5 tables, 10 equations. Preprint", "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4eba\u8138\u68c0\u7d22\u7cfb\u7edf\u7684\u8eab\u4efd\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6563\u76ee\u6807\u8eab\u4efd\u5728\u8d85\u7403\u9762\u4e0a\u7684\u5d4c\u5165\u8868\u793a\uff0c\u9632\u6b62\u5f62\u6210\u7d27\u51d1\u7684\u8eab\u4efd\u7c07\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5176\u4ed6\u8eab\u4efd\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u867d\u7136\u80fd\u5b66\u4e60\u9ad8\u5ea6\u533a\u5206\u6027\u7684\u8eab\u4efd\u7c07\u4ee5\u5b9e\u73b0\u51c6\u786e\u68c0\u7d22\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\uff0c\u53ef\u80fd\u88ab\u7528\u4e8e\u672a\u7ecf\u6388\u6743\u7684\u8eab\u4efd\u8ffd\u8e2a\u3002\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5728\u4eba\u8138\u68c0\u7d22\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u73b0\u4ee3\u57fa\u4e8e\u5d4c\u5165\u7684\u8bc6\u522b\u6a21\u578b\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u73b0\u6709\u7684\u8fd1\u4f3c\u7c7b\u522b\u9057\u5fd8\u65b9\u6cd5\uff08\u5982\u968f\u673a\u6807\u8bb0\u3001\u68af\u5ea6\u4e0a\u5347\u3001\u8fb9\u754c\u9057\u5fd8\u7b49\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u57fa\u4e8e\u5206\u6563\u7684\u9057\u5fd8\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6563\u76ee\u6807\u8eab\u4efd\u5728\u8d85\u7403\u9762\u4e0a\u7684\u5d4c\u5165\u8868\u793a\uff0c\u9632\u6b62\u5f62\u6210\u7d27\u51d1\u7684\u8eab\u4efd\u7c07\uff0c\u540c\u65f6\u4fdd\u6301\u5d4c\u5165\u7a7a\u95f4\u7684\u533a\u5206\u6027\u7ed3\u6784\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08VGGFace2\u3001CelebA\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9057\u5fd8\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u7d22\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u4eba\u8138\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u8eab\u4efd\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5206\u6563\u5f0f\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u7279\u5b9a\u8eab\u4efd\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u7cfb\u7edf\u5bf9\u5176\u4ed6\u8eab\u4efd\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.13416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13416", "abs": "https://arxiv.org/abs/2512.13416", "authors": ["Haoxuan Qu", "Qiuchi Xiang", "Yujun Cai", "Yirui Wu", "Majid Mirmehdi", "Hossein Rahmani", "Jun Liu"], "title": "Learning to Generate Cross-Task Unexploitable Examples", "comment": null, "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.", "AI": {"tldr": "\u63d0\u51faMCT-UEG\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8de8\u4efb\u52a1\u8bad\u7ec3\u751f\u6210\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u4e2a\u4eba\u56fe\u50cf\uff0c\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u6027", "motivation": "\u73b0\u6709\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u751f\u6210\u8de8\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u90fd\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u793a\u4f8b\uff0c\u9700\u8981\u63d0\u5347\u65b9\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027", "method": "\u63d0\u51faMCT-UEG\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u8bbe\u8ba1\u9762\u5411\u5e73\u5766\u6700\u5c0f\u503c\u7684\u5143\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65b9\u6848\uff0c\u4f18\u5316\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u5668\u4ee5\u6709\u6548\u4ea7\u751f\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u793a\u4f8b", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684MCT-UEG\u6846\u67b6\u80fd\u591f\u751f\u6210\u8de8\u4efb\u52a1\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u4e2a\u4eba\u56fe\u50cf\uff0c\u63d0\u5347\u4e86\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.13560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13560", "abs": "https://arxiv.org/abs/2512.13560", "authors": ["Shun Maeda", "Chunzhi Gu", "Koichiro Kamide", "Katsuya Hotta", "Shangce Gao", "Chao Zhang"], "title": "3D Human-Human Interaction Anomaly Detection", "comment": null, "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.", "AI": {"tldr": "\u63d0\u51faH2IAD\u4efb\u52a1\u548cIADNet\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u53cc\u4eba\u4ea4\u4e92\u884c\u4e3a\u5f02\u5e38\uff0c\u901a\u8fc7TASM\u548cDREM\u6a21\u5757\u5206\u522b\u6355\u6349\u65f6\u7a7a\u7279\u5f81\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5355\u4eba\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u6355\u6349\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u7684\u590d\u6742\u975e\u5bf9\u79f0\u52a8\u6001\uff0c\u800c\u4eba\u7c7b\u5929\u751f\u5177\u6709\u534f\u4f5c\u6027\uff0c\u4ea4\u4e92\u884c\u4e3a\u5f02\u5e38\u9700\u8981\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u63d0\u51faIADNet\u6a21\u578b\uff0c\u5305\u542bTemporal Attention Sharing Module (TASM)\u5171\u4eab\u8fd0\u52a8\u7f16\u7801\u4ee5\u540c\u6b65\u534f\u4f5c\u76f8\u5173\u6027\uff0c\u4ee5\u53caDistance-Based Relational Encoding Module (DREM)\u6355\u6349\u7a7a\u95f4\u793e\u4ea4\u7ebf\u7d22\uff0c\u6700\u540e\u4f7f\u7528normalizing flow\u8fdb\u884c\u5f02\u5e38\u8bc4\u5206\u3002", "result": "\u5728\u4eba\u7c7b\u4ea4\u4e92\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIADNet\u4f18\u4e8e\u73b0\u6709\u7684\u4eba\u4e3a\u4e2d\u5fc3\u5f02\u5e38\u68c0\u6d4b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6210\u529f\u5b9a\u4e49\u4e86H2IAD\u65b0\u4efb\u52a1\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684IADNet\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u65f6\u7a7a\u7279\u5f81\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u884c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
