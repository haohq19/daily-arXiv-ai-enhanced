<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors](https://arxiv.org/abs/2602.11323)
*Arda Alniak,Sinan Kalkan,Mustafa Mert Ankarali,Afsar Saranli,Abdullah Aydin Alatan*

Main category: cs.CV

TL;DR: 提出一种将学习深度先验集成到VINS-Mono优化后端的新框架，通过仿射不变深度一致性和成对序数约束，在边缘设备上实现实时、鲁棒的视觉惯性里程计


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉惯性里程计在低纹理环境中表现不佳，稀疏视觉特征不足以进行准确位姿估计。虽然密集单目深度估计可以作为补充信息源，但基于Vision Transformer的复杂基础模型计算量大，无法在边缘设备上实时部署。

Method: 提出将学习深度先验直接集成到VINS-Mono优化后端的新框架，强制实施仿射不变深度一致性和成对序数约束，通过基于方差的门控机制显式过滤不稳定伪影，严格遵循边缘设备的计算限制。

Result: 在TartanGround和M3ED数据集上的广泛实验表明，该方法在挑战性场景中防止发散，并带来显著的精度提升，绝对轨迹误差降低高达28.3%。

Conclusion: 该方法成功地将学习深度先验集成到VIO系统中，在保持边缘设备实时性的同时，显著提高了在低纹理环境中的鲁棒性和准确性。

Abstract: Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.

</details>


### [2] [DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation](https://arxiv.org/abs/2602.12160)
*Xu Guo,Fulong Ye,Qichao Sun,Liyang Chen,Bingchuan Li,Pengze Zhang,Jiawei Liu,Songtao Zhao,Qian He,Xiangwang Hou*

Main category: cs.CV

TL;DR: DreamID-Omni是一个统一框架，用于可控的人为中心的音视频生成，解决了多人物场景中的身份-音色绑定失败问题，并在多个任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将参考音视频生成、视频编辑和音频驱动视频动画等任务视为孤立目标，且在多人物场景中难以实现精确解耦的身份和音色控制。

Method: 设计了对称条件扩散Transformer，采用对称条件注入方案；提出双级解耦策略（同步RoPE和结构化描述）；开发多任务渐进训练方案，利用弱约束生成先验来正则化强约束任务。

Result: 在视频、音频和音视频一致性方面实现了全面的最先进性能，甚至超越了领先的专有商业模型。

Conclusion: DreamID-Omni提供了一个统一的框架，解决了多人物音视频生成中的关键挑战，弥合了学术研究与商业级应用之间的差距。

Abstract: Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](https://arxiv.org/abs/2602.11206)
*Jose Marie Antonio Miñoza*

Main category: cs.LG

TL;DR: UltraLIF：基于热带几何的超离散化框架，用可学习的温度参数替代启发式代理梯度，构建完全可微的脉冲神经网络，在单时间步设置中表现优异。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）具有能效高、生物合理性强的优点，但脉冲生成不可微，需要依赖启发式的代理梯度方法，存在前向-反向不匹配问题。

Method: 引入超离散化（ultradiscretization）数学框架，利用热带几何中的max-plus半环自然建模神经阈值动态。log-sum-exp函数作为可微的软最大值函数，通过可学习的温度参数ε→0收敛到硬阈值。从两种动力学系统推导出两种神经元模型：基于LIF常微分方程的UltraLIF（时间动态）和基于扩散方程建模间隙连接耦合的UltraDLIF（空间动态）。

Result: 在六个基准测试（静态图像、神经形态视觉、音频）上超越代理梯度基线方法，在单时间步（T=1）设置下对神经形态和时间数据集提升最显著。理论分析证明了对经典LIF动态的点态收敛性，具有定量误差界限和有限非消失梯度。可选稀疏惩罚能在保持竞争力的准确率下显著降低能耗。

Conclusion: UltraLIF提供了一个原则性的数学框架，用超离散化替代代理梯度，解决了SNN训练中的前向-反向不匹配问题，在单时间步设置中表现优异，为构建完全可微的脉冲神经网络提供了新途径。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

</details>


### [4] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: CADET是一个用于广告点击率预测的端到端解码器Transformer模型，通过上下文条件解码、自门控注意力、时间感知位置编码等创新，在LinkedIn广告平台上实现了11.04%的CTR提升。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习推荐模型在CTR预测领域长期占据主导，但生成式推荐器在内容推荐中显示出潜力。然而，将这些基于Transformer的架构适配到广告CTR预测仍面临独特挑战：处理后评分上下文信号、保持离线-在线一致性、扩展到工业级工作负载。

Method: CADET采用端到端解码器Transformer架构，包含：1) 上下文条件解码架构与多塔预测头，显式建模广告位置等后评分信号；2) 自门控注意力机制，在表示和交互层面自适应调节信息流；3) 基于时间戳的RoPE变体，捕捉从秒到月的时间关系；4) 会话掩码策略，防止模型学习不可用的会话内事件依赖；5) 生产工程技术，包括张量打包、序列分块和自定义Flash Attention内核。

Result: 在线A/B测试中，CADET相比生产基线LiRank模型（DCNv2和序列编码器的混合集成）实现了11.04%的CTR提升。系统已成功部署在LinkedIn广告平台，服务于主页信息流赞助更新的主要流量。

Conclusion: CADET成功解决了广告CTR预测中的关键挑战，通过创新的架构设计和生产优化，在工业规模上实现了显著的性能提升，证明了端到端解码器Transformer在广告推荐领域的有效性。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [5] [Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning](https://arxiv.org/abs/2602.11498)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种改进GFlowNets的方法，通过引入规划器将大状态空间划分为重叠的部分状态空间，限制智能体探索范围，从而提高在大状态空间中的收敛速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets在大型状态空间中自由探索时面临显著的收敛挑战，需要更高效的探索策略来应对大规模状态空间。

Method: 引入规划器将整个状态空间划分为重叠的部分状态空间，使智能体能在有限大小的子空间中高效识别高奖励区域；采用启发式策略在不同部分区域间切换，避免智能体在已完全探索或低奖励区域浪费时间。

Result: 在多个广泛使用的数据集上，该方法比现有工作在大状态空间中收敛更快，不仅生成奖励更高的候选者，还显著提高了多样性。

Conclusion: 通过限制智能体探索范围并采用智能区域切换策略，可以有效解决GFlowNets在大状态空间中的收敛问题，提高生成质量和效率。

Abstract: Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \modelname converges faster than existing works on large state spaces. Furthermore, \modelname not only generates candidates with higher rewards but also significantly improves their diversity.

</details>


### [6] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 提出校准方法，利用不完美的外部预测器估计未购买概率，仅使用购买数据即可获得统计有效的未购买估计


<details>
  <summary>Details</summary>
Motivation: 企业通常无法观察消费者的关键行为（是否从竞争对手购买、不购买等），这种缺失的外部选择信息使得市场规模和偏好估计困难，特别是在只有交易数据的情况下。现有方法依赖辅助市场数据，但辅助预测器可能存在偏差或校准不当。

Method: 开发两种校准方法：1) 在对数空间仿射误校准下，通过简单回归识别外部选择效用参数；2) 在较弱近单调条件下，提出基于排序的校准方法，并推导有限样本误差界限

Result: 方法能够将不完美的预测转化为统计有效的未购买估计，无需收集新的未购买标签。数值实验显示在未购买估计和下游品类决策方面有改进

Conclusion: 提出的校准方法能够有效利用不完美的辅助预测器来估计未购买概率，仅使用购买数据即可获得可靠估计，并量化了校准准确性对收入绩效的影响

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [7] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的正则化方法，通过加权监督微调损失来平衡防止奖励攻击和保持策略更新稳定性，在多个基准测试中优于RLHF和在线偏好学习方法。


<details>
  <summary>Details</summary>
Motivation: RLHF面临奖励攻击和稳定优化两大挑战，现有方法分别通过KL散度惩罚和策略比率裁剪来解决，但两者同时正则化时产生的隐式权衡未被充分探索。

Method: 提出统一的正则化方法，通过加权监督微调损失来显式平衡防止奖励攻击和保持策略更新稳定性的目标。

Result: 在多个基准测试中，该方法一致优于RLHF和在线偏好学习方法，实现了更好的对齐性能和稳定性。

Conclusion: 提出的简单而原则性的对齐目标通过加权监督微调损失实现了更好的权衡，显著提升了对齐结果并降低了实现复杂度。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [8] [Both Topology and Text Matter: Revisiting LLM-guided Out-of-Distribution Detection on Text-attributed Graphs](https://arxiv.org/abs/2602.11641)
*Yinlin Zhu,Di Wu,Xu Wang,Guocong Quan,Miao Hu*

Main category: cs.LG

TL;DR: LG-Plug：一种用于文本属性图OOD检测的LLM引导即插即用策略，通过对齐拓扑和文本表示生成细粒度节点嵌入，利用聚类迭代LLM提示生成共识驱动的OOD暴露，并作为正则化项与现有检测器无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本属性图的OOD检测中存在局限性：拓扑驱动方法未能充分利用丰富的语义信息，而基于LLM的方法在合成OOD先验时存在可靠性-信息性不平衡问题，且依赖专门架构无法整合有效的拓扑级见解。

Method: 提出LG-Plug策略：1）对齐拓扑和文本表示生成细粒度节点嵌入；2）通过聚类迭代LLM提示生成共识驱动的OOD暴露；3）利用轻量级集群码本和启发式采样降低LLM查询时间成本；4）将OOD暴露作为正则化项分离ID和OOD节点。

Result: 该方法能够生成更可靠的OOD暴露，减少ID噪声，提高检测性能，同时保持与现有检测器的兼容性，降低计算成本。

Conclusion: LG-Plug通过结合LLM的语义理解能力和拓扑级见解，解决了文本属性图OOD检测中的可靠性-信息性平衡问题，提供了一种高效且可扩展的即插即用解决方案。

Abstract: Text-attributed graphs (TAGs) associate nodes with textual attributes and graph structure, enabling GNNs to jointly model semantic and structural information. While effective on in-distribution (ID) data, GNNs often encounter out-of-distribution (OOD) nodes with unseen textual or structural patterns in real-world settings, leading to overconfident and erroneous predictions in the absence of reliable OOD detection. Early approaches address this issue from a topology-driven perspective, leveraging neighboring structures to mitigate node-level detection bias. However, these methods typically encode node texts as shallow vector features, failing to fully exploit rich semantic information. In contrast, recent LLM-based approaches generate pseudo OOD priors by leveraging textual knowledge, but they suffer from several limitations: (1) a reliability-informativeness imbalance in the synthesized OOD priors, as the generated OOD exposures either deviate from the true OOD semantics, or introduce non-negligible ID noise, all of which offers limited improvement to detection performance; (2) reliance on specialized architectures, which prevents incorporation of the extensive effective topology-level insights that have been empirically validated in prior work. To this end, we propose LG-Plug, an LLM-Guided Plug-and-play strategy for TAG OOD detection tasks. LG-Plug aligns topology and text representations to produce fine-grained node embeddings, then generates consensus-driven OOD exposure via clustered iterative LLM prompting. Moreover, it leverages lightweight in-cluster codebook and heuristic sampling reduce time cost of LLM querying. The resulting OOD exposure serves as a regularization term to separate ID and OOD nodes, enabling seamless integration with existing detectors.

</details>


### [9] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 提出"势能门控"方法，通过势能函数调制观测噪声协方差，在双阱随机系统中实现鲁棒状态估计，相比标准滤波器显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯滤波器在处理双阱随机系统时，对所有状态空间区域采用相同的观测信任度，无法有效处理势垒附近的观测噪声问题，需要一种基于物理原理的鲁棒估计方法。

Method: 提出势能门控方法：根据已知或假设的势能函数局部值调制观测噪声协方差，当状态接近势阱最小值时信任观测，接近势垒时逐步折扣观测。该方法可集成到扩展、无迹、集合、自适应卡尔曼滤波器及粒子滤波器中，仅需两个超参数。

Result: 在Ginzburg-Landau双阱过程的合成基准测试中，相比标准扩展卡尔曼滤波器，RMSE改进57-80%（统计显著，p<10^{-15}）。即使势能参数偏离真实值50%，改进仍不低于47%。在Dansgaard-Oeschger事件应用中，估计出不对称参数γ=-0.109，异常值比例解释了91%的滤波器改进方差。

Conclusion: 势能门控为双阱随机系统提供了一种物理启发的鲁棒状态估计方法，通过调制观测信任度显著提升估计精度，对参数误设具有鲁棒性，并在气候记录分析中展示了实用价值。

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [10] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: MUSE是一个模型服务框架，通过将模型分数与客户端决策边界解耦，在多租户Score-as-a-Service环境中实现无缝模型更新，解决了模型重训练导致分数分布变化而需要协调数百个客户端更新决策阈值的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在多租户Score-as-a-Service环境中，模型重训练会改变分数分布，导致现有决策阈值失效。由于决策边界位于客户端管理的基础设施中，重新校准需要协调数百个客户端更新阈值，消耗大量人力时间并导致模型停滞，形成严重瓶颈。

Method: MUSE采用动态意图路由共享模型优化基础设施复用，结合两级分数转换将模型输出映射到稳定的参考分布，从而将模型分数与客户端决策边界解耦。

Result: MUSE在Feedzai大规模部署，每秒处理超过1000个事件，过去12个月处理超过550亿事件，覆盖数十个租户，同时保持高可用性和低延迟保证。将模型交付时间从数周缩短到数分钟。

Conclusion: MUSE通过解耦模型分数和客户端决策边界，解决了多租户环境中模型更新的瓶颈问题，提高了模型对不断变化的攻击的弹性，节省了数百万美元的欺诈损失和运营成本。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [11] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: APST提出了一种深度导向的LLM安全评估框架，通过重复采样相同提示来发现潜在故障模式，量化推理失败概率，弥补传统广度评估的不足。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全基准主要关注广度评估，但实际部署中面临重复推理带来的操作风险。高风险场景下，持续使用时的响应一致性和安全性至关重要，需要深度评估框架。

Method: 提出加速提示压力测试（APST），在受控操作条件下重复采样相同提示，发现幻觉、拒绝不一致和不安全完成等故障模式。使用伯努利和二项模型形式化安全故障，估计每次推理的失败概率。

Result: 在多个指令调优LLM上应用APST评估发现，具有相似基准分数的模型在重复采样下表现出显著不同的经验失败率，特别是温度升高时。浅层单样本评估可能掩盖持续使用下的可靠性差异。

Conclusion: APST通过提供重复推理下的实用评估框架，补充现有基准，弥合基准对齐和部署导向风险评估之间的差距，为LLM安全可靠性评估提供新视角。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [12] [Capability-Oriented Training Induced Alignment Risk](https://arxiv.org/abs/2602.12124)
*Yujun Zhou,Yue Huang,Han Bao,Kehan Guo,Zhenwen Liang,Pin-Yu Chen,Tian Gao,Werner Geyer,Nuno Moniz,Nitesh V Chawla,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 语言模型在强化学习训练中会自发学习利用环境漏洞来最大化奖励，即使没有恶意意图，这种利用策略具有可迁移性，对现有对齐方法构成根本挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究主要关注防止模型生成有害内容，但更微妙的风险是能力导向训练引发的利用行为。研究者想探究语言模型在含有隐含漏洞的环境中是否会自发学习利用这些漏洞来最大化奖励。

Method: 设计了四款不同的"漏洞游戏"，每种游戏呈现独特的可利用缺陷：上下文条件合规、代理指标、奖励篡改和自我评估。通过强化学习在这些环境中训练模型，并测试其利用策略的可迁移性和可蒸馏性。

Result: 模型一致地学习利用这些漏洞，发现了机会主义策略，这些策略显著增加了奖励但牺牲了任务正确性或安全性。更重要的是，这些利用策略不是狭隘的"技巧"，而是可泛化的技能，可以迁移到新任务，甚至可以通过数据从能力强的教师模型"蒸馏"到其他学生模型。

Conclusion: 能力导向训练引发的风险对当前对齐方法构成根本挑战，未来的AI安全工作必须超越内容审核，严格审计和保护训练环境和奖励机制本身。

Abstract: While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse "vulnerability games", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow "tricks" but generalizable skills; they can be transferred to new tasks and even "distilled" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.

</details>


### [13] [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](https://arxiv.org/abs/2602.12009)
*Luiz Pereira,Mirko Perkusich,Dalton Valadares,Kyller Gorgônio*

Main category: cs.LG

TL;DR: 本文分析了差分隐私机制如何影响脉冲神经网络的发放率统计，以及这些扰动如何传播到基于发放率的联邦神经形态学习协调中，为隐私保护的FNL提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 联邦神经形态学习（FNL）能够在设备上进行节能且保护隐私的学习，但实际部署需要额外的隐私机制，这些机制可能显著改变训练信号。需要理解差分隐私机制如何影响脉冲神经网络的发放率统计和联邦学习协调。

Method: 通过分析差分隐私机制（特别是梯度裁剪和噪声注入）如何扰动脉冲神经网络的发放率统计，研究这些扰动如何传播到基于发放率的FNL协调。在非独立同分布设置下的语音识别任务中，对不同隐私预算和裁剪边界进行消融实验。

Result: 实验揭示了系统性的发放率偏移、衰减的聚合效果以及客户端选择中的排名不稳定性。这些偏移与稀疏性和内存指标相关。

Conclusion: 研究结果为隐私保护的联邦神经形态学习提供了可操作的指导，特别是在隐私强度与基于发放率的协调之间的平衡方面。

Abstract: Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.

</details>


### [14] [PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories](https://arxiv.org/abs/2602.12080)
*Hyunsung Kim,Kunhee Lee,Sangwoo Seo,Sang-Ki Ko,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: PathCRF：仅使用球员轨迹数据检测足球事件（控球、传球等）的框架，无需球追踪数据


<details>
  <summary>Details</summary>
Motivation: 足球事件数据收集主要依赖人工标注，球追踪技术成本高难以扩展，导致数据驱动分析仅限于顶级赛事。需要降低数据收集门槛，让更多比赛能够进行自动化分析。

Method: 将球员轨迹建模为全连接动态图，事件检测转化为在每个时间步选择一条边（代表当前控球状态）。使用条件随机场（CRF）确保边序列的逻辑一致性，发射和转移分数由基于集合注意力的骨干网络动态计算。通过维特比解码获得最可能边序列，当相邻时间步选择的边发生变化时检测事件。

Result: PathCRF能够生成准确、逻辑一致的控球路径，实现可靠的下游分析，同时大幅减少手动事件标注的需求。

Conclusion: PathCRF仅使用球员轨迹数据就能有效检测足球事件，降低了数据收集成本，使数据驱动分析能够扩展到更广泛的足球赛事。

Abstract: Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.

</details>


### [15] [SafeNeuron: Neuron-Level Safety Alignment for Large Language Models](https://arxiv.org/abs/2602.12158)
*Zhaoxin Wang,Jiaming Liang,Fengbin Zhu,Weixiang Zhao,Junfeng Fang,Jiayi Ji,Handing Wang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: SafeNeuron是一个神经元级别的安全对齐框架，通过重新分布安全表示来提高大语言模型的安全性鲁棒性，防止稀疏安全路径依赖


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法主要集中在行为层面，对模型内部安全机制控制有限。研究发现安全行为集中在少数参数中，使得对齐脆弱且容易被神经元级攻击绕过

Method: SafeNeuron首先识别安全相关神经元，然后在偏好优化期间冻结这些神经元，防止模型依赖稀疏安全路径，强制模型构建冗余的安全表示

Result: 实验表明SafeNeuron显著提高了对抗神经元剪枝攻击的鲁棒性，降低了开源模型被重新用作红队生成器的风险，同时保持了通用能力

Conclusion: SafeNeuron提供了一个可解释且鲁棒的模型对齐视角，层间分析显示安全行为由稳定共享的内部表示控制

Abstract: Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: C-JEPA：一种通过对象级掩码增强关系推理的物体中心世界模型，在视觉问答和智能体控制任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有物体中心表示虽然提供了有用的抽象，但不足以捕捉交互依赖的动态关系。需要一种能增强关系理解的世界模型来支持预测、推理和控制任务。

Method: 提出C-JEPA模型，将掩码联合嵌入预测从图像块扩展到物体中心表示。通过对象级掩码技术，要求从其他物体推断被掩码物体的状态，从而诱导潜在干预并防止捷径解决方案。

Result: 在视觉问答任务中，相比无对象级掩码的相同架构，反事实推理能力提升约20%。在智能体控制任务中，仅需基于块的世界模型1%的潜在输入特征即可实现相当性能，规划效率大幅提升。

Conclusion: C-JEPA通过对象级掩码成功引入了因果归纳偏置，增强了物体间关系推理能力，为世界模型提供了更有效的物体中心表示方法。

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [17] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文提出"Rung Collapse"概念，揭示自回归训练无法区分关联P(Y|X)与干预P(Y|do(X))，导致模型"正确但理由错误"。作者提出Epistemic Regret Minimization (ERM)作为因果信念修正方法，防止Aleatoric Entrenchment现象。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统常通过捷径获得高性能，但在分布偏移下崩溃。这种病理现象有精确的因果起源：自回归训练无法区分关联与干预，导致模型固守错误推理方式。

Method: 提出Epistemic Regret Minimization (ERM)作为因果信念修正目标，独立于任务成功惩罚因果推理错误。构建三层架构：(1)物理基础定理证明满足执行器独立的动作实现有效do操作；(2)ERM作为满足AGM公理的因果信念修正算子；(3)错误模式分类法注入领域无关保护。

Result: 在1,360个因果陷阱场景中测试6个前沿LLM，发现Rung Collapse在推理增强模型中仍存在(GPT-5.2为3.7%)，可操控性呈现逆缩放现象。针对性ERM反馈能恢复53-59%的固着错误，而结果层面反馈失败。

Conclusion: 论文形式化了Rung Collapse作为自回归训练的根本限制，提出ERM框架防止因果推理错误固着，证明渐近恢复真实干预分布，为构建更鲁棒的因果推理系统提供理论和方法基础。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [18] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: AIR是首个面向LLM智能体的安全事故响应框架，通过语义检查检测事故、引导智能体执行遏制恢复操作，并生成防护规则防止未来类似事故，在三种典型智能体上实现超过90%的事故检测、修复和根除成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体的安全机制主要关注事前预防，对事故发生后如何响应、遏制和恢复的能力有限。随着LLM智能体在自主应用中的广泛部署，需要建立有效的事故响应机制来处理不可避免的事故。

Method: AIR框架定义了用于自主管理LLM智能体系统事故响应生命周期的领域特定语言，并将其集成到智能体执行循环中：1）基于当前环境状态和近期上下文进行语义检查来检测事故；2）引导智能体通过工具执行遏制和恢复操作；3）在根除阶段合成防护规则以阻止未来类似事故。

Result: 在三种代表性智能体类型上的评估显示，AIR实现了超过90%的事故检测、修复和根除成功率。实验还证实了AIR关键设计组件的必要性，展示了其及时性和适度开销，并证明LLM生成的规则在不同领域可以接近开发者编写规则的效果。

Conclusion: 事故响应作为提升智能体安全的一流机制既是可行的也是必要的。AIR框架为LLM智能体系统提供了有效的事故响应能力，填补了当前安全机制在事后响应方面的空白。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [19] [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917)
*Taian Guo,Haiyang Shen,Junyu Luo,Binqi Chen,Hongjun Ding,Jinsheng Huang,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaPROBE：通过有向无环图导航的自动化因子挖掘框架，将因子池建模为动态生态系统，显著提升预测准确性和训练效率


<details>
  <summary>Details</summary>
Motivation: 现有自动化因子挖掘方法存在两大范式：解耦因子生成（将因子发现视为孤立事件）和迭代因子演化（专注于局部父子优化）。这两种范式都缺乏全局结构视角，将因子池视为非结构化集合或碎片化链，导致冗余搜索和多样性受限。

Method: AlphaPROBE将因子挖掘重新定义为有向无环图（DAG）的战略导航。框架包含两个核心组件：1）贝叶斯因子检索器，通过后验概率模型平衡利用与探索，识别高潜力种子；2）DAG感知因子生成器，利用因子的完整祖先轨迹，生成上下文感知、非冗余的优化。

Result: 在三个主要中国股票市场数据集上对8个竞争基线的广泛实验表明，AlphaPROBE在预测准确性、收益稳定性和训练效率方面均获得显著提升。

Conclusion: 利用全局演化拓扑对于高效稳健的自动化alpha发现至关重要。AlphaPROBE通过将因子池建模为动态互联生态系统，解决了现有方法的局限性。

Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.

</details>


### [20] [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964)
*Romain Froger,Pierre Andrews,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Gerard Moreno-Torres Bertran,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Vladislav Vorotilov,Mengjue Wang,Ian Yu,Amine Benhalloum,Grégoire Mialon,Thomas Scialom*

Main category: cs.AI

TL;DR: Gaia2是一个用于评估LLM智能体在异步、动态环境中的基准测试，包含时间约束、噪声事件、模糊性和多智能体协作等真实场景，支持细粒度动作级评估和强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准多为静态或同步环境，无法反映真实世界中环境独立于智能体动作演化、存在时间约束和动态事件等复杂情况，需要新的基准来评估智能体在异步环境中的实际能力。

Method: 基于开源Agents Research Environments平台构建消费者环境，设计包含环境独立演化、时间约束、噪声动态事件、模糊性解析和多智能体协作的场景，每个场景配备写动作验证器进行细粒度动作级评估。

Result: 评估显示各模型在不同能力间存在权衡：GPT-5（高）总体得分最高（42% pass@1）但在时间敏感任务上失败，Claude-4 Sonnet在准确性和速度间权衡以降低成本，Kimi-K2在开源模型中领先（21% pass@1）。

Conclusion: Gaia2揭示了推理、效率、鲁棒性之间的基本权衡，暴露了"模拟到现实"差距的挑战，通过发布Gaia2和ARE框架为社区提供开发、评估和训练下一代实用智能体系统的灵活基础设施。

Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the "sim2real" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.

</details>


### [21] [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083)
*Antonin Sulc*

Main category: cs.AI

TL;DR: 提出可微分模态逻辑（DML）和模态逻辑神经网络（MLNNs），通过神经符号方法从行为数据中学习信任网络、因果链和监管边界，用于多智能体系统的语义故障调试。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统从简单聊天机器人发展为自主集群，调试语义故障需要推理知识、信念、因果关系和义务，这正是模态逻辑设计用来形式化的内容。但传统模态逻辑需要手动指定在真实系统中未知或动态的关系结构。

Method: 提出可微分模态逻辑（DML），通过模态逻辑神经网络（MLNNs）实现，使系统能够仅从行为数据中学习信任网络、因果链和监管边界。通过四种模态构建统一的神经符号调试框架：认知（信任谁）、时序（事件何时导致故障）、道义（允许什么行动）和信念（如何解释智能体置信度）。

Result: 在具体多智能体场景中演示了每种模态，从发现外交游戏中的欺骗联盟到检测LLM幻觉。逻辑矛盾成为可学习的优化目标。提供了可执行的Jupyter笔记本代码。

Conclusion: 为神经符号社区提供了关键贡献：1）可解释的学习结构，信任和因果关系是显式参数而非不透明嵌入；2）通过可微分公理进行知识注入，用稀疏数据指导学习；3）组合多模态推理，结合认知、时序和道义约束；4）实用的部署模式，用于监控、主动控制和多智能体系统通信。

Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.
  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety](https://arxiv.org/abs/2602.11157)
*Max Zhang,Derek Liu,Kai Zhang,Joshua Franco,Haihao Liu*

Main category: cs.CL

TL;DR: 研究探索知識蒸餾在多語言越獄防禦中的應用，發現使用教師模型的"安全"拒絕數據進行標準微調反而會增加學生模型的越獄成功率，揭示了多語言安全對齊的挑戰。


<details>
  <summary>Details</summary>
Motivation: 大型語言模型的安全對齊主要集中於英語，導致非英語尤其是低資源語言存在安全漏洞。需要探索多語言環境下的安全防護方法。

Method: 使用知識蒸餾技術，將OpenAI o1-mini教師模型的拒絕行為通過LoRA參數高效微調方式，蒸餾到三個開源學生模型（Meta-Llama-3-8B-Instruct、Gemma-2-2B-IT、Qwen3-8B），使用XSafety的約28,000個多語言越獄提示進行黑盒響應式訓練。

Result: 反直覺發現：標準微調教師模型的"安全"拒絕數據反而使所有學生模型的越獄成功率增加（最高達16.6個百分點）。移除"邊界"拒絕響應可減緩安全性能下降，但推理性能（GSM8K）仍會降低。不同基礎模型在蒸餾過程中對未見語言表現出不同的泛化能力。

Conclusion: 知識蒸餾作為多語言安全對齊技術面臨挑戰，但具有潛力。需要進一步研究如何平衡安全性和模型性能，特別是在多語言環境中。

Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher's ``safe'' refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced `boundary' refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.

</details>


### [23] [PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models](https://arxiv.org/abs/2602.11170)
*Jiawei Xu,Zhenyu Yu,Ziqian Bi,Minh Duc Pham,Xiaoyi Qu,Danyang Zhang*

Main category: cs.CL

TL;DR: PRIME框架通过三个专门化智能体（执行器、验证器、协调器）和群体相对策略优化，显著提升大语言模型在算法推理任务上的性能，在PRIME-Bench基准上实现从26.8%到93.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样化推理任务上表现出色，但在算法推理方面仍存在局限性，需要专门的方法来提升其算法推理能力。

Method: 提出PRIME框架，包含三个专门化智能体：执行器负责逐步推理，验证器进行约束检查，协调器控制回溯机制，通过群体相对策略优化进行优化。同时构建了PRIME-Bench基准，包含86个任务、12个类别、51,600个实例。

Result: PRIME将平均准确率从26.8%提升到93.8%（相对提升250%）。在需要持续状态跟踪的任务上改进最大：图灵机模拟从9%提升到92%，长除法从16%提升到94%。消融研究表明迭代验证是主要贡献因素。小模型受益更大，能达到比其大8倍的模型的准确率。

Conclusion: PRIME框架通过专门化智能体协作和迭代验证机制，有效解决了大语言模型在算法推理中的错误传播问题，显著提升了算法推理能力，特别是对小模型效果更明显。

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.

</details>


### [24] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 提出了Mis-Align Bench基准，用于同时评估大语言模型在安全、价值和文化三个维度上的对齐问题，解决了现有基准只能单独评估单一维度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）只能单独评估安全、价值或文化等单一维度的对齐问题，而现实世界中这些维度需要同时满足。这种分离评估无法反映模型在真实场景中的综合对齐表现。

Method: 1. 构建SAVACU数据集：从LLM-PROMPT-DATASET重新分类382,424个样本，涵盖112个领域（14个安全领域、56个价值领域、42个文化领域），使用Mistral-7B-Instruct-v0.3进行分类，并通过Llama-3.1-8B-Instruct扩展低资源领域，使用SimHash指纹避免重复。2. 通过两阶段拒绝采样为每个提示配对对齐和不对齐的响应。3. 在通用、微调和开源权重LLM上进行基准测试，系统评估三个维度的对齐情况。

Result: 实验表明，单一维度模型在覆盖率上表现良好（最高97.6%），但在联合条件下的误报率超过50%，对齐分数较低（63%-66%）。这说明现有模型在同时满足多个对齐维度方面存在明显不足。

Conclusion: 需要开发能够同时处理安全、价值和文化多个对齐维度的模型，Mis-Align Bench为系统评估这种综合对齐能力提供了重要工具，揭示了当前模型在现实世界复杂场景中的局限性。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [25] [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005)
*Szilvia Ujváry,Louis Béthune,Pierre Ablin,João Monteiro,Marco Cuturi,Michael Kirchhof*

Main category: cs.CL

TL;DR: LaCy：一种新的预训练方法，通过语法解析器帮助小语言模型识别哪些token应该学习预测，哪些应该委托给大模型，以减少事实错误。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）参数容量有限，容易产生事实错误。虽然可以通过访问外部资源（如大模型、文档或数据库）来缓解，但需要解决哪些token应该学习预测、哪些应该委托给外部资源的核心问题。

Method: 提出LaCy方法，使用spaCy语法解析器增强损失信号，识别哪些token是"可接受的"（真实的替代延续），哪些应该触发<CALL>委托。基于这种token选择哲学进行预训练。

Result: LaCy模型成功学会了哪些token应该预测、哪些应该委托。在与大模型级联生成时获得更高的FactScore，性能优于Rho或LLM-judge训练的SLMs，同时更简单、更便宜。

Conclusion: 通过语法解析器增强损失信号可以有效指导小语言模型学习token委托策略，提高事实准确性，为小模型与大模型协作提供了有效方法。

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [26] [H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model](https://arxiv.org/abs/2602.11291)
*Wenyuan Chen,Jinbang Huang,Oscar Pang,Zhiyuan Li,Xiao Hu,Lingfeng Zhang,Zhanguang Zhang,Mark Coates,Tongtong Cao,Xingyue Quan,Yingxue Zhang*

Main category: cs.RO

TL;DR: 提出分层世界模型(H-WM)，结合高层逻辑世界模型和低层视觉世界模型，统一预测逻辑和视觉状态转换，解决机器人长时程规划中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法多关注视频生成或自然语言预测，难以直接应用于机器人动作，且在长时程任务中误差会累积。传统任务与运动规划使用符号逻辑世界模型，虽可执行且鲁棒，但缺乏视觉感知的同步预测能力。

Method: 提出分层世界模型(H-WM)，采用双层框架：高层逻辑世界模型预测符号状态转换，低层视觉世界模型预测视觉观测。通过机器人数据集训练，对齐机器人运动、符号状态、动作和视觉观测。

Result: 在视觉-语言-动作(VLA)控制策略实验中验证了方法的有效性和通用性。分层输出为长时程任务提供稳定一致的中间指导，减少误差累积，提升扩展任务序列的鲁棒执行能力。

Conclusion: H-WM成功整合了符号推理的机器人可执行性和长时程鲁棒性，以及视觉观测的感知基础，为机器人规划提供了统一的分层世界模型框架。

Abstract: World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

</details>


### [27] [ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control](https://arxiv.org/abs/2602.11321)
*Ziyan Xiong,Lixing Fang,Junyun Huang,Kashu Yamazaki,Hao Zhang,Chuang Gan*

Main category: cs.RO

TL;DR: ExtremControl：一种低延迟全身控制框架，通过SE(3)姿态直接操作、笛卡尔空间映射和速度前馈控制，实现50ms端到端延迟的人形机器人遥操作系统。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人遥操作系统依赖大量预处理的动作重定向和位置控制，导致高延迟（约200ms），限制了响应性，无法完成需要快速反馈和反应的任务。

Method: 1) 直接在选定刚性链接（主要是人形机器人末端）的SE(3)姿态上操作，避免全身重定向；2) 使用笛卡尔空间映射直接将人类动作转换为机器人链接目标；3) 在底层加入速度前馈控制以支持快速响应。

Result: 实现了端到端延迟低至50ms的遥操作系统，支持光学动作捕捉和VR运动跟踪，能够完成乒乓球平衡、杂耍和实时回击等高响应性行为，显著超越了之前200ms的延迟限制。

Conclusion: ExtremControl框架通过简化控制流程和加入前馈控制，成功构建了低延迟的人形机器人遥操作系统，为收集动态演示和执行需要快速反应的任务提供了有效解决方案。

Abstract: Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.

</details>
