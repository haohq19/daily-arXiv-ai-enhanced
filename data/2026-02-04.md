<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 22]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency](https://arxiv.org/abs/2602.03039)
*Geonhui Son,Jeong Ryong Lee,Dosik Hwang*

Main category: cs.CV

TL;DR: HP-GAN通过FakeTwins自监督损失和判别器一致性机制，利用预训练网络提升GAN图像生成质量和多样性


<details>
  <summary>Details</summary>
Motivation: 现有方法主要利用预训练网络计算感知损失或特征空间，但未能充分利用神经网络先验。本文旨在通过创新的自监督学习和判别器一致性机制，更有效地利用预训练网络提升GAN性能。

Method: 提出HP-GAN方法，包含两个核心策略：1) FakeTwins：利用预训练网络作为编码器计算自监督损失，通过生成图像训练生成器；2) 判别器一致性：在基于CNN和ViT特征网络的判别器之间建立一致性机制，促进协同学习并增强训练鲁棒性。

Result: 在17个数据集（包括大规模、小规模和有限数据场景，覆盖多种图像领域）上的广泛评估表明，HP-GAN在Fréchet Inception Distance（FID）指标上持续优于当前最先进方法，在图像多样性和质量方面取得显著改进。

Conclusion: HP-GAN通过创新的自监督学习和判别器一致性机制，有效利用了神经网络先验，显著提升了GAN的图像生成质量和多样性，在各种数据集和场景中均表现出优越性能。

Abstract: Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: https://github.com/higun2/HP-GAN.

</details>


### [2] [EventFlash: Towards Efficient MLLMs for Event-Based Vision](https://arxiv.org/abs/2602.03230)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Wen Jiang,Ming Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventFlash是一种高效的事件驱动多模态大语言模型，通过时空令牌稀疏化减少数据冗余并加速推理，相比基线模型实现12.4倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于事件的多模态大语言模型通常采用密集的图像式处理范式，忽视了事件流的时空稀疏性，导致计算成本高昂。需要一种更高效的方法来处理事件数据。

Method: 1) 构建EventMind大规模多样化场景数据集(超过50万指令集)；2) 设计自适应时间窗口聚合模块进行高效时间采样；3) 开发稀疏密度引导注意力模块提高空间令牌效率。

Result: EventFlash相比基线模型(EventFlash-Zero)实现12.4倍吞吐量提升，同时保持可比性能。支持处理长达1000个时间仓的事件流，显著优于EventGPT的5仓限制。

Conclusion: EventFlash通过探索时空令牌稀疏化，有效减少了事件数据冗余并加速了推理，为基于事件视觉的高效基础模型提供了新方案。

Abstract: Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.

</details>


### [3] [A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.03604)
*Basile Terver,Randall Balestriero,Megi Dervishi,David Fan,Quentin Garrido,Tushar Nagarajan,Koustuv Sinha,Wancong Zhang,Mike Rabbat,Yann LeCun,Amir Bar*

Main category: cs.CV

TL;DR: EB-JEPA是一个开源库，用于通过联合嵌入预测架构学习表示和世界模型，支持从图像到视频再到动作条件世界模型的扩展，提供模块化实现和单GPU训练。


<details>
  <summary>Details</summary>
Motivation: JEPA通过在表示空间而非像素空间进行预测，避免了生成建模的缺陷，同时捕获适合下游任务的语义特征。需要使基于能量的自监督学习更易于研究和教育。

Method: 提供模块化、自包含的JEPA实现，将图像级自监督学习技术扩展到视频（处理时间动态）和动作条件世界模型（预测控制输入效果）。支持单GPU训练。

Result: 在CIFAR-10上获得91%的表示探测准确率；在Moving MNIST上展示多步预测；在Two Rooms导航任务中达到97%的规划成功率。消融研究揭示了各正则化组件防止表示崩溃的关键重要性。

Conclusion: EB-JEPA成功展示了JEPA框架从静态图像到时间视频再到动作条件世界模型的可扩展性，为研究和教育提供了实用的自监督学习工具。

Abstract: We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.

</details>


### [4] [Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images](https://arxiv.org/abs/2602.03669)
*Sandeep Patil,Yongqi Dong,Haneen Farah,Hans Hellendoorn*

Main category: cs.CV

TL;DR: 提出一种带时空注意力机制的序列神经网络模型，用于车道线检测，在遮挡和强光等困难场景下表现优异，且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 当前车道检测方法缺乏多功能性，特别是在混合交通环境中，视觉方法常忽略图像关键区域和时空显著性，导致在严重遮挡和强光等困难情况下性能不佳。

Method: 提出基于标准编码器-解码器结构和常见神经网络骨干的序列神经网络模型，引入时空注意力机制，专注于车道线关键特征并利用连续图像帧间的时空相关性。

Result: 在三个大规模开源数据集上的实验表明，该模型在各种测试场景中优于最先进方法，具有更强的鲁棒性。时空注意力机制使模型参数更少、MACs更低，计算效率更高。

Conclusion: 提出的带时空注意力机制的序列神经网络模型在车道检测任务中表现出色，特别是在困难场景下，同时保持了计算效率，为自动驾驶系统提供了有效的解决方案。

Abstract: Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.

</details>


### [5] [LIVE: Long-horizon Interactive Video World Modeling](https://arxiv.org/abs/2602.03747)
*Junchao Huang,Ziyang Ye,Xinting Hu,Tianyu He,Guiyu Zhang,Shaoshuai Shi,Jiang Bian,Li Jiang*

Main category: cs.CV

TL;DR: LIVE提出了一种通过循环一致性约束来限制误差累积的长时域交互视频世界模型，无需教师蒸馏，在长时域视频生成上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自回归视频世界模型在短时域预测有效，但在长时域生成中，小的预测误差会随时间累积，导致生成质量下降。现有方法引入预训练教师模型和序列级分布匹配，但计算成本高且无法防止超出训练时域的误差传播。

Method: LIVE通过新颖的循环一致性目标强制有界误差累积：1）从真实帧进行前向展开；2）应用反向生成过程重建初始状态；3）在重建的终止状态上计算扩散损失，显式约束长时域误差传播。还提供了统一视角和渐进训练课程来稳定训练。

Result: 实验表明LIVE在长时域基准测试中达到最先进性能，能够生成超出训练展开长度的稳定、高质量视频。

Conclusion: LIVE通过循环一致性约束有效解决了长时域视频生成中的误差累积问题，无需教师蒸馏，在长时域视频世界建模中表现出色。

Abstract: Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.

</details>


### [6] [EventNeuS: 3D Mesh Reconstruction from a Single Event Camera](https://arxiv.org/abs/2602.03847)
*Shreyas Sachan,Viktor Rudnev,Mohamed Elgharib,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: EventNeuS：首个结合SDF和密度场学习的自监督神经模型，用于从单目彩色事件流进行3D重建，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 事件相机在许多场景中比RGB相机更有优势，但现有的事件相机3D重建方法精度严重受限，需要开发更准确的3D表示学习方法

Method: 结合3D有符号距离函数和密度场学习，引入事件监督和球谐编码来处理视角依赖效应，实现自监督学习

Result: 显著优于现有方法，平均Chamfer距离降低34%，平均绝对误差降低31%

Conclusion: EventNeuS首次成功将SDF和密度场学习与事件监督结合，为事件相机的3D重建提供了更准确的解决方案

Abstract: Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Sparse Adapter Fusion for Continual Learning in NLP](https://arxiv.org/abs/2602.02502)
*Min Zeng,Xi Chen,Haiqin Yang,Yike Guo*

Main category: cs.LG

TL;DR: SAFM是一种稀疏适配器融合方法，通过动态融合新旧适配器解决持续学习中的参数重用和灾难性遗忘问题，在性能相当的情况下使用少于60%的参数。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法面临三个主要挑战：1）跨任务参数重用效率低；2）任务不相似时容易发生灾难性遗忘；3）为每个任务不必要地引入新参数，阻碍相似任务间的知识共享。

Method: SAFM采用两阶段方法：决策阶段决定是引入新适配器、重用现有适配器还是添加空适配器，通过架构搜索优先重用或添加空适配器来最小化参数消耗；调优阶段采用分层损失鼓励适配器间差异化，有效捕获同一任务内的知识。

Result: 实验结果表明SAFM持续优于最先进方法，在性能相当的情况下使用少于60%的参数。

Conclusion: SAFM通过动态融合适配器有效解决了持续学习中的参数重用和灾难性遗忘问题，实现了高效的知识共享和参数利用。

Abstract: Continual learning in natural language processing plays a crucial role in adapting to evolving data and preventing catastrophic forgetting. Despite significant progress, existing methods still face challenges, such as inefficient parameter reuse across tasks, risking catastrophic forgetting when tasks are dissimilar, and the unnecessary introduction of new parameters for each task, which hampers knowledge sharing among similar tasks. To tackle these issues, we propose a Sparse Adapter Fusion Method (SAFM), which dynamically fuses old and new adapters to address these challenges. SAFM operates in two stages: the decision stage and the tuning stage. In the decision stage, SAFM determines whether to incorporate a new adapter, reuse an existing one, or add an empty adapter. The architecture search procedure, designed to prioritize reusing or adding empty adapters, minimizes parameter consumption and maximizes reuse. In the tuning stage, SAFM especially facilitates a layer-wise loss to encourage differentiation between adapters, effectively capturing knowledge within the same task. Experimental results consistently show that SAFM outperforms state-of-the-art (SOTA) methods, achieving comparable performance while utilizing less than 60% of the parameters.

</details>


### [8] [From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation](https://arxiv.org/abs/2602.02536)
*Tianle Gu,Kexin Huang,Lingyu Li,Ruilin Luo,Shiyang Huang,Zongqi Wang,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: UniMod提出了一种新的多模态安全审核学习范式，通过结构化推理轨迹替代稀疏二元标签，防止模型学习捷径，使用多维度奖励模型和监督策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态安全审核面临数据和监督稀疏的问题，传统二元标签导致模型学习捷径，无法学习有效的分类边界，需要更精细的推理过程。

Method: 提出UniMod学习范式，构建包含证据定位、模态评估、风险映射、策略决策和响应生成的结构化推理轨迹；开发多头部标量奖励模型UniRM提供多维度监督；引入专门优化策略解耦任务参数并平衡训练动态。

Result: UniMod在文本审核上取得竞争性性能，在多模态审核上建立新基准，仅使用领先基线不到40%的训练数据；消融实验验证了多属性轨迹推理的有效性。

Conclusion: UniMod为多模态审核提供了一个有效且高效的框架，通过密集推理轨迹替代稀疏决策，防止捷径学习，提升模型的安全语义理解能力。

Abstract: Safety moderation is pivotal for identifying harmful content. Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision. Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination. Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces. By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process. This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts. To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM). UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage. Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning. Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal benchmark using less than 40\% of the training data used by leading baselines. Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation. Supplementary materials are available at \href{https://trustworthylab.github.io/UniMod/}{project website}.

</details>


### [9] [naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement](https://arxiv.org/abs/2602.02547)
*Hankyeol Kim,Pilsung Kang*

Main category: cs.LG

TL;DR: naPINN是一种噪声自适应物理信息神经网络，通过嵌入能量模型学习预测残差的潜在分布，使用可训练可靠性门自适应过滤高能量数据点，有效处理非高斯噪声和异常值，显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在复杂测量噪声和严重异常值下性能显著下降，需要一种无需先验噪声分布知识就能从损坏测量中稳健恢复物理解的方法。

Method: naPINN在训练循环中嵌入能量模型学习预测残差的潜在分布，利用学习到的能量景观，通过可训练可靠性门自适应过滤高能量数据点，同时使用拒绝成本正则化防止有效数据被丢弃的平凡解。

Result: 在多种受非高斯噪声和不同异常值率损坏的基准偏微分方程上，naPINN显著优于现有鲁棒PINN基线，成功隔离异常值并在严重数据损坏下准确重建动力学。

Conclusion: naPINN能够从损坏测量中稳健恢复物理解，无需先验噪声分布知识，为处理复杂噪声和异常值的逆问题和控制方程发现提供了有效解决方案。

Abstract: Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.

</details>


### [10] [Discovering Data Manifold Geometry via Non-Contracting Flows](https://arxiv.org/abs/2602.02611)
*David Vigouroux,Lucas Drumetz,Ronan Fablet,François Rousseau*

Main category: cs.LG

TL;DR: 提出一种无监督方法，通过在学习环境空间中学习向量场来构建全局参考系统，这些向量场跨越未知数据流形的切空间，将样本传输到共同参考点，从而定义可解释的内在坐标。


<details>
  <summary>Details</summary>
Motivation: 传统等距目标隐含假设流形平坦性，无法有效处理复杂流形结构。需要一种能够学习全局坐标图的方法，将数据点映射到共享参考框架中，提供可解释的内在坐标表示。

Method: 学习切向量场，其流将所有样本传输到共同可学习参考点。使用非收缩约束防止退化崩溃，并推导出受流匹配启发的可扩展、无需积分的优化目标。在理论框架中证明最小化该目标可恢复全局坐标图。

Result: 在合成流形上获得正确的切向对齐和一致的全局坐标结构。在CIFAR-10上展示方法可扩展性，学习到的坐标在下游分类任务中达到有竞争力的性能。

Conclusion: 该方法能够无监督地学习数据流形的全局坐标系统，提供可解释的内在表示，在合成和真实数据集上都表现出有效性，为流形学习提供了新视角。

Abstract: We introduce an unsupervised approach for constructing a global reference system by learning, in the ambient space, vector fields that span the tangent spaces of an unknown data manifold. In contrast to isometric objectives, which implicitly assume manifold flatness, our method learns tangent vector fields whose flows transport all samples to a common, learnable reference point. The resulting arc-lengths along these flows define interpretable intrinsic coordinates tied to a shared global frame. To prevent degenerate collapse, we enforce a non-shrinking constraint and derive a scalable, integration-free objective inspired by flow matching. Within our theoretical framework, we prove that minimizing the proposed objective recovers a global coordinate chart when one exists. Empirically, we obtain correct tangent alignment and coherent global coordinate structure on synthetic manifolds. We also demonstrate the scalability of our method on CIFAR-10, where the learned coordinates achieve competitive downstream classification performance.

</details>


### [11] [MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields](https://arxiv.org/abs/2602.02671)
*Francesco Leonardi,Boris Bonev,Kaspar Riesen*

Main category: cs.LG

TL;DR: MARA模块将球形注意力扩展到分子领域，通过灵活加权局部几何相互作用提升机器学习力场的表达能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习力场大多依赖固定的角度展开，限制了局部几何相互作用加权的灵活性，需要更灵活、几何感知的模块来提升模型性能。

Method: 提出模块化角度-径向注意力(MARA)，将球形注意力扩展到SE(3)等变架构，直接操作相邻原子的角度和径向坐标，实现灵活、几何感知的局部环境加权。

Result: 在分子基准测试中，MARA改善了能量和力预测，减少了高误差事件，增强了模型鲁棒性，且能即插即用地集成到MACE等模型中。

Conclusion: 连续球形注意力是有效且可泛化的几何算子，能提高原子模型的表达能力、稳定性和可靠性。

Abstract: Machine learning force fields (MLFFs) have become essential for accurate and efficient atomistic modeling. Despite their high accuracy, most existing approaches rely on fixed angular expansions, limiting flexibility in weighting local geometric interactions. We introduce Modular Angular-Radial Attention (MARA), a module that extends spherical attention -- originally developed for SO(3) tasks -- to the molecular domain and SE(3), providing an efficient approximation of equivariant interactions. MARA operates directly on the angular and radial coordinates of neighboring atoms, enabling flexible, geometrically informed, and modular weighting of local environments. Unlike existing attention mechanisms in SE(3)-equivariant architectures, MARA can be integrated in a plug-and-play manner into models such as MACE without architectural modifications. Across molecular benchmarks, MARA improves energy and force predictions, reduces high-error events, and enhances robustness. These results demonstrate that continuous spherical attention is an effective and generalizable geometric operator that increases the expressiveness, stability, and reliability of atomistic models.

</details>


### [12] [Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data](https://arxiv.org/abs/2602.02766)
*Lucas Rosenblatt,Peihan Liu,Ryan McKenna,Natalia Ponomareva*

Main category: cs.LG

TL;DR: PATH：基于差分隐私的大型语言模型框架，用于合成纵向表格数据，相比传统边际方法能更好地保持时间连贯性和长程依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私合成表格数据研究主要关注独立同分布的行数据，忽略了纵向数据集（如电子健康记录）的时间复杂性。传统方法将用户历史扁平化为高维向量会破坏时间连贯性。

Method: 提出PATH框架，将完整表格作为合成单元，利用差分隐私微调的大型语言模型的自回归能力来生成数据，保持时间序列的连贯性。

Result: PATH相比主流边际方法，将真实轨迹的分布距离减少60%以上，状态转移错误减少近50%，同时保持相似的边际保真度。

Conclusion: PATH框架能有效捕捉传统方法忽略的长程依赖关系，为纵向数据的差分隐私合成提供了更优解决方案。

Abstract: Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.

</details>


### [13] [Provable Effects of Data Replay in Continual Learning: A Feature Learning Perspective](https://arxiv.org/abs/2602.02767)
*Meng Ding,Jinhui Xu,Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文从特征学习角度建立了持续学习中全数据回放的理论框架，发现信噪比是影响遗忘的关键因素，即使全数据回放下遗忘仍可能发生，但足够信号积累可恢复早期任务，且任务排序策略（优先高信号任务）有助于防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘的核心挑战，数据回放方法虽然简单有效，但其理论有效性（特别是全数据回放）尚未得到充分探索。本文旨在从特征学习角度建立理论框架，分析全数据回放在持续学习中的效果。

Method: 采用多视图数据模型，以信噪比（SNR）为关键分析因素，聚焦于M个任务的增量二元分类问题。通过理论分析验证两个关键结论，并在合成和真实世界实验中可视化信号学习与噪声记忆的相互作用。

Result: 1) 即使在全数据回放下，当后续任务的累积噪声主导早期任务信号时，遗忘仍会发生；2) 通过足够的信号积累，数据回放可以恢复早期任务，即使其初始学习效果不佳；3) 发现任务排序的新见解：优先处理高信号任务不仅有助于学习低信号任务，还能防止灾难性遗忘。

Conclusion: 本文建立了持续学习中全数据回放的理论框架，揭示了信噪比在遗忘机制中的关键作用，提出了任务排序策略的新见解，为理解数据回放方法的理论边界和优化持续学习算法提供了重要指导。

Abstract: Continual learning (CL) aims to train models on a sequence of tasks while retaining performance on previously learned ones. A core challenge in this setting is catastrophic forgetting, where new learning interferes with past knowledge. Among various mitigation strategies, data-replay methods, where past samples are periodically revisited, are considered simple yet effective, especially when memory constraints are relaxed. However, the theoretical effectiveness of full data replay, where all past data is accessible during training, remains largely unexplored. In this paper, we present a comprehensive theoretical framework for analyzing full data-replay training in continual learning from a feature learning perspective. Adopting a multi-view data model, we identify the signal-to-noise ratio (SNR) as a critical factor affecting forgetting. Focusing on task-incremental binary classification across $M$ tasks, our analysis verifies two key conclusions: (1) forgetting can still occur under full replay when the cumulative noise from later tasks dominates the signal from earlier ones; and (2) with sufficient signal accumulation, data replay can recover earlier tasks-even if their initial learning was poor. Notably, we uncover a novel insight into task ordering: prioritizing higher-signal tasks not only facilitates learning of lower-signal tasks but also helps prevent catastrophic forgetting. We validate our theoretical findings through synthetic and real-world experiments that visualize the interplay between signal learning and noise memorization across varying SNRs and task correlation regimes.

</details>


### [14] [BiTimeCrossNet: Time-Aware Self-Supervised Learning for Pediatric Sleep](https://arxiv.org/abs/2602.02769)
*Saurav Raj Pandey,Harlin Lee*

Main category: cs.LG

TL;DR: BiTimeCrossNet (BTCNet) 是一个用于长时间生理记录的多模态自监督学习框架，通过结合时间信息和跨模态注意力，在睡眠研究等任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将短片段视为独立样本进行训练，忽略了片段在记录中的时间位置信息（如在睡眠会话中的位置）。同时，现有方法也缺乏对生理信号间交互关系的有效建模。

Method: BTCNet 包含两个关键创新：1) 时间感知设计，将每个片段在其父记录中的时间位置信息纳入学习；2) 跨模态注意力机制，学习不同生理信号之间的成对交互关系。整个框架无需任务标签或序列级监督。

Result: 在儿科睡眠数据的六个下游任务（睡眠分期、觉醒检测、呼吸事件检测等）评估中，BTCNet 在冻结骨干网络线性探测设置下，始终优于非时间感知的变体，且性能提升可推广到独立的儿科数据集。与现有多模态自监督睡眠模型相比，BTCNet 表现强劲，尤其在呼吸相关任务上。

Conclusion: BTCNet 通过整合时间上下文和跨模态交互，为长时间生理记录提供了有效的自监督学习框架，在儿科睡眠分析任务中展现出优越性能，特别是在呼吸相关检测方面。

Abstract: We present BiTimeCrossNet (BTCNet), a multimodal self-supervised learning framework for long physiological recordings such as overnight sleep studies. While many existing approaches train on short segments treated as independent samples, BTCNet incorporates information about when each segment occurs within its parent recording, for example within a sleep session. BTCNet further learns pairwise interactions between physiological signals via cross-attention, without requiring task labels or sequence-level supervision.
  We evaluate BTCNet on pediatric sleep data across six downstream tasks, including sleep staging, arousal detection, and respiratory event detection. Under frozen-backbone linear probing, BTCNet consistently outperforms an otherwise identical non-time-aware variant, with gains that generalize to an independent pediatric dataset. Compared to existing multimodal self-supervised sleep models, BTCNet achieves strong performance, particularly on respiration-related tasks.

</details>


### [15] [Self-Hinting Language Models Enhance Reinforcement Learning](https://arxiv.org/abs/2602.03143)
*Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian*

Main category: cs.LG

TL;DR: SAGE通过注入特权提示来增加组内结果多样性，解决GRPO在稀疏奖励下优势崩溃的问题，无需测试时提示即可提升模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: GRPO在稀疏终端奖励下经常停滞，因为组内rollout经常获得相同奖励，导致相对优势崩溃和更新消失，需要解决这一问题来提升模型对齐效果。

Method: 提出SAGE框架，在训练时注入特权提示来重塑rollout分布，模型采样紧凑提示（如计划或分解）后生成解决方案，保持任务奖励不变，仅增加组内结果多样性。

Result: 在6个基准测试和3个LLM上的实验显示，SAGE始终优于GRPO，平均提升：Llama-3.2-3B-Instruct +2.0，Qwen2.5-7B-Instruct +1.2，Qwen3-4B-Instruct +1.3。

Conclusion: SAGE通过自我提示作为自适应课程，有效跟踪学习瓶颈，防止GRPO在稀疏奖励下优势崩溃，无需测试时特权信息即可提升模型对齐性能。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.

</details>


### [16] [Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels](https://arxiv.org/abs/2602.02917)
*Yunsung Chung,Keum San Chun,Migyeong Gwak,Han Feng,Yingshuo Liu,Chanho Lim,Viswam Nathan,Nassir Marrouche,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出一种基于时间衰减的样本权重学习策略，用于解决PPG生物信号与临床标签时间间隔导致的监督不可靠问题，在10种生物标志物上提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备和AI的发展推动了基于PPG的健康监测，但临床标签稀疏性使得距离实验室抽血时间较远的生物信号监督不可靠，需要解决时间间隔导致的标签可靠性下降问题。

Method: 提出一种训练策略，学习生物标志物特定的样本权重衰减函数，根据生物信号段与其真实标签的时间间隔调整样本权重，并在损失函数中使用正则化防止平凡解。

Result: 在450名参与者的智能手表PPG数据和10种生物标志物上，该方法优于基线模型：平均AUPRC为0.715，优于自监督基线（0.674）和随机森林（0.626）。线性衰减函数表现最稳健。

Conclusion: 该方法不仅提高了预测准确性，还通过学习到的衰减率提供了生物标志物PPG证据随时间失效速度的可解释性视图，揭示了不同生物标志物的时间敏感性差异。

Abstract: Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.

</details>


### [17] [Rare Event Early Detection: A Dataset of Sepsis Onset for Critically Ill Trauma Patients](https://arxiv.org/abs/2602.02930)
*Yin Jin,Tucker R. Stewart,Deyi Zhou,Chhavi Gupta,Arjita Nema,Scott C. Brakenridge,Grant E. O'Keefe,Juhua Hu*

Main category: cs.LG

TL;DR: 该研究创建了一个针对创伤患者脓毒症发作的公开数据集，并建立了早期检测的基准，强调了创伤患者脓毒症检测的特殊挑战。


<details>
  <summary>Details</summary>
Motivation: 脓毒症具有高发病率、死亡率和医疗成本，早期检测可显著改善临床结果。现有公开数据集将ICU患者视为同质群体，忽视了创伤危重患者的特殊挑战，其损伤相关炎症和器官功能障碍可能与脓毒症临床特征重叠。

Method: 从MIMIC-III数据库中提取、重新标记并验证了一个标准化的创伤后脓毒症发作公开数据集。根据ICU临床工作流程，将创伤后脓毒症早期检测框架化为每日基础的罕见事件检测问题，并通过综合实验建立通用基准。

Result: 创建了首个针对创伤患者脓毒症发作的公开数据集，建立了早期检测的基准，结果表明现有方法在该新数据集上表现有限，需要进一步的技术进步。

Conclusion: 针对创伤患者的脓毒症检测需要专门的数据集和方法，该研究提供的公开数据集和基准将推动创伤后脓毒症早期检测技术的发展。

Abstract: Sepsis is a major public health concern due to its high morbidity, mortality, and cost. Its clinical outcome can be substantially improved through early detection and timely intervention. By leveraging publicly available datasets, machine learning (ML) has driven advances in both research and clinical practice. However, existing public datasets consider ICU patients (Intensive Care Unit) as a uniform group and neglect the potential challenges presented by critically ill trauma patients in whom injury-related inflammation and organ dysfunction can overlap with the clinical features of sepsis. We propose that a targeted identification of post-traumatic sepsis is necessary in order to develop methods for early detection. Therefore, we introduce a publicly available standardized post-trauma sepsis onset dataset extracted, relabeled using standardized post-trauma clinical facts, and validated from MIMIC-III. Furthermore, we frame early detection of post-trauma sepsis onset according to clinical workflow in ICUs in a daily basis resulting in a new rare event detection problem. We then establish a general benchmark through comprehensive experiments, which shows the necessity of further advancements using this new dataset. The data code is available at https://github.com/ML4UWHealth/SepsisOnset_TraumaCohort.git.

</details>


### [18] [Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization](https://arxiv.org/abs/2602.02958)
*Haocheng Xi,Shuo Yang,Yilong Zhao,Muyang Li,Han Cai,Xingyang Li,Yujun Lin,Zhuoyang Zhang,Jintao Zhang,Xiuyu Li,Zhiying Xu,Jun Wu,Chenfeng Xu,Ion Stoica,Song Han,Kurt Keutzer*

Main category: cs.LG

TL;DR: QVG是一个无需训练的KV缓存量化框架，通过语义感知平滑和渐进残差量化技术，将自回归视频扩散模型的KV缓存内存减少高达7倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型中KV缓存内存随生成历史增长，通常超过30GB，限制了在广泛可用硬件上的部署能力，并损害了长时程生成的一致性（身份、布局、运动）。

Method: 提出Quant VideoGen (QVG)框架：1) 语义感知平滑利用视频时空冗余产生低幅值、量化友好的残差；2) 渐进残差量化采用从粗到细的多阶段方案减少量化误差，实现质量与内存的平滑权衡。

Result: 在LongCat Video、HY WorldPlay和Self Forcing基准测试中，QVG将KV缓存内存减少高达7.0倍，端到端延迟开销小于4%，在生成质量上始终优于现有基线，建立了质量与内存效率的新帕累托前沿。

Conclusion: QVG通过高效的KV缓存量化解决了自回归视频扩散模型的内存瓶颈问题，实现了高质量视频生成与内存效率的平衡，为在广泛硬件上部署提供了可行方案。

Abstract: Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.

</details>


### [19] [TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT](https://arxiv.org/abs/2602.03073)
*Rana Muhammad Shahroz Khan,Zijie Liu,Zhen Tan,Charles Fleming,Tianlong Chen*

Main category: cs.LG

TL;DR: TMS是一种无需奖励的监督微调框架，通过使用模型历史检查点创建动态课程，近似RL的在线策略优势，在保持模型能力的同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法虽然能更好地保持模型能力，但需要复杂的奖励工程、不稳定且采样成本高；而SFT虽然高效但容易因监督不匹配导致灾难性遗忘。需要一种折中方案。

Method: 提出轨迹混合监督(TMS)框架，通过模型自身历史检查点创建动态课程，最小化策略-标签分歧(PLD)，避免标准SFT中的模式崩溃问题。

Result: 在推理(MATH, GSM8K)和指令遵循基准测试中，TMS有效改善了准确率-保持能力的帕累托前沿，显著优于标准和迭代SFT，接近RL效果但无需奖励模型或验证器。

Conclusion: TMS通过动态课程学习有效缓解了SFT中的灾难性遗忘问题，在保持模型能力方面显著优于传统SFT方法，为LLM微调提供了高效且稳定的替代方案。

Abstract: Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.

</details>


### [20] [Anomaly Detection via Mean Shift Density Enhancement](https://arxiv.org/abs/2602.03293)
*Pritam Kar,Rahul Bordoloi,Olaf Wolkenhauer,Saptarshi Bej*

Main category: cs.LG

TL;DR: 提出MSDE框架，通过密度增强过程中的几何位移来检测异常，在多种异常类型和噪声环境下表现稳健


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测算法在不同异常类型下表现不一致，在噪声环境下鲁棒性不足，需要一种更通用的方法

Method: 基于加权均值漂移过程，使用UMAP模糊邻域图计算样本特定密度权重，通过迭代密度增强过程中的累积位移定义异常分数

Result: 在ADBench基准测试中，相比13个基线方法，MSDE在AUC-ROC、AUC-PR和Precision@n等指标上表现一致且稳健

Conclusion: 基于位移的评分方法为无监督异常检测提供了鲁棒的替代方案，能够处理多种异常类型和噪声环境

Abstract: Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.

</details>


### [21] [Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models](https://arxiv.org/abs/2602.03309)
*Yuelin Hu,Zhengxue Cheng,Wei Liu,Li Song*

Main category: cs.LG

TL;DR: EGSPO是一种三阶段混合训练框架，通过基于预测熵的token级梯度调制，在数学推理基准上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有混合训练方法通常在样本级别结合监督微调和强化学习，但缺乏细粒度的梯度控制。EGSPO旨在通过token级别的梯度调制来更好地平衡探索与知识保留。

Method: 三阶段框架：1) SFT专家学习建立基础策略；2) RL轨迹生成并计算每个token的预测熵；3) EGSPO机制根据熵值进行梯度分配：高熵token使用完整PPO更新鼓励探索，低熵token使用衰减PPO更新减少方差并保留知识。

Result: 在数学推理基准上取得一致改进：AIME提升3.8%，MATH提升2.9%，仅增加3.4%的计算开销。

Conclusion: EGSPO通过token级别的熵门控梯度调制，有效平衡了探索与知识保留，在数学推理任务上实现了显著性能提升且计算开销小。

Abstract: Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.

</details>


### [22] [Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing](https://arxiv.org/abs/2602.03452)
*Xin Sheng,Jiaxin Li,Yujuan Pang,Ran Peng,Yong Ma*

Main category: cs.LG

TL;DR: 提出正负配对方法改进RLVR中的提示选择，通过同时采样难但可解的正提示和易但脆弱的负提示，配合加权GRPO提升样本效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法中的提示选择通常仅基于训练准确率方差，导致优化方向不稳定且迁移能力弱。需要从机制层面重新审视提示选择，确保小批量同时提供可靠的正锚点和明确的负学习信号

Method: 提出正负配对方法：每次更新采样一个难但可解的正提示q+（低经验成功率）和一个易但脆弱的负提示q-（高但不完美的成功率）。引入加权GRPO，在配对级别重新加权二元结果，使用组归一化优势来放大q+上的罕见成功为强正指导，同时将q-上的罕见失败转为强负惩罚

Result: 在Qwen2.5-Math-7B上，每次更新使用单个配对小批量持续优于基于方差选择启发式的GRPO基线：AIME 2025 Pass@8从16.8提升到22.2，AMC23 Pass@64从94.0提升到97.0，同时与从1209个训练提示池训练的大规模RLVR保持竞争力。在Qwen2.5-Math-7B-Instruct上也观察到类似增益

Conclusion: 正负配对方法通过提供双向学习信号（正指导和负惩罚），改善了样本效率而不抑制探索，为RLVR中的提示选择提供了更有效的机制级解决方案

Abstract: Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.

</details>


### [23] [Soft-Radial Projection for Constrained End-to-End Learning](https://arxiv.org/abs/2602.03461)
*Philipp J. Schneider,Daniel Kuhn*

Main category: cs.LG

TL;DR: 提出Soft-Radial Projection层，通过径向映射将点映射到可行集内部，解决传统正交投影层梯度饱和问题，保证严格可行性同时保持雅可比矩阵满秩。


<details>
  <summary>Details</summary>
Motivation: 在安全关键系统中，将硬约束集成到深度学习至关重要。现有将预测投影到约束边界的方法面临梯度饱和瓶颈：标准正交投影将外部点坍缩到低维表面，导致雅可比矩阵秩亏，使垂直于活动约束的梯度为零，阻碍优化。

Method: 引入Soft-Radial Projection层，这是一种可微分重参数化层，通过从欧几里得空间到可行集内部的径向映射来规避梯度饱和问题。该构造保证严格可行性，同时几乎处处保持满秩雅可比矩阵。

Result: 理论上证明该架构保留了通用逼近性质，实证显示相比最先进的基于优化和投影的基线方法，具有更好的收敛行为和解决方案质量。

Conclusion: Soft-Radial Projection层有效解决了传统投影层的梯度饱和问题，在保证严格可行性的同时改善了优化性能，适用于安全关键系统的深度学习约束集成。

Abstract: Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: gradient saturation. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the interior of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.

</details>


### [24] [Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation](https://arxiv.org/abs/2602.03515)
*Hyunji Jung,Sungbin Shin,Namhoon Lee*

Main category: cs.LG

TL;DR: 异步流水线并行通过基旋转修正延迟梯度，解决梯度陈旧性问题，在训练10亿参数LLM时比最佳异步基线减少76.8%迭代次数


<details>
  <summary>Details</summary>
Motivation: 异步流水线并行虽然能消除同步执行中的流水线气泡，最大化硬件利用率，但存在梯度陈旧性问题。延迟梯度会引入优化噪声，且延迟随流水线深度线性增长，这从根本上破坏了该方法原本要提供的可扩展性。

Method: 通过基旋转修正延迟梯度。研究发现延迟梯度的有害效应在海森矩阵特征基与标准坐标基不对齐时加剧，这阻碍了Adam等坐标自适应方案有效利用曲率感知能力。基旋转可以缓解这种对齐问题。

Result: 基旋转显著加速异步设置下的收敛。例如，在训练10亿参数LLM时，使用基旋转达到相同训练损失所需的迭代次数比最佳异步流水线并行基线减少76.8%。

Conclusion: 基旋转能够有效缓解异步流水线并行中的梯度延迟问题，恢复可扩展的异步训练同时保持性能，解决了延迟梯度随流水线深度线性增长这一根本性可扩展性障碍。

Abstract: Asynchronous pipeline parallelism maximizes hardware utilization by eliminating the pipeline bubbles inherent in synchronous execution, offering a path toward efficient large-scale distributed training. However, this efficiency gain can be compromised by gradient staleness, where the immediate model updates with delayed gradients introduce noise into the optimization process. Crucially, we identify a critical, yet often overlooked, pathology: this delay scales linearly with pipeline depth, fundamentally undermining the very scalability that the method originally intends to provide. In this work, we investigate this inconsistency and bridge the gap by rectifying delayed gradients through basis rotation, restoring scalable asynchronous training while maintaining performance. Specifically, we observe that the deleterious effects of delayed gradients are exacerbated when the Hessian eigenbasis is misaligned with the standard coordinate basis. We demonstrate that this misalignment prevents coordinate-wise adaptive schemes, such as Adam, from effectively leveraging curvature-aware adaptivity. This failure leads to significant oscillations in the optimization trajectory and, consequently, slower convergence. We substantiate these findings through both rigorous theoretical analysis and empirical evaluation. To address this challenge, we propose the use of basis rotation, demonstrating that it effectively mitigates the alignment issue and significantly accelerates convergence in asynchronous settings. For example, our training of a 1B-parameter LLM with basis rotation achieves the same training loss in 76.8% fewer iterations compared to the best-performing asynchronous pipeline parallel training baseline.

</details>


### [25] [Rank-Learner: Orthogonal Ranking of Treatment Effects](https://arxiv.org/abs/2602.03517)
*Henri Arno,Dennis Frauen,Emil Javurek,Thomas Demeester,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出Rank-Learner，一种直接学习处理效应排名的两阶段学习方法，无需精确估计因果效应，具有理论保证和模型无关性。


<details>
  <summary>Details</summary>
Motivation: 许多决策问题需要按处理效应排序个体（如优先医疗干预或广告投放），但现有研究主要关注精确估计因果效应，而直接学习排序的方法尚未充分探索。

Method: 提出Rank-Learner两阶段学习器：1）优化成对学习目标直接恢复处理效应排序，无需显式CATE估计；2）具有Neyman正交性，对干扰函数估计误差鲁棒；3）模型无关，可与任意机器学习模型结合。

Result: 在广泛实验中，Rank-Learner持续优于标准CATE估计器和非正交排序方法，为按处理效应排序个体提供了新的正交两阶段学习器。

Conclusion: Rank-Learner为实践者提供了直接学习处理效应排序的有效工具，解决了精确估计不必要的问题，具有理论保证和实际应用价值。

Abstract: Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.

</details>


### [26] [CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets](https://arxiv.org/abs/2602.03641)
*Milosh Devic,Jordan Gierschendorf,David Garson*

Main category: cs.LG

TL;DR: CTTVAE+TBS：一种针对类别不平衡表格数据的条件生成模型，通过类感知三元组边界损失和自适应采样策略，提升少数类样本的生成质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 在类别严重不平衡的表格数据生成中，现有生成模型要么忽略少数类，要么生成的样本对下游学习任务没有帮助。在医疗、欺诈检测等关键领域，少数类事件虽然罕见但影响重大，需要专门的方法来生成有代表性的少数类样本。

Method: 提出CTTVAE+TBS框架：1）基于条件Transformer的表格变分自编码器（CTTVAE）；2）类感知三元组边界损失，增强潜在空间中类内紧凑性和类间分离性；3）训练时自适应采样策略（TBS），增加对少数类样本的曝光。

Result: 在六个真实世界基准测试中，CTTVAE+TBS在少数类下游任务上取得了最佳性能，有时甚至超过了在原始不平衡数据上训练的模型。同时保持了竞争性的保真度，并在隐私保护方面为插值采样方法和深度生成方法之间架起了桥梁。

Conclusion: CTTVAE+TBS通过显式优先考虑少数类的下游性能，为条件表格数据生成提供了一个鲁棒且可解释的解决方案，特别适用于医疗、欺诈检测和预测性维护等关键领域，这些领域中少数类性能的微小提升都可能至关重要。

Abstract: Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.

</details>


### [27] [ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling](https://arxiv.org/abs/2602.03678)
*Simon Dietz,Kai Klede,An Nguyen,Bjoern M Eskofier*

Main category: cs.LG

TL;DR: ContraLog是一种无需解析器的自监督日志异常检测方法，通过预测连续消息嵌入而非离散模板ID来检测异常，在多个基准数据集上表现有效。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法大多依赖日志解析器，将消息压缩为离散模板，丢弃了变量值和语义内容，这限制了检测能力。需要一种能够保留完整语义信息的解析器无关方法。

Method: ContraLog结合消息编码器和序列编码器，使用掩码语言建模和对比学习训练模型，预测被掩码的消息嵌入而非模板ID，从而直接处理原始日志消息。

Result: 在HDFS、BGL和Thunderbird基准数据集上的实验表明，ContraLog在复杂多样的日志消息上有效。生成的消息嵌入包含有意义信息，即使没有序列上下文也能预测异常。

Conclusion: 嵌入级预测是日志异常检测的有效方法，具有扩展到其他事件序列的潜力，为无需解析器的自监督异常检测提供了新思路。

Abstract: Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.

</details>


### [28] [Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity](https://arxiv.org/abs/2602.03778)
*Aneri Muni,Vincent Taboga,Esther Derman,Pierre-Luc Bacon,Erick Delage*

Main category: cs.LG

TL;DR: 提出一种新的静态CVaR目标函数公式，通过状态增强实现具有稠密奖励和收缩性质的Bellman算子，并开发了相应的风险规避算法。


<details>
  <summary>Details</summary>
Motivation: 静态CVaR（条件风险价值）在安全关键应用中用于防止罕见但灾难性事件，但传统方法依赖于状态增强，导致稀疏奖励和退化固定点问题。

Method: 提出基于状态增强的新CVaR目标函数公式，设计具有稠密每步奖励和收缩性质的Bellman算子，开发风险规避值迭代和免模型Q学习算法，使用离散化增强状态。

Result: 新方法成功学习CVaR敏感策略，实现有效的性能-安全权衡，提供收敛保证和离散化导致的近似误差界限。

Conclusion: 提出的新CVaR公式克服了传统方法的局限性，为风险规避强化学习提供了理论保证和实用算法框架。

Abstract: Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha是一个GUI代理框架，通过步级蒙特卡洛树搜索结合生成、探索和评估，实现主动规划、早期剪枝和前缀重用，在OSWorld基准上达到约77%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理通过轨迹级采样扩展测试时计算，但缺乏回归能力，无法重用部分成功结果或从早期错误中恢复。

Method: 提出Agent Alpha统一框架，通过步级蒙特卡洛树搜索（MCTS）结合生成、探索和评估，采用alpha-UCT引导搜索、比较驱动评估和多样性约束扩展。

Result: 在OSWorld基准上达到约77%的成功率，显著优于同等计算条件下的轨迹级基线方法。

Conclusion: Agent Alpha通过步级MCTS实现了有效的GUI代理规划，能够主动建模规划空间结构，支持早期剪枝和前缀重用，显著提升性能。

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [30] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling是一个基于轨迹多样性扩展的代码代理数据合成框架，通过增加轨迹多样性而非数量来提升性能，包含业务集群、蓝图驱动多代理、自适应进化和沙盒代码工具四大创新。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型通过MCP演化为工具交互代理时，其泛化能力受到低质量合成数据和数量扩展收益递减的限制。数量为中心的扩展存在早期瓶颈，未能充分利用轨迹数据。

Method: 提出TDScaling框架：1)业务集群机制捕捉真实服务逻辑依赖；2)蓝图驱动多代理范式确保轨迹连贯性；3)基于领域熵、推理模式熵和累积动作复杂度的自适应进化机制引导合成走向长尾场景；4)沙盒代码工具防止内在编码能力灾难性遗忘。

Result: 在通用工具使用基准(BFCL, tau^2-Bench)和代码代理任务(RebenchT, CodeCI, BIRD)上的实验表明，TDScaling实现了双赢：既提升了工具使用泛化能力，又增强了内在编码熟练度。将发布包含30,000+工具集群的合成数据集。

Conclusion: TDScaling通过轨迹多样性扩展而非数量扩展，在固定训练预算下实现了更好的性能-成本权衡，解决了代码代理训练中的数据质量和泛化瓶颈问题。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [31] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking框架让LLM学会动态总结推理步骤，通过折叠模式减少KV缓存依赖，实现3倍推理吞吐量提升且保持精度


<details>
  <summary>Details</summary>
Motivation: 传统长链思维推理方法面临KV缓存线性增长和注意力复杂度二次方增长的实践限制，需要更高效的推理机制

Method: 引入Accordion-Thinking框架，LLM学习通过动态总结来自我调节推理步骤粒度，采用折叠推理模式定期总结思维过程并丢弃历史token，使用强化学习进一步激励这种能力

Result: 模型学会将关键推理信息编码到紧凑总结中，折叠模式与完整展开模式的精度差距在训练中逐渐缩小直至消失，在48GB GPU内存配置下实现3倍吞吐量同时保持精度

Conclusion: 通过学习自我压缩，LLM可以在不依赖大量历史token的情况下处理复杂推理任务，结构化步骤总结提供了人类可读的推理过程记录

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [32] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: EHRWorld：基于因果序列范式训练的以患者为中心的医疗世界模型，显著优于基于LLM的基线，在长期临床模拟中表现更稳定


<details>
  <summary>Details</summary>
Motivation: 世界模型为干预下的未来状态模拟提供了原则性框架，但在医学等复杂高风险领域实现仍具挑战。虽然LLM在静态医疗推理任务上表现出色，但能否作为动态医疗世界模型来模拟疾病进展和治疗结果尚不明确。研究发现LLM在连续干预下难以维持一致的患者状态，导致长期临床模拟中的误差累积。

Method: 提出EHRWorld，一个在因果序列范式下训练的以患者为中心的医疗世界模型，同时构建了EHRWorld-110K——一个从真实世界电子健康记录中提取的大规模纵向临床数据集。

Result: 广泛评估表明，EHRWorld显著优于基于LLM的基线方法，实现了更稳定的长期模拟、对临床敏感事件的改进建模以及更优的推理效率。

Conclusion: 可靠和稳健的医疗世界建模需要在因果基础和时序演化的临床数据上进行训练，EHRWorld展示了这种方法的必要性。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors](https://arxiv.org/abs/2602.02731)
*Rohan Pandey,Haijuan Yan,Hong Yu,Jack Tsai*

Main category: cs.CL

TL;DR: 利用退伍军人事务部电子健康记录数据，通过机器学习模型预测退伍军人首次无家可归风险，发现结合社会行为因素的纵向模型能显著提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 退伍军人无家可归是美国重要的公共卫生挑战，需要主动干预策略。风险预测为预防性干预提供了可能途径，但现有研究在利用电子健康记录数据预测无家可归风险方面仍有不足。

Method: 回顾性预后研究，分析4,276,403名退伍军人事务部患者的电子健康记录数据。构建静态和时变数据表示，使用临床医生指导的逻辑建模临床状况和社会风险的持续性。比较经典机器学习、基于Transformer的掩码语言模型和微调大语言模型的性能。

Result: 将社会和行为因素纳入纵向模型使精确率-召回率曲线下面积提高15-30%。在前1%风险层，不同时间点的阳性预测值在3.93-13.80%范围内。大语言模型在区分性能上不如编码器模型，但在不同种族群体间表现差异较小。

Conclusion: 纵向、社会信息丰富的电子健康记录建模能将无家可归风险集中到可操作的层级，为高危退伍军人提供有针对性的、数据驱动的预防策略。大语言模型在公平性方面表现更好，但整体性能不如专门设计的编码器模型。

Abstract: Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.

</details>


### [34] [AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback](https://arxiv.org/abs/2602.03084)
*Zhitao Gao,Jie Ma,Xuhong Li,Pengyu Li,Ning Qu,Yaqiang Wu,Hui Liu,Jun Liu*

Main category: cs.CL

TL;DR: AERO是一个无监督的自主推理进化框架，通过双循环系统和熵基定位解决LLM自我进化中的学习区域选择和集体幻觉问题，在多个基准测试中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理中依赖专家标注数据和外部验证器，而现有的自我进化范式往往无法确定最佳学习区域，并且通过有缺陷的内部反馈强化集体幻觉和错误先验。

Method: 提出AERO框架：1）基于ZPD理论使用熵基定位来针对"可解性差距"；2）采用独立反事实校正进行鲁棒验证；3）引入交错训练策略同步功能角色能力增长并防止课程崩溃；4）在协同双循环系统中内化自我提问、回答和批评。

Result: 在跨越三个领域的九个基准测试中，AERO在Qwen3-4B-Base上平均性能提升4.57%，在Qwen3-8B-Base上提升5.10%，优于竞争基线。

Conclusion: AERO通过无监督的自主推理进化框架有效解决了LLM自我进化中的关键挑战，实现了显著的性能提升，为LLM的自主能力发展提供了新方向。

Abstract: Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \underline{A}utonomous \underline{E}volutionary \underline{R}easoning \underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\% on Qwen3-4B-Base and 5.10\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.

</details>


### [35] [Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention](https://arxiv.org/abs/2602.03338)
*Rakshith Vasudev,Melisa Russak,Dan Bikel,Waseem Alshikh*

Main category: cs.CL

TL;DR: LLM批评模型的主动干预可能造成严重性能下降，即使离线准确率很高。研究发现干预存在"破坏-恢复"权衡，并提出部署前测试来预测干预效果，避免性能退化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM批评模型被认为能提高可靠性，但其在部署时的实际影响尚不明确。研究发现即使具有高离线准确率的批评模型也可能导致性能严重下降，这表明仅凭准确率不足以判断干预是否安全。

Method: 提出"破坏-恢复"权衡框架：干预可能恢复失败的轨迹，但也可能破坏原本会成功的轨迹。基于此，设计了一个部署前测试方法，仅需50个任务的试点就能估计干预可能带来的帮助或损害，无需完全部署。

Result: 实验显示：在高成功率任务上，干预导致性能下降0到-26个百分点；而在高失败率的ALFWorld基准上，干预带来+2.8个百分点的适度改进(p=0.014)。部署前测试能正确预测这些结果。

Conclusion: LLM批评模型的准确率不足以决定干预是否安全。提出的部署前测试框架能有效识别何时不应干预，从而在部署前防止严重的性能退化。该框架的主要价值在于避免有害干预。

Abstract: Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.
  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.

</details>


### [36] [A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces](https://arxiv.org/abs/2602.03442)
*Mingxuan Du,Benfeng Xu,Chiwei Zhu,Shaohan Wang,Pengyu Wang,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: A-RAG是一个代理式RAG框架，通过向模型暴露分层检索接口，让模型参与检索决策，从而更好地利用前沿语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统未能充分利用前沿语言模型的推理和长时程工具使用能力。它们要么采用单次检索算法，要么预定义工作流程，都不让模型参与检索决策，限制了模型能力的发挥。

Method: A-RAG框架向模型提供三种分层检索工具：关键词搜索、语义搜索和分块读取，使智能体能够自适应地在多个粒度上搜索和检索信息。

Result: 在多个开放域QA基准测试中，A-RAG始终优于现有方法，且检索token数量相当或更少，表明A-RAG能有效利用模型能力并动态适应不同RAG任务。

Conclusion: A-RAG通过让模型参与检索决策，成功利用了前沿语言模型的推理能力，为未来RAG研究提供了新的框架和评估套件。

Abstract: Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.

</details>
