{"id": "2510.18040", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.18040", "abs": "https://arxiv.org/abs/2510.18040", "authors": ["Alexander Boldachev"], "title": "Subject-Event Ontology Without Global Time: Foundations and Execution Semantics", "comment": "32 pages", "summary": "A formalization of a subject-event ontology is proposed for modeling complex\ndynamic systems without reliance on global time. Key principles: (1) event as\nan act of fixation - a subject discerns and fixes changes according to models\n(conceptual templates) available to them; (2) causal order via happens-before -\nthe order of events is defined by explicit dependencies, not timestamps; (3)\nmaking the ontology executable via a declarative dataflow mechanism, ensuring\ndeterminism; (4) models as epistemic filters - a subject can only fix what\nfalls under its known concepts and properties; (5) presumption of truth - the\ndeclarative content of an event is available for computation from the moment of\nfixation, without external verification. The formalization includes nine axioms\n(A1-A9), ensuring the correctness of executable ontologies: monotonicity of\nhistory (I1), acyclicity of causality (I2), traceability (I3). Special\nattention is given to the model-based approach (A9): event validation via\nschemas, actor authorization, automatic construction of causal chains (W3)\nwithout global time. Practical applicability is demonstrated on the boldsea\nsystem - a workflow engine for executable ontologies, where the theoretical\nconstructs are implemented in BSL (Boldsea Semantic Language). The\nformalization is applicable to distributed systems, microservice architectures,\nDLT platforms, and multiperspectivity scenarios (conflicting facts from\ndifferent subjects).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u4f53-\u4e8b\u4ef6\u7684\u672c\u4f53\u8bba\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u52a8\u6001\u7cfb\u7edf\uff0c\u65e0\u9700\u4f9d\u8d56\u5168\u5c40\u65f6\u95f4\u3002\u6838\u5fc3\u539f\u5219\u5305\u62ec\u4e8b\u4ef6\u4f5c\u4e3a\u56fa\u5b9a\u884c\u4e3a\u3001\u56e0\u679c\u987a\u5e8f\u3001\u53ef\u6267\u884c\u672c\u4f53\u3001\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u8fc7\u6ee4\u5668\u3001\u4ee5\u53ca\u771f\u503c\u5047\u8bbe\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u4e2d\u5168\u5c40\u65f6\u95f4\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u548c\u56e0\u679c\u5173\u7cfb\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u67b6\u6784\u7b49\u573a\u666f\u3002", "method": "\u901a\u8fc7\u4e5d\u4e2a\u516c\u7406\uff08A1-A9\uff09\u786e\u4fdd\u53ef\u6267\u884c\u672c\u4f53\u7684\u6b63\u786e\u6027\uff0c\u5305\u62ec\u5386\u53f2\u5355\u8c03\u6027\u3001\u56e0\u679c\u65e0\u73af\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u8fdb\u884c\u4e8b\u4ef6\u9a8c\u8bc1\u3001\u53c2\u4e0e\u8005\u6388\u6743\u548c\u56e0\u679c\u94fe\u81ea\u52a8\u6784\u5efa\u3002", "result": "\u5728boldsea\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u7406\u8bba\u6784\u5efa\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u53ef\u6267\u884c\u672c\u4f53\u7684\u5de5\u4f5c\u6d41\u5f15\u64ce\uff0c\u4f7f\u7528BSL\u8bed\u8a00\u5b9e\u73b0\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u67b6\u6784\u548c\u591a\u65b9\u89c6\u89d2\u573a\u666f\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5168\u5c40\u65f6\u95f4\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u548c\u56e0\u679c\u5173\u7cfb\u5b9e\u73b0\u786e\u5b9a\u6027\u8ba1\u7b97\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.18155", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.18155", "abs": "https://arxiv.org/abs/2510.18155", "authors": ["Man-Lin Chu", "Lucian Terhorst", "Kadin Reed", "Tom Ni", "Weiwei Chen", "Rongyu Lin"], "title": "LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior", "comment": "Accepted for publication at IEEE International Conference on\n  e-Business Engineering ICEBE 2025, November 10-12, Buraydah, Saudi Arabia. 8\n  pages, 5 figures", "summary": "Simulating consumer decision-making is vital for designing and evaluating\nmarketing strategies before costly real-world deployment. However, post-event\nanalyses and rule-based agent-based models (ABMs) struggle to capture the\ncomplexity of human behavior and social interaction. We introduce an\nLLM-powered multi-agent simulation framework that models consumer decisions and\nsocial dynamics. Building on recent advances in large language model simulation\nin a sandbox environment, our framework enables generative agents to interact,\nexpress internal reasoning, form habits, and make purchasing decisions without\npredefined rules. In a price-discount marketing scenario, the system delivers\nactionable strategy-testing outcomes and reveals emergent social patterns\nbeyond the reach of conventional methods. This approach offers marketers a\nscalable, low-risk tool for pre-implementation testing, reducing reliance on\ntime-intensive post-event evaluations and lowering the risk of underperforming\ncampaigns.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6d88\u8d39\u8005\u51b3\u7b56\u548c\u793e\u4f1a\u52a8\u6001\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u540e\u5206\u6790\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u667a\u80fd\u4f53\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u548c\u793e\u4f1a\u4e92\u52a8\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8425\u9500\u7b56\u7565\u9884\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u6784\u5efa\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u8ba9\u751f\u6210\u667a\u80fd\u4f53\u80fd\u591f\u4e92\u52a8\u3001\u8868\u8fbe\u5185\u90e8\u63a8\u7406\u3001\u5f62\u6210\u4e60\u60ef\u5e76\u8fdb\u884c\u8d2d\u4e70\u51b3\u7b56\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u89c4\u5219\u3002", "result": "\u5728\u4ef7\u683c\u6298\u6263\u8425\u9500\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7684\u65b0\u5174\u793e\u4f1a\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8425\u9500\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4f4e\u98ce\u9669\u7684\u9884\u5b9e\u65bd\u6d4b\u8bd5\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u5bf9\u8017\u65f6\u7684\u4e8b\u540e\u8bc4\u4f30\u7684\u4f9d\u8d56\uff0c\u5e76\u964d\u4f4e\u4e86\u8425\u9500\u6d3b\u52a8\u8868\u73b0\u4e0d\u4f73\u7684\u98ce\u9669\u3002"}}
{"id": "2510.18546", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18546", "abs": "https://arxiv.org/abs/2510.18546", "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"], "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval", "comment": "NeurIPS 2025", "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86EfficientNav\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5185\u5b58\u68c0\u7d22\u548c\u79bb\u6563\u5185\u5b58\u7f13\u5b58\u6280\u672f\uff0c\u4f7f\u5c0f\u578bLLM\u80fd\u591f\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76ee\u6807\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u4e91\u7aef\u5de8\u578bLLM\uff0c\u65e0\u6cd5\u5728\u672c\u5730\u8bbe\u5907\u90e8\u7f72\u3002\u5c0f\u578bLLM\u7531\u4e8e\u6a21\u578b\u5bb9\u91cf\u6709\u9650\uff0c\u5728\u7406\u89e3\u590d\u6742\u5bfc\u822a\u5730\u56fe\u65f6\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u957f\u63d0\u793a\u5bfc\u81f4\u9ad8\u89c4\u5212\u5ef6\u8fdf\u3002", "method": "1. \u8bed\u4e49\u611f\u77e5\u5185\u5b58\u68c0\u7d22\uff1a\u4fee\u526a\u5bfc\u822a\u5730\u56fe\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u5e2e\u52a9\u5c0f\u578bLLM\u66f4\u597d\u7406\u89e3\u73af\u5883\n2. \u79bb\u6563\u5185\u5b58\u7f13\u5b58\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5185\u5b58\u805a\u7c7b\uff1a\u9ad8\u6548\u4fdd\u5b58\u548c\u91cd\u7528KV\u7f13\u5b58\uff0c\u51cf\u5c11\u89c4\u5212\u5ef6\u8fdf", "result": "\u5728HM3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u4e8eGPT-4\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u8fbe\u523011.1%\u7684\u63d0\u5347\uff1b\u76f8\u6bd4GPT-4\u89c4\u5212\u5668\uff0c\u5b9e\u65f6\u5ef6\u8fdf\u964d\u4f4e6.7\u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e4.7\u500d\u3002", "conclusion": "EfficientNav\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5c0f\u578bLLM\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u6267\u884c\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u65f6\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u96f6\u6837\u672c\u5bfc\u822a\u3002"}}
{"id": "2510.18643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18643", "abs": "https://arxiv.org/abs/2510.18643", "authors": ["Mattias Trende", "Petter \u00d6gren"], "title": "Least Restrictive Hyperplane Control Barrier Functions", "comment": null, "summary": "Control Barrier Functions (CBFs) can provide provable safety guarantees for\ndynamic systems. However, finding a valid CBF for a system of interest is often\nnon-trivial, especially if the shape of the unsafe region is complex and the\nCBFs are of higher order. A common solution to this problem is to make a\nconservative approximation of the unsafe region in the form of a\nline/hyperplane, and use the corresponding conservative Hyperplane-CBF when\ndeciding on safe control actions. In this letter, we note that conservative\nconstraints are only a problem if they prevent us from doing what we want.\nThus, instead of first choosing a CBF and then choosing a safe control with\nrespect to the CBF, we optimize over a combination of CBFs and safe controls to\nget as close as possible to our desired control, while still having the safety\nguarantee provided by the CBF. We call the corresponding CBF the least\nrestrictive Hyperplane-CBF. Finally, we also provide a way of creating a smooth\nparameterization of the CBF-family for the optimization, and illustrate the\napproach on a double integrator dynamical system with acceleration constraints,\nmoving through a group of arbitrarily shaped static and moving obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u5c0f\u9650\u5236\u8d85\u5e73\u9762\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316CBF\u548c\u5b89\u5168\u63a7\u5236\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\u5c3d\u53ef\u80fd\u63a5\u8fd1\u671f\u671b\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCBF\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4e0d\u5b89\u5168\u533a\u57df\u65f6\u7684\u4fdd\u5b88\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5f62\u72b6\u4e0d\u5b89\u5168\u533a\u57df\u65f6\uff0c\u901a\u5e38\u9700\u8981\u4fdd\u5b88\u5730\u8fd1\u4f3c\u4e0d\u5b89\u5168\u533a\u57df\u4e3a\u8d85\u5e73\u9762\u5f62\u5f0f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u63a7\u5236\u7ea6\u675f\u8fc7\u4e8e\u4e25\u683c\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u540c\u65f6\u4f18\u5316CBF\u548c\u5b89\u5168\u63a7\u5236\u7684\u6846\u67b6\uff0c\u800c\u4e0d\u662f\u5148\u9009\u62e9CBF\u518d\u9009\u62e9\u5b89\u5168\u63a7\u5236\u3002\u901a\u8fc7\u521b\u5efaCBF\u65cf\u7684\u5e73\u6ed1\u53c2\u6570\u5316\uff0c\u4f18\u5316\u5f97\u5230\u6700\u5c0f\u9650\u5236\u7684\u8d85\u5e73\u9762CBF\u3002", "result": "\u5728\u5177\u6709\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u53cc\u79ef\u5206\u5668\u52a8\u6001\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684\u9759\u6001\u548c\u52a8\u6001\u969c\u788d\u7269\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u51cf\u5c11\u63a7\u5236\u7ea6\u675f\u7684\u9650\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u63a7\u5236\u7ea6\u675f\u9650\u5236\u7684\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e0d\u5b89\u5168\u533a\u57df\u7684\u9ad8\u9636CBF\u8bbe\u8ba1\u95ee\u9898\u3002"}}
{"id": "2510.18697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18697", "abs": "https://arxiv.org/abs/2510.18697", "authors": ["Phuoc Nguyen", "Francesco Verdoja", "Ville Kyrki"], "title": "Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations", "comment": "Submitted to RA-L", "summary": "A fundamental aspect for building intelligent autonomous robots that can\nassist humans in their daily lives is the construction of rich environmental\nrepresentations. While advances in semantic scene representations have enriched\nrobotic scene understanding, current approaches lack a connection between\nspatial features and dynamic events; e.g., connecting the blue mug to the event\nwashing a mug. In this work, we introduce the event-grounding graph (EGG), a\nframework grounding event interactions to spatial features of a scene. This\nrepresentation allows robots to perceive, reason, and respond to complex\nspatio-temporal queries. Experiments using real robotic data demonstrate EGG's\ncapability to retrieve relevant information and respond accurately to human\ninquiries concerning the environment and events within. Furthermore, the EGG\nframework's source code and evaluation dataset are released as open-source at:\nhttps://github.com/aalto-intelligent-robotics/EGG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e8b\u4ef6\u57fa\u7840\u56fe\uff08EGG\uff09\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u4ea4\u4e92\u4e0e\u573a\u666f\u7a7a\u95f4\u7279\u5f81\u8fde\u63a5\u8d77\u6765\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u548c\u54cd\u5e94\u590d\u6742\u7684\u65f6\u7a7a\u67e5\u8be2\u3002", "motivation": "\u5f53\u524d\u8bed\u4e49\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u7279\u5f81\u4e0e\u52a8\u6001\u4e8b\u4ef6\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5bf9\u73af\u5883\u7684\u6df1\u5ea6\u7406\u89e3\u3002", "method": "\u5f00\u53d1\u4e8b\u4ef6\u57fa\u7840\u56fe\uff08EGG\uff09\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u4ea4\u4e92\u57fa\u7840\u5316\u5230\u573a\u666f\u7684\u7a7a\u95f4\u7279\u5f81\u4e0a\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEGG\u80fd\u591f\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u5e76\u51c6\u786e\u54cd\u5e94\u5173\u4e8e\u73af\u5883\u548c\u4e8b\u4ef6\u7684\u67e5\u8be2\u3002", "conclusion": "EGG\u6846\u67b6\u6709\u6548\u8fde\u63a5\u4e86\u7a7a\u95f4\u7279\u5f81\u548c\u52a8\u6001\u4e8b\u4ef6\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u573a\u666f\u7406\u89e3\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.18089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18089", "abs": "https://arxiv.org/abs/2510.18089", "authors": ["Paul-Tiberiu Miclea", "Martin Sboron", "Hardik Vaghasiya", "Hoang Thinh Nguyen", "Meet Gadara", "Thomas Schmid"], "title": "Big Data, Tiny Targets: An Exploratory Study in Machine Learning-enhanced Detection of Microplastic from Filters", "comment": null, "summary": "Microplastics (MPs) are ubiquitous pollutants with demonstrated potential to\nimpact ecosystems and human health. Their microscopic size complicates\ndetection, classification, and removal, especially in biological and\nenvironmental samples. While techniques like optical microscopy, Scanning\nElectron Microscopy (SEM), and Atomic Force Microscopy (AFM) provide a sound\nbasis for detection, applying these approaches requires usually manual analysis\nand prevents efficient use in large screening studies. To this end, machine\nlearning (ML) has emerged as a powerful tool in advancing microplastic\ndetection. In this exploratory study, we investigate potential, limitations and\nfuture directions of advancing the detection and quantification of MP particles\nand fibres using a combination of SEM imaging and machine learning-based object\ndetection. For simplicity, we focus on a filtration scenario where image\nbackgrounds exhibit a symmetric and repetitive pattern. Our findings indicate\ndifferences in the quality of YOLO models for the given task and the relevance\nof optimizing preprocessing. At the same time, we identify open challenges,\nsuch as limited amounts of expert-labeled data necessary for reliable training\nof ML models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u7ed3\u5408SEM\u6210\u50cf\u548c\u57fa\u4e8eYOLO\u7684\u673a\u5668\u5b66\u4e60\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u5728\u5fae\u5851\u6599\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3001\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411\uff0c\u91cd\u70b9\u5173\u6ce8\u5177\u6709\u5bf9\u79f0\u91cd\u590d\u80cc\u666f\u56fe\u6848\u7684\u8fc7\u6ee4\u573a\u666f\u3002", "motivation": "\u5fae\u5851\u6599\u4f5c\u4e3a\u666e\u904d\u6c61\u67d3\u7269\u5bf9\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u5065\u5eb7\u6784\u6210\u5a01\u80c1\uff0c\u4f46\u5176\u5fae\u5c0f\u5c3a\u5bf8\u4f7f\u5f97\u68c0\u6d4b\u548c\u5206\u7c7b\u53d8\u5f97\u56f0\u96be\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u5206\u6790\uff0c\u65e0\u6cd5\u9ad8\u6548\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u7b5b\u9009\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u91c7\u7528\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c(SEM)\u6210\u50cf\u4e0e\u57fa\u4e8eYOLO\u7684\u673a\u5668\u5b66\u4e60\u5bf9\u8c61\u68c0\u6d4b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u5177\u6709\u5bf9\u79f0\u91cd\u590d\u80cc\u666f\u56fe\u6848\u7684\u8fc7\u6ee4\u573a\u666f\uff0c\u5e76\u4f18\u5316\u9884\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540cYOLO\u6a21\u578b\u5728\u5fae\u5851\u6599\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u9884\u5904\u7406\u4f18\u5316\u5bf9\u68c0\u6d4b\u8d28\u91cf\u6709\u91cd\u8981\u5f71\u54cd\u3002\u540c\u65f6\u8bc6\u522b\u51fa\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u91cf\u6709\u9650\u662f\u8bad\u7ec3\u53ef\u9760\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u7279\u522b\u662fYOLO\u6a21\u578b\u5728\u5fae\u5851\u6599\u68c0\u6d4b\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u548c\u9884\u5904\u7406\u4f18\u5316\u7b49\u6311\u6218\uff0c\u672a\u6765\u9700\u8981\u66f4\u591a\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u6765\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2510.18342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18342", "abs": "https://arxiv.org/abs/2510.18342", "authors": ["Peng Tang", "Xiaoxiao Yan", "Xiaobin Hu", "Yuning Cui", "Donghao Luo", "Jiangning Zhang", "Pengcheng Xu", "Jinlong Peng", "Qingdong He", "Feiyue Huang", "Song Xue", "Tobias Lasser"], "title": "ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection", "comment": "Under Review", "summary": "Multi-class unsupervised anomaly detection (MUAD) has garnered growing\nresearch interest, as it seeks to develop a unified model for anomaly detection\nacross multiple classes, i.e., eliminating the need to train separate models\nfor distinct objects and thereby saving substantial computational resources.\nUnder the MUAD setting, while advanced Transformer-based architectures have\nbrought significant performance improvements, identity shortcuts persist: they\ndirectly copy inputs to outputs, narrowing the gap in reconstruction errors\nbetween normal and abnormal cases, and thereby making the two harder to\ndistinguish. Therefore, we propose ShortcutBreaker, a novel unified\nfeature-reconstruction framework for MUAD tasks, featuring two key innovations\nto address the issue of shortcuts. First, drawing on matrix rank inequality, we\ndesign a low-rank noisy bottleneck (LRNB) to project highdimensional features\ninto a low-rank latent space, and theoretically demonstrate its capacity to\nprevent trivial identity reproduction. Second, leveraging ViTs global modeling\ncapability instead of merely focusing on local features, we incorporate a\nglobal perturbation attention to prevent information shortcuts in the decoders.\nExtensive experiments are performed on four widely used anomaly detection\nbenchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)\nand one medical dataset (Universal Medical). The proposed method achieves a\nremarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four\ndatasets, respectively, consistently outperforming previous MUAD methods across\ndifferent scenarios.", "AI": {"tldr": "\u63d0\u51faShortcutBreaker\u6846\u67b6\u89e3\u51b3\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u79e9\u566a\u58f0\u74f6\u9888\u548c\u5168\u5c40\u6270\u52a8\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u7279\u5f81\u76f4\u63a5\u590d\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u9700\u8981\u7edf\u4e00\u6a21\u578b\u68c0\u6d4b\u591a\u7c7b\u5f02\u5e38\uff0c\u4f46\u73b0\u6709Transformer\u67b6\u6784\u5b58\u5728\u8eab\u4efd\u6377\u5f84\u95ee\u9898\u2014\u2014\u76f4\u63a5\u590d\u5236\u8f93\u5165\u5230\u8f93\u51fa\uff0c\u5bfc\u81f4\u6b63\u5e38\u4e0e\u5f02\u5e38\u6837\u672c\u7684\u91cd\u5efa\u8bef\u5dee\u5dee\u5f02\u7f29\u5c0f\uff0c\u96be\u4ee5\u533a\u5206\u3002", "method": "1. \u57fa\u4e8e\u77e9\u9635\u79e9\u4e0d\u7b49\u5f0f\u8bbe\u8ba1\u4f4e\u79e9\u566a\u58f0\u74f6\u9888(LRNB)\uff0c\u5c06\u9ad8\u7ef4\u7279\u5f81\u6295\u5f71\u5230\u4f4e\u79e9\u6f5c\u5728\u7a7a\u95f4\uff0c\u7406\u8bba\u4e0a\u9632\u6b62\u5e73\u51e1\u8eab\u4efd\u590d\u5236\uff1b2. \u5229\u7528ViT\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u5f15\u5165\u5168\u5c40\u6270\u52a8\u6ce8\u610f\u529b\u9632\u6b62\u89e3\u7801\u5668\u4e2d\u7684\u4fe1\u606f\u6377\u5f84\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff1aMVTec-AD(99.8%)\u3001ViSA(98.9%)\u3001Real-IAD(90.6%)\u548cUniversal Medical(87.8%)\u7684\u56fe\u50cf\u7ea7AUROC\uff0c\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "ShortcutBreaker\u901a\u8fc7\u89e3\u51b3\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u5728\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u7edf\u4e00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18173", "abs": "https://arxiv.org/abs/2510.18173", "authors": ["Ritam Upadhyay", "Naman Ahuja", "Rishabh Baral", "Aparna Garimella", "Vivek Gupta"], "title": "CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models", "comment": null, "summary": "LLM Driven text-to-table (T2T) systems often rely on extensive\nprompt-engineering or iterative event extraction in code-parsable formats,\nwhich boosts scores but are computationally expensive and obscure how models\nactually reason over temporal evolving narratives to summarise key information.\nWe present CMT-Bench, a diagnostic benchmark built from live cricket commentary\nthat requires dynamic table generation across two evolving schemas under a\ndense, rule-governed policy. CMT-Bench is designed to probe robustness via\nthree semantics-preserving dimensions: (i) extractive-cue ablation to separate\nextractive shortcuts from state tracking, (ii) temporal prefixing to test\nlong-context stability, and (iii) entity-form perturbations (anonymization,\noutof-distribution substitutions, role-entangling paraphrases) to assess\nsensitivity to surface variation. Across diverse long-context stateof-the-art\nLLMs, we find large drops without extractive summaries, monotonic degradation\nwith input length, and consistent accuracy drop under entity-form changes.\nComplementary distributional tests confirm significant shifts in numeric error\npatterns, indicating drift in reasoning rather than mere noise. Our results\nshow that current LLMs are brittle in dynamic Textto-table generation,\nmotivating robustness-first evaluation as a prerequisite for developing\nefficient and scalable approaches for this task.", "AI": {"tldr": "\u63d0\u51fa\u4e86CMT-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u52a8\u6001\u6587\u672c\u8f6c\u8868\u683c\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u63d0\u53d6\u7ebf\u7d22\u7f3a\u5931\u3001\u957f\u4e0a\u4e0b\u6587\u548c\u5b9e\u4f53\u5f62\u5f0f\u53d8\u5316\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u8f6c\u8868\u683c\u7cfb\u7edf\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u6216\u8fed\u4ee3\u4e8b\u4ef6\u63d0\u53d6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5bf9\u65f6\u5e8f\u6f14\u53d8\u53d9\u4e8b\u8fdb\u884c\u63a8\u7406\u3002\u9700\u8981\u5f00\u53d1\u80fd\u6d4b\u8bd5\u6a21\u578b\u9c81\u68d2\u6027\u7684\u8bca\u65ad\u57fa\u51c6\u3002", "method": "\u57fa\u4e8e\u677f\u7403\u8bc4\u8bba\u6784\u5efaCMT-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u8bed\u4e49\u4fdd\u6301\u7ef4\u5ea6\uff1a\u63d0\u53d6\u7ebf\u7d22\u6d88\u878d\u3001\u65f6\u95f4\u524d\u7f00\u5316\u548c\u5b9e\u4f53\u5f62\u5f0f\u6270\u52a8\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5404\u79cd\u5148\u8fdbLLM\u5728\u6ca1\u6709\u63d0\u53d6\u6458\u8981\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u65f6\u5355\u8c03\u9000\u5316\uff0c\u5b9e\u4f53\u5f62\u5f0f\u53d8\u5316\u4e0b\u51c6\u786e\u7387\u4e00\u81f4\u4e0b\u964d\u3002\u5206\u5e03\u6d4b\u8bd5\u663e\u793a\u6570\u503c\u9519\u8bef\u6a21\u5f0f\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u52a8\u6001\u6587\u672c\u8f6c\u8868\u683c\u751f\u6210\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8981\u4f18\u5148\u8003\u8651\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u4ee5\u5f00\u53d1\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.18196", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18196", "abs": "https://arxiv.org/abs/2510.18196", "authors": ["Yoshinari Fujinuma"], "title": "Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge", "comment": null, "summary": "Large Language Models (LLMs) are commonly used as evaluators in various\napplications, but the reliability of the outcomes remains a challenge. One such\nchallenge is using LLMs-as-judges for direct assessment, i.e., assigning scores\nfrom a specified range without any references. We first show that this\nchallenge stems from LLM judge outputs being associated with score range bias,\ni.e., LLM judge outputs are highly sensitive to pre-defined score ranges,\npreventing the search for optimal score ranges. We also show that similar\nbiases exist among models from the same family. We then mitigate this bias\nthrough contrastive decoding, achieving up to 11.3% relative improvement on\naverage in Spearman correlation with human judgments across different score\nranges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u65f6\u5b58\u5728\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u79cd\u504f\u5dee\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002", "motivation": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u65e0\u53c2\u8003\u7684\u76f4\u63a5\u8bc4\u4f30\u4e2d\uff0cLLM\u8bc4\u5224\u7ed3\u679c\u5bf9\u9884\u5b9a\u4e49\u8bc4\u5206\u8303\u56f4\u9ad8\u5ea6\u654f\u611f\uff0c\u5b58\u5728\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bc6\u522bLLM\u8bc4\u5224\u8f93\u51fa\u5b58\u5728\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u79cd\u504f\u5dee\u3002", "result": "\u4f7f\u7528\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u540e\uff0c\u5728\u4e0d\u540c\u8bc4\u5206\u8303\u56f4\u5185\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684Spearman\u76f8\u5173\u6027\u5e73\u5747\u76f8\u5bf9\u63d0\u9ad8\u4e8611.3%\u3002", "conclusion": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u65f6\u786e\u5b9e\u5b58\u5728\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898\uff0c\u4f46\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u79cd\u504f\u5dee\uff0c\u63d0\u9ad8\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.18201", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18201", "abs": "https://arxiv.org/abs/2510.18201", "authors": ["Sriharsh Bhyravajjula", "Ujwal Narayan", "Manish Shrivastava"], "title": "MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives", "comment": null, "summary": "Character arcs are important theoretical devices employed in literary studies\nto understand character journeys, identify tropes across literary genres, and\nestablish similarities between narratives. This work addresses the novel task\nof computationally generating event-centric, relation-based character arcs from\nnarratives. Providing a quantitative representation for arcs brings tangibility\nto a theoretical concept and paves the way for subsequent applications. We\npresent MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that\nextracts events, participant characters, implied emotion, and sentiment to\nmodel inter-character relations. MARCUS tracks and aggregates these relations\nacross the narrative to generate character arcs as graphical plots. We generate\ncharacter arcs from two extended fantasy series, Harry Potter and Lord of the\nRings. We evaluate our approach before outlining existing challenges,\nsuggesting applications of our pipeline, and discussing future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86MARCUS\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u53d9\u4e8b\u6587\u672c\u4e2d\u8ba1\u7b97\u751f\u6210\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u3001\u57fa\u4e8e\u5173\u7cfb\u7684\u89d2\u8272\u5f27\u7ebf\uff0c\u5e76\u5728\u300a\u54c8\u5229\u6ce2\u7279\u300b\u548c\u300a\u6307\u73af\u738b\u300b\u7cfb\u5217\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u89d2\u8272\u5f27\u7ebf\u662f\u6587\u5b66\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u7406\u8bba\u5de5\u5177\uff0c\u4f46\u7f3a\u4e4f\u8ba1\u7b97\u8868\u793a\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u8fd9\u4e00\u7406\u8bba\u6982\u5ff5\u63d0\u4f9b\u91cf\u5316\u8868\u793a\uff0c\u4e3a\u540e\u7eed\u5e94\u7528\u94fa\u5e73\u9053\u8def\u3002", "method": "\u5f00\u53d1\u4e86MARCUS NLP\u6d41\u6c34\u7ebf\uff0c\u63d0\u53d6\u4e8b\u4ef6\u3001\u53c2\u4e0e\u89d2\u8272\u3001\u9690\u542b\u60c5\u611f\u548c\u60c5\u7eea\uff0c\u5efa\u6a21\u89d2\u8272\u95f4\u5173\u7cfb\uff0c\u5e76\u5728\u53d9\u4e8b\u8fc7\u7a0b\u4e2d\u8ddf\u8e2a\u805a\u5408\u8fd9\u4e9b\u5173\u7cfb\uff0c\u4ee5\u56fe\u5f62\u5316\u65b9\u5f0f\u751f\u6210\u89d2\u8272\u5f27\u7ebf\u3002", "result": "\u6210\u529f\u4ece\u300a\u54c8\u5229\u6ce2\u7279\u300b\u548c\u300a\u6307\u73af\u738b\u300b\u4e24\u4e2a\u957f\u7bc7\u5947\u5e7b\u7cfb\u5217\u4e2d\u751f\u6210\u4e86\u89d2\u8272\u5f27\u7ebf\uff0c\u5e76\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u5c55\u793a\u4e86\u8ba1\u7b97\u751f\u6210\u89d2\u8272\u5f27\u7ebf\u7684\u53ef\u884c\u6027\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u6d41\u6c34\u7ebf\u7684\u5e94\u7528\u524d\u666f\u548c\u672a\u6765\u5de5\u4f5c\u65b9\u5411\u3002"}}
{"id": "2510.18072", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18072", "abs": "https://arxiv.org/abs/2510.18072", "authors": ["Jiajun Fan", "Chaoran Cheng", "Shuaike Shen", "Xiangxin Zhou", "Ge Liu"], "title": "Fine-tuning Flow Matching Generative Models with Intermediate Feedback", "comment": null, "summary": "Flow-based generative models have shown remarkable success in text-to-image\ngeneration, yet fine-tuning them with intermediate feedback remains\nchallenging, especially for continuous-time flow matching models. Most existing\napproaches solely learn from outcome rewards, struggling with the credit\nassignment problem. Alternative methods that attempt to learn a critic via\ndirect regression on cumulative rewards often face training instabilities and\nmodel collapse in online settings. We present AC-Flow, a robust actor-critic\nframework that addresses these challenges through three key innovations: (1)\nreward shaping that provides well-normalized learning signals to enable stable\nintermediate value learning and gradient control, (2) a novel dual-stability\nmechanism that combines advantage clipping to prevent destructive policy\nupdates with a warm-up phase that allows the critic to mature before\ninfluencing the actor, and (3) a scalable generalized critic weighting scheme\nthat extends traditional reward-weighted methods while preserving model\ndiversity through Wasserstein regularization. Through extensive experiments on\nStable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art\nperformance in text-to-image alignment tasks and generalization to unseen human\npreference models. Our results demonstrate that even with a computationally\nefficient critic model, we can robustly finetune flow models without\ncompromising generative quality, diversity, or stability.", "AI": {"tldr": "AC-Flow\u662f\u4e00\u4e2a\u7a33\u5065\u7684actor-critic\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u9020\u3001\u53cc\u7a33\u5b9a\u6027\u673a\u5236\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u8bba\u5bb6\u52a0\u6743\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u6a21\u578b\u5728\u4e2d\u95f4\u53cd\u9988\u5fae\u8c03\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5229\u7528\u4e2d\u95f4\u53cd\u9988\u8fdb\u884c\u5fae\u8c03\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u5b58\u5728\u4fe1\u7528\u5206\u914d\u95ee\u9898\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u5956\u52b1\u5851\u9020\u63d0\u4f9b\u6807\u51c6\u5316\u5b66\u4e60\u4fe1\u53f7\uff1b\u53cc\u7a33\u5b9a\u6027\u673a\u5236\u7ed3\u5408\u4f18\u52bf\u88c1\u526a\u548c\u9884\u70ed\u9636\u6bb5\uff1b\u53ef\u6269\u5c55\u7684\u5e7f\u4e49\u8bc4\u8bba\u5bb6\u52a0\u6743\u65b9\u6848\u7ed3\u5408Wasserstein\u6b63\u5219\u5316\u3002", "result": "\u5728Stable Diffusion 3\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAC-Flow\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u7684\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u3002", "conclusion": "\u5373\u4f7f\u4f7f\u7528\u8ba1\u7b97\u9ad8\u6548\u7684\u8bc4\u8bba\u5bb6\u6a21\u578b\uff0c\u4e5f\u80fd\u7a33\u5065\u5730\u5fae\u8c03\u6d41\u6a21\u578b\uff0c\u800c\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u6216\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.18082", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18082", "abs": "https://arxiv.org/abs/2510.18082", "authors": ["Donggeon David Oh", "Duy P. Nguyen", "Haimin Hu", "Jaime F. Fisac"], "title": "Provably Optimal Reinforcement Learning under Safety Filtering", "comment": "17 pages, 3 figures", "summary": "Recent advances in reinforcement learning (RL) enable its use on increasingly\ncomplex tasks, but the lack of formal safety guarantees still limits its\napplication in safety-critical settings. A common practical approach is to\naugment the RL policy with a safety filter that overrides unsafe actions to\nprevent failures during both training and deployment. However, safety filtering\nis often perceived as sacrificing performance and hindering the learning\nprocess. We show that this perceived safety-performance tradeoff is not\ninherent and prove, for the first time, that enforcing safety with a\nsufficiently permissive safety filter does not degrade asymptotic performance.\nWe formalize RL safety with a safety-critical Markov decision process (SC-MDP),\nwhich requires categorical, rather than high-probability, avoidance of\ncatastrophic failure states. Additionally, we define an associated filtered MDP\nin which all actions result in safe effects, thanks to a safety filter that is\nconsidered to be a part of the environment. Our main theorem establishes that\n(i) learning in the filtered MDP is safe categorically, (ii) standard RL\nconvergence carries over to the filtered MDP, and (iii) any policy that is\noptimal in the filtered MDP-when executed through the same filter-achieves the\nsame asymptotic return as the best safe policy in the SC-MDP, yielding a\ncomplete separation between safety enforcement and performance optimization. We\nvalidate the theory on Safety Gymnasium with representative tasks and\nconstraints, observing zero violations during training and final performance\nmatching or exceeding unfiltered baselines. Together, these results shed light\non a long-standing question in safety-filtered learning and provide a simple,\nprincipled recipe for safe RL: train and deploy RL policies with the most\npermissive safety filter that is available.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528\u8db3\u591f\u5bbd\u677e\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u4e0d\u4f1a\u964d\u4f4e\u6e10\u8fd1\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u5b8c\u5168\u5206\u79bb\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u4f7f\u7528\u3002\u5b89\u5168\u8fc7\u6ee4\u5668\u867d\u7136\u80fd\u9632\u6b62\u5931\u8d25\uff0c\u4f46\u901a\u5e38\u88ab\u8ba4\u4e3a\u4f1a\u727a\u7272\u6027\u80fd\u548c\u963b\u788d\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u5b89\u5168\u5173\u952e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08SC-MDP\uff09\u548c\u8fc7\u6ee4MDP\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u8fc7\u6ee4MDP\u4e2d\u5b66\u4e60\u662f\u5b89\u5168\u7684\uff0c\u6807\u51c6RL\u6536\u655b\u6027\u4ecd\u7136\u6210\u7acb\uff0c\u4e14\u6700\u4f18\u7b56\u7565\u5728\u8fc7\u6ee4MDP\u4e2d\u80fd\u8fbe\u5230\u4e0eSC-MDP\u4e2d\u6700\u4f73\u5b89\u5168\u7b56\u7565\u76f8\u540c\u7684\u6e10\u8fd1\u56de\u62a5\u3002", "result": "\u5728Safety Gymnasium\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u96f6\u8fdd\u89c4\uff0c\u6700\u7ec8\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u65e0\u8fc7\u6ee4\u57fa\u7ebf\u3002", "conclusion": "\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff1a\u4f7f\u7528\u53ef\u7528\u7684\u6700\u5bbd\u677e\u5b89\u5168\u8fc7\u6ee4\u5668\u6765\u8bad\u7ec3\u548c\u90e8\u7f72RL\u7b56\u7565\u3002"}}
{"id": "2510.18080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18080", "abs": "https://arxiv.org/abs/2510.18080", "authors": ["Rukuang Huang", "Sungjun Cho", "Chetan Gohil", "Oiwi Parker Jones", "Mark Woolrich"], "title": "MEG-GPT: A transformer-based foundation model for magnetoencephalography data", "comment": null, "summary": "Modelling the complex spatiotemporal patterns of large-scale brain dynamics\nis crucial for neuroscience, but traditional methods fail to capture the rich\nstructure in modalities such as magnetoencephalography (MEG). Recent advances\nin deep learning have enabled significant progress in other domains, such as\nlanguage and vision, by using foundation models at scale. Here, we introduce\nMEG-GPT, a transformer based foundation model that uses time-attention and next\ntime-point prediction. To facilitate this, we also introduce a novel\ndata-driven tokeniser for continuous MEG data, which preserves the high\ntemporal resolution of continuous MEG signals without lossy transformations. We\ntrained MEG-GPT on tokenised brain region time-courses extracted from a\nlarge-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that\nthe learnt model can generate data with realistic spatio-spectral properties,\nincluding transient events and population variability. Critically, it performs\nwell in downstream decoding tasks, improving downstream supervised prediction\ntask, showing improved zero-shot generalisation across sessions (improving\naccuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)\ncompared to a baseline methods. Furthermore, we show the model can be\nefficiently fine-tuned on a smaller labelled dataset to boost performance in\ncross-subject decoding scenarios. This work establishes a powerful foundation\nmodel for electrophysiological data, paving the way for applications in\ncomputational neuroscience and neural decoding.", "AI": {"tldr": "MEG-GPT\uff1a\u57fa\u4e8eTransformer\u7684\u8111\u78c1\u56fe\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u4e0b\u4e00\u65f6\u95f4\u70b9\u9884\u6d4b\u6765\u5efa\u6a21\u5927\u89c4\u6a21\u8111\u52a8\u529b\u5b66\uff0c\u5728\u89e3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8111\u78c1\u56fe\u7b49\u6a21\u6001\u4e2d\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u5728\u5176\u4ed6\u9886\u57df\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7528\u4e8e\u8111\u7535\u751f\u7406\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faMEG-GPT\uff0c\u4f7f\u7528\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u4e0b\u4e00\u65f6\u95f4\u70b9\u9884\u6d4b\u7684Transformer\u67b6\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u6570\u636e\u9a71\u52a8\u5206\u8bcd\u5668\u6765\u4fdd\u6301\u8fde\u7eedMEG\u4fe1\u53f7\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3002", "result": "\u6a21\u578b\u80fd\u751f\u6210\u5177\u6709\u771f\u5b9e\u65f6\u7a7a\u9891\u8c31\u7279\u6027\u7684\u6570\u636e\uff0c\u5728\u89e3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8de8\u4f1a\u8bdd\u51c6\u786e\u7387\u4ece0.54\u63d0\u5347\u81f30.59\uff0c\u8de8\u88ab\u8bd5\u51c6\u786e\u7387\u4ece0.41\u63d0\u5347\u81f30.49\uff0c\u4e14\u80fd\u901a\u8fc7\u5fae\u8c03\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7535\u751f\u7406\u6570\u636e\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u548c\u795e\u7ecf\u89e3\u7801\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.18269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18269", "abs": "https://arxiv.org/abs/2510.18269", "authors": ["Xueyi Chen", "Keda Tao", "Kele Shao", "Huan Wang"], "title": "StreamingTOM: Streaming Token Compression for Efficient Video Understanding", "comment": null, "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.", "AI": {"tldr": "StreamingTOM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u65f6\u95f4\u7f29\u51cf\u548c\u5728\u7ebf\u91cf\u5316\u5185\u5b58\u6280\u672f\uff0c\u89e3\u51b3\u6d41\u5f0f\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56e0\u679c\u6027\u548c\u7d2f\u79ef\u6027\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u6d41\u5f0f\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u7ea6\u675f\uff1a\u56e0\u679c\u6027\uff08\u65e0\u6cd5\u8bbf\u95ee\u672a\u6765\u5e27\uff09\u548c\u7d2f\u79ef\u6027\uff08token\u65e0\u9650\u589e\u957f\u5bfc\u81f4\u6548\u7387\u74f6\u9888\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u53ea\u8c03\u63a7LLM\u540e\u7684kv-cache\uff0c\u800c\u5ffd\u7565\u4e86\u6210\u672c\u9ad8\u6602\u7684LLM\u524d\u9884\u586b\u5145\u9636\u6bb5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u56e0\u679c\u65f6\u95f4\u7f29\u51cf\uff1a\u57fa\u4e8e\u76f8\u90bb\u5e27\u53d8\u5316\u548ctoken\u663e\u8457\u6027\u9009\u62e9token\uff0c\u6bcf\u5e27\u53ea\u5904\u7406\u7d27\u51d1\u7684\u89c6\u89c9token\u5b50\u96c6\uff1b2\uff09\u5728\u7ebf\u91cf\u5316\u5185\u5b58\uff1a\u4ee54\u4f4d\u683c\u5f0f\u5b58\u50a8token\uff0c\u6309\u9700\u68c0\u7d22\u76f8\u5173\u7ec4\u5e76\u53cd\u91cf\u5316\uff0c\u4fdd\u6301\u6d3b\u8dc3kv-cache\u6709\u754c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u5b9e\u73b015.7\u500dkv-cache\u538b\u7f29\uff0c\u5cf0\u503c\u5185\u5b58\u964d\u4f4e1.2\u500d\uff0cTTFT\u901f\u5ea6\u63d0\u53472\u500d\u3002\u5728\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u738763.8%\uff0c\u5728RVS\u4e0a\u8fbe\u523055.8%/3.7\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u4fdd\u6301\u6700\u5148\u8fdb\u51c6\u786e\u7387\u3002", "conclusion": "StreamingTOM\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u6709\u754c\u589e\u957f\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.18232", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18232", "abs": "https://arxiv.org/abs/2510.18232", "authors": ["Yuzheng Hu", "Ryan McKenna", "Da Yu", "Shanshan Wu", "Han Zhao", "Zheng Xu", "Peter Kairouz"], "title": "ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control", "comment": null, "summary": "Generating high-quality synthetic text under differential privacy (DP) is\ncritical for training and evaluating language models without compromising user\nprivacy. Prior work on synthesizing DP datasets often fail to preserve key\nstatistical attributes, suffer utility loss from the noise required by DP, and\nlack fine-grained control over generation. To address these challenges, we make\ntwo contributions. First, we introduce a hierarchical framework that decomposes\nDP synthetic text generation into two subtasks: feature learning and\nconditional text generation. This design explicitly incorporates learned\nfeatures into the generation process and simplifies the end-to-end synthesis\ntask. Through systematic ablations, we identify the most effective\nconfiguration: a rich tabular schema as feature, a DP tabular synthesizer, and\na DP fine-tuned conditional generator, which we term ACTG\n(Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL),\na post-training method that improves the instruction-following ability of ACTG\nfor conditional generation. ARL combines RL to boost control with an SFT anchor\non best-of-$N$ data to prevent reward hacking. Together, these components form\nour end-to-end algorithm ACTG-ARL, which advances both the quality of DP\nsynthetic text (+20% MAUVE over prior work) and the control of the conditional\ngenerator under strong privacy guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86ACTG-ARL\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u6587\u672c\u751f\u6210\u8d28\u91cf\u4e0e\u63a7\u5236\u80fd\u529b", "motivation": "\u89e3\u51b3\u73b0\u6709\u5dee\u5206\u9690\u79c1\u6587\u672c\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u7edf\u8ba1\u5c5e\u6027\u3001\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u548c\u7cbe\u7ec6\u63a7\u5236\u751f\u6210\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u5206\u5c42\u6846\u67b6\u5206\u89e3\u4e3a\u7279\u5f81\u5b66\u4e60\u548c\u6761\u4ef6\u6587\u672c\u751f\u6210\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528DP\u8868\u683c\u5408\u6210\u5668\u548cDP\u5fae\u8c03\u6761\u4ef6\u751f\u6210\u5668\uff0c\u5e76\u5f15\u5165\u951a\u5b9aRL\u65b9\u6cd5\u63d0\u5347\u63a7\u5236\u80fd\u529b", "result": "\u5728\u5f3a\u9690\u79c1\u4fdd\u8bc1\u4e0b\uff0cDP\u5408\u6210\u6587\u672c\u8d28\u91cf\u63d0\u534720%\uff08MAUVE\u6307\u6807\uff09\uff0c\u6761\u4ef6\u751f\u6210\u5668\u7684\u63a7\u5236\u80fd\u529b\u663e\u8457\u589e\u5f3a", "conclusion": "ACTG-ARL\u6846\u67b6\u5728\u5dee\u5206\u9690\u79c1\u6587\u672c\u751f\u6210\u7684\u8d28\u91cf\u548c\u63a7\u5236\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65"}}
{"id": "2510.18240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18240", "abs": "https://arxiv.org/abs/2510.18240", "authors": ["Haobin Li", "Yijie Lin", "Peng Hu", "Mouxing Yang", "Xi Peng"], "title": "Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment", "comment": "30 pages, 12 figures", "summary": "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nacross heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is\ndescribed by attributes from various modalities. Existing methods typically\nassume that both intra-entity and inter-graph correspondences are faultless,\nwhich is often violated in real-world MMKGs due to the reliance on expert\nannotations. In this paper, we reveal and study a highly practical yet\nunder-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).\nDNC refers to misalignments in both intra-entity (entity-attribute) and\ninter-graph (entity-entity and attribute-attribute) correspondences. To address\nthe DNC problem, we propose a robust MMEA framework termed RULE. RULE first\nestimates the reliability of both intra-entity and inter-graph correspondences\nvia a dedicated two-fold principle. Leveraging the estimated reliabilities,\nRULE mitigates the negative impact of intra-entity noise during attribute\nfusion and prevents overfitting to noisy inter-graph correspondences during\ninter-graph discrepancy elimination. Beyond the training-time designs, RULE\nfurther incorporates a correspondence reasoning module that uncovers the\nunderlying attribute-attribute connection across graphs, guaranteeing more\naccurate equivalent entity identification. Extensive experiments on five\nbenchmarks verify the effectiveness of our method against the DNC compared with\nseven state-of-the-art methods.The code is available at\n\\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}", "AI": {"tldr": "\u63d0\u51faRULE\u6846\u67b6\u89e3\u51b3\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u4e2d\u7684\u53cc\u91cd\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u5305\u62ec\u5b9e\u4f53-\u5c5e\u6027\u548c\u56fe\u95f4\u5bf9\u5e94\u5173\u7cfb\u7684\u566a\u58f0\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u4f30\u8ba1\u548c\u5bf9\u5e94\u63a8\u7406\u6a21\u5757\u63d0\u5347\u5bf9\u9f50\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u5b9e\u4f53\u5185\u90e8\u548c\u56fe\u95f4\u5bf9\u5e94\u5173\u7cfb\u5b8c\u7f8e\u65e0\u7f3a\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\uff0c\u5b58\u5728\u53cc\u91cd\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u5f71\u54cd\u5bf9\u9f50\u6548\u679c\u3002", "method": "RULE\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u539f\u5219\u4f30\u8ba1\u5bf9\u5e94\u5173\u7cfb\u7684\u53ef\u9760\u6027\uff0c\u5728\u5c5e\u6027\u878d\u5408\u65f6\u51cf\u8f7b\u5b9e\u4f53\u5185\u90e8\u566a\u58f0\u5f71\u54cd\uff0c\u5728\u6d88\u9664\u56fe\u95f4\u5dee\u5f02\u65f6\u907f\u514d\u5bf9\u566a\u58f0\u5bf9\u5e94\u8fc7\u5ea6\u62df\u5408\uff0c\u5e76\u52a0\u5165\u5bf9\u5e94\u63a8\u7406\u6a21\u5757\u53d1\u73b0\u56fe\u95f4\u5c5e\u6027\u8fde\u63a5\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRULE\u76f8\u6bd4\u4e03\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u53cc\u91cd\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\u3002", "conclusion": "RULE\u6846\u67b6\u901a\u8fc7\u53ef\u9760\u6027\u4f30\u8ba1\u548c\u5bf9\u5e94\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u4e2d\u7684\u53cc\u91cd\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u4f53\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.18475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18475", "abs": "https://arxiv.org/abs/2510.18475", "authors": ["Mariano Barone", "Antonio Laudante", "Giuseppe Riccio", "Antonio Romano", "Marco Postiglione", "Vincenzo Moscato"], "title": "DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP", "comment": null, "summary": "The extraction of pharmacological knowledge from regulatory documents has\nbecome a key focus in biomedical natural language processing, with applications\nranging from adverse event monitoring to AI-assisted clinical decision support.\nHowever, research in this field has predominantly relied on English-language\ncorpora such as DrugBank, leaving a significant gap in resources tailored to\nother healthcare systems. To address this limitation, we introduce DART (Drug\nAnnotation from Regulatory Texts), the first structured corpus of Italian\nSummaries of Product Characteristics derived from the official repository of\nthe Italian Medicines Agency (AIFA). The dataset was built through a\nreproducible pipeline encompassing web-scale document retrieval, semantic\nsegmentation of regulatory sections, and clinical summarization using a\nfew-shot-tuned large language model with low-temperature decoding. DART\nprovides structured information on key pharmacological domains such as\nindications, adverse drug reactions, and drug-drug interactions. To validate\nits utility, we implemented an LLM-based drug interaction checker that\nleverages the dataset to infer clinically meaningful interactions. Experimental\nresults show that instruction-tuned LLMs can accurately infer potential\ninteractions and their clinical implications when grounded in the structured\ntextual fields of DART. We publicly release our code on GitHub:\nhttps://github.com/PRAISELab-PicusLab/DART.", "AI": {"tldr": "\u63d0\u51fa\u4e86DART\uff0c\u9996\u4e2a\u57fa\u4e8e\u610f\u5927\u5229\u836f\u54c1\u7ba1\u7406\u5c40\u5b98\u65b9\u6587\u4ef6\u7684\u610f\u5927\u5229\u8bed\u836f\u54c1\u7279\u6027\u6458\u8981\u7ed3\u6784\u5316\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u586b\u8865\u975e\u82f1\u8bed\u836f\u7406\u5b66\u77e5\u8bc6\u63d0\u53d6\u7684\u8d44\u6e90\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u836f\u7406\u5b66\u77e5\u8bc6\u63d0\u53d6\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u82f1\u8bed\u8bed\u6599\u5e93\u5982DrugBank\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5176\u4ed6\u533b\u7597\u7cfb\u7edf\u7684\u8d44\u6e90\uff0c\u7279\u522b\u662f\u610f\u5927\u5229\u8bed\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u62ec\u7f51\u7edc\u89c4\u6a21\u6587\u6863\u68c0\u7d22\u3001\u76d1\u7ba1\u7ae0\u8282\u8bed\u4e49\u5206\u5272\uff0c\u4ee5\u53ca\u4f7f\u7528\u4f4e\u6e29\u5ea6\u89e3\u7801\u7684\u5c11\u6837\u672c\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e34\u5e8a\u6458\u8981\u3002", "result": "DART\u63d0\u4f9b\u4e86\u5173\u952e\u836f\u7406\u5b66\u9886\u57df\uff08\u5982\u9002\u5e94\u75c7\u3001\u4e0d\u826f\u53cd\u5e94\u3001\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff09\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u7684LLM\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u68c0\u67e5\u5668\u80fd\u591f\u51c6\u786e\u63a8\u65ad\u6f5c\u5728\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u4e34\u5e8a\u610f\u4e49\u3002", "conclusion": "\u6307\u4ee4\u8c03\u4f18\u7684LLMs\u5728\u57fa\u4e8eDART\u7ed3\u6784\u5316\u6587\u672c\u5b57\u6bb5\u65f6\u80fd\u591f\u51c6\u786e\u63a8\u65ad\u6f5c\u5728\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u4e34\u5e8a\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u610f\u5927\u5229\u8bed\u836f\u7406\u5b66\u77e5\u8bc6\u63d0\u53d6\u7684\u8d44\u6e90\u7a7a\u767d\u3002"}}
{"id": "2510.18668", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18668", "abs": "https://arxiv.org/abs/2510.18668", "authors": ["Mustafa Fuad Rifet Ibrahim", "Tunc Alkanat", "Maurice Meijer", "Felix Manthey", "Alexander Schlaefer", "Peer Stelldinger"], "title": "Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches", "comment": "Submitted to the IEEE Journal of Biomedical And Health Informatics", "summary": "The vast majority of cardiovascular diseases may be preventable if early\nsigns and risk factors are detected. Cardiovascular monitoring with body-worn\nsensor devices like sensor patches allows for the detection of such signs while\npreserving the freedom and comfort of patients. However, the analysis of the\nsensor data must be robust, reliable, efficient, and highly accurate. Deep\nlearning methods can automate data interpretation, reducing the workload of\nclinicians. In this work, we analyze the feasibility of applying deep learning\nmodels to the classification of synchronized electrocardiogram (ECG) and\nphonocardiogram (PCG) recordings on resource-constrained medical edge devices.\nWe propose a convolutional neural network with early fusion of data to solve a\nbinary classification problem. We train and validate our model on the\nsynchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset.\nOur approach reduces memory footprint and compute cost by three orders of\nmagnitude compared to the state-of-the-art while maintaining competitive\naccuracy. We demonstrate the applicability of our proposed model on medical\nedge devices by analyzing energy consumption on a microcontroller and an\nexperimental sensor device setup, confirming that on-device inference can be\nmore energy-efficient than continuous data streaming.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fc3\u7535\u4fe1\u53f7\u548c\u5fc3\u97f3\u4fe1\u53f7\u5206\u7c7b\u7684\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u8bbe\u5907\u80fd\u6301\u7eed\u76d1\u6d4b\u4f46\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u6570\u636e\u5206\u6790\u3002\u6df1\u5ea6\u5b66\u4e60\u53ef\u81ea\u52a8\u5316\u6570\u636e\u89e3\u91ca\uff0c\u51cf\u8f7b\u4e34\u5e8a\u533b\u751f\u8d1f\u62c5\uff0c\u4f46\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u91c7\u7528\u65e9\u671f\u6570\u636e\u878d\u5408\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u540c\u6b65\u5fc3\u7535\u56fe\u548c\u5fc3\u97f3\u56fe\u7684\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u5728Physionet Challenge 2016\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u65b9\u6cd5\u51cf\u5c11\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u5fae\u63a7\u5236\u5668\u548c\u5b9e\u9a8c\u4f20\u611f\u5668\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u80fd\u8017\u4f18\u52bf\uff0c\u8bc1\u660e\u8bbe\u5907\u7aef\u63a8\u7406\u6bd4\u8fde\u7eed\u6570\u636e\u6d41\u66f4\u8282\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u533b\u7597\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u5fc3\u7535\u548c\u5fc3\u97f3\u4fe1\u53f7\u5206\u7c7b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u7a7f\u6234\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18812", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18812", "abs": "https://arxiv.org/abs/2510.18812", "authors": ["Jes\u00fas Garc\u00eda Fern\u00e1ndez", "Nasir Ahmad", "Marcel van Gerven"], "title": "A Unified Perspective on Optimization in Machine Learning and Neuroscience: From Gradient Descent to Neural Adaptation", "comment": null, "summary": "Iterative optimization is central to modern artificial intelligence (AI) and\nprovides a crucial framework for understanding adaptive systems. This review\nprovides a unified perspective on this subject, bridging classic theory with\nneural network training and biological learning. Although gradient-based\nmethods, powered by the efficient but biologically implausible backpropagation\n(BP), dominate machine learning, their computational demands can hinder\nscalability in high-dimensional settings. In contrast, derivative-free or\nzeroth-order (ZO) optimization feature computationally lighter approaches that\nrely only on function evaluations and randomness. While generally less sample\nefficient, recent breakthroughs demonstrate that modern ZO methods can\neffectively approximate gradients and achieve performance competitive with BP\nin neural network models. This ZO paradigm is also particularly relevant for\nbiology. Its core principles of random exploration (probing) and\nfeedback-guided adaptation (reinforcing) parallel key mechanisms of biological\nlearning, offering a mathematically principled perspective on how the brain\nlearns. In this review, we begin by categorizing optimization approaches based\non the order of derivative information they utilize, ranging from first-,\nsecond-, and higher-order gradient-based to ZO methods. We then explore how\nthese methods are adapted to the unique challenges of neural network training\nand the resulting learning dynamics. Finally, we build upon these insights to\nview biological learning through an optimization lens, arguing that a ZO\nparadigm leverages the brain's intrinsic noise as a computational resource.\nThis framework not only illuminates our understanding of natural intelligence\nbut also holds vast implications for neuromorphic hardware, helping us design\nfast and energy-efficient AI systems that exploit intrinsic hardware noise.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u8fed\u4ee3\u4f18\u5316\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u8fde\u63a5\u7ecf\u5178\u7406\u8bba\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u548c\u751f\u7269\u5b66\u4e60\uff0c\u7279\u522b\u5173\u6ce8\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u5728AI\u548c\u751f\u7269\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u68af\u5ea6\u65b9\u6cd5\u867d\u7136\u4e3b\u5bfc\u673a\u5668\u5b66\u4e60\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u9ad8\u4e14\u5728\u751f\u7269\u5b66\u4e0a\u4e0d\u53ef\u4fe1\uff0c\u800c\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u66f4\u8f7b\u91cf\u4e14\u4e0e\u751f\u7269\u5b66\u4e60\u673a\u5236\u66f4\u76f8\u4f3c\uff0c\u4e3a\u7406\u89e3\u5927\u8111\u5b66\u4e60\u63d0\u4f9b\u4e86\u6570\u5b66\u539f\u7406\u89c6\u89d2\u3002", "method": "\u5c06\u4f18\u5316\u65b9\u6cd5\u6309\u5bfc\u6570\u4fe1\u606f\u4f7f\u7528\u7a0b\u5ea6\u5206\u7c7b\uff0c\u4ece\u4e00\u9636\u3001\u4e8c\u9636\u5230\u9ad8\u9636\u68af\u5ea6\u65b9\u6cd5\uff0c\u518d\u5230\u96f6\u9636\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6311\u6218\u548c\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u73b0\u4ee3\u96f6\u9636\u65b9\u6cd5\u80fd\u6709\u6548\u903c\u8fd1\u68af\u5ea6\uff0c\u5728\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u8fbe\u5230\u4e0e\u53cd\u5411\u4f20\u64ad\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u5176\u968f\u673a\u63a2\u7d22\u548c\u53cd\u9988\u5f15\u5bfc\u9002\u5e94\u673a\u5236\u4e0e\u751f\u7269\u5b66\u4e60\u6838\u5fc3\u673a\u5236\u5e73\u884c\u3002", "conclusion": "\u96f6\u9636\u4f18\u5316\u8303\u5f0f\u5c06\u5927\u8111\u56fa\u6709\u566a\u58f0\u4f5c\u4e3a\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e0d\u4ec5\u7167\u4eae\u4e86\u5bf9\u81ea\u7136\u667a\u80fd\u7684\u7406\u89e3\uff0c\u8fd8\u5bf9\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u8bbe\u8ba1\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u5229\u7528\u786c\u4ef6\u566a\u58f0\u7684\u5feb\u901f\u8282\u80fdAI\u7cfb\u7edf\u3002"}}
