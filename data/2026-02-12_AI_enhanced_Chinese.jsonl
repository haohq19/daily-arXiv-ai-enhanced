{"id": "2602.10182", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10182", "abs": "https://arxiv.org/abs/2602.10182", "authors": ["Benjamin R. Redhead", "Thomas L. Lee", "Peng Gu", "V\u00edctor Elvira", "Amos Storkey"], "title": "Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting", "comment": "Main Paper: 8 pages 3 figures Including Appendix and References: 19 pages 7 figures", "summary": "Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6838\u7684\u5ea6\u91cf\u65b9\u6cd5\uff1aSig-MMD\u548cCSig-MMD\uff0c\u7528\u4e8e\u6539\u8fdb\u6982\u7387\u9884\u6d4b\u8bc4\u4f30\uff0c\u80fd\u6355\u6349\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5e76\u7279\u522b\u5173\u6ce8\u5c3e\u90e8\u4e8b\u4ef6\u3002", "motivation": "\u5f53\u524d\u6982\u7387\u9884\u6d4b\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1\uff09\u5e38\u5047\u8bbe\u65f6\u95f4\u6b65\u6216\u53d8\u91cf\u95f4\u72ec\u7acb\uff1b2\uff09\u5bf9\u5c3e\u90e8\u4e8b\u4ef6\u7f3a\u4e4f\u654f\u611f\u6027\uff0c\u800c\u5c3e\u90e8\u4e8b\u4ef6\u5728\u5b9e\u9645\u51b3\u7b56\u4e2d\u6700\u4e3a\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6838\u7684\u5ea6\u91cf\uff1a\u7b7e\u540d\u6700\u5927\u5e73\u5747\u5dee\u5f02\uff08Sig-MMD\uff09\u548c\u65b0\u7684\u622a\u65adSig-MMD\uff08CSig-MMD\uff09\u3002\u5229\u7528\u7b7e\u540d\u6838\u6355\u6349\u590d\u6742\u7684\u53d8\u91cf\u95f4\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bf9\u7f3a\u5931\u6570\u636e\u4fdd\u6301\u9c81\u68d2\u3002CSig-MMD\u5f15\u5165\u622a\u65ad\u65b9\u6848\uff0c\u4f18\u5148\u8bc4\u4f30\u9884\u6d4b\u5c3e\u90e8\u4e8b\u4ef6\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4e25\u683c\u4fdd\u6301properness\u7279\u6027\u3002", "result": "\u8fd9\u4e9b\u5ea6\u91cf\u65b9\u6cd5\u80fd\u591f\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u76f4\u63a5\u591a\u6b65\u9884\u6d4b\uff0c\u4fc3\u8fdb\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6982\u7387\u7b97\u6cd5\u3002", "conclusion": "Sig-MMD\u548cCSig-MMD\u89e3\u51b3\u4e86\u5f53\u524d\u6982\u7387\u9884\u6d4b\u8bc4\u4f30\u6846\u67b6\u7684\u5173\u952e\u7f3a\u9677\uff0c\u63d0\u4f9b\u4e86\u80fd\u591f\u6355\u6349\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5e76\u7279\u522b\u5173\u6ce8\u5c3e\u90e8\u4e8b\u4ef6\u7684\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.10547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10547", "abs": "https://arxiv.org/abs/2602.10547", "authors": ["Yanchen Liu", "Yuang Fan", "Minghui Zhao", "Xiaofan Jiang"], "title": "ReSPEC: A Framework for Online Multispectral Sensor Reconfiguration in Dynamic Environments", "comment": "8 pages, 4 figures. This work has been submitted to the IEEE for possible publication", "summary": "Multi-sensor fusion is central to robust robotic perception, yet most existing systems operate under static sensor configurations, collecting all modalities at fixed rates and fidelity regardless of their situational utility. This rigidity wastes bandwidth, computation, and energy, and prevents systems from prioritizing sensors under challenging conditions such as poor lighting or occlusion. Recent advances in reinforcement learning (RL) and modality-aware fusion suggest the potential for adaptive perception, but prior efforts have largely focused on re-weighting features at inference time, ignoring the physical cost of sensor data collection. We introduce a framework that unifies sensing, learning, and actuation into a closed reconfiguration loop. A task-specific detection backbone extracts multispectral features (e.g. RGB, IR, mmWave, depth) and produces quantitative contribution scores for each modality. These scores are passed to an RL agent, which dynamically adjusts sensor configurations, including sampling frequency, resolution, sensing range, and etc., in real time. Less informative sensors are down-sampled or deactivated, while critical sensors are sampled at higher fidelity as environmental conditions evolve. We implement and evaluate this framework on a mobile rover, showing that adaptive control reduces GPU load by 29.3\\% with only a 5.3\\% accuracy drop compared to a heuristic baseline. These results highlight the potential of resource-aware adaptive sensing for embedded robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5c06\u611f\u77e5\u3001\u5b66\u4e60\u548c\u6267\u884c\u7edf\u4e00\u5230\u95ed\u73af\u91cd\u6784\u6846\u67b6\u4e2d\u7684\u81ea\u9002\u5e94\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7RL\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u914d\u7f6e\u4ee5\u8282\u7701\u8d44\u6e90", "motivation": "\u73b0\u6709\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u91c7\u7528\u9759\u6001\u914d\u7f6e\uff0c\u65e0\u8bba\u73af\u5883\u6761\u4ef6\u5982\u4f55\u90fd\u4ee5\u56fa\u5b9a\u901f\u7387\u548c\u4fdd\u771f\u5ea6\u6536\u96c6\u6240\u6709\u6a21\u6001\u6570\u636e\uff0c\u6d6a\u8d39\u5e26\u5bbd\u3001\u8ba1\u7b97\u548c\u80fd\u91cf\uff0c\u4e14\u65e0\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u4f18\u5148\u4f7f\u7528\u5173\u952e\u4f20\u611f\u5668", "method": "\u5f15\u5165\u4e00\u4e2a\u7edf\u4e00\u611f\u77e5\u3001\u5b66\u4e60\u548c\u6267\u884c\u7684\u95ed\u73af\u91cd\u6784\u6846\u67b6\uff1a\u4efb\u52a1\u7279\u5b9a\u68c0\u6d4b\u9aa8\u5e72\u63d0\u53d6\u591a\u5149\u8c31\u7279\u5f81\u5e76\u4ea7\u751f\u5404\u6a21\u6001\u7684\u8d21\u732e\u5206\u6570\uff0cRL\u4ee3\u7406\u6839\u636e\u8fd9\u4e9b\u5206\u6570\u5b9e\u65f6\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u914d\u7f6e\uff08\u91c7\u6837\u9891\u7387\u3001\u5206\u8fa8\u7387\u3001\u611f\u77e5\u8303\u56f4\u7b49\uff09", "result": "\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u548c\u8bc4\u4f30\u8be5\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u63a7\u5236\u76f8\u6bd4\u542f\u53d1\u5f0f\u57fa\u7ebf\u51cf\u5c1129.3%\u7684GPU\u8d1f\u8f7d\uff0c\u4ec5\u635f\u59315.3%\u7684\u51c6\u786e\u7387", "conclusion": "\u7ed3\u679c\u7a81\u663e\u4e86\u8d44\u6e90\u611f\u77e5\u81ea\u9002\u5e94\u611f\u77e5\u5728\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u914d\u7f6e\u5b9e\u73b0\u8d44\u6e90\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861"}}
{"id": "2602.10625", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10625", "abs": "https://arxiv.org/abs/2602.10625", "authors": ["Nanxu Gong", "Haotian Li", "Sixun Dong", "Jianxun Lian", "Yanjie Fu", "Xing Xie"], "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks", "comment": null, "summary": "Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u5e76\u4e0d\u6bd4\u975e\u63a8\u7406\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u6709\u65f6\u751a\u81f3\u66f4\u5dee\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u5728\u793e\u4ea4\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u6b63\u5f0f\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u8fd9\u4e9b\u4f18\u52bf\u662f\u5426\u80fd\u8f6c\u79fb\u5230\u5fc3\u7406\u7406\u8bba\u7b49\u793e\u4ea4\u8ba4\u77e5\u6280\u80fd\u4e0a\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u63a8\u7406\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5bf99\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u6bd4\u8f83\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u5fc3\u7406\u7406\u8bba\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u6790\u63ed\u793a\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u5e72\u9884\u65b9\u6cd5\uff1a\u6162\u5230\u5feb\u81ea\u9002\u5e94\u63a8\u7406\u548c\u601d\u8003\u5230\u5339\u914d\u6377\u5f84\u9884\u9632\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u5e76\u4e0d\u4e00\u81f4\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6709\u65f6\u8868\u73b0\u66f4\u5dee\u3002\u5206\u6790\u53d1\u73b0\uff1a1)\u6162\u601d\u8003\u5d29\u6e83\u73b0\u8c61\uff1b2)\u9002\u5ea6\u81ea\u9002\u5e94\u63a8\u7406\u6709\u76ca\uff1b3)\u9009\u9879\u5339\u914d\u6377\u5f84\u95ee\u9898\u3002\u5e72\u9884\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6b63\u5f0f\u63a8\u7406\uff08\u5982\u6570\u5b66\u3001\u7f16\u7a0b\uff09\u4e0a\u7684\u8fdb\u6b65\u4e0d\u80fd\u5b8c\u5168\u8f6c\u79fb\u5230\u5fc3\u7406\u7406\u8bba\u8fd9\u79cd\u5178\u578b\u7684\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e0a\u3002\u5b9e\u73b0\u7a33\u5065\u7684\u5fc3\u7406\u7406\u8bba\u9700\u8981\u5f00\u53d1\u8d85\u8d8a\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u7684\u72ec\u7279\u80fd\u529b\u3002"}}
{"id": "2602.10230", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10230", "abs": "https://arxiv.org/abs/2602.10230", "authors": ["Joesph An", "Phillip Keung", "Jiaqi Wang", "Orevaoghene Ahia", "Noah A. Smith"], "title": "Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs", "comment": "Under review. See https://github.com/inkitori/taudio/", "summary": "Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.", "AI": {"tldr": "\u63d0\u51fa\u5e27\u7ea7\u5185\u90e8\u5de5\u5177\u4f7f\u7528\u6846\u67b6\uff0c\u8ba9\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5229\u7528\u81ea\u8eab\u5185\u90e8\u97f3\u9891\u8868\u793a\u76f4\u63a5\u8fdb\u884c\u65f6\u95f4\u5b9a\u4f4d\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6587\u672ctoken\u7684\u65f6\u95f4\u6233\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86>50\u500d\u63a8\u7406\u52a0\u901f\u548c\u66f4\u597d\u7684\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u7684\u4efb\u52a1\uff08\u5982\u8bcd\u5bf9\u9f50\u3001\u8bf4\u8bdd\u4eba\u5206\u79bb\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u6587\u672ctoken\u751f\u6210\u65f6\u95f4\u6233\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u957f\u5ea6\u7684\u97f3\u9891\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u5e27\u7ea7\u5185\u90e8\u5de5\u5177\u4f7f\u7528\u6846\u67b6\uff0c\u8bad\u7ec3\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5229\u7528\u81ea\u8eab\u5185\u90e8\u97f3\u9891\u8868\u793a\u76f4\u63a5\u8fdb\u884c\u65f6\u95f4\u5b9a\u4f4d\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u9884\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u4e24\u4e2a\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff1a\u4e8c\u5143\u5e27\u5206\u7c7b\u5668\u548c\u65b0\u9896\u7684\u975e\u9f50\u6b21\u6cca\u677e\u8fc7\u7a0b\u635f\u5931\u51fd\u6570\u6765\u5efa\u6a21\u65f6\u95f4\u4e8b\u4ef6\u5f3a\u5ea6\u3002", "result": "\u5728\u8bcd\u5b9a\u4f4d\u3001\u8bf4\u8bdd\u4eba\u5206\u79bb\u548c\u4e8b\u4ef6\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8etoken\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b9e\u73b0\u4e86>50\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5e76\u5728\u8d85\u51fa\u5206\u5e03\u957f\u5ea6\u7684\u97f3\u9891\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u6807\u51c6\u57fa\u4e8etoken\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u5b8c\u5168\u5931\u6548\u3002", "conclusion": "\u5e27\u7ea7\u5185\u90e8\u5de5\u5177\u4f7f\u7528\u6846\u67b6\u4e3a\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u5e7b\u89c9\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.10319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10319", "abs": "https://arxiv.org/abs/2602.10319", "authors": ["Jiaxuan Zhu", "Siyu Huang"], "title": "A Low-Rank Defense Method for Adversarial Attack on Diffusion Models", "comment": "Accepted by ICME2025", "summary": "Recently, adversarial attacks for diffusion models as well as their fine-tuning process have been developed rapidly. To prevent the abuse of these attack algorithms from affecting the practical application of diffusion models, it is critical to develop corresponding defensive strategies. In this work, we propose an efficient defensive strategy, named Low-Rank Defense (LoRD), to defend the adversarial attack on Latent Diffusion Models (LDMs). LoRD introduces the merging idea and a balance parameter, combined with the low-rank adaptation (LoRA) modules, to detect and defend the adversarial samples. Based on LoRD, we build up a defense pipeline that applies the learned LoRD modules to help diffusion models defend against attack algorithms. Our method ensures that the LDM fine-tuned on both adversarial and clean samples can still generate high-quality images. To demonstrate the effectiveness of our approach, we conduct extensive experiments on facial and landscape images, and our method shows significantly better defense performance compared to the baseline methods.", "AI": {"tldr": "\u63d0\u51faLoRD\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u6a21\u5757\u548c\u5e73\u8861\u53c2\u6570\u6765\u68c0\u6d4b\u548c\u9632\u5fa1\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u5bf9\u6297\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u4e0a\u90fd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u53ca\u5176\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4e3a\u9632\u6b62\u8fd9\u4e9b\u653b\u51fb\u7b97\u6cd5\u5f71\u54cd\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u76f8\u5e94\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u63d0\u51faLoRD\u9632\u5fa1\u7b56\u7565\uff0c\u7ed3\u5408\u5408\u5e76\u601d\u60f3\u548c\u5e73\u8861\u53c2\u6570\uff0c\u5229\u7528\u4f4e\u79e9\u9002\u5e94\u6a21\u5757\u6765\u68c0\u6d4b\u548c\u9632\u5fa1\u5bf9\u6297\u6837\u672c\u3002\u57fa\u4e8eLoRD\u6784\u5efa\u9632\u5fa1\u6d41\u7a0b\uff0c\u5c06\u5b66\u4e60\u5230\u7684LoRD\u6a21\u5757\u5e94\u7528\u4e8e\u5e2e\u52a9\u6269\u6563\u6a21\u578b\u9632\u5fa1\u653b\u51fb\u7b97\u6cd5\u3002", "result": "\u5728\u9762\u90e8\u548c\u98ce\u666f\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u663e\u8457\u66f4\u597d\u7684\u9632\u5fa1\u6027\u80fd\uff0c\u786e\u4fdd\u5728\u5bf9\u6297\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u4e0a\u5fae\u8c03\u7684LDM\u4ecd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "conclusion": "LoRD\u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u80fd\u591f\u6210\u529f\u9632\u5fa1\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2602.10344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10344", "abs": "https://arxiv.org/abs/2602.10344", "authors": ["Xi Chen", "Arian Maleki", "Shirin Jalali"], "title": "Monte Carlo Maximum Likelihood Reconstruction for Digital Holography with Speckle", "comment": null, "summary": "In coherent imaging, speckle is statistically modeled as multiplicative noise, posing a fundamental challenge for image reconstruction. While maximum likelihood estimation (MLE) provides a principled framework for speckle mitigation, its application to coherent imaging system such as digital holography with finite apertures is hindered by the prohibitive cost of high-dimensional matrix inversion, especially at high resolutions. This computational burden has prevented the use of MLE-based reconstruction with physically accurate aperture modeling. In this work, we propose a randomized linear algebra approach that enables scalable MLE optimization without explicit matrix inversions in gradient computation. By exploiting the structural properties of sensing matrix and using conjugate gradient for likelihood gradient evaluation, the proposed algorithm supports accurate aperture modeling without the simplifying assumptions commonly imposed for tractability. We term the resulting method projected gradient descent with Monte Carlo estimation (PGD-MC). The proposed PGD-MC framework (i) demonstrates robustness to diverse and physically accurate aperture models, (ii) achieves substantial improvements in reconstruction quality and computational efficiency, and (iii) scales effectively to high-resolution digital holography. Extensive experiments incorporating three representative denoisers as regularization show that PGD-MC provides a flexible and effective MLE-based reconstruction framework for digital holography with finite apertures, consistently outperforming prior Plug-and-Play model-based iterative reconstruction methods in both accuracy and speed. Our code is available at: https://github.com/Computational-Imaging-RU/MC_Maximum_Likelihood_Digital_Holography_Speckle.", "AI": {"tldr": "\u63d0\u51faPGD-MC\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u4ee3\u6570\u6280\u672f\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u89e3\u51b3\u76f8\u5e72\u6210\u50cf\u4e2d\u6591\u70b9\u566a\u58f0\u548c\u6709\u9650\u5b54\u5f84\u5efa\u6a21\u7684\u8ba1\u7b97\u96be\u9898", "motivation": "\u76f8\u5e72\u6210\u50cf\u4e2d\u6591\u70b9\u566a\u58f0\u88ab\u5efa\u6a21\u4e3a\u4e58\u6027\u566a\u58f0\uff0c\u7ed9\u56fe\u50cf\u91cd\u5efa\u5e26\u6765\u6839\u672c\u6311\u6218\u3002\u6700\u5927\u4f3c\u7136\u4f30\u8ba1(MLE)\u4e3a\u6591\u70b9\u6291\u5236\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u4f46\u5728\u6570\u5b57\u5168\u606f\u7b49\u6709\u9650\u5b54\u5f84\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u7ef4\u77e9\u9635\u6c42\u9006\u7684\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u963b\u788d\u4e86\u7269\u7406\u7cbe\u786e\u5b54\u5f84\u5efa\u6a21\u7684MLE\u5e94\u7528", "method": "\u63d0\u51fa\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u4e0e\u8499\u7279\u5361\u6d1b\u4f30\u8ba1(PGD-MC)\u6846\u67b6\uff1a\u5229\u7528\u968f\u673a\u7ebf\u6027\u4ee3\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u8f6d\u68af\u5ea6\u8fdb\u884c\u4f3c\u7136\u68af\u5ea6\u8bc4\u4f30\uff0c\u907f\u514d\u663e\u5f0f\u77e9\u9635\u6c42\u9006\uff1b\u7ed3\u5408\u4e09\u79cd\u4ee3\u8868\u6027\u53bb\u566a\u5668\u4f5c\u4e3a\u6b63\u5219\u5316", "result": "PGD-MC\u6846\u67b6\uff1a(1)\u5bf9\u591a\u6837\u4e14\u7269\u7406\u7cbe\u786e\u7684\u5b54\u5f84\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6027\uff1b(2)\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff1b(3)\u80fd\u6709\u6548\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\u6570\u5b57\u5168\u606f\uff1b\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u5148\u524d\u7684\u5373\u63d2\u5373\u7528\u6a21\u578b\u8fed\u4ee3\u91cd\u5efa\u65b9\u6cd5", "conclusion": "PGD-MC\u4e3a\u6709\u9650\u5b54\u5f84\u6570\u5b57\u5168\u606f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u6709\u6548\u7684MLE\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u4ee3\u6570\u6280\u672f\u89e3\u51b3\u4e86\u4f20\u7edfMLE\u8ba1\u7b97\u74f6\u9888\uff0c\u652f\u6301\u7269\u7406\u7cbe\u786e\u7684\u5b54\u5f84\u5efa\u6a21\u800c\u4e0d\u9700\u8981\u7b80\u5316\u5047\u8bbe"}}
{"id": "2602.10303", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10303", "abs": "https://arxiv.org/abs/2602.10303", "authors": ["Haoling Wang", "Lang Zeng", "Tao Sun", "Youngjoo Cho", "Ying Ding"], "title": "ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data", "comment": null, "summary": "Predicting time-to-event outcomes when event times are interval censored is challenging because the exact event time is unobserved. Many existing survival analysis approaches for interval-censored data rely on strong model assumptions or cannot handle high-dimensional predictors. We develop ICODEN, an ordinary differential equation-based neural network for interval-censored data that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation. ICODEN does not require the proportional hazards assumption or a prespecified parametric form for the hazard function, thereby permitting flexible survival modeling. Across simulation settings with proportional or non-proportional hazards and both linear and nonlinear covariate effects, ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases. Applications to data from multiple phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) and to two Age-Related Eye Disease Studies (AREDS and AREDS2) for age-related macular degeneration (AMD) demonstrate ICODEN's robust prediction performance. In both applications, predicting time-to-AD or time-to-late AMD, ICODEN effectively uses hundreds to more than 1,000 SNPs and supports data-driven subgroup identification with differential progression risk profiles. These results establish ICODEN as a practical assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings.", "AI": {"tldr": "ICOden\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u533a\u95f4\u5220\u5931\u751f\u5b58\u6570\u636e\uff0c\u65e0\u9700\u6bd4\u4f8b\u98ce\u9669\u5047\u8bbe\u6216\u53c2\u6570\u5316\u98ce\u9669\u51fd\u6570\u5f62\u5f0f\uff0c\u5728\u9ad8\u7ef4\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "motivation": "\u533a\u95f4\u5220\u5931\u751f\u5b58\u6570\u636e\u7684\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u786e\u5207\u4e8b\u4ef6\u65f6\u95f4\u672a\u77e5\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5f3a\u6a21\u578b\u5047\u8bbe\uff0c\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u9884\u6d4b\u56e0\u5b50\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u5047\u8bbe\u66f4\u5c11\u7684\u65b9\u6cd5\u3002", "method": "ICOden\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u98ce\u9669\u51fd\u6570\uff0c\u901a\u8fc7\u6c42\u89e3\u5e38\u5fae\u5206\u65b9\u7a0b\u83b7\u5f97\u7d2f\u79ef\u98ce\u9669\u51fd\u6570\u3002\u8be5\u65b9\u6cd5\u4e0d\u8981\u6c42\u6bd4\u4f8b\u98ce\u9669\u5047\u8bbe\uff0c\u4e5f\u4e0d\u9884\u8bbe\u98ce\u9669\u51fd\u6570\u7684\u53c2\u6570\u5f62\u5f0f\uff0c\u5141\u8bb8\u7075\u6d3b\u7684\u751f\u5b58\u5efa\u6a21\u3002", "result": "\u5728\u6a21\u62df\u7814\u7a76\u4e2d\uff0c\u65e0\u8bba\u6bd4\u4f8b\u98ce\u9669\u8fd8\u662f\u975e\u6bd4\u4f8b\u98ce\u9669\uff0c\u7ebf\u6027\u6216\u975e\u7ebf\u6027\u534f\u53d8\u91cf\u6548\u5e94\uff0cICOden\u90fd\u80fd\u4fdd\u6301\u6ee1\u610f\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e14\u968f\u7740\u9884\u6d4b\u56e0\u5b50\u6570\u91cf\u589e\u52a0\u4fdd\u6301\u7a33\u5b9a\u3002\u5728ADNI\u548cAREDS/AREDS2\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cICOden\u80fd\u6709\u6548\u5229\u7528\u6570\u767e\u5230\u4e0a\u5343\u4e2aSNPs\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u4e9a\u7ec4\u8bc6\u522b\u3002", "conclusion": "ICOden\u4e3a\u9ad8\u7ef4\u751f\u7269\u533b\u5b66\u73af\u5883\u4e2d\u7684\u533a\u95f4\u5220\u5931\u751f\u5b58\u6570\u636e\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u5047\u8bbe\u8f83\u5c11\u7684\u65b9\u6cd5\u5de5\u5177\uff0c\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\u7b49\u75be\u75c5\u7684\u65f6\u95f4\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.10961", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.10961", "abs": "https://arxiv.org/abs/2602.10961", "authors": ["Simone Orelli", "Mirko Mizzoni", "Antonio Franchi"], "title": "Stability Analysis of Geometric Control for a Canonical Class of Underactuated Aerial Vehicles with Spurious Forces", "comment": null, "summary": "Standard geometric control relies on force-moment decoupling, an assumption that breaks down in many aerial platforms due to spurious forces naturally induced by control moments. While strategies for such coupled systems have been validated experimentally, a rigorous theoretical certification of their stability is currently missing. This work fills this gap by providing the first formal stability analysis for a generic class of floating rigid bodies subject to spurious forces. We introduce a canonical model and construct a Lyapunov-based proof establishing local exponential stability of the hovering equilibrium. Crucially, the analysis explicitly addresses the structural challenges - specifically the induced non-minimum-phase behavior - that prevent the application of standard cascade arguments.", "AI": {"tldr": "\u9996\u6b21\u4e3a\u53d7\u5bc4\u751f\u529b\u5f71\u54cd\u7684\u6d6e\u52a8\u521a\u4f53\u7cfb\u7edf\u63d0\u4f9b\u5f62\u5f0f\u5316\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u60ac\u505c\u5e73\u8861\u70b9\u7684\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027", "motivation": "\u4f20\u7edf\u51e0\u4f55\u63a7\u5236\u4f9d\u8d56\u529b-\u529b\u77e9\u89e3\u8026\u5047\u8bbe\uff0c\u4f46\u5728\u8bb8\u591a\u7a7a\u4e2d\u5e73\u53f0\u4e2d\uff0c\u63a7\u5236\u529b\u77e9\u4f1a\u81ea\u7136\u8bf1\u53d1\u5bc4\u751f\u529b\uff0c\u5bfc\u81f4\u8be5\u5047\u8bbe\u5931\u6548\u3002\u867d\u7136\u5df2\u6709\u9488\u5bf9\u6b64\u7c7b\u8026\u5408\u7cfb\u7edf\u7684\u7b56\u7565\u5f97\u5230\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f46\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u7a33\u5b9a\u6027\u8bc1\u660e\u3002", "method": "\u5f15\u5165\u89c4\u8303\u6a21\u578b\uff0c\u6784\u5efa\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u7684\u8bc1\u660e\uff0c\u5efa\u7acb\u60ac\u505c\u5e73\u8861\u70b9\u7684\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\u3002\u5206\u6790\u660e\u786e\u5904\u7406\u4e86\u7ed3\u6784\u6311\u6218\u2014\u2014\u7279\u522b\u662f\u8bf1\u5bfc\u7684\u975e\u6700\u5c0f\u76f8\u4f4d\u884c\u4e3a\u2014\u2014\u8fd9\u4e9b\u6311\u6218\u963b\u788d\u4e86\u6807\u51c6\u7ea7\u8054\u8bba\u8bc1\u7684\u5e94\u7528\u3002", "result": "\u4e3a\u53d7\u5bc4\u751f\u529b\u5f71\u54cd\u7684\u901a\u7528\u6d6e\u52a8\u521a\u4f53\u7c7b\u522b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5f62\u5f0f\u5316\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u60ac\u505c\u5e73\u8861\u70b9\u7684\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\u3002", "conclusion": "\u586b\u8865\u4e86\u8026\u5408\u7cfb\u7edf\u7a33\u5b9a\u6027\u7406\u8bba\u8ba4\u8bc1\u7684\u7a7a\u767d\uff0c\u4e3a\u53d7\u5bc4\u751f\u529b\u5f71\u54cd\u7684\u6d6e\u52a8\u521a\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7a33\u5b9a\u6027\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u51e0\u4f55\u63a7\u5236\u65b9\u6cd5\u5728\u975e\u89e3\u8026\u7cfb\u7edf\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.10385", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10385", "abs": "https://arxiv.org/abs/2602.10385", "authors": ["Jia Li", "Yu Hou", "Rui Zhang"], "title": "Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series", "comment": "7 pages of body text", "summary": "Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.\n  Intuitively, we need a method capable of evaluating the \"degree of alignment\" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \\emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.\n  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \\emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.", "AI": {"tldr": "LITT\u662f\u4e00\u79cd\u65b0\u578b\u65f6\u5e8fTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u5bf9\u65f6\u95f4\u7ebf\u5bf9\u9f50\u5e8f\u5217\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u65f6\u5e8f\u805a\u7126\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4e34\u5e8a\u8f68\u8ff9\u4e2a\u6027\u5316\u89e3\u91ca\u548c\u5fc3\u810f\u6bd2\u6027\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4ece\u5927\u89c4\u6a21\u65f6\u5e8f\u6570\u636e\u4e2d\u81ea\u52a8\u53d1\u73b0\u4e2a\u6027\u5316\u5e8f\u5217\u4e8b\u4ef6\u5bf9\u7cbe\u51c6\u533b\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\uff08\u5982Transformer\uff09\u5927\u591a\u5ffd\u7565\u4e8b\u4ef6\u65f6\u5e8f\u548c\u987a\u5e8f\uff0c\u65e0\u6cd5\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u3002\u9700\u8981\u4e00\u79cd\u80fd\u8bc4\u4f30\u60a3\u8005\u7279\u5b9a\u8f68\u8ff9\u5bf9\u9f50\u7a0b\u5ea6\u5e76\u8bc6\u522b\u5171\u4eab\u6a21\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLITT\uff08Timing-Transformer\uff09\u67b6\u6784\uff0c\u5c06\u65f6\u95f4\u4f5c\u4e3a\u53ef\u8ba1\u7b97\u7ef4\u5ea6\uff0c\u5728\u865a\u62df\"\u76f8\u5bf9\u65f6\u95f4\u7ebf\"\u4e0a\u4e34\u65f6\u5bf9\u9f50\u5e8f\u5217\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u65f6\u5e8f\u805a\u7126\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u4e34\u5e8a\u8f68\u8ff9\u63d0\u4f9b\u4e2a\u6027\u5316\u89e3\u91ca\u3002", "result": "\u57283276\u540d\u4e73\u817a\u764c\u60a3\u8005\u7684\u771f\u5b9e\u4e16\u754c\u7eb5\u5411EHR\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6709\u6548\u6027\uff0c\u6210\u529f\u9884\u6d4b\u5fc3\u810f\u6bd2\u6027\u8bf1\u53d1\u5fc3\u810f\u75c5\u7684\u53d1\u75c5\u65f6\u95f4\u3002\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u51c6\u548c\u6700\u5148\u8fdb\u7684\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "LITT\u4ee3\u8868\u4e86\u4e34\u5e8aAI\u7cbe\u51c6\u533b\u7597\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u4f5c\u4e3a\u53ef\u8ba1\u7b97\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u60a3\u8005\u8f68\u8ff9\u7684\u4e2a\u6027\u5316\u89e3\u91ca\u548c\u65f6\u5e8f\u6a21\u5f0f\u8bc6\u522b\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.11150", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11150", "abs": "https://arxiv.org/abs/2602.11150", "authors": ["Manan H Anjaria", "Mehmet Enes Erciyes", "Vedant Ghatnekar", "Neha Navarkar", "Haritheja Etukuru", "Xiaole Jiang", "Kanad Patel", "Dhawal Kabra", "Nicholas Wojno", "Radhika Ajay Prayage", "Soumith Chintala", "Lerrel Pinto", "Nur Muhammad Mahi Shafiullah", "Zichen Jeff Cui"], "title": "YOR: Your Own Mobile Manipulator for Generalizable Robotics", "comment": null, "summary": "Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR's capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/", "AI": {"tldr": "YOR\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u4f4e\u6210\u672c\uff08<1\u4e07\u7f8e\u5143\uff09\u7684\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u5168\u5411\u79fb\u52a8\u5e95\u76d8\u3001\u5782\u76f4\u5347\u964d\u673a\u6784\u548c\u53cc\u81c2\u7cfb\u7edf\uff0c\u7528\u4e8e\u5168\u8eab\u534f\u8c03\u64cd\u4f5c\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\u548c\u9a71\u52a8\u5668\u7684\u666e\u53ca\uff0c\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u9700\u6c42\u589e\u957f\uff0c\u4f46\u79fb\u52a8\u64cd\u4f5c\u7684\u6700\u4f73\u5f62\u6001\uff08\u5c24\u5176\u662f\u5728\u9884\u7b97\u6709\u9650\u60c5\u51b5\u4e0b\uff09\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u6613\u4e8e\u7ec4\u88c5\u7684\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4ebaYOR\uff0c\u91c7\u7528\u73b0\u6210\u7ec4\u4ef6\uff0c\u5305\u62ec\u5168\u5411\u79fb\u52a8\u5e95\u76d8\u3001\u5782\u76f4\u5347\u964d\u673a\u6784\u548c\u4e24\u4e2a\u5e26\u5939\u722a\u7684\u673a\u68b0\u81c2\uff0c\u5b9e\u73b0\u5168\u8eab\u79fb\u52a8\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "result": "YOR\u80fd\u591f\u5b8c\u6210\u9700\u8981\u5168\u8eab\u534f\u8c03\u63a7\u5236\u3001\u53cc\u624b\u64cd\u4f5c\u548c\u81ea\u4e3b\u5bfc\u822a\u7684\u4efb\u52a1\uff0c\u5728\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u4e2d\u63d0\u4f9b\u4e0e\u73b0\u6709\u5e73\u53f0\u76f8\u5f53\u7684\u529f\u80fd\uff0c\u4f46\u6210\u672c\u4ec5\u4e3a\u73b0\u6709\u5e73\u53f0\u7684\u4e00\u5c0f\u90e8\u5206\u3002", "conclusion": "YOR\u4e3a\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u529f\u80fd\u7ade\u4e89\u6027\u5f3a\u3001\u6210\u672c\u4f4e\u5ec9\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.10801", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10801", "abs": "https://arxiv.org/abs/2602.10801", "authors": ["Haotian Sheng", "Heyong Wang", "Ming Hong", "Hongman He", "Junqiu Liu"], "title": "Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.", "AI": {"tldr": "\u63d0\u51faLSCL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u8fbe\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5185\u5bb9\u751f\u6210\u5931\u771f\uff08\u5e7b\u89c9\uff09\u95ee\u9898\uff0c\u6838\u5fc3\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u81ea\u8eab\u5b58\u50a8\u77e5\u8bc6\u7684\u8ba4\u77e5\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u6837\u8868\u8fbe\u8d85\u51fa\u77e5\u8bc6\u8fb9\u754c\u7684\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u767d\u76d2\u6a21\u578b\uff0c\u800c\u9ed1\u76d2\u6a21\u578b\uff08\u4ec5\u63d0\u4f9bAPI\u8bbf\u95ee\uff09\u7684\u77e5\u8bc6\u8fb9\u754c\u8868\u8fbe\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faLSCL\uff08LLM-Supervised Confidence Learning\uff09\uff0c\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u8bbe\u8ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u4ee5\u9ed1\u76d2LLM\u7684\u8f93\u5165\u95ee\u9898\u3001\u8f93\u51fa\u7b54\u6848\u548ctoken\u6982\u7387\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u8f93\u5165\u4e0e\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u72b6\u6001\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u5b9e\u73b0\u9ed1\u76d2LLM\u77e5\u8bc6\u8fb9\u754c\u7684\u91cf\u5316\u548c\u8868\u8fbe\u3002\u5bf9\u4e8e\u4e0d\u652f\u6301token\u6982\u7387\u8bbf\u95ee\u7684\u9ed1\u76d2LLM\uff0c\u8fd8\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e3b\u6d41\u9ed1\u76d2LLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSCL\u80fd\u6709\u6548\u5e2e\u52a9\u9ed1\u76d2LLM\u51c6\u786e\u8868\u8fbe\u77e5\u8bc6\u8fb9\u754c\uff0c\u5728\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002\u66ff\u4ee3\u65b9\u6cd5\u7684\u6027\u80fd\u63a5\u8fd1LSCL\uff0c\u4e5f\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LSCL\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u8fb9\u754c\u8868\u8fbe\u7684\u95ee\u9898\uff0c\u4e3a\u51cf\u5c11\u5e7b\u89c9\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.11028", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11028", "abs": "https://arxiv.org/abs/2602.11028", "authors": ["Artsvik Avetisyan", "Sachin Kumar"], "title": "Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study", "comment": null, "summary": "Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches.\n  Methods: This study analyzes spontaneous speech transcripts from the DementiaBank Pitt Corpus using three linguistic representations: raw cleaned text, a part-of-speech (POS)-enhanced representation combining lexical and grammatical information, and a POS-only syntactic representation. Logistic regression and random forest models were evaluated under two protocols: transcript-level train-test splits and subject-level five-fold cross-validation to prevent speaker overlap. Model interpretability was examined using global feature importance, and statistical validation was conducted using Mann-Whitney U tests with Cliff's delta effect sizes.\n  Results: Across representations, models achieved stable performance, with syntactic and grammatical features retaining strong discriminative power even in the absence of lexical content. Subject-level evaluation yielded more conservative but consistent results, particularly for POS-enhanced and POS-only representations. Statistical analysis revealed significant group differences in functional word usage, lexical diversity, sentence structure, and discourse coherence, aligning closely with machine learning feature importance findings.\n  Conclusion: The results demonstrate that abstract linguistic features capture robust markers of early cognitive decline under clinically realistic evaluation. By combining interpretable machine learning with non-parametric statistical validation, this study supports the use of linguistically grounded features for transparent and reliable language-based cognitive screening.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u4e09\u79cd\u8bed\u8a00\u8868\u5f81\uff08\u539f\u59cb\u6587\u672c\u3001\u8bcd\u6027\u589e\u5f3a\u3001\u7eaf\u8bcd\u6027\uff09\u5206\u6790\u81ea\u53d1\u8bed\u97f3\uff0c\u53d1\u73b0\u8bed\u6cd5\u548c\u53e5\u6cd5\u7279\u5f81\u5728\u7f3a\u4e4f\u8bcd\u6c47\u5185\u5bb9\u65f6\u4ecd\u80fd\u6709\u6548\u533a\u5206\u8ba4\u77e5\u8870\u9000\uff0c\u652f\u6301\u57fa\u4e8e\u8bed\u8a00\u7279\u5f81\u7684\u900f\u660e\u8ba4\u77e5\u7b5b\u67e5\u3002", "motivation": "\u81ea\u53d1\u8bed\u8a00\u4e2d\u7684\u7ec6\u5fae\u53d8\u5316\u662f\u8ba4\u77e5\u8870\u9000\u7684\u6700\u65e9\u6307\u6807\u4e4b\u4e00\u3002\u8bc6\u522b\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u6807\u8bb0\u53ef\u4ee5\u652f\u6301\u900f\u660e\u4e14\u4e34\u5e8a\u57fa\u7840\u7684\u7b5b\u67e5\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e9\u671f\u75f4\u5446\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528DementiaBank Pitt Corpus\u7684\u81ea\u53d1\u8bed\u97f3\u8f6c\u5f55\uff0c\u91c7\u7528\u4e09\u79cd\u8bed\u8a00\u8868\u5f81\uff1a\u539f\u59cb\u6e05\u7406\u6587\u672c\u3001\u8bcd\u6027\u589e\u5f3a\u8868\u5f81\uff08\u7ed3\u5408\u8bcd\u6c47\u548c\u8bed\u6cd5\u4fe1\u606f\uff09\u3001\u7eaf\u8bcd\u6027\u53e5\u6cd5\u8868\u5f81\u3002\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u5728\u4e24\u79cd\u534f\u8bae\u4e0b\u8bc4\u4f30\uff1a\u8f6c\u5f55\u7ea7\u8bad\u7ec3\u6d4b\u8bd5\u5206\u5272\u548c\u53d7\u8bd5\u8005\u7ea7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff08\u9632\u6b62\u8bf4\u8bdd\u8005\u91cd\u53e0\uff09\u3002\u901a\u8fc7\u5168\u5c40\u7279\u5f81\u91cd\u8981\u6027\u68c0\u67e5\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4f7f\u7528Mann-Whitney U\u68c0\u9a8c\u548cCliff's delta\u6548\u5e94\u91cf\u8fdb\u884c\u7edf\u8ba1\u9a8c\u8bc1\u3002", "result": "\u6240\u6709\u8868\u5f81\u4e0b\u6a21\u578b\u5747\u83b7\u5f97\u7a33\u5b9a\u6027\u80fd\uff0c\u53e5\u6cd5\u548c\u8bed\u6cd5\u7279\u5f81\u5373\u4f7f\u5728\u7f3a\u4e4f\u8bcd\u6c47\u5185\u5bb9\u65f6\u4ecd\u4fdd\u6301\u5f3a\u533a\u5206\u80fd\u529b\u3002\u53d7\u8bd5\u8005\u7ea7\u8bc4\u4f30\u4ea7\u751f\u66f4\u4fdd\u5b88\u4f46\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u8bcd\u6027\u589e\u5f3a\u548c\u7eaf\u8bcd\u6027\u8868\u5f81\u3002\u7edf\u8ba1\u5206\u6790\u663e\u793a\u529f\u80fd\u8bcd\u4f7f\u7528\u3001\u8bcd\u6c47\u591a\u6837\u6027\u3001\u53e5\u5b50\u7ed3\u6784\u548c\u8bed\u7bc7\u8fde\u8d2f\u6027\u5b58\u5728\u663e\u8457\u7ec4\u95f4\u5dee\u5f02\uff0c\u4e0e\u673a\u5668\u5b66\u4e60\u7279\u5f81\u91cd\u8981\u6027\u53d1\u73b0\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u62bd\u8c61\u8bed\u8a00\u7279\u5f81\u5728\u4e34\u5e8a\u73b0\u5b9e\u8bc4\u4f30\u4e2d\u6355\u6349\u5230\u65e9\u671f\u8ba4\u77e5\u8870\u9000\u7684\u7a33\u5065\u6807\u8bb0\u3002\u901a\u8fc7\u5c06\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u4e0e\u975e\u53c2\u6570\u7edf\u8ba1\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u672c\u7814\u7a76\u652f\u6301\u4f7f\u7528\u57fa\u4e8e\u8bed\u8a00\u7684\u7279\u5f81\u8fdb\u884c\u900f\u660e\u53ef\u9760\u7684\u8bed\u8a00\u8ba4\u77e5\u7b5b\u67e5\u3002"}}
{"id": "2602.11065", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11065", "abs": "https://arxiv.org/abs/2602.11065", "authors": ["Dingkun Zhou", "Shuchang Pan", "Jiachen Lian", "Siddharth Banerjee", "Sarika Pasumarthy", "Dhruv Hebbar", "Siddhant Patel", "Zeyi Austin Li", "Kan Jen Cheng", "Sanay Bordia", "Krish Patel", "Akshaj Gupta", "Tingle Li", "Gopala Anumanchipalli"], "title": "Conversational Behavior Modeling Foundation Model With Multi-Level Perception", "comment": null, "summary": "Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conversational behaviors via a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a high quality corpus that pairs controllable, event-rich dialogue data with human-annotated labels. The GoT framework structures streaming predictions as an evolving graph, enabling a transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.", "AI": {"tldr": "\u63d0\u51faGraph-of-Thoughts\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u611f\u77e5\u5efa\u6a21\u5bf9\u8bdd\u4e2d\u7684\u601d\u7ef4\u94fe\uff0c\u9884\u6d4b\u4ea4\u6d41\u610f\u56fe\u548c\u8a00\u8bed\u884c\u4e3a\uff0c\u5b9e\u73b0\u5168\u53cc\u5de5\u5bf9\u8bdd\u7cfb\u7edf\u7684\u63a8\u7406\u548c\u51b3\u7b56\u3002", "motivation": "\u4eba\u7c7b\u5bf9\u8bdd\u7531\u9690\u542b\u7684\u601d\u7ef4\u94fe\u7ec4\u7ec7\uff0c\u8868\u73b0\u4e3a\u5b9a\u65f6\u7684\u8a00\u8bed\u884c\u4e3a\u3002\u6355\u6349\u8fd9\u79cd\u611f\u77e5\u8def\u5f84\u5bf9\u4e8e\u6784\u5efa\u81ea\u7136\u7684\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165Graph-of-Thoughts\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u591a\u5c42\u6b21\u611f\u77e5\uff0c\u901a\u8fc7\u5206\u5c42\u6807\u6ce8\u65b9\u6848\u5f62\u5f0f\u5316\u610f\u56fe\u5230\u884c\u52a8\u7684\u8def\u5f84\uff0c\u9884\u6d4b\u9ad8\u5c42\u6b21\u4ea4\u6d41\u610f\u56fe\u548c\u4f4e\u5c42\u6b21\u8a00\u8bed\u884c\u4e3a\uff0c\u5b66\u4e60\u5176\u56e0\u679c\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5168\u53cc\u5de5\u5bf9\u8bdd\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u7684\u884c\u4e3a\u68c0\u6d4b\uff0c\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u4e3a\u5168\u53cc\u5de5\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5bf9\u8bdd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "GoT\u6846\u67b6\u901a\u8fc7\u5c06\u6d41\u5f0f\u9884\u6d4b\u7ed3\u6784\u5316\u4e3a\u6f14\u5316\u56fe\uff0c\u4f7fTransformer\u80fd\u591f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8a00\u8bed\u884c\u4e3a\uff0c\u751f\u6210\u51b3\u7b56\u7684\u7b80\u660e\u7406\u7531\uff0c\u5e76\u52a8\u6001\u4f18\u5316\u5176\u63a8\u7406\uff0c\u4e3a\u81ea\u7136\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.10502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10502", "abs": "https://arxiv.org/abs/2602.10502", "authors": ["Xixuan Hao", "Guicheng Li", "Daiqiang Wu", "Xusen Guo", "Yumeng Zhu", "Zhichao Zou", "Peng Zhen", "Yao Yao", "Yuxuan Liang"], "title": "Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web", "comment": "Accepted by The Web Conference 2026", "summary": "The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.", "AI": {"tldr": "MVGR-Net\uff1a\u901a\u8fc7\u591a\u89c6\u56fe\u5730\u7406\u7a7a\u95f4\u8868\u5f81\u5b66\u4e60\u548cLLM\u63d0\u793a\u5fae\u8c03\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7f51\u7ea6\u8f66\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u7f51\u7ea6\u8f66\u670d\u52a1\u6539\u53d8\u4e86\u57ce\u5e02\u51fa\u884c\u6a21\u5f0f\uff0c\u51c6\u786e\u9884\u6d4b\u5bf9\u4f18\u5316\u4e58\u5ba2\u4f53\u9a8c\u548c\u4ea4\u901a\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5730\u7406\u7a7a\u95f4\u5f02\u8d28\u6027\u548c\u5916\u90e8\u4e8b\u4ef6\u9ad8\u5ea6\u654f\u611f\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMVGR-Net\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u9884\u8bad\u7ec3\u9636\u6bb5\u6574\u5408POI\u548c\u65f6\u5e8f\u79fb\u52a8\u6a21\u5f0f\uff0c\u4ece\u8bed\u4e49\u5c5e\u6027\u548c\u65f6\u5e8f\u79fb\u52a8\u6a21\u5f0f\u53cc\u89c6\u56fe\u5b66\u4e60\u5730\u7406\u7a7a\u95f4\u8868\u5f81\uff1b2) \u9884\u6d4b\u9636\u6bb5\u901a\u8fc7\u63d0\u793a\u8d4b\u80fd\u6846\u67b6\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u878d\u5165\u5916\u90e8\u4e8b\u4ef6\u4fe1\u606f\u3002", "result": "\u5728\u6ef4\u6ef4\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "MVGR-Net\u901a\u8fc7\u591a\u89c6\u56fe\u5730\u7406\u7a7a\u95f4\u8868\u5f81\u5b66\u4e60\u548cLLM\u63d0\u793a\u5fae\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f51\u7ea6\u8f66\u9884\u6d4b\u4e2d\u7684\u5730\u7406\u7a7a\u95f4\u5f02\u8d28\u6027\u548c\u5916\u90e8\u4e8b\u4ef6\u654f\u611f\u6027\u6311\u6218\u3002"}}
{"id": "2602.10544", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.10544", "abs": "https://arxiv.org/abs/2602.10544", "authors": ["Wuyang Zhang", "Zhen Luo", "Chuqiao Gu", "Jianming Ma", "Yebo Cao", "Wangming Yuan", "Yinzhi Jin"], "title": "Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy", "comment": "7 pages", "summary": "Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.\n  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u67b6\u6784\u5206\u79bb\u6d4b\u91cf\u63d0\u53d6\u4e0e\u6587\u672c\u751f\u6210\uff0c\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u8ba1\u7b97\u7cbe\u786e\u4e34\u5e8a\u503c\uff0c\u5b9e\u73b0\u9996\u4e2a\u4fdd\u8bc1\u4e34\u5e8a\u6d4b\u91cf\u7cbe\u5ea6\u7684\u81ea\u52a8\u5316EEG\u62a5\u544a\u7cfb\u7edf", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316EEG\u76d1\u6d4b\u7cfb\u7edf\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u4e34\u5e8aEEG\u8bb0\u5f55\u8d85\u8fc7LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9700\u8981\u6781\u7aef\u538b\u7f29(400:1+\u6bd4\u4f8b)\u7834\u574f\u65f6\u95f4\u7cbe\u5ea6\uff1b2) LLM\u7f3a\u4e4f\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u80fd\u529b\uff0c\u4f9d\u8d56\u538b\u7f29\u8868\u793a\u7684\u7edf\u8ba1\u5173\u8054\uff0c\u5bfc\u81f4\u4ea7\u751f\u4e34\u5e8a\u9519\u8bef\u7684\u6d4b\u91cf\u503c", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\u5206\u79bb\u6d4b\u91cf\u63d0\u53d6\u4e0e\u6587\u672c\u751f\u6210\uff1a1) \u538b\u7f29\u524d\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u8ba1\u7b97\u7cbe\u786e\u4e34\u5e8a\u503c\uff1b2) \u4f7f\u7528\u8de8\u6a21\u6001\u6865\u6881\u8fdb\u884cEEG\u5230\u8bed\u8a00\u7684\u7ffb\u8bd1\uff1b3) \u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e0e\u7ea6\u675f\u89e3\u7801\uff1b4) \u591a\u901f\u7387\u91c7\u6837\u4fdd\u6301\u957f\u7a0b\u4e0a\u4e0b\u6587\u540c\u65f6\u4fdd\u7559\u4e8b\u4ef6\u7ea7\u7cbe\u5ea6", "result": "\u5728TUH\u548cCHB-MIT\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u73b060%\u66f4\u5c11\u7684\u8bef\u62a5\u300150%\u66f4\u5feb\u7684\u68c0\u6d4b\u901f\u5ea6\uff0c\u4ee5\u53ca\u4e9a\u4e34\u5e8a\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u662f\u9996\u4e2a\u4fdd\u8bc1\u4e34\u5e8a\u6d4b\u91cf\u51c6\u786e\u6027\u7684\u81ea\u52a8\u5316EEG\u62a5\u544a\u7cfb\u7edf", "conclusion": "\u901a\u8fc7\u5206\u79bb\u6d4b\u91cf\u63d0\u53d6\u4e0e\u6587\u672c\u751f\u6210\u7684\u6df7\u5408\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u5316EEG\u76d1\u6d4b\u4e2d\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8aEEG\u62a5\u544a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u65b9\u6848"}}
{"id": "2602.10884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10884", "abs": "https://arxiv.org/abs/2602.10884", "authors": ["Jinqing Zhang", "Zehua Fu", "Zelin Xu", "Wenying Dai", "Qingjie Liu", "Yunhong Wang"], "title": "ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving", "comment": "ICLR 2026", "summary": "The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.", "AI": {"tldr": "TR-World\u901a\u8fc7\u65f6\u95f4\u6b8b\u5dee\u4e13\u6ce8\u52a8\u6001\u7269\u4f53\u5efa\u6a21\uff0c\u7ed3\u5408FGTR\u6a21\u5757\u5b9e\u73b0\u8f68\u8ff9\u4e0e\u672a\u6765BEV\u7279\u5f81\u4ea4\u4e92\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6027\u80fd", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u5bf9\u9759\u6001\u533a\u57df\u5197\u4f59\u5efa\u6a21\u4e14\u7f3a\u4e4f\u4e0e\u8f68\u8ff9\u7684\u6df1\u5ea6\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u6548\u679c", "method": "\u63d0\u51faTR-World\u901a\u8fc7\u65f6\u95f4\u6b8b\u5dee\u63d0\u53d6\u52a8\u6001\u7269\u4f53\u4fe1\u606f\uff0c\u7ed3\u5408FGTR\u6a21\u5757\u5b9e\u73b0\u8f68\u8ff9\u4e0e\u672a\u6765BEV\u7279\u5f81\u7684\u4ea4\u4e92\u4f18\u5316", "result": "\u5728nuScenes\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u89c4\u5212\u6027\u80fd", "conclusion": "\u4e13\u6ce8\u4e8e\u52a8\u6001\u7269\u4f53\u5efa\u6a21\u7684\u65f6\u95f4\u6b8b\u5dee\u4e16\u754c\u6a21\u578b\u7ed3\u5408\u8f68\u8ff9\u7ec6\u5316\u6a21\u5757\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7cbe\u5ea6"}}
{"id": "2602.10940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10940", "abs": "https://arxiv.org/abs/2602.10940", "authors": ["Guandong Li"], "title": "FastUSP: A Multi-Level Collaborative Acceleration Framework for Distributed Diffusion Model Inference", "comment": null, "summary": "Large-scale diffusion models such as FLUX (12B parameters) and Stable Diffusion 3 (8B parameters) require multi-GPU parallelism for efficient inference. Unified Sequence Parallelism (USP), which combines Ulysses and Ring attention mechanisms, has emerged as the state-of-the-art approach for distributed attention computation. However, existing USP implementations suffer from significant inefficiencies including excessive kernel launch overhead and suboptimal computation-communication scheduling. In this paper, we propose \\textbf{FastUSP}, a multi-level optimization framework that integrates compile-level optimization (graph compilation with CUDA Graphs and computation-communication reordering), communication-level optimization (FP8 quantized collective communication), and operator-level optimization (pipelined Ring attention with double buffering). We evaluate FastUSP on FLUX (12B) and Qwen-Image models across 2, 4, and 8 NVIDIA RTX 5090 GPUs. On FLUX, FastUSP achieves consistent \\textbf{1.12$\\times$--1.16$\\times$} end-to-end speedup over baseline USP, with compile-level optimization contributing the dominant improvement. On Qwen-Image, FastUSP achieves \\textbf{1.09$\\times$} speedup on 2 GPUs; on 4--8 GPUs, we identify a PyTorch Inductor compatibility limitation with Ring attention that prevents compile optimization, while baseline USP scales to 1.30$\\times$--1.46$\\times$ of 2-GPU performance. We further provide a detailed analysis of the performance characteristics of distributed diffusion inference, revealing that kernel launch overhead -- rather than communication latency -- is the primary bottleneck on modern high-bandwidth GPU interconnects.", "AI": {"tldr": "FastUSP\uff1a\u9488\u5bf9\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u591a\u7ea7\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u8bd1\u7ea7\u3001\u901a\u4fe1\u7ea7\u548c\u7b97\u5b50\u7ea7\u4f18\u5316\uff0c\u76f8\u6bd4\u57fa\u7ebfUSP\u5b9e\u73b01.09-1.16\u500d\u52a0\u901f", "motivation": "\u73b0\u6709Unified Sequence Parallelism\uff08USP\uff09\u5b9e\u73b0\u5b58\u5728\u663e\u8457\u4f4e\u6548\u95ee\u9898\uff0c\u5305\u62ec\u8fc7\u591a\u7684\u5185\u6838\u542f\u52a8\u5f00\u9500\u548c\u6b21\u4f18\u7684\u8ba1\u7b97-\u901a\u4fe1\u8c03\u5ea6\uff0c\u8fd9\u5728\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\uff08\u5982FLUX 12B\u548cStable Diffusion 3 8B\uff09\u7684\u591aGPU\u63a8\u7406\u4e2d\u6210\u4e3a\u74f6\u9888", "method": "\u63d0\u51faFastUSP\u591a\u7ea7\u4f18\u5316\u6846\u67b6\uff1a1\uff09\u7f16\u8bd1\u7ea7\u4f18\u5316\uff08\u4f7f\u7528CUDA Graphs\u8fdb\u884c\u56fe\u7f16\u8bd1\u548c\u8ba1\u7b97-\u901a\u4fe1\u91cd\u6392\u5e8f\uff09\uff1b2\uff09\u901a\u4fe1\u7ea7\u4f18\u5316\uff08FP8\u91cf\u5316\u96c6\u4f53\u901a\u4fe1\uff09\uff1b3\uff09\u7b97\u5b50\u7ea7\u4f18\u5316\uff08\u5e26\u53cc\u7f13\u51b2\u7684\u6d41\u6c34\u7ebfRing attention\uff09", "result": "\u5728FLUX\uff0812B\uff09\u4e0a\u5b9e\u73b01.12-1.16\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u7f16\u8bd1\u7ea7\u4f18\u5316\u8d21\u732e\u6700\u5927\u6539\u8fdb\uff1b\u5728Qwen-Image\u4e0a2GPU\u5b9e\u73b01.09\u500d\u52a0\u901f\uff0c4-8GPU\u56e0PyTorch Inductor\u517c\u5bb9\u6027\u95ee\u9898\u65e0\u6cd5\u5e94\u7528\u7f16\u8bd1\u4f18\u5316\uff1b\u5206\u6790\u663e\u793a\u73b0\u4ee3\u9ad8\u5e26\u5bbdGPU\u4e92\u8fde\u4e2d\u5185\u6838\u542f\u52a8\u5f00\u9500\u662f\u4e3b\u8981\u74f6\u9888\u800c\u975e\u901a\u4fe1\u5ef6\u8fdf", "conclusion": "FastUSP\u901a\u8fc7\u591a\u7ea7\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86USP\u5b9e\u73b0\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u5185\u6838\u542f\u52a8\u5f00\u9500\u662f\u73b0\u4ee3GPU\u7cfb\u7edf\u4e2d\u7684\u4e3b\u8981\u74f6\u9888"}}
{"id": "2602.10847", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10847", "abs": "https://arxiv.org/abs/2602.10847", "authors": ["Fanpu Cao", "Lu Dai", "Jindong Han", "Hui Xiong"], "title": "Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval", "comment": "ICLR 2026", "summary": "Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.", "AI": {"tldr": "GTR\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u7ef4\u62a4\u5168\u5c40\u65f6\u95f4\u5d4c\u5165\u548c\u52a8\u6001\u68c0\u7d22\u5bf9\u9f50\u5168\u5c40\u5468\u671f\u6a21\u5f0f\uff0c\u589e\u5f3a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u5168\u5c40\u5468\u671f\u6027\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MTSF\u6a21\u578b\u53d7\u9650\u4e8e\u6709\u9650\u7684\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8de8\u8d8a\u591a\u4e2a\u5468\u671f\u7684\u5168\u5c40\u5468\u671f\u6027\u6a21\u5f0f\uff0c\u800c\u7b80\u5355\u6269\u5c55\u5386\u53f2\u7a97\u53e3\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u4fe1\u606f\u5197\u4f59\u7b49\u95ee\u9898\u3002", "method": "GTR\u7ef4\u62a4\u6574\u4e2a\u5468\u671f\u7684\u81ea\u9002\u5e94\u5168\u5c40\u65f6\u95f4\u5d4c\u5165\uff0c\u52a8\u6001\u68c0\u7d22\u5e76\u5bf9\u9f50\u76f8\u5173\u5168\u5c40\u7247\u6bb5\u4e0e\u8f93\u5165\u5e8f\u5217\uff0c\u901a\u8fc72D\u5377\u79ef\u548c\u6b8b\u5dee\u878d\u5408\u8054\u5408\u5efa\u6a21\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGTR\u5728\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u573a\u666f\u4e2d\u5747\u80fd\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u5f15\u5165\u6700\u5c0f\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "GTR\u662f\u589e\u5f3aMTSF\u4efb\u52a1\u4e2d\u5168\u5c40\u5468\u671f\u6027\u5efa\u6a21\u7684\u9ad8\u6548\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6539\u53d8\u5bbf\u4e3b\u6a21\u578b\u67b6\u6784\u5373\u53ef\u6709\u6548\u6865\u63a5\u77ed\u671f\u89c2\u6d4b\u4e0e\u957f\u671f\u5468\u671f\u6027\u3002"}}
{"id": "2602.11020", "categories": ["cs.LG", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.11020", "abs": "https://arxiv.org/abs/2602.11020", "authors": ["Rui Ma"], "title": "When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging", "comment": null, "summary": "We study same-source multi-view learning and adversarial robustness for next-day direction prediction with financial image representations. On Shanghai Gold Exchange (SGE) spot gold data (2005-2025), we construct two window-aligned views from each rolling window: an OHLCV-rendered price/volume chart and a technical-indicator matrix. To ensure reliable evaluation, we adopt leakage-resistant time-block splits with embargo and use Matthews correlation coefficient (MCC). We find that results depend strongly on the label-noise regime: we apply an ex-post minimum-movement filter that discards samples with realized next-day absolute return below tau to define evaluation subsets with reduced near-zero label ambiguity. This induces a non-monotonic data-noise trade-off that can reveal predictive signal but eventually increases variance as sample size shrinks; the filter is used for offline benchmark construction rather than an inference-time decision rule. In the stabilized subsets, fusion is regime dependent: early fusion by channel stacking can exhibit negative transfer, whereas late fusion with dual encoders and a fusion head provides the dominant clean-performance gains; cross-view consistency regularization has secondary, backbone-dependent effects. We further evaluate test-time L-infinity perturbations using FGSM and PGD under two threat scenarios: view-constrained attacks that perturb one view and joint attacks that perturb both. We observe severe vulnerability at tiny budgets with strong view asymmetry. Late fusion consistently improves robustness under view-constrained attacks, but joint attacks remain challenging and can still cause substantial worst-case degradation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u91d1\u878d\u56fe\u50cf\u8868\u793a\u4e2d\u7684\u540c\u6e90\u591a\u89c6\u56fe\u5b66\u4e60\u548c\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u7528\u4e8e\u6b21\u65e5\u65b9\u5411\u9884\u6d4b\u3002\u7814\u7a76\u53d1\u73b0\u7ed3\u679c\u9ad8\u5ea6\u4f9d\u8d56\u6807\u7b7e\u566a\u58f0\u673a\u5236\uff0c\u901a\u8fc7\u6700\u5c0f\u6ce2\u52a8\u8fc7\u6ee4\u5668\u51cf\u5c11\u6807\u7b7e\u6a21\u7cca\u6027\uff0c\u63ed\u793a\u4e86\u6570\u636e-\u566a\u58f0\u6743\u8861\u3002\u878d\u5408\u7b56\u7565\u6548\u679c\u56e0\u673a\u5236\u800c\u5f02\uff0c\u665a\u671f\u878d\u5408\u8868\u73b0\u6700\u4f73\u3002\u5bf9\u6297\u653b\u51fb\u6d4b\u8bd5\u663e\u793a\u7cfb\u7edf\u5728\u5fae\u5c0f\u6270\u52a8\u4e0b\u8106\u5f31\uff0c\u665a\u671f\u878d\u5408\u80fd\u63d0\u5347\u5355\u89c6\u56fe\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u5982\u4f55\u6709\u6548\u5229\u7528\u591a\u89c6\u56fe\u6570\u636e\uff08\u4ef7\u683c/\u6210\u4ea4\u91cf\u56fe\u8868\u548c\u6280\u672f\u6307\u6807\u77e9\u9635\uff09\u8fdb\u884c\u6b21\u65e5\u65b9\u5411\u9884\u6d4b\uff1b2) \u5982\u4f55\u8bc4\u4f30\u548c\u63d0\u9ad8\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5bf9\u4e8e\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4ece\u4e0a\u6d77\u9ec4\u91d1\u4ea4\u6613\u6240\u73b0\u8d27\u9ec4\u91d1\u6570\u636e\u6784\u5efa\u4e24\u4e2a\u7a97\u53e3\u5bf9\u9f50\u7684\u89c6\u56fe\uff1aOHLCV\u6e32\u67d3\u7684\u4ef7\u683c/\u6210\u4ea4\u91cf\u56fe\u8868\u548c\u6280\u672f\u6307\u6807\u77e9\u9635\uff1b2) \u91c7\u7528\u9632\u6cc4\u6f0f\u7684\u65f6\u95f4\u5757\u5206\u5272\u548cMatthews\u76f8\u5173\u7cfb\u6570\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\uff1b3) \u5e94\u7528\u4e8b\u540e\u6700\u5c0f\u6ce2\u52a8\u8fc7\u6ee4\u5668\u51cf\u5c11\u6807\u7b7e\u6a21\u7cca\u6027\uff1b4) \u6bd4\u8f83\u65e9\u671f\u878d\u5408\uff08\u901a\u9053\u5806\u53e0\uff09\u548c\u665a\u671f\u878d\u5408\uff08\u53cc\u7f16\u7801\u5668+\u878d\u5408\u5934\uff09\u7b56\u7565\uff1b5) \u4f7f\u7528FGSM\u548cPGD\u65b9\u6cd5\u8bc4\u4f30\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u89c6\u56fe\u7ea6\u675f\u653b\u51fb\u548c\u8054\u5408\u653b\u51fb\u4e24\u79cd\u5a01\u80c1\u573a\u666f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1) \u9884\u6d4b\u6027\u80fd\u5f3a\u70c8\u4f9d\u8d56\u6807\u7b7e\u566a\u58f0\u673a\u5236\uff0c\u6700\u5c0f\u6ce2\u52a8\u8fc7\u6ee4\u5668\u63ed\u793a\u4e86\u975e\u5355\u8c03\u7684\u6570\u636e-\u566a\u58f0\u6743\u8861\uff1b2) \u5728\u7a33\u5b9a\u5b50\u96c6\u4e2d\uff0c\u878d\u5408\u6548\u679c\u56e0\u673a\u5236\u800c\u5f02\uff0c\u65e9\u671f\u878d\u5408\u53ef\u80fd\u51fa\u73b0\u8d1f\u8fc1\u79fb\uff0c\u800c\u665a\u671f\u878d\u5408\u5728\u5e72\u51c0\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff1b3) \u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5177\u6709\u6b21\u8981\u7684\u3001\u4f9d\u8d56\u4e8e\u9aa8\u5e72\u7f51\u7edc\u7684\u6548\u679c\uff1b4) \u5bf9\u6297\u653b\u51fb\u6d4b\u8bd5\u663e\u793a\u7cfb\u7edf\u5728\u5fae\u5c0f\u6270\u52a8\u9884\u7b97\u4e0b\u6781\u5176\u8106\u5f31\uff0c\u665a\u671f\u878d\u5408\u80fd\u63d0\u9ad8\u89c6\u56fe\u7ea6\u675f\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u8054\u5408\u653b\u51fb\u4ecd\u5177\u6311\u6218\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff1a\u91d1\u878d\u56fe\u50cf\u8868\u793a\u4e2d\u7684\u591a\u89c6\u56fe\u5b66\u4e60\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u6807\u7b7e\u566a\u58f0\u673a\u5236\u548c\u878d\u5408\u7b56\u7565\u3002\u665a\u671f\u878d\u5408\u5728\u5e72\u51c0\u6027\u80fd\u548c\u5355\u89c6\u56fe\u653b\u51fb\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5bf9\u6297\u8054\u5408\u653b\u51fb\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u6700\u5c0f\u6ce2\u52a8\u8fc7\u6ee4\u5668\u4f5c\u4e3a\u79bb\u7ebf\u57fa\u51c6\u6784\u5efa\u5de5\u5177\u800c\u975e\u63a8\u7406\u65f6\u51b3\u7b56\u89c4\u5219\uff0c\u6709\u52a9\u4e8e\u63ed\u793a\u9884\u6d4b\u4fe1\u53f7\u3002"}}
{"id": "2602.11057", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11057", "abs": "https://arxiv.org/abs/2602.11057", "authors": ["Xinyu Yuan", "Yan Qiao", "Zonghui Wang", "Wenzhi Chen"], "title": "Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models", "comment": "Published as a conference paper at ICLR 2026", "summary": "The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered \"agent\", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.", "AI": {"tldr": "Pram\uff1a\u9996\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLM\uff09\u89e3\u51b3\u591a\u5546\u54c1\u6d41\u95ee\u9898\u7684ML\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u534f\u8c03\uff0c\u5728\u4fdd\u8bc1\u6700\u4f18\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5206\u914d\u7cfb\u7edf\u7684\u5feb\u901f\u6269\u5c55\uff0c\u73b0\u6709\u4f18\u5316\u5f15\u64ce\u96be\u4ee5\u5e73\u8861\u6700\u4f18\u6027\u548c\u53ef\u8ba1\u7b97\u6027\u3002\u670d\u52a1\u63d0\u4f9b\u5546\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u6743\u8861\u56f0\u5883\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u3002", "method": "Pram\u5c06\u539f\u59cb\u95ee\u9898\u5206\u89e3\u4e3a\u5c40\u90e8\u5b50\u95ee\u9898\uff0c\u7531MLM\u9a71\u52a8\u7684\"\u667a\u80fd\u4f53\"\u89e3\u51b3\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u534f\u8c03\u8fd9\u4e9b\u5b50\u95ee\u9898\u4ee5\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u5b66\u4e60\u5728\u4e0a\u4e0b\u6587\u4e2d\u6267\u884c\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u516c\u5171\u62d3\u6251\u4e0a\uff0cPram\u6027\u80fd\u4e0e\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u5668\u76f8\u5f53\uff08\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8d8a\uff09\uff0c\u8fd0\u884c\u65f6\u95f4\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u5728\u94fe\u8def\u6545\u969c\u6216\u6d41\u91cf\u7a81\u53d1\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff08\u6027\u80fd\u4e0b\u964d<10%\uff09\u3002", "conclusion": "Pram\u662f\u9996\u4e2a\u5229\u7528MLM\u63a8\u7406\u80fd\u529b\u89e3\u51b3MCF\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u7406\u8bba\u53ef\u8bc1\u660e\u6536\u655b\u5230\u6700\u4f18\u89e3\uff0c\u5177\u6709\u76ee\u6807\u65e0\u5173\u6027\uff0c\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u4e3b\u6d41\u5206\u914d\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u63d0\u4f9b\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
