{"id": "2512.15740", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.15740", "abs": "https://arxiv.org/abs/2512.15740", "authors": ["Timothy Prescher"], "title": "The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems", "comment": "46 pages, 2 figures. Preregistered at OSF on Nov 14, 2025 (https://doi.org/10.17605/OSF.IO/BMVP3). Includes comparative analysis with OpenAI's 'Confessions' paper (Dec 3, 2025)", "summary": "Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).\n  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.\n  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.", "AI": {"tldr": "\u63d0\u51fa\u6bd4\u4f8b\u8d23\u4efb\u539f\u5219\uff08PPD\uff09\uff0c\u5c06\u9053\u5fb7\u8d23\u4efb\u5efa\u6a21\u4e3a\u968f\u8ba4\u77e5\u72b6\u6001\u53d8\u5316\u7684\u51fd\u6570\uff0c\u63ed\u793a\u4e0d\u786e\u5b9a\u6027\u4e0d\u4f1a\u6d88\u9664\u8d23\u4efb\u800c\u662f\u5c06\u5176\u8f6c\u5316\u4e3a\u4fee\u590d\u8d23\u4efb\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u516c\u5f0f\u548c\u6a21\u62df\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4f26\u7406\u6846\u67b6\u96be\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\uff0c\u901a\u5e38\u5c06\u5176\u7b80\u5355\u89c6\u4e3a\u884c\u52a8\u7ea6\u675f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u52a8\u6001\u5efa\u6a21\u9053\u5fb7\u8d23\u4efb\u5982\u4f55\u968f\u8ba4\u77e5\u72b6\u6001\u53d8\u5316\u7684\u65b0\u6846\u67b6\u3002", "method": "\u5f15\u5165\u6bd4\u4f8b\u8d23\u4efb\u539f\u5219\uff08PPD\uff09\uff0c\u7528\u6570\u5b66\u516c\u5f0f D_total = K[(1-HI) + HI * g(C_signal)] \u5efa\u6a21\u603b\u8d23\u4efb\uff0c\u5176\u4e2dK\u4e3a\u77e5\u8bc6\uff0cHI\u4e3a\u4e0d\u786e\u5b9a\u6027/\u8c26\u900a\u5ea6\uff0cC_signal\u4e3a\u60c5\u5883\u4fe1\u53f7\u5f3a\u5ea6\u3002\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u6846\u67b6\u7a33\u5b9a\u6027\u3002", "result": "\u6a21\u62df\u663e\u793a\u4fdd\u6301\u57fa\u51c6\u8c26\u900a\u7cfb\u6570\uff08\u03bb>0\uff09\u7684\u7cfb\u7edf\u80fd\u4ea7\u751f\u66f4\u7a33\u5b9a\u7684\u8d23\u4efb\u5206\u914d\uff0c\u964d\u4f4e\u8fc7\u5ea6\u81ea\u4fe1\u51b3\u7b56\u98ce\u9669\u3002\u8be5\u6846\u67b6\u5728\u4e34\u5e8a\u4f26\u7406\u3001\u6743\u5229\u6cd5\u3001\u7ecf\u6d4e\u6cbb\u7406\u548c\u4eba\u5de5\u667a\u80fd\u56db\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u8de8\u5b66\u79d1\u6709\u6548\u6027\u3002", "conclusion": "\u6bd4\u4f8b\u8d23\u4efb\u539f\u5219\u901a\u8fc7\u5c06\u8c26\u900a\u5ea6\u4f5c\u4e3a\u7cfb\u7edf\u53c2\u6570\u5f62\u5f0f\u5316\uff0c\u63d0\u4f9b\u4e86\u53ef\u6570\u5b66\u5904\u7406\u7684\u8d23\u4efb\u5efa\u6a21\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7a33\u5b9a\u539f\u5219\uff0c\u9632\u6b62\u8fc7\u5ea6\u884c\u52a8\u548c\u758f\u5ffd\uff0c\u5e73\u8861\u8ba4\u77e5\u4fe1\u5fc3\u4e0e\u60c5\u5883\u98ce\u9669\u3002"}}
{"id": "2512.15783", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15783", "abs": "https://arxiv.org/abs/2512.15783", "authors": ["Kit Tempest-Walters"], "title": "AI Epidemiology: achieving explainable AI through expert oversight patterns", "comment": "41 pages, 1 figure, 7 tables", "summary": "AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.\n  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.\n  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.", "AI": {"tldr": "AI\u6d41\u884c\u75c5\u5b66\u662f\u4e00\u4e2a\u901a\u8fc7\u5e94\u7528\u7fa4\u4f53\u5c42\u9762\u76d1\u6d4b\u65b9\u6cd5\u6765\u6cbb\u7406\u548c\u89e3\u91caAI\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7edf\u8ba1\u5173\u8054\u800c\u975e\u7406\u89e3\u5185\u90e8\u673a\u5236\u6765\u9884\u6d4bAI\u8f93\u51fa\u5931\u8d25\uff0c\u7c7b\u4f3c\u4e8e\u6d41\u884c\u75c5\u5b66\u5728\u7406\u89e3\u5206\u5b50\u673a\u5236\u524d\u901a\u8fc7\u7edf\u8ba1\u8bc1\u636e\u5b9e\u73b0\u516c\u5171\u536b\u751f\u5e72\u9884\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u5982SHAP\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff09\u5728\u9762\u5bf9\u5927\u89c4\u6a21\u90e8\u7f72\u6a21\u578b\u65f6\u5b58\u5728\u6a21\u578b\u590d\u6742\u6027\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed5\u8fc7\u6a21\u578b\u590d\u6742\u6027\u3001\u63d0\u4f9b\u6301\u7eed\u6cbb\u7406\u3001\u4e14\u4e0d\u589e\u52a0\u4e13\u5bb6\u8d1f\u62c5\u7684AI\u76d1\u7ba1\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u6807\u51c6\u5316\u6355\u83b7AI-\u4e13\u5bb6\u4ea4\u4e92\u4e3a\u7ed3\u6784\u5316\u8bc4\u4f30\u5b57\u6bb5\uff08\u98ce\u9669\u7b49\u7ea7\u3001\u5bf9\u9f50\u5206\u6570\u3001\u51c6\u786e\u5ea6\u5206\u6570\uff09\uff0c\u5c06\u8fd9\u4e9b\u4f5c\u4e3a\u66b4\u9732\u53d8\u91cf\u6765\u7edf\u8ba1\u9884\u6d4b\u8f93\u51fa\u5931\u8d25\u3002\u901a\u8fc7\u88ab\u52a8\u8ddf\u8e2a\u4e13\u5bb6\u4e0eAI\u5efa\u8bae\u7684\u8d8b\u540c\u4e0e\u5206\u6b67\uff0c\u63d0\u4f9b\u81ea\u52a8\u5ba1\u8ba1\u8ffd\u8e2a\uff0c\u5e76\u9a8c\u8bc1\u8f93\u51fa\u5931\u8d25\u5173\u8054\u4e0e\u4e13\u5bb6\u8986\u76d6\u548c\u5b9e\u9645\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u96f6\u8d1f\u62c5\u4e13\u5bb6\u63d0\u4f9b\u81ea\u52a8\u5ba1\u8ba1\u8ffd\u8e2a\uff0c\u5728\u673a\u6784\u66f4\u65b0\u6a21\u578b\u548c\u5207\u6362\u4f9b\u5e94\u5546\u65f6\u4fdd\u6301\u6cbb\u7406\u8fde\u7eed\u6027\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u5206\u6570\u548c\u8bed\u4e49\u8bc4\u4f30\uff08\u5982\"\u6b64\u5efa\u8bae\u7c7b\u4f3c\u4e8e500\u4e2a\u56e0\u8fdd\u53cd\u6307\u5357\u800c\u88ab\u4e13\u5bb6\u8986\u76d6\u7684\u6848\u4f8b\"\uff09\u4f7f\u4e13\u5bb6\u548c\u673a\u6784\u80fd\u591f\u5728AI\u8f93\u51fa\u9020\u6210\u5371\u5bb3\u524d\u68c0\u6d4b\u4e0d\u53ef\u9760\u8f93\u51fa\u3002", "conclusion": "AI\u6d41\u884c\u75c5\u5b66\u901a\u8fc7\u4f7f\u9886\u57df\u4e13\u5bb6\u65e0\u9700\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u5373\u53ef\u6cbb\u7406AI\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86AI\u76d1\u7ba1\u7684\u6c11\u4e3b\u5316\uff0c\u4e3a\u590d\u6742AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b9e\u7528\u7684\u6cbb\u7406\u6846\u67b6\u3002"}}
{"id": "2512.15925", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.15925", "abs": "https://arxiv.org/abs/2512.15925", "authors": ["Joel Mire", "Maria Antoniak", "Steven R. Wilson", "Zexin Ma", "Achyutarama R. Ganti", "Andrew Piper", "Maarten Sap"], "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception", "comment": "Presented at IC2S2 2025; Under Review (ARR Oct 2025)", "summary": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.", "AI": {"tldr": "SocialStoryFrames\uff1a\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u548c\u53d9\u4e8b\u7406\u8bba\u7684\u8bfb\u8005\u54cd\u5e94\u5efa\u6a21\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u6545\u4e8b\u4e2d\u7684\u4f5c\u8005\u610f\u56fe\u3001\u63a8\u7406\u3001\u60c5\u611f\u53cd\u5e94\u548c\u4ef7\u503c\u5224\u65ad\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8bfb\u8005\u5bf9\u6545\u4e8b\u7684\u4e30\u5bcc\u89e3\u91ca\u6027\u3001\u60c5\u611f\u6027\u548c\u8bc4\u4ef7\u6027\u53cd\u5e94\uff08\u5982\u63a8\u65ad\u53d9\u4e8b\u610f\u56fe\u3001\u5224\u65ad\u89d2\u8272\u7b49\uff09\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5efa\u6a21\u8bfb\u8005\u54cd\u5e94\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u63d0\u51faSocialStoryFrames\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u53d9\u4e8b\u7406\u8bba\u3001\u8bed\u8a00\u8bed\u7528\u5b66\u548c\u5fc3\u7406\u5b66\u6784\u5efa\u5206\u7c7b\u6cd5\u3002\u5f00\u53d1SSF-Generator\u548cSSF-Classifier\u4e24\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7\u4eba\u7c7b\u8c03\u67e5\uff08382\u540d\u53c2\u4e0e\u8005\uff09\u548c\u4e13\u5bb6\u6807\u6ce8\u9a8c\u8bc1\u3002\u5e94\u7528\u4e8eSSF-Corpus\u6570\u636e\u96c6\uff086,140\u4e2a\u793e\u4ea4\u5a92\u4f53\u6545\u4e8b\uff09\u3002", "result": "\u6a21\u578b\u9a8c\u8bc1\u6709\u6548\uff0c\u80fd\u591f\u5206\u6790\u6545\u4e8b\u8bb2\u8ff0\u610f\u56fe\u7684\u9891\u7387\u548c\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u4e0d\u540c\u793e\u533a\u7684\u53d9\u4e8b\u5b9e\u8df5\u591a\u6837\u6027\u3002\u5c55\u793a\u4e86\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7814\u7a76\u6545\u4e8b\u8bb2\u8ff0\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "SocialStoryFrames\u901a\u8fc7\u5c06\u7ec6\u7c92\u5ea6\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5efa\u6a21\u4e0e\u901a\u7528\u7684\u8bfb\u8005\u54cd\u5e94\u5206\u7c7b\u6cd5\u76f8\u7ed3\u5408\uff0c\u4e3a\u5728\u7ebf\u793e\u533a\u6545\u4e8b\u8bb2\u8ff0\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2512.15940", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15940", "abs": "https://arxiv.org/abs/2512.15940", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space", "comment": null, "summary": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.", "AI": {"tldr": "R4\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u56db\u7ef4\u65f6\u7a7a\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7ed3\u6784\u5316\u7ec8\u8eab\u8bb0\u5fc6\uff0c\u901a\u8fc7\u6784\u5efa4D\u77e5\u8bc6\u6570\u636e\u5e93\u5b9e\u73b0\u65f6\u7a7a\u611f\u77e5\u63a8\u7406\u3002", "motivation": "\u53d7\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u6784\u5efa\u6301\u4e45\u7ed3\u6784\u5316\u5185\u90e8\u8868\u5f81\u6765\u611f\u77e5\u548c\u7406\u89e3\u56db\u7ef4\u65f6\u7a7a\u73af\u5883\u7684\u80fd\u529b\u542f\u53d1\uff0c\u5e0c\u671b\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8d4b\u4e88\u7c7b\u4f3c\u7684\u65f6\u7a7a\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u590d\u6742\u7684\u63a8\u7406\u3002", "method": "R4\u6846\u67b6\u6301\u7eed\u6784\u5efa4D\u77e5\u8bc6\u6570\u636e\u5e93\uff0c\u5c06\u7269\u4f53\u7ea7\u8bed\u4e49\u63cf\u8ff0\u951a\u5b9a\u5728\u5ea6\u91cf\u7a7a\u95f4\u548c\u65f6\u95f4\u4e2d\uff0c\u5f62\u6210\u6301\u4e45\u7684\u4e16\u754c\u6a21\u578b\u3002\u63a8\u7406\u65f6\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5206\u89e3\u4e3a\u8bed\u4e49\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u952e\u6765\u68c0\u7d22\u76f8\u5173\u89c2\u5bdf\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u5177\u8eab\u95ee\u7b54\u548c\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cR4\u5728\u65f6\u7a7a\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u7406\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u65f6\u7a7a\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "R4\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5177\u8eab4D\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7ec8\u8eab\u8bb0\u5fc6\u7cfb\u7edf\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u7a7a\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u3002"}}
{"id": "2512.16302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16302", "abs": "https://arxiv.org/abs/2512.16302", "authors": ["Zixuan Chen", "Chongkai Gao", "Lin Shao", "Jieqi Shi", "Jing Huo", "Yang Gao"], "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation", "comment": "Accepted by AAAI 2026", "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.", "AI": {"tldr": "ManiLong-Shot\uff1a\u4e00\u79cd\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u4e8b\u4ef6\u5206\u89e3\u957f\u65f6\u7a0b\u4efb\u52a1\u3001\u4f7f\u7528VLM\u6216\u89c4\u5219\u542f\u53d1\u5f0f\u9a71\u52a8\u539f\u8bed\u5e8f\u5217\u5316\u3001\u5b9e\u73b0\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u957f\u65f6\u7a0b\u6293\u53d6\u64cd\u4f5c\u7684\u65b0\u6846\u67b6", "motivation": "\u5f53\u524d\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u77ed\u65f6\u7a0b\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u95ee\u9898", "method": "\u56f4\u7ed5\u7269\u7406\u4ea4\u4e92\u4e8b\u4ef6\u7ed3\u6784\u5316\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u5c06\u5176\u91cd\u6784\u4e3a\u4ea4\u4e92\u611f\u77e5\u539f\u8bed\u7684\u5e8f\u5217\u5316\u95ee\u9898\uff1b\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6216\u57fa\u4e8e\u673a\u5668\u4eba\u72b6\u6001\u53d8\u5316\u7684\u89c4\u5219\u542f\u53d1\u5f0f\u9a71\u52a8\u539f\u8bed\u5206\u89e3\uff1b\u4e3a\u6bcf\u4e2a\u539f\u8bed\u9884\u6d4b\u5173\u952e\u4ea4\u4e92\u4e0d\u53d8\u533a\u57df\uff0c\u5efa\u7acb\u6f14\u793a\u4e0e\u5f53\u524d\u89c2\u5bdf\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8ba1\u7b97\u76ee\u6807\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff", "result": "\u5728\u4ec510\u4e2a\u77ed\u65f6\u7a0b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u540e\uff0c\u80fd\u6cdb\u5316\u523020\u4e2a\u672a\u89c1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\uff08\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff09\uff0c\u76f8\u6bd4SOTA\u83b7\u5f9722.8%\u7684\u76f8\u5bf9\u63d0\u5347\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u4e09\u4e2a\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u901a\u8fc7\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u7684\u9c81\u68d2\u6267\u884c\u80fd\u529b", "conclusion": "ManiLong-Shot\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u539f\u8bed\u5206\u89e3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4ece\u77ed\u65f6\u7a0b\u4efb\u52a1\u5230\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6cdb\u5316\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.16183", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16183", "abs": "https://arxiv.org/abs/2512.16183", "authors": ["Mengfan Shen", "Kangqi Song", "Xindi Wang", "Wei Jia", "Tao Wang", "Ziqiang Han"], "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media", "comment": "41 pages,3figures and 9 tables", "summary": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eQwen2.5-7B\u6a21\u578b\u548cLoRA\u5fae\u8c03\u7684\u4fe1\u606f\u63d0\u53d6\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u8b66\u65b9\u901a\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u5728\u6b7b\u4ea1\u68c0\u6d4b\u3001\u6b7b\u4ea1\u4eba\u6570\u548c\u7701\u4efd\u4f4d\u7f6e\u63d0\u53d6\u7b49\u4efb\u52a1\u4e0a\u8fbe\u523095%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4ece\u8b66\u65b9\u4e8b\u4ef6\u901a\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u5bf9\u53ca\u65f6\u51c6\u786e\u7684\u6570\u636e\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u7684\u53d8\u5f02\u6027\u5927\u548c\u975e\u6b63\u5f0f\u6027\uff0c\u8fd9\u4e00\u4efb\u52a1\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "method": "\u91c7\u7528\u9886\u57df\u9002\u5e94\u7684\u63d0\u53d6\u7ba1\u9053\uff0c\u7ed3\u5408\u9488\u5bf9\u6027\u63d0\u793a\u5de5\u7a0b\u548c\u57fa\u4e8eLoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03Qwen2.5-7B\u6a21\u578b\uff0c\u5904\u7406\u566a\u58f0\u548c\u5f02\u6784\u6587\u672c\uff0c\u4ece4,933\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u5fae\u535a\u8b66\u65b9\u901a\u544a\u5b9e\u4f8b\u4e2d\u63d0\u53d615\u4e2a\u5173\u952e\u5b57\u6bb5\u3002", "result": "LoRA\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u57fa\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff0c\u6b7b\u4ea1\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc798.36%\uff0c\u6b7b\u4ea1\u4eba\u6570\u7cbe\u786e\u5339\u914d\u7387\u8fbe95.31%\uff0c\u7701\u4efd\u4f4d\u7f6e\u63d0\u53d6\u7cbe\u786e\u5339\u914d\u7387\u8fbe95.54%\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u4e13\u4e1a\u9886\u57df\u591a\u4efb\u52a1\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u6709\u6548\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u5316\u4e3a\u53ef\u9760\u7ed3\u6784\u5316\u6570\u636e\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2512.16030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16030", "abs": "https://arxiv.org/abs/2512.16030", "authors": ["Lukas Nel"], "title": "Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets", "comment": null, "summary": "A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\\% confidence, it should be correct 80\\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \\textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \\textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \\emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKalshiBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u672a\u6765\u4e8b\u4ef6\u7684\u8ba4\u77e5\u6821\u51c6\u80fd\u529b\uff0c\u53d1\u73b0\u6240\u6709\u524d\u6cbf\u6a21\u578b\u90fd\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5373\u4f7f\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6837\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba4\u77e5\u6821\u51c6\u80fd\u529b\uff08\u5373\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u51c6\u786e\u5ea6\u7684\u5339\u914d\u7a0b\u5ea6\uff09\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u9759\u6001\u77e5\u8bc6\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6765\u672a\u77e5\u4e8b\u4ef6\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u8bc4\u4f30\u3002", "method": "\u5f15\u5165KalshiBench\u57fa\u51c6\uff0c\u5305\u542b300\u4e2a\u6765\u81ea\u53d7\u76d1\u7ba1\u4ea4\u6613\u6240Kalshi\u7684\u9884\u6d4b\u5e02\u573a\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u7684\u771f\u5b9e\u7ed3\u679c\u53d1\u751f\u5728\u6a21\u578b\u8bad\u7ec3\u622a\u6b62\u4e4b\u540e\u3002\u8bc4\u4f30\u4e86Claude Opus 4.5\u3001GPT-5.2\u3001DeepSeek-V3.2\u3001Qwen3-235B\u548cKimi-K2\u7b49\u4e94\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u4f7f\u7528\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\u548cBrier\u6280\u80fd\u5206\u6570\u7b49\u6307\u6807\u8861\u91cf\u6821\u51c6\u6027\u80fd\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u3002\u6700\u4f73\u6821\u51c6\u6a21\u578bClaude Opus 4.5\u7684ECE\u4e3a0.120\uff0c\u4ecd\u6709\u663e\u8457\u6821\u51c6\u8bef\u5dee\u3002\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u5982GPT-5.2-XHigh\u6821\u51c6\u66f4\u5dee\uff08ECE=0.395\uff09\u3002\u4ec5\u4e00\u4e2a\u6a21\u578b\u83b7\u5f97\u6b63Brier\u6280\u80fd\u5206\u6570\uff0c\u8868\u660e\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u4e0d\u5982\u7b80\u5355\u9884\u6d4b\u57fa\u51c6\u7387\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u589e\u5f3a\u4e0d\u4f1a\u81ea\u52a8\u5e26\u6765\u6821\u51c6\u6539\u8fdb\uff0c\u8ba4\u77e5\u6821\u51c6\u662f\u4e00\u79cd\u9700\u8981\u9488\u5bf9\u6027\u5f00\u53d1\u7684\u72ec\u7acb\u80fd\u529b\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u4e13\u95e8\u6821\u51c6\u6280\u672f\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.16092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16092", "abs": "https://arxiv.org/abs/2512.16092", "authors": ["Zibin Liu", "Shunkun Liang", "Banglei Guan", "Dongcai Tan", "Yang Shang", "Qifeng Yu"], "title": "Collimator-assisted high-precision calibration method for event cameras", "comment": "4 pages, 3 figures", "summary": "Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51c6\u76f4\u5668\u95ea\u70c1\u661f\u578b\u56fe\u6848\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u8ddd\u79bb\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u573a\u666f\u4e0b\u7684\u51e0\u4f55\u6807\u5b9a\u95ee\u9898", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7b49\u4f18\u52bf\uff0c\u4f46\u5728\u957f\u8ddd\u79bb\u6d4b\u91cf\u573a\u666f\u4e0b\u7684\u51e0\u4f55\u6807\u5b9a\uff08\u5305\u62ec\u5185\u53c2\u548c\u5916\u53c2\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u957f\u8ddd\u79bb\u548c\u9ad8\u7cbe\u5ea6\u7684\u53cc\u91cd\u6d4b\u91cf\u8981\u6c42\u3002", "method": "\u4f7f\u7528\u5e26\u6709\u95ea\u70c1\u661f\u578b\u56fe\u6848\u7684\u51c6\u76f4\u5668\u8fdb\u884c\u6807\u5b9a\u3002\u9996\u5148\u57fa\u4e8e\u51c6\u76f4\u5668\u7684\u7403\u4f53\u8fd0\u52a8\u6a21\u578b\u7ebf\u6027\u6c42\u89e3\u76f8\u673a\u53c2\u6570\uff0c\u7136\u540e\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u5bf9\u8fd9\u4e9b\u53c2\u6570\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u7ec6\u5316\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u7efc\u5408\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u51c6\u76f4\u5668\u95ea\u70c1\u661f\u578b\u56fe\u6848\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u957f\u8ddd\u79bb\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u573a\u666f\u4e0b\u7684\u51e0\u4f55\u6807\u5b9a\u95ee\u9898\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2512.16133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16133", "abs": "https://arxiv.org/abs/2512.16133", "authors": ["Ren Nakagawa", "Yang Yang", "Risa Shinoda", "Hiroaki Santo", "Kenji Oyama", "Fumio Okura", "Takenao Ohkawa"], "title": "Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space", "comment": "Accepted to WACV 2026", "summary": "This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.", "AI": {"tldr": "\u63d0\u51faCattleAct\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u725b\u7fa4\u884c\u4e3a\u4ea4\u4e92\u5206\u89e3\u4e3a\u4e2a\u4f53\u52a8\u4f5c\u7ec4\u5408\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5728\u9884\u8bad\u7ec3\u52a8\u4f5c\u6f5c\u7a7a\u95f4\u4e0a\u5fae\u8c03\uff0c\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u653e\u7267\u725b\u7fa4\u7684\u884c\u4e3a\u4ea4\u4e92\u3002", "motivation": "\u667a\u80fd\u755c\u7267\u4e1a\u9700\u8981\u81ea\u52a8\u68c0\u6d4b\u725b\u7fa4\u884c\u4e3a\u4ea4\u4e92\uff08\u5982\u53d1\u60c5\u68c0\u6d4b\uff09\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4eba\u7c7b\u4ea4\u4e92\u68c0\u6d4b\uff0c\u725b\u7fa4\u4ea4\u4e92\u68c0\u6d4b\u9762\u4e34\u7f3a\u4e4f\u5168\u9762\u884c\u4e3a\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u653e\u7267\u725b\u7fa4\u7684\u4ea4\u4e92\u662f\u7f55\u89c1\u4e8b\u4ef6\u3002", "method": "\u9996\u5148\u4ece\u5927\u89c4\u6a21\u725b\u7fa4\u52a8\u4f5c\u6570\u636e\u96c6\u5b66\u4e60\u52a8\u4f5c\u6f5c\u7a7a\u95f4\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u9884\u8bad\u7ec3\u6f5c\u7a7a\u95f4\u4e0a\u5fae\u8c03\uff0c\u5c06\u7f55\u89c1\u4ea4\u4e92\u5d4c\u5165\u5230\u7edf\u4e00\u7684\u52a8\u4f5c-\u4ea4\u4e92\u6f5c\u7a7a\u95f4\u4e2d\uff0c\u6700\u540e\u5f00\u53d1\u96c6\u6210\u89c6\u9891\u548cGPS\u8f93\u5165\u7684\u5b9e\u7528\u7cfb\u7edf\u3002", "result": "\u5728\u5546\u4e1a\u89c4\u6a21\u7267\u573a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u884c\u4e3a\u4ea4\u4e92\u68c0\u6d4b\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u7528\u7684\u5b9e\u7528\u7cfb\u7edf\u3002", "conclusion": "CattleAct\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u725b\u7fa4\u4ea4\u4e92\u68c0\u6d4b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u755c\u7267\u4e1a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u4ea4\u4e92\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.15762", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15762", "abs": "https://arxiv.org/abs/2512.15762", "authors": ["Kanxue Li", "Yibing Zhan", "Hua Jin", "Chongchong Qi", "Xu Lin", "Baosheng Yu"], "title": "Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction", "comment": "Accepted by AAAI 2026", "summary": "Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.", "AI": {"tldr": "\u63d0\u51faCSA-TTA\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6837\u672c\u589e\u5f3a\u6d4b\u8bd5\u65f6\u9002\u5e94\u6765\u6539\u5584\u672f\u4e2d\u4f4e\u8840\u538b\u9884\u6d4b\uff0c\u5229\u7528\u5176\u4ed6\u60a3\u8005\u7684\u4f4e\u8840\u538b\u4e8b\u4ef6\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u672f\u4e2d\u4f4e\u8840\u538b(IOH)\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u60a3\u8005\u7279\u5f02\u6027\u5dee\u5f02\u5927\u4e14\u4f4e\u8840\u538b\u4e8b\u4ef6\u7f55\u89c1\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u5176\u4ed6\u60a3\u8005\u6570\u636e\u6765\u589e\u5f3a\u4e2a\u6027\u5316\u9884\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCSA-TTA\u6846\u67b6\uff1a1)\u6784\u5efa\u8de8\u6837\u672c\u5e93\uff0c\u5c06\u5386\u53f2\u6570\u636e\u5206\u4e3a\u4f4e\u8840\u538b\u548c\u975e\u4f4e\u8840\u538b\u6837\u672c\uff1b2)\u91c7\u7528\u7c97\u5230\u7ec6\u68c0\u7d22\u7b56\u7565\uff1a\u5148\u7528K-Shape\u805a\u7c7b\u8bc6\u522b\u4ee3\u8868\u6027\u805a\u7c7b\u4e2d\u5fc3\uff0c\u518d\u57fa\u4e8e\u5f53\u524d\u60a3\u8005\u4fe1\u53f7\u68c0\u7d22top-K\u8bed\u4e49\u76f8\u4f3c\u6837\u672c\uff1b3)\u8bad\u7ec3\u65f6\u7ed3\u5408\u81ea\u76d1\u7763\u63a9\u7801\u91cd\u5efa\u548c\u56de\u987e\u6027\u5e8f\u5217\u9884\u6d4b\u4fe1\u53f7\u3002", "result": "\u5728VitalDB\u6570\u636e\u96c6\u548c\u771f\u5b9e\u533b\u9662\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0eTimesFM\u548cUniTS\u7b49\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u96c6\u6210\u3002\u5728VitalDB\u4e0a\uff0c\u5fae\u8c03\u573a\u666f\u4e0bRecall\u548cF1\u5206\u522b\u63d0\u5347+1.33%\u548c+1.13%\uff0c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u63d0\u5347+7.46%\u548c+5.07%\uff0c\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CSA-TTA\u6846\u67b6\u901a\u8fc7\u8de8\u6837\u672c\u589e\u5f3a\u6709\u6548\u89e3\u51b3\u4e86\u672f\u4e2d\u4f4e\u8840\u538b\u9884\u6d4b\u4e2d\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u4e2a\u6027\u5316\u533b\u7597\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16250", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.16250", "abs": "https://arxiv.org/abs/2512.16250", "authors": ["Sanjoy Chowdhury", "Karren D. Yang", "Xudong Liu", "Fartash Faghri", "Pavan Kumar Anasosalu Vasu", "Oncel Tuzel", "Dinesh Manocha", "Chun-Liang Li", "Raviteja Vemulapalli"], "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding", "comment": null, "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86AMUSE\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86RAFT\u6846\u67b6\u901a\u8fc7\u5956\u52b1\u4f18\u5316\u548c\u9009\u62e9\u6027\u53c2\u6570\u9002\u5e94\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\u548cQwen3-Omni\uff09\u5728\u611f\u77e5\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u9700\u8981\u4ee3\u7406\u63a8\u7406\u7684\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u573a\u666f\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u5305\u62ec\u8ffd\u8e2a\u8bf4\u8bdd\u8005\u3001\u7ef4\u6301\u89d2\u8272\u548c\u8de8\u65f6\u95f4\u63a5\u5730\u4e8b\u4ef6\u3002\u8fd9\u4e9b\u573a\u666f\u5728\u591a\u6a21\u6001\u97f3\u89c6\u9891\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5982\u5bf9\u8bdd\u5f0f\u89c6\u9891\u52a9\u624b\u548c\u4f1a\u8bae\u5206\u6790\u3002", "method": "1. \u5f15\u5165AMUSE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u56f4\u7ed5\u9700\u8981\u4ee3\u7406\u63a8\u7406\u7684\u4efb\u52a1\u8bbe\u8ba1\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u590d\u6742\u7684\u97f3\u89c6\u9891\u4ea4\u4e92\u5206\u89e3\u4e3a\u89c4\u5212\u3001\u63a5\u5730\u548c\u53cd\u601d\u6b65\u9aa4\uff1b2. \u5728\u4e09\u79cd\u6a21\u5f0f\uff08\u96f6\u6837\u672c\u3001\u5f15\u5bfc\u3001\u4ee3\u7406\uff09\u548c\u516d\u4e2a\u4efb\u52a1\u65cf\u4e2d\u8bc4\u4f30MLLMs\uff1b3. \u63d0\u51faRAFT\u6846\u67b6\uff0c\u6574\u5408\u5956\u52b1\u4f18\u5316\u4e0e\u5185\u5728\u591a\u6a21\u6001\u81ea\u6211\u8bc4\u4f30\u4f5c\u4e3a\u5956\u52b1\uff0c\u5e76\u8fdb\u884c\u9009\u62e9\u6027\u53c2\u6570\u9002\u5e94\u4ee5\u5b9e\u73b0\u6570\u636e\u548c\u53c2\u6570\u9ad8\u6548\u66f4\u65b0\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u6240\u6709\u6a21\u5f0f\u4e0b\u90fd\u8868\u73b0\u51fa\u5f31\u7684\u591a\u8bf4\u8bdd\u4eba\u63a8\u7406\u80fd\u529b\u548c\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\u3002\u4f7f\u7528RAFT\u6846\u67b6\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe39.52%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "AMUSE\u548cRAFT\u5171\u540c\u4e3a\u68c0\u9a8c\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u5e76\u63d0\u5347\u5176\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2512.16164", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16164", "abs": "https://arxiv.org/abs/2512.16164", "authors": ["Chao Li", "Dasha Hu", "Chengyang Li", "Yuming Jiang", "Yuncheng Shen"], "title": "C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation", "comment": null, "summary": "Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.", "AI": {"tldr": "C-DGPA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c7b\u4e2d\u5fc3\u7684\u53cc\u5bf9\u9f50\u751f\u6210\u63d0\u793a\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u540c\u65f6\u4f18\u5316\u8fb9\u7f18\u5206\u5e03\u5bf9\u9f50\u548c\u6761\u4ef6\u5206\u5e03\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86VLM\u5728UDA\u4efb\u52a1\u4e2d\u7684\u9886\u57df\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u793a\u8c03\u4f18\u7b56\u7565\u4e3b\u8981\u5173\u6ce8\u8fb9\u7f18\u5206\u5e03\u5bf9\u9f50\uff0c\u4f46\u5ffd\u7565\u4e86\u6761\u4ef6\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u7c7b\u539f\u578b\u9519\u4f4d\u548c\u8bed\u4e49\u5224\u522b\u6027\u4e0b\u964d\u7b49\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u4e24\u79cd\u5206\u5e03\u5dee\u5f02\u7684\u65b9\u6cd5\u6765\u63d0\u5347VLM\u5728UDA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "C-DGPA\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1a1\uff09\u8fb9\u7f18\u5206\u5e03\u5bf9\u9f50\u5206\u652f\u4f7f\u7528\u52a8\u6001\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\u6765\u6865\u63a5\u8fb9\u7f18\u5206\u5e03\u5dee\u5f02\uff1b2\uff09\u6761\u4ef6\u5206\u5e03\u5bf9\u9f50\u5206\u652f\u5f15\u5165\u7c7b\u6620\u5c04\u673a\u5236\uff08CMM\uff09\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bed\u4e49\u63d0\u793a\u7406\u89e3\u548c\u9632\u6b62\u6e90\u57df\u8fc7\u4f9d\u8d56\u6765\u5bf9\u9f50\u6761\u4ef6\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728OfficeHome\u3001Office31\u548cVisDA-2017\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86C-DGPA\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u6240\u6709\u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "C-DGPA\u901a\u8fc7\u534f\u540c\u4f18\u5316\u7684\u53cc\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u9886\u57df\u77e5\u8bc6\u6709\u6548\u6574\u5408\u5230\u63d0\u793a\u5b66\u4e60\u4e2d\uff0c\u786e\u4fdd\u4e86\u9886\u57df\u4e0d\u53d8\u4e14\u8bed\u4e49\u5224\u522b\u6027\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u5728UDA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.16178", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16178", "abs": "https://arxiv.org/abs/2512.16178", "authors": ["M. Oltan Sevinc", "Liao Wu", "Francisco Cruz"], "title": "Towards Closing the Domain Gap with Event Cameras", "comment": "Accepted to Australasian Conference on Robotics and Automation (ACRA), 2025", "summary": "Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76f8\u6bd4\u4f20\u7edf\u76f8\u673a\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u663c\u591c\u5149\u7167\u53d8\u5316\u7684\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8c03\u6574\u5373\u53ef\u4fdd\u6301\u6027\u80fd\u4e00\u81f4\u6027", "motivation": "\u4f20\u7edf\u76f8\u673a\u5728\u7aef\u5230\u7aef\u9a7e\u9a76\u4e2d\u9762\u4e34\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u8bad\u7ec3\u6570\u636e\u6761\u4ef6\u4e0e\u90e8\u7f72\u73af\u5883\u4e0d\u5339\u914d\u65f6\u6027\u80fd\u4f1a\u5927\u5e45\u4e0b\u964d\u3002\u672c\u6587\u9488\u5bf9\u663c\u591c\u5149\u7167\u5dee\u5f02\u8fd9\u4e00\u5177\u4f53\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u63a2\u7d22\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u66ff\u4ee3\u4f20\u7edf\u76f8\u673a\uff0c\u8bc4\u4f30\u5176\u5728\u663c\u591c\u5149\u7167\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u4e0e\u7070\u5ea6\u5e27\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4e8b\u4ef6\u76f8\u673a\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u4fdd\u6301\u66f4\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u5176\u57df\u504f\u79fb\u60e9\u7f5a\u901a\u5e38\u4e0e\u7070\u5ea6\u5e27\u76f8\u5f53\u6216\u66f4\u5c0f\uff0c\u5728\u8de8\u57df\u573a\u666f\u4e2d\u63d0\u4f9b\u66f4\u4f18\u8d8a\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u662f\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u663c\u591c\u5149\u7167\u57df\u5dee\u8ddd\u95ee\u9898\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u989d\u5916\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u8de8\u57df\u6027\u80fd\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16392", "abs": "https://arxiv.org/abs/2512.16392", "authors": ["Mohammad-Javad Rezaei", "Mozafar Bag-Mohammadi"], "title": "PCIA: A Path Construction Imitation Algorithm for Global Optimization", "comment": null, "summary": "In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u542f\u53d1\u5f0f\u4f18\u5316\u7b97\u6cd5PCIA\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u4eba\u7c7b\u6784\u5efa\u548c\u4f7f\u7528\u8def\u5f84\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u968f\u673a\u79cd\u7fa4\u5bfb\u627e\u6700\u4f73\u8def\u5f84\uff0c\u5728\u6570\u5b66\u548c\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8def\u5f84\u6784\u5efa\u884c\u4e3a\u7684\u542f\u53d1\uff1a\u4eba\u4eec\u901a\u5e38\u504f\u597d\u70ed\u95e8\u8def\u7ebf\uff0c\u5f53\u8def\u5f84\u5173\u95ed\u65f6\u4f1a\u667a\u80fd\u5730\u6df7\u5408\u73b0\u6709\u8def\u5f84\u6784\u5efa\u65b0\u8def\u7ebf\uff0c\u5e76\u968f\u673a\u9009\u62e9\u4e0d\u540c\u8def\u5f84\u63a2\u7d22\u672a\u77e5\u76ee\u7684\u5730\u3002\u8fd9\u4e9b\u884c\u4e3a\u6a21\u5f0f\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "method": "PCIA\u6a21\u4eff\u4eba\u7c7b\u8def\u5f84\u6784\u5efa\u8fc7\u7a0b\uff1a1) \u751f\u6210\u968f\u673a\u79cd\u7fa4\uff08\u7c7b\u4f3c\u7fa4\u4f53\u7b97\u6cd5\uff09\uff0c\u6bcf\u4e2a\u7c92\u5b50\u4ee3\u8868\u4e00\u6761\u901a\u5f80\u76ee\u6807\u7684\u8def\u5f84\uff1b2) \u7ed3\u5408\u4eba\u7c7b\u8def\u5f84\u9009\u62e9\u7684\u4e09\u4e2a\u5173\u952e\u7279\u5f81\uff1a\u504f\u597d\u70ed\u95e8\u8def\u7ebf\u3001\u667a\u80fd\u6df7\u5408\u73b0\u6709\u8def\u5f84\u3001\u968f\u673a\u63a2\u7d22\u672a\u77e5\u8def\u5f84\u3002", "result": "\u572853\u4e2a\u6570\u5b66\u4f18\u5316\u95ee\u9898\u548c13\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e0a\u6d4b\u8bd5\uff0cPCIA\u4e0e\u6d41\u884c\u53ca\u6700\u65b0\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u76f8\u6bd4\u8868\u73b0\u51fa\u9ad8\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "PCIA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5143\u542f\u53d1\u5f0f\u4f18\u5316\u7b97\u6cd5\uff0c\u57fa\u4e8e\u4eba\u7c7b\u8def\u5f84\u6784\u5efa\u884c\u4e3a\u7684\u7075\u611f\uff0c\u5728\u591a\u79cd\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.16445", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.16445", "abs": "https://arxiv.org/abs/2512.16445", "authors": ["Roman Akramov", "Artem Khamatullin", "Svetlana Glazyrina", "Maksim Kryzhanovskiy", "Roman Ischenko"], "title": "Topic Modelling Black Box Optimization", "comment": null, "summary": "Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06LDA\u4e3b\u9898\u6a21\u578b\u4e2d\u7684\u4e3b\u9898\u6570\u91cfT\u9009\u62e9\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u79bb\u6563\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u4f18\u5316\u65b9\u6cd5\u5728\u56fa\u5b9a\u8bc4\u4f30\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "LDA\u4e3b\u9898\u6a21\u578b\u4e2d\u4e3b\u9898\u6570\u91cfT\u7684\u9009\u62e9\u5bf9\u7edf\u8ba1\u62df\u5408\u548c\u53ef\u89e3\u91ca\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u7814\u7a76\u65e8\u5728\u627e\u5230\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u9009\u62e9\u6700\u4f18\u4e3b\u9898\u6570\u91cf\u3002", "method": "\u5c06\u4e3b\u9898\u6570\u91cf\u9009\u62e9\u89c6\u4e3a\u79bb\u6563\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u6bcf\u4e2a\u51fd\u6570\u8bc4\u4f30\u5bf9\u5e94\u8bad\u7ec3LDA\u6a21\u578b\u5e76\u6d4b\u91cf\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u3002\u5728\u56fa\u5b9a\u8bc4\u4f30\u9884\u7b97\u4e0b\u6bd4\u8f83\u56db\u79cd\u4f18\u5316\u5668\uff1a\u9057\u4f20\u7b97\u6cd5(GA)\u3001\u8fdb\u5316\u7b56\u7565(ES)\u3001\u504f\u597d\u644a\u9500\u9ed1\u76d2\u4f18\u5316(PABBO)\u548c\u9510\u5ea6\u611f\u77e5\u9ed1\u76d2\u4f18\u5316(SABBO)\u3002", "result": "\u867d\u7136\u56db\u79cd\u65b9\u6cd5\u6700\u7ec8\u90fd\u80fd\u8fbe\u5230\u76f8\u4f3c\u7684\u56f0\u60d1\u5ea6\u6c34\u5e73\uff0c\u4f46\u644a\u9500\u4f18\u5316\u5668(PABBO\u548cSABBO)\u5728\u6837\u672c\u548c\u65f6\u95f4\u6548\u7387\u4e0a\u663e\u8457\u66f4\u9ad8\u3002SABBO\u901a\u5e38\u53ea\u9700\u4e00\u6b21\u8bc4\u4f30\u5c31\u80fd\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u4e3b\u9898\u6570\u91cf\uff0cPABBO\u5728\u51e0\u6b21\u8bc4\u4f30\u5185\u627e\u5230\u7ade\u4e89\u6027\u914d\u7f6e\uff0c\u800cGA\u548cES\u9700\u8981\u51e0\u4e4e\u5168\u90e8\u9884\u7b97\u624d\u80fd\u8fbe\u5230\u76f8\u540c\u533a\u57df\u3002", "conclusion": "\u644a\u9500\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u5728LDA\u4e3b\u9898\u6570\u91cf\u9009\u62e9\u95ee\u9898\u4e0a\u6bd4\u4f20\u7edf\u8fdb\u5316\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u7279\u522b\u662fSABBO\u80fd\u591f\u4ee5\u6781\u5c11\u7684\u8bc4\u4f30\u6b21\u6570\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16103", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16103", "abs": "https://arxiv.org/abs/2512.16103", "authors": ["Sandeep Neela"], "title": "AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation", "comment": "Preprint", "summary": "Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.\n  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.\n  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.", "AI": {"tldr": "AIMM\u6846\u67b6\u901a\u8fc7\u878d\u5408Reddit\u6d3b\u52a8\u3001\u673a\u5668\u4eba\u6307\u6807\u548c\u5e02\u573a\u6570\u636e\uff0c\u4e3a\u6bcf\u53ea\u80a1\u7968\u751f\u6210\u6bcf\u65e5\u64cd\u7eb5\u98ce\u9669\u8bc4\u5206\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u9a71\u52a8\u7684\u5e02\u573a\u64cd\u7eb5", "motivation": "\u5e02\u573a\u64cd\u7eb5\u65e5\u76ca\u6765\u81ea\u534f\u8c03\u7684\u793e\u4ea4\u5a92\u4f53\u6d3b\u52a8\u800c\u975e\u5b64\u7acb\u4ea4\u6613\uff0c\u96f6\u552e\u6295\u8d44\u8005\u3001\u76d1\u7ba1\u673a\u6784\u548c\u7ecf\u7eaa\u5546\u9700\u8981\u5de5\u5177\u6765\u8fde\u63a5\u5728\u7ebf\u53d9\u4e8b\u3001\u534f\u8c03\u6a21\u5f0f\u4e0e\u5e02\u573a\u884c\u4e3a", "method": "\u5f00\u53d1AIMM\u6846\u67b6\uff0c\u878d\u5408Reddit\u6d3b\u52a8\u3001\u673a\u5668\u4eba\u548c\u534f\u8c03\u6307\u6807\u4ee5\u53caOHLCV\u5e02\u573a\u7279\u5f81\uff0c\u4f7f\u7528parquet-native\u6d41\u6c34\u7ebf\u548cStreamlit\u4eea\u8868\u677f\uff0c\u6784\u5efaAIMM-GT\u6570\u636e\u96c6\uff0833\u4e2a\u6807\u8bb0\u7684\u80a1\u7968-\u4ea4\u6613\u65e5\uff09\uff0c\u91c7\u7528\u524d\u5411\u8bc4\u4f30\u548c\u9884\u6d4b\u65e5\u5fd7\u8bb0\u5f55", "result": "AIMM\u5728GME\u4e8b\u4ef6\u4e2d\u63d0\u524d22\u5929\u53d1\u51fa\u9884\u8b66\uff0c\u867d\u7136\u5f53\u524d\u6807\u8bb0\u96c6\u8f83\u5c0f\uff0833\u4e2a\u80a1\u7968-\u4ea4\u6613\u65e5\uff0c3\u4e2a\u6b63\u9762\u4e8b\u4ef6\uff09\uff0c\u4f46\u663e\u793a\u51fa\u521d\u6b65\u7684\u5224\u522b\u80fd\u529b\u548c\u65e9\u671f\u9884\u8b66\u80fd\u529b", "conclusion": "AIMM\u6846\u67b6\u4e3a\u793e\u4ea4\u5a92\u4f53\u9a71\u52a8\u7684\u5e02\u573a\u76d1\u63a7\u7814\u7a76\u63d0\u4f9b\u4e86\u4ee3\u7801\u3001\u6570\u636e\u96c6\u67b6\u6784\u548c\u4eea\u8868\u677f\u8bbe\u8ba1\uff0c\u652f\u6301\u68c0\u6d4b\u534f\u8c03\u7684\u793e\u4ea4\u5a92\u4f53\u5e02\u573a\u64cd\u7eb5\u6d3b\u52a8"}}
{"id": "2512.16856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16856", "abs": "https://arxiv.org/abs/2512.16856", "authors": ["Nenad Toma\u0161ev", "Matija Franklin", "Julian Jacobs", "S\u00e9bastien Krier", "Simon Osindero"], "title": "Distributional AGI Safety", "comment": null, "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u62fc\u51d1\u5f0fAGI\"\u5047\u8bf4\uff0c\u8ba4\u4e3a\u901a\u7528\u667a\u80fd\u53ef\u80fd\u9996\u5148\u901a\u8fc7\u591a\u4e2a\u5b50AGI\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b9e\u73b0\uff0c\u800c\u975e\u5355\u4e00\u7cfb\u7edf\u3002\u4e3a\u6b64\u9700\u8981\u8d85\u8d8a\u4e2a\u4f53\u5bf9\u9f50\uff0c\u5efa\u7acb\u5206\u5e03\u5f0fAGI\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u4ee3\u7406\u6c99\u76d2\u7ecf\u6d4e\u6765\u7ba1\u7406\u667a\u80fd\u4f53\u95f4\u4ea4\u4e92\u98ce\u9669\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e00AGI\u7cfb\u7edf\u7684\u5bf9\u9f50\uff0c\u4f46\u5ffd\u89c6\u4e86\u53e6\u4e00\u79cd\u53ef\u80fd\u6027\uff1a\u901a\u7528\u667a\u80fd\u53ef\u80fd\u9996\u5148\u901a\u8fc7\u591a\u4e2a\u5b50AGI\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u548c\u4e92\u8865\u80fd\u529b\u5b9e\u73b0\u3002\u968f\u7740\u5177\u5907\u5de5\u5177\u4f7f\u7528\u548c\u534f\u8c03\u80fd\u529b\u7684AI\u667a\u80fd\u4f53\u5feb\u901f\u90e8\u7f72\uff0c\u8fd9\u79cd\"\u62fc\u51d1\u5f0fAGI\"\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u9700\u8981\u7d27\u6025\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0fAGI\u5b89\u5168\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u8bbe\u8ba1\u548c\u5b9e\u65bd\u865a\u62df\u4ee3\u7406\u6c99\u76d2\u7ecf\u6d4e\uff08\u4e0d\u53ef\u6e17\u900f\u6216\u534a\u6e17\u900f\uff09\uff0c\u901a\u8fc7\u7a33\u5065\u7684\u5e02\u573a\u673a\u5236\u7ba1\u7406\u667a\u80fd\u4f53\u95f4\u4ea4\u6613\uff0c\u7ed3\u5408\u9002\u5f53\u7684\u53ef\u5ba1\u8ba1\u6027\u3001\u58f0\u8a89\u7ba1\u7406\u548c\u76d1\u7763\u6765\u7f13\u89e3\u96c6\u4f53\u98ce\u9669\u3002", "result": "\u5efa\u7acb\u4e86\u4ece\u4e2a\u4f53\u5bf9\u9f50\u5230\u7fa4\u4f53\u534f\u8c03\u5b89\u5168\u7684\u8303\u5f0f\u8f6c\u53d8\u6846\u67b6\uff0c\u4e3a\u5e94\u5bf9\u62fc\u51d1\u5f0fAGI\u98ce\u9669\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u65b9\u6848\uff0c\u5f3a\u8c03\u901a\u8fc7\u7ecf\u6d4e\u673a\u5236\u548c\u76d1\u7ba1\u7ed3\u6784\u6765\u7ba1\u7406\u667a\u80fd\u4f53\u96c6\u4f53\u884c\u4e3a\u3002", "conclusion": "\u62fc\u51d1\u5f0fAGI\u5047\u8bf4\u9700\u8981\u8ba4\u771f\u5bf9\u5f85\uff0cAI\u5b89\u5168\u7814\u7a76\u5fc5\u987b\u8d85\u8d8a\u4e2a\u4f53\u667a\u80fd\u4f53\u5bf9\u9f50\uff0c\u5f00\u53d1\u76f8\u5e94\u7684\u5206\u5e03\u5f0f\u5b89\u5168\u6846\u67b6\u3002\u865a\u62df\u4ee3\u7406\u6c99\u76d2\u7ecf\u6d4e\u7ed3\u5408\u5e02\u573a\u673a\u5236\u548c\u76d1\u7763\u63aa\u65bd\uff0c\u662f\u5e94\u5bf9\u667a\u80fd\u4f53\u7fa4\u4f53\u534f\u8c03\u98ce\u9669\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2512.16244", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16244", "abs": "https://arxiv.org/abs/2512.16244", "authors": ["Xueqi Ma", "Xingjun Ma", "Sarah Monazam Erfani", "Danilo Mandic", "James Bailey"], "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models", "comment": "Accepted to AAAI 2026", "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.", "AI": {"tldr": "\u63d0\u51faCFC\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u56fe\u6570\u636e\u96c6\u7684\u5f00\u653e\u96c6\u5206\u7c7b\uff0c\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684OOD\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06OOD\u6837\u672c\u89c6\u4e3a\u5355\u4e00\u7c7b\u522b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u9700\u8981\u66f4\u6df1\u5165\u4e86\u89e3OOD\u6837\u672c\u7684\u53ef\u80fd\u6807\u7b7e\u3002\u9700\u8981\u5728\u4e0d\u4f9d\u8d56\u771f\u5b9e\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u6269\u5c55OOD\u68c0\u6d4b\u5230OOD\u5206\u7c7b\u3002", "method": "CFC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1) \u7c97\u5206\u7c7b\u5668\u4f7f\u7528LLM\u63d0\u793a\u8fdb\u884cOOD\u68c0\u6d4b\u548c\u5f02\u5e38\u6807\u7b7e\u751f\u6210\uff1b2) GNN\u7ec6\u5206\u7c7b\u5668\u5229\u7528\u7c97\u5206\u7c7b\u5668\u8bc6\u522b\u7684OOD\u6837\u672c\u8bad\u7ec3\uff0c\u589e\u5f3aOOD\u68c0\u6d4b\u548cID\u5206\u7c7b\uff1b3) \u901a\u8fc7LLM\u63d0\u793a\u548c\u540e\u5904\u7406\u7684OOD\u6807\u7b7e\u5b9e\u73b0\u7cbe\u7ec6\u5316OOD\u5206\u7c7b\u3002", "result": "\u5728\u56fe\u548c\u6587\u672c\u9886\u57df\uff0cCFC\u5c06OOD\u68c0\u6d4b\u6027\u80fd\u63d0\u534710%\u4f18\u4e8eSOTA\u65b9\u6cd5\uff1b\u5728\u56fe\u6570\u636e\u96c6\u4e0aOOD\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523070%\u3002", "conclusion": "CFC\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u7684OOD\u5206\u7c7b\uff0c\u5229\u7528\u8bed\u4e49OOD\u5b9e\u4f8b\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u56fe\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00456", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "comment": "https://github.com/kiranshahi/pneumonia-analysis", "summary": "Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGrad-CAM\u7684\u5f31\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u80ba\u708e\u5206\u7c7b\u548c\u5b9a\u4f4d\uff0c\u4ec5\u9700\u56fe\u50cf\u7ea7\u6807\u7b7e\u5373\u53ef\u751f\u6210\u80ba\u708e\u533a\u57df\u70ed\u529b\u56fe\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002", "motivation": "\u80f8\u90e8X\u5149\u5e38\u7528\u4e8e\u80ba\u708e\u8bca\u65ad\uff0c\u4f46\u7cbe\u786e\u5b9a\u4f4d\u80ba\u708e\u533a\u57df\u9700\u8981\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002\u4e3a\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u9700\u8981\u5f00\u53d1\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Grad-CAM\u751f\u6210\u80ba\u708e\u533a\u57df\u70ed\u529b\u56fe\uff0c\u8bc4\u4f307\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5305\u62ecVision Transformer\uff09\uff0c\u91c7\u7528focal loss\u548c\u60a3\u8005\u7ea7\u6570\u636e\u5206\u5272\u9632\u6b62\u6570\u636e\u6cc4\u9732\u3002", "result": "\u6240\u6709\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8fbe96-98%\uff0cResNet-18\u548cEfficientNet-B0\u8868\u73b0\u6700\u4f73\uff0cMobileNet-V3\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002Grad-CAM\u70ed\u529b\u56fe\u663e\u793a\u6a21\u578b\u805a\u7126\u4e34\u5e8a\u76f8\u5173\u80ba\u533a\u57df\u3002", "conclusion": "\u5f31\u76d1\u7763\u53ef\u89e3\u91caAI\u6a21\u578b\u5728\u80ba\u708e\u7b5b\u67e5\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u589e\u5f3a\u4e34\u5e8a\u900f\u660e\u5ea6\u4e0e\u4fe1\u4efb\uff0c\u65e0\u9700\u6602\u8d35\u50cf\u7d20\u7ea7\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u80ba\u708e\u5b9a\u4f4d\u3002"}}
{"id": "2512.16523", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16523", "abs": "https://arxiv.org/abs/2512.16523", "authors": ["Zhiwei Li", "Yitian Pang", "Weining Wang", "Zhenan Sun", "Qi Li"], "title": "TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.", "AI": {"tldr": "TTP\uff1a\u9488\u5bf9CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u586b\u5145\u524d\u540e\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u504f\u79fb\u68c0\u6d4b\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u4f7f\u7528\u53ef\u8bad\u7ec3\u586b\u5145\u4fee\u590d\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u5347\u4e14\u4e0d\u635f\u5bb3\u5e72\u51c0\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u96f6\u6837\u672c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5bf9\u6297\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002\u8bad\u7ec3\u65f6\u9632\u5fa1\u9700\u8981\u6807\u6ce8\u6570\u636e\u548c\u6602\u8d35\u91cd\u8bad\u7ec3\uff0c\u800c\u73b0\u6709\u6d4b\u8bd5\u65f6\u7b56\u7565\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u5e72\u51c0\u548c\u5bf9\u6297\u8f93\u5165\uff0c\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u5e72\u51c0\u51c6\u786e\u7387\u3002", "method": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u586b\u5145\uff08TTP\uff09\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u7a7a\u95f4\u586b\u5145\u524d\u540eCLIP\u7279\u5f81\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u504f\u79fb\u68c0\u6d4b\u5bf9\u6297\u8f93\u5165\uff0c\u4f7f\u7528\u901a\u7528\u9608\u503c\uff1b2\uff09\u5bf9\u68c0\u6d4b\u5230\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4f7f\u7528\u53ef\u8bad\u7ec3\u586b\u5145\u4fee\u590d\u88ab\u7834\u574f\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u7ed3\u5408\u76f8\u4f3c\u5ea6\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff1b3\uff09\u5bf9\u5e72\u51c0\u8f93\u5165\u9ed8\u8ba4\u4fdd\u6301\u4e0d\u53d8\u6216\u53ef\u9009\u96c6\u6210\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u6280\u672f\u3002", "result": "\u5728\u591a\u79cdCLIP\u4e3b\u5e72\u7f51\u7edc\u548c\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cTTP\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u800c\u4e0d\u635f\u5bb3\u5e72\u51c0\u51c6\u786e\u7387\u3002", "conclusion": "TTP\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u9632\u5fa1\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u8106\u5f31\u6027\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u6216\u6807\u6ce8\u6570\u636e\uff0c\u5728\u4fdd\u6301\u5e72\u51c0\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.16476", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.16476", "abs": "https://arxiv.org/abs/2512.16476", "authors": ["Pengfei Sun", "Wenyu Jiang", "Piew Yoong Chee", "Paul Devos", "Dick Botteldooren"], "title": "Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning", "comment": null, "summary": "Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6279\u91cf\u5f52\u4e00\u5316(BN)\u7684\u5b8c\u5168\u6574\u6570\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u9010\u5c42\u84b8\u998f\u5b9e\u73b0\u6574\u6570\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u795e\u7ecf\u7f51\u7edc(QNNs)\u5927\u591a\u4ecd\u4f9d\u8d56\u6279\u91cf\u5f52\u4e00\u5316(BN)\u5c42\uff0c\u8fd9\u963b\u788d\u4e86\u771f\u6b63\u7684\u6574\u6570\u63a8\u7406\u90e8\u7f72\u3002\u867d\u7136\u5df2\u6709\u5c1d\u8bd5\u901a\u8fc7\u53c2\u6570\u6298\u53e0\u6216\u5b9a\u5236\u521d\u59cb\u5316\u6765\u79fb\u9664BN\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u6062\u590dBN\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e14\u901a\u5e38\u9700\u8981\u7279\u6b8a\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u9010\u5c42\u84b8\u998f\u65b9\u6848\uff1a\u4ece\u9884\u8bad\u7ec3\u7684BN-enabled\u6559\u5e08\u6a21\u578b\u5f00\u59cb\uff0c\u4f7f\u7528\u9010\u5c42\u76ee\u6807\u548c\u6e10\u8fdb\u8865\u507f\u6765\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u6d88\u9664BN\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4ec5\u4f7f\u7528\u6574\u6570\u7b97\u672f\u7684\u63a8\u7406\uff0c\u5e76\u80fd\u76f4\u63a5\u96c6\u6210\u5230\u6807\u51c6\u91cf\u5316\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u4f7f\u7528AlexNet\u67b6\u6784\uff0cBN-free\u6a21\u578b\u5728\u6fc0\u8fdb\u91cf\u5316\u4e0b\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684Top-1\u51c6\u786e\u7387\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u6574\u6570\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u548c\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684BN-free\u5b8c\u5168\u6574\u6570\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u9010\u5c42\u84b8\u998f\u89e3\u51b3\u4e86\u4f20\u7edfQNN\u4f9d\u8d56BN\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u6574\u6570\u63a8\u7406\u90e8\u7f72\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16581", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16581", "abs": "https://arxiv.org/abs/2512.16581", "authors": ["Sullivan Castro", "Artem Betlei", "Thomas Di Martino", "Nadir El Manouzi"], "title": "Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling", "comment": null, "summary": "Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted \"counter\" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.", "AI": {"tldr": "Abacus\uff1a\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u4e8b\u4ef6\u7ecf\u9a8c\u9891\u7387\u5206\u5e03\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e8f\u5217\u5b66\u4e60\u76ee\u6807\uff0c\u63d0\u5347\u5c55\u793a\u5e7f\u544a\u4e2d\u7684\u7528\u6237\u8d2d\u4e70\u884c\u4e3a\u5efa\u6a21\u6548\u679c", "motivation": "\u5c55\u793a\u5e7f\u544a\u7cfb\u7edf\u4e2d\u7528\u6237\u8d2d\u4e70\u884c\u4e3a\u5efa\u6a21\u9762\u4e34\u6b63\u6837\u672c\u7a00\u758f\u3001\u7528\u6237\u884c\u4e3a\u968f\u673a\u6027\u5bfc\u81f4\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u4e8b\u4ef6\u65f6\u95f4\u4e0d\u89c4\u5219\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\"\u8ba1\u6570\u5668\"\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u610f\u56fe\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u6f14\u5316\uff0c\u800c\u5f53\u524d\u5e8f\u5217\u6a21\u578b\u53c8\u7f3a\u5c11\u6709\u7528\u7684\u4e8b\u4ef6\u8ba1\u6570\u7edf\u8ba1\u4fe1\u606f\u3002", "method": "\u63d0\u51faAbacus\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u4e8b\u4ef6\u7684\u7ecf\u9a8c\u9891\u7387\u5206\u5e03\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u6df7\u5408\u76ee\u6807\u51fd\u6570\uff0c\u5c06Abacus\u4e0e\u5e8f\u5217\u5b66\u4e60\u76ee\u6807\u7edf\u4e00\uff0c\u7ed3\u5408\u805a\u5408\u7edf\u8ba1\u7684\u7a33\u5b9a\u6027\u548c\u5e8f\u5217\u5efa\u6a21\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAbacus\u9884\u8bad\u7ec3\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u52a0\u901f\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6536\u655b\uff0c\u6df7\u5408\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebfAUC\u63d0\u5347\u9ad8\u8fbe+6.1%\u3002", "conclusion": "Abacus\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u9884\u6d4b\u7528\u6237\u4e8b\u4ef6\u9891\u7387\u5206\u5e03\uff0c\u7ed3\u5408\u5e8f\u5217\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c55\u793a\u5e7f\u544a\u4e2d\u7528\u6237\u8d2d\u4e70\u884c\u4e3a\u5efa\u6a21\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.16822", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16822", "abs": "https://arxiv.org/abs/2512.16822", "authors": ["Qian Wang", "Zahra Yousefijamarani", "Morgan Lindsay Heisler", "Rongzhi Gu", "Bai Xiaolong", "Shan Yizhou", "Wei Zhang", "Wang Lan", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving", "comment": null, "summary": "Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.", "AI": {"tldr": "MEPIC\u662f\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u7684KV\u7f13\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5757\u5bf9\u9f50\u3001\u5757\u7ea7\u91cd\u8ba1\u7b97\u548cRoPE\u878d\u5408\u6280\u672f\uff0c\u5b9e\u73b0\u8de8\u4f4d\u7f6e\u3001\u8bf7\u6c42\u548c\u6279\u6b21\u7684KV\u7f13\u5b58\u91cd\u7528\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u4ee3LLM\u5e94\u7528\uff08\u5982\u6df1\u5ea6\u7814\u7a76\u52a9\u624b\u3001\u7f16\u7801\u4ee3\u7406\u548cRAG\u7cfb\u7edf\uff09\u9700\u8981\u53cd\u590d\u5904\u7406\u5305\u542b\u5171\u4eab\u6587\u6863\u6216\u4ee3\u7801\u5757\u7684\u957f\u63d0\u793a\u5386\u53f2\uff0c\u7ed9KV\u7f13\u5b58\u5e26\u6765\u5de8\u5927\u538b\u529b\u3002\u73b0\u6709\u524d\u7f00\u7f13\u5b58\u548c\u4f4d\u7f6e\u65e0\u5173\u7f13\u5b58\u5b58\u5728\u9650\u5236\uff0c\u5bfc\u81f4\u5373\u4f7f\u8bb8\u591a\u8bf7\u6c42\u91cd\u7528\u76f8\u540c\u5185\u5bb9\uff0c\u5185\u5b58\u8282\u7701\u6548\u679c\u4e5f\u6709\u9650\u3002", "method": "1) \u5c06\u5757KV\u5bf9\u9f50\u5230\u5206\u9875\u5b58\u50a8\uff1b2) \u5c06\u91cd\u8ba1\u7b97\u4ece\u4ee4\u724c\u7ea7\u8f6c\u79fb\u5230\u5757\u7ea7\uff0c\u4f7f\u53ea\u6709\u7b2c\u4e00\u4e2a\u5757\u662f\u8bf7\u6c42\u7279\u5b9a\u7684\uff1b3) \u901a\u8fc7\u6ce8\u610f\u529b\u5185\u6838\u4e2d\u7684RoPE\u878d\u5408\u79fb\u9664\u4f4d\u7f6e\u7f16\u7801\uff1b4) \u4f7f\u5269\u4f59\u5757\u5b8c\u5168\u53ef\u5171\u4eab\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684PIC\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0cHBM\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe2\u500d\uff1b\u5bf9\u4e8e\u957f\u63d0\u793a\uff0c\u51cf\u5c11\u9ad8\u8fbe5\u500d\uff0c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u66f4\u6539\u3002", "conclusion": "MEPIC\u901a\u8fc7\u6d88\u9664HBM\u4e2d\u7684\u5927\u90e8\u5206\u91cd\u590d\u5757KV\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5185\u5b58\u6548\u7387\uff0c\u89e3\u51b3\u4e86LLM\u5e94\u7528\u4e2dKV\u7f13\u5b58\u7684\u5185\u5b58\u538b\u529b\u95ee\u9898\u3002"}}
{"id": "2512.16866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16866", "abs": "https://arxiv.org/abs/2512.16866", "authors": ["Jiabin Xue"], "title": "Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models", "comment": null, "summary": "Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: \"How to determine labels for truly future, unseen data points\". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.", "AI": {"tldr": "\u63d0\u51faKnowledge Transformation (KT)\u65b9\u6cd5\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u3001\u4e3b\u52a8\u5b66\u4e60\u548c\u56e0\u679c\u63a8\u7406\uff0c\u4e3a\u5728\u7ebf\u8fb9\u7f18\u673a\u5668\u5b66\u4e60\u4e2d\u672a\u89c1\u6570\u636e\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u89e3\u51b3\u6807\u7b7e\u83b7\u53d6\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u9759\u6001\u6a21\u578b\u5728\u4e2d\u5fc3\u8bad\u7ec3\u540e\u90e8\u7f72\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u672a\u89c1\u6570\u636e\u3002\u5728\u7ebf\u8fb9\u7f18\u673a\u5668\u5b66\u4e60\u5141\u8bb8\u6a21\u578b\u76f4\u63a5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bad\u7ec3\u5e76\u6301\u7eed\u66f4\u65b0\uff0c\u4f46\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u5982\u4f55\u4e3a\u771f\u6b63\u672a\u6765\u7684\u672a\u89c1\u6570\u636e\u70b9\u786e\u5b9a\u6807\u7b7e\u3002", "method": "\u63d0\u51faKnowledge Transformation (KT)\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u3001\u4e3b\u52a8\u5b66\u4e60\u548c\u56e0\u679c\u63a8\u7406\u3002KT\u5728\u4e3b\u52a8\u5b66\u4e60\u4e2d\u626e\u6f14\u9884\u8a00\u673a\u89d2\u8272\uff0c\u901a\u8fc7\u5c06\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u5316\u4e3a\u5b66\u751f\u6a21\u578b\u7684\u4f2a\u6807\u7b7e\u3002\u65b9\u6cd5\u9a8c\u8bc1\u91c7\u7528\u4e24\u79cd\u4eff\u771f\u5b9e\u9a8c\u8bbe\u7f6e\uff1a(1)\u4f7f\u7528\u7a33\u5b9a\u6027\u8f83\u4f4e\u7684\u6559\u5e08\u6a21\u578b\uff0c(2)\u4f7f\u7528\u76f8\u5bf9\u66f4\u7a33\u5b9a\u7684\u6559\u5e08\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u63d0\u4f9b\u7a33\u5b9a\u7684\u6559\u5e08\u6a21\u578b\u65f6\uff0c\u5b66\u751f\u6a21\u578b\u6700\u7ec8\u80fd\u8fbe\u5230\u5176\u9884\u671f\u6700\u5927\u6027\u80fd\u3002KT\u5728\u4ee5\u4e0b\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u5728\u4f18\u52bf\uff1a(1)\u6559\u5e08\u4efb\u52a1\u5177\u6709\u901a\u7528\u6027\uff0c\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u80fd\u8db3\u591f\u5b8c\u6210\u4efb\u52a1\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\uff1b(2)\u5b66\u751f\u4efb\u52a1\u7684\u6807\u7b7e\u96be\u4ee5\u83b7\u53d6\u6216\u6210\u672c\u9ad8\u6602\u3002", "conclusion": "KT\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5728\u7ebf\u8fb9\u7f18\u673a\u5668\u5b66\u4e60\u4e2d\u672a\u89c1\u6570\u636e\u7684\u6807\u7b7e\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6559\u5e08\u4efb\u52a1\u901a\u7528\u6216\u5b66\u751f\u6807\u7b7e\u83b7\u53d6\u56f0\u96be\u7684\u573a\u666f\uff0c\u80fd\u591f\u901a\u8fc7\u77e5\u8bc6\u8f6c\u5316\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u7684\u6700\u5927\u5316\u3002"}}
{"id": "2512.16924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16924", "abs": "https://arxiv.org/abs/2512.16924", "authors": ["Hanlin Wang", "Hao Ouyang", "Qiuyu Wang", "Yue Yu", "Yihao Meng", "Wen Wang", "Ka Leong Cheng", "Shuailei Ma", "Qingyan Bai", "Yixuan Li", "Cheng Chen", "Yanhong Zeng", "Xing Zhu", "Yujun Shen", "Qifeng Chen"], "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text", "comment": "Project page and code: https://worldcanvas.github.io/", "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.", "AI": {"tldr": "WorldCanvas\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u53ef\u63d0\u793a\u4e16\u754c\u4e8b\u4ef6\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u3001\u8f68\u8ff9\u548c\u53c2\u8003\u56fe\u50cf\u5b9e\u73b0\u7528\u6237\u5bfc\u5411\u7684\u4e30\u5bcc\u6a21\u62df\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3001\u5bf9\u8c61\u8fdb\u51fa\u3001\u53c2\u8003\u5f15\u5bfc\u5916\u89c2\u7b49\u590d\u6742\u4e8b\u4ef6\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7eaf\u6587\u672c\u65b9\u6cd5\u7f3a\u4e4f\u89c6\u89c9\u63a7\u5236\uff0c\u73b0\u6709\u8f68\u8ff9\u63a7\u5236\u56fe\u50cf\u5230\u89c6\u9891\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u8868\u8fbe\u8bed\u4e49\u610f\u56fe\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u8fd0\u52a8\u3001\u8bed\u4e49\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5b9e\u73b0\u66f4\u4e30\u5bcc\u3001\u53ef\u63a7\u7684\u4e16\u754c\u4e8b\u4ef6\u6a21\u62df\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e09\u79cd\u8f93\u5165\uff1a1)\u8f68\u8ff9\u7f16\u7801\u8fd0\u52a8\u3001\u65f6\u5e8f\u548c\u53ef\u89c1\u6027\uff1b2)\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u8bed\u4e49\u610f\u56fe\uff1b3)\u53c2\u8003\u56fe\u50cf\u63d0\u4f9b\u5bf9\u8c61\u8eab\u4efd\u7684\u89c6\u89c9\u57fa\u7840\u3002\u901a\u8fc7\u8fd9\u79cd\u7ec4\u5408\u5b9e\u73b0\u8fde\u8d2f\u53ef\u63a7\u7684\u4e8b\u4ef6\u751f\u6210\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3001\u5bf9\u8c61\u8fdb\u51fa\u3001\u53c2\u8003\u5f15\u5bfc\u5916\u89c2\u548c\u53cd\u76f4\u89c9\u4e8b\u4ef6\u3002", "result": "\u751f\u6210\u7684\u89c6\u9891\u4e0d\u4ec5\u5177\u6709\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u8fd8\u5c55\u73b0\u51fa\u6d8c\u73b0\u4e00\u81f4\u6027\uff0c\u80fd\u591f\u4fdd\u6301\u5bf9\u8c61\u8eab\u4efd\u548c\u573a\u666f\u7684\u8fde\u7eed\u6027\uff0c\u5373\u4f7f\u5bf9\u8c61\u6682\u65f6\u6d88\u5931\u4e5f\u80fd\u4fdd\u6301\u4e00\u81f4\u3002\u6846\u67b6\u5c06\u4e16\u754c\u6a21\u578b\u4ece\u88ab\u52a8\u9884\u6d4b\u5668\u63d0\u5347\u4e3a\u4ea4\u4e92\u5f0f\u3001\u7528\u6237\u53ef\u5851\u7684\u6a21\u62df\u5668\u3002", "conclusion": "WorldCanvas\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e30\u5bcc\u3001\u7528\u6237\u5bfc\u5411\u7684\u4e16\u754c\u4e8b\u4ef6\u6a21\u62df\uff0c\u7ed3\u5408\u8f68\u8ff9\u3001\u6587\u672c\u548c\u53c2\u8003\u56fe\u50cf\u7684\u4f18\u52bf\uff0c\u4e3a\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4ece\u88ab\u52a8\u9884\u6d4b\u5230\u4ea4\u4e92\u5f0f\u6a21\u62df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
