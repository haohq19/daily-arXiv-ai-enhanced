{"id": "2511.02458", "categories": ["cs.CL", "cs.CE", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2511.02458", "abs": "https://arxiv.org/abs/2511.02458", "authors": ["Giulia Iadisernia", "Carolina Camassa"], "title": "Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas", "comment": "9 pages, 8-pages appendix, accepted at ICAIF 25", "summary": "We evaluate whether persona-based prompting improves Large Language Model\n(LLM) performance on macroeconomic forecasting tasks. Using 2,368\neconomics-related personas from the PersonaHub corpus, we prompt GPT-4o to\nreplicate the ECB Survey of Professional Forecasters across 50 quarterly rounds\n(2013-2025). We compare the persona-prompted forecasts against the human\nexperts panel, across four target variables (HICP, core HICP, GDP growth,\nunemployment) and four forecast horizons. We also compare the results against\n100 baseline forecasts without persona descriptions to isolate its effect. We\nreport two main findings. Firstly, GPT-4o and human forecasters achieve\nremarkably similar accuracy levels, with differences that are statistically\nsignificant yet practically modest. Our out-of-sample evaluation on 2024-2025\ndata demonstrates that GPT-4o can maintain competitive forecasting performance\non unseen events, though with notable differences compared to the in-sample\nperiod. Secondly, our ablation experiment reveals no measurable forecasting\nadvantage from persona descriptions, suggesting these prompt components can be\nomitted to reduce computational costs without sacrificing accuracy. Our results\nprovide evidence that GPT-4o can achieve competitive forecasting accuracy even\non out-of-sample macroeconomic events, if provided with relevant context data,\nwhile revealing that diverse prompts produce remarkably homogeneous forecasts\ncompared to human panels.", "AI": {"tldr": "\u8bc4\u4f30\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u662f\u5426\u80fd\u63d0\u5347LLM\u5728\u5b8f\u89c2\u7ecf\u6d4e\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u9884\u6d4b\u7cbe\u5ea6\u76f8\u4f3c\uff0c\u4f46\u89d2\u8272\u63cf\u8ff0\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u65e0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u89d2\u8272\u63d0\u793a\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b8f\u89c2\u7ecf\u6d4e\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u9884\u6d4b\u8868\u73b0\u7684\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528PersonaHub\u8bed\u6599\u5e93\u4e2d\u76842,368\u4e2a\u7ecf\u6d4e\u5b66\u76f8\u5173\u89d2\u8272\u63d0\u793aGPT-4o\uff0c\u590d\u5236ECB\u4e13\u4e1a\u9884\u6d4b\u8005\u8c03\u67e5\u768450\u4e2a\u5b63\u5ea6\u8f6e\u6b21\uff082013-2025\uff09\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5c0f\u7ec4\u548c\u65e0\u89d2\u8272\u63cf\u8ff0\u7684\u57fa\u7ebf\u9884\u6d4b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPT-4o\u4e0e\u4eba\u7c7b\u9884\u6d4b\u8005\u8fbe\u5230\u76f8\u4f3c\u7684\u51c6\u786e\u5ea6\u6c34\u5e73\uff0c\u5dee\u5f02\u867d\u7edf\u8ba1\u663e\u8457\u4f46\u5b9e\u9645\u4e0d\u5927\uff1b\u57282024-2025\u5e74\u6837\u672c\u5916\u8bc4\u4f30\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff1b\u89d2\u8272\u63cf\u8ff0\u672a\u5e26\u6765\u53ef\u6d4b\u91cf\u7684\u9884\u6d4b\u4f18\u52bf\u3002", "conclusion": "GPT-4o\u5728\u63d0\u4f9b\u76f8\u5173\u4e0a\u4e0b\u6587\u6570\u636e\u65f6\u80fd\u5728\u5b8f\u89c2\u7ecf\u6d4e\u9884\u6d4b\u4e2d\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u591a\u6837\u5316\u63d0\u793a\u4ea7\u751f\u7684\u9884\u6d4b\u76f8\u6bd4\u4eba\u7c7b\u5c0f\u7ec4\u66f4\u52a0\u540c\u8d28\u5316\uff0c\u89d2\u8272\u63cf\u8ff0\u53ef\u7701\u7565\u4ee5\u8282\u7701\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2511.02180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02180", "abs": "https://arxiv.org/abs/2511.02180", "authors": ["Mehdi Sefidgar Dilmaghani", "Waseem Shariff", "Cian Ryan", "Joe Lemley", "Peter Corcoran"], "title": "Autobiasing Event Cameras for Flickering Mitigation", "comment": null, "summary": "Understanding and mitigating flicker effects caused by rapid variations in\nlight intensity is critical for enhancing the performance of event cameras in\ndiverse environments. This paper introduces an innovative autonomous mechanism\nfor tuning the biases of event cameras, effectively addressing flicker across a\nwide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on\nadditional hardware or software for flicker filtering, our approach leverages\nthe event cameras inherent bias settings. Utilizing a simple Convolutional\nNeural Networks -CNNs, the system identifies instances of flicker in a spatial\nspace and dynamically adjusts specific biases to minimize its impact. The\nefficacy of this autobiasing system was robustly tested using a face detector\nframework under both well-lit and low-light conditions, as well as across\nvarious frequencies. The results demonstrated significant improvements:\nenhanced YOLO confidence metrics for face detection, and an increased\npercentage of frames capturing detected faces. Moreover, the average gradient,\nwhich serves as an indicator of flicker presence through edge detection,\ndecreased by 38.2 percent in well-lit conditions and by 53.6 percent in\nlow-light conditions. These findings underscore the potential of our approach\nto significantly improve the functionality of event cameras in a range of\nadverse lighting scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u81ea\u9002\u5e94\u504f\u7f6e\u8c03\u8282\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e8b\u4ef6\u76f8\u673a\u7684\u504f\u7f6e\u8bbe\u7f6e\u6765\u6291\u523625-500Hz\u8303\u56f4\u5185\u7684\u95ea\u70c1\u6548\u5e94\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u8f6f\u4ef6\u6ee4\u6ce2\u3002", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u5728\u4e0d\u540c\u5149\u7167\u73af\u5883\u4e0b\u56e0\u5149\u5f3a\u5feb\u901f\u53d8\u5316\u4ea7\u751f\u7684\u95ea\u70c1\u6548\u5e94\uff0c\u63d0\u5347\u4e8b\u4ef6\u76f8\u673a\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u7a7a\u95f4\u57df\u8bc6\u522b\u95ea\u70c1\u73b0\u8c61\uff0c\u5e76\u52a8\u6001\u8c03\u8282\u7279\u5b9a\u504f\u7f6e\u53c2\u6570\u6765\u6700\u5c0f\u5316\u95ea\u70c1\u5f71\u54cd\uff0c\u901a\u8fc7\u4eba\u8138\u68c0\u6d4b\u6846\u67b6\u5728\u591a\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u663e\u8457\u63d0\u5347YOLO\u4eba\u8138\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u589e\u52a0\u68c0\u6d4b\u5230\u4eba\u8138\u7684\u5e27\u6570\u6bd4\u4f8b\uff0c\u5728\u826f\u597d\u5149\u7167\u548c\u4f4e\u5149\u6761\u4ef6\u4e0b\u5e73\u5747\u68af\u5ea6\u5206\u522b\u964d\u4f4e38.2%\u548c53.6%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u4e8b\u4ef6\u76f8\u673a\u5728\u4e0d\u5229\u5149\u7167\u573a\u666f\u4e0b\u7684\u529f\u80fd\u8868\u73b0\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.01937", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.01937", "abs": "https://arxiv.org/abs/2511.01937", "authors": ["Abdelaziz Bounhar", "Hadi Abdine", "Evan Dufraisse", "Ahmad Chamma", "Amr Mohamed", "Dani Bouch", "Michalis Vazirgiannis", "Guokan Shang"], "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR", "comment": null, "summary": "Large language models (LLMs) trained for step-by-step reasoning often become\nexcessively verbose, raising inference cost. Standard Reinforcement Learning\nwith Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for\ntraining efficiency, leaving the model to train primarily on harder problems\nthat require longer reasoning chains. This skews the output length distribution\nupward, resulting in a \\textbf{model that conflates ``thinking longer'' with\n``thinking better''}. In this work, we show that retaining and modestly\nup-weighting moderately easy problems acts as an implicit length regularizer.\nExposing the model to solvable short-chain tasks constrains its output\ndistribution and prevents runaway verbosity. The result is\n\\textbf{\\emph{emergent brevity for free}}: the model learns to solve harder\nproblems without inflating the output length, \\textbf{ despite the absence of\nany explicit length penalization}. RLVR experiments using this approach on\n\\textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline\npass@1 AIME25 accuracy while generating solutions that are, on average, nearly\ntwice as short. The code is available at\n\\href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and\nmodels on\n\\href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging\nFace}.", "AI": {"tldr": "\u901a\u8fc7\u4fdd\u7559\u5e76\u9002\u5ea6\u52a0\u6743\u4e2d\u7b49\u96be\u5ea6\u95ee\u9898\uff0c\u53ef\u4ee5\u5728\u4e0d\u663e\u5f0f\u60e9\u7f5a\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8f93\u51fa\u7b80\u6d01\u6027\uff0c\u4f7f\u6a21\u578b\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u751f\u6210\u66f4\u77ed\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9010\u6b65\u63a8\u7406\u65f6\u5f80\u5f80\u8fc7\u4e8e\u5197\u957f\uff0c\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002\u6807\u51c6\u7684RLVR\u6d41\u7a0b\u8fc7\u6ee4\u6389\u7b80\u5355\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u4e3b\u8981\u5728\u9700\u8981\u957f\u63a8\u7406\u94fe\u7684\u96be\u9898\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u9519\u8bef\u5730\u5c06\"\u601d\u8003\u66f4\u4e45\"\u7b49\u540c\u4e8e\"\u601d\u8003\u66f4\u597d\"\u3002", "method": "\u4fdd\u7559\u5e76\u9002\u5ea6\u52a0\u6743\u4e2d\u7b49\u96be\u5ea6\u95ee\u9898\u4f5c\u4e3a\u9690\u5f0f\u957f\u5ea6\u6b63\u5219\u5316\u5668\uff0c\u8ba9\u6a21\u578b\u63a5\u89e6\u53ef\u89e3\u51b3\u7684\u77ed\u94fe\u4efb\u52a1\u6765\u7ea6\u675f\u8f93\u51fa\u5206\u5e03\uff0c\u9632\u6b62\u8fc7\u5ea6\u5197\u957f\u3002", "result": "\u5728Qwen3-4B-Thinking-2507\u4e0a\u7684RLVR\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u57fa\u7ebfpass@1 AIME25\u51c6\u786e\u7387\uff0c\u540c\u65f6\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u5e73\u5747\u7f29\u77ed\u8fd1\u4e00\u500d\u3002", "conclusion": "\u65e0\u9700\u663e\u5f0f\u957f\u5ea6\u60e9\u7f5a\u5373\u53ef\u5b9e\u73b0\u7b80\u6d01\u6027\uff0c\u6a21\u578b\u5b66\u4f1a\u89e3\u51b3\u66f4\u96be\u95ee\u9898\u800c\u4e0d\u4f1a\u589e\u52a0\u8f93\u51fa\u957f\u5ea6\uff0c\u5b9e\u73b0\u4e86\"\u514d\u8d39\u51fa\u73b0\u7684\u7b80\u6d01\u6027\"\u3002"}}
{"id": "2511.02042", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02042", "abs": "https://arxiv.org/abs/2511.02042", "authors": ["M. Z. Haider", "M. U. Ghouri", "Tayyaba Noreen", "M. Salman"], "title": "Quantum-Enhanced Generative Models for Rare Event Prediction", "comment": "IEEE Conference COMCOMAP 2025", "summary": "Rare events such as financial crashes, climate extremes, and biological\nanomalies are notoriously difficult to model due to their scarcity and\nheavy-tailed distributions. Classical deep generative models often struggle to\ncapture these rare occurrences, either collapsing low-probability modes or\nproducing poorly calibrated uncertainty estimates. In this work, we propose the\nQuantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework\nthat integrates deep latent-variable models with variational quantum circuits.\nThe framework introduces two key innovations: (1) a hybrid loss function that\njointly optimizes reconstruction fidelity and tail-aware likelihood, and (2)\nquantum randomness-driven noise injection to enhance sample diversity and\nmitigate mode collapse. Training proceeds via a hybrid loop where classical\nparameters are updated through backpropagation while quantum parameters are\noptimized using parameter-shift gradients. We evaluate QEGM on synthetic\nGaussian mixtures and real-world datasets spanning finance, climate, and\nprotein structure. Results demonstrate that QEGM reduces tail KL divergence by\nup to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion),\nwhile improving rare-event recall and coverage calibration. These findings\nhighlight the potential of QEGM as a principled approach for rare-event\nprediction, offering robustness beyond what is achievable with purely classical\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u589e\u5f3a\u751f\u6210\u6a21\u578b(QEGM)\uff0c\u4e00\u79cd\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5206\u91cf\u5b50\u7535\u8def\u589e\u5f3a\u5bf9\u7f55\u89c1\u4e8b\u4ef6\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u5c3e\u90e8KL\u6563\u5ea6\u5e76\u6539\u5584\u7f55\u89c1\u4e8b\u4ef6\u53ec\u56de\u7387\u3002", "motivation": "\u7f55\u89c1\u4e8b\u4ef6\uff08\u5982\u91d1\u878d\u5371\u673a\u3001\u6781\u7aef\u6c14\u5019\uff09\u7531\u4e8e\u7a00\u7f3a\u6027\u548c\u91cd\u5c3e\u5206\u5e03\u96be\u4ee5\u5efa\u6a21\uff0c\u4f20\u7edf\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6355\u6349\u8fd9\u4e9b\u7f55\u89c1\u4e8b\u4ef6\u65f6\u5bb9\u6613\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u6216\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u51c6\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u9690\u53d8\u91cf\u6a21\u578b\u4e0e\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff0c\u91c7\u7528\u6df7\u5408\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\u91cd\u6784\u4fdd\u771f\u5ea6\u548c\u5c3e\u90e8\u611f\u77e5\u4f3c\u7136\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u968f\u673a\u6027\u9a71\u52a8\u7684\u566a\u58f0\u6ce8\u5165\u589e\u5f3a\u6837\u672c\u591a\u6837\u6027\u3002", "result": "\u5728\u5408\u6210\u9ad8\u65af\u6df7\u5408\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u91d1\u878d\u3001\u6c14\u5019\u3001\u86cb\u767d\u8d28\u7ed3\u6784\uff09\u4e0a\u8bc4\u4f30\uff0cQEGM\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\uff08GAN\u3001VAE\u3001Diffusion\uff09\u5c06\u5c3e\u90e8KL\u6563\u5ea6\u964d\u4f4e\u8fbe50%\uff0c\u540c\u65f6\u6539\u5584\u7f55\u89c1\u4e8b\u4ef6\u53ec\u56de\u7387\u548c\u8986\u76d6\u6821\u51c6\u3002", "conclusion": "QEGM\u4e3a\u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u8d85\u8d8a\u7eaf\u7ecf\u5178\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u6f5c\u529b\u3002"}}
{"id": "2511.02734", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02734", "abs": "https://arxiv.org/abs/2511.02734", "authors": ["Jiayu Liu", "Cheng Qian", "Zhaochen Su", "Qing Zong", "Shijue Huang", "Bingxiang He", "Yi R. Fung"], "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents", "comment": null, "summary": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust.", "AI": {"tldr": "CostBench\u662f\u4e00\u4e2a\u4ee5\u6210\u672c\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u7ecf\u6d4e\u63a8\u7406\u548c\u91cd\u65b0\u89c4\u5212\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e0b\u4ee3\u7406\u90fd\u96be\u4ee5\u627e\u5230\u6210\u672c\u6700\u4f18\u89e3\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u5ea6\uff0c\u5ffd\u89c6\u4e86\u8d44\u6e90\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u4ee3\u7406\u5728\u53d8\u5316\u73af\u5883\u4e2d\u5236\u5b9a\u548c\u8c03\u6574\u6210\u672c\u6700\u4f18\u8ba1\u5212\u7684\u80fd\u529b\u3002", "method": "\u5728\u65c5\u884c\u89c4\u5212\u9886\u57df\u6784\u5efaCostBench\u57fa\u51c6\uff0c\u5305\u542b\u53ef\u901a\u8fc7\u591a\u79cd\u539f\u5b50\u548c\u590d\u5408\u5de5\u5177\u5e8f\u5217\u89e3\u51b3\u7684\u4efb\u52a1\uff0c\u652f\u6301\u56db\u79cd\u52a8\u6001\u963b\u585e\u4e8b\u4ef6\uff08\u5982\u5de5\u5177\u6545\u969c\u548c\u6210\u672c\u53d8\u5316\uff09\u6765\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u4ee3\u7406\u5728\u6210\u672c\u611f\u77e5\u89c4\u5212\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u5728\u9759\u6001\u8bbe\u7f6e\u4e2d\u7ecf\u5e38\u65e0\u6cd5\u627e\u5230\u6210\u672c\u6700\u4f18\u89e3\uff0cGPT-5\u5728\u6700\u96be\u4efb\u52a1\u4e0a\u7684\u7cbe\u786e\u5339\u914d\u7387\u4f4e\u4e8e75%\uff0c\u52a8\u6001\u6761\u4ef6\u4e0b\u6027\u80fd\u8fdb\u4e00\u6b65\u4e0b\u964d\u7ea640%\u3002", "conclusion": "CostBench\u901a\u8fc7\u8bca\u65ad\u8fd9\u4e9b\u5f31\u70b9\uff0c\u4e3a\u5f00\u53d1\u65e2\u7ecf\u6d4e\u7406\u6027\u53c8\u9c81\u68d2\u7684\u672a\u6765\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.02818", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02818", "abs": "https://arxiv.org/abs/2511.02818", "authors": ["Mohamed Bouadi", "Pratinav Seth", "Aditya Tanna", "Vinay Kumar Sankarapu"], "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning", "comment": null, "summary": "Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances in tabular in-context learning (ICL), such as TabPFN\nand TabICL, have achieved state-of-the-art performance comparable to\ngradient-boosted trees (GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)\nmulti-scale processing to capture hierarchical feature interactions; (2)\nblock-sparse attention combining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficient tabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP .", "AI": {"tldr": "Orion-MSP\u662f\u4e00\u79cd\u521b\u65b0\u7684\u8868\u683c\u6570\u636e\u4e0a\u4e0b\u6587\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5904\u7406\u3001\u5757\u7a00\u758f\u6ce8\u610f\u529b\u548cPerceiver\u98ce\u683c\u8bb0\u5fc6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u7ef4\u8868\u683c\u7684\u6709\u6548\u6269\u5c55\u3002", "motivation": "\u5f53\u524d\u8868\u683c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u5355\u5c3a\u5ea6\u7279\u5f81\u5904\u7406\u5ffd\u7565\u5c42\u6b21\u4f9d\u8d56\u3001\u5bc6\u96c6\u6ce8\u610f\u529b\u673a\u5236\u5728\u8868\u683c\u5bbd\u5ea6\u4e0a\u5448\u4e8c\u6b21\u65b9\u6269\u5c55\u3001\u4e25\u683c\u987a\u5e8f\u7ec4\u4ef6\u5904\u7406\u963b\u788d\u8fed\u4ee3\u8868\u793a\u4f18\u5316\u548c\u8de8\u7ec4\u4ef6\u901a\u4fe1\u3002", "method": "\u63d0\u51faOrion-MSP\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u591a\u5c3a\u5ea6\u5904\u7406\u6355\u83b7\u5c42\u6b21\u7279\u5f81\u4ea4\u4e92\uff1b\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7ed3\u5408\u7a97\u53e3\u5316\u3001\u5168\u5c40\u548c\u968f\u673a\u6a21\u5f0f\u5b9e\u73b0\u53ef\u6269\u5c55\u6548\u7387\u548c\u957f\u7a0b\u8fde\u63a5\uff1bPerceiver\u98ce\u683c\u8bb0\u5fc6\u5b9e\u73b0\u8de8\u7ec4\u4ef6\u7684\u5b89\u5168\u53cc\u5411\u4fe1\u606f\u6d41\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOrion-MSP\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u80fd\u591f\u6709\u6548\u6269\u5c55\u5230\u9ad8\u7ef4\u8868\u683c\u3002", "conclusion": "Orion-MSP\u4e3a\u9ad8\u6548\u8868\u683c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u67b6\u6784\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2511.02379", "categories": ["cs.LG", "cs.AI", "cs.SD", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02379", "abs": "https://arxiv.org/abs/2511.02379", "authors": ["Rohith Shinoj Kumar", "Rushdeep Dinda", "Aditya Tyagi", "Annappa B.", "Naveen Kumar M. R"], "title": "H-Infinity Filter Enhanced CNN-LSTM for Arrhythmia Detection from Heart Sound Recordings", "comment": "This is a preprint of a paper to appear at the 15th IEEE\n  International Conference on Systems Engineering and Technology (ICSET 2025)", "summary": "Early detection of heart arrhythmia can prevent severe future complications\nin cardiac patients. While manual diagnosis still remains the clinical\nstandard, it relies heavily on visual interpretation and is inherently\nsubjective. In recent years, deep learning has emerged as a powerful tool to\nautomate arrhythmia detection, offering improved accuracy, consistency, and\nefficiency. Several variants of convolutional and recurrent neural network\narchitectures have been widely explored to capture spatial and temporal\npatterns in physiological signals. However, despite these advancements, current\nmodels often struggle to generalize well in real-world scenarios, especially\nwhen dealing with small or noisy datasets, which are common challenges in\nbiomedical applications. In this paper, a novel CNN-H-Infinity-LSTM\narchitecture is proposed to identify arrhythmic heart signals from heart sound\nrecordings. This architecture introduces trainable parameters inspired by the\nH-Infinity filter from control theory, enhancing robustness and generalization.\nExtensive experimentation on the PhysioNet CinC Challenge 2016 dataset, a\npublic benchmark of heart audio recordings, demonstrates that the proposed\nmodel achieves stable convergence and outperforms existing benchmarks, with a\ntest accuracy of 99.42% and an F1 score of 98.85%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u3001H-Infinity\u6ee4\u6ce2\u5668\u548cLSTM\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u5fc3\u97f3\u8bb0\u5f55\u4e2d\u68c0\u6d4b\u5fc3\u5f8b\u5931\u5e38\uff0c\u5728PhysioNet\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8699.42%\u7684\u51c6\u786e\u7387\u548c98.85%\u7684F1\u5206\u6570\u3002", "motivation": "\u5fc3\u5f8b\u5931\u5e38\u7684\u65e9\u671f\u68c0\u6d4b\u53ef\u4ee5\u9884\u9632\u4e25\u91cd\u5e76\u53d1\u75c7\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u8bca\u65ad\u4f9d\u8d56\u89c6\u89c9\u89e3\u91ca\u4e14\u5177\u6709\u4e3b\u89c2\u6027\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5c0f\u6837\u672c\u6216\u566a\u58f0\u6570\u636e\u65f6\u3002", "method": "\u63d0\u51faCNN-H-Infinity-LSTM\u6df7\u5408\u67b6\u6784\uff0c\u5f15\u5165\u63a7\u5236\u7406\u8bba\u4e2d\u7684H-Infinity\u6ee4\u6ce2\u5668\u53ef\u8bad\u7ec3\u53c2\u6570\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728PhysioNet CinC Challenge 2016\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6536\u655b\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523099.42%\uff0cF1\u5206\u6570\u8fbe\u523098.85%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CNN-H-Infinity-LSTM\u67b6\u6784\u5728\u5fc3\u5f8b\u5931\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u7ed3\u5408\u63a7\u5236\u7406\u8bba\u6982\u5ff5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.02577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02577", "abs": "https://arxiv.org/abs/2511.02577", "authors": ["Gilad Karpel", "Ruida Zhou", "Shoham Sabach", "Mohammad Ghavamzadeh"], "title": "Directional-Clamp PPO", "comment": null, "summary": "Proximal Policy Optimization (PPO) is widely regarded as one of the most\nsuccessful deep reinforcement learning algorithms, known for its robustness and\neffectiveness across a range of problems.\n  The PPO objective encourages the importance ratio between the current and\nbehavior policies to move to the \"right\" direction -- starting from importance\nsampling ratios equal to 1, increasing the ratios for actions with positive\nadvantages and decreasing those with negative advantages. A clipping function\nis introduced to prevent over-optimization when updating the importance ratio\nin these \"right\" direction regions. Many PPO variants have been proposed to\nextend its success, most of which modify the objective's behavior by altering\nthe clipping in the \"right\" direction regions. However, due to randomness in\nthe rollouts and stochasticity of the policy optimization, we observe that the\nratios frequently move to the \"wrong\" direction during the PPO optimization.\nThis is a key factor hindering the improvement of PPO, but it has been largely\noverlooked. To address this, we propose the Directional-Clamp PPO algorithm\n(DClamp-PPO), which further penalizes the actions going to the strict \"wrong\"\ndirection regions, where the advantage is positive (negative) and importance\nratio falls below (above) $1 - \\beta$ ($1+\\beta$),\n  for a tunable parameter $\\beta \\in (0, 1)$. The penalty is by enforcing a\nsteeper loss slope, i.e., a clamp, in those regions. We demonstrate that\nDClamp-PPO consistently outperforms PPO, as well as its variants, by focusing\non modifying the objective's behavior in the \"right\" direction, across various\nMuJoCo environments, using different random seeds. The proposed method is\nshown, both theoretically and empirically, to better avoid \"wrong\" direction\nupdates while keeping the importance ratio closer to 1.", "AI": {"tldr": "\u63d0\u51fa\u4e86DClamp-PPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u91cd\u8981\u6027\u6bd4\u7387\u5411\"\u9519\u8bef\"\u65b9\u5411\u79fb\u52a8\u7684\u52a8\u4f5c\u6765\u6539\u8fdbPPO\u7b97\u6cd5\uff0c\u5728\u5404\u79cdMuJoCo\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8ePPO\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "PPO\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u7b56\u7565\u4f18\u5316\u7684\u968f\u673a\u6027\uff0c\u91cd\u8981\u6027\u6bd4\u7387\u7ecf\u5e38\u5411\"\u9519\u8bef\"\u65b9\u5411\u79fb\u52a8\uff0c\u8fd9\u662f\u963b\u788dPPO\u6539\u8fdb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u4e00\u76f4\u88ab\u5ffd\u89c6\u3002", "method": "\u63d0\u51fa\u65b9\u5411\u6027\u94b3\u4f4dPPO\u7b97\u6cd5\uff0c\u5728\u4e25\u683c\"\u9519\u8bef\"\u65b9\u5411\u533a\u57df\uff08\u4f18\u52bf\u4e3a\u6b63\u65f6\u6bd4\u7387\u4f4e\u4e8e1-\u03b2\uff0c\u4f18\u52bf\u4e3a\u8d1f\u65f6\u6bd4\u7387\u9ad8\u4e8e1+\u03b2\uff09\u65bd\u52a0\u66f4\u9661\u5ced\u7684\u635f\u5931\u659c\u7387\u6765\u60e9\u7f5a\u8fd9\u4e9b\u52a8\u4f5c\u3002", "result": "DClamp-PPO\u5728\u5404\u79cdMuJoCo\u73af\u5883\u4e2d\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u65f6\uff0c\u59cb\u7ec8\u4f18\u4e8ePPO\u53ca\u5176\u53d8\u4f53\uff0c\u7406\u8bba\u548c\u5b9e\u8bc1\u90fd\u8868\u660e\u80fd\u66f4\u597d\u5730\u907f\u514d\"\u9519\u8bef\"\u65b9\u5411\u66f4\u65b0\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u4fee\u6539\u76ee\u6807\u51fd\u6570\u5728\"\u9519\u8bef\"\u65b9\u5411\u533a\u57df\u7684\u884c\u4e3a\uff0cDClamp-PPO\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u91cd\u8981\u6027\u6bd4\u7387\u63a5\u8fd11\uff0c\u63d0\u9ad8PPO\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.02659", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.02659", "abs": "https://arxiv.org/abs/2511.02659", "authors": ["Cooper Simpson", "Stephen Becker", "Alireza Doostan"], "title": "In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization", "comment": "17 pages, 8 figures, 4 tables", "summary": "Focusing on implicit neural representations, we present a novel in situ\ntraining protocol that employs limited memory buffers of full and sketched data\nsamples, where the sketched data are leveraged to prevent catastrophic\nforgetting. The theoretical motivation for our use of sketching as a\nregularizer is presented via a simple Johnson-Lindenstrauss-informed result.\nWhile our methods may be of wider interest in the field of continual learning,\nwe specifically target in situ neural compression using implicit neural\nrepresentation-based hypernetworks. We evaluate our method on a variety of\ncomplex simulation data in two and three dimensions, over long time horizons,\nand across unstructured grids and non-Cartesian geometries. On these tasks, we\nshow strong reconstruction performance at high compression rates. Most\nimportantly, we demonstrate that sketching enables the presented in situ scheme\nto approximately match the performance of the equivalent offline method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u539f\u5730\u8bad\u7ec3\u534f\u8bae\uff0c\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u548c\u8349\u56fe\u6570\u636e\u7684\u6709\u9650\u5185\u5b58\u7f13\u51b2\u533a\uff0c\u901a\u8fc7\u8349\u56fe\u6570\u636e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u7279\u522b\u9488\u5bf9\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u795e\u7ecf\u538b\u7f29\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u5728\u8fde\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u7279\u522b\u662f\u5728\u539f\u5730\u795e\u7ecf\u538b\u7f29\u4efb\u52a1\u4e2d\uff0c\u5982\u4f55\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002\u8349\u56fe\u6570\u636e\u88ab\u7528\u4f5c\u6b63\u5219\u5316\u5668\u6765\u7ef4\u6301\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u6709\u9650\u5185\u5b58\u7f13\u51b2\u533a\u5b58\u50a8\u5b8c\u6574\u548c\u8349\u56fe\u6570\u636e\u6837\u672c\uff0c\u901a\u8fc7Johnson-Lindenstrauss\u7406\u8bba\u6307\u5bfc\u7684\u8349\u56fe\u6280\u672f\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\uff0c\u5728\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u8d85\u7f51\u7edc\u4e0a\u8fdb\u884c\u539f\u5730\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u4eff\u771f\u6570\u636e\u4e0a\uff082D/3D\u3001\u957f\u65f6\u95f4\u8de8\u5ea6\u3001\u975e\u7ed3\u6784\u5316\u7f51\u683c\u548c\u975e\u7b1b\u5361\u5c14\u51e0\u4f55\uff09\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u91cd\u5efa\u6027\u80fd\u548c\u9ad8\u538b\u7f29\u7387\uff0c\u8349\u56fe\u65b9\u6cd5\u4f7f\u539f\u5730\u65b9\u6848\u6027\u80fd\u63a5\u8fd1\u7b49\u6548\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8349\u56fe\u6280\u672f\u80fd\u591f\u6709\u6548\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f7f\u539f\u5730\u8bad\u7ec3\u65b9\u6848\u5728\u795e\u7ecf\u538b\u7f29\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u79bb\u7ebf\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8be5\u65b9\u6cd5\u5728\u8fde\u7eed\u5b66\u4e60\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
