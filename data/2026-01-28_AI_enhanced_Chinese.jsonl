{"id": "2601.19079", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19079", "abs": "https://arxiv.org/abs/2601.19079", "authors": ["Naqash Afzal", "Niklas Funk", "Erik Helmut", "Jan Peters", "Benjamin Ward-Cherrier"], "title": "Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing", "comment": null, "summary": "Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u4e8b\u4ef6\u89e6\u89c9\u4f20\u611f\u5668Evetac\u7684\u8fde\u7eed\u76f2\u6587\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u65f6\u7a7a\u5206\u5272\u548c\u8f7b\u91cf\u7ea7ResNet\u5206\u7c7b\u5668\u5904\u7406\u7a00\u758f\u4e8b\u4ef6\u6d41\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u8bc6\u522b\u3002", "motivation": "\u4f20\u7edf\u76f2\u6587\u9605\u8bfb\u5668\u91c7\u7528\u79bb\u6563\u9010\u5b57\u7b26\u626b\u63cf\uff0c\u9650\u5236\u9605\u8bfb\u901f\u5ea6\u4e14\u7834\u574f\u81ea\u7136\u6d41\u7a0b\uff1b\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u3001\u5ef6\u8fdf\u9ad8\u4e14\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u795e\u7ecf\u5f62\u6001\u4e8b\u4ef6\u89e6\u89c9\u4f20\u611f\u5668Evetac\uff0c\u7ed3\u5408\u65f6\u7a7a\u5206\u5272\u548c\u8f7b\u91cf\u7ea7ResNet\u5206\u7c7b\u5668\u5904\u7406\u8fde\u7eed\u6ed1\u52a8\u4e2d\u7684\u52a8\u6001\u63a5\u89e6\u4e8b\u4ef6\uff0c\u6a21\u62df\u4eba\u7c7b\u624b\u6307\u626b\u63cf\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u6df1\u5ea6\u4e0b\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7cbe\u5ea6\uff08\u226598%\uff09\uff0c\u8de8\u591a\u79cd\u76f2\u6587\u677f\u5e03\u5c40\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u5feb\u901f\u626b\u63cf\u4e0b\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u76f2\u6587\u677f\u4e0a\u5355\u8bcd\u7ea7\u51c6\u786e\u7387\u8d85\u8fc790%\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u4e3a\u673a\u5668\u4eba\u76f2\u6587\u9605\u8bfb\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u8f85\u52a9\u6280\u672f\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\u5177\u6709\u66f4\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2601.19155", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19155", "abs": "https://arxiv.org/abs/2601.19155", "authors": ["Qiujun Li", "Zijin Xiao", "Xulin Wang", "Zhidan Ma", "Cheng Yang", "Haifeng Li"], "title": "LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge", "comment": "9 pages, 5 figures, 3 tables", "summary": "Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \\textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\\% in zero-shot settings.", "AI": {"tldr": "LocationAgent\uff1a\u4e00\u79cd\u5206\u5c42\u5b9a\u4f4d\u4ee3\u7406\uff0c\u901a\u8fc7RER\u67b6\u6784\uff08\u63a8\u7406\u5668-\u6267\u884c\u5668-\u8bb0\u5f55\u5668\uff09\u8fdb\u884c\u5206\u5c42\u63a8\u7406\uff0c\u5c06\u5730\u7406\u8bc1\u636e\u9a8c\u8bc1\u5378\u8f7d\u5230\u5916\u90e8\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u5c06\u4f4d\u7f6e\u77e5\u8bc6\u548c\u63a8\u7406\u6a21\u5f0f\u5185\u5316\u4e3a\u9759\u6001\u8bb0\u5fc6\uff0c\u5bb9\u6613\u5728\u5f00\u653e\u4e16\u754c\u6216\u9700\u8981\u52a8\u6001\u77e5\u8bc6\u7684\u573a\u666f\u4e2d\u51fa\u73b0\u4e8b\u5b9e\u5e7b\u89c9\u548c\u6cdb\u5316\u74f6\u9888\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u5206\u5c42\u63a8\u7406\u5e76\u9a8c\u8bc1\u5730\u7406\u8bc1\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLocationAgent\u5206\u5c42\u5b9a\u4f4d\u4ee3\u7406\uff1a1\uff09\u91c7\u7528RER\u67b6\u6784\uff08\u63a8\u7406\u5668-\u6267\u884c\u5668-\u8bb0\u5f55\u5668\uff09\u5b9e\u73b0\u5206\u5c42\u63a8\u7406\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u548c\u4e0a\u4e0b\u6587\u538b\u7f29\u9632\u6b62\u591a\u6b65\u63a8\u7406\u6f02\u79fb\uff1b2\uff09\u6784\u5efa\u7ebf\u7d22\u63a2\u7d22\u5de5\u5177\u5957\u4ef6\u8fdb\u884c\u8bc1\u636e\u9a8c\u8bc1\uff1b3\uff09\u521b\u5efaCCL-Bench\u4e2d\u6587\u57ce\u5e02\u5b9a\u4f4d\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "LocationAgent\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u81f3\u5c1130%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5c42\u63a8\u7406\u903b\u8f91\u4fdd\u7559\u5728\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u5c06\u5730\u7406\u8bc1\u636e\u9a8c\u8bc1\u5378\u8f7d\u5230\u5916\u90e8\u5de5\u5177\uff0cLocationAgent\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5e7b\u89c9\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.19290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19290", "abs": "https://arxiv.org/abs/2601.19290", "authors": ["Yimeng Wang", "Jiaxing Zhao", "Hongbin Xie", "Hexing Ma", "Yuzhen Lei", "Shuangxue Liu", "Xuan Song", "Zichen Zhang", "Haoran Zhang"], "title": "MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning", "comment": null, "summary": "Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.", "AI": {"tldr": "MetaGen\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u89d2\u8272\u7a7a\u95f4\u548c\u534f\u4f5c\u62d3\u6251\u7ed3\u6784\uff0c\u63d0\u9ad8\u4efb\u52a1\u5339\u914d\u5ea6\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u89d2\u8272\u5e93\u548c\u51bb\u7ed3\u7684\u4ea4\u4e92\u62d3\u6251\u7ed3\u6784\uff0c\u8fd9\u79cd\u521a\u6027\u8bbe\u8ba1\u5bfc\u81f4\u4efb\u52a1\u4e0d\u5339\u914d\u3001\u65e0\u6cd5\u53ca\u65f6\u9002\u5e94\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u65b0\u8bc1\u636e\uff0c\u5e76\u4e14\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\u3002", "method": "MetaGen\u5728\u63a8\u7406\u65f6\u65e0\u9700\u66f4\u65b0\u57fa\u7840\u6a21\u578b\u6743\u91cd\uff0c\u901a\u8fc7\u751f\u6210\u548c\u91cd\u5199\u67e5\u8be2\u6761\u4ef6\u5316\u7684\u89d2\u8272\u89c4\u8303\u6765\u7ef4\u62a4\u53ef\u63a7\u7684\u52a8\u6001\u89d2\u8272\u6c60\uff0c\u7136\u540e\u56f4\u7ed5\u6700\u5c0f\u9aa8\u5e72\u7f51\u7edc\u5b9e\u4f8b\u5316\u7ea6\u675f\u6267\u884c\u56fe\u3002\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53cd\u9988\u4fe1\u53f7\u8fed\u4ee3\u66f4\u65b0\u89d2\u8272\u63d0\u793a\u5e76\u8c03\u6574\u7ed3\u6784\u51b3\u7b56\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u548c\u591a\u6b65\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaGen\u76f8\u6bd4\u5f3a\u5927\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u6743\u8861\u65b9\u9762\u90fd\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "MetaGen\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89d2\u8272\u7a7a\u95f4\u548c\u534f\u4f5c\u62d3\u6251\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u521a\u6027\u8bbe\u8ba1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u9002\u5e94\u6027\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2601.18981", "categories": ["cs.LG", "cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18981", "abs": "https://arxiv.org/abs/2601.18981", "authors": ["Ruslan Abdulin", "Mohammad Rasoul Narimani"], "title": "Attention-Enhanced Graph Filtering for False Data Injection Attack Detection and Localization", "comment": null, "summary": "The increasing deployment of Internet-of-Things (IoT)-enabled measurement devices in modern power systems has expanded the cyberattack surface of the grid. As a result, this critical infrastructure is increasingly exposed to cyberattacks, including false data injection attacks (FDIAs) that compromise measurement integrity and threaten reliable system operation. Existing FDIA detection methods primarily exploit spatial correlations and network topology using graph-based learning; however, these approaches often rely on high-dimensional representations and shallow classifiers, limiting their ability to capture local structural dependencies and global contextual relationships. Moreover, naively incorporating Transformer architectures can result in overly deep models that struggle to model localized grid dynamics. This paper proposes a joint FDIA detection and localization framework that integrates auto-regressive moving average (ARMA) graph convolutional filters with an Encoder-Only Transformer architecture. The ARMA-based graph filters provide robust, topology-aware feature extraction and adaptability to abrupt spectral changes, while the Transformer encoder leverages self-attention to capture long-range dependencies among grid elements without sacrificing essential local context. The proposed method is evaluated using real-world load data from the New York Independent System Operator (NYISO) applied to the IEEE 14- and 300-bus systems. Numerical results demonstrate that the proposed model effectively exploits both the state and topology of the power grid, achieving high accuracy in detecting FDIA events and localizing compromised nodes.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408ARMA\u56fe\u5377\u79ef\u6ee4\u6ce2\u5668\u548cEncoder-Only Transformer\u7684\u8054\u5408FDIA\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6846\u67b6\uff0c\u6709\u6548\u5229\u7528\u7535\u7f51\u72b6\u6001\u548c\u62d3\u6251\u4fe1\u606f\uff0c\u5728IEEE\u6d4b\u8bd5\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u653b\u51fb\u68c0\u6d4b\u548c\u8282\u70b9\u5b9a\u4f4d\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u6d4b\u91cf\u8bbe\u5907\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7535\u7f51\u9762\u4e34\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff08FDIAs\uff09\u7684\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fe\u5b66\u4e60\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u7f51\u7edc\u62d3\u6251\uff0c\u4f46\u901a\u5e38\u91c7\u7528\u9ad8\u7ef4\u8868\u793a\u548c\u6d45\u5c42\u5206\u7c7b\u5668\uff0c\u96be\u4ee5\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u4f9d\u8d56\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002\u76f4\u63a5\u4f7f\u7528Transformer\u67b6\u6784\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8fc7\u6df1\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u5c40\u90e8\u7535\u7f51\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u8054\u5408FDIA\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6846\u67b6\uff0c\u6574\u5408\u81ea\u56de\u5f52\u79fb\u52a8\u5e73\u5747\uff08ARMA\uff09\u56fe\u5377\u79ef\u6ee4\u6ce2\u5668\u548cEncoder-Only Transformer\u67b6\u6784\u3002ARMA\u56fe\u6ee4\u6ce2\u5668\u63d0\u4f9b\u9c81\u68d2\u7684\u62d3\u6251\u611f\u77e5\u7279\u5f81\u63d0\u53d6\uff0c\u9002\u5e94\u7a81\u53d1\u9891\u8c31\u53d8\u5316\uff1bTransformer\u7f16\u7801\u5668\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u7535\u7f51\u5143\u7d20\u95f4\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u7559\u5fc5\u8981\u7684\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u4f7f\u7528\u7ebd\u7ea6\u72ec\u7acb\u7cfb\u7edf\u8fd0\u8425\u5546\uff08NYISO\uff09\u7684\u771f\u5b9e\u8d1f\u8377\u6570\u636e\uff0c\u5728IEEE 14-\u548c300-\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u6709\u6548\u5229\u7528\u4e86\u7535\u7f51\u72b6\u6001\u548c\u62d3\u6251\u4fe1\u606f\uff0c\u5728\u68c0\u6d4bFDIA\u4e8b\u4ef6\u548c\u5b9a\u4f4d\u53d7\u635f\u8282\u70b9\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684ARMA\u56fe\u5377\u79ef\u6ee4\u6ce2\u5668\u4e0eTransformer\u7f16\u7801\u5668\u7ed3\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u7684\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u95ee\u9898\uff0c\u65e2\u6355\u6349\u4e86\u5c40\u90e8\u7ed3\u6784\u4f9d\u8d56\uff0c\u53c8\u5efa\u6a21\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u4e3a\u7535\u7f51\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19532", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19532", "abs": "https://arxiv.org/abs/2601.19532", "authors": ["Marthe Ballon", "Andres Algaba", "Brecht Verbeken", "Vincent Ginis"], "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge", "comment": "17 pages, 10 figures, 3 tables", "summary": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.", "AI": {"tldr": "Omni-MATH-2\u662f\u4e00\u4e2a\u7ecf\u8fc7\u4eba\u5de5\u4fee\u8ba2\u7684\u6570\u5b66\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b4181\u4e2a\u7cbe\u786e\u7b54\u6848\u95ee\u9898\u548c247\u4e2a\u975e\u6807\u51c6\u6807\u8bb0\u95ee\u9898\uff0c\u901a\u8fc7\u5ba1\u8ba1\u786e\u4fdd\u53ef\u7f16\u8bd1\u6027\u3001\u53ef\u89e3\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u96c6\u5f15\u8d77\u7684\u566a\u58f0\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6570\u636e\u96c6\u4e0d\u51c6\u786e\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u524a\u5f31\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u3002\u4f5c\u8005\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u66f4\u5e72\u51c0\u3001\u66f4\u53ef\u9760\u7684\u6570\u5b66\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "method": "\u521b\u5efaOmni-MATH-2\u6570\u636e\u96c6\uff0c\u5305\u542b4181\u4e2a\u7cbe\u786e\u7b54\u6848\u95ee\u9898\u548c247\u4e2a\u975e\u6807\u51c6\u6807\u8bb0\u95ee\u9898\u3002\u5bf9\u6bcf\u4e2a\u95ee\u9898\u8fdb\u884c\u4eba\u5de5\u5ba1\u8ba1\uff0c\u786e\u4fddLaTeX\u53ef\u7f16\u8bd1\u6027\u3001\u53ef\u89e3\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5305\u62ec\u6dfb\u52a0\u7f3a\u5931\u56fe\u8868\u6216\u4fe1\u606f\u3001\u6807\u8bb0\u9700\u8981\u8bc1\u660e/\u4f30\u8ba1/\u56fe\u50cf\u7684\u95ee\u9898\u3001\u53bb\u9664\u5197\u4f59\u5185\u5bb9\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bc4\u4f30\u8bc4\u4f30\u8005\u5f15\u8d77\u7684\u566a\u58f0\uff0c\u6bd4\u8f83GPT-5 mini\u548c\u539f\u59cbOmni-Judge\u7684\u8868\u73b0\u3002", "result": "\u6570\u636e\u96c6\u4fee\u8ba2\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u96c6\u5f15\u8d77\u7684\u566a\u58f0\u3002\u8bc4\u4f30\u8005\u6bd4\u8f83\u663e\u793a\uff0c\u5728\u8bc4\u4f30\u8005\u5206\u6b67\u4e2d\uff0cOmni-Judge\u572896.4%\u7684\u60c5\u51b5\u4e0b\u662f\u9519\u8bef\u7684\uff0c\u8868\u660e\u5176\u65e0\u6cd5\u533a\u5206\u6a21\u578b\u80fd\u529b\u5dee\u5f02\u3002\u968f\u7740\u95ee\u9898\u96be\u5ea6\u589e\u52a0\uff0c\u9700\u8981\u66f4\u80dc\u4efb\u7684\u8bc4\u4f30\u8005\u6765\u9632\u6b62\u8bc4\u4f30\u9519\u8bef\u63a9\u76d6\u6a21\u578b\u95f4\u7684\u771f\u5b9e\u5dee\u5f02\u3002\u4e24\u79cd\u8bc4\u4f30\u8005\u90fd\u65e0\u6cd5\u8bc6\u522b\u6807\u8bb0\u95ee\u9898\u5b50\u96c6\u7684\u5f53\u524d\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u6570\u636e\u96c6\u8d28\u91cf\u548c\u8bc4\u4f30\u8005\u53ef\u9760\u6027\u5bf9\u4e8e\u5f00\u53d1\u51c6\u786e\u7684\u6a21\u578b\u6027\u80fd\u57fa\u51c6\u90fd\u81f3\u5173\u91cd\u8981\u3002\u4ec5\u9760\u6539\u8fdb\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u786e\u4fdd\u53ef\u9760\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u540c\u65f6\u5173\u6ce8\u6570\u636e\u96c6\u8d28\u91cf\u548c\u8bc4\u4f30\u8005\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u65f6\u3002"}}
{"id": "2601.19022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19022", "abs": "https://arxiv.org/abs/2601.19022", "authors": ["Antanas Zilinskas", "Robert N. Shorten", "Jakub Marecek"], "title": "EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting", "comment": null, "summary": "Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.", "AI": {"tldr": "EVEREST\uff1a\u57fa\u4e8eTransformer\u7684\u7f55\u89c1\u4e8b\u4ef6\u6982\u7387\u9884\u6d4b\u67b6\u6784\uff0c\u96c6\u6210\u6ce8\u610f\u529b\u74f6\u9888\u3001\u8bc1\u636e\u5934\u3001\u6781\u503c\u5934\u548c\u8f7b\u91cf\u7ea7\u524d\u5146\u5934\uff0c\u5728\u7a7a\u95f4\u5929\u6c14\u6570\u636e\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u957f\u7a0b\u4f9d\u8d56\u548c\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u63d0\u4f9b\u6821\u51c6\u9884\u6d4b\u548c\u5c3e\u90e8\u98ce\u9669\u4f30\u8ba1\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faEVEREST\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u53ef\u5b66\u4e60\u6ce8\u610f\u529b\u74f6\u9888\uff08\u8f6f\u805a\u5408\u65f6\u5e8f\u52a8\u6001\uff09\u3001\u8bc1\u636e\u5934\uff08\u901a\u8fc7Normal-Inverse-Gamma\u5206\u5e03\u4f30\u8ba1\u4e0d\u786e\u5b9a\u5ea6\uff09\u3001\u6781\u503c\u5934\uff08\u4f7f\u7528\u5e7f\u4e49\u5e15\u7d2f\u6258\u5206\u5e03\u5efa\u6a21\u5c3e\u90e8\u98ce\u9669\uff09\u3001\u8f7b\u91cf\u7ea7\u524d\u5146\u5934\uff08\u65e9\u671f\u4e8b\u4ef6\u68c0\u6d4b\uff09\u3002\u901a\u8fc7\u590d\u5408\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\uff0c\u90e8\u7f72\u65f6\u4ec5\u4f7f\u7528\u5355\u4e2a\u5206\u7c7b\u5934", "result": "\u5728\u5341\u5e74\u7a7a\u95f4\u5929\u6c14\u6570\u636e\u4e0a\uff0c\u5bf9C\u7ea7\u8000\u6591\u572824/48/72\u5c0f\u65f6\u9884\u6d4b\u8303\u56f4\u5185\u5206\u522b\u8fbe\u52300.973/0.970/0.966\u7684True Skill Statistic\uff0c\u6a21\u578b\u7d27\u51d1\uff08\u7ea60.81M\u53c2\u6570\uff09\uff0c\u53ef\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u9ad8\u6548\u8bad\u7ec3", "conclusion": "EVEREST\u4e3a\u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u76d1\u63a7\u3001\u5929\u6c14\u548c\u536b\u661f\u8bca\u65ad\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002\u672a\u6765\u9700\u6269\u5c55\u5230\u6d41\u5f0f\u548c\u591a\u6a21\u6001\u9884\u6d4b"}}
{"id": "2601.19451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19451", "abs": "https://arxiv.org/abs/2601.19451", "authors": ["Isha Pandey", "Ashish Mittal", "Vartul Bahuguna", "Ganesh Ramakrishnan"], "title": "Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition", "comment": null, "summary": "Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSMEAR-MoE\uff0c\u4e00\u79cd\u7a33\u5b9a\u7684\u6df7\u5408\u4e13\u5bb6\u6295\u5f71\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8bed\u8a00ASR\u4e2d\u5355\u6295\u5f71\u5668\u96be\u4ee5\u6355\u6349\u591a\u6837\u5316\u58f0\u5b66-\u8bed\u4e49\u6620\u5c04\u7684\u95ee\u9898\uff0c\u5728\u56db\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684WER\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684ASR\u7cfb\u7edf\u5728\u5355\u8bed\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5355\u4e2a\u6295\u5f71\u5668\u96be\u4ee5\u6355\u6349\u591a\u8bed\u8a00ASR\u6240\u9700\u7684\u591a\u6837\u5316\u58f0\u5b66\u5230\u8bed\u4e49\u6620\u5c04\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSMEAR-MoE\uff08\u7a33\u5b9a\u7684\u6df7\u5408\u4e13\u5bb6\u6295\u5f71\u5668\uff09\uff0c\u786e\u4fdd\u6240\u6709\u4e13\u5bb6\u90fd\u80fd\u83b7\u5f97\u5bc6\u96c6\u68af\u5ea6\u6d41\uff0c\u9632\u6b62\u4e13\u5bb6\u5d29\u6e83\uff0c\u540c\u65f6\u652f\u6301\u8de8\u8bed\u8a00\u5171\u4eab\u3002\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5355\u4f53\u3001\u9759\u6001\u591a\u6295\u5f71\u5668\u548c\u52a8\u6001MoE\u8bbe\u8ba1\u3002", "result": "\u5728\u56db\u79cd\u5370\u5ea6\u8bed\u8a00\uff08\u5370\u5730\u8bed\u3001\u9a6c\u62c9\u5730\u8bed\u3001\u6cf0\u7c73\u5c14\u8bed\u3001\u6cf0\u5362\u56fa\u8bed\uff09\u4e0a\uff0cSMEAR-MoE\u76f8\u6bd4\u5355\u6295\u5f71\u5668\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.6%\u7684\u76f8\u5bf9WER\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u8fd0\u884c\u65f6\u6548\u7387\u3002\u4e13\u5bb6\u8def\u7531\u5206\u6790\u663e\u793a\u8bed\u8a00\u76f8\u5173\u7684\u4e13\u5bb6\u5171\u4eab\u6a21\u5f0f\u3002", "conclusion": "\u7a33\u5b9a\u7684\u591a\u4e13\u5bb6\u6295\u5f71\u5668\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u591a\u8bed\u8a00ASR\u7684\u5173\u952e\uff0cSMEAR-MoE\u901a\u8fc7\u9632\u6b62\u4e13\u5bb6\u5d29\u6e83\u548c\u4fc3\u8fdb\u8de8\u8bed\u8a00\u5171\u4eab\uff0c\u5728\u591a\u8bed\u8a00ASR\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.19768", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19768", "abs": "https://arxiv.org/abs/2601.19768", "authors": ["Shir Rozenfeld", "Rahul Pankajakshan", "Itay Zloczower", "Eyal Lenga", "Gilad Gressel", "Yisroel Mirsky"], "title": "GAVEL: Towards rule-based safety through activation monitoring", "comment": "Accepted to ICLR 2026", "summary": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684\u6fc0\u6d3b\u5b89\u5168\u65b0\u8303\u5f0f\uff0c\u5c06LLM\u6fc0\u6d3b\u5efa\u6a21\u4e3a\u53ef\u7ec4\u5408\u7684\u8ba4\u77e5\u5143\u7d20\uff0c\u901a\u8fc7\u8c13\u8bcd\u89c4\u5219\u5b9e\u65f6\u68c0\u6d4b\u8fdd\u89c4\u884c\u4e3a\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u53ef\u5b9a\u5236\u3001\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u76d1\u63a7\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6fc0\u6d3b\u7684\u5b89\u5168\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u4f4e\u3001\u7075\u6d3b\u6027\u5dee\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u3001\u53ef\u5b9a\u5236\u4e14\u900f\u660e\u7684\u65b9\u6cd5\u6765\u68c0\u6d4bLLM\u7684\u6f5c\u5728\u6709\u5bb3\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u89c4\u5219\u5f0f\u6fc0\u6d3b\u5b89\u5168\u6846\u67b6\uff1a1) \u5c06\u6fc0\u6d3b\u5efa\u6a21\u4e3a\u8ba4\u77e5\u5143\u7d20\uff08CEs\uff09\u2014\u2014\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u56e0\u5b50\uff1b2) \u5728CEs\u4e0a\u5b9a\u4e49\u8c13\u8bcd\u89c4\u5219\uff1b3) \u5b9e\u65f6\u68c0\u6d4b\u89c4\u5219\u8fdd\u89c4\uff1b4) \u63d0\u4f9b\u81ea\u52a8\u5316\u89c4\u5219\u521b\u5efa\u5de5\u5177GAVEL\u3002", "result": "\u7ec4\u5408\u5f0f\u89c4\u5219\u6fc0\u6d3b\u5b89\u5168\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u652f\u6301\u9886\u57df\u5b9a\u5236\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u5ba1\u8ba1\u7684AI\u6cbb\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5c06\u5f00\u6e90GAVEL\u6846\u67b6\u548c\u81ea\u52a8\u5316\u89c4\u5219\u521b\u5efa\u5de5\u5177\u3002", "conclusion": "\u57fa\u4e8e\u89c4\u5219\u7684\u6fc0\u6d3b\u5b89\u5168\u8303\u5f0f\u901a\u8fc7\u8ba4\u77e5\u5143\u7d20\u548c\u8c13\u8bcd\u89c4\u5219\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u3001\u53ef\u5b9a\u5236\u3001\u53ef\u89e3\u91ca\u7684LLM\u5b89\u5168\u76d1\u63a7\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2601.19605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19605", "abs": "https://arxiv.org/abs/2601.19605", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "Andr\u00e9 Freitas"], "title": "Decompose-and-Formalise: Recursively Verifiable Natural Language Inference", "comment": null, "summary": "Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $\u03b8$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.", "AI": {"tldr": "\u63d0\u51fa\u5206\u89e3\u4e0e\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u524d\u63d0-\u5047\u8bbe\u5bf9\u5206\u89e3\u4e3a\u8574\u542b\u6811\uff0c\u81ea\u5e95\u5411\u4e0a\u9a8c\u8bc1\u4ee5\u9694\u79bb\u5931\u8d25\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5c40\u90e8\u8bca\u65ad\u5f15\u5bfc\u7684\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u89e3\u91ca\u9a8c\u8bc1\u7387\u5e76\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u7ed3\u5408\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u5b58\u5728\u6311\u6218\uff1a\u957f\u800c\u590d\u6742\u7684\u8f93\u5165\u548c\u591a\u6b65\u63a8\u7406\u653e\u5927\u4e86\u5f62\u5f0f\u5316\u9519\u8bef\uff0c\u5355\u4e2a\u5c40\u90e8\u4e0d\u5339\u914d\u5c31\u53ef\u80fd\u5bfc\u81f4\u8bc1\u660e\u65e0\u6548\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u6602\u8d35\u7684\u5168\u5c40\u91cd\u65b0\u751f\u6210\u6765\u5904\u7406\u5931\u8d25\uff0c\u96be\u4ee5\u4ece\u8bc1\u660e\u5668\u8bca\u65ad\u4e2d\u5b9a\u4f4d\u8d23\u4efb\u8303\u56f4\u6216\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u5206\u89e3\u4e0e\u5f62\u5f0f\u5316\u6846\u67b6\uff1a(1) \u5c06\u524d\u63d0-\u5047\u8bbe\u5bf9\u5206\u89e3\u4e3a\u539f\u5b50\u6b65\u9aa4\u7684\u8574\u542b\u6811\uff1b(2) \u81ea\u5e95\u5411\u4e0a\u9a8c\u8bc1\u6811\u4ee5\u5c06\u5931\u8d25\u9694\u79bb\u5230\u7279\u5b9a\u8282\u70b9\uff1b(3) \u6267\u884c\u5c40\u90e8\u8bca\u65ad\u5f15\u5bfc\u7684\u7ec6\u5316\u800c\u975e\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u5f15\u5165\u57fa\u4e8e\u4e8b\u4ef6\u7684\u903b\u8f91\u5f62\u5f0f\u4e2d\u7684\u03b8-\u66ff\u6362\uff0c\u4ee5\u5f3a\u5236\u6267\u884c\u4e00\u81f4\u7684\u8bba\u5143\u89d2\u8272\u7ed1\u5b9a\uff0c\u63d0\u9ad8\u5f62\u5f0f\u5316\u7684\u5fe0\u5b9e\u5ea6\u3002", "result": "\u5728\u4e94\u4e2aLLM\u9aa8\u5e72\u4e0a\u8fdb\u884c\u7684\u4e00\u7cfb\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u89e3\u91ca\u9a8c\u8bc1\u7387\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e8626.2%\u300121.7%\u300121.6%\u548c48.9%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7ec6\u5316\u8fed\u4ee3\u6b21\u6570\u548c\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u4fdd\u6301\u4e86\u5f3a\u5927\u7684NLI\u51c6\u786e\u6027\u3002", "conclusion": "\u5206\u89e3\u4e0e\u5f62\u5f0f\u5316\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u3001\u5c40\u90e8\u5316\u5931\u8d25\u548c\u8bca\u65ad\u5f15\u5bfc\u7684\u7ec6\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u5f62\u5f0f\u5316\u9519\u8bef\u548c\u5931\u8d25\u5904\u7406\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2601.19657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19657", "abs": "https://arxiv.org/abs/2601.19657", "authors": ["Zihou Zhang", "Zheyong Xie", "Li Zhong", "Haifeng Liu", "Shaosheng Cao"], "title": "One Token Is Enough: Improving Diffusion Language Models with a Sink Token", "comment": null, "summary": "Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u4e00\u4e2a\u989d\u5916\u7684sink token\u6765\u89e3\u51b3moving sink\u73b0\u8c61\uff0c\u901a\u8fc7\u4fee\u6539\u6ce8\u610f\u529b\u63a9\u7801\u4f7f\u8be5token\u4ec5\u5173\u6ce8\u81ea\u8eab\u4f46\u5bf9\u5176\u4ed6token\u5168\u5c40\u53ef\u89c1\uff0c\u4ece\u800c\u7a33\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5e76\u884c\u6587\u672c\u751f\u6210\u7684\u4f18\u70b9\uff0c\u4f46\u5b58\u5728moving sink\u73b0\u8c61\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002sink token\u5728Transformer\u503c\u7a7a\u95f4\u4e2d\u5177\u6709\u4f4e\u8303\u6570\u8868\u793a\uff0cmoving sink\u73b0\u8c61\u662f\u9632\u6b62\u4fe1\u606f\u8fc7\u5ea6\u6df7\u5408\u7684\u4fdd\u62a4\u673a\u5236\uff0c\u4f46\u5176\u5728\u6269\u6563\u6b65\u9aa4\u4e2d\u4e0d\u53ef\u9884\u6d4b\u7684\u4f4d\u7f6e\u4f1a\u635f\u5bb3\u63a8\u7406\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u989d\u5916sink token\uff0c\u901a\u8fc7\u4fee\u6539\u6ce8\u610f\u529b\u63a9\u7801\u5b9e\u73b0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e00\u4e2a\u7279\u6b8atoken\uff0c\u8be5token\u4ec5\u80fd\u5173\u6ce8\u81ea\u8eab\uff0c\u4f46\u5bf9\u6240\u6709\u5176\u4ed6token\u5168\u5c40\u53ef\u89c1\u3002\u8fd9\u79cd\u65b9\u6cd5\u7a33\u5b9a\u4e86\u6ce8\u610f\u529bsink\uff0c\u6539\u5584\u4e86\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165\u5355\u4e2a\u989d\u5916token\u80fd\u7a33\u5b9a\u6ce8\u610f\u529bsink\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8bc1\u5b9e\u8be5token\u7684\u6709\u6548\u6027\u4e0e\u5176\u4f4d\u7f6e\u65e0\u5173\uff0c\u4e14\u8bed\u4e49\u5185\u5bb9\u53ef\u5ffd\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u7a33\u5065\u4e13\u7528\u7ed3\u6784sink\u7684\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u989d\u5916sink token\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u7684moving sink\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19433", "abs": "https://arxiv.org/abs/2601.19433", "authors": ["Jisheng Chu", "Wenrui Li", "Rui Zhao", "Wangmeng Zuo", "Shifeng Chen", "Xiaopeng Fan"], "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming", "comment": null, "summary": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.", "AI": {"tldr": "RoamScene3D\uff1a\u57fa\u4e8e\u8bed\u4e49\u63a8\u7406\u548c\u51e0\u4f55\u7ea6\u675f\u7684\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u56fe\u5f15\u5bfc\u81ea\u9002\u5e94\u6f2b\u6e38\u8f68\u8ff9\u548c\u8fd0\u52a8\u6ce8\u5165\u4fee\u590d\u6a21\u578b\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u95f4\u76f2\u533a\u548c\u9759\u60012D\u5148\u9a8c\u9650\u5236", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u76f2\u533a\uff0c\u4f9d\u8d56\u9884\u5b9a\u4e49\u8f68\u8ff9\u65e0\u6cd5\u5229\u7528\u663e\u8457\u5bf9\u8c61\u95f4\u5185\u5728\u5173\u7cfb\uff0c\u96be\u4ee5\u7406\u89e3\u8bed\u4e49\u5e03\u5c40\u548c\u63a8\u65ad\u906e\u6321\u5185\u5bb9\uff1b\u540c\u65f6\u5f53\u524d\u4fee\u590d\u6a21\u578b\u57282D\u56fe\u50cf\u7a7a\u95f4\u64cd\u4f5c\uff0c\u96be\u4ee5\u5408\u7406\u586b\u5145\u76f8\u673a\u8fd0\u52a8\u9020\u6210\u7684\u7a7a\u6d1e", "method": "1\uff09\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7f16\u7801\u5bf9\u8c61\u5173\u7cfb\u7684\u573a\u666f\u56fe\uff0c\u5f15\u5bfc\u76f8\u673a\u611f\u77e5\u663e\u8457\u5bf9\u8c61\u8fb9\u754c\u5e76\u89c4\u5212\u81ea\u9002\u5e94\u6f2b\u6e38\u8f68\u8ff9\uff1b2\uff09\u63d0\u51fa\u8fd0\u52a8\u6ce8\u5165\u4fee\u590d\u6a21\u578b\uff0c\u5728\u5408\u6210\u5168\u666f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u6574\u5408\u771f\u5b9e\u76f8\u673a\u8f68\u8ff9\u4f7f\u5176\u9002\u5e94\u76f8\u673a\u8fd0\u52a8", "result": "\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u548c\u51e0\u4f55\u7ea6\u675f\uff0cRoamScene3D\u5728\u751f\u6210\u4e00\u81f4\u4e14\u903c\u771f\u76843D\u573a\u666f\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "RoamScene3D\u6210\u529f\u5f25\u5408\u4e86\u8bed\u4e49\u5f15\u5bfc\u4e0e\u7a7a\u95f4\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u8bed\u4e49\u5173\u7cfb\u63a8\u7406\u548c\u81ea\u9002\u5e94\u76f8\u673a\u8f68\u8ff9\u89c4\u5212\uff0c\u80fd\u591f\u751f\u6210\u4e00\u81f4\u4e14\u903c\u771f\u76843D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2601.19488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19488", "abs": "https://arxiv.org/abs/2601.19488", "authors": ["Yizhao Han", "Tianxing Shi", "Zhao Wang", "Zifan Xu", "Zhiyuan Pu", "Mingxiao Li", "Qian Zhang", "Wei Yin", "Xiao-Xiao Long"], "title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation", "comment": null, "summary": "Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.", "AI": {"tldr": "\u63d0\u51faENkG\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u5019\u9009token\u6570\u91cf\u6765\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4e2d\u9759\u6001top-k/top-p\u91c7\u6837\u7b56\u7565\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u957f\u5e8f\u5217\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891token\u4e0e\u8bed\u8a00token\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff1a\u89c6\u9891token\u8bed\u4e49\u5bc6\u5ea6\u4f4e\u3001\u65f6\u7a7a\u5197\u4f59\u5ea6\u9ad8\uff0c\u5bfc\u81f4\u9759\u6001top-k/top-p\u91c7\u6837\u7b56\u7565\u5728\u89c6\u9891\u751f\u6210\u4e2d\u6548\u679c\u4e0d\u4f73\u3002\u4f4e\u4e0d\u786e\u5b9a\u6027\u533a\u57df\uff08\u9759\u6001\u80cc\u666f\uff09\u4f1a\u5f15\u5165\u4e0d\u5fc5\u8981\u968f\u673a\u6027\uff0c\u9ad8\u4e0d\u786e\u5b9a\u6027\u533a\u57df\uff08\u524d\u666f\u7269\u4f53\uff09\u5219\u5bb9\u6613\u9677\u5165\u65e9\u671f\u9519\u8bef\u7d2f\u79ef\uff0c\u6700\u7ec8\u4e25\u91cd\u964d\u4f4e\u957f\u5e8f\u5217\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u71b5\u5f15\u5bfc\u7684k-Guard\uff08ENkG\uff09\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636e\u6bcf\u4e2atoken\u9884\u6d4b\u5206\u5e03\u7684\u71b5\u503c\uff08\u91cf\u5316token\u5206\u6563\u7a0b\u5ea6\uff09\u81ea\u9002\u5e94\u8c03\u6574\u5019\u9009token\u6570\u91cf\u3002\u4f4e\u71b5\u533a\u57df\u4f7f\u7528\u8f83\u5c11\u5019\u9009\u4ee5\u6291\u5236\u5197\u4f59\u566a\u58f0\u5e76\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u9ad8\u71b5\u533a\u57df\u4f7f\u7528\u66f4\u591a\u5019\u9009\u4ee5\u51cf\u8f7b\u9519\u8bef\u7d2f\u79ef\u3002\u8be5\u65b9\u6cd5\u6a21\u578b\u65e0\u5173\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u9759\u6001top-k/top-p\u7b56\u7565\uff0cENkG\u5728\u611f\u77e5\u8d28\u91cf\u548c\u7ed3\u6784\u7a33\u5b9a\u6027\u65b9\u9762\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "ENkG\u91c7\u6837\u7b56\u7565\u901a\u8fc7\u81ea\u9002\u5e94\u5019\u9009token\u6570\u91cf\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u9759\u6001\u91c7\u6837\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u56de\u5f52\u89c6\u9891\u89e3\u7801\u5668\u63d0\u4f9b\u4e86\u66f4\u5408\u9002\u7684\u91c7\u6837\u65b9\u6cd5\u3002"}}
{"id": "2601.19189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19189", "abs": "https://arxiv.org/abs/2601.19189", "authors": ["Benjamin Turtel", "Paul Wilczewski", "Danny Franklin", "Kris Skotheim"], "title": "Foresight Learning for SEC Risk Prediction", "comment": null, "summary": "Risk disclosures in SEC filings describe potential adverse events but rarely quantify their likelihood, limiting their usefulness for probabilistic analysis. A central obstacle is the absence of large-scale, risk-level supervision linking disclosed risks to realized outcomes.\n  We introduce a fully automated data generation pipeline that converts qualitative SEC risk disclosures into temporally grounded supervision using only public data. For each filing, the pipeline generates firm-specific, time-bounded risk queries from the Risk Factors section and labels them by automatically resolving outcomes against subsequent disclosures.\n  Using this dataset of risk queries and outcomes grounded in SEC filings, we train a compact large language model to estimate the probability that a disclosed risk will materialize within a specified horizon. Despite its modest size, the resulting model substantially improves over pretrained and heuristic baselines, and outperforms frontier general-purpose models, including GPT-5, on probabilistic accuracy and calibration.\n  More broadly, this work demonstrates that Foresight Learning enables scalable and fully automated training of domain-specific expert models using only raw, chronological, in-domain text -- without proprietary data, external corpora, or manual annotation. The resulting models achieve frontier-level performance while remaining deployable on a single GPU. This result suggests a general pathway for learning calibrated, decision-relevant signals from naturally occurring enterprise documents.\n  To support transparency and reproducibility, we open-source the evaluation dataset used in this study.\n  Evaluation Data: https://huggingface.co/datasets/LightningRodLabs/sec_risk_questions_test_set\n  Data Generation Platform: https://lightningrod.ai/\n  SDK: https://github.com/lightning-rod-labs/lightningrod-python-sdk", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5c06SEC\u98ce\u9669\u62ab\u9732\u8f6c\u5316\u4e3a\u5e26\u65f6\u95f4\u6807\u7b7e\u7684\u76d1\u7763\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684LLM\u6765\u9884\u6d4b\u98ce\u9669\u5b9e\u73b0\u6982\u7387\uff0c\u6027\u80fd\u4f18\u4e8eGPT-5\u7b49\u524d\u6cbf\u6a21\u578b\u3002", "motivation": "SEC\u6587\u4ef6\u4e2d\u7684\u98ce\u9669\u62ab\u9732\u901a\u5e38\u53ea\u63cf\u8ff0\u6f5c\u5728\u4e0d\u5229\u4e8b\u4ef6\u800c\u4e0d\u91cf\u5316\u5176\u53ef\u80fd\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u6982\u7387\u5206\u6790\u7684\u6709\u7528\u6027\u3002\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u98ce\u9669\u7ea7\u522b\u7684\u76d1\u7763\u6570\u636e\u6765\u5c06\u62ab\u9732\u7684\u98ce\u9669\u4e0e\u5b9e\u9645\u7ed3\u679c\u8054\u7cfb\u8d77\u6765\u3002", "method": "1. \u5f00\u53d1\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5b9a\u6027SEC\u98ce\u9669\u62ab\u9732\u8f6c\u5316\u4e3a\u65f6\u95f4\u57fa\u7840\u76d1\u7763\u6570\u636e\uff1b2. \u4ece\u98ce\u9669\u56e0\u7d20\u90e8\u5206\u751f\u6210\u516c\u53f8\u7279\u5b9a\u3001\u6709\u65f6\u95f4\u9650\u5236\u7684\u98ce\u9669\u67e5\u8be2\uff1b3. \u901a\u8fc7\u81ea\u52a8\u89e3\u6790\u540e\u7eed\u62ab\u9732\u7ed3\u679c\u6765\u6807\u6ce8\u67e5\u8be2\uff1b4. \u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7d27\u51d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u4f30\u8ba1\u98ce\u9669\u5728\u7279\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u5b9e\u73b0\u7684\u6982\u7387\u3002", "result": "\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u9002\u4e2d\uff0c\u4f46\u5728\u6982\u7387\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u548c\u542f\u53d1\u5f0f\u57fa\u7ebf\uff0c\u5e76\u4e14\u8d85\u8d8a\u4e86\u5305\u62ecGPT-5\u5728\u5185\u7684\u524d\u6cbf\u901a\u7528\u6a21\u578b\u3002\u6a21\u578b\u53ef\u5728\u5355\u4e2aGPU\u4e0a\u90e8\u7f72\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\"\u524d\u77bb\u5b66\u4e60\"\u80fd\u591f\u4ec5\u4f7f\u7528\u539f\u59cb\u3001\u6309\u65f6\u95f4\u987a\u5e8f\u6392\u5217\u7684\u9886\u57df\u5185\u6587\u672c\uff0c\u65e0\u9700\u4e13\u6709\u6570\u636e\u3001\u5916\u90e8\u8bed\u6599\u5e93\u6216\u624b\u52a8\u6807\u6ce8\uff0c\u5373\u53ef\u5b9e\u73b0\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\u6a21\u578b\u7684\u53ef\u6269\u5c55\u3001\u5168\u81ea\u52a8\u5316\u8bad\u7ec3\u3002\u4e3a\u4ece\u4f01\u4e1a\u6587\u6863\u4e2d\u5b66\u4e60\u6821\u51c6\u7684\u3001\u4e0e\u51b3\u7b56\u76f8\u5173\u7684\u4fe1\u53f7\u63d0\u4f9b\u4e86\u4e00\u6761\u901a\u7528\u8def\u5f84\u3002"}}
{"id": "2601.19526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19526", "abs": "https://arxiv.org/abs/2601.19526", "authors": ["Fouad Boutaleb", "Emery Pierson", "Mohamed Daoudi", "Cl\u00e9mence Nineuil", "Ali Amad", "Fabien D'Hondt"], "title": "A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder", "comment": null, "summary": "Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.\n  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.\n  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.", "AI": {"tldr": "\u63d0\u51fa\u975e\u4fb5\u5165\u5f0f\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u5355\u76eeRGB\u89c6\u9891\u8f6c\u6362\u4e3a\u4e34\u5e8a\u76f8\u51733D\u6b65\u6001\u8fd0\u52a8\u5b66\uff0c\u7528\u4e8e\u68c0\u6d4b\u6291\u90c1\u75c7\u7684\u7cbe\u795e\u8fd0\u52a8\u8fdf\u7f13\uff0c\u51c6\u786e\u7387\u8fbe83.3%", "motivation": "\u76ee\u524d\u6291\u90c1\u75c7\u8bca\u65ad\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u3001\u53ef\u89e3\u91ca\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002\u7cbe\u795e\u8fd0\u52a8\u8fdf\u7f13\u662f\u6291\u90c1\u75c7\u6838\u5fc3\u75c7\u72b6\uff0c\u4f46\u4e34\u5e8a\u8bc4\u4f30\u4e3b\u89c2\u6027\u5f3a\u30023D\u52a8\u4f5c\u6355\u6349\u867d\u5ba2\u89c2\u4f46\u8bbe\u5907\u6602\u8d35\u4e0d\u4fbf\u4e34\u5e8a\u5e38\u89c4\u4f7f\u7528\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u666e\u901a\u6444\u50cf\u5934\u7684\u5ba2\u89c2\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "1) \u63d0\u51fa\u8ba1\u7b97\u6846\u67b6\u5c06\u5355\u76eeRGB\u89c6\u9891\u8f6c\u6362\u4e3a3D\u6b65\u6001\u8fd0\u52a8\u5b66\uff1a\u4f7f\u7528\u91cd\u529b\u89c6\u56fe\u5750\u6807\u548c\u65b0\u9896\u7684\u8f68\u8ff9\u6821\u6b63\u7b97\u6cd5\uff0c\u5229\u7528\u6539\u826f\u7248Timed Up and Go\u534f\u8bae\u95ed\u73af\u62d3\u6251\u7f13\u89e3\u5355\u76ee\u6df1\u5ea6\u8bef\u5dee\uff1b2) \u4ece\u5355\u6444\u50cf\u5934\u63d0\u53d6297\u4e2a\u660e\u786e\u6b65\u6001\u751f\u7269\u529b\u5b66\u6807\u5fd7\u7269\uff1b3) \u9488\u5bf9\u5c0f\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u5f15\u5165\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u8bc6\u522b\u7a33\u5065\u8fd0\u52a8\u7279\u5f81\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728CALYPSO\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff1a1) \u68c0\u6d4b\u7cbe\u795e\u8fd0\u52a8\u8fdf\u7f13\u51c6\u786e\u7387\u8fbe83.3%\uff1b2) \u89e3\u91ca\u603b\u4f53\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea664%\u7684\u65b9\u5dee(R\u00b2=0.64)\uff1b3) \u53d1\u73b0\u8e1d\u5173\u8282\u63a8\u8fdb\u529b\u51cf\u5c11\u548c\u9aa8\u76c6\u6d3b\u52a8\u53d7\u9650\u4e0e\u6291\u90c1\u8fd0\u52a8\u8868\u578b\u5f3a\u76f8\u5173\uff1b4) \u8bc1\u660e\u8eab\u4f53\u8fd0\u52a8\u53ef\u4f5c\u4e3a\u8ba4\u77e5\u72b6\u6001\u7684\u7a33\u5065\u4ee3\u7406\u6307\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6291\u90c1\u75c7\u5ba2\u89c2\u76d1\u6d4b\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u8bc1\u660e\u7269\u7406\u8fd0\u52a8\u80fd\u6709\u6548\u53cd\u6620\u8ba4\u77e5\u72b6\u6001\uff0c\u53ef\u5728\u6807\u51c6\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u6291\u90c1\u75c7\u7684\u5ba2\u89c2\u8bc4\u4f30\u548c\u76d1\u6d4b\u3002"}}
{"id": "2601.19285", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19285", "abs": "https://arxiv.org/abs/2601.19285", "authors": ["Xinyu Zhou", "Jiawei Zhang", "Stephen J. Wright"], "title": "Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework", "comment": "61pages,32 figures", "summary": "Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5b58\u5728\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u751f\u6210\u6837\u672c\u53ef\u80fd\u5b8c\u5168\u590d\u5236\u8bad\u7ec3\u6837\u672c\u3002\u672c\u6587\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u6b64\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff08\u566a\u58f0\u65e0\u6761\u4ef6\u5316\u548c\u6e29\u5ea6\u5e73\u6ed1\uff09\u6765\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u4f18\u5f02\uff0c\u4f46\u9762\u4e34\u8bb0\u5fc6\u5316\u6311\u6218\uff0c\u5373\u751f\u6210\u6837\u672c\u53ef\u80fd\u5b8c\u5168\u590d\u5236\u8bad\u7ec3\u6837\u672c\u3002\u9700\u8981\u7406\u8bba\u89e3\u91ca\u6b64\u73b0\u8c61\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u7406\u8bba\u6846\u67b6\uff1a\u8bc1\u660e\u7ecf\u9a8c\u5f97\u5206\u51fd\u6570\u662f\u9ad8\u65af\u5206\u5e03\u5f97\u5206\u51fd\u6570\u7684\u52a0\u6743\u548c\uff0c\u6743\u91cd\u4e3a\u5c16\u9510\u7684softmax\u51fd\u6570\uff0c\u5bfc\u81f4\u5355\u4e2a\u8bad\u7ec3\u6837\u672c\u4e3b\u5bfc\u5f97\u5206\u51fd\u6570\uff1b2. \u566a\u58f0\u65e0\u6761\u4ef6\u5316\uff1a\u8ba9\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u81ea\u9002\u5e94\u786e\u5b9a\u5176\u5f97\u5206\u51fd\u6570\u6743\u91cd\uff0c\u589e\u52a0\u66f4\u591a\u8bad\u7ec3\u6837\u672c\u7684\u5f71\u54cd\uff1b3. \u6e29\u5ea6\u5e73\u6ed1\uff1a\u5f15\u5165\u6e29\u5ea6\u53c2\u6570\u63a7\u5236softmax\u6743\u91cd\u7684\u5e73\u6ed1\u5ea6\uff0c\u51cf\u5c11\u5355\u4e2a\u6837\u672c\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002", "result": "\u7406\u8bba\u5206\u6790\u89e3\u91ca\u4e86\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u5e73\u6ed1\u8fd1\u4f3c\u52a0\u6743\u548c\u6765\u6539\u5584\u6cdb\u5316\u7684\u673a\u5236\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u8bc1\u660e\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u80fd\u6709\u6548\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\u6e90\u4e8e\u7ecf\u9a8c\u5f97\u5206\u51fd\u6570\u7684\u5c16\u9510\u52a0\u6743\u7ed3\u6784\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5e73\u6ed1\u8fd1\u4f3c\u7684\u6cdb\u5316\u673a\u5236\uff0c\u63d0\u51fa\u7684\u566a\u58f0\u65e0\u6761\u4ef6\u5316\u548c\u6e29\u5ea6\u5e73\u6ed1\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.19296", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19296", "abs": "https://arxiv.org/abs/2601.19296", "authors": ["Yongjae Lee", "Eunhee Park", "Daesan Park", "Dongho Kim", "Jongho Choi", "Hyerim Bae"], "title": "Process-Aware Procurement Lead Time Prediction for Shipyard Delay Mitigation", "comment": null, "summary": "Accurately predicting procurement lead time (PLT) remains a challenge in engineered-to-order industries such as shipbuilding and plant construction, where delays in a single key component can disrupt project timelines. In shipyards, pipe spools are critical components; installed deep within hull blocks soon after steel erection, any delay in their procurement can halt all downstream tasks. Recognizing their importance, existing studies predict PLT using the static physical attributes of pipe spools. However, procurement is inherently a dynamic, multi-stakeholder business process involving a continuous sequence of internal and external events at the shipyard, factors often overlooked in traditional approaches. To address this issue, this paper proposes a novel framework that combines event logs, dataset records of the procurement events, with static attributes to predict PLT. The temporal attributes of each event are extracted to reflect the continuity and temporal context of the process. Subsequently, a deep sequential neural network combined with a multi-layered perceptron is employed to integrate these static and dynamic features, enabling the model to capture both structural and contextual information in procurement. Comparative experiments are conducted using real-world pipe spool procurement data from a globally renowned South Korean shipbuilding corporation. Three tasks are evaluated, which are production, post-processing, and procurement lead time prediction. The results show a 22.6% to 50.4% improvement in prediction performance in terms of mean absolute error over the best-performing existing approaches across the three tasks. These findings indicate the value of considering procurement process information for more accurate PLT prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e8b\u4ef6\u65e5\u5fd7\u548c\u9759\u6001\u5c5e\u6027\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u9020\u8239\u4e1a\u4e2d\u7ba1\u6bb5\u91c7\u8d2d\u7684\u63d0\u524d\u671f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728MAE\u6307\u6807\u4e0a\u63d0\u5347\u4e8622.6%\u523050.4%\u3002", "motivation": "\u5728\u9020\u8239\u7b49\u6309\u8ba2\u5355\u8bbe\u8ba1\u7684\u884c\u4e1a\u4e2d\uff0c\u51c6\u786e\u9884\u6d4b\u91c7\u8d2d\u63d0\u524d\u671f\uff08PLT\uff09\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5355\u4e2a\u5173\u952e\u7ec4\u4ef6\uff08\u5982\u7ba1\u6bb5\uff09\u7684\u5ef6\u8fdf\u4f1a\u7834\u574f\u6574\u4e2a\u9879\u76ee\u65f6\u95f4\u8868\u3002\u73b0\u6709\u7814\u7a76\u4ec5\u4f7f\u7528\u7ba1\u6bb5\u7684\u9759\u6001\u7269\u7406\u5c5e\u6027\u8fdb\u884c\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u91c7\u8d2d\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u6d89\u53ca\u8239\u5382\u5185\u5916\u8fde\u7eed\u4e8b\u4ef6\u7684\u591a\u65b9\u52a8\u6001\u4e1a\u52a1\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u65e5\u5fd7\uff08\u91c7\u8d2d\u4e8b\u4ef6\u7684\u8bb0\u5f55\uff09\u4e0e\u9759\u6001\u5c5e\u6027\u76f8\u7ed3\u5408\u3002\u63d0\u53d6\u6bcf\u4e2a\u4e8b\u4ef6\u7684\u65f6\u95f4\u5c5e\u6027\u4ee5\u53cd\u6620\u8fc7\u7a0b\u7684\u8fde\u7eed\u6027\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u7136\u540e\u91c7\u7528\u6df1\u5ea6\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u4e0e\u591a\u5c42\u611f\u77e5\u673a\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6574\u5408\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u91c7\u8d2d\u4e2d\u7684\u7ed3\u6784\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u4f7f\u7528\u97e9\u56fd\u77e5\u540d\u9020\u8239\u516c\u53f8\u7684\u771f\u5b9e\u7ba1\u6bb5\u91c7\u8d2d\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u751f\u4ea7\u3001\u540e\u5904\u7406\u548c\u91c7\u8d2d\u63d0\u524d\u671f\u9884\u6d4b\u4e09\u4e2a\u4efb\u52a1\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u65b9\u9762\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u4e8622.6%\u523050.4%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8003\u8651\u91c7\u8d2d\u8fc7\u7a0b\u4fe1\u606f\u5bf9\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u91c7\u8d2d\u63d0\u524d\u671f\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u5c06\u4e8b\u4ef6\u65e5\u5fd7\u4e0e\u9759\u6001\u5c5e\u6027\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u6309\u8ba2\u5355\u8bbe\u8ba1\u884c\u4e1a\u7684\u4f9b\u5e94\u94fe\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.19895", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19895", "abs": "https://arxiv.org/abs/2601.19895", "authors": ["Chen Chen", "Lai Wei"], "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep", "comment": null, "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.", "AI": {"tldr": "Keel\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Highway-style\u8fde\u63a5\u66ff\u4ee3\u4f20\u7edf\u6b8b\u5dee\u8fde\u63a5\u7684Post-LN Transformer\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc71000\u5c42\u7684\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524dLLM\u6269\u5c55\u9762\u4e34\u74f6\u9888\uff1a\u5bbd\u5ea6\u6269\u5c55\u6536\u76ca\u9012\u51cf\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u65e0\u6cd5\u63d0\u5347\u57fa\u7840\u8868\u8fbe\u80fd\u529b\uff0c\u800c\u6df1\u5ea6\u6269\u5c55\u7406\u8bba\u4e0a\u5177\u6709\u66f4\u597d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u73b0\u6709Transformer\u67b6\u6784\u5728\u6781\u7aef\u6df1\u5ea6\u4e0b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002Post-LN\u56e0\u4e0d\u7a33\u5b9a\u88abPre-LN\u53d6\u4ee3\uff0c\u4f46\u5176\u5931\u8d25\u6a21\u5f0f\u6e90\u4e8eResNet-style\u6b8b\u5dee\u8def\u5f84\u5bfc\u81f4\u7684\u68af\u5ea6\u6d88\u5931\u3002", "method": "\u63d0\u51faKeel\u67b6\u6784\uff0c\u5728Post-LN Transformer\u4e2d\u7528Highway-style\u8fde\u63a5\u66ff\u4ee3\u4f20\u7edf\u7684\u6b8b\u5dee\u8fde\u63a5\uff0c\u4fdd\u6301\u6b8b\u5dee\u5206\u652f\u7684\u68af\u5ea6\u6d41\u52a8\uff0c\u9632\u6b62\u4fe1\u53f7\u4ece\u9876\u5c42\u5230\u5e95\u5c42\u7684\u6d88\u5931\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u7279\u6b8a\u521d\u59cb\u5316\u6216\u590d\u6742\u4f18\u5316\u6280\u5de7\u3002", "result": "Keel\u80fd\u591f\u5728\u8d85\u8fc71000\u5c42\u7684\u6781\u7aef\u6df1\u5ea6\u4e0b\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5728\u56f0\u60d1\u5ea6\u548c\u6df1\u5ea6\u6269\u5c55\u7279\u6027\u4e0a\u6301\u7eed\u4f18\u4e8ePre-LN\uff0c\u5c55\u793a\u4e86Post-LN\u4e0eHighway-style\u8fde\u63a5\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "Post-LN\u4e0eHighway-style\u8fde\u63a5\u7ed3\u5408\u4e3a\u6784\u5efa\u6df1\u5ea6\u53ef\u6269\u5c55\u7684LLM\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u65e0\u9650\u6df1\u5ea6\u67b6\u6784\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.19659", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19659", "abs": "https://arxiv.org/abs/2601.19659", "authors": ["Mao-Lin Luo", "Zi-Hao Zhou", "Yi-Lin Zhang", "Yuanyu Wan", "Tong Wei", "Min-Ling Zhang"], "title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation", "comment": "Accepted at ICLR 2026", "summary": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.", "AI": {"tldr": "KeepLoRA\uff1a\u4e00\u79cd\u7528\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LoRA\u53c2\u6570\u66f4\u65b0\u9650\u5236\u5728\u6b8b\u5dee\u5b50\u7a7a\u95f4\u6765\u5e73\u8861\u9884\u8bad\u7ec3\u77e5\u8bc6\u4fdd\u7559\u3001\u5df2\u5b66\u4efb\u52a1\u77e5\u8bc6\u4fdd\u6301\u548c\u65b0\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u9700\u8981\u5e73\u8861\u4e09\u4e2a\u76f8\u4e92\u7ade\u4e89\u7684\u76ee\u6807\uff1a\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u3001\u4fdd\u6301\u5df2\u5b66\u4efb\u52a1\u5e8f\u5217\u7684\u77e5\u8bc6\u3001\u4ee5\u53ca\u7ef4\u6301\u83b7\u53d6\u65b0\u77e5\u8bc6\u7684\u53ef\u5851\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e73\u8861\u8fd9\u4e9b\u76ee\u6807\u3002", "method": "\u9996\u5148\u5206\u6790\u6a21\u578b\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\uff0c\u53d1\u73b0\u901a\u7528\u77e5\u8bc6\u4e3b\u8981\u7f16\u7801\u5728\u4e3b\u6210\u5206\u5b50\u7a7a\u95f4\uff0c\u800c\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u7f16\u7801\u5728\u6b8b\u5dee\u5b50\u7a7a\u95f4\u3002\u57fa\u4e8e\u6b64\uff0cKeepLoRA\u901a\u8fc7\u5c06\u65b0\u4efb\u52a1\u7684\u68af\u5ea6\u6295\u5f71\u5230\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e3b\u6210\u5206\u5b50\u7a7a\u95f4\u548c\u5148\u524d\u4efb\u52a1\u7279\u5f81\u4e3b\u5bfc\u65b9\u5411\u6b63\u4ea4\u7684\u5b50\u7a7a\u95f4\uff0c\u5c06LoRA\u53c2\u6570\u66f4\u65b0\u9650\u5236\u5728\u6b8b\u5dee\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u5e72\u6270\u5148\u524d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u8bc1\u5b9eKeepLoRA\u80fd\u6709\u6548\u5e73\u8861\u4e09\u4e2a\u76ee\u6807\uff0c\u5e76\u5728\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "KeepLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u5206\u6790\u6307\u5bfc\u7684\u68af\u5ea6\u6295\u5f71\u7b56\u7565\uff0c\u5728\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u77e5\u8bc6\u4fdd\u7559\u3001\u4efb\u52a1\u4fdd\u6301\u548c\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.19336", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19336", "abs": "https://arxiv.org/abs/2601.19336", "authors": ["Zhao-Han Peng", "Shaohui Li", "Zhi Li", "Shulan Ruan", "Yu Liu", "You He"], "title": "From Observations to Events: Event-Aware World Model for Reinforcement Learning", "comment": "43 pages, accepted by ICLR 2026", "summary": "While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.", "AI": {"tldr": "EAWM\u63d0\u51fa\u4e8b\u4ef6\u611f\u77e5\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4e8b\u4ef6\u751f\u6210\u548c\u5206\u5272\u5b66\u4e60\u4e8b\u4ef6\u611f\u77e5\u8868\u793a\uff0c\u63d0\u5347\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7ed3\u6784\u76f8\u4f3c\u573a\u666f\u95f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u53d7\u5230\u7eb9\u7406\u3001\u989c\u8272\u7b49\u865a\u5047\u53d8\u5316\u7684\u5f71\u54cd\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u4eba\u7c7b\u5c06\u8fde\u7eed\u611f\u5b98\u6d41\u5206\u5272\u4e3a\u79bb\u6563\u4e8b\u4ef6\u5e76\u4f9d\u8d56\u5173\u952e\u4e8b\u4ef6\u8fdb\u884c\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u611f\u77e5\u4e16\u754c\u6a21\u578b(EAWM)\u6846\u67b6\uff0c\u5305\u542b\u81ea\u52a8\u4e8b\u4ef6\u751f\u6210\u5668\u4ece\u539f\u59cb\u89c2\u5bdf\u4e2d\u63a8\u5bfc\u4e8b\u4ef6\uff0c\u5f15\u5165\u901a\u7528\u4e8b\u4ef6\u5206\u5272\u5668\u8bc6\u522b\u4e8b\u4ef6\u8fb9\u754c\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9884\u6d4b\u5851\u9020\u8868\u793a\u7a7a\u95f4\u4ee5\u6355\u6349\u6709\u610f\u4e49\u7684\u65f6\u7a7a\u8f6c\u6362\u3002", "result": "\u5728Atari 100K\u3001Craftax 1M\u3001DeepMind Control 500K\u548cDMC-GB2 500K\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEAWM\u5c06\u5f3aMBRL\u57fa\u7ebf\u7684\u6027\u80fd\u63d0\u534710%-45%\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "EAWM\u901a\u8fc7\u4e8b\u4ef6\u611f\u77e5\u8868\u793a\u5b66\u4e60\u6709\u6548\u63d0\u5347\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u4e3a\u4e0d\u540c\u4e16\u754c\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2601.19717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19717", "abs": "https://arxiv.org/abs/2601.19717", "authors": ["Yitong Yang", "Xuexin Liu", "Yinglin Wang", "Jing Wang", "Hao Dou", "Changshuo Wang", "Shuting He"], "title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization", "comment": null, "summary": "3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.", "AI": {"tldr": "DiffStyle3D\uff1a\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u76843D\u9ad8\u65af\u6cfc\u6e85\u98ce\u683c\u8fc1\u79fb\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u4f18\u5316\uff0c\u5f15\u5165\u6ce8\u610f\u529b\u611f\u77e5\u635f\u5931\u548c\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u6cd5\uff0c\u63d0\u5347\u98ce\u683c\u5316\u8d28\u91cf\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u67093D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8eVGG\u548cCLIP\u7684\u65b9\u6cd5\u96be\u4ee5\u5728\u6a21\u578b\u5185\u90e8\u5efa\u6a21\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u800c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u4e00\u81f4\u6027\uff0c\u4f46\u4f9d\u8d56\u53bb\u566a\u65b9\u5411\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u53c8\u8bad\u7ec3\u7a33\u5b9a\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDiffStyle3D\u6846\u67b6\uff1a1\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u4f18\u5316\uff1b2\uff09\u5f15\u5165\u6ce8\u610f\u529b\u611f\u77e5\u635f\u5931\uff0c\u5728\u81ea\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u9f50\u98ce\u683c\u7279\u5f81\uff0c\u540c\u65f6\u901a\u8fc7\u5185\u5bb9\u7279\u5f81\u5bf9\u9f50\u4fdd\u6301\u539f\u59cb\u5185\u5bb9\uff1b3\uff09\u63d0\u51fa\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u6cd5\uff0c\u5c06\u51e0\u4f55\u4fe1\u606f\u878d\u5165\u81ea\u6ce8\u610f\u529b\u5b9e\u73b0\u8de8\u89c6\u89d2\u5bf9\u5e94\u5efa\u6a21\uff1b4\uff09\u6784\u5efa\u51e0\u4f55\u611f\u77e5\u63a9\u7801\uff0c\u9632\u6b62\u91cd\u53e0\u533a\u57df\u5197\u4f59\u4f18\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDiffStyle3D\u5728\u98ce\u683c\u5316\u8d28\u91cf\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u66f4\u597d\u7684\u98ce\u683c\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "DiffStyle3D\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u4f18\u5316\u3001\u6ce8\u610f\u529b\u611f\u77e5\u635f\u5931\u548c\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u67093D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a3D\u9ad8\u65af\u6cfc\u6e85\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19479", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19479", "abs": "https://arxiv.org/abs/2601.19479", "authors": ["Victoria Catterall", "Cise Midoglu", "Stephen Lynch"], "title": "Time-to-Injury Forecasting in Elite Female Football: A DeepHit Survival Approach", "comment": null, "summary": "Injury occurrence in football poses significant challenges for athletes and teams, carrying personal, competitive, and financial consequences. While machine learning has been applied to injury prediction before, existing approaches often rely on static pre-season data and binary outcomes, limiting their real-world utility. This study investigates the feasibility of using a DeepHit neural network to forecast time-to-injury from longitudinal athlete monitoring data, while providing interpretable predictions. The analysis utilised the publicly available SoccerMon dataset, containing two seasons of training, match, and wellness records from elite female footballers. Data was pre-processed through cleaning, feature engineering, and the application of three imputation strategies. Baseline models (Random Forest, XGBoost, Logistic Regression) were optimised via grid search for benchmarking, while the DeepHit model, implemented with a multilayer perceptron backbone, was evaluated using chronological and leave-one-player-out (LOPO) validation. DeepHit achieved a concordance index of 0.762, outperforming baseline models and delivering individualised, time-varying risk estimates. Shapley Additive Explanations (SHAP) identified clinically relevant predictors consistent with established risk factors, enhancing interpretability. Overall, this study provides a novel proof of concept: survival modelling with DeepHit shows strong potential to advance injury forecasting in football, offering accurate, explainable, and actionable insights for injury prevention across competitive levels.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528DeepHit\u795e\u7ecf\u7f51\u7edc\u5bf9\u8db3\u7403\u8fd0\u52a8\u5458\u7684\u53d7\u4f24\u65f6\u95f4\u8fdb\u884c\u9884\u6d4b\uff0c\u901a\u8fc7\u751f\u5b58\u5206\u6790\u6a21\u578b\u63d0\u4f9b\u4e2a\u4f53\u5316\u3001\u968f\u65f6\u95f4\u53d8\u5316\u7684\u98ce\u9669\u4f30\u8ba1\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u8db3\u7403\u8fd0\u52a8\u4e2d\u7684\u4f24\u75c5\u95ee\u9898\u5bf9\u8fd0\u52a8\u5458\u548c\u56e2\u961f\u5177\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u591a\u4f9d\u8d56\u9759\u6001\u5b63\u524d\u6570\u636e\u548c\u4e8c\u5143\u7ed3\u679c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u7eb5\u5411\u76d1\u6d4b\u6570\u636e\u3001\u63d0\u4f9b\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u7ed3\u679c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684SoccerMon\u6570\u636e\u96c6\uff08\u5305\u542b\u7cbe\u82f1\u5973\u5b50\u8db3\u7403\u8fd0\u52a8\u5458\u4e24\u4e2a\u8d5b\u5b63\u7684\u8bad\u7ec3\u3001\u6bd4\u8d5b\u548c\u5065\u5eb7\u8bb0\u5f55\uff09\uff0c\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u548c\u4e09\u79cd\u63d2\u8865\u7b56\u7565\u8fdb\u884c\u9884\u5904\u7406\u3002\u5efa\u7acb\u57fa\u7ebf\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001\u903b\u8f91\u56de\u5f52\uff09\u8fdb\u884c\u7f51\u683c\u641c\u7d22\u4f18\u5316\uff0c\u540c\u65f6\u91c7\u7528\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\u7684DeepHit\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u9a8c\u8bc1\u548c\u7559\u4e00\u7403\u5458\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "DeepHit\u6a21\u578b\u83b7\u5f970.762\u7684\u4e00\u81f4\u6027\u6307\u6570\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u4f9b\u4e2a\u4f53\u5316\u3001\u968f\u65f6\u95f4\u53d8\u5316\u7684\u98ce\u9669\u4f30\u8ba1\u3002\u901a\u8fc7SHAP\u65b9\u6cd5\u8bc6\u522b\u51fa\u4e0e\u4e34\u5e8a\u76f8\u5173\u7684\u9884\u6d4b\u56e0\u5b50\uff0c\u8fd9\u4e9b\u56e0\u5b50\u4e0e\u5df2\u77e5\u98ce\u9669\u56e0\u7d20\u4e00\u81f4\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6982\u5ff5\u9a8c\u8bc1\uff1a\u4f7f\u7528DeepHit\u8fdb\u884c\u751f\u5b58\u5efa\u6a21\u5728\u8db3\u7403\u4f24\u75c5\u9884\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u4e0d\u540c\u7ade\u6280\u6c34\u5e73\u7684\u4f24\u75c5\u9884\u9632\u63d0\u4f9b\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.19791", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.19791", "abs": "https://arxiv.org/abs/2601.19791", "authors": ["Mingyue Xu", "Gal Vardi", "Itay Safran"], "title": "To Grok Grokking: Provable Grokking in Ridge Regression", "comment": null, "summary": "We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persists long after overfitting has manifested; and (iii) the generalization error eventually becomes arbitrarily small. Moreover, we show, both theoretically and empirically, that grokking can be amplified or eliminated in a principled manner through proper hyperparameter tuning. To the best of our knowledge, these are the first rigorous quantitative bounds on the generalization delay (which we refer to as the \"grokking time\") in terms of training hyperparameters. Lastly, going beyond the linear setting, we empirically demonstrate that our quantitative bounds also capture the behavior of grokking on non-linear neural networks. Our results suggest that grokking is not an inherent failure mode of deep learning, but rather a consequence of specific training conditions, and thus does not require fundamental changes to the model architecture or learning algorithm to avoid.", "AI": {"tldr": "\u8bba\u6587\u5728\u7ebf\u6027\u56de\u5f52\u4e2d\u8bc1\u660e\u4e86grokking\u73b0\u8c61\uff08\u8fc7\u62df\u5408\u540e\u5ef6\u8fdf\u6cdb\u5316\uff09\uff0c\u7ed9\u51fa\u4e86\u6cdb\u5316\u5ef6\u8fdf\u65f6\u95f4\u7684\u5b9a\u91cf\u754c\u9650\uff0c\u5e76\u8868\u660e\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u53ef\u4ee5\u63a7\u5236\u6216\u6d88\u9664grokking\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684grokking\u73b0\u8c61\u2014\u2014\u6a21\u578b\u5728\u8fc7\u62df\u5408\u540e\u5f88\u957f\u65f6\u95f4\u624d\u51fa\u73b0\u6cdb\u5316\u80fd\u529b\u3002\u76ee\u524d\u5bf9\u8fd9\u4e00\u73b0\u8c61\u7684\u673a\u5236\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u4e2d\u7684\u5b9a\u91cf\u5206\u6790\u3002", "method": "\u4f7f\u7528\u5e26\u6743\u91cd\u8870\u51cf\u7684\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fc7\u53c2\u6570\u5316\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u4ece\u7406\u8bba\u4e0a\u8bc1\u660egrokking\u7684\u4e09\u4e2a\u9636\u6bb5\uff0c\u63a8\u5bfc\u6cdb\u5316\u5ef6\u8fdf\u65f6\u95f4\u7684\u5b9a\u91cf\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u5728\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7ebf\u6027\u56de\u5f52\u4e2d\u786e\u5b9e\u5b58\u5728grokking\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u65e9\u671f\u8fc7\u62df\u5408\u3001\u957f\u671f\u6cdb\u5316\u5dee\u3001\u6700\u7ec8\u6cdb\u5316\u8bef\u5dee\u4efb\u610f\u5c0f\u3002\u7ed9\u51fa\u4e86\"grokking\u65f6\u95f4\"\u7684\u5b9a\u91cf\u754c\u9650\uff0c\u8868\u660e\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u53ef\u4ee5\u653e\u5927\u6216\u6d88\u9664grokking\u73b0\u8c61\u3002", "conclusion": "grokking\u4e0d\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u800c\u662f\u7279\u5b9a\u8bad\u7ec3\u6761\u4ef6\u7684\u7ed3\u679c\uff0c\u4e0d\u9700\u8981\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u5b66\u4e60\u7b97\u6cd5\u5c31\u80fd\u907f\u514d\u3002\u7406\u8bba\u7ed3\u679c\u5728\u975e\u7ebf\u6027\u7f51\u7edc\u4e2d\u4e5f\u5f97\u5230\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
