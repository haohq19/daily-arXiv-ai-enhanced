{"id": "2508.16807", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16807", "abs": "https://arxiv.org/abs/2508.16807", "authors": ["Marco S. Tayar", "Lucas K. de Oliveira", "Juliano D. Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach", "comment": null, "summary": "Inspecting confined industrial infrastructure, such as ventilation shafts, is\na hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)\noffer a promising alternative, but GPS-denied environments require robust\ncontrol policies to prevent collisions. Deep Reinforcement Learning (DRL) has\nemerged as a powerful framework for developing such policies, and this paper\nprovides a comparative study of two leading DRL algorithms for this task: the\non-policy Proximal Policy Optimization (PPO) and the off-policy Soft\nActor-Critic (SAC). The training was conducted with procedurally generated duct\nenvironments in Genesis simulation environment. A reward function was designed\nto guide a drone through a series of waypoints while applying a significant\npenalty for collisions. PPO learned a stable policy that completed all\nevaluation episodes without collision, producing smooth trajectories. By\ncontrast, SAC consistently converged to a suboptimal behavior that traversed\nonly the initial segments before failure. These results suggest that, in\nhazard-dense navigation, the training stability of on-policy methods can\noutweigh the nominal sample efficiency of off-policy algorithms. More broadly,\nthe study provides evidence that procedurally generated, high-fidelity\nsimulations are effective testbeds for developing and benchmarking robust\nnavigation policies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u6bd4\u7814\u7a76\u4e86PPO\u548cSAC\u4e24\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728GPS\u5931\u6548\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u6027\u80fd\uff0c\u53d1\u73b0PPO\u5728\u5371\u9669\u5bc6\u96c6\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4eba\u5de5\u68c0\u67e5\u9650\u5236\u6027\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd(\u5982\u901a\u98ce\u7ba1\u9053)\u5371\u9669\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u7814\u7a76\u5728GPS\u5931\u6548\u73af\u5883\u4e0b\u7684\u7a33\u5065\u65e0\u4eba\u673a\u63a7\u5236\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u7a0b\u5e8f\u751f\u6210\u7684\u7ba1\u9053\u73af\u5883\u5728Genesis\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3PPO\u548cSAC\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u65e0\u4eba\u673a\u7a81\u7834\u79bb\u7ebf\u70b9\u5e76\u907f\u514d\u78b0\u649e\u3002", "result": "PPO\u5b66\u4e60\u5230\u4e86\u7a33\u5b9a\u7b56\u7565\uff0c\u5b8c\u6210\u6240\u6709\u8bc4\u4f30\u6f14\u7ec3\u4e14\u4ea7\u751f\u5e73\u6ed1\u8f68\u8ff9\uff1bSAC\u59cb\u7ec3\u4e8e\u6b21\u4f18\u884c\u4e3a\uff0c\u53ea\u80fd\u7a7f\u884c\u521d\u59cb\u6bb5\u843d\u540e\u5931\u8d25\u3002", "conclusion": "\u5728\u5371\u9669\u5bc6\u96c6\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u65b9\u7a0b\u7b56\u7565\u65b9\u6cd5\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u8986\u76d6\u4e86\u65b9\u7a0b\u5916\u7b97\u6cd5\u7684\u6807\u79f0\u6837\u672c\u6548\u7387\uff0c\u7a0b\u5e8f\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u6a21\u62df\u662f\u5f00\u53d1\u7a33\u5065\u5bfc\u822a\u7b56\u7565\u7684\u6709\u6548\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2508.16634", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16634", "abs": "https://arxiv.org/abs/2508.16634", "authors": ["Zhendong Yang", "Jie Wang", "Liansong Zong", "Xiaorong Liu", "Quan Qian", "Shiqian Chen"], "title": "Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations", "comment": null, "summary": "Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to\ncontinuously learn from new fault classes with only a few samples without\nforgetting old ones, is critical for real-world industrial systems. However,\nthis challenging task severely amplifies the issues of catastrophic forgetting\nof old knowledge and overfitting on scarce new data. To address these\nchallenges, this paper proposes a novel framework built upon Dual-Granularity\nRepresentations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN\nexplicitly decouples feature learning into two parallel streams: 1) a\nfine-grained representation stream, which utilizes a novel Multi-Order\nInteraction Aggregation module to capture discriminative, class-specific\nfeatures from the limited new samples. 2) a coarse-grained representation\nstream, designed to model and preserve general, class-agnostic knowledge shared\nacross all fault types. These two representations are dynamically fused by a\nmulti-semantic cross-attention mechanism, where the stable coarse-grained\nknowledge guides the learning of fine-grained features, preventing overfitting\nand alleviating feature conflicts. To further mitigate catastrophic forgetting,\nwe design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a\ndecoupled Balanced Random Forest classifier is employed to counter the decision\nboundary bias caused by data imbalance. Extensive experiments on the TEP\nbenchmark and a real-world MFF dataset demonstrate that our proposed DGGN\nachieves superior diagnostic performance and stability compared to\nstate-of-the-art FSC-FD approaches. Our code is publicly available at\nhttps://github.com/MentaY/DGGN", "AI": {"tldr": "\u63d0\u51faDGGN\u6846\u67b6\u89e3\u51b3\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u6545\u969c\u8bca\u65ad\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u7c92\u5ea6\u8868\u793a\u548c\u8fb9\u754c\u611f\u77e5\u7b56\u7565\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u548c\u8fc7\u62df\u5408", "motivation": "\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b0\u6545\u969c\u7c7b\u522b\u4f46\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u548c\u8fc7\u62df\u5408\u7684\u4e25\u91cd\u6311\u6218", "method": "\u53cc\u7c92\u5ea6\u6307\u5bfc\u7f51\u7edc(DGGN)\uff1a\u7ec6\u7c92\u5ea6\u6d41\u6355\u83b7\u7c7b\u522b\u7279\u5b9a\u7279\u5f81\uff0c\u7c97\u7c92\u5ea6\u6d41\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\uff1b\u8fb9\u754c\u611f\u77e5\u6837\u672c\u4f18\u5148\u7b56\u7565\uff1b\u89e3\u8026\u5e73\u8861\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668", "result": "\u5728TEP\u57fa\u51c6\u548c\u771f\u5b9eMFF\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cDGGN\u76f8\u6bd4\u73b0\u6709FSC-FD\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u8bca\u65ad\u6027\u80fd\u548c\u7a33\u5b9a\u6027", "conclusion": "DGGN\u6846\u67b6\u901a\u8fc7\u53cc\u7c92\u5ea6\u8868\u793a\u5b66\u4e60\u548c\u8fb9\u754c\u611f\u77e5\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2508.16752", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16752", "abs": "https://arxiv.org/abs/2508.16752", "authors": ["Marco N. Bochernitsan", "Rodrigo C. Barros", "Lucas S. Kupssinsk\u00fc"], "title": "A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers", "comment": null, "summary": "Achieving fairness in text-to-image generation demands mitigating social\nbiases without compromising visual fidelity, a challenge critical to\nresponsible AI. Current fairness evaluation procedures for text-to-image models\nrely on qualitative judgment or narrow comparisons, which limit the capacity to\nassess both fairness and utility in these models and prevent reproducible\nassessment of debiasing methods. Existing approaches typically employ ad-hoc,\nhuman-centered visual inspections that are both error-prone and difficult to\nreplicate. We propose a method for evaluating fairness and utility in\ntext-to-image models using Pareto-optimal frontiers across hyperparametrization\nof debiasing methods. Our method allows for comparison between distinct\ntext-to-image models, outlining all configurations that optimize fairness for a\ngiven utility and vice-versa. To illustrate our evaluation method, we use\nNormalized Shannon Entropy and ClipScore for fairness and utility evaluation,\nrespectively. We assess fairness and utility in Stable Diffusion, Fair\nDiffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that\nmost default hyperparameterizations of the text-to-image model are dominated\nsolutions in the fairness-utility space, and it is straightforward to find\nbetter hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5e15\u7d2f\u6258\u6700\u4f18\u524d\u6cbf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u516c\u5e73\u6027\u548c\u6548\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u6765\u5e73\u8861\u516c\u5e73\u6027\u548c\u89c6\u89c9\u8d28\u91cf", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u5b9a\u6027\u5224\u65ad\u6216\u6709\u9650\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u96be\u4ee5\u540c\u65f6\u8bc4\u4f30\u516c\u5e73\u6027\u548c\u6548\u7528", "method": "\u4f7f\u7528\u5e15\u7d2f\u6258\u6700\u4f18\u524d\u6cbf\u5206\u6790\u4e0d\u540c\u53bb\u504f\u65b9\u6cd5\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u91c7\u7528\u5f52\u4e00\u5316\u9999\u519c\u71b5\u8bc4\u4f30\u516c\u5e73\u6027\uff0cClipScore\u8bc4\u4f30\u6548\u7528", "result": "\u8bc4\u4f30\u4e86\u591a\u4e2a\u4e3b\u6d41\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u53d1\u73b0\u5927\u591a\u6570\u9ed8\u8ba4\u8d85\u53c2\u6570\u914d\u7f6e\u5728\u516c\u5e73\u6027-\u6548\u7528\u7a7a\u95f4\u4e2d\u88ab\u652f\u914d\uff0c\u53ef\u4ee5\u8f7b\u677e\u627e\u5230\u66f4\u597d\u7684\u8d85\u53c2\u6570", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u6548\u7528\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u91cf\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u66f4\u4f18\u7684\u8d85\u53c2\u6570\u914d\u7f6e"}}
{"id": "2508.16788", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16788", "abs": "https://arxiv.org/abs/2508.16788", "authors": ["Bhagesh Gaur", "Karan Gupta", "Aseem Srivastava", "Manish Gupta", "Md Shad Akhtar"], "title": "Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities", "comment": "Full Paper accepted in EMNLP Findings 2025", "summary": "Online Mental Health Communities (OMHCs) provide crucial peer and expert\nsupport, yet many posts remain unanswered due to missing support attributes\nthat signal the need for help. We present a novel framework that identifies\nthese gaps and prompts users to enrich their posts, thereby improving\nengagement. To support this, we introduce REDDME, a new dataset of 4,760 posts\nfrom mental health subreddits annotated for the span and intensity of three key\nsupport attributes: event what happened?, effect what did the user experience?,\nand requirement what support they need?. Next, we devise a hierarchical\ntaxonomy, CueTaxo, of support attributes for controlled question generation.\nFurther, we propose MH-COPILOT, a reinforcement learning-based system that\nintegrates (a) contextual attribute-span identification, (b) support attribute\nintensity classification, (c) controlled question generation via a hierarchical\ntaxonomy, and (d) a verifier for reward modeling. Our model dynamically\nassesses posts for the presence/absence of support attributes, and generates\ntargeted prompts to elicit missing information. Empirical results across four\nnotable language models demonstrate significant improvements in attribute\nelicitation and user engagement. A human evaluation further validates the\nmodel's effectiveness in real-world OMHC settings.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6MH-COPILOT\uff0c\u901a\u8fc7\u8bc6\u522b\u7f3a\u5931\u7684\u652f\u6301\u5c5e\u6027\u5e76\u63d0\u793a\u7528\u6237\u8865\u5145\u4fe1\u606f\uff0c\u6765\u63d0\u9ad8\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u793e\u533a\u4e2d\u7684\u5e16\u5b50\u56de\u590d\u7387\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u793e\u533a\u4e2d\u8bb8\u591a\u5e16\u5b50\u56e0\u7f3a\u4e4f\u5173\u952e\u652f\u6301\u5c5e\u6027\u800c\u65e0\u4eba\u56de\u590d\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc6\u522b\u8fd9\u4e9b\u7f3a\u53e3\u5e76\u63d0\u793a\u7528\u6237\u8865\u5145\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5e16\u5b50\u8d28\u91cf\u548c\u53c2\u4e0e\u5ea6\u3002", "method": "\u7814\u7a76\u4eba\u5458\u9996\u5148\u6784\u5efa\u4e86REDDME\u6570\u636e\u96c6\uff084,760\u4e2a\u5fc3\u7406\u5065\u5eb7\u5b50\u7ea2\u677f\u5e16\u5b50\uff09\uff0c\u6807\u6ce8\u4e86\u4e8b\u4ef6\u3001\u5f71\u54cd\u548c\u9700\u6c42\u4e09\u4e2a\u5173\u952e\u652f\u6301\u5c5e\u6027\u7684\u8303\u56f4\u548c\u5f3a\u5ea6\u3002\u63d0\u51faCueTaxo\u5c42\u6b21\u5206\u7c7b\u6cd5\u6765\u63a7\u5236\u95ee\u9898\u751f\u6210\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684MH-COPILOT\u7cfb\u7edf\uff0c\u5305\u542b\u5c5e\u6027\u8bc6\u522b\u3001\u5f3a\u5ea6\u5206\u7c7b\u3001\u63a7\u5236\u95ee\u9898\u751f\u6210\u548c\u9a8c\u8bc1\u5668\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u5c5e\u6027\u5f15\u5bfc\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u5584\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u793e\u533a\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "conclusion": "MH-COPILOT\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5e16\u5b50\u4e2d\u7f3a\u5931\u7684\u652f\u6301\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u63d0\u793a\u6765\u5f15\u5bfc\u7528\u6237\u8865\u5145\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u5e16\u5b50\u8d28\u91cf\u548c\u793e\u533a\u53c2\u4e0e\u5ea6\uff0c\u4e3a\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17643", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17643", "abs": "https://arxiv.org/abs/2508.17643", "authors": ["Krishna Vinod", "Prithvi Jai Ramesh", "Pavan Kumar B N", "Bharatesh Chakravarthi"], "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation", "comment": null, "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90ROS\u5305\uff0c\u7528\u4e8e\u5728Gazebo\u6a21\u62df\u5668\u4e2d\u4eceRGB\u76f8\u673a\u751f\u6210\u4e8b\u4ef6\u6d41\uff0c\u5e76\u7814\u7a76\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u7684\u673a\u5668\u4eba\u7b56\u7565\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u529f\u8017\u7b49\u4f18\u52bf\uff0c\u9002\u5408\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u65f6\u673a\u5668\u4eba\u611f\u77e5\uff0c\u4f46\u4e3b\u6d41\u673a\u5668\u4eba\u6a21\u62df\u5668\u4e2d\u7f3a\u4e4f\u5408\u6210\u4e8b\u4ef6\u89c6\u89c9\u6a21\u62df\uff0c\u963b\u788d\u4e86\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u7528\u6237\u53cb\u597d\u7684v2e ROS\u5305\uff0c\u7528\u4e8eGazebo\u6a21\u62df\uff0c\u80fd\u591f\u4eceRGB\u76f8\u673a\u6d41\u65e0\u7f1d\u751f\u6210\u4e8b\u4ef6\u6d41\u3002\u4f7f\u7528\u8be5\u5305\u7814\u7a76\u57fa\u4e8e\u4e8b\u4ef6\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u7b56\u7565\uff0c\u5e76\u5728\u79fb\u52a8\u673a\u5668\u4eba\u76ee\u6807\u8ddf\u968f\u548c\u673a\u68b0\u81c2\u76ee\u6807\u68c0\u6d4b\u4e0e\u6293\u53d6\u4e24\u4e2a\u4ee3\u8868\u6027\u573a\u666f\u4e2d\u4e0eRGB\u57fa\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e8b\u4ef6\u5f15\u5bfc\u7684\u7b56\u7565\u5728\u5404\u79cd\u64cd\u4f5c\u6761\u4ef6\u4e0b\u59cb\u7ec8\u63d0\u4f9b\u7ade\u4e89\u4f18\u52bf\uff0c\u7a81\u663e\u4e86\u4e8b\u4ef6\u9a71\u52a8\u611f\u77e5\u5728\u6539\u5584\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u66f4\u5e7f\u6cdb\u5730\u96c6\u6210\u5230\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u9a71\u52a8\u611f\u77e5\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.16859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16859", "abs": "https://arxiv.org/abs/2508.16859", "authors": ["Jinpeng Hu", "Hongchang Shi", "Chongyuan Dai", "Zhuo Li", "Peipei Song", "Meng Wang"], "title": "Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark", "comment": "ACM Multimedia 2025", "summary": "Multimodal large language models (MLLMs) have been widely applied across\nvarious fields due to their powerful perceptual and reasoning capabilities. In\nthe realm of psychology, these models hold promise for a deeper understanding\nof human emotions and behaviors. However, recent research primarily focuses on\nenhancing their emotion recognition abilities, leaving the substantial\npotential in emotion reasoning, which is crucial for improving the naturalness\nand effectiveness of human-machine interactions. Therefore, in this paper, we\nintroduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)\nbenchmark, which encompasses 1,451 video data from real-life scenarios, along\nwith 5,101 progressive questions. These questions cover various aspects,\nincluding emotion recognition, potential causes of emotions, future action\nprediction, etc. Besides, we propose a multi-agent framework, where each agent\nspecializes in a specific aspect, such as background context, character\ndynamics, and event details, to improve the system's reasoning capabilities.\nFurthermore, we conduct experiments with existing MLLMs and our agent-based\nmethod on the proposed benchmark, revealing that most models face significant\nchallenges with this task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u4e0e\u63a8\u7406\u57fa\u51c6(MTMEUR)\uff0c\u5305\u542b1,451\u4e2a\u771f\u5b9e\u573a\u666f\u89c6\u9891\u548c5,101\u4e2a\u6e10\u8fdb\u5f0f\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u63d0\u5347\u60c5\u611f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u9886\u57df\u4e3b\u8981\u5173\u6ce8\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u800c\u5ffd\u89c6\u4e86\u60c5\u611f\u63a8\u7406\u8fd9\u4e00\u5bf9\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u81ea\u7136\u6027\u548c\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u8f6e\u5bf9\u8bdd\u7684MTMEUR\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u6ce8\u4e8e\u7279\u5b9a\u65b9\u9762\uff08\u5982\u80cc\u666f\u4e0a\u4e0b\u6587\u3001\u89d2\u8272\u52a8\u6001\u3001\u4e8b\u4ef6\u7ec6\u8282\uff09\u6765\u589e\u5f3a\u7cfb\u7edf\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u63d0\u51fa\u7684\u57fa\u51c6\u4e0a\u5bf9\u73b0\u6709MLLM\u548c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u60c5\u611f\u63a8\u7406\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u60c5\u611f\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2508.17130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17130", "abs": "https://arxiv.org/abs/2508.17130", "authors": ["Catherine Hoier", "Khandaker Mamun Ahmed"], "title": "Structural Damage Detection Using AI Super Resolution and Visual Language Model", "comment": null, "summary": "Natural disasters pose significant challenges to timely and accurate damage\nassessment due to their sudden onset and the extensive areas they affect.\nTraditional assessment methods are often labor-intensive, costly, and hazardous\nto personnel, making them impractical for rapid response, especially in\nresource-limited settings. This study proposes a novel, cost-effective\nframework that leverages aerial drone footage, an advanced AI-based video\nsuper-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a\n27 billion parameter Visual Language Model (VLM). This integrated system is\ndesigned to improve low-resolution disaster footage, identify structural\ndamage, and classify buildings into four damage categories, ranging from\nno/slight damage to total destruction, along with associated risk levels. The\nmethodology was validated using pre- and post-event drone imagery from the 2023\nTurkey earthquakes (courtesy of The Guardian) and satellite data from the 2013\nMoore Tornado (xBD dataset). The framework achieved a classification accuracy\nof 84.5%, demonstrating its ability to provide highly accurate results.\nFurthermore, the system's accessibility allows non-technical users to perform\npreliminary analyses, thereby improving the responsiveness and efficiency of\ndisaster management efforts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65e0\u4eba\u673a\u5f71\u50cf\u3001VRT\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u548cGemma3:27b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u707e\u5bb3\u635f\u5bb3\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u571f\u8033\u5176\u5730\u9707\u548c\u6469\u5c14\u9f99\u5377\u98ce\u6570\u636e\u4e0a\u8fbe\u523084.5%\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u707e\u5bb3\u635f\u5bb3\u8bc4\u4f30\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u96be\u4ee5\u5feb\u901f\u54cd\u5e94\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u3001\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848", "method": "\u6574\u5408\u65e0\u4eba\u673a\u5f71\u50cf\u3001VRT\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u548c27B\u53c2\u6570\u7684Gemma3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7cfb\u7edf\u8fdb\u884c\u5f71\u50cf\u589e\u5f3a\u3001\u7ed3\u6784\u635f\u4f24\u8bc6\u522b\u548c\u56db\u7c7b\u635f\u5bb3\u7a0b\u5ea6\u5206\u7c7b", "result": "\u57282023\u571f\u8033\u5176\u5730\u9707\u548c2013\u6469\u5c14\u9f99\u5377\u98ce\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8fbe\u523084.5%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7ed3\u679c", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u53ef\u8bbf\u95ee\u7684\u521d\u6b65\u5206\u6790\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u707e\u5bb3\u7ba1\u7406\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u6548\u7387"}}
{"id": "2508.17637", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17637", "abs": "https://arxiv.org/abs/2508.17637", "authors": ["Chenxu Yang", "Ruipeng Jia", "Mingyu Zheng", "Naibin Gu", "Zheng Lin", "Siyuan Chen", "Weichong Yin", "Hua Wu", "Weiping Wang"], "title": "Weights-Rotated Preference Optimization for Large Language Models", "comment": "EMNLP 2025", "summary": "Despite the efficacy of Direct Preference Optimization (DPO) in aligning\nLarge Language Models (LLMs), reward hacking remains a pivotal challenge. This\nissue emerges when LLMs excessively reduce the probability of rejected\ncompletions to achieve high rewards, without genuinely meeting their intended\ngoals. As a result, this leads to overly lengthy generation lacking diversity,\nas well as catastrophic forgetting of knowledge. We investigate the underlying\nreason behind this issue, which is representation redundancy caused by neuron\ncollapse in the parameter space. Hence, we propose a novel Weights-Rotated\nPreference Optimization (RoPO) algorithm, which implicitly constrains the\noutput layer logits with the KL divergence inherited from DPO and explicitly\nconstrains the intermediate hidden states by fine-tuning on a multi-granularity\northogonal matrix. This design prevents the policy model from deviating too far\nfrom the reference model, thereby retaining the knowledge and expressive\ncapabilities acquired during pre-training and SFT stages. Our RoPO achieves up\nto a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by\n6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters,\ndemonstrating its effectiveness in alleviating the reward hacking problem of\nDPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoPO\u7b97\u6cd5\u6765\u89e3\u51b3DPO\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u6743\u91cd\u65cb\u8f6c\u548c\u6b63\u4ea4\u7ea6\u675f\u9632\u6b62\u6a21\u578b\u8fc7\u5ea6\u504f\u79bb\u53c2\u8003\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "DPO\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u6a21\u578b\u8fc7\u5ea6\u964d\u4f4e\u88ab\u62d2\u7edd\u8865\u5168\u7684\u6982\u7387\u4ee5\u83b7\u5f97\u9ad8\u5956\u52b1\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u957f\u3001\u7f3a\u4e4f\u591a\u6837\u6027\u4ee5\u53ca\u707e\u96be\u6027\u9057\u5fd8\u77e5\u8bc6", "method": "\u63d0\u51fa\u6743\u91cd\u65cb\u8f6c\u504f\u597d\u4f18\u5316(RoPO)\u7b97\u6cd5\uff0c\u9690\u5f0f\u7ea6\u675f\u8f93\u51fa\u5c42logits\u7684KL\u6563\u5ea6\uff0c\u663e\u5f0f\u7ea6\u675f\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u901a\u8fc7\u591a\u7c92\u5ea6\u6b63\u4ea4\u77e9\u9635\u5fae\u8c03", "result": "\u5728AlpacaEval 2\u4e0a\u63d0\u53473.27\u5206\uff0c\u5728MT-Bench\u4e0a\u4ee5\u4ec50.015%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf6.2-7.5\u5206", "conclusion": "RoPO\u6709\u6548\u7f13\u89e3\u4e86DPO\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u4fdd\u7559\u4e86\u9884\u8bad\u7ec3\u548cSFT\u9636\u6bb5\u83b7\u5f97\u7684\u77e5\u8bc6\u548c\u8868\u8fbe\u80fd\u529b"}}
{"id": "2508.17767", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17767", "abs": "https://arxiv.org/abs/2508.17767", "authors": ["Guangwei Zhang", "Qisheng Su", "Jiateng Liu", "Cheng Qian", "Yanzhou Pan", "Yanjie Fu", "Denghui Zhang"], "title": "ISACL: Internal State Analyzer for Copyrighted Training Data Leakage", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,\nespecially when such data is used for training but not intended for\ndistribution. Traditional methods address these leaks only after content is\ngenerated, which can lead to the exposure of sensitive information. This study\nintroduces a proactive approach: examining LLMs' internal states before text\ngeneration to detect potential leaks. By using a curated dataset of copyrighted\nmaterials, we trained a neural network classifier to identify risks, allowing\nfor early intervention by stopping the generation process or altering outputs\nto prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)\nsystem, this framework ensures adherence to copyright and licensing\nrequirements while enhancing data privacy and ethical standards. Our results\nshow that analyzing internal states effectively mitigates the risk of\ncopyrighted data leakage, offering a scalable solution that fits smoothly into\nAI workflows, ensuring compliance with copyright regulations while maintaining\nhigh-quality text generation. The implementation is available on\nGitHub.\\footnote{https://github.com/changhu73/Internal_states_leakage}", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u72b6\u6001\u6765\u4e3b\u52a8\u68c0\u6d4b\u7248\u6743\u6570\u636e\u6cc4\u9732\u98ce\u9669\u7684\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u751f\u6210\u524d\u8fdb\u884c\u5e72\u9884\u4ee5\u9632\u6b62\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u65e0\u610f\u4e2d\u66b4\u9732\u53d7\u7248\u6743\u4fdd\u62a4\u6216\u4e13\u6709\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ea\u80fd\u5728\u5185\u5bb9\u751f\u6210\u540e\u5904\u7406\u6cc4\u9732\u95ee\u9898\uff0c\u5b58\u5728\u654f\u611f\u4fe1\u606f\u66b4\u9732\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u7cbe\u9009\u7684\u7248\u6743\u6750\u6599\u6570\u636e\u96c6\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u72b6\u6001\u5728\u6587\u672c\u751f\u6210\u524d\u68c0\u6d4b\u6f5c\u5728\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u96c6\u6210\u5b9e\u73b0\u65e9\u671f\u5e72\u9884\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5206\u6790\u5185\u90e8\u72b6\u6001\u80fd\u6709\u6548\u964d\u4f4e\u7248\u6743\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6587\u672c\u751f\u6210\u7684\u540c\u65f6\u786e\u4fdd\u7b26\u5408\u7248\u6743\u6cd5\u89c4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAI\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5e73\u6ed1\u96c6\u6210\u7684\u4e3b\u52a8\u9632\u62a4\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u9690\u79c1\u548c\u4f26\u7406\u6807\u51c6\uff0c\u786e\u4fdd\u7248\u6743\u548c\u8bb8\u53ef\u8981\u6c42\u7684\u5408\u89c4\u6027\u3002"}}
{"id": "2508.17405", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17405", "abs": "https://arxiv.org/abs/2508.17405", "authors": ["Avishag Shapira", "Simon Shigol", "Asaf Shabtai"], "title": "FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats", "comment": null, "summary": "The widespread adoption of machine learning (ML) systems increased attention\nto their security and emergence of adversarial machine learning (AML)\ntechniques that exploit fundamental vulnerabilities in ML systems, creating an\nurgent need for comprehensive risk assessment for ML-based systems. While\ntraditional risk assessment frameworks evaluate conventional cybersecurity\nrisks, they lack ability to address unique challenges posed by AML threats.\nExisting AML threat evaluation approaches focus primarily on technical attack\nrobustness, overlooking crucial real-world factors like deployment\nenvironments, system dependencies, and attack feasibility. Attempts at\ncomprehensive AML risk assessment have been limited to domain-specific\nsolutions, preventing application across diverse systems. Addressing these\nlimitations, we present FRAME, the first comprehensive and automated framework\nfor assessing AML risks across diverse ML-based systems. FRAME includes a novel\nrisk assessment method that quantifies AML risks by systematically evaluating\nthree key dimensions: target system's deployment environment, characteristics\nof diverse AML techniques, and empirical insights from prior research. FRAME\nincorporates a feasibility scoring mechanism and LLM-based customization for\nsystem-specific assessments. Additionally, we developed a comprehensive\nstructured dataset of AML attacks enabling context-aware risk assessment. From\nan engineering application perspective, FRAME delivers actionable results\ndesigned for direct use by system owners with only technical knowledge of their\nsystems, without expertise in AML. We validated it across six diverse\nreal-world applications. Our evaluation demonstrated exceptional accuracy and\nstrong alignment with analysis by AML experts. FRAME enables organizations to\nprioritize AML risks, supporting secure AI deployment in real-world\nenvironments.", "AI": {"tldr": "FRAME\u662f\u4e00\u4e2a\u5168\u9762\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u98ce\u9669\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u90e8\u7f72\u73af\u5883\u3001\u653b\u51fb\u6280\u672f\u548c\u5b9e\u8bc1\u7814\u7a76\u4e09\u4e2a\u7ef4\u5ea6\u6765\u91cf\u5316\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u653b\u51fb\u9c81\u68d2\u6027\uff0c\u5ffd\u89c6\u4e86\u90e8\u7f72\u73af\u5883\u3001\u7cfb\u7edf\u4f9d\u8d56\u548c\u653b\u51fb\u53ef\u884c\u6027\u7b49\u73b0\u5b9e\u56e0\u7d20\uff0c\u4e14\u7f3a\u4e4f\u8de8\u9886\u57df\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "FRAME\u6846\u67b6\u5305\u542b\u65b0\u9896\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u76ee\u6807\u7cfb\u7edf\u90e8\u7f72\u73af\u5883\u3001\u591a\u6837\u5316AML\u6280\u672f\u7279\u5f81\u3001\u4ee5\u53ca\u5148\u524d\u7814\u7a76\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002\u91c7\u7528\u53ef\u884c\u6027\u8bc4\u5206\u673a\u5236\u548c\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u7279\u5b9a\u8bc4\u4f30\u5b9a\u5236\uff0c\u5e76\u5f00\u53d1\u4e86\u5168\u9762\u7684\u7ed3\u6784\u5316AML\u653b\u51fb\u6570\u636e\u96c6\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u4e0eAML\u4e13\u5bb6\u5206\u6790\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u80fd\u591f\u4e3a\u7cfb\u7edf\u6240\u6709\u8005\u63d0\u4f9b\u53ef\u76f4\u63a5\u4f7f\u7528\u7684\u53ef\u64cd\u4f5c\u7ed3\u679c\u3002", "conclusion": "FRAME\u4f7f\u7ec4\u7ec7\u80fd\u591f\u4f18\u5148\u5904\u7406AML\u98ce\u9669\uff0c\u652f\u6301\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b89\u5168\u90e8\u7f72AI\u7cfb\u7edf\uff0c\u586b\u8865\u4e86\u73b0\u6709\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.17412", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.17412", "abs": "https://arxiv.org/abs/2508.17412", "authors": ["Dongseok Kim", "Wonjun Jeong", "Gisung Oh"], "title": "Convergence and Generalization of Anti-Regularization for Parametric Models", "comment": "39 pages, 1 figure", "summary": "We propose Anti-regularization (AR), which adds a sign-reversed reward term\nto the loss to intentionally increase model expressivity in the small-sample\nregime, and then attenuates this intervention with a power-law decay as the\nsample size grows. We formalize spectral safety and trust-region conditions,\nand design a lightweight stability safeguard that combines a projection\noperator with gradient clipping, ensuring stable intervention under stated\nassumptions. Our analysis spans linear smoothers and the Neural Tangent Kernel\n(NTK) regime, providing practical guidance on selecting the decay exponent by\nbalancing empirical risk against variance. Empirically, AR reduces underfitting\nwhile preserving generalization and improving calibration in both regression\nand classification. Ablation studies confirm that the decay schedule and the\nstability safeguard are critical to preventing overfitting and numerical\ninstability. We further examine a degrees-of-freedom targeting schedule that\nkeeps per-sample complexity approximately constant. AR is simple to implement\nand reproducible, integrating cleanly into standard empirical risk minimization\npipelines. It enables robust learning in data- and resource-constrained\nsettings by intervening only when beneficial and fading away when unnecessary.", "AI": {"tldr": "\u63d0\u51fa\u53cd\u6b63\u5219\u5316(AR)\u65b9\u6cd5\uff0c\u5728\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u901a\u8fc7\u7b26\u53f7\u53cd\u8f6c\u7684\u5956\u52b1\u9879\u589e\u52a0\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u968f\u6837\u672c\u91cf\u589e\u957f\u4ee5\u5e42\u5f8b\u8870\u51cf\u51cf\u5f31\u5e72\u9884\u3002\u5305\u542b\u7a33\u5b9a\u6027\u4fdd\u969c\u673a\u5236\uff0c\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u51cf\u5c11\u6b20\u62df\u5408\u540c\u65f6\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6b20\u62df\u5408\u95ee\u9898\uff0c\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6cd5\u53ef\u80fd\u8fc7\u5ea6\u7ea6\u675f\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u589e\u5f3a\u8868\u8fbe\u6027\u3001\u6570\u636e\u5145\u8db3\u65f6\u81ea\u52a8\u9000\u51fa\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u635f\u5931\u51fd\u6570\u4e2d\u6dfb\u52a0\u7b26\u53f7\u53cd\u8f6c\u7684\u5956\u52b1\u9879\u6765\u589e\u52a0\u6a21\u578b\u8868\u8fbe\u6027\uff0c\u91c7\u7528\u5e42\u5f8b\u8870\u51cf\u673a\u5236\u968f\u6837\u672c\u91cf\u589e\u52a0\u51cf\u5f31\u5e72\u9884\uff0c\u8bbe\u8ba1\u5305\u542b\u6295\u5f71\u7b97\u5b50\u548c\u68af\u5ea6\u88c1\u526a\u7684\u7a33\u5b9a\u6027\u4fdd\u969c\u673a\u5236\uff0c\u652f\u6301\u7ebf\u6027\u5e73\u6ed1\u5668\u548cNTK\u673a\u5236\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAR\u80fd\u6709\u6548\u51cf\u5c11\u6b20\u62df\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u6cdb\u5316\u6027\u80fd\u548c\u6539\u5584\u6821\u51c6\u6027\uff0c\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u8870\u51cf\u8ba1\u5212\u548c\u7a33\u5b9a\u6027\u4fdd\u969c\u5bf9\u9632\u6b62\u8fc7\u62df\u5408\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u3002", "conclusion": "AR\u65b9\u6cd5\u7b80\u5355\u6613\u5b9e\u73b0\uff0c\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u6d41\u7a0b\u4e2d\uff0c\u5728\u6570\u636e\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9c81\u68d2\u5b66\u4e60\uff0c\u53ea\u5728\u6709\u76ca\u65f6\u5e72\u9884\u5e76\u5728\u4e0d\u5fc5\u8981\u65f6\u81ea\u52a8\u9000\u51fa\u3002"}}
{"id": "2508.17521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17521", "abs": "https://arxiv.org/abs/2508.17521", "authors": ["YongKyung Oh", "Seungsu Kam", "Dong-Young Lim", "Sungil Kim"], "title": "Modeling Irregular Astronomical Time Series with Neural Stochastic Delay Differential Equations", "comment": null, "summary": "Astronomical time series from large-scale surveys like LSST are often\nirregularly sampled and incomplete, posing challenges for classification and\nanomaly detection. We introduce a new framework based on Neural Stochastic\nDelay Differential Equations (Neural SDDEs) that combines stochastic modeling\nwith neural networks to capture delayed temporal dynamics and handle irregular\nobservations. Our approach integrates a delay-aware neural architecture, a\nnumerical solver for SDDEs, and mechanisms to robustly learn from noisy, sparse\nsequences. Experiments on irregularly sampled astronomical data demonstrate\nstrong classification accuracy and effective detection of novel astrophysical\nevents, even with partial labels. This work highlights Neural SDDEs as a\nprincipled and practical tool for time series analysis under observational\nconstraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u968f\u673a\u5ef6\u8fdf\u5fae\u5206\u65b9\u7a0b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u5929\u6587\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5728\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "motivation": "\u5927\u89c4\u6a21\u5929\u6587\u8c03\u67e5\u5982LSST\u4ea7\u751f\u7684\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u4e0d\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7ed9\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5", "method": "\u7ed3\u5408\u968f\u673a\u5efa\u6a21\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u5ef6\u8fdf\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u3001SDDE\u6570\u503c\u6c42\u89e3\u5668\uff0c\u4ee5\u53ca\u4ece\u566a\u58f0\u7a00\u758f\u5e8f\u5217\u4e2d\u7a33\u5065\u5b66\u4e60\u7684\u673a\u5236", "result": "\u5728\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u5929\u6587\u6570\u636e\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u65b0\u7684\u5929\u4f53\u7269\u7406\u4e8b\u4ef6\uff0c\u5373\u4f7f\u5728\u90e8\u5206\u6807\u7b7e\u60c5\u51b5\u4e0b", "conclusion": "\u795e\u7ecf\u968f\u673a\u5ef6\u8fdf\u5fae\u5206\u65b9\u7a0b\u662f\u5728\u89c2\u6d4b\u7ea6\u675f\u4e0b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u539f\u7406\u6027\u548c\u5b9e\u7528\u5de5\u5177"}}
{"id": "2508.17586", "categories": ["cs.LG", "68T50"], "pdf": "https://arxiv.org/pdf/2508.17586", "abs": "https://arxiv.org/abs/2508.17586", "authors": ["Daniel Frees", "Aditri Bhagirath", "Moritz Bolling"], "title": "Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA", "comment": null, "summary": "While Large Language Models (LLMs) have revolutionized artificial\nintelligence, fine-tuning LLMs is extraordinarily computationally expensive,\npreventing smaller businesses and research teams with limited GPU resources\nfrom engaging with new research. Hu et al and Liu et al introduce Low-Rank\nAdaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) as highly\nefficient and performant solutions to the computational challenges of LLM\nfine-tuning, demonstrating huge speedups and memory usage savings for models\nsuch as GPT-3 and RoBERTa. We seek to expand upon the original LoRA and DoRA\npapers by benchmarking efficiency and performance of LoRA and DoRA when applied\nto a much smaller scale of language model: our case study here is the compact\nminBERT model. Our findings reveal that optimal custom configurations of LoRA\nand DoRA, coupled with Automatic Mixed Precision (AMP), significantly enhance\ntraining efficiency without compromising performance. Furthermore, while the\nparameterization of minBERT is significantly smaller than GPT-3, our results\nvalidate the observation that gradient updates to language models are\ninherently low-rank even in small model space, observing that rank 1\ndecompositions yield negligible performance deficits. Furthermore, aided by our\nhighly efficient minBERT implementation, we investigate numerous architectures,\ncustom loss functions, and hyperparameters to ultimately train an optimal\nensembled multitask minBERT model to simultaneously perform sentiment analysis,\nparaphrase detection, and similarity scoring.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LoRA\u548cDoRA\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578bminBERT\u4e0a\u7684\u5e94\u7528\u6548\u679c\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u5c0f\u6a21\u578b\u4e0a\u68af\u5ea6\u66f4\u65b0\u4e5f\u5177\u6709\u4f4e\u79e9\u7279\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u914d\u7f6e\u548cAMP\u6280\u672f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u800c\u4e0d\u635f\u5931\u6027\u80fd\u3002", "motivation": "\u867d\u7136LoRA\u548cDoRA\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6548\u679c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e9b\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u6a21\u578bminBERT\u4e0a\u7684\u9002\u7528\u6027\u548c\u6548\u679c\u3002", "method": "\u4f7f\u7528LoRA\u548cDoRA\u65b9\u6cd5\u5bf9minBERT\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7ed3\u5408\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6(AMP)\u6280\u672f\uff0c\u6d4b\u8bd5\u4e0d\u540c\u79e9\u7684\u5206\u89e3\u6548\u679c\uff0c\u5e76\u63a2\u7d22\u591a\u79cd\u67b6\u6784\u3001\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u5728\u5c0f\u6a21\u578b\u4e0a\uff0c\u68af\u5ea6\u66f4\u65b0\u4e5f\u5177\u6709\u4f4e\u79e9\u7279\u6027\uff0c\u79e91\u5206\u89e3\u51e0\u4e4e\u4e0d\u4f1a\u9020\u6210\u6027\u80fd\u635f\u5931\u3002\u901a\u8fc7\u4f18\u5316\u914d\u7f6e\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u6700\u7ec8\u8bad\u7ec3\u51fa\u80fd\u591f\u540c\u65f6\u6267\u884c\u60c5\u611f\u5206\u6790\u3001\u590d\u8ff0\u68c0\u6d4b\u548c\u76f8\u4f3c\u5ea6\u8bc4\u5206\u7684\u6700\u4f18\u96c6\u6210\u591a\u4efb\u52a1\u6a21\u578b\u3002", "conclusion": "LoRA\u548cDoRA\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0a\u540c\u6837\u6709\u6548\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6a21\u578b\u68af\u5ea6\u66f4\u65b0\u7684\u4f4e\u79e9\u7279\u6027\u5177\u6709\u666e\u904d\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2508.17442", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17442", "abs": "https://arxiv.org/abs/2508.17442", "authors": ["Liyang Peng", "Sihan Zhu", "Yunjie Guo"], "title": "Multi-Level LVLM Guidance for Untrimmed Video Action Recognition", "comment": null, "summary": "Action recognition and localization in complex, untrimmed videos remain a\nformidable challenge in computer vision, largely due to the limitations of\nexisting methods in capturing fine-grained actions, long-term temporal\ndependencies, and high-level semantic information from low-level visual\nfeatures. This paper introduces the Event-Contextualized Video Transformer\n(ECVT), a novel architecture that leverages the advanced semantic understanding\ncapabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT\nemploys a dual-branch design, comprising a Video Encoding Branch for\nspatio-temporal feature extraction and a Cross-Modal Guidance Branch. The\nlatter utilizes an LVLM to generate multi-granularity semantic descriptions,\nincluding Global Event Prompting for macro-level narrative and Temporal\nSub-event Prompting for fine-grained action details. These multi-level textual\ncues are integrated into the video encoder's learning process through\nsophisticated mechanisms such as adaptive gating for high-level semantic\nfusion, cross-modal attention for fine-grained feature refinement, and an event\ngraph module for temporal context calibration. Trained end-to-end with a\ncomprehensive loss function incorporating semantic consistency and temporal\ncalibration terms, ECVT significantly enhances the model's ability to\nunderstand video temporal structures and event logic. Extensive experiments on\nActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves\nstate-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3\nand mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.", "AI": {"tldr": "ECVT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u8bbe\u8ba1\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u7c92\u5ea6\u8bed\u4e49\u63cf\u8ff0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u672a\u4fee\u526a\u89c6\u9891\u4e2d\u7684\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u548c\u957f\u671f\u65f6\u5e8f\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u672a\u4fee\u526a\u89c6\u9891\u4e2d\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u3001\u957f\u671f\u65f6\u5e8f\u4f9d\u8d56\u548c\u4ece\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\u4e2d\u63d0\u53d6\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u8bbe\u8ba1\uff1a\u89c6\u9891\u7f16\u7801\u5206\u652f\u8d1f\u8d23\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff0c\u8de8\u6a21\u6001\u5f15\u5bfc\u5206\u652f\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u7c92\u5ea6\u8bed\u4e49\u63cf\u8ff0\uff08\u5168\u5c40\u4e8b\u4ef6\u63d0\u793a\u548c\u65f6\u5e8f\u5b50\u4e8b\u4ef6\u63d0\u793a\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u95e8\u63a7\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u4e8b\u4ef6\u56fe\u6a21\u5757\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728ActivityNet v1.3\u6570\u636e\u96c6\u4e0a\u8fbe\u523040.5%\u7684\u5e73\u5747mAP\uff0c\u5728THUMOS14\u6570\u636e\u96c6\u4e0a\u8fbe\u523067.1%\u7684mAP@0.5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "ECVT\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u521b\u65b0\u7684\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u6210\u529f\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u5e8f\u7ed3\u6784\u548c\u4e8b\u4ef6\u903b\u8f91\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17537", "categories": ["cs.CV", "68T45", "I.4.5"], "pdf": "https://arxiv.org/pdf/2508.17537", "abs": "https://arxiv.org/abs/2508.17537", "authors": ["Petr Hruby", "Marc Pollefeys"], "title": "Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks", "comment": "13 pages, 5 figures", "summary": "We address the problem of estimating both translational and angular velocity\nof a camera from asynchronous point tracks, a formulation relevant to rolling\nshutter and event cameras. Since the original problem is non-polynomial, we\npropose a polynomial approximation, classify the resulting minimal problems,\nand determine their algebraic degrees. Furthermore, we develop minimal solvers\nfor several problems with low degrees and evaluate them on synthetic and real\ndatasets. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9879\u5f0f\u8fd1\u4f3c\u65b9\u6cd5\u89e3\u51b3\u76f8\u673a\u5e73\u79fb\u548c\u89d2\u901f\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u5f00\u53d1\u6700\u5c0f\u6c42\u89e3\u5668\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1", "motivation": "\u89e3\u51b3\u6eda\u52a8\u5feb\u95e8\u548c\u4e8b\u4ef6\u76f8\u673a\u4e2d\u4ece\u5f02\u6b65\u70b9\u8f68\u8ff9\u4f30\u8ba1\u76f8\u673a\u5e73\u79fb\u548c\u89d2\u901f\u5ea6\u7684\u95ee\u9898\uff0c\u539f\u59cb\u95ee\u9898\u662f\u975e\u591a\u9879\u5f0f\u7684", "method": "\u63d0\u51fa\u591a\u9879\u5f0f\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5bf9\u6700\u5c0f\u95ee\u9898\u8fdb\u884c\u5206\u7c7b\u5e76\u786e\u5b9a\u4ee3\u6570\u5ea6\uff0c\u5f00\u53d1\u4f4e\u4ee3\u6570\u5ea6\u7684\u6700\u5c0f\u6c42\u89e3\u5668", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u5f00\u53d1\u7684\u6c42\u89e3\u5668\uff0c\u4ee3\u7801\u5c06\u516c\u5f00", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f8\u673a\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u6eda\u52a8\u5feb\u95e8\u548c\u4e8b\u4ef6\u76f8\u673a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17894", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.17894", "abs": "https://arxiv.org/abs/2508.17894", "authors": ["Iason Ioannis Panagos", "Giorgos Sfikas", "Christophoros Nikou"], "title": "Designing Practical Models for Isolated Word Visual Speech Recognition", "comment": "Double-column format, 13 pages with references, 2 figures", "summary": "Visual speech recognition (VSR) systems decode spoken words from an input\nsequence using only the video data. Practical applications of such systems\ninclude medical assistance as well as human-machine interactions. A VSR system\nis typically employed in a complementary role in cases where the audio is\ncorrupt or not available. In order to accurately predict the spoken words,\nthese architectures often rely on deep neural networks in order to extract\nmeaningful representations from the input sequence. While deep architectures\nachieve impressive recognition performance, relying on such models incurs\nsignificant computation costs which translates into increased resource demands\nin terms of hardware requirements and results in limited applicability in\nreal-world scenarios where resources might be constrained. This factor prevents\nwider adoption and deployment of speech recognition systems in more practical\napplications. In this work, we aim to alleviate this issue by developing\narchitectures for VSR that have low hardware costs. Following the standard\ntwo-network design paradigm, where one network handles visual feature\nextraction and another one utilizes the extracted features to classify the\nentire sequence, we develop lightweight end-to-end architectures by first\nbenchmarking efficient models from the image classification literature, and\nthen adopting lightweight block designs in a temporal convolution network\nbackbone. We create several unified models with low resource requirements but\nstrong recognition performance. Experiments on the largest public database for\nEnglish words demonstrate the effectiveness and practicality of our developed\nmodels. Code and trained models will be made publicly available.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b(VSR)\u7cfb\u7edf\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u901a\u8fc7\u91c7\u7528\u6548\u7387\u9ad8\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u548c\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\u6838\u5fc3\uff0c\u5728\u4fdd\u6301\u8bc6\u522b\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u867d\u7136\u6df1\u5ea6\u7f51\u7edc\u5728VSR\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u786c\u4ef6\u8981\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u67b6\u6784\u4ee5\u4fc3\u8fdbVSR\u7cfb\u7edf\u7684\u66f4\u5e7f\u6cdb\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u6807\u51c6\u7684\u53cc\u7f51\u7edc\u8bbe\u8ba1\u8303\u5f0f\uff1a\u4e00\u4e2a\u7f51\u7edc\u5904\u7406\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\uff0c\u53e6\u4e00\u4e2a\u8fdb\u884c\u5e8f\u5217\u5206\u7c7b\u3002\u9996\u5148\u57fa\u4e8e\u56fe\u50cf\u5206\u7c7b\u9886\u57df\u7684\u9ad8\u6548\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5728\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\u6838\u5fc3\u4e2d\u91c7\u7528\u8f7b\u91cf\u7ea7\u5757\u8bbe\u8ba1\u3002", "result": "\u5728\u6700\u5927\u7684\u516c\u5171\u82f1\u8bed\u5355\u8bcd\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5f00\u53d1\u51fa\u591a\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u8d44\u6e90\u9700\u6c42\u4f4e\u4f46\u8bc6\u522b\u6027\u80fd\u5f3a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u5f00\u53d1\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u7684\u7ed3\u5408VSR\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u9ad8\u8bc6\u522b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u8d44\u6e90\u9700\u6c42\uff0c\u4e3aVSR\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u66f4\u5e7f\u6cdb\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002\u4ee3\u7801\u548c\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2508.18052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18052", "abs": "https://arxiv.org/abs/2508.18052", "authors": ["Silvia Beddar-Wiesing", "Alice Moallemy-Oureh"], "title": "Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) are known to match the distinguishing power of\nthe 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with\nthe unfolding tree equivalence classes of graphs. Preserving this equivalence,\nGNNs can universally approximate any target function on graphs in probability\nup to any precision. However, these results are limited to attributed\ndiscrete-dynamic graphs represented as sequences of connected graph snapshots.\nReal-world systems, such as communication networks, financial transaction\nnetworks, and molecular interactions, evolve asynchronously and may split into\ndisconnected components. In this paper, we extend the theory of attributed\ndiscrete-dynamic graphs to attributed continuous-time dynamic graphs with\narbitrary connectivity. To this end, we introduce a continuous-time dynamic\n1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,\nand identify a class of continuous-time dynamic GNNs (CGNNs) based on\ndiscrete-dynamic GNN architectures that retain both distinguishing power and\nuniversal approximation guarantees. Our constructive proofs further yield\npractical design guidelines, emphasizing a compact and expressive CGNN\narchitecture with piece-wise continuously differentiable temporal functions to\nprocess asynchronous, disconnected graphs.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\uff0c\u4ece\u79bb\u6563\u52a8\u6001\u56fe\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u56fe\uff0c\u63d0\u51fa\u4e86\u8fde\u7eed\u65f6\u95f4\u52a8\u60011-WL\u6d4b\u8bd5\u548c\u5bf9\u5e94\u7684CGNN\u67b6\u6784\uff0c\u4fdd\u6301\u4e86\u533a\u5206\u80fd\u529b\u548c\u901a\u7528\u903c\u8fd1\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\uff08\u5982\u901a\u4fe1\u7f51\u7edc\u3001\u91d1\u878d\u4ea4\u6613\u7f51\u7edc\u3001\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\uff09\u662f\u5f02\u6b65\u6f14\u5316\u7684\uff0c\u53ef\u80fd\u5206\u88c2\u6210\u65ad\u5f00\u8fde\u63a5\u7684\u7ec4\u4ef6\uff0c\u800c\u73b0\u6709GNN\u7406\u8bba\u4ec5\u9650\u4e8e\u79bb\u6563\u52a8\u6001\u56fe\u5e8f\u5217\u3002", "method": "\u5f15\u5165\u8fde\u7eed\u65f6\u95f4\u52a8\u60011-WL\u6d4b\u8bd5\uff0c\u8bc1\u660e\u5176\u4e0e\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u5c55\u5f00\u6811\u7684\u7b49\u4ef7\u6027\uff0c\u57fa\u4e8e\u79bb\u6563\u52a8\u6001GNN\u67b6\u6784\u8bbe\u8ba1\u8fde\u7eed\u65f6\u95f4\u52a8\u6001GNN\uff08CGNN\uff09\uff0c\u4f7f\u7528\u5206\u6bb5\u8fde\u7eed\u53ef\u5fae\u5206\u65f6\u95f4\u51fd\u6570\u5904\u7406\u5f02\u6b65\u65ad\u5f00\u56fe\u3002", "result": "\u5efa\u7acb\u4e86\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u56fe\u7684\u7b49\u4ef7\u7406\u8bba\u6846\u67b6\uff0c\u8bc6\u522b\u51fa\u5177\u6709\u533a\u5206\u80fd\u529b\u548c\u901a\u7528\u903c\u8fd1\u4fdd\u8bc1\u7684CGNN\u7c7b\u522b\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u6307\u5357\u3002", "conclusion": "\u6210\u529f\u5c06GNN\u7406\u8bba\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u56fe\u9886\u57df\uff0c\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5f02\u6b65\u6f14\u5316\u7684\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u67b6\u6784\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2508.17846", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17846", "abs": "https://arxiv.org/abs/2508.17846", "authors": ["Yang Chen", "Yanbin Wei", "Ke Jin", "Yi Kong", "James Kwok", "Yu Zhang"], "title": "Alternating Training-based Label Smoothing Enhances Prompt Generalization", "comment": null, "summary": "Recent advances in pre-trained vision-language models have demonstrated\nremarkable zero-shot generalization capabilities. To further enhance these\nmodels' adaptability to various downstream tasks, prompt tuning has emerged as\na parameter-efficient fine-tuning method. However, despite its efficiency, the\ngeneralization ability of prompt remains limited. In contrast, label smoothing\n(LS) has been widely recognized as an effective regularization technique that\nprevents models from becoming over-confident and improves their generalization.\nThis inspires us to explore the integration of LS with prompt tuning. However,\nwe have observed that the vanilla LS even weakens the generalization ability of\nprompt tuning. To address this issue, we propose the Alternating Training-based\nLabel Smoothing (ATLaS) method, which alternately trains with standard one-hot\nlabels and soft labels generated by LS to supervise the prompt tuning.\nMoreover, we introduce two types of efficient offline soft labels, including\nClass-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide\ninter-class or instance-class relationships for prompt tuning. The theoretical\nproperties of the proposed ATLaS method are analyzed. Extensive experiments\ndemonstrate that the proposed ATLaS method, combined with CSL and ISL,\nconsistently enhances the generalization performance of prompt tuning.\nMoreover, the proposed ATLaS method exhibits high compatibility with prevalent\nprompt tuning methods, enabling seamless integration into existing methods.", "AI": {"tldr": "\u63d0\u51faATLaS\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u4f7f\u7528one-hot\u6807\u7b7e\u548c\u6807\u7b7e\u5e73\u6ed1\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u63d0\u5347\u63d0\u793a\u8c03\u4f18\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u9ad8\u6548\u79bb\u7ebf\u8f6f\u6807\u7b7e\uff08CSL\u548cISL\uff09\u6765\u63d0\u4f9b\u7c7b\u95f4\u6216\u5b9e\u4f8b-\u7c7b\u5173\u7cfb\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u63d0\u793a\u8c03\u4f18\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u6807\u7b7e\u5e73\u6ed1(LS)\u4f5c\u4e3a\u6709\u6548\u6b63\u5219\u5316\u6280\u672f\u53ef\u4ee5\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408\uff0c\u4f46\u4f20\u7edfLS\u53cd\u800c\u4f1a\u524a\u5f31\u63d0\u793a\u8c03\u4f18\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4ea4\u66ff\u8bad\u7ec3\u6807\u7b7e\u5e73\u6ed1(ATLaS)\u65b9\u6cd5\uff0c\u4ea4\u66ff\u4f7f\u7528\u6807\u51c6one-hot\u6807\u7b7e\u548cLS\u751f\u6210\u7684\u8f6f\u6807\u7b7e\u6765\u76d1\u7763\u63d0\u793a\u8c03\u4f18\u3002\u5f15\u5165\u7c7b\u95f4\u8f6f\u6807\u7b7e(CSL)\u548c\u5b9e\u4f8b\u7ea7\u8f6f\u6807\u7b7e(ISL)\u4e24\u79cd\u9ad8\u6548\u79bb\u7ebf\u8f6f\u6807\u7b7e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cATLaS\u65b9\u6cd5\u7ed3\u5408CSL\u548cISL\u80fd\u6301\u7eed\u63d0\u5347\u63d0\u793a\u8c03\u4f18\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e14\u4e0e\u4e3b\u6d41\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u9ad8\u5ea6\u517c\u5bb9\u3002", "conclusion": "ATLaS\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6807\u7b7e\u5e73\u6ed1\u5728\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u673a\u5236\u548c\u9ad8\u6548\u8f6f\u6807\u7b7e\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u8c03\u4f18\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.17975", "categories": ["cs.CV", "math.LO"], "pdf": "https://arxiv.org/pdf/2508.17975", "abs": "https://arxiv.org/abs/2508.17975", "authors": ["Md Shahi Amran Hossain", "Abu Shad Ahammed", "Sayeri Mukherjee", "Roman Obermaisser"], "title": "Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving", "comment": null, "summary": "The use of computer vision in automotive is a trending research in which\nsafety and security are a primary concern. In particular, for autonomous\ndriving, preventing road accidents requires highly accurate object detection\nunder diverse conditions. To address this issue, recently the International\nOrganization for Standardization (ISO) released the 8800 norm, providing\nstructured frameworks for managing associated AI relevant risks. However,\nchallenging scenarios such as adverse weather or low lighting often introduce\ndata drift, leading to degraded model performance and potential safety\nviolations. In this work, we present a novel hybrid computer vision\narchitecture trained with thousands of synthetic image data from the road\nenvironment to improve robustness in unseen drifted environments. Our dual mode\nframework utilized YOLO version 8 for swift detection and incorporated a\nfive-layer CNN for verification. The system functioned in sequence and improved\nthe detection accuracy by more than 90\\% when tested with drift-augmented road\nimages. The focus was to demonstrate how such a hybrid model can provide better\nroad safety when working together in a hybrid structure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u67b6\u6784\uff0c\u7ed3\u5408YOLOv8\u5feb\u901f\u68c0\u6d4b\u548c\u4e94\u5c42CNN\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u63d0\u9ad8\u5728\u6570\u636e\u6f02\u79fb\u73af\u5883\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc790%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u6076\u52a3\u5929\u6c14\u548c\u4f4e\u5149\u7167\u7b49\u6761\u4ef6\u5bfc\u81f4\u6570\u636e\u6f02\u79fb\uff0c\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u5e76\u5e26\u6765\u5b89\u5168\u9690\u60a3\u3002ISO 8800\u6807\u51c6\u63d0\u4f9b\u4e86AI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\uff0c\u4f46\u9700\u8981\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff1aYOLOv8\u7528\u4e8e\u5feb\u901f\u76ee\u6807\u68c0\u6d4b\uff0c\u4e94\u5c42CNN\u7528\u4e8e\u9a8c\u8bc1\u3002\u4f7f\u7528\u6570\u5343\u5f20\u9053\u8def\u73af\u5883\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u5e8f\u5217\u5904\u7406\u65b9\u5f0f\u63d0\u9ad8\u5728\u6f02\u79fb\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6f02\u79fb\u589e\u5f3a\u7684\u9053\u8def\u56fe\u50cf\u6d4b\u8bd5\u4e2d\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8690%\u4ee5\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u6f02\u79fb\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6df7\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u67b6\u6784\u80fd\u6709\u6548\u5e94\u5bf9\u6570\u636e\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u66f4\u597d\u7684\u9053\u8def\u5b89\u5168\u4fdd\u969c\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.18071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18071", "abs": "https://arxiv.org/abs/2508.18071", "authors": ["Zhenyang Li", "Xiaoyang Bai", "Jinfan Lu", "Pengfei Shen", "Edmund Y. Lam", "Yifan Peng"], "title": "EventTracer: Fast Path Tracing-based Event Stream Rendering", "comment": null, "summary": "Simulating event streams from 3D scenes has become a common practice in\nevent-based vision research, as it meets the demand for large-scale, high\ntemporal frequency data without setting up expensive hardware devices or\nundertaking extensive data collections. Yet existing methods in this direction\ntypically work with noiseless RGB frames that are costly to render, and\ntherefore they can only achieve a temporal resolution equivalent to 100-300\nFPS, far lower than that of real-world event data. In this work, we propose\nEventTracer, a path tracing-based rendering pipeline that simulates\nhigh-fidelity event sequences from complex 3D scenes in an efficient and\nphysics-aware manner. Specifically, we speed up the rendering process via low\nsample-per-pixel (SPP) path tracing, and train a lightweight event spiking\nnetwork to denoise the resulting RGB videos into realistic event sequences. To\ncapture the physical properties of event streams, the network is equipped with\na bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a\nbidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at\na speed of about 4 minutes per second of 720p video, and it inherits the merit\nof accurate spatiotemporal modeling from its path tracing backbone. We show in\ntwo downstream tasks that EventTracer captures better scene details and\ndemonstrates a greater similarity to real-world event data than other event\nsimulators, which establishes it as a promising tool for creating large-scale\nevent-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based\nvision, and boosting various application scenarios such as robotics, autonomous\ndriving, and VRAR.", "AI": {"tldr": "EventTracer\u662f\u4e00\u4e2a\u57fa\u4e8e\u8def\u5f84\u8ffd\u8e2a\u7684\u9ad8\u6548\u4e8b\u4ef6\u6d41\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u4f4e\u91c7\u6837\u8def\u5f84\u8ffd\u8e2a\u548c\u8f7b\u91cf\u7ea7\u8109\u51b2\u7f51\u7edc\u53bb\u566a\uff0c\u80fd\u591f\u4ee54\u5206\u949f/\u79d2\u7684\u901f\u5ea6\u751f\u6210720p\u9ad8\u4fdd\u771f\u4e8b\u4ef6\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u6a21\u62df\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u6d41\u6a21\u62df\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u65e0\u566a\u58f0RGB\u5e27\uff0c\u6e32\u67d3\u6210\u672c\u9ad8\u4e14\u65f6\u95f4\u5206\u8fa8\u7387\u4f4e\uff08100-300 FPS\uff09\uff0c\u8fdc\u4f4e\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u7684\u9891\u7387\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7269\u7406\u51c6\u786e\u7684\u4e8b\u4ef6\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4f4e\u91c7\u6837\u8def\u5f84\u8ffd\u8e2a\u52a0\u901f\u6e32\u67d3\u8fc7\u7a0b\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4e8b\u4ef6\u8109\u51b2\u7f51\u7edc\uff08\u4f7f\u7528\u53cc\u6781\u6027\u6cc4\u6f0f\u79ef\u5206\u53d1\u653e\u5355\u5143\u548c\u53cc\u5411\u5730\u7403\u79fb\u52a8\u8ddd\u79bb\u635f\u5931\uff09\u5c06\u53bb\u566a\u540e\u7684RGB\u89c6\u9891\u8f6c\u6362\u4e3a\u771f\u5b9e\u4e8b\u4ef6\u5e8f\u5217\u3002", "result": "EventTracer\u80fd\u4ee5\u7ea64\u5206\u949f/\u79d2\u7684\u901f\u5ea6\u751f\u6210720p\u89c6\u9891\uff0c\u5728\u65f6\u7a7a\u5efa\u6a21\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u4e8b\u4ef6\u6a21\u62df\u5668\uff0c\u5728\u7ec6\u8282\u6355\u6349\u548c\u4e0e\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u76f8\u4f3c\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "EventTracer\u662f\u521b\u5efa\u5927\u89c4\u6a21\u4e8b\u4ef6-RGB\u6570\u636e\u96c6\u7684\u6709\u524d\u666f\u5de5\u5177\uff0c\u80fd\u591f\u7f29\u5c0f\u4e8b\u4ef6\u89c6\u89c9\u7684\u6a21\u62df-\u73b0\u5b9e\u5dee\u8ddd\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548cVR/AR\u7b49\u5e94\u7528\u573a\u666f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.18187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18187", "abs": "https://arxiv.org/abs/2508.18187", "authors": ["Xuan-Bac Nguyen", "Thanh-Dat Truong", "Pawan Sinha", "Khoa Luu"], "title": "BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding", "comment": null, "summary": "Memory decay makes it harder for the human brain to recognize visual objects\nand retain details. Consequently, recorded brain signals become weaker,\nuncertain, and contain poor visual context over time. This paper presents one\nof the first vision-learning approaches to address this problem. First, we\nstatistically and experimentally demonstrate the existence of inconsistency in\nbrain signals and its impact on the Vision-Brain Understanding (VBU) model. Our\nfindings show that brain signal representations shift over recording sessions,\nleading to compounding bias, which poses challenges for model learning and\ndegrades performance. Then, we propose a new Bias-Mitigation Continual Learning\n(BRAIN) approach to address these limitations. In this approach, the model is\ntrained in a continual learning setup and mitigates the growing bias from each\nlearning step. A new loss function named De-bias Contrastive Learning is also\nintroduced to address the bias problem. In addition, to prevent catastrophic\nforgetting, where the model loses knowledge from previous sessions, the new\nAngular-based Forgetting Mitigation approach is introduced to preserve learned\nknowledge in the model. Finally, the empirical experiments demonstrate that our\napproach achieves State-of-the-Art (SOTA) performance across various\nbenchmarks, surpassing prior and non-continual learning methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBRAIN\u7684\u504f\u7f6e\u7f13\u89e3\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53bb\u504f\u5bf9\u6bd4\u5b66\u4e60\u548c\u57fa\u4e8e\u89d2\u5ea6\u7684\u9057\u5fd8\u7f13\u89e3\u6280\u672f\uff0c\u89e3\u51b3\u8111\u4fe1\u53f7\u968f\u65f6\u95f4\u8870\u51cf\u5bfc\u81f4\u7684\u8868\u793a\u504f\u79fb\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u8bb0\u5fc6\u8870\u51cf\u5bfc\u81f4\u8111\u4fe1\u53f7\u968f\u65f6\u95f4\u53d8\u5f31\u3001\u4e0d\u786e\u5b9a\u4e14\u7f3a\u4e4f\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u5bf9\u89c6\u89c9-\u8111\u7406\u89e3\u6a21\u578b\u9020\u6210\u6311\u6218\uff0c\u8868\u73b0\u4e3a\u8111\u4fe1\u53f7\u8868\u793a\u968f\u8bb0\u5f55\u4f1a\u8bdd\u53d1\u751f\u504f\u79fb\uff0c\u4ea7\u751f\u7d2f\u79ef\u504f\u7f6e\u3002", "method": "\u63d0\u51faBRAIN\u504f\u7f6e\u7f13\u89e3\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff1a1\uff09\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u6a21\u578b\uff1b2\uff09\u5f15\u5165\u53bb\u504f\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u51fd\u6570\u89e3\u51b3\u504f\u7f6e\u95ee\u9898\uff1b3\uff09\u91c7\u7528\u57fa\u4e8e\u89d2\u5ea6\u7684\u9057\u5fd8\u7f13\u89e3\u65b9\u6cd5\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u548c\u975e\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u8111\u4fe1\u53f7\u8870\u51cf\u5e26\u6765\u7684\u8868\u793a\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u7684BRAIN\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7d2f\u79ef\u504f\u7f6e\u548c\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3a\u89c6\u89c9-\u8111\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
