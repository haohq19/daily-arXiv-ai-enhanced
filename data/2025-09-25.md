<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: iFinder是一个结构化语义基础框架，通过将行车记录仪视频转换为分层可解释的数据结构，将感知与推理解耦，从而在零样本驾驶视频理解任务中显著优于端到端视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在领域特定任务（如行车记录仪视频分析）中缺乏结构化归纳偏置，现有视频视觉语言模型在空间推理、因果推理和事件可解释性方面存在困难。

Method: iFinder采用模块化、无需训练的流程，使用预训练视觉模型提取关键线索（物体姿态、车道位置、物体轨迹），并分层组织成帧级和视频级结构，结合三块提示策略实现逐步推理。

Result: 在四个公开行车记录仪视频基准测试中，iFinder在零样本驾驶基准上显著优于端到端视觉语言模型，事故推理准确率提升高达39%。

Conclusion: 通过将LLMs与驾驶领域特定表示相结合，iFinder为零样本、可解释和可靠的行车记录仪视频理解提供了替代端到端视觉语言模型的方案。

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [2] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: 提出FMISeg模型，通过频域多模态交互实现语言引导的医学图像分割，在肺部感染区域分割任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效融合视觉和语言模态，无法充分增强视觉特征表示和消除语义无关信息，导致分割性能不理想

Method: FMISeg是一种后融合模型，在解码器中建立语言特征与频域视觉特征的交互，包含频域特征双向交互模块和语言引导频域特征交互模块

Result: 在QaTa-COV19和MosMedData+数据集上的实验表明，该方法在定性和定量评估上都优于最先进的方法

Conclusion: FMISeg通过频域多模态交互有效提升了语言引导医学图像分割的性能

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models](https://arxiv.org/abs/2509.19465)
*Kin G. Olivares,Malcolm Wolff,Tatiana Konstantinova,Shankar Ramasubramanian,Andrew Gordon Wilson,Andres Potapczynski,Willa Potosnak,Mengfei Cao,Boris Oreshkin,Dmitry Efimov*

Main category: cs.LG

TL;DR: 本文分析了跨频迁移学习（CFTL）在时间序列预测中的基准评估问题，发现当前评估方法存在多个缺陷，并通过大规模实验证明统计模型在多个数据集上优于现有的基础预测模型。


<details>
  <summary>Details</summary>
Motivation: 当前CFTL基准评估存在多个问题：过度依赖小规模评估数据集、样本量处理不当、报告次优统计模型、未能考虑预训练和测试数据集重叠的风险。这些缺陷导致对CFTL性能的评估不准确。

Method: 重新实现了广泛采用的神经预测网络并适配CFTL设置；仅在专有和合成数据上进行预训练，避免测试泄漏；在15个大型多样的公共预测竞赛数据集上进行评估。

Result: 统计模型的准确性经常被低估。统计模型及其集成在sCRPS上比现有FFMs高出8.2%以上，在MASE上高出20%以上。但合成数据集预训练确实能将FFM的准确性提高7%。

Conclusion: 当前CFTL基准评估存在严重缺陷，统计模型在实际应用中表现优于神经基础预测模型。合成数据预训练对FFMs有积极影响，但整体上传统统计方法仍更具优势。

Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework
for curating large-scale time series datasets to pre-train foundation
forecasting models (FFMs). Although CFTL has shown promise, current
benchmarking practices fall short of accurately assessing its performance. This
shortcoming stems from many factors: an over-reliance on small-scale evaluation
datasets; inadequate treatment of sample size when computing summary
statistics; reporting of suboptimal statistical models; and failing to account
for non-negligible risks of overlap between pre-training and test datasets. To
address these limitations, we introduce a unified reimplementation of
widely-adopted neural forecasting networks, adapting them for the CFTL setup;
we pre-train only on proprietary and synthetic data, being careful to prevent
test leakage; and we evaluate on 15 large, diverse public forecast competition
datasets. Our empirical analysis reveals that statistical models' accuracy is
frequently underreported. Notably, we confirm that statistical models and their
ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and
by more than 20% MASE, across datasets. However, we also find that synthetic
dataset pre-training does improve the accuracy of a FFM by 7% percent.

</details>


### [4] [Metriplectic Conditional Flow Matching for Dissipative Dynamics](https://arxiv.org/abs/2509.19526)
*Ali Baheri,Lars Lindemann*

Main category: cs.LG

TL;DR: MCFM是一种学习耗散动力学的方法，通过将保守-耗散分裂构建到向量场和结构保持采样器中，确保不违反第一原理，从而在长期推演中保持稳定。


<details>
  <summary>Details</summary>
Motivation: 神经代理模型在长期推演中常常注入能量导致不稳定，需要一种能够保持物理第一原理的方法来学习耗散动力学。

Method: MCFM通过条件流匹配在短时转换上训练，避免长推演伴随计算。在推理时使用Strang-prox方案交替进行辛更新和近端度量步骤，确保离散能量衰减。

Result: 在受控机械基准测试中，MCFM产生的相图更接近真实情况，能量增加和正能量率事件显著少于同等表达能力的无约束神经流，同时匹配终端分布拟合。

Conclusion: MCFM提供了连续和离散时间保证，将参数化和采样器与守恒、单调耗散和稳定推演联系起来，是一种有效的结构保持学习方法。

Abstract: Metriplectic conditional flow matching (MCFM) learns dissipative dynamics
without violating first principles. Neural surrogates often inject energy and
destabilize long-horizon rollouts; MCFM instead builds the
conservative-dissipative split into both the vector field and a structure
preserving sampler. MCFM trains via conditional flow matching on short
transitions, avoiding long rollout adjoints. In inference, a Strang-prox scheme
alternates a symplectic update with a proximal metric step, ensuring discrete
energy decay; an optional projection enforces strict decay when a trusted
energy is available. We provide continuous and discrete time guarantees linking
this parameterization and sampler to conservation, monotonic dissipation, and
stable rollouts. On a controlled mechanical benchmark, MCFM yields phase
portraits closer to ground truth and markedly fewer energy-increase and
positive energy rate events than an equally expressive unconstrained neural
flow, while matching terminal distributional fit.

</details>


### [5] [Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion](https://arxiv.org/abs/2509.19661)
*Puning Zhao,Zhikun Zhang,Bo Sun,Li Shen,Liang Zhang,Shaowei Wang,Zhe Liu*

Main category: cs.LG

TL;DR: 提出一种基于小波展开的局部差分隐私下的分布估计方法，通过优先估计低阶小波系数来防止概率质量被错误分配到远离真实值的位置。


<details>
  <summary>Details</summary>
Motivation: 现有的局部差分隐私分布估计方法主要针对分类数据，在数值数据上效果不佳，特别是无法有效防止概率质量被错误分配到远离真实值的位置。

Method: 使用小波展开表示样本分布，在局部差分隐私约束下估计小波系数，优先估计低阶系数以确保宏观层面的准确估计。

Result: 理论分析证明了方法的有效性，实验表明在小波展开方法在Wasserstein距离和KS距离上显著优于现有解决方案。

Conclusion: 小波展开方法为局部差分隐私下的数值数据分布估计提供了有效的解决方案，能够更好地保持分布的宏观结构特征。

Abstract: Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.

</details>


### [6] [Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods](https://arxiv.org/abs/2509.19750)
*Kainat*

Main category: cs.LG

TL;DR: 本研究提出了一种基于BERT回归模型的新方法，利用语音信号无创预测动脉血压，为心血管健康监测提供了创新解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统袖带式血压测量方法存在白大褂高血压和隐匿性高血压等问题，结果不一致。需要一种更舒适、准确的实时监测方法。

Method: 利用BERT模型分析语音信号的声学特征，建立语音特征与血压水平之间的相关性，通过深度学习技术提取相关模式。

Result: 在95名参与者的数据集上，模型表现出色：收缩压MAE为13.6 mmHg（R²=0.99），舒张压MAE为12.4 mmHg（R²=0.94），训练和验证损失分析显示有效学习且过拟合最小。

Conclusion: 深度学习与语音分析的结合为血压监测提供了可行的替代方案，在远程医疗和健康监测领域具有重要应用前景，能够提升患者护理和心血管健康管理。

Abstract: This research presents a novel method for noninvasive arterial blood pressure
ABP prediction using speech signals employing a BERT based regression model
Arterial blood pressure is a vital indicator of cardiovascular health and
accurate monitoring is essential in preventing hypertension related
complications Traditional cuff based methods often yield inconsistent results
due to factors like whitecoat and masked hypertension Our approach leverages
the acoustic characteristics of speech capturing voice features to establish
correlations with blood pressure levels Utilizing advanced deep learning
techniques we analyze speech signals to extract relevant patterns enabling real
time monitoring without the discomfort of conventional methods In our study we
employed a dataset comprising recordings from 95 participants ensuring diverse
representation The BERT model was fine tuned on extracted features from speech
leading to impressive performance metrics achieving a mean absolute error MAE
of 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood
pressure DBP with R scores of 099 and 094 respectively These results indicate
the models robustness in accurately predicting blood pressure levels
Furthermore the training and validation loss analysis demonstrates effective
learning and minimal overfitting Our findings suggest that integrating deep
learning with speech analysis presents a viable alternative for blood pressure
monitoring paving the way for improved applications in telemedicine and remote
health monitoring By providing a user friendly and accurate method for blood
pressure assessment this research has significant implications for enhancing
patient care and proactive management of cardiovascular health

</details>


### [7] [Frictional Q-Learning](https://arxiv.org/abs/2509.19771)
*Hyunwoo Kim,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: 提出Frictional Q-learning算法，通过类比经典力学中的静摩擦力与离策略强化学习中的外推误差，约束智能体动作空间以防止策略漂移到不受支持的动作区域。


<details>
  <summary>Details</summary>
Motivation: 解决离策略强化学习中的外推误差问题，防止策略在训练过程中偏离经验回放缓冲区中支持的动作分布。

Method: 扩展批量约束强化学习，在约束智能体动作空间的同时保持与正交动作空间流形的距离，鼓励行为与回放缓冲区相似。

Result: 算法具有鲁棒的训练特性，在标准连续控制基准测试中取得了有竞争力的性能表现。

Conclusion: Frictional Q-learning通过物理启发的约束机制有效解决了外推误差问题，同时保持了算法的简洁性和直观的物理解释。

Abstract: We draw an analogy between static friction in classical mechanics and
extrapolation error in off-policy RL, and use it to formulate a constraint that
prevents the policy from drifting toward unsupported actions. In this study, we
present Frictional Q-learning, a deep reinforcement learning algorithm for
continuous control, which extends batch-constrained reinforcement learning. Our
algorithm constrains the agent's action space to encourage behavior similar to
that in the replay buffer, while maintaining a distance from the manifold of
the orthonormal action space. The constraint preserves the simplicity of
batch-constrained, and provides an intuitive physical interpretation of
extrapolation error. Empirically, we further demonstrate that our algorithm is
robustly trained and achieves competitive performance across standard
continuous control benchmarks.

</details>


### [8] [PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection](https://arxiv.org/abs/2509.19774)
*Xiaocheng Fang,Jiarui Jin,Haoyu Wang,Che Liu,Jieyi Cai,Guangkun Nie,Jun Li,Hongyan Li,Shenda Hong*

Main category: cs.LG

TL;DR: PPGFlowECG是一个两阶段框架，通过CardioAlign编码器在共享潜在空间中对齐PPG和ECG信号，并使用潜在整流流生成高保真度的ECG信号，解决了PPG到ECG转换中的生理语义对齐问题。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）是心脏监测的金标准，但依赖专业设备和人员限制了连续监测的可行性。光电容积脉搏波（PPG）虽然可连续监测，但缺乏明确的电生理信息，无法进行确定性诊断。生成模型有望将PPG转换为临床有价值的ECG信号，但现有方法面临生理语义不对齐和高维信号建模复杂性的挑战。

Method: 提出PPGFlowECG两阶段框架：1）使用CardioAlign编码器在共享潜在空间中对齐PPG和ECG信号；2）应用潜在整流流生成高保真度的ECG信号。该方法在MCMED数据集上进行实验，这是包含超过1000万对PPG-ECG样本的新临床数据集。

Result: 实验结果表明该方法在PPG到ECG转换和心血管疾病检测方面具有有效性。心脏病专家评估确认合成的ECG达到高保真度并提高了诊断可靠性。

Conclusion: PPGFlowECG框架在心血管筛查方面具有实际应用潜力，能够生成高质量的ECG信号用于临床诊断。

Abstract: In clinical practice, electrocardiography (ECG) remains the gold standard for
cardiac monitoring, providing crucial insights for diagnosing a wide range of
cardiovascular diseases (CVDs). However, its reliance on specialized equipment
and trained personnel limits feasibility for continuous routine monitoring.
Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks
definitive electrophysiological information, preventing conclusive diagnosis.
Generative models present a promising approach to translate PPG into clinically
valuable ECG signals, yet current methods face substantial challenges,
including the misalignment of physiological semantics in generative models and
the complexity of modeling in high-dimensional signals. To this end, we propose
PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent
space via the CardioAlign Encoder and employs latent rectified flow to generate
ECGs with high fidelity and interpretability. To the best of our knowledge,
this is the first study to experiment on MCMED, a newly released clinical-grade
dataset comprising over 10 million paired PPG-ECG samples from more than
118,000 emergency department visits with expert-labeled cardiovascular disease
annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG
translation and cardiovascular disease detection. Moreover, cardiologist-led
evaluations confirm that the synthesized ECGs achieve high fidelity and improve
diagnostic reliability, underscoring our method's potential for real-world
cardiovascular screening.

</details>


### [9] [Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials](https://arxiv.org/abs/2509.19877)
*Shi Yin,Zujian Dai,Xinyang Pan,Lixin He*

Main category: cs.LG

TL;DR: NextHAM是一种用于材料电子结构哈密顿量预测的神经E(3)对称性和表达性校正方法，通过引入零阶哈密顿量、Transformer架构和新的训练目标，在Materials-HAM-SOC数据集上实现了优异的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统DFT方法计算电子结构哈密顿量计算成本高，而现有深度学习方法在原子类型多样性和哈密顿量高维复杂性方面面临泛化性能挑战。

Method: 1) 引入零阶哈密顿量作为神经回归模型的输入描述符和初始估计；2) 设计具有严格E(3)对称性的神经Transformer架构；3) 提出新的训练目标确保实空间和倒空间哈密顿量的准确性。

Result: 在包含17,000个材料结构的Materials-HAM-SOC基准测试中，NextHAM在哈密顿量和能带结构预测方面表现出优异的准确性和效率。

Conclusion: NextHAM通过方法论创新和高质量数据集构建，推进了哈密顿量预测的通用深度学习范式，解决了传统方法面临的泛化挑战。

Abstract: Deep learning methods for electronic-structure Hamiltonian prediction has
offered significant computational efficiency advantages over traditional DFT
methods, yet the diversity of atomic types, structural patterns, and the
high-dimensional complexity of Hamiltonians pose substantial challenges to the
generalization performance. In this work, we contribute on both the methodology
and dataset sides to advance universal deep learning paradigm for Hamiltonian
prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and
expressive correction method for efficient and generalizable materials
electronic-structure Hamiltonian prediction. First, we introduce the
zeroth-step Hamiltonians, which can be efficiently constructed by the initial
charge density of DFT, as informative descriptors of neural regression model in
the input level and initial estimates of the target Hamiltonian in the output
level, so that the regression model directly predicts the correction terms to
the target ground truths, thereby significantly simplifying the input-output
mapping for learning. Second, we present a neural Transformer architecture with
strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian
prediction. Third, we propose a novel training objective to ensure the accuracy
performance of Hamiltonians in both real space and reciprocal space, preventing
error amplification and the occurrence of "ghost states" caused by the large
condition number of the overlap matrix. On the dataset side, we curate a
high-quality broad-coverage large benchmark, namely Materials-HAM-SOC,
comprising 17,000 material structures spanning 68 elements from six rows of the
periodic table and explicitly incorporating SOC effects. Experimental results
on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and
efficiency in predicting Hamiltonians and band structures.

</details>


### [10] [Failure Modes of Maximum Entropy RLHF](https://arxiv.org/abs/2509.20265)
*Ömer Veysel Çağatan,Barış Akgün*

Main category: cs.LG

TL;DR: 本文证明了Simple Preference Optimization (SimPO)可推导为具有长度归一化温度的最大熵强化学习，为这种无参考方法提供了理论基础。研究发现最大熵RL在在线RLHF设置中会出现过优化和不稳定的KL动态，而SimPO在离线设置中表现良好。


<details>
  <summary>Details</summary>
Motivation: 受SimPO在离线偏好优化中强大表现的启发，研究最大熵RL是否能在在线RLHF设置中取得类似结果。

Method: 通过实验比较最大熵RL和KL约束方法在在线RLHF设置中的表现，分析训练稳定性和过优化现象。

Result: 最大熵RL在在线设置中持续表现出过优化和不稳定的KL动态，即使学习率很低；而KL约束方法能保持稳定训练。熵正则化无法防止奖励黑客攻击，且与过优化相关。

Conclusion: SimPO在离线设置中成功而最大熵RL在在线场景中困难的原因可能是无参考方法在在线和离线偏好学习中面临不同的挑战。

Abstract: In this paper, we show that Simple Preference Optimization (SimPO) can be
derived as Maximum Entropy Reinforcement Learning with length-normalized
temperature, providing a theoretical foundation for this reference-free method.
Motivated by SimPO's strong performance in offline preference optimization, we
investigate whether Maximum Entropy RL can achieve similar results in online
RLHF settings. Our experiments find that Maximum Entropy RL consistently
exhibits overoptimization and unstable KL dynamics, even at very low learning
rates. Unlike KL-constrained methods that maintain stable training, entropy
regularization fails to prevent reward hacking and appears to correlate with
overoptimization. Lastly, we discuss possible explanations for why SimPO
succeeds in offline settings while Maximum Entropy RL struggles in online
scenarios. Our findings suggest that reference-free approaches may face
distinct challenges when applied to online or offline preference learning.

</details>


### [11] [Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation](https://arxiv.org/abs/2509.20269)
*Matteo Cardoni,Sam Leroux*

Main category: cs.LG

TL;DR: 提出了一种结合反向传播和预测编码的混合训练方法，用于在资源受限的边缘设备上实现高效的在线域适应。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在动态现实环境中部署时，由于输入数据分布变化（如传感器漂移、光照变化）导致的性能下降问题，需要持续模型适应。

Method: 首先使用反向传播离线训练深度神经网络获得高初始性能，然后使用预测编码进行在线适应，以恢复因输入数据分布变化而损失的准确性。

Result: 在MNIST和CIFAR-10数据集上的实验结果表明，该混合策略能够以较低的计算开销实现有效的适应。

Conclusion: 该方法结合了反向传播在初始表示学习中的鲁棒性和预测编码在持续学习中的计算效率，为动态环境中保持模型性能提供了有前景的解决方案。

Abstract: As deep neural networks are increasingly deployed in dynamic, real-world
environments, relying on a single static model is often insufficient. Changes
in input data distributions caused by sensor drift or lighting variations
necessitate continual model adaptation. In this paper, we propose a hybrid
training methodology that enables efficient on-device domain adaptation by
combining the strengths of Backpropagation and Predictive Coding. The method
begins with a deep neural network trained offline using Backpropagation to
achieve high initial performance. Subsequently, Predictive Coding is employed
for online adaptation, allowing the model to recover accuracy lost due to
shifts in the input data distribution. This approach leverages the robustness
of Backpropagation for initial representation learning and the computational
efficiency of Predictive Coding for continual learning, making it particularly
well-suited for resource-constrained edge devices or future neuromorphic
accelerators. Experimental results on the MNIST and CIFAR-10 datasets
demonstrate that this hybrid strategy enables effective adaptation with a
reduced computational overhead, offering a promising solution for maintaining
model performance in dynamic environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: CON-QA是一个混合隐私保护框架，专门用于企业合同的安全问答，结合本地和云端LLM来保护敏感信息


<details>
  <summary>Details</summary>
Motivation: 企业越来越多地将云端LLM集成到法律文档工作流中，保护敏感合同信息（包括个人身份信息和商业敏感条款）已成为关键挑战

Method: CON-QA框架通过三个阶段运作：语义查询分解和查询感知文档块检索、敏感实体匿名化、匿名化响应生成和原始答案重建

Result: 经验评估和详细人工评估证实CON-QA有效维护隐私和实用性，保持答案质量，维护法律条款语义保真度，显著降低隐私风险

Conclusion: CON-QA展示了其在安全企业级合同文档中的实际适用性

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
*Jiawei Lian,Jianhong Pan,Lefan Wang,Yi Wang,Shaohui Mei,Lap-Pui Chau*

Main category: cs.CL

TL;DR: 本文提出了一种新的语义表示攻击方法，通过利用语义表示空间而非精确文本模式来绕过LLM的对齐防护，解决了现有方法在攻击效果和提示自然性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前针对对齐LLM的攻击方法通常针对精确的肯定响应模式，存在收敛有限、提示不自然和计算成本高的问题。需要一种新的攻击范式来克服这些限制。

Method: 提出了语义表示攻击范式，利用包含等效有害含义的多样化响应的语义表示空间。开发了语义表示启发式搜索算法，通过保持可解释性的增量扩展来生成语义连贯且简洁的对抗提示。

Result: 该方法在18个LLM上实现了前所未有的攻击成功率（平均89.41%，其中11个模型达到100%），同时保持了隐蔽性和效率。

Conclusion: 语义表示攻击在攻击效果、提示自然性和计算效率方面具有整体优势，为对抗对齐LLM提供了新的有效方法。

Abstract: Large Language Models (LLMs) increasingly employ alignment techniques to
prevent harmful outputs. Despite these safeguards, attackers can circumvent
them by crafting prompts that induce LLMs to generate harmful content.
  Current methods typically target exact affirmative responses, such as ``Sure,
here is...'', suffering from limited convergence, unnatural prompts, and high
computational costs.
  We introduce Semantic Representation Attack, a novel paradigm that
fundamentally reconceptualizes adversarial objectives against aligned LLMs.
  Rather than targeting exact textual patterns, our approach exploits the
semantic representation space comprising diverse responses with equivalent
harmful meanings.
  This innovation resolves the inherent trade-off between attack efficacy and
prompt naturalness that plagues existing methods.
  The Semantic Representation Heuristic Search algorithm is proposed to
efficiently generate semantically coherent and concise adversarial prompts by
maintaining interpretability during incremental expansion.
  We establish rigorous theoretical guarantees for semantic convergence and
demonstrate that our method achieves unprecedented attack success rates
(89.41\% averaged across 18 LLMs, including 100\% on 11 models) while
maintaining stealthiness and efficiency.
  Comprehensive experimental results confirm the overall superiority of our
Semantic Representation Attack.
  The code will be publicly available.

</details>


### [14] [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
*Ryan Shea,Zhou Yu*

Main category: cs.CL

TL;DR: AutoSpec是一个安全、智能的框架，用于自动起草专利说明书，通过将起草过程分解为可管理的子任务，使用开源语言模型和定制工具来解决专利起草的挑战。


<details>
  <summary>Details</summary>
Motivation: 专利起草过程昂贵且耗时，但现有语言模型面临信息保密性、长文本处理和技术写作等挑战，需要开发安全的自动化解决方案。

Method: 将专利起草过程分解为序列化子任务，每个子任务由增强定制工具的开源小语言模型解决，确保安全性和专业性。

Result: 通过与专利律师合作设计的新型评估协议，自动和专家评估显示AutoSpec在专利起草任务上优于现有基线方法。

Conclusion: AutoSpec框架成功解决了专利起草自动化的关键挑战，为安全高效的专利自动化提供了可行方案。

Abstract: Patents play a critical role in driving technological innovation by granting
inventors exclusive rights to their inventions. However the process of drafting
a patent application is often expensive and time-consuming, making it a prime
candidate for automation. Despite recent advancements in language models,
several challenges hinder the development of robust automated patent drafting
systems. First, the information within a patent application is highly
confidential, which often prevents the use of closed-source LLMs for automating
this task. Second, the process of drafting a patent application is difficult
for even the most advanced language models due to their long context, technical
writing style, and specialized domain knowledge. To address these challenges,
we introduce AutoSpec, a secure, agentic framework for Automatically drafting
patent Specification. Our approach decomposes the drafting process into a
sequence of manageable subtasks, each solvable by smaller, open-source language
models enhanced with custom tools tailored for drafting patent specification.
To assess our system, we design a novel evaluation protocol in collaboration
with experienced patent attorneys. Our automatic and expert evaluations show
that AutoSpec outperforms existing baselines on a patent drafting task.

</details>


### [15] [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
*Chaojun Nie,Jun Zhou,Guanxiang Wang,Shisong Wud,Zichen Wang*

Main category: cs.CL

TL;DR: 提出RLAG方法解决LLMs在领域特定任务中的知识稀缺和时间滞后问题，通过强化学习从增强生成中嵌入关键领域知识


<details>
  <summary>Details</summary>
Motivation: LLMs在领域特定任务中表现有限，因为训练数据中专业信息代表性不足且数据集静态，导致知识差距。现有方法如CPT和SFT存在局限性

Method: 提出RLAG方法，在采样生成和模型优化之间迭代循环，通过计算奖励来有效嵌入关键且上下文连贯的领域知识。选择最高对数概率的生成输出作为采样结果，计算三个定制奖励指标指导优化

Result: 在医学、法律、天文学和时事数据集上的实验结果表明，该方法显著优于基线方法

Conclusion: RLAG方法能有效解决LLMs在领域应用中的知识稀缺问题，提升领域专业知识的准确性和解释合理性

Abstract: Large language models (LLMs) often exhibit limited performance on
domain-specific tasks due to the natural disproportionate representation of
specialized information in their training data and the static nature of these
datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain
applications. While post-training on domain datasets can embed knowledge into
models, existing approaches have some limitations. Continual Pre-Training (CPT)
treats all tokens in domain documents with equal importance, failing to
prioritize critical knowledge points, while supervised fine-tuning (SFT) with
question-answer pairs struggles to develop the coherent knowledge structures
necessary for complex reasoning tasks. To address these challenges, we propose
Reinforcement Learning from Augmented Generation (RLAG). Our approach
iteratively cycles between sampling generations and optimizing the model
through calculated rewards, effectively embedding critical and contextually
coherent domain knowledge. We select generated outputs with the highest log
probabilities as the sampling result, then compute three tailored reward
metrics to guide the optimization process. To comprehensively evaluate domain
expertise, we assess answer accuracy and the rationality of explanations
generated for correctly answered questions. Experimental results across
medical, legal, astronomy, and current events datasets demonstrate that our
proposed method significantly outperforms baseline approaches. Our code and
data are open sourced at https://github.com/ChaojunNie/RLAG.

</details>


### [16] [Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian](https://arxiv.org/abs/2509.20168)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本文提出了一种基于模板的探测方法，用于评估多语言大语言模型中的性别偏见，特别关注低资源语言波斯语，并引入了领域特定性别偏斜指数(DS-GSI)来量化偏见程度。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注高资源语言中的性别偏见，但低资源语言仍缺乏充分研究。为确保多语言LLM的公平性，需要开发有效方法来检测和量化这些语言中的性别刻板印象。

Method: 采用模板化探测方法，通过真实数据验证，评估GPT-4o mini、DeepSeek R1、Gemini 2.0 Flash和Qwen QwQ 32B四个模型在四个语义领域中的表现，重点关注波斯语。

Result: 所有模型都表现出性别刻板印象，波斯语中的性别偏见比英语更严重，其中体育领域显示出最严重的性别偏见。

Conclusion: 研究强调了包容性NLP实践的必要性，并为评估其他低资源语言的偏见提供了框架。

Abstract: Multilingual Large Language Models (LLMs) are increasingly used worldwide,
making it essential to ensure they are free from gender bias to prevent
representational harm. While prior studies have examined such biases in
high-resource languages, low-resource languages remain understudied. In this
paper, we propose a template-based probing methodology, validated against
real-world data, to uncover gender stereotypes in LLMs. As part of this
framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a
metric that quantifies deviations from gender parity. We evaluate four
prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,
across four semantic domains, focusing on Persian, a low-resource language with
distinct linguistic features. Our results show that all models exhibit gender
stereotypes, with greater disparities in Persian than in English across all
domains. Among these, sports reflect the most rigid gender biases. This study
underscores the need for inclusive NLP practices and provides a framework for
assessing bias in other low-resource languages.

</details>
