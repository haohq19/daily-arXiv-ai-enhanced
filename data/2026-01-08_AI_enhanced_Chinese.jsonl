{"id": "2601.03267", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03267", "abs": "https://arxiv.org/abs/2601.03267", "authors": ["Aaditya Singh", "Adam Fry", "Adam Perelman", "Adam Tart", "Adi Ganesh", "Ahmed El-Kishky", "Aidan McLaughlin", "Aiden Low", "AJ Ostrow", "Akhila Ananthram", "Akshay Nathan", "Alan Luo", "Alec Helyar", "Aleksander Madry", "Aleksandr Efremov", "Aleksandra Spyra", "Alex Baker-Whitcomb", "Alex Beutel", "Alex Karpenko", "Alex Makelov", "Alex Neitz", "Alex Wei", "Alexandra Barr", "Alexandre Kirchmeyer", "Alexey Ivanov", "Alexi Christakis", "Alistair Gillespie", "Allison Tam", "Ally Bennett", "Alvin Wan", "Alyssa Huang", "Amy McDonald Sandjideh", "Amy Yang", "Ananya Kumar", "Andre Saraiva", "Andrea Vallone", "Andrei Gheorghe", "Andres Garcia Garcia", "Andrew Braunstein", "Andrew Liu", "Andrew Schmidt", "Andrey Mereskin", "Andrey Mishchenko", "Andy Applebaum", "Andy Rogerson", "Ann Rajan", "Annie Wei", "Anoop Kotha", "Anubha Srivastava", "Anushree Agrawal", "Arun Vijayvergiya", "Ashley Tyra", "Ashvin Nair", "Avi Nayak", "Ben Eggers", "Bessie Ji", "Beth Hoover", "Bill Chen", "Blair Chen", "Boaz Barak", "Borys Minaiev", "Botao Hao", "Bowen Baker", "Brad Lightcap", "Brandon McKinzie", "Brandon Wang", "Brendan Quinn", "Brian Fioca", "Brian Hsu", "Brian Yang", "Brian Yu", "Brian Zhang", "Brittany Brenner", "Callie Riggins Zetino", "Cameron Raymond", "Camillo Lugaresi", "Carolina Paz", "Cary Hudson", "Cedric Whitney", "Chak Li", "Charles Chen", "Charlotte Cole", "Chelsea Voss", "Chen Ding", "Chen Shen", "Chengdu Huang", "Chris Colby", "Chris Hallacy", "Chris Koch", "Chris Lu", "Christina Kaplan", "Christina Kim", "CJ Minott-Henriques", "Cliff Frey", "Cody Yu", "Coley Czarnecki", "Colin Reid", "Colin Wei", "Cory Decareaux", "Cristina Scheau", "Cyril Zhang", "Cyrus Forbes", "Da Tang", "Dakota Goldberg", "Dan Roberts", "Dana Palmie", "Daniel Kappler", "Daniel Levine", "Daniel Wright", "Dave Leo", "David Lin", "David Robinson", "Declan Grabb", "Derek Chen", "Derek Lim", "Derek Salama", "Dibya Bhattacharjee", "Dimitris Tsipras", "Dinghua Li", "Dingli Yu", "DJ Strouse", "Drew Williams", "Dylan Hunn", "Ed Bayes", "Edwin Arbus", "Ekin Akyurek", "Elaine Ya Le", "Elana Widmann", "Eli Yani", "Elizabeth Proehl", "Enis Sert", "Enoch Cheung", "Eri Schwartz", "Eric Han", "Eric Jiang", "Eric Mitchell", "Eric Sigler", "Eric Wallace", "Erik Ritter", "Erin Kavanaugh", "Evan Mays", "Evgenii Nikishin", "Fangyuan Li", "Felipe Petroski Such", "Filipe de Avila Belbute Peres", "Filippo Raso", "Florent Bekerman", "Foivos Tsimpourlas", "Fotis Chantzis", "Francis Song", "Francis Zhang", "Gaby Raila", "Garrett McGrath", "Gary Briggs", "Gary Yang", "Giambattista Parascandolo", "Gildas Chabot", "Grace Kim", "Grace Zhao", "Gregory Valiant", "Guillaume Leclerc", "Hadi Salman", "Hanson Wang", "Hao Sheng", "Haoming Jiang", "Haoyu Wang", "Haozhun Jin", "Harshit Sikchi", "Heather Schmidt", "Henry Aspegren", "Honglin Chen", "Huida Qiu", "Hunter Lightman", "Ian Covert", "Ian Kivlichan", "Ian Silber", "Ian Sohl", "Ibrahim Hammoud", "Ignasi Clavera", "Ikai Lan", "Ilge Akkaya", "Ilya Kostrikov", "Irina Kofman", "Isak Etinger", "Ishaan Singal", "Jackie Hehir", "Jacob Huh", "Jacqueline Pan", "Jake Wilczynski", "Jakub Pachocki", "James Lee", "James Quinn", "Jamie Kiros", "Janvi Kalra", "Jasmyn Samaroo", "Jason Wang", "Jason Wolfe", "Jay Chen", "Jay Wang", "Jean Harb", "Jeffrey Han", "Jeffrey Wang", "Jennifer Zhao", "Jeremy Chen", "Jerene Yang", "Jerry Tworek", "Jesse Chand", "Jessica Landon", "Jessica Liang", "Ji Lin", "Jiancheng Liu", "Jianfeng Wang", "Jie Tang", "Jihan Yin", "Joanne Jang", "Joel Morris", "Joey Flynn", "Johannes Ferstad", "Johannes Heidecke", "John Fishbein", "John Hallman", "Jonah Grant", "Jonathan Chien", "Jonathan Gordon", "Jongsoo Park", "Jordan Liss", "Jos Kraaijeveld", "Joseph Guay", "Joseph Mo", "Josh Lawson", "Josh McGrath", "Joshua Vendrow", "Joy Jiao", "Julian Lee", "Julie Steele", "Julie Wang", "Junhua Mao", "Kai Chen", "Kai Hayashi", "Kai Xiao", "Kamyar Salahi", "Kan Wu", "Karan Sekhri", "Karan Sharma", "Karan Singhal", "Karen Li", "Kenny Nguyen", "Keren Gu-Lemberg", "Kevin King", "Kevin Liu", "Kevin Stone", "Kevin Yu", "Kristen Ying", "Kristian Georgiev", "Kristie Lim", "Kushal Tirumala", "Kyle Miller", "Lama Ahmad", "Larry Lv", "Laura Clare", "Laurance Fauconnet", "Lauren Itow", "Lauren Yang", "Laurentia Romaniuk", "Leah Anise", "Lee Byron", "Leher Pathak", "Leon Maksin", "Leyan Lo", "Leyton Ho", "Li Jing", "Liang Wu", "Liang Xiong", "Lien Mamitsuka", "Lin Yang", "Lindsay McCallum", "Lindsey Held", "Liz Bourgeois", "Logan Engstrom", "Lorenz Kuhn", "Louis Feuvrier", "Lu Zhang", "Lucas Switzer", "Lukas Kondraciuk", "Lukasz Kaiser", "Manas Joglekar", "Mandeep Singh", "Mandip Shah", "Manuka Stratta", "Marcus Williams", "Mark Chen", "Mark Sun", "Marselus Cayton", "Martin Li", "Marvin Zhang", "Marwan Aljubeh", "Matt Nichols", "Matthew Haines", "Max Schwarzer", "Mayank Gupta", "Meghan Shah", "Melody Huang", "Meng Dong", "Mengqing Wang", "Mia Glaese", "Micah Carroll", "Michael Lampe", "Michael Malek", "Michael Sharman", "Michael Zhang", "Michele Wang", "Michelle Pokrass", "Mihai Florian", "Mikhail Pavlov", "Miles Wang", "Ming Chen", "Mingxuan Wang", "Minnia Feng", "Mo Bavarian", "Molly Lin", "Moose Abdool", "Mostafa Rohaninejad", "Nacho Soto", "Natalie Staudacher", "Natan LaFontaine", "Nathan Marwell", "Nelson Liu", "Nick Preston", "Nick Turley", "Nicklas Ansman", "Nicole Blades", "Nikil Pancha", "Nikita Mikhaylin", "Niko Felix", "Nikunj Handa", "Nishant Rai", "Nitish Keskar", "Noam Brown", "Ofir Nachum", "Oleg Boiko", "Oleg Murk", "Olivia Watkins", "Oona Gleeson", "Pamela Mishkin", "Patryk Lesiewicz", "Paul Baltescu", "Pavel Belov", "Peter Zhokhov", "Philip Pronin", "Phillip Guo", "Phoebe Thacker", "Qi Liu", "Qiming Yuan", "Qinghua Liu", "Rachel Dias", "Rachel Puckett", "Rahul Arora", "Ravi Teja Mullapudi", "Raz Gaon", "Reah Miyara", "Rennie Song", "Rishabh Aggarwal", "RJ Marsan", "Robel Yemiru", "Robert Xiong", "Rohan Kshirsagar", "Rohan Nuttall", "Roman Tsiupa", "Ronen Eldan", "Rose Wang", "Roshan James", "Roy Ziv", "Rui Shu", "Ruslan Nigmatullin", "Saachi Jain", "Saam Talaie", "Sam Altman", "Sam Arnesen", "Sam Toizer", "Sam Toyer", "Samuel Miserendino", "Sandhini Agarwal", "Sarah Yoo", "Savannah Heon", "Scott Ethersmith", "Sean Grove", "Sean Taylor", "Sebastien Bubeck", "Sever Banesiu", "Shaokyi Amdo", "Shengjia Zhao", "Sherwin Wu", "Shibani Santurkar", "Shiyu Zhao", "Shraman Ray Chaudhuri", "Shreyas Krishnaswamy", "Shuaiqi", "Xia", "Shuyang Cheng", "Shyamal Anadkat", "Sim\u00f3n Posada Fishman", "Simon Tobin", "Siyuan Fu", "Somay Jain", "Song Mei", "Sonya Egoian", "Spencer Kim", "Spug Golden", "SQ Mah", "Steph Lin", "Stephen Imm", "Steve Sharpe", "Steve Yadlowsky", "Sulman Choudhry", "Sungwon Eum", "Suvansh Sanjeev", "Tabarak Khan", "Tal Stramer", "Tao Wang", "Tao Xin", "Tarun Gogineni", "Taya Christianson", "Ted Sanders", "Tejal Patwardhan", "Thomas Degry", "Thomas Shadwell", "Tianfu Fu", "Tianshi Gao", "Timur Garipov", "Tina Sriskandarajah", "Toki Sherbakov", "Tomer Kaftan", "Tomo Hiratsuka", "Tongzhou Wang", "Tony Song", "Tony Zhao", "Troy Peterson", "Val Kharitonov", "Victoria Chernova", "Vineet Kosaraju", "Vishal Kuo", "Vitchyr Pong", "Vivek Verma", "Vlad Petrov", "Wanning Jiang", "Weixing Zhang", "Wenda Zhou", "Wenlei Xie", "Wenting Zhan", "Wes McCabe", "Will DePue", "Will Ellsworth", "Wulfie Bain", "Wyatt Thompson", "Xiangning Chen", "Xiangyu Qi", "Xin Xiang", "Xinwei Shi", "Yann Dubois", "Yaodong Yu", "Yara Khakbaz", "Yifan Wu", "Yilei Qian", "Yin Tat Lee", "Yinbo Chen", "Yizhen Zhang", "Yizhong Xiong", "Yonglong Tian", "Young Cha", "Yu Bai", "Yu Yang", "Yuan Yuan", "Yuanzhi Li", "Yufeng Zhang", "Yuguang Yang", "Yujia Jin", "Yun Jiang", "Yunyun Wang", "Yushi Wang", "Yutian Liu", "Zach Stubenvoll", "Zehao Dou", "Zheng Wu", "Zhigang Wang"], "title": "OpenAI GPT-5 System Card", "comment": null, "summary": "This is the system card published alongside the OpenAI GPT-5 launch, August 2025.\n  GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say 'think hard about this' in the prompt). The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries.\n  This system card focuses primarily on gpt-5-thinking and gpt-5-main, while evaluations for other models are available in the appendix. The GPT-5 system not only outperforms previous models on benchmarks and answers questions more quickly, but -- more importantly -- is more useful for real-world queries. We've made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, and have leveled up GPT-5's performance in three of ChatGPT's most common uses: writing, coding, and health. All of the GPT-5 models additionally feature safe-completions, our latest approach to safety training to prevent disallowed content.\n  Similarly to ChatGPT agent, we have decided to treat gpt-5-thinking as High capability in the Biological and Chemical domain under our Preparedness Framework, activating the associated safeguards. While we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm -- our defined threshold for High capability -- we have chosen to take a precautionary approach.", "AI": {"tldr": "GPT-5\u662f\u4e00\u4e2a\u7edf\u4e00\u7cfb\u7edf\uff0c\u5305\u542b\u667a\u80fd\u5feb\u901f\u6a21\u578b\u3001\u6df1\u5ea6\u63a8\u7406\u6a21\u578b\u548c\u5b9e\u65f6\u8def\u7531\u5668\uff0c\u901a\u8fc7\u5b89\u5168\u8bad\u7ec3\u51cf\u5c11\u5e7b\u89c9\u548c\u5949\u627f\uff0c\u5728\u5199\u4f5c\u3001\u7f16\u7a0b\u548c\u5065\u5eb7\u9886\u57df\u8868\u73b0\u63d0\u5347\uff0c\u5e76\u5bf9\u751f\u7269\u5316\u5b66\u9886\u57df\u91c7\u53d6\u9884\u9632\u6027\u5b89\u5168\u63aa\u65bd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u66f4\u9ad8\u6548\u3001\u66f4\u5b9e\u7528\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u89e3\u51b3\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3001\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5728\u5173\u952e\u5e94\u7528\u9886\u57df\uff08\u5199\u4f5c\u3001\u7f16\u7a0b\u3001\u5065\u5eb7\uff09\u63d0\u4f9b\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7cfb\u7edf\u67b6\u6784\uff1a1) \u667a\u80fd\u5feb\u901f\u6a21\u578b\u5904\u7406\u5927\u591a\u6570\u95ee\u9898\uff1b2) \u6df1\u5ea6\u63a8\u7406\u6a21\u578b\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff1b3) \u5b9e\u65f6\u8def\u7531\u5668\u6839\u636e\u5bf9\u8bdd\u7c7b\u578b\u3001\u590d\u6742\u5ea6\u3001\u5de5\u5177\u9700\u6c42\u548c\u7528\u6237\u610f\u56fe\u52a8\u6001\u9009\u62e9\u6a21\u578b\uff1b4) \u8def\u7531\u5668\u901a\u8fc7\u7528\u6237\u5207\u6362\u884c\u4e3a\u3001\u504f\u597d\u8bc4\u5206\u548c\u6b63\u786e\u6027\u6d4b\u91cf\u8fdb\u884c\u6301\u7eed\u8bad\u7ec3\uff1b5) \u4f7f\u7528\u5b89\u5168\u5b8c\u6210\u6280\u672f\u9632\u6b62\u7981\u6b62\u5185\u5bb9\uff1b6) \u5bf9\u751f\u7269\u5316\u5b66\u9886\u57df\u91c7\u53d6\u9ad8\u7ea7\u522b\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "result": "GPT-5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5148\u524d\u6a21\u578b\uff0c\u54cd\u5e94\u901f\u5ea6\u66f4\u5feb\uff0c\u5bf9\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u66f4\u6709\u7528\uff1b\u5728\u51cf\u5c11\u5e7b\u89c9\u3001\u6539\u8fdb\u6307\u4ee4\u9075\u5faa\u3001\u51cf\u5c11\u5949\u627f\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff1b\u5728\u5199\u4f5c\u3001\u7f16\u7a0b\u548c\u5065\u5eb7\u8fd9\u4e09\u4e2aChatGPT\u6700\u5e38\u7528\u9886\u57df\u8868\u73b0\u63d0\u5347\uff1b\u5efa\u7acb\u4e86\u751f\u7269\u5316\u5b66\u9886\u57df\u7684\u9ad8\u7ea7\u5b89\u5168\u9632\u62a4\u6846\u67b6\u3002", "conclusion": "GPT-5\u901a\u8fc7\u521b\u65b0\u7684\u7edf\u4e00\u67b6\u6784\u548c\u6301\u7eed\u5b66\u4e60\u673a\u5236\uff0c\u5728\u6027\u80fd\u3001\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u5e94\u7528\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u91c7\u53d6\u9884\u9632\u6027\u5b89\u5168\u63aa\u65bd\u5e94\u5bf9\u6f5c\u5728\u98ce\u9669\uff0c\u4ee3\u8868\u4e86AI\u7cfb\u7edf\u8bbe\u8ba1\u7684\u91cd\u8981\u8fdb\u6b65\u3002"}}
{"id": "2601.03327", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03327", "abs": "https://arxiv.org/abs/2601.03327", "authors": ["Nicolas Caron", "Christophe Guyeux", "Hassan Noura", "Benjamin Aynes"], "title": "Extreme-value forest fire prediction A study of the Loss Function in an Ordinality Scheme", "comment": null, "summary": "Wildfires are highly imbalanced natural hazards in both space and severity, making the prediction of extreme events particularly challenging. In this work, we introduce the first ordinal classification framework for forecasting wildfire severity levels directly aligned with operational decision-making in France. Our study investigates the influence of loss-function design on the ability of neural models to predict rare yet critical high-severity fire occurrences. We compare standard cross-entropy with several ordinal-aware objectives, including the proposed probabilistic TDeGPD loss derived from a truncated discrete exponentiated Generalized Pareto Distribution. Through extensive benchmarking over multiple architectures and real operational data, we show that ordinal supervision substantially improves model performance over conventional approaches. In particular, the Weighted Kappa Loss (WKLoss) achieves the best overall results, with more than +0.1 IoU gain on the most extreme severity classes while maintaining competitive calibration quality. However, performance remains limited for the rarest events due to their extremely low representation in the dataset. These findings highlight the importance of integrating both severity ordering, data imbalance considerations, and seasonality risk into wildfire forecasting systems. Future work will focus on incorporating seasonal dynamics and uncertainty information into training to further improve the reliability of extreme-event prediction.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7528\u4e8e\u6cd5\u56fd\u91ce\u706b\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u7684\u5e8f\u6570\u5206\u7c7b\u6846\u67b6\uff0c\u6bd4\u8f83\u4e0d\u540c\u635f\u5931\u51fd\u6570\u5bf9\u7f55\u89c1\u9ad8\u4e25\u91cd\u5ea6\u706b\u707e\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5e8f\u6570\u76d1\u7763\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u91ce\u706b\u5728\u7a7a\u95f4\u548c\u4e25\u91cd\u7a0b\u5ea6\u4e0a\u9ad8\u5ea6\u4e0d\u5e73\u8861\uff0c\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u5f00\u53d1\u4e0e\u6cd5\u56fd\u8fd0\u8425\u51b3\u7b56\u76f4\u63a5\u5bf9\u9f50\u7684\u91ce\u706b\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u6846\u67b6\uff0c\u7279\u522b\u5173\u6ce8\u7f55\u89c1\u4f46\u5173\u952e\u7684\u9ad8\u4e25\u91cd\u5ea6\u706b\u707e\u9884\u6d4b", "method": "\u5f15\u5165\u9996\u4e2a\u91ce\u706b\u4e25\u91cd\u7a0b\u5ea6\u5e8f\u6570\u5206\u7c7b\u6846\u67b6\uff0c\u6bd4\u8f83\u6807\u51c6\u4ea4\u53c9\u71b5\u4e0e\u591a\u79cd\u5e8f\u6570\u611f\u77e5\u76ee\u6807\u51fd\u6570\uff0c\u5305\u62ec\u63d0\u51fa\u7684\u57fa\u4e8e\u622a\u65ad\u79bb\u6563\u6307\u6570\u5e7f\u4e49\u5e15\u7d2f\u6258\u5206\u5e03\u7684\u6982\u7387TDeGPD\u635f\u5931\u3002\u5728\u591a\u79cd\u67b6\u6784\u548c\u771f\u5b9e\u8fd0\u8425\u6570\u636e\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5", "result": "\u5e8f\u6570\u76d1\u7763\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u52a0\u6743Kappa\u635f\u5931\uff08WKLoss\uff09\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6700\u6781\u7aef\u4e25\u91cd\u5ea6\u7c7b\u522b\u4e0a\u83b7\u5f97\u8d85\u8fc7+0.1 IoU\u589e\u76ca\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u6821\u51c6\u8d28\u91cf\u3002\u4f46\u7531\u4e8e\u6570\u636e\u96c6\u4e2d\u7f55\u89c1\u4e8b\u4ef6\u4ee3\u8868\u6027\u6781\u4f4e\uff0c\u5bf9\u6700\u7f55\u89c1\u4e8b\u4ef6\u7684\u9884\u6d4b\u6027\u80fd\u4ecd\u7136\u6709\u9650", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u4e25\u91cd\u7a0b\u5ea6\u6392\u5e8f\u3001\u6570\u636e\u4e0d\u5e73\u8861\u8003\u8651\u548c\u5b63\u8282\u6027\u98ce\u9669\u6574\u5408\u5230\u91ce\u706b\u9884\u6d4b\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4e13\u6ce8\u4e8e\u5c06\u5b63\u8282\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u7eb3\u5165\u8bad\u7ec3\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u7684\u53ef\u9760\u6027"}}
{"id": "2601.03369", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03369", "abs": "https://arxiv.org/abs/2601.03369", "authors": ["Sha Luo", "Yogesh Prabhu", "Tim Ossowski", "Kaiping Chen", "Junjie Hu"], "title": "RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models", "comment": null, "summary": "With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.", "AI": {"tldr": "\u63d0\u51faRiskCueBench\u89c6\u9891\u98ce\u9669\u9884\u6d4b\u57fa\u51c6\uff0c\u8981\u6c42\u6a21\u578b\u4ece\u98ce\u9669\u4fe1\u53f7\u7247\u6bb5\uff08\u800c\u975e\u5b8c\u6574\u89c6\u9891\uff09\u9884\u6d4b\u6f5c\u5728\u5b89\u5168\u4e8b\u4ef6\uff0c\u63ed\u793a\u73b0\u6709\u7cfb\u7edf\u5728\u65e9\u671f\u98ce\u9669\u8bc6\u522b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u98ce\u9669\u8bc4\u4f30\u7814\u7a76\u901a\u5e38\u8ba9\u6a21\u578b\u8bbf\u95ee\u5305\u542b\u4e8b\u6545\u672c\u8eab\u7684\u5b8c\u6574\u89c6\u9891\u5e8f\u5217\uff0c\u8fd9\u5927\u5927\u964d\u4f4e\u4e86\u4efb\u52a1\u96be\u5ea6\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002\u9700\u8981\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165RiskCueBench\u89c6\u9891\u7406\u89e3\u57fa\u51c6\uff0c\u5bf9\u89c6\u9891\u8fdb\u884c\u7cbe\u7ec6\u6807\u6ce8\u4ee5\u8bc6\u522b\u98ce\u9669\u4fe1\u53f7\u7247\u6bb5\u2014\u2014\u5373\u6307\u793a\u6f5c\u5728\u5b89\u5168\u5173\u6ce8\u7684\u6700\u65e9\u65f6\u523b\uff0c\u8981\u6c42\u6a21\u578b\u57fa\u4e8e\u65e9\u671f\u89c6\u89c9\u4fe1\u53f7\u9884\u6d4b\u672a\u6765\u98ce\u9669\u4e8b\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u7cfb\u7edf\u5728\u89e3\u91ca\u6f14\u53d8\u60c5\u51b5\u548c\u57fa\u4e8e\u65e9\u671f\u89c6\u89c9\u4fe1\u53f7\u9884\u6d4b\u672a\u6765\u98ce\u9669\u4e8b\u4ef6\u65b9\u9762\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u7a81\u663e\u4e86\u89c6\u9891\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u7684\u91cd\u8981\u6311\u6218\u3002", "conclusion": "RiskCueBench\u57fa\u51c6\u66f4\u597d\u5730\u53cd\u6620\u4e86\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u9891\u98ce\u9669\u9884\u6d4b\u7cfb\u7edf\u5728\u65e9\u671f\u98ce\u9669\u8bc6\u522b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u8981\u6311\u6218\u3002"}}
{"id": "2601.03482", "categories": ["cs.AI", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.03482", "abs": "https://arxiv.org/abs/2601.03482", "authors": ["Stefan Konigorski", "Johannes E. Vedder", "Babajide Alamu Owoyele", "\u0130brahim \u00d6zkan"], "title": "Personalization of Large Foundation Models for Health Interventions", "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM)", "summary": "Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.", "AI": {"tldr": "\u5927\u578b\u57fa\u7840\u6a21\u578b(LFMs)\u5728\u533b\u7597AI\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u65e0\u6cd5\u66ff\u4ee3N-of-1\u8bd5\u9a8c\u8fdb\u884c\u4e2a\u6027\u5316\u6cbb\u7597\u63a8\u8350\uff0c\u4e24\u8005\u5e94\u4e92\u8865\u7ed3\u5408", "motivation": "\u63a2\u8ba8\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u4e2a\u6027\u5316\u533b\u7597\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5176\u9884\u6d4b\u80fd\u529b\u4e0e\u56e0\u679c\u63a8\u65ad\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u53ca\u533b\u7597AI\u4e2d\u5b58\u5728\u7684\u591a\u4e2a\u6096\u8bba\uff08\u6cdb\u5316\u6027\u6096\u8bba\u3001\u9690\u79c1-\u6027\u80fd\u6096\u8bba\u7b49\uff09", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff1aLFMs\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u4ece\u7fa4\u4f53\u6a21\u5f0f\u4e2d\u5feb\u901f\u751f\u6210\u5047\u8bbe\u548c\u5e72\u9884\u5019\u9009\u65b9\u6848\uff0cN-of-1\u8bd5\u9a8c\uff08\u4ea4\u53c9\u81ea\u6211\u5b9e\u9a8c\uff09\u4e3a\u4e2a\u4f53\u63d0\u4f9b\u56e0\u679c\u9a8c\u8bc1\uff0c\u4e24\u8005\u7ed3\u5408\u5b9e\u73b0\u4e2a\u6027\u5316\u533b\u7597", "result": "LFMs\u65e0\u6cd5\u66ff\u4ee3N-of-1\u8bd5\u9a8c\uff0c\u4f46\u4e24\u8005\u53ef\u4ee5\u4e92\u8865\uff1aLFMs\u64c5\u957f\u57fa\u4e8e\u7fa4\u4f53\u6570\u636e\u7684\u5047\u8bbe\u751f\u6210\uff0cN-of-1\u8bd5\u9a8c\u64c5\u957f\u4e2a\u4f53\u56e0\u679c\u63a8\u65ad\uff0c\u7ed3\u5408\u6846\u67b6\u80fd\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u591a\u4e2a\u6096\u8bba", "conclusion": "\u660e\u786e\u9884\u6d4b\u4e0e\u56e0\u679c\u7684\u8fb9\u754c\uff0c\u901a\u8fc7LFMs\u4e0eN-of-1\u8bd5\u9a8c\u7684\u4e92\u8865\u7ed3\u5408\uff0c\u80fd\u591f\u8d1f\u8d23\u4efb\u5730\u5c06AI\u6574\u5408\u5230\u4e2a\u6027\u5316\u533b\u7597\u4e2d\uff0c\u89e3\u51b3\u73b0\u6709\u6096\u8bba\u5e76\u5b9e\u73b0\u771f\u6b63\u7684\u4e2a\u6027\u5316\u6cbb\u7597\u63a8\u8350"}}
{"id": "2601.03904", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03904", "abs": "https://arxiv.org/abs/2601.03904", "authors": ["Korbinian Moller", "Glenn Johannes Tungka", "Lucas J\u00fcrgens", "Johannes Betz"], "title": "Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware", "comment": "7 pages, submitted to the IEEE Intelligent Vehicles Symposium (IV 2026), Detroit, MI, United States", "summary": "Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u4e3b\u52a8\u5b89\u5168\u6269\u5c55\u65b9\u6848\uff0c\u5728\u5d4c\u5165\u5f0f\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u91c7\u6837\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u4e3a\u6545\u969c\u64cd\u4f5c\u63d0\u4f9b\u786e\u5b9a\u6027\u5b9e\u65f6\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982\u5728\u7ebf\u9a8c\u8bc1\uff09\u53ea\u80fd\u68c0\u6d4b\u4e0d\u53ef\u884c\u89c4\u5212\u8f93\u51fa\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u673a\u5236\u5728\u4e3b\u89c4\u5212\u5668\u6545\u969c\u65f6\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c\u3002\u9700\u8981\u4e3a\u6545\u969c\u64cd\u4f5c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u4e3b\u52a8\u5b89\u5168\u6269\u5c55\u3002", "method": "\u5728\u6c7d\u8f66\u7ea7\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u8fd0\u884c\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\uff09\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u91c7\u6837\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u5728\u53d7\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6301\u7eed\u8ba1\u7b97\u8f68\u8ff9\uff0c\u4e3a\u672a\u6765\u7d27\u6025\u89c4\u5212\u67b6\u6784\u5960\u5b9a\u57fa\u7840\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5177\u6709\u786e\u5b9a\u6027\u65f6\u5e8f\u884c\u4e3a\uff0c\u5ef6\u8fdf\u6709\u754c\u4e14\u6296\u52a8\u6700\u5c0f\uff0c\u9a8c\u8bc1\u4e86\u5728\u5b89\u5168\u53ef\u8ba4\u8bc1\u786c\u4ef6\u4e0a\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u4e3b\u52a8\u540e\u5907\u673a\u5236\u6574\u5408\u5230\u4e0b\u4e00\u4ee3\u5b89\u5168\u6846\u67b6\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5269\u4f59\u6311\u6218\uff0c\u4e3a\u6545\u969c\u64cd\u4f5c\u81ea\u52a8\u9a7e\u9a76\u7684\u4e3b\u52a8\u5b89\u5168\u6269\u5c55\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2601.03907", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03907", "abs": "https://arxiv.org/abs/2601.03907", "authors": ["Mohammadreza Koolani", "Simeon Bamford", "Petr Trunin", "Simon F. M\u00fcller-Cleve", "Matteo Lo Preti", "Fulvio Mastrogiovanni", "Lucia Beccai", "Chiara Bartolozzi"], "title": "An Event-Based Opto-Tactile Skin", "comment": "Accepted for publication in Frontiers in Neuromorphic Engineering. 23 pages, 9 figures", "summary": "This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.", "AI": {"tldr": "\u57fa\u4e8eDVS\u548c\u67d4\u6027\u5149\u5b66\u6ce2\u5bfc\u7684\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u4f20\u611f\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u76ee\u89c6\u89c9\u548cDBSCAN\u805a\u7c7b\u5b9e\u73b0\u5927\u9762\u79ef\u8f6f\u76ae\u80a4\u4e0a\u7684\u6309\u538b\u5b9a\u4f4d\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u6570\u636e\u7f29\u51cf\u4e0b\u4ecd\u4fdd\u6301\u529f\u80fd", "motivation": "\u5f00\u53d1\u5927\u9762\u79ef\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u7528\u4e8e\u8f6f\u673a\u5668\u4eba\u548c\u4ea4\u4e92\u73af\u5883\uff0c\u4f20\u7edf\u5d4c\u5165\u5f0f\u5149\u63a5\u6536\u5668\u9700\u8981\u91cd\u590d\u626b\u63cf\uff0c\u800cDVS\u4e8b\u4ef6\u9a71\u52a8\u65b9\u5f0f\u80fd\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u4f20\u611f\u65b9\u6848", "method": "\u5c06\u4e24\u4e2aDVS\u76f8\u673a\u4fa7\u5411\u89c2\u5bdf\u67d4\u6027\u7845\u80f6\u5149\u5b66\u6ce2\u5bfc\u76ae\u80a4\uff0c\u901a\u8fc7\u4eae\u5ea6\u53d8\u5316\u4ea7\u751f\u4e8b\u4ef6\uff0c\u4f7f\u7528DBSCAN\u805a\u7c7b\u5206\u6790\u63a5\u89e6\u4e8b\u4ef6\uff0c\u901a\u8fc7\u4e09\u89d2\u6d4b\u91cf\u4f30\u8ba12D\u76ae\u80a4\u8868\u9762\u7684\u6309\u538b\u4f4d\u7f6e", "result": "\u57284620 mm\u00b2\u63a2\u6d4b\u533a\u57df\u5185\uff0c95%\u53ef\u89c1\u6309\u538b\u7684\u5b9a\u4f4dRMSE\u4e3a4.66 mm\uff1b\u5373\u4f7f\u4e8b\u4ef6\u6570\u636e\u7f29\u51cf\u81f31/1024\uff0c\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u4ec5\u589e\u81f39.33 mm\uff0c85%\u8bd5\u9a8c\u4ecd\u80fd\u6709\u6548\u5b9a\u4f4d\uff1b\u68c0\u6d4b\u5ef6\u8fdf\u5206\u5e03\u7279\u5f81\u5bbd\u5ea6\u4e3a31 ms", "conclusion": "\u8be5\u4e8b\u4ef6\u9a71\u52a8\u89e6\u89c9\u4f20\u611f\u7cfb\u7edf\u5728\u5927\u9762\u79ef\u67d4\u6027\u76ae\u80a4\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u6570\u636e\u7a00\u758f\u5316\u4e0b\u4ecd\u80fd\u5de5\u4f5c\uff0c\u4e3a\u4f4e\u529f\u8017\u3001\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u7684\u8f6f\u673a\u5668\u4eba\u89e6\u89c9\u4f20\u611f\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.03401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03401", "abs": "https://arxiv.org/abs/2601.03401", "authors": ["Ruihan Zhang", "Jun Sun"], "title": "Rendering Data Unlearnable by Exploiting LLM Alignment Mechanisms", "comment": null, "summary": "Large language models (LLMs) are increasingly trained on massive, heterogeneous text corpora, raising serious concerns about the unauthorised use of proprietary or personal data during model training. In this work, we address the problem of data protection against unwanted model learning in a realistic black-box setting. We propose Disclaimer Injection, a novel data-level defence that renders text unlearnable to LLMs. Rather than relying on model-side controls or explicit data removal, our approach exploits the models' own alignment mechanisms: by injecting carefully designed alignment-triggering disclaimers to prevent effective learning. Through layer-wise analysis, we find that fine-tuning on such protected data induces persistent activation of alignment-related layers, causing alignment constraints to override task learning even on common inputs. Consequently, models trained on such data exhibit substantial and systematic performance degradation compared to standard fine-tuning. Our results identify alignment behaviour as a previously unexplored lever for data protection and, to our knowledge, present the first practical method for restricting data learnability at LLM scale without requiring access to or modification of the training pipeline.", "AI": {"tldr": "\u63d0\u51faDisclaimer Injection\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u9f50\u89e6\u53d1\u514d\u8d23\u58f0\u660e\uff0c\u4f7f\u6587\u672c\u5bf9LLM\u4e0d\u53ef\u5b66\u4e60\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u5bf9\u9f50\u673a\u5236\u5b9e\u73b0\u6570\u636e\u4fdd\u62a4", "motivation": "LLM\u5728\u8bad\u7ec3\u4e2d\u5927\u91cf\u4f7f\u7528\u5f02\u6784\u6587\u672c\u8bed\u6599\uff0c\u5f15\u53d1\u5bf9\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u4e13\u6709\u6216\u4e2a\u4eba\u6570\u636e\u7684\u62c5\u5fe7\uff0c\u9700\u8981\u5728\u4e0d\u8bbf\u95ee\u6216\u4fee\u6539\u8bad\u7ec3\u6d41\u7a0b\u7684\u60c5\u51b5\u4e0b\u4fdd\u62a4\u6570\u636e\u4e0d\u88ab\u6a21\u578b\u5b66\u4e60", "method": "\u63d0\u51faDisclaimer Injection\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u4e2d\u6ce8\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u9f50\u89e6\u53d1\u514d\u8d23\u58f0\u660e\uff0c\u5229\u7528LLM\u81ea\u8eab\u7684\u5bf9\u9f50\u673a\u5236\u963b\u6b62\u6709\u6548\u5b66\u4e60\u3002\u901a\u8fc7\u5c42\u7ea7\u5206\u6790\u53d1\u73b0\uff0c\u5728\u53d7\u4fdd\u62a4\u6570\u636e\u4e0a\u5fae\u8c03\u4f1a\u5bfc\u81f4\u5bf9\u9f50\u76f8\u5173\u5c42\u6301\u7eed\u6fc0\u6d3b", "result": "\u5728\u53d7\u4fdd\u62a4\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\u6807\u51c6\u5fae\u8c03\u8868\u73b0\u51fa\u663e\u8457\u4e14\u7cfb\u7edf\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5bf9\u9f50\u7ea6\u675f\u4f1a\u8986\u76d6\u4efb\u52a1\u5b66\u4e60\uff0c\u5373\u4f7f\u5bf9\u5e38\u89c1\u8f93\u5165\u4e5f\u662f\u5982\u6b64", "conclusion": "\u5bf9\u9f50\u884c\u4e3a\u662f\u6570\u636e\u4fdd\u62a4\u4e2d\u4e00\u4e2a\u5148\u524d\u672a\u88ab\u63a2\u7d22\u7684\u6760\u6746\uff0c\u8be5\u65b9\u6cd5\u662f\u5728\u4e0d\u8bbf\u95ee\u6216\u4fee\u6539\u8bad\u7ec3\u6d41\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u5728LLM\u89c4\u6a21\u4e0a\u9650\u5236\u6570\u636e\u53ef\u5b66\u4e60\u6027\u7684\u9996\u4e2a\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2601.03672", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03672", "abs": "https://arxiv.org/abs/2601.03672", "authors": ["Chen Zhang", "Kepu Zhang", "Jiatong Zhang", "Xiao Zhang", "Jun Xu"], "title": "Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction", "comment": null, "summary": "Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.", "AI": {"tldr": "Sandwich Reasoning (SandwichR) \u63d0\u51fa\u4e86\u4e00\u79cd Answer-Reasoning-Answer \u8303\u5f0f\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u67e5\u8be2\u7ea0\u6b63\u7684\u5ef6\u8fdf\u3002", "motivation": "\u67e5\u8be2\u7ea0\u6b63\u662f\u73b0\u4ee3\u641c\u7d22\u6d41\u6c34\u7ebf\u7684\u5173\u952e\u5165\u53e3\uff0c\u9700\u8981\u5728\u5b9e\u65f6\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u867d\u7136\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u80fd\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5176\u5ef6\u8fdf\u8fc7\u9ad8\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002\u63d0\u524d\u8f93\u51fa\u7b54\u6848\u867d\u7136\u80fd\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u7b54\u6848\u4e0e\u540e\u7eed\u63a8\u7406\u65e0\u5173\uff0c\u65e0\u6cd5\u5229\u7528\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa Sandwich Reasoning (SandwichR) \u65b9\u6cd5\uff0c\u91c7\u7528 Answer-Reasoning-Answer \u8303\u5f0f\uff1a\u5148\u751f\u6210\u521d\u59cb\u7ea0\u6b63\uff0c\u518d\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\uff0c\u6700\u540e\u751f\u6210\u6700\u7ec8\u7cbe\u70bc\u7ea0\u6b63\u3002\u901a\u8fc7\u4e00\u81f4\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u5956\u52b1\u6765\u5bf9\u9f50\u521d\u59cb\u548c\u6700\u7ec8\u7ea0\u6b63\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8fb9\u754c\u7684\u62d2\u7edd\u91c7\u6837\u6765\u4f18\u5148\u5904\u7406\u63a8\u7406\u5f71\u54cd\u6700\u5927\u7684\u8fb9\u754c\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSandwichR \u5728\u8fbe\u5230\u4e0e\u6807\u51c6 CoT \u76f8\u5f53\u7684 SOTA \u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86 40-70% \u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u641c\u7d22\u4e2d\u7684\u5ef6\u8fdf-\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\u3002\u540c\u65f6\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u67e5\u8be2\u7ea0\u6b63\u6570\u636e\u96c6\u3002", "conclusion": "SandwichR \u901a\u8fc7\u5c06\u5feb\u901f\u521d\u59cb\u7b54\u6848\u4e0e\u4e8b\u540e\u63a8\u7406\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u67e5\u8be2\u7ea0\u6b63\u800c\u4e0d\u727a\u7272\u63a8\u7406\u611f\u77e5\u7684\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86 CoT \u63a8\u7406\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.03434", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03434", "abs": "https://arxiv.org/abs/2601.03434", "authors": ["Zibo Liu", "Muyang Li", "Zhe Jiang", "Shigang Chen"], "title": "VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding", "comment": null, "summary": "News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.", "AI": {"tldr": "VNU-Bench\uff1a\u9996\u4e2a\u591a\u6e90\u8de8\u89c6\u9891\u65b0\u95fb\u7406\u89e3\u57fa\u51c6\uff0c\u5305\u542b429\u4e2a\u65b0\u95fb\u7ec4\u30011,405\u4e2a\u89c6\u9891\u548c2,501\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\uff0c\u6311\u6218\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b", "motivation": "\u73b0\u6709\u65b0\u95fb\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u6e90\u3001\u89c6\u9891\u5185\u63a8\u7406\uff0c\u800c\u73b0\u5b9e\u65b0\u95fb\u6d88\u8d39\u672c\u8d28\u4e0a\u662f\u591a\u6e90\u7684\u2014\u2014\u540c\u4e00\u4e8b\u4ef6\u7531\u4e0d\u540c\u5a92\u4f53\u4ee5\u4e92\u8865\u7ec6\u8282\u3001\u4e0d\u540c\u53d9\u4e8b\u9009\u62e9\u548c\u6709\u65f6\u76f8\u4e92\u77db\u76fe\u7684\u8bf4\u6cd5\u62a5\u9053\u3002\u7a33\u5065\u7684\u65b0\u95fb\u7406\u89e3\u9700\u8981\u6a21\u578b\u80fd\u591f\u6bd4\u8f83\u4e0d\u540c\u6765\u6e90\u7684\u89c2\u70b9\u3001\u5bf9\u9f50\u8de8\u6765\u6e90\u7684\u591a\u6a21\u6001\u8bc1\u636e\u5e76\u7efc\u5408\u591a\u6e90\u4fe1\u606f\u3002", "method": "1. \u8bbe\u8ba1\u65b0\u578b\u95ee\u9898\u7c7b\u578b\uff0c\u4ece\u591a\u4e2a\u89d2\u5ea6\u6d4b\u8bd5\u6a21\u578b\u7406\u89e3\u591a\u6e90\u591a\u6a21\u6001\u65b0\u95fb\u7684\u80fd\u529b\n2. \u63d0\u51fa\u6df7\u5408\u4eba\u673aQA\u751f\u6210\u6d41\u7a0b\uff0c\u89e3\u51b3\u8de8\u6e90\u65b0\u95fb\u7406\u89e3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6784\u5efa\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8d28\u91cf\u63a7\u5236\u95ee\u9898\n3. \u6784\u5efa\u5305\u542b429\u4e2a\u65b0\u95fb\u7ec4\u30011,405\u4e2a\u89c6\u9891\u548c2,501\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\u7684\u6570\u636e\u96c6", "result": "\u5bf9\u95ed\u6e90\u548c\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cVNU-Bench\u5bf9\u5f53\u524dMLLMs\u6784\u6210\u4e86\u5b9e\u8d28\u6027\u6311\u6218\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u591a\u6e90\u8de8\u89c6\u9891\u65b0\u95fb\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "VNU-Bench\u586b\u8865\u4e86\u591a\u6e90\u8de8\u89c6\u9891\u65b0\u95fb\u7406\u89e3\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u73b0\u5b9e\u65b0\u95fb\u6d88\u8d39\u573a\u666f\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u591a\u6e90\u4fe1\u606f\u6574\u5408\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.03517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.03517", "abs": "https://arxiv.org/abs/2601.03517", "authors": ["Sarim Chaudhry"], "title": "Semantic Belief-State World Model for 3D Human Motion Prediction", "comment": null, "summary": "Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.", "AI": {"tldr": "SBWM\u5c06\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u91cd\u6784\u4e3a\u4eba\u4f53\u6d41\u5f62\u4e0a\u7684\u6f5c\u5728\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u901a\u8fc7\u4fe1\u5ff5\u72b6\u6001\u6f14\u5316\u800c\u975e\u76f4\u63a5\u9884\u6d4b\u59ff\u6001\uff0c\u5b9e\u73b0\u7a33\u5b9a\u957f\u65f6\u7a0b\u63a8\u6f14", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u590d\u5408\u6f02\u79fb\u3001\u5e73\u5747\u59ff\u6001\u584c\u9677\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u5dee\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u5c06\u89c2\u6d4b\u91cd\u5efa\u4e0e\u52a8\u529b\u5b66\u5efa\u6a21\u6df7\u5728\u4e00\u8d77\uff0c\u7f3a\u4e4f\u5bf9\u8fd0\u52a8\u6f5c\u5728\u539f\u56e0\u7684\u663e\u5f0f\u8868\u793a", "method": "\u63d0\u51fa\u8bed\u4e49\u4fe1\u5ff5\u72b6\u6001\u4e16\u754c\u6a21\u578b(SBWM)\uff0c\u5728\u4eba\u4f53\u6d41\u5f62\u4e0a\u8fdb\u884c\u6f5c\u5728\u52a8\u529b\u5b66\u6a21\u62df\u3002\u6a21\u578b\u7ef4\u62a4\u5faa\u73af\u6982\u7387\u4fe1\u5ff5\u72b6\u6001\uff0c\u5176\u6f14\u5316\u72ec\u7acb\u4e8e\u59ff\u6001\u91cd\u5efa\u5b66\u4e60\uff0c\u5e76\u4e0eSMPL-X\u89e3\u5256\u53c2\u6570\u5316\u5bf9\u9f50\uff0c\u5f62\u6210\u7ed3\u6784\u4fe1\u606f\u74f6\u9888", "result": "SBWM\u5b9e\u73b0\u4e86\u8fde\u8d2f\u7684\u957f\u65f6\u7a0b\u63a8\u6f14\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u57fa\u4e8eRSSM\u3001transformer\u548c\u6269\u6563\u7684\u65b9\u6cd5", "conclusion": "\u5c06\u4eba\u4f53\u89c6\u4e3a\u4e16\u754c\u6a21\u578b\u72b6\u6001\u7a7a\u95f4\u7684\u4e00\u90e8\u5206\u800c\u975e\u8f93\u51fa\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u8fd0\u52a8\u6a21\u62df\u548c\u9884\u6d4b\u7684\u65b9\u5f0f\uff0c\u4e3a\u7a33\u5b9a\u957f\u65f6\u7a0b\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2601.03526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.03526", "abs": "https://arxiv.org/abs/2601.03526", "authors": ["Zhicheng Zhao", "Fengjiao Peng", "Jinquan Yan", "Wei Lu", "Chenglong Li", "Jin Tang"], "title": "Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution", "comment": null, "summary": "Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.", "AI": {"tldr": "PCNet\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u70ed\u6210\u50cf\u65e0\u4eba\u673a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u7269\u7406\u7ea6\u675f\u8de8\u6a21\u6001\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u5206\u8fa8\u7387\u4e92\u589e\u5f3a\u6a21\u5757\u548c\u70ed\u4f20\u5bfc\u7269\u7406\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u548c\u7269\u7406\u4e0d\u4e00\u81f4\u4f2a\u5f71\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5149\u5b66\u5f15\u5bfc\u7684\u70ed\u6210\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u901a\u5e38\u538b\u7f29\u5149\u5b66\u7279\u5f81\u4ee5\u5339\u914d\u70ed\u6210\u50cf\u7279\u5f81\u7ef4\u5ea6\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5bf9\u8d85\u5206\u8fa8\u7387\u6709\u76ca\u7684\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\uff0c\u5e76\u4e14\u7531\u4e8e\u5ffd\u7565\u6a21\u6001\u95f4\u6210\u50cf\u7269\u7406\u5dee\u5f02\u800c\u5f15\u5165\u7eb9\u7406\u626d\u66f2\u548c\u8fb9\u7f18\u6a21\u7cca\u7b49\u7269\u7406\u4e0d\u4e00\u81f4\u4f2a\u5f71\u3002", "method": "\u63d0\u51faPCNet\u6846\u67b6\uff1a1) \u8de8\u5206\u8fa8\u7387\u4e92\u589e\u5f3a\u6a21\u5757(CRME)\u8054\u5408\u4f18\u5316\u70ed\u6210\u50cf\u8d85\u5206\u8fa8\u7387\u548c\u5149\u5b66\u5230\u70ed\u6210\u50cf\u6a21\u6001\u8f6c\u6362\uff0c\u5b9e\u73b0\u53cc\u5411\u7279\u5f81\u4ea4\u4e92\u540c\u65f6\u4fdd\u7559\u9ad8\u9891\u5149\u5b66\u5148\u9a8c\uff1b2) \u7269\u7406\u9a71\u52a8\u70ed\u4f20\u5bfc\u6a21\u5757(PDTM)\u5c06\u4e8c\u7ef4\u70ed\u4f20\u5bfc\u878d\u5165\u5149\u5b66\u5f15\u5bfc\uff0c\u5efa\u6a21\u7a7a\u95f4\u53d8\u5316\u7684\u70ed\u4f20\u5bfc\u7279\u6027\uff1b3) \u6e29\u5ea6\u4e00\u81f4\u6027\u635f\u5931\u786e\u4fdd\u751f\u6210\u7684\u70ed\u6210\u50cf\u7b26\u5408\u771f\u5b9e\u70ed\u8f90\u5c04\u539f\u7406\u3002", "result": "\u5728VGTSR2.0\u548cDroneVehicle\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPCNet\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5305\u62ec\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PCNet\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u7684\u8de8\u6a21\u6001\u589e\u5f3a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u70ed\u6210\u50cf\u65e0\u4eba\u673a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u548c\u7269\u7406\u4e0d\u4e00\u81f4\u4f2a\u5f71\u95ee\u9898\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.03565", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03565", "abs": "https://arxiv.org/abs/2601.03565", "authors": ["Vaibhav Gupta", "Florian Grensing", "Beyza Cinar", "Maria Maleshkova"], "title": "A Proposed Paradigm for Imputing Missing Multi-Sensor Data in the Healthcare Domain", "comment": "21 Pages, 6 Figures, 7 Tables", "summary": "Chronic diseases such as diabetes pose significant management challenges, particularly due to the risk of complications like hypoglycemia, which require timely detection and intervention. Continuous health monitoring through wearable sensors offers a promising solution for early prediction of glycemic events. However, effective use of multisensor data is hindered by issues such as signal noise and frequent missing values. This study examines the limitations of existing datasets and emphasizes the temporal characteristics of key features relevant to hypoglycemia prediction. A comprehensive analysis of imputation techniques is conducted, focusing on those employed in state-of-the-art studies. Furthermore, imputation methods derived from machine learning and deep learning applications in other healthcare contexts are evaluated for their potential to address longer gaps in time-series data. Based on this analysis, a systematic paradigm is proposed, wherein imputation strategies are tailored to the nature of specific features and the duration of missing intervals. The review concludes by emphasizing the importance of investigating the temporal dynamics of individual features and the implementation of multiple, feature-specific imputation techniques to effectively address heterogeneous temporal patterns inherent in the data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8fde\u7eed\u5065\u5eb7\u76d1\u6d4b\u4e2d\u591a\u4f20\u611f\u5668\u6570\u636e\u7f3a\u5931\u503c\u63d2\u8865\u6280\u672f\uff0c\u9488\u5bf9\u4f4e\u8840\u7cd6\u9884\u6d4b\uff0c\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u7279\u6027\u548c\u7f3a\u5931\u65f6\u957f\u5b9a\u5236\u63d2\u8865\u7b56\u7565\u7684\u7cfb\u7edf\u8303\u5f0f\u3002", "motivation": "\u6162\u6027\u75be\u75c5\u5982\u7cd6\u5c3f\u75c5\u7684\u7ba1\u7406\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u4f4e\u8840\u7cd6\u7b49\u5e76\u53d1\u75c7\u9700\u8981\u53ca\u65f6\u68c0\u6d4b\u548c\u5e72\u9884\u3002\u867d\u7136\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u8fde\u7eed\u5065\u5eb7\u76d1\u6d4b\u4e3a\u8840\u7cd6\u4e8b\u4ef6\u65e9\u671f\u9884\u6d4b\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u591a\u4f20\u611f\u5668\u6570\u636e\u5b58\u5728\u4fe1\u53f7\u566a\u58f0\u548c\u9891\u7e41\u7f3a\u5931\u503c\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u6709\u6548\u5229\u7528\u3002", "method": "1. \u5206\u6790\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e0e\u4f4e\u8840\u7cd6\u9884\u6d4b\u76f8\u5173\u7684\u5173\u952e\u7279\u5f81\u7684\u65f6\u95f4\u7279\u6027\uff1b2. \u5bf9\u73b0\u6709\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u63d2\u8865\u6280\u672f\u8fdb\u884c\u5168\u9762\u5206\u6790\uff1b3. \u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u5176\u4ed6\u533b\u7597\u573a\u666f\u4e2d\u5e94\u7528\u7684\u63d2\u8865\u65b9\u6cd5\u5904\u7406\u957f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7f3a\u5931\u7684\u6f5c\u529b\uff1b4. \u63d0\u51fa\u7cfb\u7edf\u8303\u5f0f\uff0c\u6839\u636e\u7279\u5b9a\u7279\u5f81\u7684\u6027\u8d28\u548c\u7f3a\u5931\u95f4\u9694\u7684\u6301\u7eed\u65f6\u95f4\u5b9a\u5236\u63d2\u8865\u7b56\u7565\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u63d2\u8865\u8303\u5f0f\uff0c\u5f3a\u8c03\u9700\u8981\u6839\u636e\u7279\u5f81\u7279\u6027\u548c\u7f3a\u5931\u65f6\u957f\u91c7\u7528\u5b9a\u5236\u5316\u7684\u63d2\u8865\u7b56\u7565\uff0c\u800c\u4e0d\u662f\u5355\u4e00\u65b9\u6cd5\u3002\u8be5\u8303\u5f0f\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u6570\u636e\u4e2d\u56fa\u6709\u7684\u5f02\u8d28\u65f6\u95f4\u6a21\u5f0f\u3002", "conclusion": "\u7efc\u8ff0\u5f3a\u8c03\u7814\u7a76\u4e2a\u4f53\u7279\u5f81\u7684\u65f6\u95f4\u52a8\u6001\u6027\u548c\u5b9e\u65bd\u591a\u4e2a\u7279\u5f81\u7279\u5b9a\u63d2\u8865\u6280\u672f\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u6709\u6548\u5904\u7406\u6570\u636e\u4e2d\u56fa\u6709\u7684\u5f02\u8d28\u65f6\u95f4\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u4f4e\u8840\u7cd6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.03658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03658", "abs": "https://arxiv.org/abs/2601.03658", "authors": ["Basile Tousside", "Janis Mohr", "J\u00f6rg Frochte"], "title": "Group and Exclusive Sparse Regularization-based Continual Learning of CNNs", "comment": "12 pages, Canadian Artificial Intelligence Association (CAIAC)", "summary": "We present a regularization-based approach for continual learning (CL) of fixed capacity convolutional neural networks (CNN) that does not suffer from the problem of catastrophic forgetting when learning multiple tasks sequentially. This method referred to as Group and Exclusive Sparsity based Continual Learning (GESCL) avoids forgetting of previous tasks by ensuring the stability of the CNN via a stability regularization term, which prevents filters detected as important for past tasks to deviate too much when learning a new task. On top of that, GESCL makes the network plastic via a plasticity regularization term that leverage the over-parameterization of CNNs to efficiently sparsify the network and tunes unimportant filters making them relevant for future tasks. Doing so, GESCL deals with significantly less parameters and computation compared to CL approaches that either dynamically expand the network or memorize past tasks' data. Experiments on popular CL vision benchmarks show that GESCL leads to significant improvements over state-of-the-art method in terms of overall CL performance, as measured by classification accuracy as well as in terms of avoiding catastrophic forgetting.", "AI": {"tldr": "GESCL\u662f\u4e00\u79cd\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u6b63\u5219\u5316\u9879\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u5229\u7528CNN\u7684\u8fc7\u53c2\u6570\u5316\u7279\u6027\u8fdb\u884c\u7a00\u758f\u5316\uff0c\u76f8\u6bd4\u52a8\u6001\u6269\u5c55\u7f51\u7edc\u6216\u8bb0\u5fc6\u6570\u636e\u7684\u65b9\u6cd5\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u89e3\u51b3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1\u65f6\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u52a8\u6001\u6269\u5c55\u7f51\u7edc\u6216\u8bb0\u5fc6\u8fc7\u53bb\u4efb\u52a1\u6570\u636e\u5e26\u6765\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faGESCL\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6b63\u5219\u5316\u9879\uff1a\u7a33\u5b9a\u6027\u6b63\u5219\u5316\u9632\u6b62\u5bf9\u8fc7\u53bb\u4efb\u52a1\u91cd\u8981\u7684\u6ee4\u6ce2\u5668\u53c2\u6570\u53d8\u5316\u8fc7\u5927\uff1b\u53ef\u5851\u6027\u6b63\u5219\u5316\u5229\u7528CNN\u7684\u8fc7\u53c2\u6570\u5316\u7279\u6027\uff0c\u7a00\u758f\u5316\u7f51\u7edc\u5e76\u8c03\u6574\u4e0d\u91cd\u8981\u6ee4\u6ce2\u5668\u4f7f\u5176\u9002\u5e94\u672a\u6765\u4efb\u52a1\u3002", "result": "\u5728\u6d41\u884c\u7684\u6301\u7eed\u5b66\u4e60\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGESCL\u5728\u6574\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u548c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GESCL\u901a\u8fc7\u7ed3\u5408\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u6b63\u5219\u5316\uff0c\u5728\u56fa\u5b9a\u5bb9\u91cfCNN\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2601.03781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.03781", "abs": "https://arxiv.org/abs/2601.03781", "authors": ["Xiaokun Sun", "Zezhong Wu", "Zewen Ding", "Linli Xu"], "title": "MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction", "comment": null, "summary": "Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.", "AI": {"tldr": "\u63d0\u51faMVP\uff08Masked Video Prediction\uff09\u540e\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u853d\u7684\u8fde\u7eed\u89c6\u9891\u7247\u6bb5\u6765\u589e\u5f3aVideoLLMs\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u56e0\u679c\u7406\u89e3\u80fd\u529b\uff0c\u4f7f\u7528GRPO\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684VideoLLMs\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u5185\u5bb9\u7406\u89e3\uff0c\u7f3a\u4e4f\u5bf9\u5185\u5728\u65f6\u5e8f\u8fde\u8d2f\u6027\u548c\u5e27\u95f4\u76f8\u5173\u6027\u7684\u663e\u5f0f\u76d1\u7763\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u590d\u6742\u52a8\u6001\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u56e0\u679c\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faMVP\u540e\u8bad\u7ec3\u76ee\u6807\uff1a\u8981\u6c42\u6a21\u578b\u4ece\u4e00\u7ec4\u5177\u6709\u6311\u6218\u6027\u7684\u5e72\u6270\u9879\u4e2d\u91cd\u5efa\u88ab\u906e\u853d\u7684\u8fde\u7eed\u89c6\u9891\u7247\u6bb5\uff0c\u5f3a\u5236\u6a21\u578b\u5173\u6ce8\u4e8b\u4ef6\u7684\u5e8f\u5217\u903b\u8f91\u548c\u65f6\u5e8f\u4e0a\u4e0b\u6587\u3002\u5f15\u5165\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u5c06\u4efb\u610f\u89c6\u9891\u8bed\u6599\u5e93\u8f6c\u6362\u4e3aMVP\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u91c7\u7528\u5e26\u6709\u7ec6\u7c92\u5ea6\u5956\u52b1\u51fd\u6570\u7684GRPO\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u9891\u4e0a\u4e0b\u6587\u548c\u65f6\u5e8f\u5c5e\u6027\u7684\u7406\u89e3\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cMVP\u901a\u8fc7\u76f4\u63a5\u5f3a\u5316\u65f6\u5e8f\u63a8\u7406\u548c\u56e0\u679c\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "MVP\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u540e\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u65f6\u5e8f\u8fde\u8d2f\u6027\u548c\u5e27\u95f4\u76f8\u5173\u6027\uff0c\u6709\u6548\u589e\u5f3a\u4e86VideoLLMs\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u56e0\u679c\u7406\u89e3\u80fd\u529b\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u590d\u6742\u52a8\u6001\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2601.03649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03649", "abs": "https://arxiv.org/abs/2601.03649", "authors": ["Gengyang Li", "Wang Cai", "Yifeng Gao", "Yunfang Wu"], "title": "SyncThink: A Training-Free Strategy to Align Inference Termination with Reasoning Saturation", "comment": "14 pages, 8 figures", "summary": "Chain-of-Thought (CoT) prompting improves reasoning but often produces long and redundant traces that substantially increase inference cost. We present SyncThink, a training-free and plug-and-play decoding method that reduces CoT overhead without modifying model weights. We find that answer tokens attend weakly to early reasoning and instead focus on the special token \"/think\", indicating an information bottleneck. Building on this observation, SyncThink monitors the model's own reasoning-transition signal and terminates reasoning. Experiments on GSM8K, MMLU, GPQA, and BBH across three DeepSeek-R1 distilled models show that SyncThink achieves 62.00 percent average Top-1 accuracy using 656 generated tokens and 28.68 s latency, compared to 61.22 percent, 2141 tokens, and 92.01 s for full CoT decoding. On long-horizon tasks such as GPQA, SyncThink can further yield up to +8.1 absolute accuracy by preventing over-thinking.", "AI": {"tldr": "SyncThink\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u6a21\u578b\u81ea\u8eab\u7684\u63a8\u7406\u8f6c\u6362\u4fe1\u53f7\u6765\u63d0\u524d\u7ec8\u6b62\u63a8\u7406\uff0c\u663e\u8457\u51cf\u5c11CoT\u63a8\u7406\u5f00\u9500\u3002", "motivation": "CoT\u63d0\u793a\u867d\u7136\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u4ea7\u751f\u5197\u957f\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5927\u5e45\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11CoT\u5f00\u9500\u3002", "method": "\u57fa\u4e8e\u53d1\u73b0\u7b54\u6848token\u5bf9\u65e9\u671f\u63a8\u7406\u5173\u6ce8\u8f83\u5f31\u800c\u805a\u7126\u4e8e\"/think\"\u7279\u6b8atoken\u7684\u73b0\u8c61\uff0cSyncThink\u76d1\u63a7\u6a21\u578b\u81ea\u8eab\u7684\u63a8\u7406\u8f6c\u6362\u4fe1\u53f7\uff0c\u5728\u9002\u5f53\u65f6\u673a\u7ec8\u6b62\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728GSM8K\u3001MMLU\u3001GPQA\u548cBBH\u6570\u636e\u96c6\u4e0a\uff0cSyncThink\u5e73\u5747Top-1\u51c6\u786e\u7387\u8fbe\u523062.00%\uff0c\u4ec5\u9700656\u4e2atoken\u548c28.68\u79d2\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u5b8c\u6574CoT\u89e3\u7801\u768461.22%\u30012141\u4e2atoken\u548c92.01\u79d2\u6709\u660e\u663e\u4f18\u52bf\u3002\u5728GPQA\u7b49\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\uff0cSyncThink\u8fd8\u80fd\u901a\u8fc7\u9632\u6b62\"\u8fc7\u5ea6\u601d\u8003\"\u83b7\u5f97\u6700\u9ad8+8.1\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "SyncThink\u901a\u8fc7\u76d1\u63a7\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u5b9e\u73b0\u63a8\u7406\u63d0\u524d\u7ec8\u6b62\uff0c\u6709\u6548\u51cf\u5c11CoT\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2601.03955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.03955", "abs": "https://arxiv.org/abs/2601.03955", "authors": ["Xu Zhang", "Cheng Da", "Huan Yang", "Kun Gai", "Ming Lu", "Zhan Ma"], "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation", "comment": "Technical report", "summary": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.", "AI": {"tldr": "ResTok\u662f\u4e00\u79cd1D\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u6784\u5efa\u5206\u5c42\u6b8b\u5dee\u8868\u793a\u6765\u6062\u590d\u89c6\u89c9\u7684\u5c42\u6b21\u5316\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u5728ImageNet-256\u4e0a\u4ec5\u97009\u6b65\u91c7\u6837\u5373\u53ef\u8fbe\u52302.34 gFID\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u8a00\u5efa\u6a21\u8bbe\u8ba1\u76841D\u89c6\u89c9\u5206\u8bcd\u5668\u5c06\u89c6\u89c9\u6570\u636e\u89c6\u4e3a\u5e73\u5766\u7684\u5e8f\u5217\u5316token\u6d41\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u7684\u5c42\u6b21\u5316\u548c\u6b8b\u5dee\u7f51\u7edc\u8bbe\u8ba1\u7279\u6027\uff0c\u800c\u8fd9\u4e9b\u7279\u6027\u5bf9\u89c6\u89c9\u6a21\u578b\u7684\u6536\u655b\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faResTok\uff0c\u4e00\u79cd\u6784\u5efa\u56fe\u50cftoken\u548c\u6f5c\u5728token\u5206\u5c42\u6b8b\u5dee\u76841D\u89c6\u89c9\u5206\u8bcd\u5668\u3002\u901a\u8fc7\u6e10\u8fdb\u5408\u5e76\u83b7\u5f97\u5206\u5c42\u8868\u793a\uff0c\u5b9e\u73b0\u6bcf\u5c42\u7684\u8de8\u5c42\u7ea7\u7279\u5f81\u878d\u5408\uff1b\u8bed\u4e49\u6b8b\u5dee\u9632\u6b62\u4fe1\u606f\u91cd\u53e0\uff0c\u4ea7\u751f\u66f4\u96c6\u4e2d\u7684\u6f5c\u5728\u5206\u5e03\u3002\u540c\u65f6\u5f15\u5165\u5206\u5c42AR\u751f\u6210\u5668\uff0c\u901a\u8fc7\u4e00\u6b21\u9884\u6d4b\u6574\u4e2a\u5c42\u7ea7\u7684\u6f5c\u5728token\u6765\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "\u5728ImageNet-256\u4e0a\u4ec5\u75289\u4e2a\u91c7\u6837\u6b65\u9aa4\u5c31\u8fbe\u5230\u4e862.34 gFID\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002\u8de8\u5c42\u7ea7\u7ed1\u5b9a\u81ea\u7136\u6d8c\u73b0\uff0c\u65e0\u9700\u663e\u5f0f\u7ea6\u675f\u3002", "conclusion": "\u5728\u89c6\u89c9\u5206\u8bcd\u4e2d\u6062\u590d\u5206\u5c42\u6b8b\u5dee\u5148\u9a8c\u80fd\u663e\u8457\u6539\u5584\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0cResTok\u901a\u8fc7\u5c06\"\u89c6\u89c9\"\u7279\u6027\u91cd\u65b0\u5f15\u5165\u89c6\u89c9\u5efa\u6a21\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2601.04151", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04151", "abs": "https://arxiv.org/abs/2601.04151", "authors": ["Jun Wang", "Chunyu Qiang", "Yuxin Guo", "Yiran Wang", "Xijuan Zeng", "Chen Zhang", "Pengfei Wan"], "title": "Klear: Unified Multi-Task Audio-Video Joint Generation", "comment": null, "summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.", "AI": {"tldr": "Klear\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5355\u5854\u67b6\u6784\u3001\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\u548c\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u97f3\u9891-\u89c6\u89c9\u5f02\u6b65\u3001\u5507\u8bed\u5bf9\u9f50\u5dee\u548c\u5355\u6a21\u6001\u9000\u5316\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u97f3\u9891-\u89c6\u89c9\u5f02\u6b65\u3001\u5507\u8bed\u5bf9\u9f50\u5dee\u3001\u5355\u6a21\u6001\u9000\u5316\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u97f3\u9891-\u89c6\u89c9\u5bf9\u5e94\u5efa\u6a21\u5f31\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u9ad8\u8d28\u91cf\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "1. \u67b6\u6784\uff1a\u91c7\u7528\u5355\u5854\u8bbe\u8ba1\uff0c\u7edf\u4e00DiT\u5757\u548cOmni-Full Attention\u673a\u5236\uff1b2. \u8bad\u7ec3\uff1a\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u7b56\u7565\uff08\u968f\u673a\u6a21\u6001\u63a9\u7801\u5230\u8054\u5408\u4f18\u5316\uff09\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1b3. \u6570\u636e\uff1a\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u5e26\u5bc6\u96c6\u6807\u6ce8\u7684\u97f3\u9891-\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u7b5b\u9009\u9ad8\u8d28\u91cf\u5bf9\u9f50\u7684\u4e09\u5143\u7ec4\u3002", "result": "Klear\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u5927\u5e45\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0eVeo 3\u76f8\u5f53\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u8bed\u4e49\u548c\u65f6\u95f4\u5bf9\u9f50\u3001\u9075\u5faa\u6307\u4ee4\u7684\u97f3\u9891-\u89c6\u9891\u5185\u5bb9\uff0c\u5728\u8054\u5408\u548c\u5355\u6a21\u6001\u8bbe\u7f6e\u4e0b\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u9c81\u68d2\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u573a\u666f\u3002", "conclusion": "Klear\u901a\u8fc7\u7edf\u4e00\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u97f3\u9891-\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u97f3\u9891-\u89c6\u89c9\u5bf9\u9f50\u751f\u6210\u3002"}}
{"id": "2601.04164", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04164", "abs": "https://arxiv.org/abs/2601.04164", "authors": ["Alberto Marfoglia", "Jong Ho Jhee", "Adrien Coulet"], "title": "Clinical Data Goes MEDS? Let's OWL make sense of it", "comment": "12 pages, 5 tables, 4 figures", "summary": "The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.", "AI": {"tldr": "MEDS-OWL\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7OWL\u672c\u4f53\uff0c\u5c06\u533b\u7597\u4e8b\u4ef6\u6570\u636e\u6807\u51c6\uff08MEDS\uff09\u4e0e\u8bed\u4e49\u7f51\u751f\u6001\u7cfb\u7edf\u8fde\u63a5\uff0c\u901a\u8fc7RDF\u56fe\u8868\u793aMEDS\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86Python\u8f6c\u6362\u5de5\u5177meds2rdf\u3002", "motivation": "\u533b\u7597\u6570\u636e\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u8bed\u4e49\u660e\u786e\u7684\u8868\u793a\uff0c\u5bfc\u81f4\u8de8\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u91cd\u590d\u6027\u6709\u9650\u3002MEDS\u867d\u7136\u63d0\u4f9b\u4e86\u4e8b\u4ef6\u4e2d\u5fc3\u7684\u6570\u636e\u6a21\u578b\uff0c\u4f46\u672a\u4e0e\u8bed\u4e49\u7f51\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u3002", "method": "\u5f00\u53d1MEDS-OWL\u672c\u4f53\uff0813\u4e2a\u7c7b\u300110\u4e2a\u5bf9\u8c61\u5c5e\u6027\u300120\u4e2a\u6570\u636e\u5c5e\u6027\u300124\u4e2aOWL\u516c\u7406\uff09\uff0c\u5e76\u5b9e\u73b0meds2rdf Python\u5e93\u5c06MEDS\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u7b26\u5408\u672c\u4f53\u7684RDF\u56fe\u3002\u5728\u9885\u5185\u52a8\u8109\u7624\u7834\u88c2\u7684\u5408\u6210\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u6f14\u793a\uff0c\u4f7f\u7528SHACL\u7ea6\u675f\u9a8c\u8bc1\u7ed3\u679c\u56fe\u3002", "result": "MEDS-OWL\u9996\u4e2a\u7248\u672c\u6210\u529f\u5b9e\u73b0\uff0c\u7ed3\u5408meds2rdf\u80fd\u591f\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u7b26\u5408FAIR\u539f\u5219\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u6eaf\u6e90\u611f\u77e5\u53d1\u5e03\u548c\u57fa\u4e8e\u4e8b\u4ef6\u7684\u4e34\u5e8a\u6570\u636e\u4e92\u64cd\u4f5c\u3002\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06MEDS\u4e0e\u8bed\u4e49\u7f51\u8fde\u63a5\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u4e34\u5e8a\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u7684\u8bed\u4e49\u5c42\uff0c\u4e3a\u540e\u7eed\u57fa\u4e8e\u56fe\u7684\u5206\u6790\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u533b\u7597\u6570\u636e\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2601.03785", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03785", "abs": "https://arxiv.org/abs/2601.03785", "authors": ["Dehao Tao", "Guoliang Ma", "Yongfeng Huang", "Minghu Jiang"], "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "comment": null, "summary": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "AI": {"tldr": "Membox\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u9898\u8fde\u7eed\u6027\u7684\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7Topic Loom\u5c06\u5bf9\u8bdd\u6309\u4e3b\u9898\u5206\u7ec4\u5b58\u50a8\uff0cTrace Weaver\u6062\u590d\u957f\u7a0b\u4e8b\u4ef6\u65f6\u95f4\u7ebf\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u9075\u5faa\"\u788e\u7247\u5316-\u8865\u507f\"\u8303\u5f0f\uff0c\u5c06\u5bf9\u8bdd\u6d41\u5206\u89e3\u4e3a\u5b64\u7acb\u8bed\u53e5\u5b58\u50a8\uff0c\u7136\u540e\u901a\u8fc7\u5d4c\u5165\u68c0\u7d22\u6062\u590d\u8fde\u8d2f\u6027\uff0c\u8fd9\u7834\u574f\u4e86\u53d9\u4e8b\u548c\u56e0\u679c\u6d41\uff0c\u5e76\u4f7f\u68c0\u7d22\u504f\u5411\u8bcd\u6c47\u76f8\u4f3c\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u4e3b\u9898\u8fde\u7eed\u6027\u7684\u8bb0\u5fc6\u67b6\u6784\u3002", "method": "\u63d0\u51famembox\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff1a1) Topic Loom\u4ee5\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u6301\u7eed\u76d1\u63a7\u5bf9\u8bdd\uff0c\u5c06\u8fde\u7eed\u76f8\u540c\u4e3b\u9898\u7684\u8f6e\u6b21\u5206\u7ec4\u4e3a\u8fde\u8d2f\u7684\"\u8bb0\u5fc6\u76d2\"\uff1b2) Trace Weaver\u5c06\u5bc6\u5c01\u7684\u8bb0\u5fc6\u76d2\u94fe\u63a5\u6210\u957f\u7a0b\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u8f68\u8ff9\uff0c\u6062\u590d\u8de8\u4e0d\u8fde\u7eed\u6027\u7684\u5b8f\u89c2\u4e3b\u9898\u91cd\u73b0\u3002", "result": "\u5728LoCoMo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMembox\u5728\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe68%\u7684F1\u63d0\u5347\uff0c\u4f18\u4e8eMem0\u3001A-MEM\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\u4ec5\u4f7f\u7528\u73b0\u6709\u65b9\u6cd5\u6240\u9700\u4e0a\u4e0b\u6587\u4ee4\u724c\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u95f4\u53d6\u5f97\u4f18\u8d8a\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e3b\u9898\u8fde\u7eed\u6027\uff0cMembox\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba4\u77e5\u6fc0\u52b1\u7684\u673a\u5236\uff0c\u663e\u8457\u589e\u5f3a\u4e86LLM\u4ee3\u7406\u7684\u8fde\u8d2f\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.04181", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04181", "abs": "https://arxiv.org/abs/2601.04181", "authors": ["Nia Touko", "Matthew O A Ellis", "Cristiano Capone", "Alessio Burrello", "Elisa Donati", "Luca Manneschi"], "title": "Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition", "comment": null, "summary": "Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, \"plug-and-play\" myoelectric control for long-term prosthetic use.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u4f7f\u7528TCN\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u56e0\u679c\u81ea\u9002\u5e94\u6279\u5f52\u4e00\u5316\u3001GMM\u5bf9\u9f50\u4e0e\u7ecf\u9a8c\u56de\u653e\u3001\u5143\u5b66\u4e60\u4e09\u79cd\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8868\u9762\u808c\u7535\u4fe1\u53f7\u957f\u671f\u89e3\u7801\u7684\u8de8\u4f1a\u8bdd\u7a33\u5b9a\u6027\u3002", "motivation": "\u8868\u9762\u808c\u7535\u4fe1\u53f7\u957f\u671f\u89e3\u7801\u53d7\u7535\u6781\u504f\u79fb\u3001\u808c\u8089\u75b2\u52b3\u548c\u59ff\u52bf\u53d8\u5316\u5f15\u8d77\u7684\u4fe1\u53f7\u6f02\u79fb\u5f71\u54cd\uff0c\u73b0\u6709\u6a21\u578b\u5728\u4f1a\u8bdd\u95f4\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4e14\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u5927\u6570\u636e\u96c6\u6216\u9ad8\u8ba1\u7b97\u91cf\uff0c\u4e0d\u9002\u7528\u4e8e\u80fd\u6548\u7a7f\u6234\u8bbe\u5907\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u4f7f\u7528TCN\u9aa8\u5e72\u7f51\u7edc\uff0c\u5305\u542b\u4e09\u79cd\u90e8\u7f72\u5c31\u7eea\u7b56\u7565\uff1a1) \u56e0\u679c\u81ea\u9002\u5e94\u6279\u5f52\u4e00\u5316\u5b9e\u73b0\u5b9e\u65f6\u7edf\u8ba1\u5bf9\u9f50\uff1b2) GMM\u5bf9\u9f50\u4e0e\u7ecf\u9a8c\u56de\u653e\u9632\u6b62\u9057\u5fd8\uff1b3) \u5143\u5b66\u4e60\u5b9e\u73b0\u5feb\u901f\u5c11\u6837\u672c\u6821\u51c6\u3002", "result": "\u5728NinaPro DB6\u591a\u4f1a\u8bdd\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u663e\u8457\u7f29\u5c0f\u4f1a\u8bdd\u95f4\u51c6\u786e\u7387\u5dee\u8ddd\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002\u7ecf\u9a8c\u56de\u653e\u66f4\u65b0\u5728\u6709\u9650\u6570\u636e\u4e0b\u63d0\u4f9b\u4f18\u8d8a\u7a33\u5b9a\u6027\uff0c\u5143\u5b66\u4e60\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "\u4e3a\u957f\u671f\u5047\u80a2\u4f7f\u7528\u7684\u7a33\u5065\"\u5373\u63d2\u5373\u7528\"\u808c\u7535\u63a7\u5236\u5efa\u7acb\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6846\u67b6\u3002"}}
{"id": "2601.03812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03812", "abs": "https://arxiv.org/abs/2601.03812", "authors": ["Adilkhan Alikhanov", "Aidar Amangeldi", "Diar Demeubay", "Dilnaz Akhmetzhan", "Nurbek Moldakhmetov", "Omar Polat", "Galymzhan Zharas"], "title": "AI Generated Text Detection", "comment": null, "summary": "The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fDistilBERT\uff09\u5728\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe88.11%\uff0cROC-AUC\u8fbe0.96\uff0c\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5b66\u751f\u8d8a\u6765\u8d8a\u591a\u5730\u5c06AI\u751f\u6210\u5185\u5bb9\u5f53\u4f5c\u81ea\u5df1\u7684\u4f5c\u4e1a\u63d0\u4ea4\uff0c\u8fd9\u8fdd\u53cd\u4e86\u5b66\u672f\u8bda\u4fe1\u3002\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u6709\u6548\u7684AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528HC3\u548cDAIGT v2\u4e24\u4e2a\u6570\u636e\u96c6\u6784\u5efa\u7edf\u4e00\u57fa\u51c6\uff0c\u91c7\u7528\u57fa\u4e8e\u4e3b\u9898\u7684\u6570\u636e\u5206\u5272\u9632\u6b62\u4fe1\u606f\u6cc4\u9732\u3002\u8bc4\u4f30\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08TF-IDF\u903b\u8f91\u56de\u5f52\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08BiLSTM\u548cDistilBERT\uff09\u3002", "result": "TF-IDF\u903b\u8f91\u56de\u5f52\u8fbe\u523082.87%\u7684\u51c6\u786e\u7387\uff1bBiLSTM\u8fbe\u523088.86%\u51c6\u786e\u7387\uff1bDistilBERT\u8fbe\u523088.11%\u51c6\u786e\u7387\u4e14ROC-AUC\u6700\u9ad8\uff080.96\uff09\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e0a\u4e0b\u6587\u8bed\u4e49\u5efa\u6a21\u6bd4\u8bcd\u6c47\u7279\u5f81\u66f4\u6709\u6548\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662fDistilBERT\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u901a\u8fc7\u9002\u5f53\u8bc4\u4f30\u534f\u8bae\u51cf\u8f7b\u4e3b\u9898\u8bb0\u5fc6\u7684\u91cd\u8981\u6027\u3002\u672a\u6765\u5c06\u6269\u5c55\u6570\u636e\u96c6\u591a\u6837\u6027\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u66f4\u5c0f\u7684\u6a21\u578b\u548c\u786c\u4ef6\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2601.03997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03997", "abs": "https://arxiv.org/abs/2601.03997", "authors": ["Jos\u00e9 Pedro Evans", "Lu\u00eds Filipe Cunha", "Purifica\u00e7\u00e3o Silvano", "Al\u00edpio Jorge", "Nuno Guimar\u00e3es", "S\u00e9rgio Nunes", "Ricardo Campos"], "title": "VotIE: Information Extraction from Meeting Minutes", "comment": null, "summary": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.", "AI": {"tldr": "\u63d0\u51faVotIE\u4efb\u52a1\u7528\u4e8e\u4ece\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6295\u7968\u4fe1\u606f\uff0c\u5efa\u7acb\u8461\u8404\u7259\u5e02\u653f\u7eaa\u8981\u57fa\u51c6\uff0c\u53d1\u73b0\u5fae\u8c03\u7f16\u7801\u5668\u5728\u57df\u5185\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8de8\u5e02\u6cdb\u5316\u65f6few-shot LLM\u66f4\u9c81\u68d2\uff0c\u4e0d\u8fc7\u8ba1\u7b97\u6210\u672c\u9650\u5236\u5176\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u5e02\u653f\u4f1a\u8bae\u7eaa\u8981\u8bb0\u5f55\u5730\u65b9\u6c11\u4e3b\u51b3\u7b56\uff0c\u4f46\u4e0d\u540c\u4e8e\u6807\u51c6\u5316\u8bae\u4f1a\u8bb0\u5f55\uff0c\u5b83\u4eec\u4ee5\u9ad8\u5ea6\u5f02\u6784\u7684\u81ea\u7531\u53d9\u8ff0\u6587\u672c\u7f16\u7801\u6295\u7968\u7ed3\u679c\uff0c\u5404\u5e02\u683c\u5f0f\u5dee\u5f02\u5927\uff0c\u7ed9\u81ea\u52a8\u5316\u63d0\u53d6\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "\u5f15\u5165VotIE\uff08\u6295\u7968\u4fe1\u606f\u63d0\u53d6\uff09\u4efb\u52a1\uff0c\u4f7f\u7528\u8461\u8404\u7259\u5e02\u653f\u7eaa\u8981\u5efa\u7acb\u9996\u4e2a\u57fa\u51c6\uff08\u57fa\u4e8eCitiLink\u8bed\u6599\u5e93\uff09\uff0c\u6bd4\u8f83\u5fae\u8c03\u7f16\u7801\u5668\uff08XLM-R-CRF\uff09\u4e0e\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5728\u8de8\u5e02\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "1. \u57df\u5185\u8bc4\u4f30\uff1a\u5fae\u8c03\u7f16\u7801\u5668\uff08XLM-R-CRF\uff09\u8868\u73b0\u6700\u5f3a\uff0c\u8fbe\u523093.2%\u5b8fF1\uff0c\u4f18\u4e8e\u751f\u6210\u5f0f\u65b9\u6cd5\uff1b2. \u8de8\u5e02\u8bc4\u4f30\uff1a\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u800cfew-shot LLM\u5c55\u73b0\u66f4\u5f3a\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u4e0b\u964d\u663e\u8457\u8f83\u5c0f\u3002", "conclusion": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u6a21\u578b\u5728\u6cdb\u5316\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u5b9e\u9645\u5e94\u7528\uff0c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7f16\u7801\u5668\u4ecd\u662f\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\u7684\u66f4\u5b9e\u7528\u9009\u62e9\u3002\u516c\u5f00\u53d1\u5e03\u57fa\u51c6\u3001\u8bad\u7ec3\u6a21\u578b\u548c\u8bc4\u4f30\u6846\u67b6\u4ee5\u652f\u6301\u884c\u653fNLP\u7814\u7a76\u3002"}}
{"id": "2601.04055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04055", "abs": "https://arxiv.org/abs/2601.04055", "authors": ["Prith Sharma", "Austin Z. Henley"], "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients", "comment": null, "summary": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.", "AI": {"tldr": "MPO\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u5f0f\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u63d0\u793a\u89c6\u4e3a\u7ed3\u6784\u5316\u5bf9\u8c61\uff0c\u901a\u8fc7\u5c40\u90e8\u6587\u672c\u68af\u5ea6\u72ec\u7acb\u4f18\u5316\u5404\u4e2a\u8bed\u4e49\u90e8\u5206\uff0c\u63d0\u9ad8\u5c0f\u89c4\u6a21\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5c06\u63d0\u793a\u89c6\u4e3a\u6574\u4f53\u6587\u672c\u5757\uff0c\u96be\u4ee5\u5b9a\u4f4d\u9519\u8bef\u3001\u4fdd\u7559\u5173\u952e\u6307\u4ee4\u6216\u9632\u6b62\u63d0\u793a\u65e0\u9650\u589e\u957f\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u4f9d\u8d56\u663e\u5f0f\u7ed3\u6784\u7684\u5c0f\u578b\u5f00\u6e90\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "MPO\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u56fa\u5b9a\u8bed\u4e49\u90e8\u5206\uff08\u7cfb\u7edf\u89d2\u8272\u3001\u4e0a\u4e0b\u6587\u3001\u4efb\u52a1\u63cf\u8ff0\u3001\u7ea6\u675f\u3001\u8f93\u51fa\u683c\u5f0f\uff09\uff0c\u5e94\u7528\u7531\u6279\u8bc4\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5c40\u90e8\u6587\u672c\u68af\u5ea6\u72ec\u7acb\u4f18\u5316\u6bcf\u4e2a\u90e8\u5206\uff0c\u901a\u8fc7\u53bb\u91cd\u6574\u5408\u66f4\u65b0\u4ee5\u51cf\u5c11\u5197\u4f59\u548c\u7ec4\u4ef6\u95f4\u5e72\u6270\u3002", "result": "\u5728ARC-Challenge\u548cMMLU\u4e24\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528LLaMA-3 8B-Instruct\u548cMistral-7B-Instruct\u4f5c\u4e3a\u6c42\u89e3\u6a21\u578b\uff0cMPO\u59cb\u7ec8\u4f18\u4e8e\u672a\u8c03\u4f18\u7684\u7ed3\u6784\u5316\u63d0\u793a\u548cTextGrad\u57fa\u7ebf\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u6539\u53d8\u63d0\u793a\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u4fdd\u6301\u56fa\u5b9a\u63d0\u793a\u6a21\u5f0f\u7684\u540c\u65f6\u5e94\u7528\u5c40\u90e8\u3001\u5206\u8282\u4f18\u5316\u662f\u63d0\u9ad8\u5c0f\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u7684\u6709\u6548\u5b9e\u7528\u65b9\u6cd5\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u4f18\u5316\u8fc7\u7a0b\u3002"}}
