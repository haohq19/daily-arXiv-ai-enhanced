{"id": "2602.12284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12284", "abs": "https://arxiv.org/abs/2602.12284", "authors": ["Han Jinzhen", "Kim Jisung", "Yang Jong Soo", "Yun Hong Sik"], "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification", "comment": null, "summary": "Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u707e\u5bb3\u63a8\u6587\u5206\u7c7b\u6846\u67b6\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728Llama 3.1 8B\u4e0a\u901a\u8fc7LoRA\u548cQLoRA\u5b9e\u73b0\u4e86\u9ad8\u6548\u90e8\u7f72\uff0c\u53d1\u73b0RAG\u7b56\u7565\u4f1a\u964d\u4f4e\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u4eba\u9053\u4e3b\u4e49\u4fe1\u606f\u53ca\u65f6\u5206\u7c7b\u5bf9\u6709\u6548\u7684\u707e\u5bb3\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7d27\u6025\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6574\u5408\u548c\u6807\u51c6\u5316HumAID\u6570\u636e\u96c6\u6784\u5efa\u7edf\u4e00\u5b9e\u9a8c\u8bed\u6599\u5e93\uff0c\u5f62\u6210\u53cc\u4efb\u52a1\u57fa\u51c6\uff1a\u4eba\u9053\u4e3b\u4e49\u4fe1\u606f\u5206\u7c7b\u548c\u4e8b\u4ef6\u7c7b\u578b\u8bc6\u522b\u3002\u7cfb\u7edf\u8bc4\u4f30\u63d0\u793a\u7b56\u7565\u3001LoRA\u5fae\u8c03\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u5728Llama 3.1 8B\u4e0a\u7684\u8868\u73b0\u3002", "result": "LoRA\u5b9e\u73b0\u4e8679.62%\u7684\u4eba\u9053\u4e3b\u4e49\u5206\u7c7b\u51c6\u786e\u7387\uff08\u6bd4\u96f6\u6837\u672c\u5b66\u4e60\u63d0\u9ad837.79%\uff09\uff0c\u4ec5\u8bad\u7ec3\u7ea62%\u7684\u53c2\u6570\uff1bQLoRA\u4ee550%\u5185\u5b58\u6210\u672c\u5b9e\u73b0LoRA\u6027\u80fd\u768499.4%\uff1bRAG\u7b56\u7565\u4f1a\u56e0\u68c0\u7d22\u793a\u4f8b\u7684\u6807\u7b7e\u566a\u58f0\u800c\u964d\u4f4e\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6784\u5efa\u53ef\u9760\u5371\u673a\u60c5\u62a5\u7cfb\u7edf\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.12389", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12389", "abs": "https://arxiv.org/abs/2602.12389", "authors": ["Siyuan Li", "Yunjia Wu", "Yiyong Xiao", "Pingyang Huang", "Peize Li", "Ruitong Liu", "Yan Wen", "Te Sun", "Fangyi Pei"], "title": "Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting", "comment": null, "summary": "Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots", "AI": {"tldr": "\u63d0\u51faEntity State Tuning (EST)\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u62a4\u6301\u4e45\u4e14\u6301\u7eed\u6f14\u5316\u7684\u5b9e\u4f53\u72b6\u6001\u6765\u89e3\u51b3TKG\u9884\u6d4b\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u9aa8\u5e72\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TKG\u9884\u6d4b\u65b9\u6cd5\u5927\u591a\u662f\u65e0\u72b6\u6001\u7684\uff0c\u6bcf\u6b21\u65f6\u95f4\u6233\u90fd\u4ece\u6709\u9650\u67e5\u8be2\u7a97\u53e3\u91cd\u65b0\u8ba1\u7b97\u5b9e\u4f53\u8868\u793a\uff0c\u5bfc\u81f4\"\u60c5\u666f\u6027\u9057\u5fd8\"\u548c\u957f\u671f\u4f9d\u8d56\u5feb\u901f\u8870\u51cf\uff0c\u9650\u5236\u4e86\u957f\u671f\u9884\u6d4b\u80fd\u529b\u3002", "method": "EST\u6846\u67b6\u5305\u542b\uff1a1) \u5168\u5c40\u72b6\u6001\u7f13\u51b2\u533a\u5b58\u50a8\u6301\u4e45\u5b9e\u4f53\u72b6\u6001\uff1b2) \u62d3\u6251\u611f\u77e5\u72b6\u6001\u611f\u77e5\u5668\u5c06\u5b9e\u4f53\u72b6\u6001\u5148\u9a8c\u6ce8\u5165\u7ed3\u6784\u7f16\u7801\uff1b3) \u7edf\u4e00\u65f6\u5e8f\u4e0a\u4e0b\u6587\u6a21\u5757\u805a\u5408\u72b6\u6001\u589e\u5f3a\u4e8b\u4ef6\uff1b4) \u53cc\u8f68\u6f14\u5316\u673a\u5236\u5e73\u8861\u53ef\u5851\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u5c06\u66f4\u65b0\u4e0a\u4e0b\u6587\u5199\u56de\u5168\u5c40\u72b6\u6001\u5185\u5b58\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEST\u6846\u67b6\u4e00\u81f4\u6027\u5730\u63d0\u5347\u4e86\u591a\u79cd\u9aa8\u5e72\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u72b6\u6001\u6301\u4e45\u6027\u5bf9\u957f\u671fTKG\u9884\u6d4b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "EST\u901a\u8fc7\u7ef4\u62a4\u6301\u7eed\u6f14\u5316\u7684\u5b9e\u4f53\u72b6\u6001\u6709\u6548\u89e3\u51b3\u4e86TKG\u9884\u6d4b\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3aTKG\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u6301\u4e45\u8bb0\u5fc6\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.12338", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12338", "abs": "https://arxiv.org/abs/2602.12338", "authors": ["Farshad Zeinali", "Mahdi Boloursaz Mashhadi", "Dusit Niyato", "Rahim Tafazolli"], "title": "Wireless TokenCom: RL-Based Tokenizer Agreement for Multi-User Wireless Token Communications", "comment": "Submitted to IEEE TVT for possible publication", "summary": "Token Communications (TokenCom) has recently emerged as an effective new paradigm, where tokens are the unified units of multimodal communications and computations, enabling efficient digital semantic- and goal-oriented communications in future wireless networks. To establish a shared semantic latent space, the transmitters/receivers in TokenCom need to agree on an identical tokenizer model and codebook. To this end, an initial Tokenizer Agreement (TA) process is carried out in each communication episode, where the transmitter/receiver cooperate to choose from a set of pre-trained tokenizer models/ codebooks available to them both for efficient TokenCom. In this correspondence, we investigate TA in a multi-user downlink wireless TokenCom scenario, where the base station equipped with multiple antennas transmits video token streams to multiple users. We formulate the corresponding mixed-integer non-convex problem, and propose a hybrid reinforcement learning (RL) framework that integrates a deep Q-network (DQN) for joint tokenizer agreement and sub-channel assignment, with a deep deterministic policy gradient (DDPG) for beamforming. Simulation results show that the proposed framework outperforms baseline methods in terms of semantic quality and resource efficiency, while reducing the freezing events in video transmission by 68% compared to the conventional H.265-based scheme.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u591a\u7528\u6237\u65e0\u7ebfTokenCom\u4e2d\u7684Tokenizer\u534f\u8bae\u95ee\u9898\uff0c\u7ed3\u5408DQN\u8fdb\u884ctokenizer\u9009\u62e9\u548c\u5b50\u4fe1\u9053\u5206\u914d\uff0cDDPG\u8fdb\u884c\u6ce2\u675f\u6210\u5f62\uff0c\u663e\u8457\u63d0\u5347\u8bed\u4e49\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "TokenCom\u4f5c\u4e3a\u65b0\u5174\u901a\u4fe1\u8303\u5f0f\uff0c\u9700\u8981\u6536\u53d1\u53cc\u65b9\u5c31tokenizer\u6a21\u578b\u548c\u7801\u672c\u8fbe\u6210\u4e00\u81f4\u3002\u5728\u591a\u7528\u6237\u4e0b\u884c\u65e0\u7ebfTokenCom\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u9ad8\u6548\u8fdb\u884cTokenizer\u534f\u8bae\uff08TA\uff09\u8fc7\u7a0b\uff0c\u8054\u5408\u4f18\u5316tokenizer\u9009\u62e9\u3001\u5b50\u4fe1\u9053\u5206\u914d\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u63d0\u5347\u8bed\u4e49\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\uff0c\u662f\u672c\u6587\u7684\u7814\u7a76\u52a8\u673a\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u8054\u5408\u4f18\u5316tokenizer\u534f\u8bae\u548c\u5b50\u4fe1\u9053\u5206\u914d\uff1b2\uff09\u4f7f\u7528\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u3002\u5c06\u6df7\u5408\u6574\u6570\u975e\u51f8\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7RL\u65b9\u6cd5\u534f\u540c\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728\u8bed\u4e49\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edfH.265\u65b9\u6848\u76f8\u6bd4\uff0c\u89c6\u9891\u4f20\u8f93\u4e2d\u7684\u51bb\u7ed3\u4e8b\u4ef6\u51cf\u5c11\u4e8668%\u3002", "conclusion": "\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u7528\u6237\u65e0\u7ebfTokenCom\u4e2d\u7684Tokenizer\u534f\u8bae\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316tokenizer\u9009\u62e9\u3001\u5b50\u4fe1\u9053\u5206\u914d\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u8bed\u4e49\u548c\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.12414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12414", "abs": "https://arxiv.org/abs/2602.12414", "authors": ["Maximilian Idahl", "Benedikt Droste", "Bj\u00f6rn Pl\u00fcster", "Jan Philipp Harries"], "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale", "comment": "Release: https://hf.co/collections/ellamind/propella-1", "summary": "Since FineWeb-Edu, data curation for LLM pretraining has predominantly relied on single scalar quality scores produced by small classifiers. A single score conflates multiple quality dimensions, prevents flexible filtering, and offers no interpretability. We introduce propella-1, a family of small multilingual LLMs (0.6B, 1.7B, 4B parameters) that annotate text documents across 18 properties organized into six categories: core content, classification, quality and value, audience and purpose, safety and compliance, and geographic relevance. The models support 57 languages and produce structured JSON annotations conforming to a predefined schema. Evaluated against a frontier commercial LLM as a reference annotator, the 4B model achieves higher agreement than much larger general-purpose models. We release propella-annotations, a dataset of over three billion document annotations covering major pretraining corpora including data from FineWeb-2, FinePDFs, HPLT 3.0, and Nemotron-CC. Using these annotations, we present a multi-dimensional compositional analysis of widely used pretraining datasets, revealing substantial differences in quality, reasoning depth, and content composition that single-score approaches cannot capture. All model weights and annotations are released under permissive, commercial-use licenses.", "AI": {"tldr": "\u63d0\u51fapropella-1\u7cfb\u5217\u5c0f\u578b\u591a\u8bed\u8a00LLM\uff0c\u7528\u4e8e\u6587\u672c\u6587\u6863\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\u6807\u6ce8\uff0c\u66ff\u4ee3\u4f20\u7edf\u5355\u4e00\u8bc4\u5206\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u5305\u542b30\u4ebf\u6587\u6863\u6807\u6ce8\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524dLLM\u9884\u8bad\u7ec3\u6570\u636e\u7b5b\u9009\u4e3b\u8981\u4f9d\u8d56\u5c0f\u578b\u5206\u7c7b\u5668\u4ea7\u751f\u7684\u5355\u4e00\u8d28\u91cf\u5206\u6570\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a1\uff09\u5c06\u591a\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u6df7\u4e3a\u4e00\u8c08\uff1b2\uff09\u65e0\u6cd5\u7075\u6d3b\u8fc7\u6ee4\uff1b3\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u591a\u7ef4\u5ea6\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1propella-1\u7cfb\u5217\u5c0f\u578b\u591a\u8bed\u8a00LLM\uff080.6B\u30011.7B\u30014B\u53c2\u6570\uff09\uff0c\u652f\u630157\u79cd\u8bed\u8a00\uff0c\u6309\u7167\u9884\u5b9a\u4e49\u6a21\u5f0f\u751f\u6210\u7ed3\u6784\u5316JSON\u6807\u6ce8\uff0c\u6db5\u76d618\u4e2a\u5c5e\u6027\uff0c\u5206\u4e3a\u516d\u4e2a\u7c7b\u522b\uff1a\u6838\u5fc3\u5185\u5bb9\u3001\u5206\u7c7b\u3001\u8d28\u91cf\u4e0e\u4ef7\u503c\u3001\u53d7\u4f17\u4e0e\u76ee\u7684\u3001\u5b89\u5168\u5408\u89c4\u3001\u5730\u7406\u76f8\u5173\u6027\u3002", "result": "4B\u6a21\u578b\u5728\u6807\u6ce8\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u901a\u7528\u6a21\u578b\uff1b\u53d1\u5e03\u4e86propella-annotations\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc730\u4ebf\u6587\u6863\u6807\u6ce8\uff0c\u8986\u76d6FineWeb-2\u3001FinePDFs\u3001HPLT 3.0\u3001Nemotron-CC\u7b49\u4e3b\u8981\u9884\u8bad\u7ec3\u8bed\u6599\uff1b\u591a\u7ef4\u5ea6\u5206\u6790\u63ed\u793a\u4e86\u4f20\u7edf\u5355\u4e00\u8bc4\u5206\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7684\u8d28\u91cf\u3001\u63a8\u7406\u6df1\u5ea6\u548c\u5185\u5bb9\u6784\u6210\u5dee\u5f02\u3002", "conclusion": "propella-1\u6a21\u578b\u63d0\u4f9b\u4e86\u6bd4\u5355\u4e00\u8bc4\u5206\u66f4\u7cbe\u7ec6\u3001\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u7684\u6570\u636e\u6807\u6ce8\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u6539\u8fdbLLM\u9884\u8bad\u7ec3\u6570\u636e\u7b5b\u9009\uff0c\u6240\u6709\u6a21\u578b\u6743\u91cd\u548c\u6807\u6ce8\u6570\u636e\u90fd\u91c7\u7528\u5546\u4e1a\u53cb\u597d\u7684\u8bb8\u53ef\u534f\u8bae\u53d1\u5e03\u3002"}}
{"id": "2602.12421", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.12421", "abs": "https://arxiv.org/abs/2602.12421", "authors": ["Minduli C. Wijayatunga", "Julian Guinane", "Nathan D. Wallace", "Xiaofeng Wu"], "title": "An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation", "comment": null, "summary": "Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures.\n  CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \\pm 44.46$ mm in relative position and $1.25 \\pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \\pm 5.29$ mm in position and $2.23 \\pm 1.72$ mm/s in velocity.", "AI": {"tldr": "CORTEX\u662f\u4e00\u4e2a\u7528\u4e8e\u8fd1\u8ddd\u79bb\u4ea4\u4f1a\u5bf9\u63a5\u7684\u81ea\u4e3b\u611f\u77e5-\u4f18\u5316\u8f68\u8ff9\u8bbe\u8ba1\u4e0e\u5236\u5bfc\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u4e0e\u51f8\u4f18\u5316\uff0c\u5177\u5907\u5b9e\u65f6\u91cd\u89c4\u5212\u548c\u5b89\u5168\u8f68\u9053\u4e2d\u6b62\u80fd\u529b\uff0c\u5728\u4eff\u771f\u548c\u786c\u4ef6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u536b\u661f\u670d\u52a1\u4efb\u52a1\u9700\u8981\u5728\u4e25\u683c\u7684\u5b89\u5168\u548c\u64cd\u4f5c\u7ea6\u675f\u4e0b\u6267\u884c\u8fd1\u8ddd\u79bb\u4ea4\u4f1a\u5bf9\u63a5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u4ee5\u9002\u5e94\u673a\u8f7d\u4f7f\u7528\uff0c\u5e76\u5e94\u5bf9\u611f\u77e5\u3001\u6267\u884c\u548c\u52a8\u529b\u5b66\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "CORTEX\u6846\u67b6\u6574\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u7ba1\u9053\u4e0e\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u8f68\u8ff9\u8bbe\u8ba1\u548c\u5236\u5bfc\uff0c\u5305\u62ec\u53c2\u8003\u8f68\u8ff9\u91cd\u65b0\u751f\u6210\u548c\u5b89\u5168\u8f68\u9053\u4e2d\u6b62\u903b\u8f91\uff0c\u4ee5\u5e94\u5bf9\u4f20\u611f\u5668\u6545\u969c\u548c\u53d1\u52a8\u673a\u6545\u969c\u5bfc\u81f4\u7684\u5927\u504f\u5dee\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u8f6f\u4ef6\u4eff\u771f\u4e2d\uff0c\u8499\u7279\u5361\u6d1b\u6d4b\u8bd5\u5728\u6700\u4e25\u82db\u60c5\u51b5\u4e0b\u8fbe\u5230\u7ec8\u7aef\u5bf9\u63a5\u8bef\u5dee\uff1a\u4f4d\u7f6e36.85\u00b144.46\u6beb\u7c73\uff0c\u901f\u5ea61.25\u00b12.26\u6beb\u7c73/\u79d2\u3002\u5728\u5e73\u9762\u6c14\u6d6e\u6d4b\u8bd5\u53f0\u4e0a\uff0c18\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff0810\u4e2a\u6807\u79f0\uff0c8\u4e2a\u975e\u6807\u79f0\uff09\u8fbe\u5230\u4f4d\u7f6e\u8bef\u5dee8.09\u00b15.29\u6beb\u7c73\uff0c\u901f\u5ea6\u8bef\u5dee2.23\u00b11.72\u6beb\u7c73/\u79d2\u3002", "conclusion": "CORTEX\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u4e3b\u3001\u611f\u77e5\u542f\u7528\u7684\u5b9e\u65f6\u8f68\u8ff9\u8bbe\u8ba1\u4e0e\u5236\u5bfc\uff0c\u5728\u4eff\u771f\u548c\u786c\u4ef6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u80fd\uff0c\u80fd\u591f\u5904\u7406\u4f20\u611f\u5668\u6545\u969c\u548c\u53d1\u52a8\u673a\u6545\u969c\u7b49\u975e\u6807\u79f0\u60c5\u51b5\u3002"}}
{"id": "2602.12635", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12635", "abs": "https://arxiv.org/abs/2602.12635", "authors": ["Pengxiang Zhao", "Hui-Ling Zhen", "Xing Li", "Han Bao", "Weizhe Lin", "Zhiyuan Yang", "Ziwei Yu", "Xin Wang", "Mingxuan Yuan", "Xianzhi Yu", "Zhenhua Dong"], "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats", "comment": null, "summary": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.", "AI": {"tldr": "HiFloat\u6d6e\u70b9\u683c\u5f0f\uff08HiF8\u548cHiF4\uff09\u4e13\u4e3aAscend NPU\u8bbe\u8ba1\uff0c\u57284\u4f4d\u91cf\u5316\u4e2d\u901a\u8fc7\u5206\u5c42\u7f29\u653e\u907f\u514d\u7cbe\u5ea6\u5d29\u6e83\uff0c\u517c\u5bb9\u73b0\u6709PTQ\u6846\u67b6\uff0c\u4e3aNPU\u4e0a\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u4f4e\u6bd4\u7279\u6d6e\u70b9\u683c\u5f0f\uff08\u5982MXFP\u548cNVFP4\uff09\u4e3a\u7cbe\u5ea6\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u9700\u8981\u8bc4\u4f30\u4e13\u4e3aAscend NPU\u8bbe\u8ba1\u7684HiFloat\u683c\u5f0f\u5bb6\u65cf\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u6bd4\u8f83\u8bc4\u4f30HiFloat\u683c\u5f0f\uff08HiF8\u548cHiF4\uff09\uff0c\u6db5\u76d6\u6743\u91cd-\u6fc0\u6d3b\u548cKV\u7f13\u5b58\u4efb\u52a1\uff0c\u5206\u6790\u4e0d\u540c\u683c\u5f0f\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u83b7\u5f97\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a1) INT8\u9002\u5408\u7a84\u8303\u56f4\u6570\u636e\uff0c\u6d6e\u70b9\u683c\u5f0f\u5728\u9ad8\u65b9\u5dee\u6570\u636e\u4e2d\u8868\u73b0\u66f4\u4f73\uff1b2) \u57284\u4f4d\u91cf\u5316\u4e2d\uff0cHiF4\u7684\u5206\u5c42\u7f29\u653e\u907f\u514d\u4e86\u6574\u6570\u683c\u5f0f\u7684\u7cbe\u5ea6\u5d29\u6e83\uff1b3) HiFloat\u5b8c\u5168\u517c\u5bb9\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u6846\u67b6\u3002", "conclusion": "HiFloat\u4e3aNPU\u4e0a\u7684\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u57284\u4f4d\u91cf\u5316\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.12549", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12549", "abs": "https://arxiv.org/abs/2602.12549", "authors": ["Yue Lin", "Yang Liu", "Dong Wang", "Huchuan Lu"], "title": "Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking", "comment": "Accepted by ICRA 2026", "summary": "The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.", "AI": {"tldr": "Eva-Tracker\uff1a\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u8ddf\u8e2a\u7684\u53ef\u89c1\u6027\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u7684FoV-ESDF\u6d88\u9664ESDF\u66f4\u65b0\u5f00\u9500\uff0c\u5e76\u96c6\u6210\u76ee\u6807\u91cd\u83b7\u6062\u590d\u80fd\u529b", "motivation": "\u4f20\u7edfESDF\u5728\u53ef\u89c1\u6027\u8bc4\u4f30\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u9891\u7e41\u66f4\u65b0\u5e26\u6765\u5de8\u5927\u8ba1\u7b97\u5f00\u9500\uff0c\u5f71\u54cd\u8ddf\u8e2a\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u548c\u6548\u7387", "method": "1) \u76ee\u6807\u8f68\u8ff9\u9884\u6d4b\u548c\u53ef\u89c1\u6027\u611f\u77e5\u521d\u59cb\u8def\u5f84\u751f\u6210\u7b97\u6cd5\uff1b2) \u9884\u8ba1\u7b97\u7684FoV-ESDF\uff08\u89c6\u91ceESDF\uff09\u5b9e\u73b0\u5feb\u901f\u53ef\u89c1\u6027\u8bc4\u4f30\uff1b3) \u57fa\u4e8e\u53ef\u5fae\u5206FoV-ESDF\u76ee\u6807\u7684\u8f68\u8ff9\u4f18\u5316", "result": "\u5728\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u8ddf\u8e2a\u7ed3\u679c\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e", "conclusion": "Eva-Tracker\u901a\u8fc7\u6d88\u9664ESDF\u66f4\u65b0\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u7a7a\u4e2d\u8ddf\u8e2a\uff0c\u5728\u4fdd\u6301\u8fde\u7eed\u53ef\u89c1\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u62c5"}}
{"id": "2602.13166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13166", "abs": "https://arxiv.org/abs/2602.13166", "authors": ["Hugo Henry", "Arthur Tsai", "Kelly Cohen"], "title": "Optimal Take-off under Fuzzy Clearances", "comment": "12 pages, 12 figures, conference paper", "summary": "This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u969c\u788d\u7269\u89c4\u907f\u67b6\u6784\uff0c\u7ed3\u5408\u6700\u4f18\u63a7\u5236\u4e0e\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u81ea\u9002\u5e94\u7ea6\u675f\u5904\u7406\uff0c\u4f46\u53d1\u73b0FALCON\u548cIPOPT\u8f6f\u4ef6\u5b58\u5728\u517c\u5bb9\u6027\u95ee\u9898\u5bfc\u81f4\u7ea6\u675f\u65e0\u6cd5\u6267\u884c\u3002", "motivation": "\u7ecf\u5178\u6700\u4f18\u63a7\u5236\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u822a\u7a7a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u9700\u8981\u53ef\u89e3\u91ca\u51b3\u7b56\uff0c\u4fc3\u4f7f\u5f00\u53d1\u80fd\u81ea\u9002\u5e94\u5904\u7406\u7ea6\u675f\u7684\u6df7\u5408\u67b6\u6784\u3002", "method": "\u91c7\u7528\u4e09\u5c42Takagi-Sugeno-Kang\u6a21\u7cca\u7cfb\u7edf\uff0c\u57fa\u4e8eFAA\u548cEASA\u6cd5\u89c4\u8c03\u6574\u7ea6\u675f\u534a\u5f84\u3001\u7d27\u6025\u7a0b\u5ea6\u548c\u6fc0\u6d3b\u51b3\u7b56\uff0c\u5c06\u6a21\u7cca\u751f\u6210\u7684\u95f4\u9699\u4f5c\u4e3a\u8f6f\u7ea6\u675f\u7eb3\u5165\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528FALCON\u5de5\u5177\u7bb1\u548cIPOPT\u6c42\u89e3\u3002", "result": "\u6982\u5ff5\u9a8c\u8bc1\u663e\u793a\u6bcf\u6b21\u8fed\u4ee3\u8ba1\u7b97\u65f6\u95f42-3\u79d2\uff0c\u9002\u5408\u8fd1\u5b9e\u65f6\u5e94\u7528\uff0c\u4f46\u53d1\u73b0FALCON\u548cIPOPT\u6700\u65b0\u7248\u672c\u5b58\u5728\u8f6f\u4ef6\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u62c9\u683c\u6717\u65e5\u60e9\u7f5a\u9879\u59cb\u7ec8\u4e3a\u96f6\uff0c\u5bfc\u81f4\u7ea6\u675f\u65e0\u6cd5\u6267\u884c\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u8f6f\u4ef6\u517c\u5bb9\u6027\u95ee\u9898\u3002\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u9a8c\u8bc1\u65e9\u671f\u8f6f\u4ef6\u7248\u672c\u3001\u4f18\u5316\u6a21\u7cca\u96b6\u5c5e\u51fd\u6570\u3001\u6269\u5c55\u5230\u9ad8\u4fdd\u771f\u98de\u673a\u6a21\u578b\u548c\u968f\u673a\u969c\u788d\u73af\u5883\u3002"}}
{"id": "2602.12806", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12806", "abs": "https://arxiv.org/abs/2602.12806", "authors": ["Nata\u0161a Kr\u010do", "Zexi Yao", "Matthieu Meeus", "Yves-Alexandre de Montjoye"], "title": "RAT-Bench: A Comprehensive Benchmark for Text Anonymization", "comment": null, "summary": "Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.", "AI": {"tldr": "RAT-Bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u518d\u8bc6\u522b\u98ce\u9669\u7684\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7f8e\u56fd\u4eba\u53e3\u7edf\u8ba1\u751f\u6210\u5408\u6210\u6587\u672c\uff0c\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709\u5de5\u5177\u5728\u975e\u6807\u51c6\u76f4\u63a5\u6807\u8bc6\u7b26\u548c\u95f4\u63a5\u6807\u8bc6\u7b26\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cLLM\u533f\u540d\u5316\u5de5\u5177\u63d0\u4f9b\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\uff08\u5982Microsoft Presidio\u3001Anthropic PII purifier\uff09\u901a\u5e38\u53ea\u8bc4\u4f30\u5176\u79fb\u9664\u7279\u5b9a\u6807\u8bc6\u7b26\uff08\u5982\u59d3\u540d\uff09\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u9632\u6b62\u518d\u8bc6\u522b\u65b9\u9762\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u518d\u8bc6\u522b\u98ce\u9669\u7684\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faRAT-Bench\u57fa\u51c6\u6d4b\u8bd5\uff1a1\uff09\u4f7f\u7528\u7f8e\u56fd\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u751f\u6210\u5305\u542b\u5404\u79cd\u76f4\u63a5\u548c\u95f4\u63a5\u6807\u8bc6\u7b26\u7684\u5408\u6210\u6587\u672c\uff1b2\uff09\u6db5\u76d6\u4e0d\u540c\u9886\u57df\u3001\u8bed\u8a00\u548c\u96be\u5ea6\u7ea7\u522b\uff1b3\uff09\u8bc4\u4f30\u57fa\u4e8eNER\u548cLLM\u7684\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\uff1b4\uff09\u57fa\u4e8eLLM\u653b\u51fb\u8005\u80fd\u4ece\u533f\u540d\u5316\u6587\u672c\u4e2d\u6b63\u786e\u63a8\u65ad\u7684\u5c5e\u6027\uff0c\u8ba1\u7b97\u5728\u7f8e\u56fd\u4eba\u53e3\u4e2d\u7684\u518d\u8bc6\u522b\u98ce\u9669\uff1b5\uff09\u9002\u5f53\u8003\u8651\u6807\u8bc6\u7b26\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "result": "1\uff09\u4e0d\u540c\u5de5\u5177\u80fd\u529b\u5dee\u5f02\u5f88\u5927\uff1b2\uff09\u5373\u4f7f\u6700\u597d\u7684\u5de5\u5177\u4e5f\u8fdc\u975e\u5b8c\u7f8e\uff0c\u7279\u522b\u662f\u5728\u76f4\u63a5\u6807\u8bc6\u7b26\u975e\u6807\u51c6\u4e66\u5199\u548c\u95f4\u63a5\u6807\u8bc6\u7b26\u80fd\u5b9e\u73b0\u518d\u8bc6\u522b\u7684\u60c5\u51b5\u4e0b\uff1b3\uff09\u57fa\u4e8eLLM\u7684\u533f\u540d\u5316\u5de5\u5177\uff08\u5305\u62ec\u65b0\u7684\u8fed\u4ee3\u533f\u540d\u5316\u5668\uff09\u63d0\u4f9b\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff1b4\uff09LLM\u5de5\u5177\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\uff0c\u7279\u522b\u662f\u8981\u5904\u7406\u975e\u6807\u51c6\u6807\u8bc6\u7b26\u548c\u95f4\u63a5\u6807\u8bc6\u7b26\u3002\u5c06\u53d1\u5e03RAT-Bench\u57fa\u51c6\u5e76\u9f13\u52b1\u793e\u533a\u6269\u5c55\uff0c\u7279\u522b\u662f\u6269\u5c55\u5230\u5176\u4ed6\u5730\u7406\u533a\u57df\u3002"}}
{"id": "2602.12590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12590", "abs": "https://arxiv.org/abs/2602.12590", "authors": ["Jinze Chen", "Wei Zhai", "Han Han", "Tiankai Ma", "Yang Cao", "Bin Li", "Zheng-Jun Zha"], "title": "Unbiased Gradient Estimation for Event Binning via Functional Backpropagation", "comment": null, "summary": "Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\\% lower RMS error and 1.57$\\times$ faster convergence. On complex downstream tasks, we achieve 9.4\\% lower EPE in self-supervised optical flow, and 5.1\\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u504f\u68af\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u5f31\u5bfc\u6570\u89e3\u51b3\u4e8b\u4ef6\u89c6\u89c9\u4e2dbinning\u51fd\u6570\u4e0d\u8fde\u7eed\u5bfc\u81f4\u7684\u68af\u5ea6\u622a\u65ad\u95ee\u9898", "motivation": "\u4e8b\u4ef6\u89c6\u89c9\u5c06\u52a8\u6001\u573a\u666f\u7f16\u7801\u4e3a\u5f02\u6b65\u65f6\u7a7a\u8109\u51b2\uff08\u4e8b\u4ef6\uff09\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u4e8b\u4ef6\u5206\u7bb1\u6210\u5e27\uff0c\u4f46\u5206\u7bb1\u51fd\u6570\u4e0d\u8fde\u7eed\u5bfc\u81f4\u68af\u5ea6\u622a\u65ad\uff0c\u8feb\u4f7f\u7b97\u6cd5\u4f9d\u8d56\u5e27\u7ea7\u7279\u5f81\u3002\u76f4\u63a5\u5b66\u4e60\u539f\u59cb\u4e8b\u4ef6\u5219\u56e0\u5206\u7bb1\u64cd\u4f5c\u7684\u4e0d\u8fde\u7eed\u6027\u5bfc\u81f4\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u65e0\u504f\u68af\u5ea6\u4f30\u8ba1\u6846\u67b6\uff1a\u5728\u524d\u5411\u4f20\u64ad\u4fdd\u6301\u8f93\u51fa\u4e0d\u53d8\u7684\u540c\u65f6\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5408\u6210\u5f31\u5bfc\u6570\u3002\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u5206\u90e8\u79ef\u5206\uff1a\u5c06\u76ee\u6807\u51fd\u6570\u63d0\u5347\u4e3a\u6cdb\u51fd\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5f97\u5230\u5206\u7bb1\u51fd\u6570\u5bfc\u6570\u7684\u79ef\u5206\u5f62\u5f0f\uff0c\u5176\u4e2d\u4f59\u5207\u51fd\u6570\u81ea\u7136\u51fa\u73b0\u3002\u901a\u8fc7\u4ece\u91c7\u6837\u7684\u4f59\u5207\u5411\u91cf\u91cd\u6784\u4f59\u5207\u51fd\u6570\uff0c\u8ba1\u7b97\u5f31\u5bfc\u6570\uff0c\u53ef\u8bc1\u660e\u5339\u914d\u5e73\u6ed1\u548c\u975e\u5e73\u6ed1\u76ee\u6807\u7684\u957f\u671f\u6709\u9650\u5dee\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u5728\u7b80\u5355\u4f18\u5316\u578b\u81ea\u8fd0\u52a8\u4f30\u8ba1\u4e2d\uff0cRMS\u8bef\u5dee\u964d\u4f4e3.2%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb1.57\u500d\uff1b\u5728\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u81ea\u76d1\u7763\u5149\u6d41\u4f30\u8ba1\u7684EPE\u964d\u4f4e9.4%\uff0cSLAM\u7684RMS\u8bef\u5dee\u964d\u4f4e5.1%\uff0c\u663e\u8457\u63d0\u5347\u4e8b\u4ef6\u89c6\u89c9\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5408\u6210\u5f31\u5bfc\u6570\u89e3\u51b3\u4e86\u4e8b\u4ef6\u89c6\u89c9\u4e2d\u5206\u7bb1\u51fd\u6570\u4e0d\u8fde\u7eed\u5bfc\u81f4\u7684\u68af\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u504f\u68af\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.12506", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12506", "abs": "https://arxiv.org/abs/2602.12506", "authors": ["Rosie Zhao", "Anshul Shah", "Xiaoyu Zhu", "Xinke Deng", "Zhongyu Jiang", "Yang Yang", "Joerg Liebelt", "Arnab Mondal"], "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs", "comment": null, "summary": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.", "AI": {"tldr": "RL\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u63d0\u5347\uff0c\u4f46\u5bf9\u6587\u672c\u6270\u52a8\u654f\u611f\uff0c\u5b58\u5728\u51c6\u786e\u6027-\u5fe0\u5b9e\u6027\u6743\u8861\uff0c\u9700\u8981\u540c\u65f6\u5173\u6ce8\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "RL\u5fae\u8c03\u5df2\u6210\u4e3a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u6269\u5c55\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u5b58\u5728\u89c6\u89c9\u57fa\u7840\u8584\u5f31\u3001\u5e7b\u89c9\u548c\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u7ebf\u7d22\u7b49\u8106\u5f31\u6027\u3002\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u8106\u5f31\u6027\u5e76\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u53d7\u63a7\u6587\u672c\u6270\u52a8\uff08\u8bef\u5bfc\u6027\u6807\u9898\u6216\u9519\u8bef\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\uff09\u6765\u6d4b\u8bd5\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5206\u6790RL\u5fae\u8c03\u52a8\u6001\uff0c\u63a2\u7d22\u5bf9\u6297\u6027\u589e\u5f3a\u548c\u5fe0\u5b9e\u6027\u611f\u77e5\u5956\u52b1\u7684\u6548\u679c\uff0c\u4f7f\u7528\u71b5\u57fa\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u6821\u51c6\u3002", "result": "\u6587\u672c\u6270\u52a8\u5bfc\u81f4\u9c81\u68d2\u6027\u548c\u7f6e\u4fe1\u5ea6\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u601d\u7ef4\u94fe\u4e00\u81f4\u6027\u65f6\u3002RL\u5fae\u8c03\u5b58\u5728\u51c6\u786e\u6027-\u5fe0\u5b9e\u6027\u6743\u8861\uff1a\u63d0\u9ad8\u57fa\u51c6\u51c6\u786e\u6027\u4f46\u964d\u4f4e\u601d\u7ef4\u94fe\u53ef\u9760\u6027\u548c\u5bf9\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u5bf9\u6297\u6027\u589e\u5f3a\u63d0\u9ad8\u9c81\u68d2\u6027\u4f46\u4e0d\u80fd\u9632\u6b62\u5fe0\u5b9e\u6027\u6f02\u79fb\uff0c\u5fe0\u5b9e\u6027\u611f\u77e5\u5956\u52b1\u53ef\u4ee5\u6062\u590d\u7b54\u6848\u4e0e\u63a8\u7406\u7684\u5bf9\u9f50\uff0c\u4f46\u4e0e\u589e\u5f3a\u7ed3\u5408\u65f6\u8bad\u7ec3\u53ef\u80fd\u5d29\u6e83\u5230\u6377\u5f84\u7b56\u7565\u3002", "conclusion": "\u4ec5\u57fa\u4e8e\u51c6\u786e\u6027\u7684\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u548c\u91c7\u7528\u540c\u65f6\u5f3a\u8c03\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u5fe0\u5b9e\u6027\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2602.12971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12971", "abs": "https://arxiv.org/abs/2602.12971", "authors": ["YukTungSamuel Fang", "Zhikang Shi", "Jiabin Qiu", "Zixuan Chen", "Jieqi Shi", "Hao Xu", "Jing Huo", "Yang Gao"], "title": "INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval", "comment": null, "summary": "Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/", "AI": {"tldr": "INHerit-SG \u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u7684\u8bed\u4e49\u573a\u666f\u56fe\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u548c\u663e\u5f0f\u8bed\u4e49\u951a\u70b9\u6765\u66f4\u597d\u5730\u5bf9\u9f50\u4eba\u7c7b\u610f\u56fe\uff0c\u91c7\u7528\u5f02\u6b65\u53cc\u8fdb\u7a0b\u67b6\u6784\u548c\u4e8b\u4ef6\u89e6\u53d1\u66f4\u65b0\u673a\u5236\uff0c\u5728\u590d\u6742\u67e5\u8be2\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u573a\u666f\u56fe\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u7ebf\u6279\u5904\u7406\u6216\u9690\u5f0f\u7279\u5f81\u5d4c\u5165\uff0c\u96be\u4ee5\u652f\u6301\u590d\u6742\u73af\u5883\u4e2d\u53ef\u89e3\u91ca\u7684\u4eba\u7c7b\u610f\u56fe\u63a8\u7406\uff0c\u4e0e\u5177\u8eab\u4efb\u52a1\u9700\u6c42\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002", "method": "1) \u5c06\u5730\u56fe\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ed3\u6784\u5316\u3001RAG\u5c31\u7eea\u7684\u77e5\u8bc6\u5e93\uff0c\u5f15\u5165\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u663e\u5f0f\u8bed\u4e49\u951a\u70b9\uff1b2) \u91c7\u7528\u5f02\u6b65\u53cc\u8fdb\u7a0b\u67b6\u6784\u548cFloor-Room-Area-Object\u5c42\u6b21\u7ed3\u6784\uff0c\u89e3\u8026\u51e0\u4f55\u5206\u5272\u4e0e\u8bed\u4e49\u63a8\u7406\uff1b3) \u4e8b\u4ef6\u89e6\u53d1\u7684\u5730\u56fe\u66f4\u65b0\u673a\u5236\u4ec5\u5728\u53d1\u751f\u6709\u610f\u4e49\u8bed\u4e49\u4e8b\u4ef6\u65f6\u91cd\u7ec4\u56fe\uff1b4) \u4f7f\u7528\u591a\u89d2\u8272LLM\u5206\u89e3\u67e5\u8be2\u5e76\u5904\u7406\u903b\u8f91\u5426\u5b9a\uff0c\u91c7\u7528\u786c\u5230\u8f6f\u8fc7\u6ee4\u7b56\u7565\u786e\u4fdd\u9c81\u68d2\u63a8\u7406\u3002", "result": "\u5728HM3DSem-SQR\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5728\u590d\u6742\u67e5\u8be2\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u4e0b\u6e38\u5bfc\u822a\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "INHerit-SG\u901a\u8fc7\u663e\u5f0f\u53ef\u89e3\u91ca\u6027\u63d0\u9ad8\u4e86\u590d\u6742\u68c0\u7d22\u7684\u6210\u529f\u7387\u548c\u53ef\u9760\u6027\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u4eba\u7c7b\u4ea4\u4e92\u4efb\u52a1\uff0c\u540c\u65f6\u4ee5\u76f8\u5bf9\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u4fdd\u6301\u957f\u671f\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.12696", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12696", "abs": "https://arxiv.org/abs/2602.12696", "authors": ["Umar Marikkar", "Syed Sameed Husain", "Muhammad Awais", "Sara Atito"], "title": "Channel-Aware Probing for Multi-Channel Imaging", "comment": null, "summary": "Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.", "AI": {"tldr": "\u63d0\u51faChannel-Aware Probing (CAP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u7279\u5f81\u7f16\u7801\u548c\u5206\u79bb\u6c60\u5316\u6765\u5229\u7528\u591a\u901a\u9053\u6210\u50cf\u6570\u636e\u7684\u901a\u9053\u95f4\u591a\u6837\u6027\uff0c\u63d0\u5347\u51bb\u7ed3\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u591a\u901a\u9053\u6210\u50cf\u6570\u636e\u4e2d\u901a\u9053\u914d\u7f6e\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u53d8\u5316\uff0c\u963b\u788d\u4e86\u56fa\u5b9a\u901a\u9053\u8bad\u7ec3\u548c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u91cd\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5168\u5fae\u8c03\uff0c\u800c\u51bb\u7ed3\u7f16\u7801\u5668\u7684\u63a2\u6d4b\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u7b56\u7565\u5728\u591a\u901a\u9053\u6210\u50cf\u9886\u57df\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faChannel-Aware Probing (CAP)\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Independent Feature Encoding (IFE) - \u72ec\u7acb\u7f16\u7801\u6bcf\u4e2a\u901a\u9053\uff1b2) Decoupled Pooling (DCP) - \u5148\u5728\u901a\u9053\u5185\u6c60\u5316\uff0c\u518d\u8de8\u901a\u9053\u805a\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u901a\u9053\u6210\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAP\u4e00\u81f4\u63d0\u5347\u63a2\u6d4b\u6027\u80fd\uff0c\u5339\u914d\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u5e76\u5927\u5e45\u7f29\u5c0f\u4e0e\u5168\u5fae\u8c03\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "CAP\u901a\u8fc7\u5229\u7528\u591a\u901a\u9053\u6210\u50cf\u6570\u636e\u7684\u5185\u5728\u901a\u9053\u95f4\u591a\u6837\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u51bb\u7ed3\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u591a\u901a\u9053\u6210\u50cf\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a2\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2602.12526", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12526", "abs": "https://arxiv.org/abs/2602.12526", "authors": ["Qinhang Wu", "Sen Lin", "Ming Zhang", "Yingbin Liang", "Ness B. Shroff"], "title": "Constraint-Rectified Training for Efficient Chain-of-Thought", "comment": null, "summary": "Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), especially when combined with reinforcement learning (RL) based post-training methods. While longer reasoning traces can improve answer quality and unlock abilities such as self-correction, they also incur high inference costs and often introduce redundant steps, known as overthinking. Recent research seeks to develop efficient reasoning strategies that balance reasoning length and accuracy, either through length-aware reward design or prompt-based calibration. However, these heuristic-based approaches may suffer from severe accuracy drop and be very sensitive to hyperparameters. To address these problems, we introduce CRT (Constraint-Rectified Training), a principled post-training framework based on reference-guarded constrained optimization, yielding a more stable and interpretable formulation for efficient reasoning. CRT alternates between minimizing reasoning length and rectifying accuracy only when performance falls below the reference, enabling stable and effective pruning of redundant reasoning. We further extend CRT with a two-stage training scheme that first discovers the shortest reliable reasoning patterns and then refines accuracy under a learnt length budget, preventing the re-emergence of verbose CoT. Our comprehensive evaluation shows that this framework consistently reduces token usage while maintaining answer quality at a robust and reliable level. Further analysis reveals that CRT improves reasoning efficiency not only by shortening responses but also by reducing internal language redundancy, leading to a new evaluation metric. Moreover, CRT-based training naturally yields a sequence of intermediate checkpoints that span a spectrum of explanation lengths while preserving correctness, enabling fine-grained control over reasoning verbosity without retraining.", "AI": {"tldr": "CRT\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u6700\u5c0f\u5316\u63a8\u7406\u957f\u5ea6\u548c\u4ec5\u5728\u6027\u80fd\u4f4e\u4e8e\u53c2\u8003\u65f6\u4fee\u6b63\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6709\u6548\u7684\u5197\u4f59\u63a8\u7406\u526a\u679d\uff0c\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u867d\u7136\u957f\u63a8\u7406\u94fe\u80fd\u63d0\u5347LLM\u7684\u7b54\u6848\u8d28\u91cf\u548c\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u9ad8\u63a8\u7406\u6210\u672c\u548c\u5197\u4f59\u6b65\u9aa4\uff08\u8fc7\u5ea6\u601d\u8003\uff09\u3002\u73b0\u6709\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u7387\u4e0b\u964d\u4e25\u91cd\u548c\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6848\u3002", "method": "\u63d0\u51faCRT\uff08\u7ea6\u675f\u4fee\u6b63\u8bad\u7ec3\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u53c2\u8003\u4fdd\u62a4\u7684\u7ea6\u675f\u4f18\u5316\uff0c\u4ea4\u66ff\u6700\u5c0f\u5316\u63a8\u7406\u957f\u5ea6\u548c\u4ec5\u5728\u6027\u80fd\u4f4e\u4e8e\u53c2\u8003\u65f6\u4fee\u6b63\u51c6\u786e\u7387\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff1a\u9996\u5148\u53d1\u73b0\u6700\u77ed\u53ef\u9760\u63a8\u7406\u6a21\u5f0f\uff0c\u7136\u540e\u5728\u5b66\u4e60\u5230\u7684\u957f\u5ea6\u9884\u7b97\u4e0b\u4f18\u5316\u51c6\u786e\u7387\u3002", "result": "CRT\u80fd\u6301\u7eed\u51cf\u5c11token\u4f7f\u7528\uff0c\u540c\u65f6\u5c06\u7b54\u6848\u8d28\u91cf\u7ef4\u6301\u5728\u7a33\u5065\u53ef\u9760\u7684\u6c34\u5e73\u3002\u4e0d\u4ec5\u7f29\u77ed\u54cd\u5e94\u957f\u5ea6\uff0c\u8fd8\u51cf\u5c11\u5185\u90e8\u8bed\u8a00\u5197\u4f59\uff0c\u4ea7\u751f\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002\u8bad\u7ec3\u8fc7\u7a0b\u81ea\u7136\u4ea7\u751f\u4e00\u7cfb\u5217\u4e2d\u95f4\u68c0\u67e5\u70b9\uff0c\u53ef\u5728\u4e0d\u540c\u89e3\u91ca\u957f\u5ea6\u4e0b\u4fdd\u6301\u6b63\u786e\u6027\u3002", "conclusion": "CRT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u51cf\u5c11\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\uff0c\u5e76\u80fd\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u5197\u957f\u5ea6\u63a7\u5236\u3002"}}
{"id": "2602.13081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13081", "abs": "https://arxiv.org/abs/2602.13081", "authors": ["Oscar Lima", "Marc Vinci", "Martin G\u00fcnther", "Marian Renz", "Alexander Sung", "Sebastian Stock", "Johannes Brust", "Lennart Niecksch", "Zongyao Yi", "Felix Igelbrink", "Benjamin Kisliuk", "Martin Atzmueller", "Joachim Hertzberg"], "title": "Agentic AI for Robot Control: Flexible but still Fragile", "comment": null, "summary": "Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u80fd\u529b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u89c4\u5212-\u6267\u884c\u5faa\u73af\u9009\u62e9\u548c\u8c03\u7528\u673a\u5668\u4eba\u6280\u80fd\uff0c\u5728\u79fb\u52a8\u64cd\u4f5c\u548c\u519c\u4e1a\u5bfc\u822a\u4e24\u4e2a\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u3002", "motivation": "\u5229\u7528\u751f\u6210\u5f0f\u6a21\u578b\u7684\u80fd\u529b\u548c\u5e38\u8bc6\u5148\u9a8c\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u6a21\u7cca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u8fed\u4ee3\u89c4\u5212-\u6267\u884c\u5faa\u73af\uff0c\u652f\u6301\u7ed3\u6784\u5316\u5185\u7701\u3001\u663e\u5f0f\u4e8b\u4ef6\u68c0\u67e5\u548c\u64cd\u4f5c\u5458\u5e72\u9884\uff0c\u901a\u8fc7\u66f4\u65b0\u7cfb\u7edf\u63d0\u793a\u548c\u5de5\u5177\u63a5\u53e3\u7ed1\u5b9a\u5b9e\u73b0\u8de8\u5e73\u53f0\u8fc1\u79fb\u3002", "result": "\u5728\u4e24\u4e2a\u7269\u7406\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\uff1a\u5ba4\u5185\u79fb\u52a8\u64cd\u4f5c\uff08Mobipick\uff09\u548c\u81ea\u4e3b\u519c\u4e1a\u5bfc\u822a\uff08Valdemar\uff09\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u4f46\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u5305\u62ec\u975e\u786e\u5b9a\u6027\u6b21\u4f18\u884c\u4e3a\u3001\u6307\u4ee4\u8ddf\u968f\u9519\u8bef\u548c\u5bf9\u63d0\u793a\u89c4\u8303\u7684\u9ad8\u654f\u611f\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u5177\u6709\u67b6\u6784\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5e73\u53f0\u548c\u4efb\u52a1\u9886\u57df\uff0c\u4f46\u5f53\u524d\u5b58\u5728\u8106\u5f31\u6027\u95ee\u9898\uff0c\u9700\u8981\u5728\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2602.12542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12542", "abs": "https://arxiv.org/abs/2602.12542", "authors": ["Pengfei Hu", "Chang Lu", "Feifan Liu", "Yue Ning"], "title": "Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference", "comment": null, "summary": "Deep learning models for clinical event prediction on electronic health records (EHR) often suffer performance degradation when deployed under different data distributions. While domain adaptation (DA) methods can mitigate such shifts, its \"black-box\" nature prevents widespread adoption in clinical practice where transparency is essential for trust and safety. We propose ExtraCare to decompose patient representations into invariant and covariant components. By supervising these two components and enforcing their orthogonality during training, our model preserves label information while exposing domain-specific variation at the same time for more accurate predictions than most feature alignment models. More importantly, it offers human-understandable explanations by mapping sparse latent dimensions to medical concepts and quantifying their contributions via targeted ablations. ExtraCare is evaluated on two real-world EHR datasets across multiple domain partition settings, demonstrating superior performance along with enhanced transparency, as evidenced by its accurate predictions and explanations from extensive case studies.", "AI": {"tldr": "ExtraCare\u662f\u4e00\u4e2a\u7528\u4e8e\u4e34\u5e8a\u4e8b\u4ef6\u9884\u6d4b\u7684\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u60a3\u8005\u8868\u5f81\u5206\u89e3\u4e3a\u4e0d\u53d8\u548c\u534f\u53d8\u7ec4\u4ef6\uff0c\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u90e8\u7f72\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u73b0\u6709\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u7f13\u89e3\u8fd9\u79cd\u504f\u79fb\uff0c\u4f46\u5176\"\u9ed1\u76d2\"\u7279\u6027\u963b\u788d\u4e86\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u9700\u8981\u900f\u660e\u6027\u6765\u5efa\u7acb\u4fe1\u4efb\u548c\u786e\u4fdd\u5b89\u5168\u3002", "method": "ExtraCare\u5c06\u60a3\u8005\u8868\u5f81\u5206\u89e3\u4e3a\u4e0d\u53d8\u7ec4\u4ef6\u548c\u534f\u53d8\u7ec4\u4ef6\u3002\u901a\u8fc7\u76d1\u7763\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u5b83\u4eec\u6b63\u4ea4\uff0c\u6a21\u578b\u5728\u4fdd\u7559\u6807\u7b7e\u4fe1\u606f\u7684\u540c\u65f6\u66b4\u9732\u9886\u57df\u7279\u5b9a\u7684\u53d8\u5316\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5c06\u7a00\u758f\u6f5c\u5728\u7ef4\u5ea6\u6620\u5c04\u5230\u533b\u5b66\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u6d88\u878d\u91cf\u5316\u5b83\u4eec\u7684\u8d21\u732e\uff0c\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684EHR\u6570\u636e\u96c6\u4e0a\u7684\u591a\u4e2a\u9886\u57df\u5212\u5206\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\uff0cExtraCare\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u589e\u5f3a\u7684\u900f\u660e\u6027\u3002\u5e7f\u6cdb\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u51c6\u786e\u7684\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ExtraCare\u901a\u8fc7\u5206\u89e3\u60a3\u8005\u8868\u5f81\u4e3a\u4e0d\u53d8\u548c\u534f\u53d8\u7ec4\u4ef6\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u4e34\u5e8a\u4e8b\u4ef6\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u56e0\"\u9ed1\u76d2\"\u7279\u6027\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\u7684\u95ee\u9898\u3002"}}
{"id": "2602.12567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12567", "abs": "https://arxiv.org/abs/2602.12567", "authors": ["Mohammad Partohaghighi", "Roummel Marcia", "Bruce J. West", "YangQuan Chen"], "title": "Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling", "comment": "This manuscript is under review in IEEE Transactions on Transportation Electrification", "summary": "Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.", "AI": {"tldr": "\u63d0\u51faFO-RI-FedAvg\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u7cd9\u5ea6\u611f\u77e5\u6b63\u5219\u5316\u548c\u5206\u6570\u9636\u4f18\u5316\u6539\u8fdb\u8054\u90a6\u5b66\u4e60\u5728\u7535\u52a8\u6c7d\u8f66\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u6027", "motivation": "\u7535\u52a8\u6c7d\u8f66\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u8fde\u63a5\u4e0d\u7a33\u5b9a\u3001\u5ba2\u6237\u7aef\u53c2\u4e0e\u53d8\u5316\u5927\u3001\u6570\u636e\u5206\u5e03\u5dee\u5f02\u663e\u8457\u7b49\u6311\u6218\uff0c\u4f20\u7edfFedAvg\u65b9\u6cd5\u5728\u8fd9\u4e9b\u73b0\u5b9e\u7ea6\u675f\u4e0b\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u6f02\u79fb\u548c\u6536\u655b\u9000\u5316", "method": "FO-RI-FedAvg\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5ba2\u6237\u7aef\u673a\u5236\uff1a1) \u81ea\u9002\u5e94\u7c97\u7cd9\u5ea6\u611f\u77e5\u8fd1\u7aef\u6b63\u5219\u5316\uff0c\u6839\u636e\u5c40\u90e8\u635f\u5931\u51fd\u6570\u7c97\u7cd9\u5ea6\u52a8\u6001\u8c03\u6574\u5411\u5168\u5c40\u6a21\u578b\u7684\u62c9\u529b\uff1b2) \u5206\u6570\u9636\u5c40\u90e8\u4f18\u5316\uff0c\u5f15\u5165\u77ed\u671f\u8bb0\u5fc6\u5e73\u6ed1\u51b2\u7a81\u7684\u66f4\u65b0\u65b9\u5411\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u6807\u51c6FedAvg\u670d\u52a1\u5668\u805a\u5408\uff0c\u4ec5\u589e\u52a0\u53ef\u5206\u644a\u7684\u5143\u7d20\u7ea7\u64cd\u4f5c", "result": "\u5728VED\u548ceVED\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7535\u52a8\u6c7d\u8f66\u80fd\u8017\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFO-RI-FedAvg\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u7a33\u5b9a\u7684\u6536\u655b\uff0c\u7279\u522b\u662f\u5728\u5ba2\u6237\u7aef\u53c2\u4e0e\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b", "conclusion": "FO-RI-FedAvg\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u7684FedAvg\u6269\u5c55\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u4fa7\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u7535\u52a8\u6c7d\u8f66\u8054\u90a6\u5b66\u4e60\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387"}}
{"id": "2602.12591", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12591", "abs": "https://arxiv.org/abs/2602.12591", "authors": ["Hemant Prasad", "Daisuke Ikefuji", "Shin Tominaga", "Hitoshi Sakurai", "Manabu Otani"], "title": "Vehicle behaviour estimation for abnormal event detection using distributed fiber optic sensing", "comment": null, "summary": "The distributed fiber-optic sensing (DFOS) system is a cost-effective wide-area traffic monitoring technology that utilizes existing fiber infrastructure to effectively detect traffic congestions. However, detecting single-lane abnormalities, that lead to congestions, is still a challenge. These single-lane abnormalities can be detected by monitoring lane change behaviour of vehicles, performed to avoid congestion along the monitoring section of a road. This paper presents a method to detect single-lane abnormalities by tracking individual vehicle paths and detecting vehicle lane changes along a section of a road. We propose a method to estimate the vehicle position at all time instances and fit a path using clustering techniques. We detect vehicle lane change by monitoring any change in spectral centroid of vehicle vibrations by tracking a reference vehicle along a highway. The evaluation of our proposed method with real traffic data showed 80% accuracy for lane change detection events that represent presence of abnormalities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u5149\u7ea4\u4f20\u611f\u7684\u4ea4\u901a\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u8f66\u8f86\u8def\u5f84\u548c\u68c0\u6d4b\u8f66\u9053\u53d8\u6362\u6765\u8bc6\u522b\u5355\u8f66\u9053\u5f02\u5e38", "motivation": "\u5206\u5e03\u5f0f\u5149\u7ea4\u4f20\u611f\u7cfb\u7edf\u867d\u7136\u80fd\u6709\u6548\u76d1\u6d4b\u4ea4\u901a\u62e5\u5835\uff0c\u4f46\u96be\u4ee5\u68c0\u6d4b\u5bfc\u81f4\u62e5\u5835\u7684\u5355\u8f66\u9053\u5f02\u5e38\u3002\u8fd9\u4e9b\u5f02\u5e38\u53ef\u4ee5\u901a\u8fc7\u76d1\u6d4b\u8f66\u8f86\u4e3a\u907f\u514d\u62e5\u5835\u800c\u8fdb\u884c\u7684\u8f66\u9053\u53d8\u6362\u884c\u4e3a\u6765\u8bc6\u522b\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u6280\u672f\u4f30\u8ba1\u8f66\u8f86\u5728\u6240\u6709\u65f6\u95f4\u70b9\u7684\u4f4d\u7f6e\u5e76\u62df\u5408\u8def\u5f84\uff0c\u901a\u8fc7\u8ffd\u8e2a\u53c2\u8003\u8f66\u8f86\u5e76\u76d1\u6d4b\u5176\u632f\u52a8\u9891\u8c31\u8d28\u5fc3\u7684\u53d8\u5316\u6765\u68c0\u6d4b\u8f66\u9053\u53d8\u6362", "result": "\u4f7f\u7528\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u8bc4\u4f30\uff0c\u8f66\u9053\u53d8\u6362\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523080%\uff0c\u80fd\u591f\u6709\u6548\u4ee3\u8868\u5f02\u5e38\u5b58\u5728", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5355\u8f66\u9053\u5f02\u5e38\uff0c\u4e3a\u5206\u5e03\u5f0f\u5149\u7ea4\u4f20\u611f\u7cfb\u7edf\u7684\u4ea4\u901a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u80fd\u529b"}}
{"id": "2602.12606", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12606", "abs": "https://arxiv.org/abs/2602.12606", "authors": ["Justin Gu", "Rishabh Ranjan", "Charilaos Kanatsoulis", "Haiming Tang", "Martin Jurkovic", "Valter Hudovernik", "Mark Znidar", "Pranshu Chaturvedi", "Parth Shroff", "Fengyu Li", "Jure Leskovec"], "title": "RelBench v2: A Large-Scale Benchmark and Repository for Relational Data", "comment": "Website: https://relbench.stanford.edu", "summary": "Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.", "AI": {"tldr": "RelBench v2 \u662f\u4e00\u4e2a\u6269\u5c55\u7684\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65b0\u589e\u4e864\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u81ea\u52a8\u8865\u5168\u4efb\u52a1\uff0c\u6574\u5408\u4e86\u5916\u90e8\u57fa\u51c6\uff0c\u5305\u542b11\u4e2a\u6570\u636e\u96c6\u300129\u4e2a\u8868\u30012200\u4e07\u884c\u6570\u636e\uff0c\u5b9e\u9a8c\u663e\u793a\u5173\u7cfb\u6a21\u578b\u4f18\u4e8e\u5355\u8868\u57fa\u7ebf\u3002", "motivation": "\u968f\u7740\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u5411\u66f4\u5927\u6a21\u578b\u548c\u5173\u7cfb\u57fa\u7840\u6a21\u578b\u53d1\u5c55\uff0c\u9700\u8981\u53ef\u6269\u5c55\u4e14\u771f\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u63a8\u52a8\u8fdb\u5c55\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728\u89c4\u6a21\u3001\u4efb\u52a1\u591a\u6837\u6027\u548c\u5916\u90e8\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1) \u65b0\u589e4\u4e2a\u5927\u89c4\u6a21\u5173\u7cfb\u6570\u636e\u96c6\uff08\u5b66\u672f\u51fa\u7248\u7269\u3001\u4f01\u4e1a\u8d44\u6e90\u89c4\u5212\u3001\u6d88\u8d39\u8005\u5e73\u53f0\u3001\u4e34\u5e8a\u8bb0\u5f55\uff09\uff1b2) \u5f15\u5165\u81ea\u52a8\u8865\u5168\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5728\u5173\u7cfb\u8868\u4e2d\u63a8\u65ad\u7f3a\u5931\u5c5e\u6027\u503c\uff1b3) \u6574\u5408\u5916\u90e8\u57fa\u51c6\uff1a\u5c06Temporal Graph Benchmark\u8f6c\u6362\u4e3a\u5173\u7cfb\u6a21\u5f0f\uff0c\u4e0eReDeLEx\u63a5\u53e3\u8fde\u63a570+\u771f\u5b9e\u6570\u636e\u5e93\uff0c\u7eb3\u51654DBInfer\u6570\u636e\u96c6\uff1b4) \u6269\u5c55\u81f311\u4e2a\u6570\u636e\u96c6\u300129\u4e2a\u8868\u30012200\u4e07\u884c\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u8865\u5168\u3001\u9884\u6d4b\u548c\u63a8\u8350\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u5355\u8868\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u663e\u5f0f\u5efa\u6a21\u5173\u7cfb\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002\u57fa\u51c6\u6d4b\u8bd5\u89c4\u6a21\u663e\u8457\u6269\u5927\uff0c\u4efb\u52a1\u7c7b\u578b\u66f4\u52a0\u4e30\u5bcc\u3002", "conclusion": "RelBench v2 \u4e3a\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u8bc4\u4f30\u573a\u666f\uff0c\u4fc3\u8fdb\u4e86\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u7cfb\u7edf\u6027\u8fdb\u5c55\u3002"}}
{"id": "2602.12916", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12916", "abs": "https://arxiv.org/abs/2602.12916", "authors": ["Haobin Li", "Yutong Yang", "Yijie Lin", "Dai Xiang", "Mouxing Yang", "Xi Peng"], "title": "Reliable Thinking with Images", "comment": "26 pages, 19 figures", "summary": "As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.", "AI": {"tldr": "RTWI\u65b9\u6cd5\u901a\u8fc7\u7edf\u4e00\u6587\u672c\u4e2d\u5fc3\u7684\u65b9\u5f0f\u8bc4\u4f30\u89c6\u89c9\u7ebf\u7d22\u548c\u6587\u672c\u63a8\u7406\u94fe\u7684\u53ef\u9760\u6027\uff0c\u4f7f\u7528\u9c81\u68d2\u8fc7\u6ee4\u548c\u6295\u7968\u6a21\u5757\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u566a\u58f0\u601d\u7ef4\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\"Thinking with Images\"\u65b9\u6cd5\u5047\u8bbe\u56fe\u50cf-\u6587\u672c\u63a8\u7406\u94fe\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u7531\u4e8e\u591a\u6a21\u6001\u7406\u89e3\u7684\u590d\u6742\u6027\uff0c\u8fd9\u79cd\u5047\u8bbe\u5bb9\u6613\u88ab\u8fdd\u53cd\uff0c\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51faReliable Thinking with Images\u65b9\u6cd5\uff1a1\uff09\u4ee5\u7edf\u4e00\u6587\u672c\u4e2d\u5fc3\u7684\u65b9\u5f0f\u8bc4\u4f30\u89c6\u89c9\u7ebf\u7d22\u548c\u6587\u672c\u63a8\u7406\u94fe\u7684\u53ef\u9760\u6027\uff1b2\uff09\u4f7f\u7528\u9c81\u68d2\u8fc7\u6ee4\u6a21\u5757\u9632\u6b62\u566a\u58f0\u601d\u7ef4\u6c61\u67d3\u6700\u7ec8\u7b54\u6848\uff1b3\uff09\u91c7\u7528\u6295\u7968\u673a\u5236\u786e\u4fdd\u7b54\u6848\u7684\u53ef\u9760\u6027", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RTWI\u65b9\u6cd5\u5728\u89e3\u51b3\u566a\u58f0\u601d\u7ef4\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027", "conclusion": "RTWI\u901a\u8fc7\u53ef\u9760\u6027\u8bc4\u4f30\u548c\u9c81\u68d2\u8fc7\u6ee4\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u566a\u58f0\u601d\u7ef4\u95ee\u9898\uff0c\u63d0\u5347\u4e86MLLMs\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd"}}
{"id": "2602.12919", "categories": ["cs.CV", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.12919", "abs": "https://arxiv.org/abs/2602.12919", "authors": ["Xiao Wang", "Xingxing Xiong", "Jinfeng Gao", "Xufeng Lou", "Bo Jiang", "Si-bao Chen", "Yaowei Wang", "Yonghong Tian"], "title": "EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition", "comment": null, "summary": "Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID", "AI": {"tldr": "\u63d0\u51faEPRBench\u4e8b\u4ef6\u6d41\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b10K\u4e8b\u4ef6\u5e8f\u5217\u548c65K\u4e8b\u4ef6\u5e27\uff0c\u5e76\u57fa\u4e8eLLM\u751f\u6210\u573a\u666f\u63cf\u8ff0\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u53ef\u89c1\u5149\u76f8\u673a\u5728\u4f4e\u5149\u7167\u3001\u8fc7\u66dd\u3001\u9ad8\u901f\u8fd0\u52a8\u7b49\u6311\u6218\u6761\u4ef6\u4e0b\u4e0d\u7a33\u5b9a\uff0c\u800c\u4e8b\u4ef6\u6d41\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u76ee\u524d\u8be5\u9886\u57df\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u57fa\u51c6\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "1) \u6784\u5efaEPRBench\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u624b\u6301\u548c\u8f66\u8f7d\u8bbe\u5907\u91c7\u96c610K\u4e8b\u4ef6\u5e8f\u5217\u548c65K\u4e8b\u4ef6\u5e27\uff0c\u6db5\u76d6\u591a\u6837\u5316\u89c6\u89d2\u3001\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\uff1b2) \u901a\u8fc7LLM\u751f\u6210\u573a\u666f\u63cf\u8ff0\u5e76\u4eba\u5de5\u6807\u6ce8\uff1b3) \u63d0\u51fa\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff1a\u5229\u7528LLM\u4ece\u539f\u59cb\u4e8b\u4ef6\u6d41\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u6307\u5bfc\u7a7a\u95f4\u6ce8\u610f\u529btoken\u9009\u62e9\u3001\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u3002", "result": "1) \u53d1\u5e03\u9ad8\u8d28\u91cfEPRBench\u6570\u636e\u96c6\uff1b2) \u5728\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e8615\u79cd\u6700\u5148\u8fdb\u7684VPR\u7b97\u6cd5\uff0c\u4e3a\u672a\u6765\u6bd4\u8f83\u63d0\u4f9b\u57fa\u7ebf\uff1b3) \u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u4e0d\u4ec5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5730\u70b9\u8bc6\u522b\uff0c\u8fd8\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "EPRBench\u586b\u8865\u4e86\u4e8b\u4ef6\u6d41VPR\u9886\u57df\u7684\u6570\u636e\u96c6\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u5730\u70b9\u8bc6\u522b\uff0c\u4e3a\u672a\u6765\u4e8b\u4ef6\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.13191", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13191", "abs": "https://arxiv.org/abs/2602.13191", "authors": ["Sayan Deb Sarkar", "R\u00e9mi Pautrat", "Ondrej Miksik", "Marc Pollefeys", "Iro Armeni", "Mahdi Rad", "Mihai Dusmanu"], "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models", "comment": "Project Page: https://sayands.github.io/cope/", "summary": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\\%$ and token usage by up to $93\\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u89c6\u9891\u7f16\u89e3\u7801\u539f\u8bed\uff08\u8fd0\u52a8\u5411\u91cf\u548c\u6b8b\u5dee\uff09\u7684VideoLM\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd", "motivation": "\u73b0\u6709VideoLM\u65b9\u6cd5\u4f7f\u7528\u5173\u952e\u5e27\u91c7\u6837\u4f1a\u9057\u6f0f\u5b8f\u89c2\u4e8b\u4ef6\u548c\u5fae\u89c2\u7ec6\u8282\uff0c\u4e14\u5904\u7406\u5b8c\u6574\u56fe\u50cf\u5e27\u5e26\u6765\u5de8\u5927\u8ba1\u7b97\u5f00\u9500", "method": "\u5229\u7528\u89c6\u9891\u7f16\u89e3\u7801\u539f\u8bed\uff08\u8fd0\u52a8\u5411\u91cf\u548c\u6b8b\u5dee\uff09\u7f16\u7801\u89c6\u9891\u5197\u4f59\u548c\u7a00\u758f\u6027\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7Transformer\u7f16\u7801\u5668\u805a\u5408\u8fd9\u4e9b\u539f\u8bed\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u7b56\u7565\u5bf9\u9f50\u56fe\u50cf\u7f16\u7801\u5668\u8868\u793a", "result": "\u76f8\u6bd4\u6807\u51c6VideoLM\uff0c\u9996token\u751f\u6210\u65f6\u95f4\u51cf\u5c1186%\uff0ctoken\u4f7f\u7528\u91cf\u51cf\u5c1193%\uff0c\u572814\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u4fdd\u6301\u6216\u8d85\u8d8a\u6027\u80fd", "conclusion": "\u5229\u7528\u89c6\u9891\u7f16\u89e3\u7801\u539f\u8bed\u662f\u9ad8\u6548\u89c6\u9891\u8bed\u8a00\u5efa\u6a21\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u7ef4\u6301\u6216\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd"}}
{"id": "2602.12798", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.12798", "abs": "https://arxiv.org/abs/2602.12798", "authors": ["Andreas Boltres", "Niklas Freymuth", "Gerhard Neumann"], "title": "Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?", "comment": null, "summary": "Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \\emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.", "AI": {"tldr": "Placer\u4f7f\u7528\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u5c06\u7f51\u7edc\u72b6\u6001\u8f6c\u6362\u4e3a\u8282\u70b9\u5d4c\u5165\uff0c\u5b9e\u73b0\u5feb\u901f\u8d2a\u5a6a\u4e0b\u4e00\u8df3\u8def\u7531\uff0c\u540c\u65f6\u63d0\u4f9b\u8def\u7531\u51b3\u7b56\u7684\u53ef\u89c6\u5316\u89e3\u91ca", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9065\u6d4b\u611f\u77e5\u8def\u7531\u65b9\u6cd5\u727a\u7272\u4e86\u8def\u7531\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u4e3a\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u662f\u9ed1\u76d2\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u673a\u5668\u5b66\u4e60\u5904\u7406\u590d\u6742\u7f51\u7edc\u72b6\u6001\u4f9d\u8d56\uff0c\u53c8\u80fd\u4fdd\u6301\u8def\u7531\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u5c06\u7f51\u7edc\u72b6\u6001\u8f6c\u6362\u4e3a\u6f5c\u5728\u8282\u70b9\u5d4c\u5165\uff0c\u8fd9\u4e9b\u5d4c\u5165\u652f\u6301\u5feb\u901f\u8d2a\u5a6a\u4e0b\u4e00\u8df3\u8def\u7531\uff0c\u65e0\u9700\u76f4\u63a5\u89e3\u51b3\u5168\u5bf9\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u5e76\u80fd\u53ef\u89c6\u5316\u7f51\u7edc\u4e8b\u4ef6\u5982\u4f55\u5f71\u54cd\u8def\u7531\u51b3\u7b56\u3002", "result": "Placer\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u5904\u7406\u7f51\u7edc\u72b6\u6001\u4e0e\u8def\u7531\u4e4b\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u63d0\u4f9b\u8def\u7531\u51b3\u7b56\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u589e\u5f3a\u4e86\u9065\u6d4b\u611f\u77e5\u8def\u7531\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Placer\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u548c\u8282\u70b9\u5d4c\u5165\u6280\u672f\uff0c\u5728\u4fdd\u6301\u673a\u5668\u5b66\u4e60\u4f18\u52bf\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u8def\u7531\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u7f51\u7edc\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89c6\u5316\u548c\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.12828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12828", "abs": "https://arxiv.org/abs/2602.12828", "authors": ["Zhan Qu", "Michael F\u00e4rber"], "title": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories", "comment": null, "summary": "Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.", "AI": {"tldr": "GRAIL\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u51e0\u4f55\u8868\u793a\u548c\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u9884\u6d4b\u4e34\u5e8a\u4e8b\u4ef6\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u7f16\u7801\u5c42\u6b21\u548c\u6570\u636e\u9a71\u52a8\u5173\u8054\uff0c\u5728\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u4e34\u5e8a\u56fe\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u63a8\u7406\u65f6\u91cd\u6392\u5668\u63d0\u5347\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u4ece\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9884\u6d4b\u672a\u6765\u4e34\u5e8a\u4e8b\u4ef6\u9762\u4e34\u6311\u6218\uff1a\u7a00\u758f\u7684\u591a\u7c7b\u578b\u4e34\u5e8a\u4e8b\u4ef6\u3001\u5c42\u6b21\u5316\u533b\u5b66\u8bcd\u6c47\u8868\u3001\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u7ed3\u6784\u5316\u5386\u53f2\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u6765\u51c6\u786e\u9884\u6d4b\u60a3\u8005\u4e0b\u4e00\u6b21\u5c31\u8bca\u7684\u4e34\u5e8a\u4e8b\u4ef6\u3002", "method": "GRAIL\u6846\u67b6\uff1a1) \u6784\u5efa\u7edf\u4e00\u4e34\u5e8a\u56fe\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u7f16\u7801\u7cfb\u7edf\u5c42\u6b21\u548c\u6570\u636e\u9a71\u52a8\u7684\u8de8\u4e8b\u4ef6\u7c7b\u578b\u65f6\u95f4\u5173\u8054\uff1b2) \u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5d4c\u5165\u8be5\u56fe\uff1b3) \u5c06\u6bcf\u6b21\u5c31\u8bca\u603b\u7ed3\u4e3a\u6982\u7387\u6027\u4e2d\u5fc3\u4e8b\u4ef6\u4ee5\u53bb\u566a\u7a00\u758f\u89c2\u6d4b\uff1b4) \u63a8\u7406\u65f6\u68c0\u7d22\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u5408\u7406\u672a\u6765\u4e8b\u4ef6\u96c6\u5408\uff0c\u4e0e\u5c42\u6b21\u548c\u65f6\u95f4\u8fdb\u5c55\u5bf9\u9f50\uff1b5) \u53ef\u9009\u4f7f\u7528LLM\u4f5c\u4e3a\u7ea6\u675f\u63a8\u7406\u65f6\u91cd\u6392\u5668\u4f18\u5316\u6392\u540d\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRAIL\u6301\u7eed\u6539\u8fdb\u591a\u7c7b\u578b\u4e0b\u6b21\u5c31\u8bca\u9884\u6d4b\uff0c\u5e76\u4ea7\u751f\u66f4\u7b26\u5408\u5c42\u6b21\u4e00\u81f4\u6027\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "GRAIL\u901a\u8fc7\u7ed3\u6784\u5316\u51e0\u4f55\u8868\u793a\u548c\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u7a00\u758f\u7eb5\u5411EHR\u9884\u6d4b\u4e34\u5e8a\u4e8b\u4ef6\u7684\u6311\u6218\uff0c\u7ed3\u5408\u56fe\u8868\u793a\u3001\u53cc\u66f2\u5d4c\u5165\u548cLLM\u91cd\u6392\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u5c42\u6b21\u4e00\u81f4\u7684\u9884\u6d4b\u3002"}}
{"id": "2602.12833", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12833", "abs": "https://arxiv.org/abs/2602.12833", "authors": ["Zhan Qu", "Michael F\u00e4rber"], "title": "TRACE: Temporal Reasoning via Agentic Context Evolution for Streaming Electronic Health Records (EHRs)", "comment": null, "summary": "Large Language Models (LLMs) encode extensive medical knowledge but struggle to apply it reliably to longitudinal patient trajectories, where evolving clinical states, irregular timing, and heterogeneous events degrade performance over time. Existing adaptation strategies rely on fine-tuning or retrieval-based augmentation, which introduce computational overhead, privacy constraints, or instability under long contexts. We introduce TRACE (Temporal Reasoning via Agentic Context Evolution), a framework that enables temporal clinical reasoning with frozen LLMs by explicitly structuring and maintaining context rather than extending context windows or updating parameters. TRACE operates over a dual-memory architecture consisting of a static Global Protocol encoding institutional clinical rules and a dynamic Individual Protocol tracking patient-specific state. Four agentic components, Router, Reasoner, Auditor, and Steward, coordinate over this structured memory to support temporal inference and state evolution. The framework maintains bounded inference cost via structured state compression and selectively audits safety-critical clinical decisions. Evaluated on longitudinal clinical event streams from MIMIC-IV, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable and auditable reasoning traces.", "AI": {"tldr": "TRACE\u6846\u67b6\u901a\u8fc7\u53cc\u8bb0\u5fc6\u67b6\u6784\u548c\u56db\u4e2a\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u4f7f\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u7eb5\u5411\u60a3\u8005\u8f68\u8ff9\u4e2d\u8fdb\u884c\u65f6\u95f4\u4e34\u5e8a\u63a8\u7406\uff0c\u65e0\u9700\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u66f4\u65b0\u53c2\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u533b\u5b66\u77e5\u8bc6\uff0c\u4f46\u5728\u5904\u7406\u7eb5\u5411\u60a3\u8005\u8f68\u8ff9\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4e34\u5e8a\u72b6\u6001\u6f14\u53d8\u3001\u4e0d\u89c4\u5219\u65f6\u95f4\u5b89\u6392\u548c\u5f02\u8d28\u4e8b\u4ef6\u4f1a\u968f\u65f6\u95f4\u964d\u4f4e\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5fae\u8c03\u6216\u68c0\u7d22\u589e\u5f3a\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u9690\u79c1\u9650\u5236\u6216\u957f\u4e0a\u4e0b\u6587\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "method": "TRACE\u6846\u67b6\u91c7\u7528\u53cc\u8bb0\u5fc6\u67b6\u6784\uff1a\u9759\u6001\u7684\u5168\u5c40\u534f\u8bae\u7f16\u7801\u673a\u6784\u4e34\u5e8a\u89c4\u5219\uff0c\u52a8\u6001\u7684\u4e2a\u4f53\u534f\u8bae\u8ddf\u8e2a\u60a3\u8005\u7279\u5b9a\u72b6\u6001\u3002\u56db\u4e2a\u667a\u80fd\u4f53\u7ec4\u4ef6\uff08\u8def\u7531\u5668\u3001\u63a8\u7406\u5668\u3001\u5ba1\u8ba1\u5458\u3001\u7ba1\u7406\u5458\uff09\u5728\u6b64\u7ed3\u6784\u5316\u8bb0\u5fc6\u4e0a\u534f\u8c03\u5de5\u4f5c\uff0c\u652f\u6301\u65f6\u95f4\u63a8\u7406\u548c\u72b6\u6001\u6f14\u53d8\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u72b6\u6001\u538b\u7f29\u4fdd\u6301\u6709\u9650\u63a8\u7406\u6210\u672c\uff0c\u5e76\u9009\u62e9\u6027\u5ba1\u8ba1\u5b89\u5168\u5173\u952e\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "result": "\u5728MIMIC-IV\u7684\u7eb5\u5411\u4e34\u5e8a\u4e8b\u4ef6\u6d41\u4e0a\u8bc4\u4f30\uff0cTRACE\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u3001\u534f\u8bae\u9075\u4ece\u6027\u548c\u4e34\u5e8a\u5b89\u5168\u6027\uff0c\u4f18\u4e8e\u957f\u4e0a\u4e0b\u6587\u548c\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u89e3\u91ca\u548c\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "TRACE\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u548c\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u4f7f\u51bb\u7ed3\u7684LLM\u80fd\u591f\u6709\u6548\u5904\u7406\u7eb5\u5411\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u6709\u9650\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.12869", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12869", "abs": "https://arxiv.org/abs/2602.12869", "authors": ["Zhan Qu", "Michael F\u00e4rber"], "title": "X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting", "comment": null, "summary": "Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.", "AI": {"tldr": "X-VORTEX\u662f\u4e00\u4e2a\u57fa\u4e8e\u589e\u5f3a\u91cd\u53e0\u7406\u8bba\u7684\u65f6\u7a7a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u65e0\u6807\u7b7e\u7684LiDAR\u70b9\u4e91\u5e8f\u5217\u4e2d\u5b66\u4e60\u7269\u7406\u611f\u77e5\u7684\u5c3e\u6da1\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u7a00\u758f\u6027\u548c\u65f6\u53d8\u6da1\u6d41\u52a8\u529b\u5b66\u4e24\u5927\u6838\u5fc3\u6311\u6218\u3002", "motivation": "\u98de\u673a\u5c3e\u6da1\u662f\u5f3a\u70c8\u7684\u7a7a\u6c14\u6e4d\u6d41\uff0c\u5bf9\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u6784\u6210\u5b89\u5168\u548c\u5bb9\u91cf\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u6bcf\u6b21\u626b\u63cf\u89c6\u4e3a\u72ec\u7acb\u7684\u76d1\u7763\u5206\u5272\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u7ed3\u6784\uff0c\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u5b9e\u8df5\u4e2d\u6536\u96c6\u7684\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u3002", "method": "\u63d0\u51faX-VORTEX\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5f31\u6270\u52a8\u5e8f\u5217\u548c\u5f3a\u589e\u5f3a\u5bf9\u5e94\u5e8f\u5217\u6784\u5efa\u914d\u5bf9\u8f93\u5165\uff08\u4f7f\u7528\u65f6\u95f4\u5b50\u91c7\u6837\u548c\u7a7a\u95f4\u63a9\u7801\uff09\uff0c\u91c7\u7528\u65f6\u95f4\u5206\u5e03\u51e0\u4f55\u7f16\u7801\u5668\u63d0\u53d6\u6bcf\u5e27\u7279\u5f81\uff0c\u5e8f\u5217\u805a\u5408\u5668\u5efa\u6a21\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4e2d\u7684\u6da1\u6d41\u72b6\u6001\u6f14\u5316\u3002", "result": "\u5728\u8d85\u8fc7100\u4e07\u6b21LiDAR\u626b\u63cf\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cX-VORTEX\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6da1\u6d41\u4e2d\u5fc3\u5b9a\u4f4d\uff0c\u4ec5\u4f7f\u7528\u76d1\u7763\u57fa\u7ebf\u6240\u9700\u6807\u8bb0\u6570\u636e\u76841%\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u8868\u793a\u652f\u6301\u51c6\u786e\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "conclusion": "X-VORTEX\u901a\u8fc7\u65f6\u7a7a\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u5c3e\u6da1\u8ddf\u8e2a\u4e2d\u7684\u4f20\u611f\u5668\u7a00\u758f\u6027\u548c\u65f6\u53d8\u52a8\u6001\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u8bb0\u6570\u636e\u9700\u6c42\uff0c\u4e3a\u5b9e\u9645\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.12976", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12976", "abs": "https://arxiv.org/abs/2602.12976", "authors": ["Jin Li", "Kleanthis Malialis", "Christos G. Panayiotou", "Marios M. Polycarpou"], "title": "Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling", "comment": "accepted", "summary": "In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.", "AI": {"tldr": "VAE++ESDD\uff1a\u4e00\u79cd\u7ed3\u5408\u589e\u91cf\u5b66\u4e60\u548c\u53cc\u5c42\u96c6\u6210\uff08VAE\u96c6\u6210\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff0c\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u5668\u96c6\u6210\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u65e0\u6807\u7b7e\u6d41\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u6570\u5b57\u4e16\u754c\u4e2d\u5927\u91cf\u6d41\u6570\u636e\u65e0\u6807\u7b7e\uff0c\u96be\u4ee5\u68c0\u6d4b\u5f02\u5e38\uff0c\u7279\u522b\u662f\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u5b58\u5728\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u968f\u65f6\u95f4\u4e0b\u964d\u3002", "method": "\u63d0\u51faVAE++ESDD\u65b9\u6cd5\uff0c\u91c7\u7528\u589e\u91cf\u5b66\u4e60\u548c\u53cc\u5c42\u96c6\u6210\uff1a1\uff09\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u96c6\u6210\u7528\u4e8e\u5f02\u5e38\u9884\u6d4b\uff1b2\uff09\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u5668\u96c6\u6210\uff0c\u6bcf\u4e2a\u68c0\u6d4b\u5668\u4f7f\u7528\u57fa\u4e8e\u7edf\u8ba1\u7684\u6982\u5ff5\u6f02\u79fb\u673a\u5236\u3002", "result": "\u5728\u5177\u6709\u6781\u4f4e\u5f02\u5e38\u7387\u548c\u5404\u79cd\u6f02\u79fb\u7279\u5f81\u7684\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "VAE++ESDD\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u65e0\u6807\u7b7e\u6d41\u6570\u636e\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u548c\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2602.13128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13128", "abs": "https://arxiv.org/abs/2602.13128", "authors": ["Mohamed Tarraf", "Alex Chan", "Alex Yakovlev", "Rishad Shafik"], "title": "Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models", "comment": "Pre-print of latest work", "summary": "Binary Neural Networks (BNNs) offer a low-complexity and energy-efficient alternative to traditional full-precision neural networks by constraining their weights and activations to binary values. However, their discrete, highly non-linear behavior makes them difficult to explain, validate and formally verify. As a result, BNNs remain largely opaque, limiting their suitability in safety-critical domains, where causal transparency and behavioral guarantees are essential. In this work, we introduce a Petri net (PN)-based framework that captures the BNN's internal operations as event-driven processes. By \"eventizing\" their operations, we expose their causal relationships and dependencies for a fine-grained analysis of concurrency, ordering, and state evolution. Here, we construct modular PN blueprints for core BNN components including activation, gradient computation and weight updates, and compose them into a complete system-level model. We then validate the composed PN against a reference software-based BNN, verify it against reachability and structural checks to establish 1-safeness, deadlock-freeness, mutual exclusion and correct-by-construction causal sequencing, before we assess its scalability and complexity at segment, component, and system levels using the automated measurement tools in Workcraft. Overall, this framework enables causal introspection of transparent and event-driven BNNs that are amenable to formal reasoning and verification.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePetri\u7f51\u7684\u6846\u67b6\uff0c\u5c06\u4e8c\u503c\u795e\u7ecf\u7f51\u7edc\u7684\u4e8b\u4ef6\u9a71\u52a8\u8fc7\u7a0b\u5efa\u6a21\uff0c\u5b9e\u73b0\u56e0\u679c\u900f\u660e\u5ea6\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1", "motivation": "\u4e8c\u503c\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u4f4e\u590d\u6742\u5ea6\u3001\u9ad8\u80fd\u6548\uff0c\u4f46\u5176\u79bb\u6563\u975e\u7ebf\u6027\u884c\u4e3a\u96be\u4ee5\u89e3\u91ca\u3001\u9a8c\u8bc1\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u9650\u5236\u4e86\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528", "method": "\u5f15\u5165Petri\u7f51\u6846\u67b6\uff0c\u5c06BNN\u5185\u90e8\u64cd\u4f5c\u5efa\u6a21\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u8fc7\u7a0b\uff0c\u6784\u5efa\u6838\u5fc3\u7ec4\u4ef6\uff08\u6fc0\u6d3b\u3001\u68af\u5ea6\u8ba1\u7b97\u3001\u6743\u91cd\u66f4\u65b0\uff09\u7684\u6a21\u5757\u5316PN\u84dd\u56fe\uff0c\u5e76\u7ec4\u5408\u6210\u7cfb\u7edf\u7ea7\u6a21\u578b", "result": "\u9a8c\u8bc1\u4e86PN\u6a21\u578b\u4e0e\u53c2\u8003\u8f6f\u4ef6BNN\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u53ef\u8fbe\u6027\u548c\u7ed3\u6784\u68c0\u67e5\u9a8c\u8bc1\u4e861-\u5b89\u5168\u6027\u3001\u65e0\u6b7b\u9501\u3001\u4e92\u65a5\u548c\u6b63\u786e\u56e0\u679c\u5e8f\u5217\uff0c\u4f7f\u7528Workcraft\u5de5\u5177\u8bc4\u4f30\u4e86\u53ef\u6269\u5c55\u6027\u548c\u590d\u6742\u6027", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4e8b\u4ef6\u9a71\u52a8BNN\u7684\u56e0\u679c\u5185\u7701\uff0c\u4f7f\u5176\u9002\u5408\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u4e86BNN\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u9002\u7528\u6027"}}
