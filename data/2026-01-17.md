<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 首个合成水下事件相机光流基准数据集，通过物理渲染生成真实水下光学效果，为水下事件感知算法提供评估基准。


<details>
  <summary>Details</summary>
Motivation: 水下成像面临波长相关光衰减、悬浮颗粒散射、浑浊模糊和非均匀照明等挑战，传统相机难以获取真实运动数据。事件相机虽有微秒级分辨率和动态范围优势，但缺乏结合真实水下光学与准确光流的数据集。

Method: 基于物理光线追踪的RGBD序列生成合成水下基准数据集，使用现代视频到事件转换管道处理渲染的水下视频，生成真实事件数据流，包含密集的真实光流、深度和相机运动信息。

Result: 建立了首个水下事件光流基准数据集，对最先进的基于学习和模型的光流预测方法进行基准测试，分析水下光传输对事件形成和运动估计精度的影响。

Conclusion: 该数据集为未来水下事件感知算法的开发和评估建立了新基准，相关代码和数据集已公开。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [2] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 提出了VERHallu基准来评估视频大语言模型的事件关系幻觉问题，包括因果、时序和子事件关系，并提出了关键帧传播策略来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注视频中事件、物体和场景存在的幻觉，而忽视了事件关系幻觉。视频大语言模型在处理密集事件关系推理时存在困难，往往过度依赖先验知识而未能充分利用帧级线索。

Method: 1. 提出VERHallu基准，包含关系分类、问答和反事实问答三种任务类型，涵盖因果、时序和子事件关系；2. 提出关键帧传播策略，在中间层重新分配帧级注意力以增强多事件理解。

Result: 当前最先进的视频大语言模型在密集事件关系推理上表现不佳，虽然对关键事件有较强的定位能力，但经常忽略周围的子事件，导致对事件关系的理解不完整不准确。关键帧传播策略能有效缓解事件关系幻觉且不影响推理速度。

Conclusion: 事件关系幻觉是视频大语言模型的一个重要问题，VERHallu基准为评估这一问题提供了全面框架，而关键帧传播策略为解决这一问题提供了有效方法，能够在不牺牲效率的情况下提升模型对事件关系的理解能力。

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [3] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: LaViT框架通过对齐潜在视觉思维而非静态嵌入，解决多模态推理中的感知鸿沟问题，显著提升视觉基础能力


<details>
  <summary>Details</summary>
Motivation: 当前多模态潜在推理通常依赖外部监督（如辅助图像），忽视了内在的视觉注意力动态。研究发现存在关键的感知鸿沟：学生模型经常模仿教师的文本输出，但关注完全不同的视觉区域，实质上依赖语言先验而非基础感知。

Method: 提出LaViT框架，通过让学生模型自回归地重建教师的视觉语义和注意力轨迹（在文本生成之前），强制对齐潜在视觉思维。采用课程感知门控机制防止捷径学习。

Result: LaViT显著增强了视觉基础能力，在复杂推理任务上获得高达+16.9%的提升，使紧凑的3B模型能够超越更大的开源变体和GPT-4o等专有模型。

Conclusion: 通过对齐潜在视觉思维而非静态嵌入，LaViT有效解决了多模态推理中的感知鸿沟问题，为更可靠的多模态推理提供了新方向。

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [4] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出视频异常推理(VAR)新任务，从描述性理解提升到结构化多阶段推理，并构建包含8,641个视频的大规模数据集，开发了支持自适应分层推理的Vad-R1-Plus模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频异常检测与理解领域主要局限于异常定位或事后描述，缺乏显式推理过程、风险意识和决策导向解释，需要从描述性理解提升到结构化推理。

Method: 1) 定义VAR任务，要求模型在回答异常相关问题前进行渐进式推理；2) 构建包含8,641个视频、50,000+样本的数据集，采用PerCoAct-CoT结构化标注；3) 提出异常感知组相对策略优化增强弱监督下推理可靠性；4) 开发Vad-R1-Plus端到端MLLM模型，支持自适应分层推理和风险感知决策。

Result: 提出的基准和方法有效提升了MLLM在VAR任务上的推理能力，在开源和专有基线模型上都取得了更好的性能。

Conclusion: VAR任务将视频异常分析从描述性理解提升到结构化多阶段推理，通过大规模数据集和PerCoAct-CoT标注框架，结合提出的优化方法和Vad-R1-Plus模型，显著推进了MLLM在视频异常推理领域的能力。

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [5] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: Alterbute是一种基于扩散模型的图像编辑方法，专注于修改物体的内在属性（颜色、纹理、材质、形状），同时保持其感知身份和场景上下文。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖无监督先验但难以保持物体身份，要么使用过于严格的监督限制了内在属性的有效变化。需要一种既能保持物体身份又能实现有意义内在属性编辑的方法。

Method: 1) 使用宽松的训练目标，允许模型在身份参考图像、目标内在属性文本描述、背景图像和物体掩码的条件下改变内在和外在属性；推理时重用原始背景和掩码限制外在变化。2) 引入视觉命名实体(VNEs)作为细粒度视觉身份类别，通过视觉语言模型从大型公共图像数据集中自动提取VNE标签和内在属性描述。

Result: Alterbute在保持身份的物体内在属性编辑任务上优于现有方法。

Conclusion: Alterbute通过结合宽松训练目标和视觉命名实体的方法，实现了高质量、身份保持的物体内在属性编辑，解决了现有方法的局限性。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不可持续问题，数据生成者在价值分配中几乎得不到回报，大部分价值被聚合者获取，这威胁到整个机器学习生态的可持续性。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是揭示机器学习价值链中的经济不平等问题。研究发现数据从输入到模型权重再到合成输出的过程中，技术信号被不断提炼，但经济权益却从数据生成者那里被剥离，这不仅是经济福利问题，更威胁到当前学习算法的可持续性反馈循环。

Method: 通过分析73个公开数据交易案例，研究数据价值分配模式，识别出三个结构性缺陷：缺失溯源、不对称议价能力和非动态定价。沿着机器学习价值链追踪这些问题，并提出公平数据价值交换（EDVEX）框架。

Result: 分析显示，大部分价值流向聚合者，创作者版税几乎为零，交易条款普遍不透明。数据生成者在价值分配中被边缘化，这构成了机器学习生态系统的结构性风险。

Conclusion: 机器学习价值链存在结构性不可持续问题，需要建立公平的数据价值交换机制。提出的EDVEX框架旨在创建一个惠及所有参与者的最小市场，并指出了社区可以做出具体贡献的研究方向。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [7] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出一种混合Transformer架构，用连续深度神经ODE块替代离散中间层，通过学习到的控制信号实现推理时生成属性控制


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏推理时对生成属性的灵活控制能力。需要一种能够将深度作为连续变量、通过控制信号实现属性调节的架构

Method: 设计混合Transformer架构，用连续深度神经ODE块替换离散中间层。使用学习向量场F_θ(H, τ, u)，其中u是通过显式拼接注入的低维控制信号。采用伴随方法实现O(1)内存训练

Result: 1) 梯度流稳定，无梯度爆炸/消失；2) 语义控制达到98%/88%的正负情感控制准确率；3) 连续插值验证，固定与自适应求解器轨迹差异仅0.068%；4) 效率与标准离散基线相当。自适应ODE求解器揭示了学习动力学中的几何结构

Conclusion: 带有学习控制信号的连续深度动力学为可操控语言生成提供了可行且高效的机制，控制信号将向量场划分为具有不同曲率特征的动态机制

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [8] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 研究XGBoost模型在点击率预测中的时间聚合特征，发现滑动窗口设计相比目标编码能提升ROC AUC约0.0066-0.0082，事件计数窗口有微小额外增益，而间隔窗口和分桶窗口表现较差。


<details>
  <summary>Details</summary>
Motivation: 在点击率预测中，如何在遵守"无前瞻性"特征约束（仅使用历史数据）的情况下，有效利用时间序列信息进行特征工程，特别是比较不同时间聚合窗口设计的效果。

Method: 使用Avazu点击率预测数据集，采用严格的时序分割和"无前瞻性"特征约束。比较了时间感知目标编码基线模型与添加实体历史时间聚合特征的模型，测试了多种窗口设计（滑动窗口、事件计数窗口、间隔窗口、分桶窗口）。

Result: 滑动窗口相比目标编码基线提升ROC AUC约0.0066-0.0082，PR AUC约0.0084-0.0094。事件计数窗口有微小但一致的额外提升，而间隔窗口和分桶窗口表现不如简单滑动窗口。

Conclusion: 建议实践中默认使用滑动窗口进行时间聚合，当边际ROC AUC增益重要时可考虑添加事件计数窗口，间隔窗口和分桶窗口在此数据集和协议下效果不佳。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [9] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出Graph Regularized PCA (GR-PCA)，通过图正则化处理非独立同分布噪声的高维数据，学习稀疏精度图并偏向图拉普拉斯低频傅里叶模式，提高结构保真度。


<details>
  <summary>Details</summary>
Motivation: 高维数据常存在变量间依赖关系，违反PCA的等方差噪声假设。当噪声在特征间非独立同分布时，需要能处理非球形协方差的方法。

Method: 提出图正则化PCA，通过学习稀疏精度图，将载荷偏向图拉普拉斯的低频傅里叶模式，抑制高频信号，保留图一致的低频信号。

Result: 在多种图拓扑、信噪比和稀疏度下评估，相比主流方法能更好集中方差在目标支撑集上，产生更低图拉普拉斯能量的载荷，保持样本外重建竞争力。

Conclusion: GR-PCA提供实用的结构感知降维方法，在高频信号图相关时优势明显，实现简单、模块化且可扩展，在保持预测性能的同时提高结构保真度。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 论文提出了一个分析AI存在性风险的框架，基于两个前提：AI将变得极其强大；极其强大的AI会毁灭人类。通过这两个前提构建了人类存续的分类，并评估了不同存续故事面临的挑战和应对策略。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成存在性风险的争论日益激烈。本文旨在建立一个系统性的框架来思考AI的存在性风险问题，澄清相关讨论并提供分析工具。

Method: 构建了一个基于两个前提的分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提构建了四种人类存续的分类，分析每种存续故事面临的挑战，并探讨相应的应对策略。

Result: 提出了一个系统性的分类框架，将人类存续的可能性分为四类：科学障碍阻止AI变得极其强大；人类禁止AI研究；极其强大的AI因目标设定而不毁灭人类；能够可靠检测并禁用有毁灭目标的AI系统。分析了每种情况的挑战，并基于此框架对P(doom)进行了粗略估计。

Conclusion: AI存在性风险的分析需要系统性的框架，不同的存续故事面临不同的挑战并需要不同的应对策略。该分类框架有助于澄清相关讨论，为风险评估和政策制定提供基础，并能够对AI毁灭人类的概率进行更结构化的估计。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [11] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 提出单次规划方法解决计算机使用代理的安全漏洞，通过可信规划器在观察恶意内容前生成完整执行图，提供可证明的控制流完整性保证，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: AI代理易受提示注入攻击，现有唯一可靠防御是架构隔离，但计算机使用代理需要持续观察UI状态来决策动作，这与安全所需的隔离存在根本冲突。

Method: 引入单次规划方法：可信规划器在观察任何潜在恶意内容前生成包含条件分支的完整执行图，提供可证明的控制流完整性保证，防止指令注入攻击。

Result: 在OSWorld上评估，保持前沿模型57%性能的同时，将较小开源模型性能提升达19%，证明严格安全性和实用性可在计算机使用代理中共存。

Conclusion: 通过单次规划方法解决了计算机使用代理的安全与功能冲突，但需要额外措施防止分支导向攻击，展示了安全与实用性可协调实现。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [12] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的目标路由方法，仅在关键步骤（可能引发级联错误的步骤）使用大模型，而让较小模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给单一模型，将所有推理步骤视为同等重要。多步推理任务（如数学问题求解）容易发生级联失败，单个错误步骤会导致整个解决方案崩溃。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了从简单阈值策略到更复杂策略的多种路由方法，这些策略考虑长时程精度-成本权衡和步骤正确性估计的不确定性。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前路由方法，成本效率提高5倍；更高级的策略使用80%更少的大模型token就能匹配强大昂贵模型的性能。在AIME等更难基准上，TRIM实现了高达6倍的成本效率提升。

Conclusion: 所有方法在数学推理任务上都能有效泛化，表明步骤级难度代表了推理的基本特征。目标化的步骤级干预能够从根本上改变推理效率，将昂贵调用限制在那些大模型能够防止级联错误的关键步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [13] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个用于HRV解释的临床推理框架，通过八步可追溯推理步骤和Z-score优先级层次结构，有效减少生理幻觉，在情感分类中实现37.3%准确率和69.6%临床推理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在HRV解释中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标的短数据不稳定性，以及过度依赖群体标准而忽视个体化基线。这些问题阻碍了LLM在HRV分析中的临床应用。

Method: 提出C-GRASP框架，包含八个可追溯推理步骤，核心是Z-score优先级层次结构，优先考虑个体化基线变化而非群体统计。系统通过自动RSA感知护栏减少频谱幻觉，防止频域指标污染。采用RAG增强管道，并与高规模推理模型（如MedGemma3-thinking）集成。

Result: 在DREAMER数据集的414个试验中，C-GRASP在4类情感分类中达到37.3%准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点，能防止原生LLM常见的"群体偏差"。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平道路，实现了从群体统计到个体化临床推理的转变。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [14] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出SafeProbing方法，通过在解码过程中显式利用LLMs内部的安全相关信号，实现早期检测不安全内容，有效防御越狱攻击同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs经过安全对齐，但现有对齐往往是浅层的，容易受到越狱攻击。现有的防御机制（如解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不鲁棒，要么过度降低模型效用。

Method: 通过观察发现，即使成功越狱，模型在生成过程中内部仍会表现出潜在的安全相关信号，但这些信号被模型追求流畅续写的驱动所覆盖。基于此，提出显式提取和利用这些潜在安全信号的方法，在解码过程中早期检测不安全内容。

Result: 在各种越狱攻击上的实验表明，该方法显著增强了安全性，同时在良性输入上保持较低的过度拒绝率，并保持了响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向。代码已开源。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

TL;DR: BHyT是一种替代Pre-LN的归一化方法，通过双曲正切非线性结合数据驱动的输入边界控制，解决了深度网络中的稳定性问题，同时提高了训练效率。


<details>
  <summary>Details</summary>
Motivation: Pre-LN虽然是大型语言模型的标准选择，但存在效率低下（重复统计计算）和深度诅咒问题（随着层数增加，隐藏状态幅度和方差增大导致训练不稳定）。现有的效率导向的无归一化方法（如DyT）在深度网络中仍然脆弱。

Method: 提出Bounded Hyperbolic Tanh (BHyT)方法：1) 结合tanh非线性激活函数和显式的数据驱动输入边界控制，将激活值限制在非饱和范围内；2) 每个块只计算一次精确统计量，用轻量级方差近似替代第二次归一化，提高效率；3) 提供理论稳定性保证。

Result: BHyT在预训练中表现出更好的稳定性和效率：平均训练速度比RMSNorm快15.8%，token生成吞吐量平均提高4.2%，同时在语言理解和推理基准测试中匹配或超越了RMSNorm的推理性能和鲁棒性。

Conclusion: BHyT作为Pre-LN的替代方案，能够同时解决深度网络中的稳定性问题和效率问题，在保持或提升性能的同时显著加速训练和推理过程。

Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Formal Safety Guarantees for Autonomous Vehicles using Barrier Certificates](https://arxiv.org/abs/2601.09740)
*Oumaima Barhoumi,Mohamed H Zaki,Sofiène Tahar*

Main category: cs.RO

TL;DR: 本文提出了一种结合屏障证书与可解释交通冲突指标的形式化验证安全框架，用于确保联网自动驾驶车辆在动态混合交通环境中的安全性，通过SMT求解器验证安全条件，并在真实高速公路数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现代AI驱动的自动驾驶系统虽然能够感知复杂场景并做出实时决策，但通常作为黑箱运行，缺乏可解释性和严格的安全保证。在动态混合交通环境中，与人类驾驶车辆的交互引入了不确定性和安全挑战，需要可验证的安全框架。

Method: 开发了一个形式化验证的安全框架，将屏障证书（BCs）与可解释的交通冲突指标（特别是作为时空安全指标的碰撞时间TTC）相结合。使用可满足性模理论（SMT）求解器验证安全条件，并采用自适应控制机制确保车辆实时遵守这些约束。

Result: 在真实世界高速公路数据集上的评估显示，不安全交互显著减少：TTC低于3秒阈值的事件减少了高达40%，在某些车道中完全消除了冲突。该方法提供了可解释且可证明的安全保证。

Conclusion: 该方法为自动驾驶提供了一个实用且可扩展的安全策略，通过形式化验证框架结合屏障证书和可解释安全指标，实现了可证明的安全保证，解决了AI黑箱系统的可解释性和安全性问题。

Abstract: Modern AI technologies enable autonomous vehicles to perceive complex scenes, predict human behavior, and make real-time driving decisions. However, these data-driven components often operate as black boxes, lacking interpretability and rigorous safety guarantees. Autonomous vehicles operate in dynamic, mixed-traffic environments where interactions with human-driven vehicles introduce uncertainty and safety challenges. This work develops a formally verified safety framework for Connected and Autonomous Vehicles (CAVs) that integrates Barrier Certificates (BCs) with interpretable traffic conflict metrics, specifically Time-to-Collision (TTC) as a spatio-temporal safety metric. Safety conditions are verified using Satisfiability Modulo Theories (SMT) solvers, and an adaptive control mechanism ensures vehicles comply with these constraints in real time. Evaluation on real-world highway datasets shows a significant reduction in unsafe interactions, with up to 40\% fewer events where TTC falls below a 3 seconds threshold, and complete elimination of conflicts in some lanes. This approach provides both interpretable and provable safety guarantees, demonstrating a practical and scalable strategy for safe autonomous driving.

</details>


### [17] [CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments](https://arxiv.org/abs/2601.10116)
*Xintong Zhang,Junfeng Chen,Yuxiao Zhu,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: CoCoPlan是一个多机器人协作框架，通过联合优化任务规划和间歇性通信，在动态时空任务分布和有限通信条件下实现高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要全时通信，要么依赖固定调度或成对协议，无法有效适应有限通信下的动态时空任务分布，导致协调效率低下。

Method: 采用分支定界架构联合编码任务分配和通信事件，自适应目标函数平衡任务效率和通信延迟，通信事件优化模块策略性地确定何时、何地以及如何重建全局连接。

Result: 实验表明，CoCoPlan在任务完成率上比现有方法提高22.4%，通信开销降低58.6%，可扩展性支持多达100个机器人在动态环境中运行。

Conclusion: CoCoPlan通过联合优化任务规划和团队间歇性通信，有效解决了有限通信条件下多机器人系统的协调问题，在动态环境中实现了高效、可扩展的协作。

Abstract: Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.

</details>


### [18] [The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation](https://arxiv.org/abs/2601.10268)
*Eszter Birtalan,Miklós Koller*

Main category: cs.RO

TL;DR: 研究通过仿真评估6种不同触觉传感器配置（密度和布局）对强化学习性能的影响，发现一种配置在两种实验设置下均表现最佳，为机器人手设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 触觉传感器在机器人领域应用日益重要，能提供接触面信息改善抓握稳定性。但目前机器人手设计中传感器配置差异很大且占用大量空间，缺乏系统评估不同配置对学习性能影响的研究。

Method: 使用仿真评估6种不同密度和布局的触觉传感器配置，采用双实验设置确保结果不依赖于特定物理模拟器、机器人手模型或机器学习算法，通过强化学习评估各配置性能。

Result: 结果显示存在特定设置的影响，以及跨6种传感器化仿真的普遍效应，识别出一种配置在两种实验设置下始终表现最佳。

Conclusion: 研究结果为未来机器人手设计（包括假肢）提供指导，帮助优化触觉传感器配置以提升学习性能。

Abstract: Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.

</details>
