{"id": "2510.07356", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.07356", "abs": "https://arxiv.org/abs/2510.07356", "authors": ["Lingcheng Kong", "Jiateng Wei", "Hanzhang Shen", "Huan Wang"], "title": "ConCuR: Conciseness Makes State-of-the-Art Kernel Generation", "comment": null, "summary": "GPU kernel generation by LLMs has recently experienced rapid development,\nleveraging test-time scaling and reinforcement learning techniques. However, a\nkey challenge for kernel generation is the scarcity of high-quality data, as\nmost high-quality kernels are proprietary and not open-source. This challenge\nprevents us from leveraging supervised fine-tuning to align LLMs to the kernel\ngeneration task. To address this challenge, we develop a pipeline that\ngenerates and curates high-quality CUDA kernels with reasoning traces,\nmotivated by a critical observation that concise yet informative reasoning\ntraces result in robust generation of high-performance kernels. Using this\npipeline, we construct our dataset ConCuR and introduce our model KernelCoder,\nwhich is the first model trained on a curated dataset consisting of PyTorch,\nreasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,\nour model achieves significant improvements over the existing top-performing\nmodel, QwQ-32B, and outperforms all open-source models fine-tuned for kernel\ngeneration, as well as frontier models such as DeepSeek-V3.1-Think and\nClaude-4-sonnet. Finally, we show that the average reasoning length can serve\nas a metric to assess the difficulty of kernel generation tasks. The\nobservations, metrics, and our data collection and curation pipeline can help\nobtain better data in the kernel generation task in the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u751f\u6210\u548c\u7b5b\u9009\u9ad8\u8d28\u91cfCUDA\u5185\u6838\u7684\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86ConCuR\u6570\u636e\u96c6\u548cKernelCoder\u6a21\u578b\uff0c\u5728KernelBench\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5185\u6838\u751f\u6210\u4efb\u52a1\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u9ad8\u8d28\u91cf\u5185\u6838\u662f\u4e13\u6709\u4e14\u4e0d\u5f00\u6e90\u7684\uff0c\u8fd9\u963b\u788d\u4e86\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u6765\u5bf9\u9f50LLMs\u8fdb\u884c\u5185\u6838\u751f\u6210\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u548c\u7b5b\u9009\u5e26\u6709\u63a8\u7406\u8f68\u8ff9\u7684\u9ad8\u8d28\u91cfCUDA\u5185\u6838\u7684\u6d41\u7a0b\uff0c\u6784\u5efaConCuR\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3KernelCoder\u6a21\u578b\u3002", "result": "\u5728KernelBench\u8bbe\u7f6e\u4e2d\uff0c\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578bQwQ-32B\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5f00\u6e90\u5185\u6838\u751f\u6210\u5fae\u8c03\u6a21\u578b\u4ee5\u53ca\u524d\u6cbf\u6a21\u578b\u5982DeepSeek-V3.1-Think\u548cClaude-4-sonnet\u3002", "conclusion": "\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u53ef\u4ee5\u4f5c\u4e3a\u8bc4\u4f30\u5185\u6838\u751f\u6210\u4efb\u52a1\u96be\u5ea6\u7684\u6307\u6807\uff0c\u8be5\u7814\u7a76\u7684\u6570\u636e\u6536\u96c6\u548c\u7b5b\u9009\u6d41\u7a0b\u6709\u52a9\u4e8e\u672a\u6765\u83b7\u5f97\u66f4\u597d\u7684\u5185\u6838\u751f\u6210\u6570\u636e\u3002"}}
{"id": "2510.07567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07567", "abs": "https://arxiv.org/abs/2510.07567", "authors": ["Karuna Bhaila", "Aneesh Komanduri", "Minh-Hao Van", "Xintao Wu"], "title": "Cross-Modal Attention Guided Unlearning in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated immense capabilities in\nmulti-modal understanding and inference tasks such as Visual Question Answering\n(VQA), which requires models to infer outputs based on visual and textual\ncontext simultaneously. Such inference abilities of large-scale pretrained\nmodels are often attributed to the massive scale of pre-training data collected\nacross several domains. However, the models may memorize private and/or\nsensitive information during training and regurgitate it in inference.\nRecently, machine unlearning has been leveraged to address the leakage of\nprivate data in LLMs. VLMs add a layer of complexity to this process, as the\nvisual context in the query may also contain sensitive information in addition\nto the text. To address this issue, we explore unlearning for vision-language\nmodels, specifically for the VQA task. We explore the role of visual tokens for\noutput generation in VLMs using cross-modal attention and utilize it to\nformulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and\nefficient VLM unlearning framework. In contrast to computationally expensive\nmodel finetuning methods, CAGUL utilizes external modules to encode unlearning\ninformation in visual tokens of low importance for relevant queries. We find\nthat the transformed visual tokens not only prevent leakage but also retain\nreference model behavior. Experimental results show that our method performs\nbetter or on par with finetuning-based baselines without altering the\npre-trained model parameters or incurring retraining costs, making it a\npractical and effective unlearning solution for VLMs.", "AI": {"tldr": "\u63d0\u51faCAGUL\u6846\u67b6\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5f15\u5bfc\u6765\u9632\u6b62\u654f\u611f\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u8bb0\u5fc6\u5e76\u6cc4\u9732\u9690\u79c1\u654f\u611f\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u7684\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u6a21\u578b\uff0c\u89c6\u89c9\u6a21\u6001\u589e\u52a0\u4e86\u9057\u5fd8\u7684\u590d\u6742\u6027\u3002", "method": "\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5206\u6790\u89c6\u89c9token\u5bf9\u8f93\u51fa\u7684\u8d21\u732e\uff0c\u901a\u8fc7\u5916\u90e8\u6a21\u5757\u5728\u4f4e\u91cd\u8981\u6027\u89c6\u89c9token\u4e2d\u7f16\u7801\u9057\u5fd8\u4fe1\u606f\uff0c\u4e0d\u6539\u53d8\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u5fae\u8c03\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u80fd\u6709\u6548\u9632\u6b62\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u8003\u6a21\u578b\u884c\u4e3a\u3002", "conclusion": "CAGUL\u662f\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\uff0c\u4e0d\u6539\u53d8\u9884\u8bad\u7ec3\u53c2\u6570\u3002"}}
{"id": "2510.07674", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07674", "abs": "https://arxiv.org/abs/2510.07674", "authors": ["Lucas Chen", "Shrutheesh Raman Iyer", "Zachary Kingston"], "title": "Differentiable Particle Optimization for Fast Sequential Manipulation", "comment": "8 pages, 7 figures, 3 tables. Under review", "summary": "Sequential robot manipulation tasks require finding collision-free\ntrajectories that satisfy geometric constraints across multiple object\ninteractions in potentially high-dimensional configuration spaces. Solving\nthese problems in real-time and at large scales has remained out of reach due\nto computational requirements. Recently, GPU-based acceleration has shown\npromising results, but prior methods achieve limited performance due to CPU-GPU\ndata transfer overhead and complex logic that prevents full hardware\nutilization. To this end, we present SPaSM (Sampling Particle optimization for\nSequential Manipulation), a fully GPU-parallelized framework that compiles\nconstraint evaluation, sampling, and gradient-based optimization into optimized\nCUDA kernels for end-to-end trajectory optimization without CPU coordination.\nThe method consists of a two-stage particle optimization strategy: first\nsolving placement constraints through massively parallel sampling, then lifting\nsolutions to full trajectory optimization in joint space. Unlike hierarchical\napproaches, SPaSM jointly optimizes object placements and robot trajectories to\nhandle scenarios where motion feasibility constrains placement options.\nExperimental evaluation on challenging benchmarks demonstrates solution times\nin the realm of $\\textbf{milliseconds}$ with a 100% success rate; a\n$4000\\times$ speedup compared to existing approaches.", "AI": {"tldr": "SPaSM\u662f\u4e00\u4e2a\u5b8c\u5168GPU\u5e76\u884c\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ea6\u675f\u8bc4\u4f30\u3001\u91c7\u6837\u548c\u68af\u5ea6\u4f18\u5316\u7f16\u8bd1\u4e3a\u4f18\u5316\u7684CUDA\u5185\u6838\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u65e0\u9700CPU\u534f\u8c03\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6beb\u79d2\u7ea7\u7684\u6c42\u89e3\u65f6\u95f4\u548c100%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u987a\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u5728\u53ef\u80fd\u7684\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u4e2d\u627e\u5230\u6ee1\u8db3\u591a\u4e2a\u7269\u4f53\u4ea4\u4e92\u51e0\u4f55\u7ea6\u675f\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\u3002\u7531\u4e8e\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4e00\u76f4\u96be\u4ee5\u5b9e\u73b0\u3002\u73b0\u6709\u7684GPU\u52a0\u901f\u65b9\u6cd5\u7531\u4e8eCPU-GPU\u6570\u636e\u4f20\u8f93\u5f00\u9500\u548c\u590d\u6742\u903b\u8f91\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u3002", "method": "SPaSM\u91c7\u7528\u4e24\u9636\u6bb5\u7c92\u5b50\u4f18\u5316\u7b56\u7565\uff1a\u9996\u5148\u901a\u8fc7\u5927\u89c4\u6a21\u5e76\u884c\u91c7\u6837\u89e3\u51b3\u653e\u7f6e\u7ea6\u675f\uff0c\u7136\u540e\u5c06\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u5230\u5173\u8282\u7a7a\u95f4\u4e2d\u7684\u5b8c\u6574\u8f68\u8ff9\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u5c06\u7ea6\u675f\u8bc4\u4f30\u3001\u91c7\u6837\u548c\u68af\u5ea6\u4f18\u5316\u7f16\u8bd1\u4e3a\u4f18\u5316\u7684CUDA\u5185\u6838\uff0c\u5b9e\u73b0\u5b8c\u5168GPU\u5e76\u884c\u5316\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPaSM\u5b9e\u73b0\u4e86\u6beb\u79d2\u7ea7\u7684\u6c42\u89e3\u65f6\u95f4\u548c100%\u7684\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e864000\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SPaSM\u901a\u8fc7\u5b8c\u5168GPU\u5e76\u884c\u5316\u548c\u4f18\u5316\u7684CUDA\u5185\u6838\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u987a\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5b9e\u65f6\u8ba1\u7b97\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u901f\u5ea6\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2510.07520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07520", "abs": "https://arxiv.org/abs/2510.07520", "authors": ["Rayyan Merchant", "Kevin Tang"], "title": "ParsTranslit: Truly Versatile Tajik-Farsi Transliteration", "comment": null, "summary": "As a digraphic language, the Persian language utilizes two written standards:\nPerso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite\nthe significant similarity between the dialects of each country, script\ndifferences prevent simple one-to-one mapping, hindering written communication\nand interaction between Tajikistan and its Persian-speaking ``siblings''. To\novercome this, previously-published efforts have investigated machine\ntransliteration models to convert between the two scripts. Unfortunately, most\nefforts did not use datasets other than those they created, limiting these\nmodels to certain domains of text such as archaic poetry or word lists. A truly\nusable transliteration system must be capable of handling varied domains,\nmeaning that suck models lack the versatility required for real-world usage.\nThe contrast in domain between data also obscures the task's true difficulty.\nWe present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi\ntransliteration trained across all available datasets, and present two datasets\nof our own. Our results across domains provide clearer understanding of the\ntask, and set comprehensive comparable leading benchmarks. Overall, our model\nachieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik\nand 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available\nat https://anonymous.4open.science/r/ParsTranslit-FB30/.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u6ce2\u65af\u8bed-\u5854\u5409\u514b\u8bed\u53cc\u5411\u97f3\u8bd1\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728\u8de8\u9886\u57df\u6587\u672c\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u57fa\u51c6\u3002", "motivation": "\u6ce2\u65af\u8bed\u4f7f\u7528\u4e24\u79cd\u4e66\u5199\u6807\u51c6\uff08\u6ce2\u65af-\u963f\u62c9\u4f2f\u5b57\u6bcd\u548c\u5854\u5409\u514b-\u897f\u91cc\u5c14\u5b57\u6bcd\uff09\uff0c\u963b\u788d\u4e86\u5854\u5409\u514b\u65af\u5766\u4e0e\u5176\u4ed6\u6ce2\u65af\u8bed\u56fd\u5bb6\u7684\u4e66\u9762\u4ea4\u6d41\u3002\u73b0\u6709\u97f3\u8bd1\u6a21\u578b\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6587\u672c\uff0c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u7684\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff0c\u5728\u6240\u6709\u53ef\u7528\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8d21\u732e\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5854\u5409\u514b\u8bed\u548c\u6ce2\u65af\u8bed\u4e4b\u95f4\u7684\u53cc\u5411\u97f3\u8bd1\u3002", "result": "\u6a21\u578b\u5728Farsi\u5230Tajik\u65b9\u5411\u83b7\u5f97chrF++ 87.91\u548c\u6807\u51c6\u5316CER 0.05\uff0cTajik\u5230Farsi\u65b9\u5411\u83b7\u5f97chrF++ 92.28\u548c\u6807\u51c6\u5316CER 0.04\uff0c\u5728\u6240\u6709\u9886\u57df\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u4efb\u52a1\u96be\u5ea6\u7406\u89e3\uff0c\u5efa\u7acb\u4e86\u5168\u9762\u7684\u53ef\u6bd4\u57fa\u51c6\uff0c\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4e3a\u5b9e\u9645\u8de8\u6587\u5316\u4ea4\u6d41\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07865", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07865", "abs": "https://arxiv.org/abs/2510.07865", "authors": ["Guowei Zou", "Haitao Wang", "Hejun Wu", "Yukun Qian", "Yuhang Wang", "Weibing Li"], "title": "DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation", "comment": "Website with code: https://guowei-zou.github.io/dm1/", "summary": "The ability to learn multi-modal action distributions is indispensable for\nrobotic manipulation policies to perform precise and robust control. Flow-based\ngenerative models have recently emerged as a promising solution to learning\ndistributions of actions, offering one-step action generation and thus\nachieving much higher sampling efficiency compared to diffusion-based methods.\nHowever, existing flow-based policies suffer from representation collapse, the\ninability to distinguish similar visual representations, leading to failures in\nprecise manipulation tasks. We propose DM1 (MeanFlow with Dispersive\nRegularization for One-Step Robotic Manipulation), a novel flow matching\nframework that integrates dispersive regularization into MeanFlow to prevent\ncollapse while maintaining one-step efficiency. DM1 employs multiple dispersive\nregularization variants across different intermediate embedding layers,\nencouraging diverse representations across training batches without introducing\nadditional network modules or specialized training procedures. Experiments on\nRoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s\nvs. 2-3.5s) and improves success rates by 10-20 percentage points, with the\nLift task reaching 99% success over 85% of the baseline. Real-robot deployment\non a Franka Panda further validates that DM1 transfers effectively from\nsimulation to the physical world. To the best of our knowledge, this is the\nfirst work to leverage representation regularization to enable flow-based\npolicies to achieve strong performance in robotic manipulation, establishing a\nsimple yet powerful approach for efficient and robust manipulation.", "AI": {"tldr": "DM1\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u6563\u6b63\u5219\u5316\u7684\u6d41\u5339\u914d\u6846\u67b6\uff0c\u89e3\u51b3\u6d41\u57fa\u7b56\u7565\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e00\u6b65\u751f\u6210\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd", "motivation": "\u6d41\u57fa\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4e2d\u5177\u6709\u91c7\u6837\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u8868\u793a\u5d29\u6e83\u95ee\u9898\uff0c\u65e0\u6cd5\u533a\u5206\u76f8\u4f3c\u7684\u89c6\u89c9\u8868\u793a\uff0c\u5bfc\u81f4\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u5931\u8d25", "method": "\u5728MeanFlow\u4e2d\u96c6\u6210\u591a\u79cd\u5206\u6563\u6b63\u5219\u5316\u53d8\u4f53\uff0c\u5728\u4e0d\u540c\u4e2d\u95f4\u5d4c\u5165\u5c42\u9f13\u52b1\u8bad\u7ec3\u6279\u6b21\u5185\u7684\u8868\u793a\u591a\u6837\u6027\uff0c\u65e0\u9700\u989d\u5916\u7f51\u7edc\u6a21\u5757\u6216\u4e13\u95e8\u8bad\u7ec3\u8fc7\u7a0b", "result": "\u5728RoboMimic\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b020-40\u500d\u63a8\u7406\u52a0\u901f(0.07s vs 2-3.5s)\uff0c\u6210\u529f\u7387\u63d0\u534710-20\u4e2a\u767e\u5206\u70b9\uff0cLift\u4efb\u52a1\u8fbe\u523099%\u6210\u529f\u7387\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u9a8c\u8bc1\u4e86\u4ece\u4eff\u771f\u5230\u7269\u7406\u4e16\u754c\u7684\u6709\u6548\u8fc1\u79fb", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5229\u7528\u8868\u793a\u6b63\u5219\u5316\u4f7f\u6d41\u57fa\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u5f3a\u6027\u80fd\u7684\u5de5\u4f5c\uff0c\u4e3a\u9ad8\u6548\u9c81\u68d2\u64cd\u4f5c\u5efa\u7acb\u4e86\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5"}}
{"id": "2510.07614", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07614", "abs": "https://arxiv.org/abs/2510.07614", "authors": ["Amine Barrak"], "title": "Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines", "comment": null, "summary": "Sequential multi-agent systems built with large language models (LLMs) can\nautomate complex software tasks, but they are hard to trust because errors\nquietly pass from one stage to the next. We study a traceable and accountable\npipeline, meaning a system with clear roles, structured handoffs, and saved\nrecords that let us trace who did what at each step and assign blame when\nthings go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We\nevaluate eight configurations of three state-of-the-art LLMs on three\nbenchmarks and analyze where errors start, how they spread, and how they can be\nfixed. Our results show: (1) adding a structured, accountable handoff between\nagents markedly improves accuracy and prevents the failures common in simple\npipelines; (2) models have clear role-specific strengths and risks (e.g.,\nsteady planning vs. high-variance critiquing), which we quantify with repair\nand harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent,\nwith heterogeneous pipelines often the most efficient. Overall, we provide a\npractical, data-driven method for designing, tracing, and debugging reliable,\npredictable, and accountable multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u8ffd\u6eaf\u548c\u95ee\u8d23\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7Planner->Executor->Critic\u6d41\u6c34\u7ebf\u7ed3\u6784\uff0c\u5206\u6790\u4e86\u9519\u8bef\u4f20\u64ad\u673a\u5236\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u987a\u5e8f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u81ea\u52a8\u5316\u590d\u6742\u8f6f\u4ef6\u4efb\u52a1\uff0c\u4f46\u7531\u4e8e\u9519\u8bef\u4f1a\u5728\u5404\u9636\u6bb5\u95f4\u9759\u9ed8\u4f20\u9012\uff0c\u5bfc\u81f4\u7cfb\u7edf\u96be\u4ee5\u4fe1\u4efb\u3002\u9700\u8981\u5efa\u7acb\u53ef\u8ffd\u6eaf\u548c\u95ee\u8d23\u7684\u6d41\u6c34\u7ebf\u6765\u8ffd\u8e2a\u9519\u8bef\u6765\u6e90\u3002", "method": "\u91c7\u7528Planner->Executor->Critic\u6d41\u6c34\u7ebf\u7ed3\u6784\uff0c\u8bc4\u4f30\u4e868\u79cd\u914d\u7f6e\u7684\u4e09\u79cd\u6700\u5148\u8fdbLLM\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u9519\u8bef\u8d77\u59cb\u3001\u4f20\u64ad\u548c\u4fee\u590d\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u5728\u667a\u80fd\u4f53\u95f4\u6dfb\u52a0\u7ed3\u6784\u5316\u3001\u53ef\u95ee\u8d23\u7684\u4ea4\u63a5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\uff1b(2)\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u89d2\u8272\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u548c\u98ce\u9669\uff1b(3)\u51c6\u786e\u6027-\u6210\u672c-\u5ef6\u8fdf\u6743\u8861\u662f\u4efb\u52a1\u4f9d\u8d56\u7684\uff0c\u5f02\u6784\u6d41\u6c34\u7ebf\u901a\u5e38\u6700\u6709\u6548\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u3001\u8ffd\u8e2a\u548c\u8c03\u8bd5\u53ef\u9760\u3001\u53ef\u9884\u6d4b\u548c\u53ef\u95ee\u8d23\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2510.07752", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07752", "abs": "https://arxiv.org/abs/2510.07752", "authors": ["Junhao He", "Jiaxu Wang", "Jia Li", "Mingyuan Sun", "Qiang Zhang", "Jiahang Cao", "Ziyi Zhang", "Yi Gu", "Jingkai Sun", "Renjing Xu"], "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream", "comment": "Accepted by TVCG", "summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB\nvideos is challenging. This is because large inter-frame motions will increase\nthe uncertainty of the solution space. For example, one pixel in the first\nframe might have more choices to reach the corresponding pixel in the second\nframe. Event cameras can asynchronously capture rapid visual changes and are\nrobust to motion blur, but they do not provide color information. Intuitively,\nthe event stream can provide deterministic constraints for the inter-frame\nlarge motion by the event trajectories. Hence, combining\nlow-temporal-resolution images with high-framerate event streams can address\nthis challenge. However, it is challenging to jointly optimize Dynamic 3DGS\nusing both RGB and event modalities due to the significant discrepancy between\nthese two data modalities. This paper introduces a novel framework that jointly\noptimizes dynamic 3DGS from the two modalities. The key idea is to adopt event\nmotion priors to guide the optimization of the deformation fields. First, we\nextract the motion priors encoded in event streams by using the proposed LoCM\nunsupervised fine-tuning framework to adapt an event flow estimator to a\ncertain unseen scene. Then, we present the geometry-aware data association\nmethod to build the event-Gaussian motion correspondence, which is the primary\nfoundation of the pipeline, accompanied by two useful strategies, namely motion\ndecomposition and inter-frame pseudo-label. Extensive experiments show that our\nmethod outperforms existing image and event-based approaches across synthetic\nand real scenes and prove that our method can effectively optimize dynamic 3DGS\nwith the help of event data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4e\u5e27\u7387RGB\u89c6\u9891\u548c\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6765\u91cd\u5efa\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u53d8\u5f62\u573a\u4f18\u5316\uff0c\u89e3\u51b3\u5927\u5e27\u95f4\u8fd0\u52a8\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002", "motivation": "\u4ece\u4f4e\u5e27\u7387RGB\u89c6\u9891\u91cd\u5efa\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5927\u5e27\u95f4\u8fd0\u52a8\u4f1a\u589e\u52a0\u89e3\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u4e8b\u4ef6\u76f8\u673a\u80fd\u5f02\u6b65\u6355\u6349\u5feb\u901f\u89c6\u89c9\u53d8\u5316\u4e14\u5bf9\u8fd0\u52a8\u6a21\u7cca\u9c81\u68d2\uff0c\u4f46\u7f3a\u4e4f\u989c\u8272\u4fe1\u606f\uff0c\u5c06\u4e24\u8005\u7ed3\u5408\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u53d8\u5f62\u573a\u4f18\u5316\uff1a1\uff09\u4f7f\u7528LoCM\u65e0\u76d1\u7763\u5fae\u8c03\u6846\u67b6\u63d0\u53d6\u4e8b\u4ef6\u6d41\u4e2d\u7684\u8fd0\u52a8\u5148\u9a8c\uff1b2\uff09\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u6570\u636e\u5173\u8054\u65b9\u6cd5\u5efa\u7acb\u4e8b\u4ef6-\u9ad8\u65af\u8fd0\u52a8\u5bf9\u5e94\u5173\u7cfb\uff1b3\uff09\u4f7f\u7528\u8fd0\u52a8\u5206\u89e3\u548c\u5e27\u95f4\u4f2a\u6807\u7b7e\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u548c\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e8b\u4ef6\u6570\u636e\u80fd\u6709\u6548\u4f18\u5316\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4f4e\u5e27\u7387RGB\u56fe\u50cf\u548c\u9ad8\u5e27\u7387\u4e8b\u4ef6\u6d41\uff0c\u5229\u7528\u4e8b\u4ef6\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u53d8\u5f62\u573a\u4f18\u5316\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\u4e2d\u7684\u5927\u5e27\u95f4\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002"}}
{"id": "2510.07790", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07790", "abs": "https://arxiv.org/abs/2510.07790", "authors": ["Hao Wu", "Wei Liu"], "title": "GCPO: When Contrast Fails, Go Gold", "comment": null, "summary": "Reinforcement learning has been widely applied to enhance the reasoning\ncapabilities of large language models. Extending the inference limits of\nsmaller models has become a prominent research focus. However, algorithms such\nas Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the\nupper bound of a model's rollout responses is entirely determined by the model\nitself, preventing the acquisition of knowledge from samples that are either\nall incorrect or all correct. In this paper, we introduce Group Contrastive\nPolicy Optimization (GCPO), a method that incorporates external standard\nreference answers. When the model cannot solve a problem, the reference answer\nsupplies the correct response, steering the model toward an unequivocally\naccurate update direction. This approach offers two main advantages: (1) it\nimproves training efficiency by fully utilizing every sample; (2) it enables\nthe model to emulate the problem solving strategy of the reference answer\nduring training, thereby enhancing generalization in reasoning. GCPO achieves\noutstanding results across multiple benchmark datasets, yielding substantial\nimprovements over the baseline model. Our code is available at:\nhttps://github.com/AchoWu/GCPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86Group Contrastive Policy Optimization (GCPO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u6807\u51c6\u53c2\u8003\u7b54\u6848\u6765\u89e3\u51b3GRPO\u7b97\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982GRPO\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff1a\u6a21\u578brollout\u54cd\u5e94\u7684\u4e0a\u9650\u5b8c\u5168\u7531\u6a21\u578b\u81ea\u8eab\u51b3\u5b9a\uff0c\u65e0\u6cd5\u4ece\u5168\u9519\u6216\u5168\u5bf9\u6837\u672c\u4e2d\u83b7\u53d6\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u5916\u90e8\u53c2\u8003\u7b54\u6848\u6765\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "GCPO\u65b9\u6cd5\u5f15\u5165\u5916\u90e8\u6807\u51c6\u53c2\u8003\u7b54\u6848\uff0c\u5f53\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898\u65f6\uff0c\u53c2\u8003\u7b54\u6848\u63d0\u4f9b\u6b63\u786e\u54cd\u5e94\uff0c\u5f15\u5bfc\u6a21\u578b\u5411\u660e\u786e\u6b63\u786e\u7684\u66f4\u65b0\u65b9\u5411\u5b66\u4e60\u3002\u8fd9\u79cd\u65b9\u6cd5\u5145\u5206\u5229\u7528\u6bcf\u4e2a\u6837\u672c\uff0c\u5e76\u8ba9\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u6a21\u4eff\u53c2\u8003\u7b54\u6848\u7684\u89e3\u9898\u7b56\u7565\u3002", "result": "GCPO\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "GCPO\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u53c2\u8003\u7b54\u6848\u6709\u6548\u89e3\u51b3\u4e86GRPO\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.07830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07830", "abs": "https://arxiv.org/abs/2510.07830", "authors": ["Houqiang Zhong", "Zhenglong Wu", "Sihua Fu", "Zihan Zheng", "Xin Jin", "Xiaoyun Zhang", "Li Song", "Qiang Hu"], "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic\nrendering in compact scenes, but scaling to large urban environments introduces\nsevere aliasing artifacts and optimization instability, especially under\nhigh-resolution (e.g., 4K) rendering. These artifacts, manifesting as\nflickering textures and jagged edges, arise from the mismatch between Gaussian\nprimitives and the multi-scale nature of urban geometry. While existing\n``divide-and-conquer'' pipelines address scalability, they fail to resolve this\nfidelity gap. In this paper, we propose PrismGS, a physically-grounded\nregularization framework that improves the intrinsic rendering behavior of 3D\nGaussians. PrismGS integrates two synergistic regularizers. The first is\npyramidal multi-scale supervision, which enforces consistency by supervising\nthe rendering against a pre-filtered image pyramid. This compels the model to\nlearn an inherently anti-aliased representation that remains coherent across\ndifferent viewing scales, directly mitigating flickering textures. This is\ncomplemented by an explicit size regularization that imposes a\nphysically-grounded lower bound on the dimensions of the 3D Gaussians. This\nprevents the formation of degenerate, view-dependent primitives, leading to\nmore stable and plausible geometric surfaces and reducing jagged edges. Our\nmethod is plug-and-play and compatible with existing pipelines. Extensive\nexperiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS\nachieves state-of-the-art performance, yielding significant PSNR gains around\n1.5 dB against CityGaussian, while maintaining its superior quality and\nrobustness under demanding 4K rendering.", "AI": {"tldr": "PrismGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u76843D\u9ad8\u65af\u6cfc\u6e85\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91d1\u5b57\u5854\u591a\u5c3a\u5ea6\u76d1\u7763\u548c\u663e\u5f0f\u5c3a\u5bf8\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u5927\u573a\u666f\u6e32\u67d3\u4e2d\u7684\u952f\u9f7f\u4f2a\u5f71\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5927\u578b\u57ce\u5e02\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u952f\u9f7f\u4f2a\u5f71\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u7279\u522b\u662f\u57284K\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u4e0b\uff0c\u8868\u73b0\u4e3a\u95ea\u70c1\u7eb9\u7406\u548c\u952f\u9f7f\u8fb9\u7f18\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u4fdd\u771f\u5ea6\u5dee\u8ddd\u3002", "method": "PrismGS\u96c6\u6210\u4e86\u4e24\u4e2a\u534f\u540c\u6b63\u5219\u5316\u5668\uff1a\u91d1\u5b57\u5854\u591a\u5c3a\u5ea6\u76d1\u7763\uff08\u901a\u8fc7\u9884\u6ee4\u6ce2\u56fe\u50cf\u91d1\u5b57\u5854\u5f3a\u5236\u6e32\u67d3\u4e00\u81f4\u6027\uff09\u548c\u663e\u5f0f\u5c3a\u5bf8\u6b63\u5219\u5316\uff08\u5bf93D\u9ad8\u65af\u5c3a\u5bf8\u65bd\u52a0\u7269\u7406\u57fa\u7840\u7684\u4e0b\u754c\u7ea6\u675f\uff09\u3002", "result": "\u5728MatrixCity\u3001Mill-19\u548cUrbanScene3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPrismGS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4CityGaussian\u83b7\u5f97\u4e86\u7ea61.5 dB\u7684PSNR\u589e\u76ca\uff0c\u57284K\u6e32\u67d3\u4e0b\u4fdd\u6301\u4e86\u4f18\u8d8a\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PrismGS\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u6539\u55843D\u9ad8\u65af\u7684\u56fa\u6709\u6e32\u67d3\u884c\u4e3a\uff0c\u6709\u6548\u7f13\u89e3\u95ea\u70c1\u7eb9\u7406\u548c\u952f\u9f7f\u8fb9\u7f18\u95ee\u9898\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u7ba1\u9053\u517c\u5bb9\u3002"}}
{"id": "2510.08571", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08571", "abs": "https://arxiv.org/abs/2510.08571", "authors": ["Animikh Aich", "Adwait Kulkarni", "Eshed Ohn-Bar"], "title": "Scalable Offline Metrics for Autonomous Driving", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "summary": "Real-World evaluation of perception-based planning models for robotic\nsystems, such as autonomous vehicles, can be safely and inexpensively conducted\noffline, i.e., by computing model prediction error over a pre-collected\nvalidation dataset with ground-truth annotations. However, extrapolating from\noffline model performance to online settings remains a challenge. In these\nsettings, seemingly minor errors can compound and result in test-time\ninfractions or collisions. This relationship is understudied, particularly\nacross diverse closed-loop metrics and complex urban maneuvers. In this work,\nwe revisit this undervalued question in policy evaluation through an extensive\nset of experiments across diverse conditions and metrics. Based on analysis in\nsimulation, we find an even worse correlation between offline and online\nsettings than reported by prior studies, casting doubts on the validity of\ncurrent evaluation practices and metrics for driving policies. Next, we bridge\nthe gap between offline and online evaluation. We investigate an offline metric\nbased on epistemic uncertainty, which aims to capture events that are likely to\ncause errors in closed-loop settings. The resulting metric achieves over 13%\nimprovement in correlation compared to previous offline metrics. We further\nvalidate the generalization of our findings beyond the simulation environment\nin real-world settings, where even greater gains are observed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u611f\u77e5\u89c4\u5212\u6a21\u578b\u5728\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\u4e0e\u5728\u7ebf\u6027\u80fd\u76f8\u5173\u6027\u8f83\u5dee\uff0c\u63d0\u51fa\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7b49\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u611f\u77e5\u89c4\u5212\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u79bb\u7ebf\u8bc4\u4f30\u9a8c\u8bc1\uff0c\u4f46\u79bb\u7ebf\u6027\u80fd\u4e0e\u5728\u7ebf\u5b9e\u9645\u8868\u73b0\u4e4b\u95f4\u7684\u5173\u8054\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u7cfb\u7edf\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u5b9e\u9a8c\u5206\u6790\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u76f8\u5173\u6027\u6bd4\u5148\u524d\u7814\u7a76\u62a5\u9053\u7684\u66f4\u5dee\uff0c\u63d0\u51fa\u7684\u65b0\u79bb\u7ebf\u6307\u6807\u76f8\u6bd4\u73b0\u6709\u6307\u6807\u76f8\u5173\u6027\u63d0\u5347\u8d85\u8fc713%\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "\u5f53\u524d\u9a7e\u9a76\u7b56\u7565\u8bc4\u4f30\u5b9e\u8df5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\u80fd\u66f4\u597d\u5730\u6865\u63a5\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u7684\u5dee\u8ddd\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u653f\u7b56\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.07648", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07648", "abs": "https://arxiv.org/abs/2510.07648", "authors": ["Md Hasibul Amin", "Tamzid Tanvi Alam"], "title": "Continual Learning for Adaptive AI Systems", "comment": "5 pages 2 figures 2 tables", "summary": "Continual learning the ability of a neural network to learn multiple\nsequential tasks without losing previously acquired knowledge remains a\nsignificant obstacle to developing truly adaptive artificial intelligence. Deep\nlearning models have achieved remarkable results in various applications, but\noverfitting remains a common issue. Regularization techniques can help prevent\noverfitting by adding constraints to the model's parameters. To prevent\ncatastrophic forgetting, in this paper we introduce a novel regularization\ntechnique based on inter-cluster separation (ICS) in the loss function, which\npenalizes the model for producing outputs that are far away from the centroids\nof the clusters formed by the data from previous tasks. We also performed\nhyperparameter tuning to find the optimal weighting of the proposed\nregularization term. This ensures clearer separation between tasks in the\nneural network's internal representation, reducing overlap and mitigating\nforgetting. Using the standard 5-task Split CIFAR-10 benchmark and a ResNet-18\narchitecture, we demonstrate ICS's effectiveness in maintaining strong\nperformance on initial tasks. However, our results also highlight limitations\nin long-term knowledge retention, particularly when the number of tasks\nincreases. This underscores the complexity and trade-offs inherent in continual\nlearning and points toward avenues for further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c07\u95f4\u5206\u79bb(ICS)\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u635f\u5931\u51fd\u6570\u4e2d\u60e9\u7f5a\u8fdc\u79bb\u5148\u524d\u4efb\u52a1\u7c07\u4e2d\u5fc3\u7684\u8f93\u51fa\u6765\u6539\u5584\u4efb\u52a1\u5206\u79bb\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u662f\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u4e22\u5931\u5148\u524d\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u591a\u4e2a\u987a\u5e8f\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u771f\u6b63\u81ea\u9002\u5e94\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u7c07\u95f4\u5206\u79bb(ICS)\u7684\u6b63\u5219\u5316\u6280\u672f\uff0c\u5728\u635f\u5931\u51fd\u6570\u4e2d\u60e9\u7f5a\u6a21\u578b\u4ea7\u751f\u8fdc\u79bb\u5148\u524d\u4efb\u52a1\u6570\u636e\u5f62\u6210\u7684\u7c07\u4e2d\u5fc3\u7684\u8f93\u51fa\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u4ee5\u627e\u5230\u6700\u4f73\u6b63\u5219\u5316\u6743\u91cd\u3002", "result": "\u5728\u6807\u51c65\u4efb\u52a1Split CIFAR-10\u57fa\u51c6\u6d4b\u8bd5\u548cResNet-18\u67b6\u6784\u4e0a\uff0cICS\u5728\u4fdd\u6301\u521d\u59cb\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u4f46\u5728\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5f53\u4efb\u52a1\u6570\u91cf\u589e\u52a0\u65f6\u3002", "conclusion": "ICS\u65b9\u6cd5\u5728\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u6709\u6548\uff0c\u4f46\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u590d\u6742\u6027\u548c\u6743\u8861\u4ecd\u7136\u5b58\u5728\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.07775", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07775", "abs": "https://arxiv.org/abs/2510.07775", "authors": ["Omar Mahmoud", "Ali Khalil", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs", "comment": null, "summary": "Hallucination in large language models (LLMs) has been widely studied in\nrecent years, with progress in both detection and mitigation aimed at improving\ntruthfulness. Yet, a critical side effect remains largely overlooked: enhancing\ntruthfulness can negatively impact safety alignment. In this paper, we\ninvestigate this trade-off and show that increasing factual accuracy often\ncomes at the cost of weakened refusal behavior. Our analysis reveals that this\narises from overlapping components in the model that simultaneously encode\nhallucination and refusal information, leading alignment methods to suppress\nfactual knowledge unintentionally. We further examine how fine-tuning on benign\ndatasets, even when curated for safety, can degrade alignment for the same\nreason. To address this, we propose a method that disentangles refusal-related\nfeatures from hallucination features using sparse autoencoders, and preserves\nrefusal behavior during fine-tuning through subspace orthogonalization. This\napproach prevents hallucinations from increasing while maintaining safety\nalignment.We evaluate our method on commonsense reasoning tasks and harmful\nbenchmarks (AdvBench and StrongReject). Results demonstrate that our approach\npreserves refusal behavior and task utility, mitigating the trade-off between\ntruthfulness and safety.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53d1\u73b0\u63d0\u5347LLMs\u771f\u5b9e\u6027\u4f1a\u524a\u5f31\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u63d0\u51fa\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u79bb\u62d2\u7edd\u76f8\u5173\u7279\u5f81\u4e0e\u5e7b\u89c9\u7279\u5f81\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u771f\u5b9e\u6027\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u7814\u7a76\u4e2d\uff0c\u589e\u5f3a\u771f\u5b9e\u6027\u5f80\u5f80\u4ee5\u727a\u7272\u5b89\u5168\u5bf9\u9f50\u4e3a\u4ee3\u4ef7\uff0c\u8fd9\u79cd\u6743\u8861\u5173\u7cfb\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u79bb\u62d2\u7edd\u76f8\u5173\u7279\u5f81\u4e0e\u5e7b\u89c9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5b50\u7a7a\u95f4\u6b63\u4ea4\u5316\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u62d2\u7edd\u884c\u4e3a\u3002", "result": "\u5728\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u548c\u6709\u5bb3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u4fdd\u6301\u4e86\u62d2\u7edd\u884c\u4e3a\u548c\u4efb\u52a1\u6548\u7528\uff0c\u7f13\u89e3\u4e86\u771f\u5b9e\u6027\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u7279\u5f81\u5206\u79bb\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u5e7b\u89c9\u7684\u60c5\u51b5\u4e0b\u7ef4\u6301\u5b89\u5168\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u6027\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002"}}
{"id": "2510.08562", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08562", "abs": "https://arxiv.org/abs/2510.08562", "authors": ["Zhiyu Zheng", "Shaoyu Chen", "Haoran Yin", "Xinbang Zhang", "Jialv Zou", "Xinggang Wang", "Qian Zhang", "Lefei Zhang"], "title": "ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving (E2EAD) systems, which learn to predict future\ntrajectories directly from sensor data, are fundamentally challenged by the\ninherent spatio-temporal imbalance of trajectory data. This imbalance creates a\nsignificant optimization burden, causing models to learn spurious correlations\ninstead of causal inference, while also prioritizing uncertain, distant\npredictions, thereby compromising immediate safety. To address these issues, we\npropose ResAD, a novel Normalized Residual Trajectory Modeling framework.\nInstead of predicting the future trajectory directly, our approach reframes the\nlearning task to predict the residual deviation from a deterministic inertial\nreference. The inertial reference serves as a counterfactual, forcing the model\nto move beyond simple pattern recognition and instead identify the underlying\ncausal factors (e.g., traffic rules, obstacles) that necessitate deviations\nfrom a default, inertially-guided path. To deal with the optimization imbalance\ncaused by uncertain, long-term horizons, ResAD further incorporates Point-wise\nNormalization of the predicted residual. It re-weights the optimization\nobjective, preventing large-magnitude errors associated with distant, uncertain\nwaypoints from dominating the learning signal. Extensive experiments validate\nthe effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a\nstate-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two\ndenoising steps, demonstrating that our approach significantly simplifies the\nlearning task and improves model performance. The code will be released to\nfacilitate further research.", "AI": {"tldr": "\u63d0\u51faResAD\u6846\u67b6\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u6570\u636e\u65f6\u7a7a\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0e\u60ef\u6027\u53c2\u8003\u7684\u6b8b\u5dee\u504f\u5dee\u6765\u6539\u8fdb\u6a21\u578b\u6027\u80fd", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u8f68\u8ff9\u6570\u636e\u56fa\u6709\u7684\u65f6\u7a7a\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8fd9\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u56e0\u679c\u63a8\u7406\uff0c\u540c\u65f6\u4f18\u5148\u8003\u8651\u4e0d\u786e\u5b9a\u7684\u8fdc\u8ddd\u79bb\u9884\u6d4b\u800c\u635f\u5bb3\u5373\u65f6\u5b89\u5168\u6027", "method": "ResAD\u6846\u67b6\u91c7\u7528\u5f52\u4e00\u5316\u6b8b\u5dee\u8f68\u8ff9\u5efa\u6a21\uff0c\u9884\u6d4b\u4e0e\u786e\u5b9a\u6027\u60ef\u6027\u53c2\u8003\u7684\u6b8b\u5dee\u504f\u5dee\uff0c\u5e76\u4f7f\u7528\u9010\u70b9\u5f52\u4e00\u5316\u5904\u7406\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cResAD\u4f7f\u7528\u4ec5\u6709\u4e24\u4e2a\u53bb\u566a\u6b65\u9aa4\u7684\u666e\u901a\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u4e8688.6\u7684\u6700\u4f18PDMS\u5206\u6570", "conclusion": "ResAD\u65b9\u6cd5\u663e\u8457\u7b80\u5316\u4e86\u5b66\u4e60\u4efb\u52a1\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u5b66\u4e60\u76ee\u6807\u4e3a\u9884\u6d4b\u4e0e\u60ef\u6027\u53c2\u8003\u7684\u6b8b\u5dee\u504f\u5dee\u6765\u89e3\u51b3\u8f68\u8ff9\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898"}}
{"id": "2510.07735", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07735", "abs": "https://arxiv.org/abs/2510.07735", "authors": ["Rongchao Xu", "Kunlin Cai", "Lin Jiang", "Dahai Yu", "Zhiqing Hong", "Yuan Tian", "Guang Wang"], "title": "GeoGen: A Two-stage Coarse-to-Fine Framework for Fine-grained Synthetic Location-based Social Network Trajectory Generation", "comment": null, "summary": "Location-Based Social Network (LBSN) check-in trajectory data are important\nfor many practical applications, like POI recommendation, advertising, and\npandemic intervention. However, the high collection costs and ever-increasing\nprivacy concerns prevent us from accessing large-scale LBSN trajectory data.\nThe recent advances in synthetic data generation provide us with a new\nopportunity to achieve this, which utilizes generative AI to generate synthetic\ndata that preserves the characteristics of real data while ensuring privacy\nprotection. However, generating synthetic LBSN check-in trajectories remains\nchallenging due to their spatially discrete, temporally irregular nature and\nthe complex spatio-temporal patterns caused by sparse activities and uncertain\nhuman mobility. To address this challenge, we propose GeoGen, a two-stage\ncoarse-to-fine framework for large-scale LBSN check-in trajectory generation.\nIn the first stage, we reconstruct spatially continuous, temporally regular\nlatent movement sequences from the original LBSN check-in trajectories and then\ndesign a Sparsity-aware Spatio-temporal Diffusion model (S$^2$TDiff) with an\nefficient denosing network to learn their underlying behavioral patterns. In\nthe second stage, we design Coarse2FineNet, a Transformer-based Seq2Seq\narchitecture equipped with a dynamic context fusion mechanism in the encoder\nand a multi-task hybrid-head decoder, which generates fine-grained LBSN\ntrajectories based on coarse-grained latent movement sequences by modeling\nsemantic relevance and behavioral uncertainty. Extensive experiments on four\nreal-world datasets show that GeoGen excels state-of-the-art models for both\nfidelity and utility evaluation, e.g., it increases over 69% and 55% in\ndistance and radius metrics on the FS-TKY dataset.", "AI": {"tldr": "GeoGen\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7c97\u5230\u7ec6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u7684\u57fa\u4e8e\u4f4d\u7f6e\u7684\u793e\u4ea4\u7f51\u7edc\u7b7e\u5230\u8f68\u8ff9\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7a7a\u95f4\u79bb\u6563\u3001\u65f6\u95f4\u4e0d\u89c4\u5219\u8f68\u8ff9\u65f6\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u9ad8\u6536\u96c6\u6210\u672c\u548c\u65e5\u76ca\u589e\u957f\u7684\u9690\u79c1\u62c5\u5fe7\uff0c\u83b7\u53d6\u5927\u89c4\u6a21LBSN\u8f68\u8ff9\u6570\u636e\u53d8\u5f97\u56f0\u96be\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406LBSN\u7b7e\u5230\u8f68\u8ff9\u7684\u7a7a\u95f4\u79bb\u6563\u3001\u65f6\u95f4\u4e0d\u89c4\u5219\u7279\u6027\u4ee5\u53ca\u590d\u6742\u65f6\u7a7a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u7a00\u758f\u611f\u77e5\u65f6\u7a7a\u6269\u6563\u6a21\u578b(S\u00b2TDiff)\u4ece\u539f\u59cb\u8f68\u8ff9\u91cd\u5efa\u8fde\u7eed\u89c4\u5219\u7684\u8fd0\u52a8\u5e8f\u5217\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528Transformer-based Seq2Seq\u67b6\u6784Coarse2FineNet\uff0c\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u878d\u5408\u548c\u591a\u4efb\u52a1\u6df7\u5408\u5934\u89e3\u7801\u5668\u751f\u6210\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGeoGen\u5728\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u8bc4\u4f30\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728FS-TKY\u6570\u636e\u96c6\u4e0a\u8ddd\u79bb\u548c\u534a\u5f84\u6307\u6807\u5206\u522b\u63d0\u9ad8\u4e8669%\u548c55%\u4ee5\u4e0a\u3002", "conclusion": "GeoGen\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684LBSN\u7b7e\u5230\u8f68\u8ff9\u6570\u636e\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u548c\u5b9e\u7528\u6027\uff0c\u4e3aPOI\u63a8\u8350\u3001\u5e7f\u544a\u6295\u653e\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07990", "abs": "https://arxiv.org/abs/2510.07990", "authors": ["Gaurvi Goyal", "Pham Cong Thuong", "Arren Glover", "Masayoshi Mizuno", "Chiara Bartolozzi"], "title": "GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network", "comment": null, "summary": "Human Pose Estimation is a crucial module in human-machine interaction\napplications and, especially since the rise in deep learning technology, robust\nmethods are available to consumers using RGB cameras and commercial GPUs. On\nthe other hand, event-based cameras have gained popularity in the vision\nresearch community for their low latency and low energy advantages that make\nthem ideal for applications where those resources are constrained like portable\nelectronics and mobile robots. In this work we propose a Graph Neural Network,\nGraphEnet, that leverages the sparse nature of event camera output, with an\nintermediate line based event representation, to estimate 2D Human Pose of a\nsingle person at a high frequency. The architecture incorporates a novel offset\nvector learning paradigm with confidence based pooling to estimate the human\npose. This is the first work that applies Graph Neural Networks to event data\nfor Human Pose Estimation. The code is open-source at\nhttps://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.", "AI": {"tldr": "\u63d0\u51faGraphEnet\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u76842D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\uff0c\u5229\u7528\u5176\u7a00\u758f\u7279\u6027\u5b9e\u73b0\u9ad8\u9891\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u80fd\u8017\u4f18\u52bf\uff0c\u9002\u5408\u4fbf\u643a\u8bbe\u5907\u548c\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u7a00\u758f\u7279\u6027\u8fdb\u884c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u8f93\u51fa\uff0c\u91c7\u7528\u57fa\u4e8e\u7ebf\u7684\u4e2d\u95f4\u4e8b\u4ef6\u8868\u793a\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u504f\u79fb\u5411\u91cf\u5b66\u4e60\u8303\u5f0f\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6c60\u5316\u65b9\u6cd5\u3002", "result": "\u8fd9\u662f\u9996\u4e2a\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9891\u5355\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "GraphEnet\u6210\u529f\u5c55\u793a\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u4e8b\u4ef6\u76f8\u673a\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08003", "abs": "https://arxiv.org/abs/2510.08003", "authors": ["Weihuang Lin", "Yiwei Ma", "Jiayi Ji", "Xiaoshuai Sun", "Rongrong Ji"], "title": "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning", "comment": null, "summary": "Composed Image Retrieval (CIR), which aims to find a target image from a\nreference image and a modification text, presents the core challenge of\nperforming unified reasoning across visual and semantic modalities. While\ncurrent approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more\nrecent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown\nprogress, they predominantly function as ``black boxes.\" This inherent opacity\nnot only prevents users from understanding the retrieval rationale but also\nrestricts the models' ability to follow complex, fine-grained instructions. To\novercome these limitations, we introduce CIR-CoT, the first end-to-end\nretrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)\nreasoning. By compelling the model to first generate an interpretable reasoning\nchain, CIR-CoT enhances its ability to capture crucial cross-modal\ninteractions, leading to more accurate retrieval while making its decision\nprocess transparent. Since existing datasets like FashionIQ and CIRR lack the\nnecessary reasoning data, a key contribution of our work is the creation of\nstructured CoT annotations using a three-stage process involving a caption,\nreasoning, and conclusion. Our model is then fine-tuned to produce this\nstructured output before encoding its final retrieval intent into a dedicated\nembedding. Comprehensive experiments show that CIR-CoT achieves highly\ncompetitive performance on in-domain datasets (FashionIQ, CIRR) and\ndemonstrates remarkable generalization on the out-of-domain CIRCO dataset,\nestablishing a new path toward more effective and trustworthy retrieval\nsystems.", "AI": {"tldr": "CIR-CoT\u662f\u9996\u4e2a\u96c6\u6210\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u7aef\u5230\u7aef\u68c0\u7d22\u5bfc\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\u6765\u63d0\u9ad8\u8de8\u6a21\u6001\u4ea4\u4e92\u7406\u89e3\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u900f\u660e\u7684\u56fe\u50cf\u68c0\u7d22\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u4f5c\u4e3a\"\u9ed1\u76d2\"\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u7ec6\u7c92\u5ea6\u6307\u4ee4\u3002", "method": "\u63d0\u51faCIR-CoT\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8fc7\u7a0b\u751f\u6210\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u6ce8\u91ca\uff08\u63cf\u8ff0\u3001\u63a8\u7406\u3001\u7ed3\u8bba\uff09\uff0c\u7136\u540e\u5fae\u8c03\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u6700\u540e\u5c06\u68c0\u7d22\u610f\u56fe\u7f16\u7801\u5230\u4e13\u7528\u5d4c\u5165\u4e2d\u3002", "result": "\u5728\u9886\u57df\u5185\u6570\u636e\u96c6\uff08FashionIQ\u3001CIRR\uff09\u4e0a\u83b7\u5f97\u9ad8\u5ea6\u7ade\u4e89\u529b\u8868\u73b0\uff0c\u5728\u9886\u57df\u5916CIRCO\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CIR-CoT\u4e3a\u6784\u5efa\u66f4\u6709\u6548\u548c\u53ef\u4fe1\u8d56\u7684\u68c0\u7d22\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u94fe\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2510.08114", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08114", "abs": "https://arxiv.org/abs/2510.08114", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Amirhossein Farshi Sotoudeh"], "title": "Can Risk-taking AI-Assistants suitably represent entities", "comment": null, "summary": "Responsible AI demands systems whose behavioral tendencies can be effectively\nmeasured, audited, and adjusted to prevent inadvertently nudging users toward\nrisky decisions or embedding hidden biases in risk aversion. As language models\n(LMs) are increasingly incorporated into AI-driven decision support systems,\nunderstanding their risk behaviors is crucial for their responsible deployment.\nThis study investigates the manipulability of risk aversion (MoRA) in LMs,\nexamining their ability to replicate human risk preferences across diverse\neconomic scenarios, with a focus on gender-specific attitudes, uncertainty,\nrole-based decision-making, and the manipulability of risk aversion. The\nresults indicate that while LMs such as DeepSeek Reasoner and\nGemini-2.0-flash-lite exhibit some alignment with human behaviors, notable\ndiscrepancies highlight the need to refine bio-centric measures of\nmanipulability. These findings suggest directions for refining AI design to\nbetter align human and AI risk preferences and enhance ethical decision-making.\nThe study calls for further advancements in model design to ensure that AI\nsystems more accurately replicate human risk preferences, thereby improving\ntheir effectiveness in risk management contexts. This approach could enhance\nthe applicability of AI assistants in managing risk.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u8bed\u8a00\u6a21\u578b\u5728\u98ce\u9669\u538c\u6076\u53ef\u64cd\u7eb5\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u867d\u7136\u4e00\u4e9b\u6a21\u578b\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6709\u4e00\u5b9a\u5bf9\u9f50\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u6539\u8fdb\u751f\u7269\u4e2d\u5fc3\u7684\u53ef\u64cd\u7eb5\u6027\u6d4b\u91cf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230AI\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u98ce\u9669\u884c\u4e3a\u5bf9\u4e8e\u8d1f\u8d23\u4efb\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u9632\u6b62\u7cfb\u7edf\u65e0\u610f\u4e2d\u5f15\u5bfc\u7528\u6237\u505a\u51fa\u98ce\u9669\u51b3\u7b56\u6216\u5d4c\u5165\u9690\u85cf\u504f\u89c1\u3002", "method": "\u7814\u7a76\u8003\u5bdf\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7ecf\u6d4e\u573a\u666f\u4e0b\u590d\u5236\u4eba\u7c7b\u98ce\u9669\u504f\u597d\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u6027\u522b\u7279\u5b9a\u6001\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u57fa\u4e8e\u89d2\u8272\u7684\u51b3\u7b56\u5236\u5b9a\u4ee5\u53ca\u98ce\u9669\u538c\u6076\u7684\u53ef\u64cd\u7eb5\u6027\u3002", "result": "DeepSeek Reasoner\u548cGemini-2.0-flash-lite\u7b49\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6709\u4e00\u5b9a\u5bf9\u9f50\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7a81\u663e\u4e86\u6539\u8fdb\u751f\u7269\u4e2d\u5fc3\u53ef\u64cd\u7eb5\u6027\u6d4b\u91cf\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5728\u6a21\u578b\u8bbe\u8ba1\u65b9\u9762\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u786e\u4fddAI\u7cfb\u7edf\u66f4\u51c6\u786e\u5730\u590d\u5236\u4eba\u7c7b\u98ce\u9669\u504f\u597d\uff0c\u4ece\u800c\u63d0\u9ad8\u5728\u98ce\u9669\u7ba1\u7406\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u589e\u5f3aAI\u52a9\u624b\u5728\u98ce\u9669\u7ba1\u7406\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.07835", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07835", "abs": "https://arxiv.org/abs/2510.07835", "authors": ["Weisen Jiang", "Sinno Jialin Pan"], "title": "MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation", "comment": "Accepted By NeurIPS 2025", "summary": "This paper introduces MetaDefense, a novel framework for defending against\nfinetuning-based jailbreak attacks in large language models (LLMs). We observe\nthat existing defense mechanisms fail to generalize to harmful queries\ndisguised by unseen attack templates, despite LLMs being capable of\ndistinguishing disguised harmful queries in the embedding space. Based on these\ninsights, we propose a two-stage defense approach: (i) pre-generation defense\nthat detects harmful queries before response generation begins, and (ii)\nmid-generation defense that monitors partial responses during generation to\nprevent outputting more harmful content. Our MetaDefense trains the LLM to\npredict the harmfulness of both queries and partial responses using specialized\nprompts, enabling early termination of potentially harmful interactions.\nExtensive experiments across multiple LLM architectures (LLaMA-2-7B,\nQwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense\nsignificantly outperforms existing defense mechanisms, achieving robust defense\nagainst harmful queries with seen and unseen attack templates while maintaining\ncompetitive performance on benign tasks. Code is available at\nhttps://github.com/ws-jiang/MetaDefense.", "AI": {"tldr": "MetaDefense\u662f\u4e00\u4e2a\u9632\u5fa1LLM\u5fae\u8c03\u8d8a\u72f1\u653b\u51fb\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9632\u5fa1\u65b9\u6cd5\u5728\u751f\u6210\u524d\u548c\u751f\u6210\u4e2d\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u673a\u5236\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u653b\u51fb\u6a21\u677f\u4f2a\u88c5\u7684\u6709\u5bb3\u67e5\u8be2\uff0c\u5c3d\u7ba1LLM\u5728\u5d4c\u5165\u7a7a\u95f4\u80fd\u591f\u533a\u5206\u8fd9\u4e9b\u67e5\u8be2\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u9632\u5fa1\uff1a\u751f\u6210\u524d\u9632\u5fa1\u68c0\u6d4b\u6709\u5bb3\u67e5\u8be2\uff0c\u751f\u6210\u4e2d\u9632\u5fa1\u76d1\u63a7\u90e8\u5206\u54cd\u5e94\uff1b\u4f7f\u7528\u4e13\u95e8\u63d0\u793a\u8bad\u7ec3LLM\u9884\u6d4b\u67e5\u8be2\u548c\u90e8\u5206\u54cd\u5e94\u7684\u6709\u5bb3\u6027\u3002", "result": "\u5728\u591a\u4e2aLLM\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMetaDefense\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u673a\u5236\uff0c\u5bf9\u53ef\u89c1\u548c\u672a\u89c1\u653b\u51fb\u6a21\u677f\u7684\u6709\u5bb3\u67e5\u8be2\u90fd\u80fd\u5b9e\u73b0\u9c81\u68d2\u9632\u5fa1\uff0c\u540c\u65f6\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "MetaDefense\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u65e9\u671f\u7ec8\u6b62\u6f5c\u5728\u6709\u5bb3\u4ea4\u4e92\uff0c\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u3002"}}
{"id": "2510.08263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08263", "abs": "https://arxiv.org/abs/2510.08263", "authors": ["Shunyu An", "Miao Wang", "Yongchao Li", "Dong Wan", "Lina Wang", "Ling Qin", "Liqin Gao", "Congyao Fan", "Zhiyong Mao", "Jiange Pu", "Wenji Xia", "Dong Zhao", "Rui Hu", "Ji Lu", "Guiyue Zhou", "Baoyu Tang", "Yanqin Gao", "Yongsheng Du", "Daigang Xu", "Lingjun Huang", "Baoli Wang", "Xiwen Zhang", "Luyao Wang", "Shilong Liu"], "title": "Co-TAP: Three-Layer Agent Interaction Protocol Technical Report", "comment": null, "summary": "This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer\nagent interaction protocol designed to address the challenges faced by\nmulti-agent systems across the three core dimensions of Interoperability,\nInteraction and Collaboration, and Knowledge Sharing. We have designed and\nproposed a layered solution composed of three core protocols: the Human-Agent\nInteraction Protocol (HAI), the Unified Agent Protocol (UAP), and the\nMemory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction\nlayer, standardizing the flow of information between users, interfaces, and\nagents by defining a standardized, event-driven communication paradigm. This\nensures the real-time performance, reliability, and synergy of interactions. As\nthe core of the infrastructure layer, UAP is designed to break down\ncommunication barriers among heterogeneous agents through unified service\ndiscovery and protocol conversion mechanisms, thereby enabling seamless\ninterconnection and interoperability of the underlying network. MEK, in turn,\noperates at the cognitive layer. By establishing a standardized ''Memory (M) -\nExtraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the\nability to learn from individual experiences and form shareable knowledge,\nthereby laying the foundation for the realization of true collective\nintelligence. We believe this protocol framework will provide a solid\nengineering foundation and theoretical guidance for building the next\ngeneration of efficient, scalable, and intelligent multi-agent applications.", "AI": {"tldr": "Co-TAP\u662f\u4e00\u4e2a\u4e09\u5c42\u667a\u80fd\u4f53\u4ea4\u4e92\u534f\u8bae\uff0c\u901a\u8fc7HAI\u3001UAP\u548cMEK\u4e09\u4e2a\u6838\u5fc3\u534f\u8bae\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e92\u64cd\u4f5c\u6027\u3001\u4ea4\u4e92\u534f\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u9762\u4e34\u7684\u6311\u6218\uff1a\u4e92\u64cd\u4f5c\u6027\u3001\u4ea4\u4e92\u534f\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\uff0c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u667a\u80fd\u7684\u591a\u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u5c42\u534f\u8bae\uff1aHAI\uff08\u4eba\u673a\u4ea4\u4e92\u534f\u8bae\uff09\u6807\u51c6\u5316\u4fe1\u606f\u6d41\uff0cUAP\uff08\u7edf\u4e00\u667a\u80fd\u4f53\u534f\u8bae\uff09\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u4e92\u8054\uff0cMEK\uff08\u8bb0\u5fc6-\u63d0\u53d6-\u77e5\u8bc6\u534f\u8bae\uff09\u5efa\u7acb\u8ba4\u77e5\u94fe\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u534f\u8bae\u6846\u67b6\uff0c\u80fd\u591f\u786e\u4fdd\u5b9e\u65f6\u6027\u80fd\u3001\u53ef\u9760\u6027\u3001\u534f\u540c\u6027\uff0c\u6253\u7834\u901a\u4fe1\u969c\u788d\uff0c\u5b9e\u73b0\u5e95\u5c42\u7f51\u7edc\u65e0\u7f1d\u4e92\u8054\uff0c\u5e76\u652f\u6301\u96c6\u4f53\u667a\u80fd\u7684\u5b9e\u73b0\u3002", "conclusion": "Co-TAP\u534f\u8bae\u6846\u67b6\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u667a\u80fd\u7684\u591a\u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u5de5\u7a0b\u57fa\u7840\u548c\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2510.07974", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07974", "abs": "https://arxiv.org/abs/2510.07974", "authors": ["Jialu Du", "Guiyang Hou", "Yihui Fu", "Chen Wu", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu"], "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning", "comment": "15 pages, 10 figures", "summary": "While large language models (LLMs) excel in mathematical and code reasoning,\nwe observe they struggle with social reasoning tasks, exhibiting cognitive\nconfusion, logical inconsistencies, and conflation between objective world\nstates and subjective belief states. Through deteiled analysis of DeepSeek-R1's\nreasoning trajectories, we find that LLMs frequently encounter reasoning\nimpasses and tend to output contradictory terms like \"tricky\" and \"confused\"\nwhen processing scenarios with multiple participants and timelines, leading to\nerroneous reasoning or infinite loops. The core issue is their inability to\ndisentangle objective reality from agents' subjective beliefs. To address this,\nwe propose an adaptive world model-enhanced reasoning mechanism that constructs\na dynamic textual world model to track entity states and temporal sequences. It\ndynamically monitors reasoning trajectories for confusion indicators and\npromptly intervenes by providing clear world state descriptions, helping models\nnavigate through cognitive dilemmas. The mechanism mimics how humans use\nimplicit world models to distinguish between external events and internal\nbeliefs. Evaluations on three social benchmarks demonstrate significant\nimprovements in accuracy (e.g., +10% in Hi-ToM) while reducing computational\ncosts (up to 33.8% token reduction), offering a simple yet effective solution\nfor deploying LLMs in social contexts.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u4e16\u754c\u6a21\u578b\u589e\u5f3a\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u52a8\u6001\u6587\u672c\u4e16\u754c\u6a21\u578b\u6765\u8ddf\u8e2a\u5b9e\u4f53\u72b6\u6001\u548c\u65f6\u95f4\u5e8f\u5217\uff0c\u89e3\u51b3LLM\u5728\u793e\u4f1a\u63a8\u7406\u4efb\u52a1\u4e2d\u6df7\u6dc6\u5ba2\u89c2\u73b0\u5b9e\u4e0e\u4e3b\u89c2\u4fe1\u5ff5\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u793e\u4f1a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u8ba4\u77e5\u6df7\u6dc6\u3001\u903b\u8f91\u4e0d\u4e00\u81f4\u4ee5\u53ca\u5ba2\u89c2\u4e16\u754c\u72b6\u6001\u4e0e\u4e3b\u89c2\u4fe1\u5ff5\u72b6\u6001\u7684\u6df7\u6dc6\u95ee\u9898\u3002", "method": "\u6784\u5efa\u52a8\u6001\u6587\u672c\u4e16\u754c\u6a21\u578b\u6765\u8ddf\u8e2a\u5b9e\u4f53\u72b6\u6001\u548c\u65f6\u95f4\u5e8f\u5217\uff0c\u52a8\u6001\u76d1\u63a7\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6df7\u6dc6\u6307\u6807\uff0c\u53ca\u65f6\u63d0\u4f9b\u6e05\u6670\u7684\u4e16\u754c\u72b6\u6001\u63cf\u8ff0\u6765\u5e2e\u52a9\u6a21\u578b\u5e94\u5bf9\u8ba4\u77e5\u56f0\u5883\u3002", "result": "\u5728\u4e09\u4e2a\u793e\u4f1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u51c6\u786e\u7387\u63d0\u5347\uff08\u5982Hi-ToM\u4e2d+10%\uff09\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff08\u6700\u591a\u51cf\u5c1133.8%\u7684token\u4f7f\u7528\uff09\u3002", "conclusion": "\u8be5\u673a\u5236\u4e3a\u5728\u793e\u4ea4\u573a\u666f\u4e2d\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u4eff\u4eba\u7c7b\u4f7f\u7528\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u6765\u533a\u5206\u5916\u90e8\u4e8b\u4ef6\u548c\u5185\u90e8\u4fe1\u5ff5\u7684\u65b9\u5f0f\u3002"}}
{"id": "2510.07922", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.07922", "abs": "https://arxiv.org/abs/2510.07922", "authors": ["Murtaza Rangwala", "Farag Azzedin", "Richard O. Sinnott", "Rajkumar Buyya"], "title": "SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening", "comment": "23 pages, 5 figures, Code Available:\n  https://doi.org/10.5281/zenodo.17223405", "summary": "Decentralized Federated Learning (DFL) enables privacy-preserving\ncollaborative training without centralized servers, but remains vulnerable to\nByzantine attacks where malicious clients submit corrupted model updates.\nExisting Byzantine-robust DFL defenses rely on similarity-based neighbor\nscreening that requires every client to exchange and compare complete\nhigh-dimensional model vectors with all neighbors in each training round,\ncreating prohibitive communication and computational costs that prevent\ndeployment at web scale. We propose SketchGuard, a general framework that\ndecouples Byzantine filtering from model aggregation through sketch-based\nneighbor screening. SketchGuard compresses $d$-dimensional models to\n$k$-dimensional sketches ($k \\ll d$) using Count Sketch for similarity\ncomparisons, then selectively fetches full models only from accepted neighbors,\nreducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| +\nd|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \\le |N_i|$ is the\naccepted neighbor count. We establish rigorous convergence guarantees in both\nstrongly convex and non-convex settings, proving that Count Sketch compression\npreserves Byzantine resilience with controlled degradation bounds where\napproximation errors introduce only a $(1+O(\\epsilon))$ factor in the effective\nthreshold parameter. Comprehensive experiments across multiple datasets,\nnetwork topologies, and attack scenarios demonstrate that SketchGuard maintains\nidentical robustness to state-of-the-art methods while reducing computation\ntime by up to 82% and communication overhead by 50-70% depending on filtering\neffectiveness, with benefits scaling multiplicatively with model dimensionality\nand network connectivity. These results establish the viability of sketch-based\ncompression as a fundamental enabler of robust DFL at web scale.", "AI": {"tldr": "SketchGuard\u662f\u4e00\u4e2a\u901a\u8fc7\u8349\u56fe\u538b\u7f29\u6280\u672f\u964d\u4f4e\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u62dc\u5360\u5ead\u653b\u51fb\u9632\u5fa1\u6210\u672c\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u901a\u4fe1\u590d\u6742\u5ea6\u4eceO(d|N_i|)\u964d\u4f4e\u5230O(k|N_i| + d|S_i|)\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u62dc\u5360\u5ead\u9c81\u68d2DFL\u9632\u5fa1\u65b9\u6cd5\u9700\u8981\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5728\u6bcf\u8f6e\u8bad\u7ec3\u4e2d\u4e0e\u6240\u6709\u90bb\u5c45\u4ea4\u6362\u548c\u6bd4\u8f83\u5b8c\u6574\u7684\u9ad8\u7ef4\u6a21\u578b\u5411\u91cf\uff0c\u4ea7\u751f\u4e86\u8fc7\u9ad8\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u963b\u788d\u4e86\u5728Web\u89c4\u6a21\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u4f7f\u7528Count Sketch\u5c06d\u7ef4\u6a21\u578b\u538b\u7f29\u5230k\u7ef4\u8349\u56fe(k\u226ad)\u8fdb\u884c\u76f8\u4f3c\u6027\u6bd4\u8f83\uff0c\u7136\u540e\u4ec5\u4ece\u88ab\u63a5\u53d7\u7684\u90bb\u5c45\u5904\u9009\u62e9\u6027\u83b7\u53d6\u5b8c\u6574\u6a21\u578b\uff0c\u5c06\u62dc\u5360\u5ead\u8fc7\u6ee4\u4e0e\u6a21\u578b\u805a\u5408\u89e3\u8026\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u7f51\u7edc\u62d3\u6251\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSketchGuard\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u540c\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe82%\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1150-70%\u3002", "conclusion": "\u57fa\u4e8e\u8349\u56fe\u538b\u7f29\u7684\u6280\u672f\u662f\u5b9e\u73b0Web\u89c4\u6a21\u9c81\u68d2DFL\u7684\u57fa\u672c\u63a8\u52a8\u8005\uff0c\u5176\u4f18\u52bf\u968f\u7740\u6a21\u578b\u7ef4\u5ea6\u548c\u7f51\u7edc\u8fde\u63a5\u6027\u7684\u589e\u52a0\u800c\u500d\u589e\u3002"}}
{"id": "2510.08398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08398", "abs": "https://arxiv.org/abs/2510.08398", "authors": ["Zeqing Wang", "Xinyu Wei", "Bairui Li", "Zhen Guo", "Jinrui Zhang", "Hongyang Wei", "Keze Wang", "Lei Zhang"], "title": "VideoVerse: How Far is Your T2V Generator from a World Model?", "comment": "24 Pages, 8 Figures, 11 Tables", "summary": "The recent rapid advancement of Text-to-Video (T2V) generation technologies,\nwhich are critical to build ``world models'', makes the existing benchmarks\nincreasingly insufficient to evaluate state-of-the-art T2V models. First,\ncurrent evaluation dimensions, such as per-frame aesthetic quality and temporal\nconsistency, are no longer able to differentiate state-of-the-art T2V models.\nSecond, event-level temporal causality, which not only distinguishes video from\nother modalities but also constitutes a crucial component of world models, is\nseverely underexplored in existing benchmarks. Third, existing benchmarks lack\na systematic assessment of world knowledge, which are essential capabilities\nfor building world models. To address these issues, we introduce VideoVerse, a\ncomprehensive benchmark that focuses on evaluating whether a T2V model could\nunderstand complex temporal causality and world knowledge in the real world. We\ncollect representative videos across diverse domains (e.g., natural landscapes,\nsports, indoor scenes, science fiction, chemical and physical experiments) and\nextract their event-level descriptions with inherent temporal causality, which\nare then rewritten into text-to-video prompts by independent annotators. For\neach prompt, we design a suite of binary evaluation questions from the\nperspective of dynamic and static properties, with a total of ten carefully\ndefined evaluation dimensions. In total, our VideoVerse comprises 300 carefully\ncurated prompts, involving 815 events and 793 binary evaluation questions.\nConsequently, a human preference aligned QA-based evaluation pipeline is\ndeveloped by using modern vision-language models. Finally, we perform a\nsystematic evaluation of state-of-the-art open-source and closed-source T2V\nmodels on VideoVerse, providing in-depth analysis on how far the current T2V\ngenerators are from world models.", "AI": {"tldr": "VideoVerse\u662f\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30T2V\u6a21\u578b\u5bf9\u590d\u6742\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u4e8b\u4ef6\u7ea7\u65f6\u95f4\u56e0\u679c\u6027\u548c\u4e16\u754c\u77e5\u8bc6\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709T2V\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u5df2\u65e0\u6cd5\u533a\u5206\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u4e25\u91cd\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u7ea7\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u8bc4\u4f30\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e16\u754c\u77e5\u8bc6\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u800c\u8fd9\u4e9b\u80fd\u529b\u5bf9\u4e8e\u6784\u5efa\u4e16\u754c\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6536\u96c6\u8de8\u9886\u57df\u4ee3\u8868\u6027\u89c6\u9891\uff0c\u63d0\u53d6\u5177\u6709\u5185\u5728\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u4e8b\u4ef6\u7ea7\u63cf\u8ff0\uff0c\u7531\u72ec\u7acb\u6807\u6ce8\u8005\u91cd\u5199\u4e3a\u6587\u672c\u5230\u89c6\u9891\u63d0\u793a\uff0c\u8bbe\u8ba1\u5305\u542b10\u4e2a\u7ef4\u5ea6\u7684\u4e8c\u5143\u8bc4\u4f30\u95ee\u9898\uff0c\u5f00\u53d1\u57fa\u4e8e\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50QA\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "VideoVerse\u5305\u542b300\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u63d0\u793a\uff0c\u6d89\u53ca815\u4e2a\u4e8b\u4ef6\u548c793\u4e2a\u4e8c\u5143\u8bc4\u4f30\u95ee\u9898\uff0c\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90T2V\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30T2V\u6a21\u578b\u662f\u5426\u63a5\u8fd1\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u6df1\u5165\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524dT2V\u751f\u6210\u5668\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.08214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08214", "abs": "https://arxiv.org/abs/2510.08214", "authors": ["Qiang Yang", "Xiuying Chen", "Changsheng Ma", "Rui Yin", "Xin Gao", "Xiangliang Zhang"], "title": "SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets", "comment": "13 pages, 13 figures, 6 tables", "summary": "The global impact of the COVID-19 pandemic has highlighted the need for a\ncomprehensive understanding of public sentiment and reactions. Despite the\navailability of numerous public datasets on COVID-19, some reaching volumes of\nup to 100 billion data points, challenges persist regarding the availability of\nlabeled data and the presence of coarse-grained or inappropriate sentiment\nlabels. In this paper, we introduce SenWave, a novel fine-grained\nmulti-language sentiment analysis dataset specifically designed for analyzing\nCOVID-19 tweets, featuring ten sentiment categories across five languages. The\ndataset comprises 10,000 annotated tweets each in English and Arabic, along\nwith 30,000 translated tweets in Spanish, French, and Italian, derived from\nEnglish tweets. Additionally, it includes over 105 million unlabeled tweets\ncollected during various COVID-19 waves. To enable accurate fine-grained\nsentiment classification, we fine-tuned pre-trained transformer-based language\nmodels using the labeled tweets. Our study provides an in-depth analysis of the\nevolving emotional landscape across languages, countries, and topics, revealing\nsignificant insights over time. Furthermore, we assess the compatibility of our\ndataset with ChatGPT, demonstrating its robustness and versatility in various\napplications. Our dataset and accompanying code are publicly accessible on the\nrepository\\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that\nthis work will foster further exploration into fine-grained sentiment analysis\nfor complex events within the NLP community, promoting more nuanced\nunderstanding and research innovations.", "AI": {"tldr": "SenWave\u662f\u4e00\u4e2a\u9488\u5bf9COVID-19\u63a8\u6587\u7684\u591a\u8bed\u8a00\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e2a\u60c5\u611f\u7c7b\u522b\uff0c\u8986\u76d6\u82f1\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u548c\u610f\u5927\u5229\u8bed\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u9884\u8bad\u7ec3transformer\u6a21\u578b\u7684\u5206\u7c7b\u65b9\u6cd5\u548cChatGPT\u517c\u5bb9\u6027\u8bc4\u4f30\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u671f\u95f4\uff0c\u867d\u7136\u6709\u5927\u91cf\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4f46\u5b58\u5728\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u3001\u60c5\u611f\u6807\u7b7e\u7c92\u5ea6\u7c97\u6216\u4e0d\u6070\u5f53\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u60c5\u611f\u5206\u6790\u6765\u7406\u89e3\u516c\u4f17\u60c5\u7eea\u53d8\u5316\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10,000\u6761\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u6807\u6ce8\u63a8\u6587\u300130,000\u6761\u7ffb\u8bd1\u63a8\u6587\uff08\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u3001\u610f\u5927\u5229\u8bed\uff09\u4ee5\u53ca1.05\u4ebf\u6761\u672a\u6807\u6ce8\u63a8\u6587\u7684SenWave\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3transformer\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u7c7b\u3002", "result": "\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u60c5\u611f\u5206\u7c7b\uff0c\u6df1\u5165\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u8a00\u3001\u56fd\u5bb6\u548c\u4e3b\u9898\u7684\u60c5\u7eea\u6f14\u53d8\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u4e0eChatGPT\u7684\u517c\u5bb9\u6027\uff0c\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "SenWave\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u5c06\u4fc3\u8fdbNLP\u793e\u533a\u5bf9\u590d\u6742\u4e8b\u4ef6\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u7814\u7a76\uff0c\u63a8\u52a8\u66f4\u7ec6\u81f4\u7406\u89e3\u548c\u7814\u7a76\u521b\u65b0\u3002"}}
{"id": "2510.08177", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08177", "abs": "https://arxiv.org/abs/2510.08177", "authors": ["Jiaan Luo", "Feng Hong", "Qiang Hu", "Xiaofeng Cao", "Feng Liu", "Jiangchao Yao"], "title": "Long-tailed Recognition with Model Rebalancing", "comment": null, "summary": "Long-tailed recognition is ubiquitous and challenging in deep learning and\neven in the downstream finetuning of foundation models, since the skew class\ndistribution generally prevents the model generalization to the tail classes.\nDespite the promise of previous methods from the perspectives of data\naugmentation, loss rebalancing and decoupled training etc., consistent\nimprovement in the broad scenarios like multi-label long-tailed recognition is\ndifficult. In this study, we dive into the essential model capacity impact\nunder long-tailed context, and propose a novel framework, Model Rebalancing\n(MORE), which mitigates imbalance by directly rebalancing the model's parameter\nspace. Specifically, MORE introduces a low-rank parameter component to mediate\nthe parameter space allocation guided by a tailored loss and sinusoidal\nreweighting schedule, but without increasing the overall model complexity or\ninference costs. Extensive experiments on diverse long-tailed benchmarks,\nspanning multi-class and multi-label tasks, demonstrate that MORE significantly\nimproves generalization, particularly for tail classes, and effectively\ncomplements existing imbalance mitigation methods. These results highlight\nMORE's potential as a robust plug-and-play module in long-tailed settings.", "AI": {"tldr": "\u63d0\u51faMORE\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u91cd\u65b0\u5e73\u8861\u6a21\u578b\u53c2\u6570\u7a7a\u95f4\u6765\u7f13\u89e3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u4f7f\u7528\u4f4e\u79e9\u53c2\u6570\u7ec4\u4ef6\u548c\u6b63\u5f26\u91cd\u52a0\u6743\u8c03\u5ea6\uff0c\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u6216\u63a8\u7406\u6210\u672c", "motivation": "\u957f\u5c3e\u8bc6\u522b\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u4e2d\u666e\u904d\u5b58\u5728\u4e14\u5177\u6709\u6311\u6218\u6027\uff0c\u7c7b\u522b\u5206\u5e03\u504f\u659c\u963b\u788d\u6a21\u578b\u5bf9\u5c3e\u90e8\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u5f15\u5165\u4f4e\u79e9\u53c2\u6570\u7ec4\u4ef6\u6765\u8c03\u8282\u53c2\u6570\u7a7a\u95f4\u5206\u914d\uff0c\u91c7\u7528\u5b9a\u5236\u635f\u5931\u548c\u6b63\u5f26\u91cd\u52a0\u6743\u8c03\u5ea6\uff0c\u4e0d\u589e\u52a0\u6574\u4f53\u6a21\u578b\u590d\u6742\u5ea6", "result": "\u5728\u591a\u6837\u5316\u7684\u957f\u5c3e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u5584\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u5c3e\u90e8\u7c7b\u522b\uff0c\u6709\u6548\u8865\u5145\u73b0\u6709\u4e0d\u5e73\u8861\u7f13\u89e3\u65b9\u6cd5", "conclusion": "MORE\u4f5c\u4e3a\u957f\u5c3e\u8bbe\u7f6e\u4e2d\u7684\u7a33\u5065\u5373\u63d2\u5373\u7528\u6a21\u5757\u5177\u6709\u6f5c\u529b"}}
{"id": "2510.08365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08365", "abs": "https://arxiv.org/abs/2510.08365", "authors": ["Yukai Song", "Pengfei Zhou", "C\u00e9sar Escobar-Viera", "Candice Biernesser", "Wei Huang", "Jingtong Hu"], "title": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media", "comment": null, "summary": "Suicide rates have risen worldwide in recent years, underscoring the urgent\nneed for proactive prevention strategies. Social media provides valuable\nsignals, as many at-risk individuals - who often avoid formal help due to\nstigma - choose instead to share their distress online. Yet detecting implicit\nsuicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle\nemotional cues, remains highly challenging. Lightweight models like BERT handle\nexplicit signals but fail on subtle implicit ones, while large language models\n(LLMs) capture nuance at prohibitive computational cost. To address this gap,\nwe propose a two-stage voting architecture that balances efficiency and\nrobustness. In Stage 1, a lightweight BERT classifier rapidly resolves\nhigh-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to\neither (i) a multi-perspective LLM voting framework to maximize recall on\nimplicit ideation, or (ii) a feature-based ML ensemble guided by\npsychologically grounded indicators extracted via prompt-engineered LLMs for\nefficiency and interpretability. To the best of our knowledge, this is among\nthe first works to operationalize LLM-extracted psychological features as\nstructured vectors for suicide risk detection. On two complementary datasets -\nexplicit-dominant Reddit and implicit-only DeepSuiMind - our framework\noutperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%\non implicit ones, and reducing the cross-domain gap below 2%, while\nsignificantly lowering LLM cost.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6295\u7968\u67b6\u6784\u7528\u4e8e\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\uff0c\u7b2c\u4e00\u9636\u6bb5\u7528\u8f7b\u91cfBERT\u5904\u7406\u660e\u786e\u6848\u4f8b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528LLM\u6295\u7968\u6216\u57fa\u4e8e\u5fc3\u7406\u7279\u5f81\u7684ML\u96c6\u6210\u5904\u7406\u9690\u6666\u6848\u4f8b\uff0c\u5e73\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u6740\u7387\u4e0a\u5347\u9700\u8981\u4e3b\u52a8\u9884\u9632\u7b56\u7565\uff0c\u793e\u4ea4\u5a92\u4f53\u63d0\u4f9b\u6709\u4ef7\u503c\u4fe1\u53f7\uff0c\u4f46\u68c0\u6d4b\u9690\u6666\u81ea\u6740\u610f\u5ff5\uff08\u901a\u8fc7\u9690\u55bb\u3001\u8bbd\u523a\u7b49\u95f4\u63a5\u8868\u8fbe\uff09\u6781\u5177\u6311\u6218\uff0c\u73b0\u6709\u8f7b\u91cf\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9690\u6666\u4fe1\u53f7\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5BERT\u5206\u7c7b\u5668\u5feb\u901f\u5904\u7406\u9ad8\u7f6e\u4fe1\u5ea6\u660e\u786e\u6848\u4f8b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u6a21\u7cca\u8f93\u5165\u5347\u7ea7\u5230\u591a\u89c6\u89d2LLM\u6295\u7968\u6846\u67b6\u6216\u57fa\u4e8eLLM\u63d0\u53d6\u5fc3\u7406\u7279\u5f81\u6307\u5bfc\u7684ML\u96c6\u6210\u3002", "result": "\u5728\u4e24\u4e2a\u4e92\u8865\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u660e\u786e\u6848\u4f8bF1\u8fbe98.0%\uff0c\u9690\u6666\u6848\u4f8bF1\u8fbe99.7%\uff0c\u8de8\u57df\u5dee\u8ddd\u4f4e\u4e8e2%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4eLLM\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u4e2d\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u662f\u9996\u6279\u5c06LLM\u63d0\u53d6\u7684\u5fc3\u7406\u7279\u5f81\u4f5c\u4e3a\u7ed3\u6784\u5316\u5411\u91cf\u7528\u4e8e\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u7684\u5de5\u4f5c\u4e4b\u4e00\u3002"}}
{"id": "2510.08236", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08236", "abs": "https://arxiv.org/abs/2510.08236", "authors": ["Konrad L\u00f6hr", "Shuzhou Yuan", "Michael F\u00e4rber"], "title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u653f\u6cbb\u7f57\u76d8\u6d4b\u8bd5\u8bc4\u4f30\u4e868\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u7684\u653f\u6cbb\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u4f20\u64ad\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5448\u73b0\u4e00\u81f4\u7684\u5de6\u503e\u653f\u6cbb\u503e\u5411\uff0c\u4e14\u901a\u8fc7\u8bed\u8a00\u53d8\u5316\u5f15\u53d1\u7684\u9690\u6027\u523b\u677f\u5370\u8c61\u6bd4\u663e\u6027\u523b\u677f\u5370\u8c61\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4f1a\u4fe1\u606f\u4f20\u64ad\u548c\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u7406\u89e3\u5176\u5728\u653f\u6cbb\u9886\u57df\u7684\u6f5c\u5728\u504f\u89c1\u5bf9\u4e8e\u9632\u6b62\u5bf9\u516c\u4f17\u8206\u8bba\u548c\u6c11\u4e3b\u8fdb\u7a0b\u4ea7\u751f\u4e0d\u5f53\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e8c\u7ef4\u653f\u6cbb\u7f57\u76d8\u6d4b\u8bd5\u8bc4\u4f30\u6a21\u578b\u56fa\u6709\u653f\u6cbb\u503e\u5411\uff0c\u4f7f\u7528\u89d2\u8272\u63d0\u793a\u63a2\u7d22\u663e\u6027\u523b\u677f\u5370\u8c61\uff0c\u5e76\u901a\u8fc7\u591a\u8bed\u8a00\u7248\u672c\u6d4b\u8bd5\u63ed\u793a\u9690\u6027\u523b\u677f\u5370\u8c61\u3002", "result": "\u6240\u6709\u88ab\u8c03\u67e5\u6a21\u578b\u5747\u663e\u793a\u4e00\u81f4\u7684\u5de6\u503e\u653f\u6cbb\u503e\u5411\uff1b\u4e0d\u540c\u6a21\u578b\u7684\u523b\u677f\u5370\u8c61\u6027\u8d28\u548c\u7a0b\u5ea6\u5dee\u5f02\u5f88\u5927\uff1b\u9690\u6027\u523b\u677f\u5370\u8c61\u6bd4\u663e\u6027\u523b\u677f\u5370\u8c61\u66f4\u660e\u663e\uff1b\u5927\u591a\u6570\u6a21\u578b\u7684\u9690\u6027\u548c\u663e\u6027\u523b\u677f\u5370\u8c61\u5b58\u5728\u663e\u8457\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u653f\u6cbb\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u5176\u56fa\u6709\u504f\u89c1\u5177\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u900f\u660e\u5ea6\u6216\"\u610f\u8bc6\"\u3002"}}
