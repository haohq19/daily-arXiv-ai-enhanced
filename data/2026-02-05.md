<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 提出可解释的计算机视觉框架，用于增材制造中孔隙检测与临界性评估，通过SHAP分析发现表面距离是预测孔隙临界性的最主要因素


<details>
  <summary>Details</summary>
Motivation: 增材制造中的内部孔隙是影响结构性能的关键缺陷，现有自动检测方法缺乏可解释性，工程师无法理解临界性预测的物理基础

Method: 使用灰度切片重建三维体积数据，通过强度阈值和连通分量分析识别500个孔隙；提取几何特征（尺寸、纵横比、范围、空间位置）；构建孔隙交互网络；使用机器学习模型预测临界性，并通过SHAP分析量化特征贡献

Result: 归一化表面距离对模型预测的贡献比其他所有描述符高一个数量级；孔隙尺寸影响最小，几何参数影响可忽略；表面接近度与临界性呈强反比关系，揭示了边界驱动的失效机制

Conclusion: 该可解释框架实现了透明的缺陷评估，为增材制造的工艺优化和质量控制提供了可操作的见解

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

</details>


### [2] [VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding](https://arxiv.org/abs/2602.04094)
*Junbo Zou,Ziheng Huang,Shengjie Zhang,Liwen Zhang,Weining Shen*

Main category: cs.CV

TL;DR: VideoBrain：一种通过双智能体自适应采样的长视频理解框架，在减少30-40%帧数的同时提升性能3.5-9.0%


<details>
  <summary>Details</summary>
Motivation: 长视频理解面临计算约束与信息分布之间的固有矛盾。现有方法要么均匀采样（可能丢失信息），要么单次选择关键帧（无法纠正错误选择），需要更智能的自适应采样策略。

Method: 提出VideoBrain端到端框架，包含两个互补智能体：基于CLIP的语义检索智能体（跨视频检索语义信息）和均匀采样智能体（在区间内密集采样）。VLM直接感知帧并推理信息充分性。引入行为感知奖励函数和数据分类流程，防止模型滥用智能体。

Result: 在四个长视频基准测试中，相比基线方法性能提升3.5%到9.0%，同时减少30-40%的帧数使用。在短视频基准测试上表现出强大的跨数据集泛化能力。

Conclusion: VideoBrain通过自适应采样策略有效解决了长视频理解的计算效率问题，在减少计算成本的同时显著提升了性能，展示了智能体协作在视频理解中的潜力。

Abstract: Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.

</details>


### [3] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: HoloEv-Net：一种高效的事件行为识别框架，通过紧凑全息时空表示和全局频谱门控模块，解决了现有方法的计算冗余、结构冗余和频谱信息利用不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有事件行为识别方法存在三个主要问题：(1) 密集体素表示的计算冗余，(2) 多分支架构的结构冗余，(3) 频谱信息在捕捉全局运动模式中的利用不足。

Method: 提出HoloEv-Net框架：1) 紧凑全息时空表示(CHSR)，将水平空间线索隐式嵌入到时间-高度视图中，在2D表示中保留3D时空上下文；2) 全局频谱门控(GSG)模块，利用快速傅里叶变换在频域进行全局令牌混合。

Result: 在THU-EACT-50-CHL、HARDVS和DailyDVS-200数据集上分别取得10.29%、1.71%和6.25%的性能提升；轻量级变体HoloEv-Net-Small相比重型基线减少5.4倍参数、300倍FLOPs和2.4倍延迟。

Conclusion: HoloEv-Net通过解决现有方法的冗余问题并充分利用频谱信息，实现了高效且高性能的事件行为识别，特别适合边缘部署应用。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [4] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 研究将OpenEMMA端到端驾驶框架适配到doScenes数据集，通过乘客式语言指令条件化轨迹规划，显著提升驾驶行为预测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随规划器大多依赖仿真或固定命令词汇，限制了真实世界的泛化能力。doScenes是首个连接自由形式指令与真实世界运动数据的数据集，需要建立可复现的指令条件化规划基准。

Method: 将doScenes指令作为乘客式提示集成到OpenEMMA视觉语言界面中，该框架基于多模态大语言模型，输入前摄像头视图和自车状态，输出10步速度-曲率轨迹。

Result: 在849个标注场景上评估，指令条件化显著提升鲁棒性，防止极端基线失败，平均ADE降低98.7%。去除异常值后，良好表述的提示仍能提升ADE达5.1%。

Conclusion: 研究建立了指令感知规划的可复现基准，分析了什么构成OpenEMMA框架的"好"指令，并发布了评估提示和脚本供社区使用。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [5] [Adaptive 1D Video Diffusion Autoencoder](https://arxiv.org/abs/2602.04220)
*Yao Teng,Minxuan Lin,Xian Liu,Shuai Wang,Xiao Yang,Xihui Liu*

Main category: cs.CV

TL;DR: 提出One-DVA：基于Transformer的自适应1D编码和扩散解码的视频自编码器，解决现有视频自编码器的固定压缩率、不灵活架构和确定性解码器问题


<details>
  <summary>Details</summary>
Motivation: 现有视频自编码器存在三个主要问题：1) 固定压缩率导致简单视频浪费token；2) 不灵活的CNN架构无法支持变长潜在建模；3) 确定性解码器难以从压缩潜在表示中恢复细节

Method: 提出One-DVA框架：编码器使用基于查询的视觉Transformer提取时空特征并生成潜在表示，通过变长dropout机制动态调整潜在长度；解码器是像素空间扩散Transformer，以潜在表示为条件重建视频；采用两阶段训练策略

Result: 在相同压缩比下，性能与3D-CNN VAE相当；支持自适应压缩，可实现更高压缩比；通过正则化潜在分布和微调解码器，更好地支持下游生成任务

Conclusion: One-DVA解决了现有视频自编码器的局限性，支持自适应压缩和变长潜在建模，为下游生成任务提供了更好的基础

Abstract: Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.

</details>


### [6] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 提出用于非正式住区大规模测绘的新半监督分割框架，通过类感知自适应阈值和原型银行系统解决数据稀缺和质量问题，在八个城市验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家快速城市化导致非正式住区增长，但大规模测绘面临标注稀缺、光谱模糊和标注噪声等数据质量挑战。

Method: 1) 构建拉合尔基准数据集及卡拉奇、孟买配套数据集；2) 提出新半监督分割框架，包含类感知自适应阈值机制防止少数类抑制，原型银行系统通过历史学习的高保真特征表示强制语义一致性。

Result: 方法在三大洲八个城市上优于最先进的半监督基线，仅用10%源标签训练的模型在未见地理区域达到0.461 mIoU，优于完全监督模型的零样本泛化能力。

Conclusion: 提出的框架有效解决了非正式住区测绘中的数据质量和类别不平衡问题，展示了强大的领域迁移能力，为大规模城市测绘提供了实用解决方案。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [7] [PEPR: Privileged Event-based Predictive Regularization for Domain Generalization](https://arxiv.org/abs/2602.04583)
*Gabriele Magrini,Federico Becattini,Niccolò Biondi,Pietro Pala*

Main category: cs.CV

TL;DR: 提出PEPR框架，利用事件相机作为特权信息训练鲁棒的RGB模型，通过预测事件特征而非直接对齐来提升域泛化能力


<details>
  <summary>Details</summary>
Motivation: 视觉感知的深度神经网络对域偏移高度敏感，这限制了在真实世界部署的鲁棒性。RGB图像语义丰富但域依赖性强，而事件流稀疏但更域不变，需要一种方法能结合两者优势

Method: 提出特权事件预测正则化(PEPR)框架，将学习使用特权信息(LUPI)重新定义为共享潜在空间中的预测问题。训练RGB编码器预测事件潜在特征，而不是直接进行跨模态对齐，从而在不牺牲语义丰富性的情况下提取鲁棒性

Result: 训练得到的独立RGB模型在日间到夜间等域偏移场景下表现出更强的鲁棒性，在目标检测和语义分割任务上均优于基于对齐的基线方法

Conclusion: PEPR框架通过预测性正则化有效利用事件相机的特权信息，训练出更鲁棒的RGB模型，解决了域泛化挑战，为单模态模型在真实世界部署提供了有效方案

Abstract: Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.

</details>


### [8] [PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation](https://arxiv.org/abs/2602.04876)
*Jiahao Zhan,Zizhang Li,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: PerpetualWonder 是一个混合生成模拟器，能够从单张图像生成长时程、动作条件的4D场景，通过统一表示和闭环系统解决物理状态与视觉表示解耦的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单张图像生成长时程4D场景时失败，因为它们的物理状态与视觉表示是解耦的，这导致生成式细化无法更新底层物理状态以支持后续交互。

Method: 1. 引入首个真正的闭环系统；2. 提出新颖的统一表示，在物理状态和视觉基元之间建立双向链接，允许生成式细化同时修正动力学和外观；3. 引入鲁棒的更新机制，从多视角收集监督以解决优化模糊性。

Result: 实验表明，从单张图像出发，PerpetualWonder 能够成功模拟复杂、多步骤的长时程交互，保持物理合理性和视觉一致性。

Conclusion: PerpetualWonder 通过统一表示和闭环系统解决了物理状态与视觉表示解耦的问题，实现了从单张图像生成长时程、动作条件的4D场景模拟。

Abstract: We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: WIND是一个统一的气象基础模型，无需任务特定微调即可处理多种天气气候任务，通过视频扩散模型和逆问题求解实现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在天气气候建模中高度碎片化，不同任务需要专门训练的模型。需要统一的基础模型来替代这些专门化基线。

Method: 使用自监督视频重建目标预训练无条件视频扩散模型，学习大气稳健先验。推理时将领域特定问题框架化为逆问题，通过后验采样求解。

Result: WIND能够处理概率预测、时空降尺度、稀疏重建、守恒定律执行等多种任务，并能生成全球变暖情景下的极端天气反事实情景。

Conclusion: 结合生成视频建模和逆问题求解，WIND为AI大气建模提供了计算高效的新范式，实现了统一的基础模型方法。

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [10] [On the use of LLMs to generate a dataset of Neural Networks](https://arxiv.org/abs/2602.04388)
*Nadia Daoudi,Jordi Cabot*

Main category: cs.LG

TL;DR: 利用大语言模型自动生成包含608个样本的神经网络数据集，用于验证神经网络工具的有效性


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开、多样化的神经网络数据集来系统评估神经网络验证、重构和迁移工具的有效性

Method: 使用大语言模型自动生成神经网络数据集，涵盖多样架构组件、输入数据类型和任务，并通过静态分析和符号追踪验证正确性

Result: 生成了包含608个样本的数据集，每个样本都符合精确的设计选择，数据集已公开可用

Conclusion: 该数据集为社区提供了评估神经网络可靠性和适应性研究的标准基准，有助于推进相关工具和方法的发展

Abstract: Neural networks are increasingly used to support decision-making. To verify their reliability and adaptability, researchers and practitioners have proposed a variety of tools and methods for tasks such as NN code verification, refactoring, and migration. These tools play a crucial role in guaranteeing both the correctness and maintainability of neural network architectures, helping to prevent implementation errors, simplify model updates, and ensure that complex networks can be reliably extended and reused. Yet, assessing their effectiveness remains challenging due to the lack of publicly diverse datasets of neural networks that would allow systematic evaluation. To address this gap, we leverage large language models (LLMs) to automatically generate a dataset of neural networks that can serve as a benchmark for validation. The dataset is designed to cover diverse architectural components and to handle multiple input data types and tasks. In total, 608 samples are generated, each conforming to a set of precise design choices. To further ensure their consistency, we validate the correctness of the generated networks using static analysis and symbolic tracing. We make the dataset publicly available to support the community in advancing research on neural network reliability and adaptability.

</details>


### [11] [RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models](https://arxiv.org/abs/2602.04448)
*Jiacheng Liang,Yuhui Wang,Tanqiu Jiang,Ting Wang*

Main category: cs.LG

TL;DR: RASA：针对MoE语言模型的路由感知专家级对齐框架，通过修复安全关键专家而非全局参数更新来提升安全对齐效果


<details>
  <summary>Details</summary>
Motivation: MoE语言模型由于稀疏路由机制，在标准全参数微调下可能出现退化优化行为，导致安全对齐效果不佳。初步实验发现，全参数安全微调可能通过路由或专家主导效应降低攻击成功率，而非直接修复安全关键专家。

Method: 提出RASA框架：1）识别被成功越狱攻击过度激活的专家；2）在固定路由下仅对这些安全关键专家进行选择性微调；3）强制路由与安全对齐上下文的一致性。

Result: 在两种代表性MoE架构和多样化越狱攻击上，RASA实现了近乎完美的鲁棒性、强大的跨攻击泛化能力，显著减少过度拒绝，同时在MMLU、GSM8K和TruthfulQA等基准测试中保持通用能力。

Conclusion: MoE模型的安全对齐应从全局参数更新转向有针对性的专家修复，RASA提供了一种实用且保持架构特性的替代方案，优于先前方法。

Abstract: Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.

</details>


### [12] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: AdaCNP是一个用于极端天气条件下电力负荷概率预测的模型，通过共享嵌入空间学习相似性，对历史上下文信息进行重加权，能够在极端样本稀缺的情况下实现少样本适应，提供更可靠的预测分布。


<details>
  <summary>Details</summary>
Motivation: 极端天气会显著改变电力消费行为，导致负荷曲线出现尖峰和剧烈波动。传统预测模型在这些时期容易不准确，可能导致电力系统供应短缺或局部过载，迫使采取紧急措施如减载，增加服务中断和公共安全风险。主要挑战在于极端事件会引发负荷模式的突然转变，而相关极端样本稀缺且不规则，使得可靠学习和校准变得困难。

Method: AdaCNP是一个用于数据稀缺条件下的概率预测模型。它在共享嵌入空间中学习相似性，对每个目标数据评估历史上下文段与当前条件的相关性，并相应地对上下文信息进行重加权。这种设计即使在极端样本稀缺的情况下也能突出最有信息量的历史证据，使模型能够对先前未见过的极端模式进行少样本适应。模型还能在不进行昂贵目标域微调的情况下产生预测分布，支持风险感知决策。

Result: 在真实世界电力系统负荷数据上的评估显示，AdaCNP在极端时期更加鲁棒，相对于最强基线减少了22%的均方误差，同时实现了最低的负对数似然，表明其概率输出更加可靠。这些结果表明AdaCNP能有效缓解突然分布转变和极端样本稀缺的综合影响。

Conclusion: AdaCNP能够有效缓解突然分布转变和极端样本稀缺的综合影响，为极端事件下的弹性电力系统运行提供更可信赖的预测，支持更稳健的电力系统运行。

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [13] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: MTS-JEPA：一种用于多元时间序列异常预测的架构，通过多分辨率预测目标和软码本瓶颈解决JEPA框架中的表示崩溃问题，能有效分离瞬态冲击与长期趋势，并在早期预警任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列对关键基础设施至关重要，需要主动预测异常以降低风险。虽然联合嵌入预测架构（JEPA）为建模系统潜在演化提供了有前景的框架，但其应用受到表示崩溃和无法捕捉不同时间尺度前兆信号的限制。

Method: 提出MTS-JEPA架构，集成多分辨率预测目标和软码本瓶颈。该设计明确将瞬态冲击与长期趋势解耦，并利用码本捕捉离散状态转换。这种约束同时作为内在正则化器确保优化稳定性。

Result: 在标准基准测试上的实证评估证实，该方法有效防止退化解，并在早期预警协议下实现了最先进的性能。

Conclusion: MTS-JEPA通过多分辨率预测和软码本瓶颈解决了JEPA在多元时间序列异常预测中的关键限制，为关键基础设施的主动风险缓解提供了有效工具。

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [14] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 提出DDL-MSPMF双阶段TransUNet框架，融合多源降水产品和ERA5物理预测因子，提升中国区域降水估计精度，特别改善极端降水检测能力。


<details>
  <summary>Details</summary>
Motivation: 多源降水产品存在空间异质性偏差和极端降水检测能力有限的问题，限制了其在水文气候监测中的应用价值，需要开发更精确的融合方法。

Method: 开发双阶段TransUNet框架：第一阶段分类器估计日降水发生概率，第二阶段回归器融合分类器输出和所有预测因子，以0.25度分辨率估计中国2001-2020年日降水量。

Result: DDL-MSPMF在季节尺度表现最佳（R=0.75；RMSE=2.70 mm/day），相比单回归器设置更稳健；对强降水（>25 mm/day）在华东大部分地区提高公平威胁评分，更好地再现2021年7月郑州暴雨空间格局；在青藏高原数据稀缺区域也表现出适用性。

Conclusion: 该框架为降水融合和极端事件评估提供了可扩展且可解释的方法，通过SHAP分析强调了降水发生概率和地表压力的重要性，提供物理可解释的诊断。

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 本文提出一个七维分类法来系统分析医疗领域LLM智能体的能力现状，通过对49项研究进行量化评估，揭示了当前医疗AI智能体在知识整合、多智能体架构等方面较为成熟，但在事件触发激活、漂移检测与缓解、治疗规划等行动导向任务上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM智能体的研究缺乏统一的分析框架，现有文献多为宽泛综述或针对单一能力的深入探讨，导致医疗工作者难以形成共识性理解。本文旨在填补这一空白，为医疗AI智能体研究提供系统化的评估框架。

Method: 提出七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务），包含29个操作性子维度。采用明确的纳入排除标准和三级标签（完全实现、部分实现、未实现），对49项研究进行系统映射和量化分析。

Result: 分析揭示了明显的不对称性：知识管理中的外部知识整合普遍实现（约76%完全实现），而交互模式中的事件触发激活基本缺失（约92%未实现），适应与学习中的漂移检测与缓解罕见（约98%未实现）。架构上多智能体设计占主导（约82%完全实现）。核心任务中信息中心能力领先，但治疗规划等行动导向领域仍有显著差距（约59%未实现）。

Conclusion: 医疗LLM智能体在知识整合和多智能体架构方面已取得进展，但在事件驱动交互、持续适应能力和行动导向任务方面存在明显不足。该分类法为系统评估医疗AI智能体提供了实用框架，有助于指导未来研究方向。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry](https://arxiv.org/abs/2602.04206)
*Hsien-Jyh Liao*

Main category: cs.CL

TL;DR: Soft-FSM：一种神经符号架构，通过外部确定性状态控制器强制实现单调进展，解决LLM在程序性约束任务中的停滞问题，在台湾刑事杀人案件交叉询问中达到97%以上完整性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言流畅性方面表现优异，但在明确的程序性约束下可靠完成长期任务时存在困难。在法律交叉询问中，纯概率生成往往保持行为连贯性但无法确保程序推进，这种失败被描述为程序性停滞。

Method: 提出Soft-FSM神经符号架构，通过外部确定性状态控制器强制实现对累积关键信息单元(KIUs)的单调进展控制，结合神经网络的灵活性和符号系统的确定性。

Result: 在三个真实世界台湾刑事杀人案件实验中，基线方法完整性低于40%，而Soft-FSM始终达到97%以上完整性且冗余度接近零，显著优于传统方法。

Conclusion: 在某些领域，可靠的完成任务不能仅依赖LLM的涌现行为，而需要通过明确且可验证的外部状态控制来强制保证，神经符号方法为解决程序性约束任务提供了有效途径。

Abstract: Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.

</details>


### [17] [Proxy Compression for Language Modeling](https://arxiv.org/abs/2602.04289)
*Lin Zheng,Xinyu Li,Qian Liu,Xiachong Feng,Lingpeng Kong*

Main category: cs.CL

TL;DR: 代理压缩训练方案：在训练时同时使用原始字节和压缩视图，推理时仅用原始字节，显著提升训练效率并超越纯字节级基线


<details>
  <summary>Details</summary>
Motivation: 现代语言模型训练依赖固定分词器，将模型与外部压缩器耦合。需要一种既能保持压缩输入效率优势，又能在推理时提供端到端原始字节接口的训练方案

Method: 提出代理压缩训练方案：在训练过程中，一个语言模型同时在原始字节序列和外部压缩器生成的压缩视图上进行联合训练，使模型学习内部对齐压缩序列和原始字节

Result: 在代码语言建模实验中，代理压缩显著提升训练效率，在固定计算预算下显著优于纯字节级基线。随着模型规模增大，收益更明显，代理训练模型最终匹配或超越分词器方法

Conclusion: 代理压缩提供了高效的训练方案，在推理时仅使用原始字节操作，同时保持了字节级建模的固有鲁棒性，实现了训练效率与推理灵活性的平衡

Abstract: Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.

</details>
