{"id": "2601.07964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07964", "abs": "https://arxiv.org/abs/2601.07964", "authors": ["Alexander Boldachev"], "title": "Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling", "comment": "25 pages, 6 figures", "summary": "This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u6267\u884c\u672c\u4f53\u8bba\u5728\u6e38\u620f\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7boldsea\u6846\u67b6\u5b9e\u73b0\u4ece\u7b97\u6cd5\u884c\u4e3a\u7f16\u7a0b\u5230\u8bed\u4e49\u4e16\u754c\u5efa\u6a21\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4f7f\u667a\u80fd\u4f53\u884c\u4e3a\u4ece\u58f0\u660e\u5f0f\u9886\u57df\u89c4\u5219\u81ea\u7136\u6d8c\u73b0\u800c\u975e\u663e\u5f0f\u7f16\u7801\u3002", "motivation": "\u89e3\u51b3\u6e38\u620fAI\u67b6\u6784\u4e2d\u7684\u8bed\u4e49-\u8fc7\u7a0b\u9e3f\u6c9f\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u884c\u4e3a\u6811\u548c\u9762\u5411\u76ee\u6807\u884c\u52a8\u89c4\u5212\uff09\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u5e94\u8be5\u505a\u4ec0\u4e48\uff0c\u800c\u53ef\u6267\u884c\u672c\u4f53\u8bba\u5219\u5efa\u6a21\u52a8\u4f5c\u4f55\u65f6\u53d8\u5f97\u53ef\u80fd\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u6d8c\u73b0\u3002", "method": "\u4f7f\u7528boldsea\u6846\u67b6\u5b9e\u73b0\u53ef\u6267\u884c\u672c\u4f53\u8bba\uff0c\u901a\u8fc7\u5728\u751f\u5b58\u6e38\u620f\u573a\u666f\uff08Winter Feast\uff09\u4e2d\u5e94\u7528\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u6570\u636e\u6d41\u6761\u4ef6\u800c\u975e\u663e\u5f0f\u62a2\u5360\u903b\u8f91\u5b9e\u73b0\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u4e2d\u65ad\uff0c\u5e76\u4e0e\u884c\u4e3a\u6811\u548c\u9762\u5411\u76ee\u6807\u884c\u52a8\u89c4\u5212\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53ef\u6267\u884c\u672c\u4f53\u8bba\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8bed\u4e49-\u8fc7\u7a0b\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u6d8c\u73b0\uff0c\u5177\u6709\u65f6\u95f4\u4e8b\u4ef6\u56fe\u56fa\u6709\u7684\u8c03\u8bd5\u4f18\u52bf\uff0c\u5e76\u5c55\u793a\u4e86LLM\u9a71\u52a8\u8fd0\u884c\u65f6\u6a21\u578b\u751f\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "\u53ef\u6267\u884c\u672c\u4f53\u8bba\u4ee3\u8868\u4e86\u6e38\u620fAI\u5f00\u53d1\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u7b97\u6cd5\u884c\u4e3a\u7f16\u7a0b\u8f6c\u5411\u8bed\u4e49\u4e16\u754c\u5efa\u6a21\uff0c\u4e3a\u89e3\u51b3\u6e38\u620fAI\u67b6\u6784\u4e2d\u7684\u6839\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u5177\u6709\u4e0eLLM\u7b49\u65b0\u6280\u672f\u96c6\u6210\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.07845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07845", "abs": "https://arxiv.org/abs/2601.07845", "authors": ["Shree Charran R", "Rahul Kumar Dubey"], "title": "Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration", "comment": null, "summary": "Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fb9\u7f18AI\u7684\u5b9e\u65f6\u8def\u8fb9\u611f\u77e5\u8282\u70b9\uff0c\u7528\u4e8e\u591a\u7c7b\u4ea4\u901a\u8fdd\u89c4\u68c0\u6d4b\u548c\u5b89\u5168\u4e8b\u4ef6\u5206\u53d1\uff0c\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8fdd\u89c4\u8bc6\u522b\u548cV2X\u901a\u4fe1\u3002", "motivation": "\u65b0\u5174\u7ecf\u6d4e\u4f53\u5982\u5370\u5ea6\u5feb\u901f\u673a\u52a8\u5316\u5bfc\u81f4\u6267\u6cd5\u4e25\u91cd\u4e0d\u8db3\uff082023\u5e741100\u4e07\u8fdd\u89c4\u8bb0\u5f55 vs \u6bcf4000\u8f86\u8f66\u4ec51\u540d\u8b66\u5bdf\uff09\uff0c\u4f20\u7edf\u76d1\u63a7\u548c\u4eba\u5de5\u7f5a\u5355\u65e0\u6cd5\u5e94\u5bf9\u6b64\u89c4\u6a21\uff0c\u9700\u8981\u81ea\u4e3b\u3001\u534f\u4f5c\u3001\u8282\u80fd\u7684\u8fb9\u7f18AI\u611f\u77e5\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u96c6\u6210YOLOv8 Nano\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u591a\u76ee\u6807\u68c0\u6d4b\uff0cDeepSORT\u5b9e\u73b0\u65f6\u5e8f\u4e00\u81f4\u8f66\u8f86\u8ddf\u8e2a\uff0c\u89c4\u5219\u5f15\u5bfcOCR\u540e\u5904\u7406\u5f15\u64ce\u8bc6\u522b\u9000\u5316\u6216\u591a\u8bed\u8a00\u8f66\u724c\uff08\u7b26\u5408MoRTH AIS 159\u548cISO 7591\u6807\u51c6\uff09\u3002\u90e8\u7f72\u5728NVIDIA Jetson Nano\u4e0a\uff0c\u901a\u8fc7TensorRT FP16\u91cf\u5316\u4f18\u5316\u3002", "result": "\u7cfb\u7edf\u57289.6W\u529f\u8017\u4e0b\u7ef4\u630128-30 FPS\u63a8\u7406\uff0c\u8fbe\u523097.7%\u8fdd\u89c4\u68c0\u6d4b\u51c6\u786e\u7387\u548c84.9% OCR\u7cbe\u5ea6\uff0c\u8986\u76d65\u7c7b\u8fdd\u89c4\uff08\u95ef\u7ea2\u706f\u3001\u6591\u9a6c\u7ebf\u8fdd\u89c4\u3001\u9006\u884c\u3001\u975e\u6cd5\u6389\u5934\u3001\u8d85\u901f\uff09\u3002\u76f8\u6bd4YOLOv4 Tiny\u7b49\u6a21\u578b\uff0c\u83b7\u5f9710.7% mAP\u63d0\u5347\u548c1.4\u500d\u80fd\u6548\u6bd4\u6539\u8fdb\u3002", "conclusion": "\u8def\u8fb9\u8fb9\u7f18AI\u5206\u6790\u53ef\u4ee5\u589e\u5f3a\u534f\u4f5c\u611f\u77e5\u548c\u4e3b\u52a8\u9053\u8def\u5b89\u5168\u7ba1\u7406\uff0c\u901a\u8fc7V2X\u534f\u8bae\u5411\u8054\u7f51\u8f66\u8f86\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u540e\u7aef\u53d1\u5e03\u6807\u51c6\u5316\u5b89\u5168\u4e8b\u4ef6\uff08CAM\u548cDENM\u7c7b\u578b\uff09\uff0c\u5728IEEE\u667a\u80fd\u8f66\u8f86\u751f\u6001\u7cfb\u7edf\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.08246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08246", "abs": "https://arxiv.org/abs/2601.08246", "authors": ["Yifan Han", "Pengfei Yi", "Junyan Li", "Hanqing Wang", "Gaojing Zhang", "Qi Peng Liu", "Wenzhao Lian"], "title": "FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models", "comment": null, "summary": "Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u9ad8\u6548\u6846\u67b6\uff0c\u4ece\u4eba\u7c7b\u89c6\u9891\u63d0\u53d6\u8bed\u4e49\u6293\u53d6\u5148\u9a8c\uff0c\u5b9e\u73b0\u8de8\u591a\u79cd\u7075\u5de7\u624b\u7684\u6293\u53d6\u5408\u6210\uff0c\u65e0\u9700\u4e3a\u6bcf\u79cd\u624b\u6536\u96c6\u5927\u91cf\u6293\u53d6\u6570\u636e\u3002", "motivation": "\u7075\u5de7\u6293\u53d6\u5408\u6210\u9762\u4e34\u9ad8\u7ef4\u5ea6\u548c\u8fd0\u52a8\u591a\u6837\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u786c\u4ef6\u7279\u5b9a\u7684\u6293\u53d6\u6570\u636e\u96c6\uff08\u4eff\u771f\u6216\u771f\u5b9e\u91c7\u96c6\uff09\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u65b0\u8bbe\u8ba1\u7684\u7075\u5de7\u624b\u3002", "method": "1) \u4ece\u539f\u59cb\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u63d0\u53d6\u65f6\u95f4\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u6293\u53d6\u529f\u80fd\u8bed\u4e49\uff1b2) \u4e0e\u6df1\u5ea6\u56fe\u50cf\u76843D\u573a\u666f\u51e0\u4f55\u878d\u5408\uff0c\u63a8\u65ad\u8bed\u4e49\u57fa\u7840\u7684\u63a5\u89e6\u76ee\u6807\uff1b3) \u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u91cd\u5b9a\u5411\u6a21\u5757\u5c06\u529f\u80fd\u8868\u793a\u6620\u5c04\u5230\u4e0d\u540c\u7075\u5de7\u624b\uff0c\u65e0\u9700\u4e3a\u6bcf\u79cd\u624b\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u7cfb\u7edf\u80fd\u751f\u6210\u7a33\u5b9a\u3001\u529f\u80fd\u9002\u5f53\u7684\u591a\u63a5\u89e6\u6293\u53d6\uff0c\u5728\u5e38\u89c1\u7269\u4f53\u548c\u5de5\u5177\u4e0a\u53ef\u9760\u6210\u529f\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5b9e\u4f8b\u3001\u59ff\u6001\u53d8\u5316\u548c\u591a\u79cd\u624b\u5177\u73b0\u5316\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6f14\u793a\u548c\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u786c\u4ef6\u65e0\u5173\u7684\u7075\u5de7\u64cd\u4f5c\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u8bc1\u660e\u5355\u4e00\u6df1\u5ea6\u6a21\u6001\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u8bed\u4e49\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u6293\u53d6\u5408\u6210\u3002"}}
{"id": "2601.08166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08166", "abs": "https://arxiv.org/abs/2601.08166", "authors": ["Mohammad Pivezhandi", "Mahdi Banisharif", "Abusayeed Saifullah", "Ali Jannesari"], "title": "ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms", "comment": "39 pages, 12 figures, 8 tables (including appendix)", "summary": "Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6838\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u70ed\u7ba1\u7406\u548c\u80fd\u8017\u4f18\u5316\u8c03\u5ea6\uff0c\u7ed3\u5408LLM\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DVFS\u548c\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5229\u7528\u7387\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5ffd\u7565\u505c\u987f\u65f6\u95f4\uff0c\u57fa\u4e8e\u79bb\u7ebf\u5206\u6790\u7684\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u8fd0\u884c\u65f6\u53d8\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u3001\u65e0\u9700\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\u7684\u9ad8\u6548\u8c03\u5ea6\u65b9\u6848\u3002", "method": "1) \u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5206\u89e3\u6307\u6570\u7ea7\u52a8\u4f5c\u7a7a\u95f4\uff1b2) \u7ed3\u5408\u56de\u5f52\u6280\u672f\u7684\u73af\u5883\u6a21\u578b\u9884\u6d4b\u70ed\u529b\u5b66\u548c\u6027\u80fd\u72b6\u6001\uff1b3) LLM\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u4eceOpenMP\u4ee3\u7801\u4e2d\u63d0\u53d613\u4e2a\u4ee3\u7801\u7ea7\u7279\u5f81\uff1b4) Dyna-Q\u542f\u53d1\u6846\u67b6\u6574\u5408\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u3002", "result": "\u5728\u591a\u4e2a\u786c\u4ef6\u5e73\u53f0(BOTS\u3001PolybenchC\u57fa\u51c6\u6d4b\u8bd5)\u4e0a\u9a8c\u8bc1\uff1a\u76f8\u6bd4Linux ondemand governor\uff0c\u80fd\u6548\u63d0\u53477.09\u500d\uff0cmakespan\u63d0\u53474.0\u500d\u3002\u9996\u6b21\u51b3\u7b56\u5ef6\u8fdf\u6bd4\u57fa\u4e8e\u8868\u683c\u7684\u5206\u6790\u5feb8300\u500d\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u65e0\u6a21\u578b\u65b9\u6cd5\u5feb20\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u70ed\u7ba1\u7406\u548c\u80fd\u8017\u4f18\u5316\u8c03\u5ea6\uff0c\u652f\u6301\u96f6\u6837\u672c\u90e8\u7f72\u5230\u65b0\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u52a8\u6001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.08665", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08665", "abs": "https://arxiv.org/abs/2601.08665", "authors": ["Shaoan Wang", "Yuanfei Luo", "Xingyu Chen", "Aocheng Luo", "Dongyue Li", "Chang Liu", "Sheng Chen", "Yangang Zhang", "Junzhi Yu"], "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory", "comment": "Project page: https://wsakobe.github.io/VLingNav-web/", "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.", "AI": {"tldr": "VLingNav\uff1a\u57fa\u4e8e\u8bed\u8a00\u8ba4\u77e5\u7684VLA\u5bfc\u822a\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u673a\u5236\u548c\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\uff0c\u5728\u591a\u79cd\u5bfc\u822a\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5177\u8eab\u5bfc\u822a\u4e2d\u4e3b\u8981\u4f9d\u8d56\u4ece\u89c2\u5bdf\u5230\u52a8\u4f5c\u7684\u88ab\u52a8\u6620\u5c04\uff0c\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u548c\u6301\u4e45\u8bb0\u5fc6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u3001\u957f\u65f6\u7a0b\u7684\u5bfc\u822a\u4efb\u52a1\u3002", "method": "1\uff09\u53d7\u4eba\u7c7b\u8ba4\u77e5\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u673a\u5236\uff0c\u52a8\u6001\u89e6\u53d1\u663e\u5f0f\u63a8\u7406\uff1b2\uff09\u5f00\u53d1\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u8bed\u4e49\u8bb0\u5fc6\uff1b3\uff09\u6784\u5efaNav-AdaCoT-2.9M\u6570\u636e\u96c6\uff0c\u5305\u542b\u81ea\u9002\u5e94CoT\u6807\u6ce8\uff1b4\uff09\u52a0\u5165\u5728\u7ebf\u4e13\u5bb6\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u3002", "result": "VLingNav\u5728\u591a\u79cd\u5177\u8eab\u5bfc\u822a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u591f\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6267\u884c\u5404\u79cd\u5bfc\u822a\u4efb\u52a1\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VLingNav\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u8ba4\u77e5\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u63a8\u7406\u548c\u6301\u4e45\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6709\u6548\u8fc1\u79fb\u3002"}}
{"id": "2601.08224", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08224", "abs": "https://arxiv.org/abs/2601.08224", "authors": ["Daesuk Kwon", "Won-gi Paeng"], "title": "An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3", "comment": "20 pages, 3 tables", "summary": "General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.", "AI": {"tldr": "SANC(E3)\u662f\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u8868\u5f81\u5355\u5143\u4e0d\u5e94\u9884\u5148\u7ed9\u5b9a\uff0c\u800c\u5e94\u901a\u8fc7\u7ade\u4e89\u9009\u62e9\u3001\u91cd\u5efa\u548c\u538b\u7f29\u5728\u6709\u9650\u6fc0\u6d3b\u5bb9\u91cf\u4e0b\u4f5c\u4e3a\u7a33\u5b9a\u7ed3\u679c\u6d8c\u73b0\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u80fd\u91cf\u51fd\u6570E3\u6765\u7edf\u4e00\u611f\u77e5\u3001\u60f3\u8c61\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u884c\u52a8\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u9884\u8bbe\u4e86\u56fa\u5b9a\u7684\u539f\u59cb\u5355\u5143\uff08\u5982token\u3001\u50cf\u7d20\u7b49\uff09\uff0c\u7ed5\u8fc7\u4e86\u8868\u5f81\u5355\u5143\u5982\u4f55\u6d8c\u73b0\u548c\u7a33\u5b9a\u7684\u6838\u5fc3\u95ee\u9898\u3002\u901a\u7528\u667a\u80fd\u9700\u8981\u5c06\u7ecf\u9a8c\u91cd\u7ec4\u4e3a\u80fd\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9884\u6d4b\u548c\u884c\u52a8\u7684\u5185\u90e8\u7ed3\u6784\u3002", "method": "\u63d0\u51faSANC(E3)\u516c\u7406\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u516c\u7406\uff1a\u6709\u9650\u5bb9\u91cf\u3001\u5171\u73b0\u5173\u8054\u3001\u76f8\u4f3c\u6027\u7ade\u4e89\u3001\u7f6e\u4fe1\u5ea6\u7a33\u5b9a\u3001\u91cd\u5efa-\u538b\u7f29-\u66f4\u65b0\u6743\u8861\u3002\u91c7\u7528\u4f2a\u5185\u5b58\u6620\u5c04I/O\u673a\u5236\uff0c\u4f7f\u5185\u90e8\u56de\u653e\u7684\u683c\u5f0f\u5854\u4e0e\u5916\u90e8\u611f\u5b98\u8f93\u5165\u901a\u8fc7\u76f8\u540c\u516c\u7406\u8def\u5f84\u5904\u7406\u3002", "result": "\u4ece\u516c\u7406\u63a8\u5bfc\u51fa12\u4e2a\u547d\u9898\uff0c\u8868\u660e\u7c7b\u522b\u5f62\u6210\u3001\u5c42\u6b21\u7ec4\u7ec7\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u9ad8\u7ea7\u8ba4\u77e5\u6d3b\u52a8\u90fd\u53ef\u4ee5\u7406\u89e3\u4e3aE3\u6700\u5c0f\u5316\u4e0b\u7684\u683c\u5f0f\u5854\u5b8c\u6210\u8fc7\u7a0b\u3002\u5b9e\u73b0\u4e86\u611f\u77e5\u3001\u60f3\u8c61\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u884c\u52a8\u5728\u5355\u4e00\u8868\u5f81\u548c\u80fd\u91cf\u8fc7\u7a0b\u4e2d\u7684\u7edf\u4e00\u3002", "conclusion": "SANC(E3)\u4e3a\u7406\u89e3\u667a\u80fd\u5982\u4f55\u4ece\u6709\u9650\u8d44\u6e90\u7ea6\u675f\u4e0b\u6d8c\u73b0\u7a33\u5b9a\u8868\u5f81\u5355\u5143\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c06\u8ba4\u77e5\u529f\u80fd\u7edf\u4e00\u4e3a\u683c\u5f0f\u5854\u5b8c\u6210\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u8fc7\u7a0b\uff0c\u4e3a\u6784\u5efa\u66f4\u63a5\u8fd1\u4eba\u7c7b\u667a\u80fd\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.08141", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08141", "abs": "https://arxiv.org/abs/2601.08141", "authors": ["Muhammad Taimoor Hassan", "Jawad Ahmed", "Muhammad Awais"], "title": "Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training", "comment": null, "summary": "Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.", "AI": {"tldr": "Qalb\u662f\u4e00\u4e2a\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5f00\u53d1\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e4c\u5c14\u90fd\u8bedNLP\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u4f5c\u4e3a\u62e5\u67092.3\u4ebf\u4f7f\u7528\u8005\u7684\u8bed\u8a00\uff0c\u5728\u73b0\u4ee3NLP\u7cfb\u7edf\u4e2d\u4ee3\u8868\u6027\u4e25\u91cd\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4e4c\u5c14\u90fd\u8bed\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5904\u7406\u8be5\u8bed\u8a00\u7684\u590d\u6742\u5f62\u6001\u3001\u4ece\u53f3\u5230\u5de6\u7684Nastaliq\u6587\u5b57\u548c\u4e30\u5bcc\u7684\u6587\u5b66\u4f20\u7edf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4eceLLaMA 3.1 8B\u5f00\u59cb\uff0c\u572819.7\u4ebftoken\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5305\u542b18.4\u4ebf\u4e4c\u5c14\u90fd\u8bedtoken\u548c1.4\u4ebf\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1token\uff1b2\uff09\u5728Alif Urdu-instruct\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "Qalb\u5728\u4e4c\u5c14\u90fd\u8bed\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u52a0\u6743\u5e73\u5747\u5f97\u520690.34\uff0c\u6bd4\u4e4b\u524d\u7684SOTA\u6a21\u578bAlif-1.0-Instruct\uff0887.1\uff09\u9ad8\u51fa3.24\u5206\uff0c\u6bd4\u57fa\u7840LLaMA-3.1 8B-Instruct\u6a21\u578b\u9ad8\u51fa44.64\u5206\uff0c\u5728\u4e03\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8bed\u8a00\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u6709\u9488\u5bf9\u6027\u7684\u6307\u4ee4\u5fae\u8c03\uff0c\u80fd\u591f\u6709\u6548\u5c06\u57fa\u7840\u6a21\u578b\u9002\u5e94\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4e3a\u4e4c\u5c14\u90fd\u8bedNLP\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08408", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08408", "abs": "https://arxiv.org/abs/2601.08408", "authors": ["Yizhan Feng", "Hichem Snoussi", "Jing Teng", "Jian Liu", "Yuyang Wang", "Abel Cherouat", "Tian Wang"], "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2", "comment": "The Tenth International Conference on Data Mining and Big Data (DMBD'2025)", "summary": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBLIP-2\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4efb\u52a1\u5e73\u53f0\uff0c\u96c6\u6210YOLO-World\u548cYOLOv8-Seg\u6a21\u578b\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9ad8\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u590d\u6742\u573a\u666f\u4e2d\u9700\u8981\u5b9e\u65f6\u89c6\u89c9\u7406\u89e3\u548c\u4ea4\u4e92\uff0c\u4f46\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u4e0e\u65e0\u4eba\u673a\u8fb9\u7f18\u8bbe\u5907\u7684\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002", "method": "1) \u5c06BLIP-2\u4e0eYOLO\u6a21\u578b\u6df1\u5ea6\u96c6\u6210\uff0c\u5229\u7528YOLO\u7684\u7cbe\u786e\u611f\u77e5\u7ed3\u679c\uff1b2) \u8bbe\u8ba1\u57fa\u4e8eK-Means\u805a\u7c7b\u7684\u5185\u5bb9\u611f\u77e5\u5173\u952e\u5e27\u91c7\u6837\u673a\u5236\uff1b3) \u5b9e\u65bd\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u9002\u5e94\u63d0\u793a\u4f18\u5316\u65b9\u6848\uff0c\u5c06YOLO\u7684\u7ed3\u6784\u5316\u4e8b\u4ef6\u65e5\u5fd7\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u606f\u6ce8\u5165\u3002", "result": "\u5e73\u53f0\u6269\u5c55\u4e86BLIP-2\u7684\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u65e0\u9700\u5728\u65e0\u4eba\u673a\u6570\u636e\u4e0a\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u5373\u53ef\u6709\u6548\u5904\u7406\u89c6\u9891\u7ea7\u4ea4\u4e92\u4efb\u52a1\uff0c\u751f\u6210\u51c6\u786e\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f93\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u5e73\u53f0\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210YOLO\u6a21\u578b\u548c\u4f18\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u89c6\u89c9\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2601.08241", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08241", "abs": "https://arxiv.org/abs/2601.08241", "authors": ["Michele Fiori", "Gabriele Civitarese", "Marco Colussi", "Claudio Bettini"], "title": "Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence", "comment": null, "summary": "Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u5206\u5272\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u96f6\u6837\u672cADL\u8bc6\u522b\u65b9\u6cd5\uff0c\u8d85\u8d8a\u65f6\u95f4\u5206\u5272\u548c\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u96f6\u6837\u672cADL\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u65f6\u95f4\u5206\u5272\uff0c\u4e0eLLM\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u4e14\u7f3a\u4e4f\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5", "method": "\u91c7\u7528\u4e8b\u4ef6\u5206\u5272\u66ff\u4ee3\u65f6\u95f4\u5206\u5272\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5", "result": "\u4e8b\u4ef6\u5206\u5272\u5728\u590d\u6742\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u65f6\u95f4\u5206\u5272\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff1b\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u80fd\u6709\u6548\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u9884\u6d4b", "conclusion": "\u4e8b\u4ef6\u5206\u5272\u4e0e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u76f8\u7ed3\u5408\u663e\u8457\u63d0\u5347\u96f6\u6837\u672cADL\u8bc6\u522b\u6027\u80fd\uff0c\u5373\u4f7f\u4f7f\u7528\u76f8\u5bf9\u8f83\u5c0f\u7684LLM\u4e5f\u80fd\u53d6\u5f97\u4f18\u5f02\u6548\u679c"}}
{"id": "2601.08500", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08500", "abs": "https://arxiv.org/abs/2601.08500", "authors": ["Cristian Santini", "Marieke Van Erp", "Mehwish Alam"], "title": "It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models", "comment": null, "summary": "Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.", "AI": {"tldr": "MHEL-LLaMo\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5386\u53f2\u6587\u672c\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u6570\u533a\u5206\u7b80\u5355\u548c\u56f0\u96be\u6837\u672c\uff0c\u4ec5\u5728\u56f0\u96be\u6837\u672c\u4e0a\u4f7f\u7528LLM\uff0c\u5728\u516d\u4e2a\u6b27\u6d32\u8bed\u8a00\u7684\u5386\u53f2\u6587\u672c\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5386\u53f2\u6587\u672c\u7684\u5b9e\u4f53\u94fe\u63a5\u9762\u4e34\u8bed\u8a00\u53d8\u5f02\u3001\u566a\u58f0\u8f93\u5165\u548c\u8bed\u4e49\u6f14\u53d8\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u8981\u4e48\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u89c4\u5219\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u4f4e\u8d44\u6e90\u5386\u53f2\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u3002", "method": "\u63d0\u51faMHEL-LLaMo\u65e0\u76d1\u7763\u96c6\u6210\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u591a\u8bed\u8a00\u53cc\u7f16\u7801\u5668BELA\u8fdb\u884c\u5019\u9009\u5b9e\u4f53\u68c0\u7d22\uff1b2) \u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u7684LLM\u8fdb\u884cNIL\u9884\u6d4b\u548c\u5019\u9009\u9009\u62e9\uff08\u63d0\u793a\u94fe\uff09\uff1b3) \u5229\u7528SLM\u7f6e\u4fe1\u5ea6\u5206\u6570\u533a\u5206\u7b80\u5355\u548c\u56f0\u96be\u6837\u672c\uff0c\u4ec5\u5728\u56f0\u96be\u6837\u672c\u4e0a\u5e94\u7528LLM\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u907f\u514d\u7b80\u5355\u6837\u672c\u4e0a\u7684\u5e7b\u89c9\u3002", "result": "\u5728\u516d\u4e2a\u6b27\u6d32\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u82ac\u5170\u8bed\u3001\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u745e\u5178\u8bed\uff09\u7684\u56db\u4e2a\u5386\u53f2\u6587\u672c\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0cMHEL-LLaMo\u65e0\u9700\u5fae\u8c03\u5373\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u5386\u53f2\u5b9e\u4f53\u94fe\u63a5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MHEL-LLaMo\u901a\u8fc7\u7ed3\u5408SLM\u548cLLM\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5386\u53f2\u6587\u672c\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u573a\u666f\u3002\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2601.08510", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08510", "abs": "https://arxiv.org/abs/2601.08510", "authors": ["Qiuyu Tian", "Yiding Li", "Fengyi Chen", "Zequn Liu", "Youyong Kong", "Fan Guo", "Yuyao Li", "Jinjing Shen", "Zhijing Xie", "Yiyun Luo", "Xin Zhang"], "title": "STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays", "comment": "66 pages, 9 figures", "summary": "Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.", "AI": {"tldr": "STAGE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7535\u5f71\u5267\u672c\u53d9\u4e8b\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u3001\u573a\u666f\u4e8b\u4ef6\u6458\u8981\u3001\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u548c\u89d2\u8272\u626e\u6f14\u56db\u4e2a\u4efb\u52a1\uff0c\u57fa\u4e8e150\u90e8\u4e2d\u82f1\u6587\u7535\u5f71\u7684\u5267\u672c\u548c\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u95ee\u7b54\u6216\u5bf9\u8bdd\u751f\u6210\u7b49\u5355\u4e00\u5b50\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u6a21\u578b\u80fd\u5426\u6784\u5efa\u8fde\u8d2f\u6545\u4e8b\u4e16\u754c\u5e76\u5728\u591a\u79cd\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u80fd\u529b\u3002\u7535\u5f71\u5267\u672c\u4f5c\u4e3a\u957f\u7bc7\u53d9\u4e8b\u6587\u672c\uff0c\u5305\u542b\u590d\u6742\u7684\u4eba\u7269\u5173\u7cfb\u3001\u65f6\u5e8f\u4e8b\u4ef6\u548c\u5bf9\u8bdd\u4ea4\u4e92\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "STAGE\u5b9a\u4e49\u4e86\u56db\u4e2a\u57fa\u4e8e\u5171\u4eab\u53d9\u4e8b\u4e16\u754c\u8868\u793a\u7684\u4efb\u52a1\uff1a1) \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff1b2) \u573a\u666f\u7ea7\u4e8b\u4ef6\u6458\u8981\uff1b3) \u957f\u4e0a\u4e0b\u6587\u5267\u672c\u95ee\u7b54\uff1b4) \u5267\u672c\u5185\u89d2\u8272\u626e\u6f14\u3002\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86150\u90e8\u4e2d\u82f1\u6587\u7535\u5f71\u7684\u6e05\u6d17\u540e\u5267\u672c\u3001\u7cbe\u5fc3\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u4ee5\u53ca\u4e8b\u4ef6\u548c\u4eba\u7269\u4e2d\u5fc3\u7684\u6807\u6ce8\u3002", "result": "STAGE\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u6784\u5efa\u4e16\u754c\u8868\u793a\u3001\u62bd\u8c61\u548c\u9a8c\u8bc1\u53d9\u4e8b\u4e8b\u4ef6\u3001\u957f\u53d9\u4e8b\u63a8\u7406\u4ee5\u53ca\u751f\u6210\u89d2\u8272\u4e00\u81f4\u54cd\u5e94\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002", "conclusion": "STAGE\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u53d9\u4e8b\u4e16\u754c\u8868\u793a\u548c\u591a\u4efb\u52a1\u8bc4\u4f30\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6a21\u578b\u5bf9\u957f\u7bc7\u53d9\u4e8b\u6587\u672c\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u6545\u4e8b\u4e16\u754c\u4e00\u81f4\u6027\u548c\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u3002"}}
{"id": "2601.08332", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08332", "abs": "https://arxiv.org/abs/2601.08332", "authors": ["Ahmed A. Hashim", "Ali Al-Shuwaili", "Asraa Saeed", "Ali Al-Bayaty"], "title": "IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks", "comment": "11 pages, 6 figures", "summary": "Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.", "AI": {"tldr": "\u63d0\u51faIGAN\u6a21\u578b\uff0c\u7ed3\u5408Inception\u5377\u79ef\u548c\u7a7a\u6d1e\u5377\u79ef\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5728CUB-200\u548cImageNet\u6570\u636e\u96c6\u4e0aFID\u63d0\u534728-33%", "motivation": "\u73b0\u6709GAN\u6a21\u578b\uff08\u5982DCGAN\u3001BigGAN\u3001StyleGAN\uff09\u5728\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u5e73\u8861\u95ee\u9898\uff0c\u5bb9\u6613\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u548c\u4e0d\u7a33\u5b9a\u68af\u5ea6\u95ee\u9898", "method": "\u63d0\u51faInception GAN (IGAN)\u6a21\u578b\uff0c\u7ed3\u5408Inception\u542f\u53d1\u7684\u6df1\u5ea6\u5377\u79ef\u548c\u7a7a\u6d1e\u5377\u79ef\uff0c\u5728\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u4e2d\u4f7f\u7528dropout\u548c\u8c31\u5f52\u4e00\u5316\u6280\u672f", "result": "\u5728CUB-200\u6570\u636e\u96c6\u4e0aFID\u4e3a13.12\uff0cImageNet\u4e0a\u4e3a15.08\uff0c\u76f8\u6bd4SOTA\u63d0\u534728-33%\uff1bInception Score\u5206\u522b\u4e3a9.27\u548c68.25\uff0c\u663e\u793a\u56fe\u50cf\u591a\u6837\u6027\u548c\u8d28\u91cf\u63d0\u5347", "conclusion": "IGAN\u6a21\u578b\u80fd\u591f\u5e73\u8861\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6846\u67b6"}}
{"id": "2601.08418", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08418", "abs": "https://arxiv.org/abs/2601.08418", "authors": ["Jihang Li", "Qing Liu", "Zulong Chen", "Jing Wang", "Wei Wang", "Chuanfei Xu", "Zeyi Wen"], "title": "Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance", "comment": null, "summary": "Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.", "AI": {"tldr": "Taxon\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u5c42\u7a0e\u7801\u9884\u6d4b\u7684\u8bed\u4e49\u5bf9\u9f50\u4e13\u5bb6\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u95e8\u63a7\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548cLLM\u84b8\u998f\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6e90\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u963f\u91cc\u5df4\u5df4\u7a0e\u52a1\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u90e8\u7f72\u3002", "motivation": "\u7a0e\u7801\u9884\u6d4b\u662f\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u81ea\u52a8\u5f00\u7968\u548c\u5408\u89c4\u7ba1\u7406\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e0d\u8db3\u3002\u4ea7\u54c1\u9700\u8981\u51c6\u786e\u6620\u5c04\u5230\u56fd\u5bb6\u6807\u51c6\u5b9a\u4e49\u7684\u591a\u5c42\u5206\u7c7b\u5c42\u6b21\u4e2d\uff0c\u9519\u8bef\u4f1a\u5bfc\u81f4\u8d22\u52a1\u4e0d\u4e00\u81f4\u548c\u76d1\u7ba1\u98ce\u9669\u3002", "method": "1) \u7279\u5f81\u95e8\u63a7\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff1a\u81ea\u9002\u5e94\u5730\u5c06\u591a\u6a21\u6001\u7279\u5f81\u8def\u7531\u5230\u4e0d\u540c\u5206\u7c7b\u5c42\u6b21\uff1b2) \u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u578b\uff1a\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u800c\u6765\uff0c\u4f5c\u4e3a\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\u4ea7\u54c1\u6807\u9898\u4e0e\u5b98\u65b9\u7a0e\u5b9a\u4e49\u7684\u5bf9\u9f50\uff1b3) \u591a\u6e90\u8bad\u7ec3\u7ba1\u9053\uff1a\u7ed3\u5408\u7b56\u5212\u7684\u7a0e\u52a1\u6570\u636e\u5e93\u3001\u53d1\u7968\u9a8c\u8bc1\u65e5\u5fd7\u548c\u5546\u5bb6\u6ce8\u518c\u6570\u636e\uff0c\u63d0\u4f9b\u7ed3\u6784\u548c\u8bed\u4e49\u76d1\u7763\u3002", "result": "\u5728\u4e13\u6709TaxCode\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u3002\u5b8c\u6574\u5206\u5c42\u8def\u5f84\u91cd\u5efa\u7a0b\u5e8f\u663e\u8457\u63d0\u9ad8\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u83b7\u5f97\u6700\u9ad8\u603b\u4f53F1\u5206\u6570\u3002\u5df2\u5728\u963f\u91cc\u5df4\u5df4\u7a0e\u52a1\u7cfb\u7edf\u90e8\u7f72\uff0c\u65e5\u5747\u5904\u7406\u8d8550\u4e07\u7a0e\u7801\u67e5\u8be2\uff0c\u4e1a\u52a1\u9ad8\u5cf0\u671f\u8fbe500\u4e07\u4ee5\u4e0a\u8bf7\u6c42\uff0c\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u5747\u63d0\u5347\u3002", "conclusion": "Taxon\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7a0e\u7801\u9884\u6d4b\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u5c42\u6b21\u7ed3\u6784\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u5bb6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u7279\u5f81\u5904\u7406\u548cLLM\u77e5\u8bc6\u84b8\u998f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u7a0e\u7801\u9884\u6d4b\u7cfb\u7edf\uff0c\u5df2\u5728\u963f\u91cc\u5df4\u5df4\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u4e2d\u9a8c\u8bc1\u6709\u6548\u3002"}}
{"id": "2601.08375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08375", "abs": "https://arxiv.org/abs/2601.08375", "authors": ["Yuan Gao", "Di Cao", "Xiaohuan Xi", "Sheng Nie", "Shaobo Xia", "Cheng Wang"], "title": "Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation", "comment": null, "summary": "Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.", "AI": {"tldr": "\u63d0\u51faLoGo\u6846\u67b6\uff0c\u7528\u4e8e\u5730\u7406\u7a7a\u95f4\u70b9\u4e91\u7684\u65e0\u6e90\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u53cc\u5171\u8bc6\u673a\u5236\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5730\u7406\u7a7a\u95f4\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u9762\u4e34\u8de8\u533a\u57df\u57df\u504f\u79fb\u95ee\u9898\uff0c\u73b0\u6709\u57df\u9002\u5e94\u65b9\u6cd5\u9700\u8981\u6e90\u57df\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u9690\u79c1\u3001\u653f\u7b56\u548c\u4f20\u8f93\u9650\u5236\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u65e0\u6e90\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLoGo\u6846\u67b6\uff1a1) \u5c40\u90e8\u5c42\u9762\uff1a\u7c7b\u5e73\u8861\u539f\u578b\u4f30\u8ba1\u6a21\u5757\uff0c\u91c7\u7528\u7c7b\u5185\u72ec\u7acb\u951a\u70b9\u6316\u6398\u7b56\u7565\u800c\u975e\u5168\u5c40\u9608\u503c\u8fc7\u6ee4\uff1b2) \u5168\u5c40\u5c42\u9762\uff1a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5168\u5c40\u5206\u5e03\u5bf9\u9f50\u6a21\u5757\uff0c\u5c06\u4f2a\u6807\u7b7e\u5206\u914d\u5efa\u6a21\u4e3a\u5168\u5c40\u4f18\u5316\u95ee\u9898\uff1b3) \u53cc\u4e00\u81f4\u6027\u4f2a\u6807\u7b7e\u8fc7\u6ee4\u673a\u5236\uff0c\u4ec5\u4fdd\u7559\u5c40\u90e8\u591a\u589e\u5f3a\u96c6\u6210\u9884\u6d4b\u4e0e\u5168\u5c40\u6700\u4f18\u4f20\u8f93\u5206\u914d\u4e00\u81f4\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u7528\u4e8e\u81ea\u8bad\u7ec3\u3002", "result": "\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u65b9\u6cd5\u8bbe\u8ba1\u65e8\u5728\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u7684\u7279\u5f81\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u7ea0\u6b63\u5934\u7c7b\u522b\u7684\u8fc7\u5ea6\u4e3b\u5bfc\uff0c\u9632\u6b62\u6a21\u578b\u9884\u6d4b\u504f\u5411\u591a\u6570\u7c7b\u522b\u3002", "conclusion": "LoGo\u662f\u9488\u5bf9\u5730\u7406\u7a7a\u95f4\u70b9\u4e91\u8bbe\u8ba1\u7684\u521b\u65b0SFUDA\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u53cc\u5171\u8bc6\u673a\u5236\u6709\u6548\u5904\u7406\u57df\u504f\u79fb\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u65e0\u6e90\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.08631", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08631", "abs": "https://arxiv.org/abs/2601.08631", "authors": ["Yaohui Huang", "Runmin Zou", "Yun Wang", "Laeeq Aslam", "Ruipeng Dong"], "title": "M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting", "comment": "Accepted by AAAI 2026", "summary": "Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.", "AI": {"tldr": "M\u00b2FMoE\uff1a\u4e00\u79cd\u6781\u7aef\u81ea\u9002\u5e94\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u591a\u89c6\u56fe\u9891\u7387\u5efa\u6a21\u540c\u65f6\u5b66\u4e60\u5e38\u89c4\u548c\u6781\u7aef\u6a21\u5f0f\uff0c\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u65e0\u9700\u6781\u7aef\u4e8b\u4ef6\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u6781\u7aef\u4e8b\u4ef6\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u4e3a\u6781\u7aef\u4e8b\u4ef6\u5177\u6709\u9ad8\u65b9\u5dee\u3001\u4e0d\u89c4\u5219\u52a8\u6001\u548c\u7a00\u758f\u4f46\u9ad8\u5f71\u54cd\u7279\u6027\u3002\u867d\u7136\u6709\u4e9b\u65b9\u6cd5\u52a0\u5165\u8f85\u52a9\u4fe1\u53f7\uff0c\u4f46\u4ecd\u65e0\u6cd5\u6355\u6349\u6781\u7aef\u4e8b\u4ef6\u7684\u590d\u6742\u65f6\u95f4\u52a8\u6001\u3002", "method": "\u63d0\u51faM\u00b2FMoE\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1) \u591a\u89c6\u56fe\u9891\u7387\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\uff0c\u5728\u5085\u91cc\u53f6\u548c\u5c0f\u6ce2\u57df\u4e3a\u4e0d\u540c\u9891\u5e26\u5206\u914d\u4e13\u5bb6\uff1b2) \u591a\u5206\u8fa8\u7387\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u4ece\u7c97\u5230\u7ec6\u5c42\u6b21\u805a\u5408\u9891\u7387\u7279\u5f81\uff1b3) \u65f6\u95f4\u95e8\u63a7\u96c6\u6210\u6a21\u5757\uff0c\u52a8\u6001\u5e73\u8861\u957f\u671f\u8d8b\u52bf\u548c\u77ed\u671f\u9891\u7387\u611f\u77e5\u7279\u5f81\u3002", "result": "\u5728\u5177\u6709\u6781\u7aef\u6a21\u5f0f\u7684\u771f\u5b9e\u4e16\u754c\u6c34\u6587\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM\u00b2FMoE\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u4e0d\u9700\u8981\u6781\u7aef\u4e8b\u4ef6\u6807\u7b7e\u3002", "conclusion": "M\u00b2FMoE\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u591a\u89c6\u56fe\u9891\u7387\u5efa\u6a21\u6709\u6548\u6355\u6349\u6781\u7aef\u4e8b\u4ef6\u7684\u590d\u6742\u52a8\u6001\uff0c\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u9ad8\u65b9\u5dee\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.08484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08484", "abs": "https://arxiv.org/abs/2601.08484", "authors": ["MD Fatin Ishraque Ayon", "Sabrin Nahar", "Ataur Rahman", "Md. Taslim Arif", "Abdul Hasib", "A. S. M. Ahsanul Sarkar Akib"], "title": "An IoT-Enabled Smart Aquarium System for Real-Time Water Quality Monitoring and Automated Feeding", "comment": null, "summary": "Maintaining optimal water quality in aquariums is critical for aquatic health but remains challenging due to the need for continuous monitoring of multiple parameters. Traditional manual methods are inefficient, labor-intensive, and prone to human error, often leading to suboptimal aquatic conditions. This paper presents an IoT-based smart aquarium system that addresses these limitations by integrating an ESP32 microcontroller with multiple sensors (pH, TDS, temperature, turbidity) and actuators (servo feeder, water pump) for comprehensive real-time water quality monitoring and automated control. The system architecture incorporates edge processing capabilities, cloud connectivity via Blynk IoT platform, and an intelligent alert mechanism with configurable cooldown periods to prevent notification fatigue. Experimental evaluation in a 10-liter aquarium environment demonstrated the system's effectiveness, achieving 96\\% average sensor accuracy and 1.2-second response time for anomaly detection. The automated feeding and water circulation modules maintained 97\\% operational reliability throughout extended testing, significantly reducing manual intervention while ensuring stable aquatic conditions. This research demonstrates that cost-effective IoT solutions can revolutionize aquarium maintenance, making aquatic ecosystem management more accessible, reliable, and efficient for both residential and commercial applications.", "AI": {"tldr": "\u57fa\u4e8eESP32\u5fae\u63a7\u5236\u5668\u7684\u7269\u8054\u7f51\u667a\u80fd\u9c7c\u7f38\u7cfb\u7edf\uff0c\u96c6\u6210\u591a\u79cd\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\uff0c\u5b9e\u73b0\u6c34\u8d28\u5b9e\u65f6\u76d1\u6d4b\u4e0e\u81ea\u52a8\u5316\u63a7\u5236\uff0c\u901a\u8fc7\u4e91\u7aef\u5e73\u53f0\u7ba1\u7406\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u9c7c\u7f38\u6c34\u8d28\u6548\u7387\u4f4e\u3001\u52b3\u52a8\u5f3a\u5ea6\u5927\u4e14\u6613\u51fa\u9519\uff0c\u5bfc\u81f4\u6c34\u751f\u73af\u5883\u4e0d\u7406\u60f3\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7ef4\u6301\u6700\u4f73\u6c34\u8d28\u3002", "method": "\u91c7\u7528ESP32\u5fae\u63a7\u5236\u5668\u96c6\u6210pH\u3001TDS\u3001\u6e29\u5ea6\u3001\u6d4a\u5ea6\u4f20\u611f\u5668\u4ee5\u53ca\u4f3a\u670d\u5582\u98df\u5668\u3001\u6c34\u6cf5\u7b49\u6267\u884c\u5668\uff0c\u7ed3\u5408\u8fb9\u7f18\u5904\u7406\u80fd\u529b\u548cBlynk\u7269\u8054\u7f51\u5e73\u53f0\u4e91\u7aef\u8fde\u63a5\uff0c\u914d\u7f6e\u667a\u80fd\u8b66\u62a5\u673a\u5236\u548c\u51b7\u5374\u671f\u9632\u6b62\u901a\u77e5\u75b2\u52b3\u3002", "result": "\u572810\u5347\u9c7c\u7f38\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u5e73\u5747\u4f20\u611f\u5668\u51c6\u786e\u7387\u8fbe96%\uff0c\u5f02\u5e38\u68c0\u6d4b\u54cd\u5e94\u65f6\u95f41.2\u79d2\uff0c\u81ea\u52a8\u5316\u5582\u98df\u548c\u6c34\u5faa\u73af\u6a21\u5757\u8fd0\u884c\u53ef\u9760\u6027\u8fbe97%\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "\u4f4e\u6210\u672c\u7269\u8054\u7f51\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u9769\u65b0\u9c7c\u7f38\u7ef4\u62a4\uff0c\u4f7f\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u7ba1\u7406\u5bf9\u4f4f\u5b85\u548c\u5546\u4e1a\u5e94\u7528\u90fd\u66f4\u52a0\u4fbf\u6377\u3001\u53ef\u9760\u548c\u9ad8\u6548\u3002"}}
