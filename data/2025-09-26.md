<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization](https://arxiv.org/abs/2509.20785)
*Jincai Song,Haipeng Chen,Jun Qin,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对跨域半监督领域泛化(CD-SSDG)问题的双监督非对称协同训练框架DAC，解决了医学图像分割中标注数据有限且存在域偏移的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统SSDG方法假设每个源域的标注和未标注数据都可用，但实际中标注数据有限且存在域偏移。本文探索更实用的CD-SSDG场景，其中训练数据内部也存在域偏移。

Method: 提出DAC框架，基于双子模型协同训练，集成特征级监督和非对称辅助任务，通过特征空间互补监督解决伪标签不准确问题，并增强域不变特征学习。

Result: 在Fundus、Polyp和SCGM等真实医学图像分割数据集上的实验表明，DAC框架具有强大的泛化能力。

Conclusion: DAC框架有效解决了CD-SSDG问题，在存在域偏移的医学图像分割任务中表现出优异的性能。

Abstract: Semi-supervised domain generalization (SSDG) in medical image segmentation
offers a promising solution for generalizing to unseen domains during testing,
addressing domain shift challenges and minimizing annotation costs. However,
conventional SSDG methods assume labeled and unlabeled data are available for
each source domain in the training set, a condition that is not always met in
practice. The coexistence of limited annotation and domain shift in the
training set is a prevalent issue. Thus, this paper explores a more practical
and challenging scenario, cross-domain semi-supervised domain generalization
(CD-SSDG), where domain shifts occur between labeled and unlabeled training
data, in addition to shifts between training and testing sets. Existing SSDG
methods exhibit sub-optimal performance under such domain shifts because of
inaccurate pseudolabels. To address this issue, we propose a novel
dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.
Building upon the co-training paradigm with two sub-models offering cross
pseudo supervision, our DAC framework integrates extra feature-level
supervision and asymmetric auxiliary tasks for each sub-model. This
feature-level supervision serves to address inaccurate pseudo supervision
caused by domain shifts between labeled and unlabeled data, utilizing
complementary supervision from the rich feature space. Additionally, two
distinct auxiliary self-supervised tasks are integrated into each sub-model to
enhance domain-invariant discriminative feature learning and prevent model
collapse. Extensive experiments on real-world medical image segmentation
datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust
generalizability of the proposed DAC framework.

</details>


### [2] [Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification](https://arxiv.org/abs/2509.20899)
*Patrick Knab,Sascha Marton,Philipp J. Schubert,Drago Guggiana,Christian Bartelt*

Main category: cs.CV

TL;DR: 提出了MoTIF框架，将概念瓶颈模型扩展到视频分类领域，通过处理时间序列数据来捕捉动作和事件的时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型主要针对静态图像，无法有效处理视频数据中的时间依赖性，而时间依赖对于捕捉动作和事件至关重要。

Method: 基于Transformer架构设计MoTIF框架，处理任意长度的视频序列，通过识别重复出现的概念模式（motifs）来描述和解释动作。

Result: 概念建模范式可以有效地迁移到视频数据，在保持竞争力的性能的同时，更好地理解时间上下文中概念的贡献。

Conclusion: MoTIF成功地将概念瓶颈模型扩展到视频领域，提供了全局概念重要性、局部概念相关性和时间依赖性的多视角分析能力。

Abstract: Conceptual models such as Concept Bottleneck Models (CBMs) have driven
substantial progress in improving interpretability for image classification by
leveraging human-interpretable concepts. However, extending these models from
static images to sequences of images, such as video data, introduces a
significant challenge due to the temporal dependencies inherent in videos,
which are essential for capturing actions and events. In this work, we
introduce MoTIF (Moving Temporal Interpretable Framework), an architectural
design inspired by a transformer that adapts the concept bottleneck framework
for video classification and handles sequences of arbitrary length. Within the
video domain, concepts refer to semantic entities such as objects, attributes,
or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across
time - forming motifs collectively describing and explaining actions. Our
design explicitly enables three complementary perspectives: global concept
importance across the entire video, local concept relevance within specific
windows, and temporal dependencies of a concept over time. Our results
demonstrate that the concept-based modeling paradigm can be effectively
transferred to video data, enabling a better understanding of concept
contributions in temporal contexts while maintaining competitive performance.
Code available at github.com/patrick-knab/MoTIF.

</details>


### [3] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff是一种模拟器约束的扩散模型，通过将环境参数直接集成到去噪过程中，无需在推理时重复调用模拟器即可高效生成物理上合理的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将基于模拟器的运动投影层整合到扩散过程中以确保物理合理性，但由于模拟器的顺序性质，这些方法计算成本高昂且无法并行化。

Method: 将模拟器运动投影解释为扩散过程中的引导形式，提出SimDiff模型，通过将环境参数（如重力、风力）直接条件化到去噪过程中，避免推理时的重复模拟器调用。

Result: SimDiff能够高效生成物理上合理的运动，并提供对不同物理系数的细粒度控制，同时成功泛化到未见过的环境参数组合，展示了组合泛化能力。

Conclusion: SimDiff通过将物理约束直接集成到扩散模型中，实现了高效且可控的物理合理运动生成，解决了现有模拟器方法计算效率低的问题。

Abstract: Generating physically plausible human motion is crucial for applications such
as character animation and virtual reality. Existing approaches often
incorporate a simulator-based motion projection layer to the diffusion process
to enforce physical plausibility. However, such methods are computationally
expensive due to the sequential nature of the simulator, which prevents
parallelization. We show that simulator-based motion projection can be
interpreted as a form of guidance, either classifier-based or classifier-free,
within the diffusion process. Building on this insight, we propose SimDiff, a
Simulator-constrained Diffusion Model that integrates environment parameters
(e.g., gravity, wind) directly into the denoising process. By conditioning on
these parameters, SimDiff generates physically plausible motions efficiently,
without repeated simulator calls at inference, and also provides fine-grained
control over different physical coefficients. Moreover, SimDiff successfully
generalizes to unseen combinations of environmental parameters, demonstrating
compositional generalization.

</details>


### [4] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: 提出SNCE方法，通过操纵单个神经元精确防止有害内容生成，在保持图像质量的同时实现概念擦除


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型存在生成有害内容的安全风险，现有概念擦除方法难以在精确移除目标概念的同时最小化图像质量下降

Method: 训练稀疏自编码器将文本嵌入映射到稀疏解耦的潜在空间，设计基于调制频率评分的神经元识别方法定位有害概念相关神经元，通过抑制特定神经元激活实现精确概念擦除

Result: 在多个基准测试中达到最先进的目标概念擦除效果，同时保持非目标概念的生成能力，对对抗攻击具有强鲁棒性

Conclusion: SNCE方法通过单神经元操作实现了精确的概念擦除，在安全性和图像质量之间取得了良好平衡

Abstract: Text-to-image models exhibit remarkable capabilities in image generation.
However, they also pose safety risks of generating harmful content. A key
challenge of existing concept erasure methods is the precise removal of target
concepts while minimizing degradation of image quality. In this paper, we
propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can
precisely prevent harmful content generation by manipulating only a single
neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text
embeddings into a sparse, disentangled latent space, where individual neurons
align tightly with atomic semantic concepts. To accurately locate neurons
responsible for harmful concepts, we design a novel neuron identification
method based on the modulated frequency scoring of activation patterns. By
suppressing activations of the harmful concept-specific neuron, SNCE achieves
surgical precision in concept erasure with minimal disruption to image quality.
Experiments on various benchmarks demonstrate that SNCE achieves
state-of-the-art results in target concept erasure, while preserving the
model's generation capabilities for non-target concepts. Additionally, our
method exhibits strong robustness against adversarial attacks, significantly
outperforming existing methods.

</details>


### [5] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出MedVSR框架，解决医学视频超分辨率中的对齐困难和伪影问题，通过跨状态空间传播和内部状态空间重建模块提升重建质量


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频对准确诊断至关重要，但受硬件限制难以获取。现有VSR模型在处理医学视频时面临相机抖动、噪声、帧间突变等独特挑战，导致光流误差大、对齐困难，且容易产生伪影和扭曲特征误导医生

Method: 提出MedVSR框架：1）跨状态空间传播（CSSP）将远距离帧投影为状态空间模型中的控制矩阵，选择性传播一致且信息丰富的特征以实现有效对齐；2）内部状态空间重建（ISSR）模块通过联合长程空间特征学习和大核短程信息聚合来增强组织结构并减少伪影

Result: 在包括内窥镜和白内障手术在内的四种医学场景数据集上的实验表明，MedVSR在重建性能和效率方面显著优于现有VSR模型

Conclusion: MedVSR通过专门设计的对齐和重建机制，有效解决了医学视频超分辨率的独特挑战，为临床诊断提供了更高质量的医学视频重建方案

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641)
*Giovana Morais,Magdalena Fuentes*

Main category: cs.LG

TL;DR: 本文研究了音频大语言模型是否真正在听音频还是仅依赖文本推理，通过MM-SHAP框架量化各模态对模型输出的贡献。


<details>
  <summary>Details</summary>
Motivation: 音频大语言模型能够进行音乐对话，但不确定它们是否真正在听音频内容，还是主要依赖文本信息进行推理。

Method: 采用MM-SHAP框架（基于Shapley值的性能无关评分），在MuChoMusic基准上评估两个模型，量化文本和音频模态对模型预测的相对贡献。

Result: 准确率更高的模型更多依赖文本来回答问题，但进一步检查发现即使音频整体贡献较低，模型仍能成功定位关键声音事件，表明音频信息并未被完全忽略。

Conclusion: 这是MM-SHAP在音频大语言模型中的首次应用，为可解释AI和音频研究的未来发展奠定了基础。

Abstract: Audio Large Language Models (Audio LLMs) enable human-like conversation about
music, yet it is unclear if they are truly listening to the audio or just using
textual reasoning, as recent benchmarks suggest. This paper investigates this
issue by quantifying the contribution of each modality to a model's output. We
adapt the MM-SHAP framework, a performance-agnostic score based on Shapley
values that quantifies the relative contribution of each modality to a model's
prediction. We evaluate two models on the MuChoMusic benchmark and find that
the model with higher accuracy relies more on text to answer questions, but
further inspection shows that even if the overall audio contribution is low,
models can successfully localize key sound events, suggesting that audio is not
entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs
and we hope it will serve as a foundational step for future research in
explainable AI and audio.

</details>


### [7] [FERD: Fairness-Enhanced Data-Free Robustness Distillation](https://arxiv.org/abs/2509.20793)
*Zhengxiao Li,Liming Lu,Xu Zheng,Siyuan Liang,Zhenghan Chen,Yongbin Zhou,Shuchao Pang*

Main category: cs.LG

TL;DR: 本文提出了FERD框架，首个关注公平性的数据自由鲁棒性蒸馏方法，通过调整对抗样本的比例和分布来解决不同类别间鲁棒性差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由鲁棒性蒸馏方法只关注整体鲁棒性，忽视了不同类别间的鲁棒性公平问题，导致学生模型在不同类别上表现出显著的鲁棒性差异。

Method: FERD采用鲁棒性引导的类别重加权策略调整样本比例，生成公平感知示例(FAEs)和统一目标对抗示例(UTAEs)来平衡特征表示和攻击目标分布。

Result: 在三个公开数据集上的实验表明，FERD在所有对抗攻击下都达到了最先进的最差类别鲁棒性，在CIFAR-10上使用MobileNet-V2时，FGSM和AutoAttack下的最差类别鲁棒性分别提升了15.1%和6.4%。

Conclusion: FERD框架在鲁棒性和公平性方面都表现出优越性能，有效解决了数据自由鲁棒性蒸馏中的公平性问题。

Abstract: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from
the teacher to the student without accessing the training data. While existing
methods focus on overall robustness, they overlook the robust fairness issues,
leading to severe disparity of robustness across different categories. In this
paper, we find two key problems: (1) student model distilled with equal class
proportion data behaves significantly different across distinct categories; and
(2) the robustness of student model is not stable across different attacks
target. To bridge these gaps, we present the first Fairness-Enhanced data-free
Robustness Distillation (FERD) framework to adjust the proportion and
distribution of adversarial examples. For the proportion, FERD adopts a
robustness-guided class reweighting strategy to synthesize more samples for the
less robust categories, thereby improving robustness of them. For the
distribution, FERD generates complementary data samples for advanced robustness
distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a
uniformity constraint on feature-level predictions, which suppress the
dominance of class-specific non-robust features, providing a more balanced
representation across all categories. Then, FERD constructs Uniform-Target
Adversarial Examples (UTAEs) from FAEs by applying a uniform target class
constraint to avoid biased attack directions, which distribute the attack
targets across all categories and prevents overfitting to specific vulnerable
categories. Extensive experiments on three public datasets show that FERD
achieves state-of-the-art worst-class robustness under all adversarial attack
(e.g., the worst-class robustness under FGSM and AutoAttack are improved by
15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior
performance in both robustness and fairness aspects.

</details>


### [8] [Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition](https://arxiv.org/abs/2509.20840)
*Jiaqi Tang,Yinsong Xu,Yang Liu,Qingchao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段训练框架来解决多模态融合中的模态竞争问题，通过单模态训练来塑造初始状态，然后进行联合训练。


<details>
  <summary>Details</summary>
Motivation: 多模态融合在联合训练中经常出现模态竞争问题，其中一个模态主导学习过程，导致其他模态优化不足。现有方法大多在联合学习阶段解决此问题，而忽视了模型初始状态的关键影响。

Method: 提出了两阶段训练框架：1）单模态训练阶段塑造初始状态；2）开发了有效竞争强度（ECS）概念来量化模态竞争强度；3）提出FastPID作为可计算诊断指标，通过部分信息分解测量模态特异性独特性、冗余性和协同性；4）异步训练控制器动态平衡模态并定位理想初始状态。

Result: 在多个基准测试上的实验表明，该方法实现了最先进的性能。

Conclusion: 研究表明，塑造预融合模型的初始状态是一种有效策略，可以在竞争开始前缓解模态竞争，可靠地实现协同多模态融合。

Abstract: Multi-modal fusion often suffers from modality competition during joint
training, where one modality dominates the learning process, leaving others
under-optimized. Overlooking the critical impact of the model's initial state,
most existing methods address this issue during the joint learning stage. In
this study, we introduce a two-stage training framework to shape the initial
states through unimodal training before the joint training. First, we propose
the concept of Effective Competitive Strength (ECS) to quantify a modality's
competitive strength. Our theoretical analysis further reveals that properly
shaping the initial ECS by unimodal training achieves a provably tighter error
bound. However, ECS is computationally intractable in deep neural networks. To
bridge this gap, we develop a framework comprising two core components: a
fine-grained computable diagnostic metric and an asynchronous training
controller. For the metric, we first prove that mutual information(MI) is a
principled proxy for ECS. Considering MI is induced by per-modality marginals
and thus treats each modality in isolation, we further propose FastPID, a
computationally efficient and differentiable solver for partial information
decomposition, which decomposes the joint distribution's information into
fine-grained measurements: modality-specific uniqueness, redundancy, and
synergy. Guided by these measurements, our asynchronous controller dynamically
balances modalities by monitoring uniqueness and locates the ideal initial
state to start joint training by tracking peak synergy. Experiments on diverse
benchmarks demonstrate that our method achieves state-of-the-art performance.
Our work establishes that shaping the pre-fusion models' initial state is a
powerful strategy that eases competition before it starts, reliably unlocking
synergistic multi-modal fusion.

</details>


### [9] [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](https://arxiv.org/abs/2509.21161)
*Giuseppe Serra,Florian Buettner*

Main category: cs.LG

TL;DR: 本文提出DATS方法，通过距离感知的温度缩放来解决持续学习中的校准问题，无需任务信息即可自适应调整温度参数，显著降低跨任务的校准误差。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要从数据角度解决校准问题，使用单一温度参数忽略了任务特异性，导致不同任务间校准误差波动较大。在安全关键应用中，模型不仅需要预测性能，还需要可靠的不确定性校准。

Method: 提出距离感知温度缩放(DATS)，结合原型距离估计和距离感知校准，无需先验任务信息即可推断任务接近度并分配自适应温度参数。

Result: 在标准基准测试和真实世界不平衡生物医学数据集上的广泛实验表明，DATS方法相比最先进方法在降低跨任务校准误差方面表现稳定、可靠且一致。

Conclusion: DATS方法通过距离感知的温度缩放有效解决了持续学习中的校准问题，为安全关键应用提供了更可靠的模型不确定性评估。

Abstract: Continual Learning (CL) is recently gaining increasing attention for its
ability to enable a single model to learn incrementally from a sequence of new
classes. In this scenario, it is important to keep consistent predictive
performance across all the classes and prevent the so-called Catastrophic
Forgetting (CF). However, in safety-critical applications, predictive
performance alone is insufficient. Predictive models should also be able to
reliably communicate their uncertainty in a calibrated manner - that is, with
confidence scores aligned to the true frequencies of target events. Existing
approaches in CL address calibration primarily from a data-centric perspective,
relying on a single temperature shared across all tasks. Such solutions
overlook task-specific differences, leading to large fluctuations in
calibration error across tasks. For this reason, we argue that a more
principled approach should adapt the temperature according to the distance to
the current task. However, the unavailability of the task information at test
time/during deployment poses a major challenge to achieve the intended
objective. For this, we propose Distance-Aware Temperature Scaling (DATS),
which combines prototype-based distance estimation with distance-aware
calibration to infer task proximity and assign adaptive temperatures without
prior task information. Through extensive empirical evaluation on both standard
benchmarks and real-world, imbalanced datasets taken from the biomedical
domain, our approach demonstrates to be stable, reliable and consistent in
reducing calibration error across tasks compared to state-of-the-art
approaches.

</details>


### [10] [Federated Flow Matching](https://arxiv.org/abs/2509.21250)
*Zifan Wang,Anqi Dong,Mahmoud Selim,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出了联邦流匹配（FFM）框架，用于在隐私约束下训练流匹配模型，解决了分布式数据无法集中训练的问题。


<details>
  <summary>Details</summary>
Motivation: 当前数据分散在设备和机构中，隐私、所有权和法规限制阻止了数据集中化，因此需要在不进行中心化聚合的情况下直接从分布式数据本地训练生成模型。

Method: 提出了三种方法：FFM-vanilla（本地独立训练）、FFM-LOT（使用局部最优传输改进流直度）和FFM-GOT（基于最优传输半对偶公式的联邦策略，通过共享全局势函数协调客户端耦合）。

Result: 在合成和图像数据集上的实验表明，FFM能够在保护隐私的同时提高流直度和样本质量，性能与集中式基线相当。

Conclusion: FFM框架成功实现了隐私保护下的分布式训练，在联邦设置下提升了流匹配模型的性能。

Abstract: Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.

</details>


### [11] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: 提出了PSPO方法，通过概率平滑替代传统的比率裁剪，在保持梯度信号的同时创建软信任区域，显著提升大语言模型在推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法如PPO和GRPO依赖比率裁剪来稳定更新，但裁剪会丢弃信息并引入梯度不连续性。需要一种既能防止不稳定更新又能保留更多信息的方法。

Method: 提出概率平滑策略优化（PSPO），在计算重要性比率之前将当前策略的概率向旧策略平滑，类似于标签平滑。该方法在GRPO框架中实现为GR-PSPO。

Result: 在GSM8K、SVAMP、ASDiv和MATH-500等数据集上的实验表明，相比裁剪的GRPO，GR-PSPO在0.5B和1.5B模型上性能提升超过20%（0.5B: 39.7% vs 17.6%；1.5B: 59.4% vs 37.8%），推理过程更清晰简洁。

Conclusion: PSPO通过概率平滑有效解决了传统裁剪方法的局限性，在保持训练稳定性的同时显著提升模型性能，为语言模型强化学习提供了更优的优化策略。

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种用于时间触发系统的动态调度重建框架，通过将AI生成或启发式调度优先级转换为可执行调度，解决消息碰撞、循环锁定等问题，确保系统安全约束。


<details>
  <summary>Details</summary>
Motivation: 时间触发系统在动态环境中面临消息碰撞、循环锁定和无效调度等挑战，这些会危及系统安全和性能。

Method: 开发重建模型，通过系统化转换调度优先级为可执行调度，包含安全检查、高效分配算法和恢复机制，处理硬件故障和模式转换等意外事件。

Result: 实验表明该框架显著提升系统适应性、操作完整性和运行时性能，同时保持计算效率。

Conclusion: 为安全关键时间触发系统提供了实用可扩展的安全调度生成解决方案，支持在高度动态不确定环境下的可靠实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [13] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本文提出了一种集成在元调度器中的自适应在线学习单元，通过强化学习实时探索新的调度解决方案，解决传统离线训练AI调度推理时构建全面多调度图的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练方法在构建考虑所有可能场景的多调度图时面临资源密集和不可行的挑战，特别是在处理硬件故障、松弛变化或模式变化等上下文事件时。离线训练生成的多调度图只是完整空间的子集，仅关注最可能和关键的上下文事件。

Method: 在元调度器中集成自适应在线学习单元，使用强化学习模型在在线模式下持续探索和发现新的调度解决方案，从而扩展多调度图并随时间提升系统性能。

Result: 在线学习单元中的多个强化学习模型不仅促进了新解决方案的发现，还在引入更严格截止时间或新性能标准时优化了现有调度器。

Conclusion: 通过实时训练持续优化AI推理，系统保持灵活性并能够满足不断变化的需求，从而确保在大规模安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [14] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 提出Reflective Cognitive Architecture (RCA)框架，通过协调多个LLM从直接经验中学习，实现高精度预测和高质量解释的平衡，在医疗疾病预测中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和大语言模型方法难以平衡预测准确性和可解释性，要么准确但解释不清晰，要么解释流畅但统计支持不足。需要开发能像人类专家一样深度理解数据的模型。

Method: RCA框架包含迭代规则精炼机制（从预测错误中改进逻辑）和分布感知规则检查机制（基于数据集全局统计进行推理），通过预测准确性驱动深度理解。

Result: 在1个私有和2个公共数据集上评估，相比22个基线模型，RCA在准确性和鲁棒性上相对提升达40%，并能生成清晰、逻辑、基于证据且平衡的解释。

Conclusion: RCA通过深度数据理解实现了高准确性和高质量解释的协同提升，为创建真正可信赖的临床决策支持系统展示了潜力。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出了一个新颖框架，通过修改外交事件叙述来将公众情绪从负面转向中性或正面，利用语言模型预测公众反应并生成反事实文本修改。


<details>
  <summary>Details</summary>
Motivation: 公众情绪在外交中至关重要，但传统调查方法耗时费力且缺乏前瞻性分析能力，需要更高效的数据驱动方法来指导外交沟通策略。

Method: 首先训练语言模型预测外交事件的公众反应，构建包含事件描述和公众讨论的数据集；然后基于传播理论和专家意见确定文本修改特征，开发反事实生成算法系统性地产生修改版本。

Result: 该框架成功将公众情绪转向更有利状态，成功率高达70%。

Conclusion: 该框架可作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，帮助更好地构建外交倡议或事件报道以培养理想的公众情绪。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [16] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 该论文研究发现LLM在训练过程中会学习到语法模板与领域之间的虚假相关性，这种相关性有时会覆盖提示语义，影响模型性能，甚至可能被用于绕过安全微调。


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何同时处理任务指令对的语义、领域和语法信息，特别是语法可能传达的隐含信息及其对模型行为的影响。

Method: 使用合成训练数据集分析语法-领域相关性对OLMo-2模型性能的影响，建立评估框架检测预训练模型中的这种现象，并通过案例研究探讨其对安全微调的影响。

Result: 语法-领域相关性会降低OLMo-2模型在实体知识任务上的性能（平均0.51±0.06），在FlanV2数据集的子集上，这种现象出现在开源（OLMo-2-7B；Llama-4-Maverick）和闭源（GPT-4o）模型中，并可被用于绕过OLMo-2-7B Instruct和GPT-4o的拒绝机制。

Conclusion: 需要明确测试语法-领域相关性，并确保训练数据中特别是领域内的语法多样性，以防止此类虚假相关性。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework](https://arxiv.org/abs/2509.20705)
*Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti*

Main category: cs.RO

TL;DR: BIM2RDT框架通过AI技术将静态BIM模型转化为动态的机器人就绪数字孪生，整合几何语义、IoT传感器和机器人视觉数据，提出SG-ICP点云配准算法提升对齐精度，并集成实时安全监控。


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业BIM数据与实时现场条件之间的差距，提升施工安全性和数字化管理水平。

Method: 采用语义重力ICP算法利用LLM推理推断物体方向先验，结合YOLOE目标检测和Shi-Tomasi角点检测，集成实时手部振动监测。

Result: SG-ICP相比标准ICP在特征遮挡场景下对齐误差降低64.3%-88.3%，实时安全监控触发警告提升ISO 5349-1合规性。

Conclusion: BIM2RDT框架成功实现了从静态BIM到动态数字孪生的转化，显著提升了施工安全性和机器人操作精度。

Abstract: The adoption of cyber-physical systems and jobsite intelligence that connects
design models, real-time site sensing, and autonomous field operations can
dramatically enhance digital management in the construction industry. This
paper introduces BIM2RDT (Building Information Models to Robot-Ready Site
Digital Twins), an agentic artificial intelligence (AI) framework designed to
transform static Building Information Modeling (BIM) into dynamic, robot-ready
digital twins (DTs) that prioritize safety during execution. The framework
bridges the gap between pre-existing BIM data and real-time site conditions by
integrating three key data streams: geometric and semantic information from BIM
models, activity data from IoT sensor networks, and visual-spatial data
collected by robots during site traversal. The methodology introduces
Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that
leverages large language model (LLM) reasoning. Unlike traditional methods,
SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors
based on BIM semantics, improving alignment accuracy by avoiding convergence on
local minima. This creates a feedback loop where robot-collected data updates
the DT, which in turn optimizes paths for missions. The framework employs YOLOE
object detection and Shi-Tomasi corner detection to identify and track
construction elements while using BIM geometry as a priori maps. The framework
also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping
sensor-detected safety events to the digital twin using IFC standards for
intervention. Experiments demonstrate SG-ICP's superiority over standard ICP,
achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with
occluded features, ensuring plausible orientations. HAV integration triggers
warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.

</details>
