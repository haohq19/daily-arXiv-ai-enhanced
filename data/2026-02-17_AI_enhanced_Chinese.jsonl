{"id": "2602.13455", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13455", "abs": "https://arxiv.org/abs/2602.13455", "authors": ["Phyllis Nabangi", "Abdul-Jalil Zakaria", "Jema David Ndibwile"], "title": "Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety", "comment": "Accepted at the Second IJCAI AI for Good Symposium in Africa, hosted by Deep Learning Indaba, 7 pages, 1 figure", "summary": "The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East.\n  We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language.\n  This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u65af\u74e6\u5e0c\u91cc\u8bed\u4e2d\u7684\u7f51\u7edc\u6b3a\u51cc\u548c\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08SVM\u3001\u903b\u8f91\u56de\u5f52\u3001\u51b3\u7b56\u6811\uff09\u7ed3\u5408SMOTE\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u96c6\u5c0f\u4e14\u4e0d\u5e73\u8861\uff0c\u7ed3\u679c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u6570\u5b57\u6280\u672f\u53d1\u5c55\u589e\u52a0\u4e86\u7f51\u7edc\u6b3a\u51cc\u98ce\u9669\uff0c\u9700\u8981\u52a0\u5f3a\u68c0\u6d4b\u9884\u9632\u63aa\u65bd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u513f\u7ae5\u3002\u65af\u74e6\u5e0c\u91cc\u8bed\u4f5c\u4e3a\u975e\u6d32\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\uff08\u8d85\u8fc71\u4ebf\u4f7f\u7528\u8005\uff09\uff0c\u4f46\u5c5e\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u8bed\u8a00\u8d44\u6e90\u548c\u6280\u672f\u652f\u6301\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u73b0\u5b9e\u610f\u4e49\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5305\u62ec\u652f\u6301\u5411\u91cf\u673a(SVM)\u3001\u903b\u8f91\u56de\u5f52\u548c\u51b3\u7b56\u6811\uff0c\u901a\u8fc7\u53c2\u6570\u8c03\u4f18\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u7528SMOTE\uff08\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u6280\u672f\uff09\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5168\u9762\u5206\u6790\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u3002", "result": "\u6a21\u578b\u5728\u9ad8\u7ef4\u6587\u672c\u6570\u636e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u4e0d\u5e73\u8861\uff0c\u7814\u7a76\u7ed3\u679c\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u5230\u9650\u5236\u3002\u5404\u6a21\u578b\u5728\u68c0\u6d4b\u6a21\u7cca\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6027\u80fd\u7279\u70b9\uff0c\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u5206\u6790\u6765\u8bc4\u4f30\u5176\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u513f\u7ae5\u5728\u7ebf\u5b89\u5168\u73af\u5883\u5efa\u8bbe\u505a\u51fa\u8d21\u732e\uff0c\u5efa\u8bae\u6269\u5927\u6570\u636e\u96c6\u3001\u91c7\u7528\u66f4\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u63d0\u9ad8\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u4e2d\u5728\u589e\u5f3a\u6570\u636e\u9c81\u68d2\u6027\u3001\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u548c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u4ee5\u521b\u5efa\u66f4\u5168\u9762\u548c\u6587\u5316\u654f\u611f\u7684\u68c0\u6d4b\u673a\u5236\u3002"}}
{"id": "2602.13217", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13217", "abs": "https://arxiv.org/abs/2602.13217", "authors": ["Zerui Cheng", "Jiashuo Liu", "Chunjie Wu", "Jianzhu Yao", "Pramod Viswanath", "Ge Zhang", "Wenhao Huang"], "title": "VeRA: Verified Reasoning Data Augmentation at Scale", "comment": "36 pages; VeRA technical report", "summary": "The main issue with most evaluation schemes today is their \"static\" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.\n  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.\n  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.", "AI": {"tldr": "VeRA\u63d0\u51fa\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u63a8\u7406\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u5c06\u9759\u6001\u57fa\u51c6\u95ee\u9898\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u89c4\u8303\uff0c\u81ea\u52a8\u751f\u6210\u65e0\u9650\u9a8c\u8bc1\u53d8\u4f53\uff0c\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u65b9\u6848\u56e0\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u95ee\u9898\u5bfc\u81f4\u7684\u8bb0\u5fc6\u5316\u3001\u683c\u5f0f\u5229\u7528\u548c\u9971\u548c\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u8bc4\u4f30\u65b9\u6848\u5b58\u5728\"\u9759\u6001\"\u672c\u8d28\u95ee\u9898\uff1a\u76f8\u540c\u95ee\u9898\u88ab\u91cd\u590d\u4f7f\u7528\uff0c\u5bfc\u81f4\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8bb0\u5fc6\u5316\u3001\u683c\u5f0f\u5229\u7528\u7b49\u65b9\u5f0f\u83b7\u5f97\u9ad8\u5206\uff0c\u800c\u975e\u771f\u6b63\u5c55\u793a\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u963b\u788d\u4e86\u51c6\u786e\u8861\u91cfAI\u771f\u5b9e\u8fdb\u5c55\uff0c\u9700\u8981\u6784\u5efa\u6027\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u800c\u975e\u4e8b\u540e\u68c0\u6d4b\u3002", "method": "VeRA\u6846\u67b6\u5c06\u57fa\u51c6\u95ee\u9898\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u89c4\u8303\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1)\u5e26\u5360\u4f4d\u7b26\u69fd\u7684\u81ea\u7136\u8bed\u8a00\u6a21\u677f\uff1b(2)\u91c7\u6837\u6709\u6548\u914d\u7f6e\u7684\u4e00\u81f4\u6027\u751f\u6210\u5668\uff1b(3)\u9a8c\u8bc1\u53c2\u6570\u5e76\u8ba1\u7b97\u6b63\u786e\u7b54\u6848\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u5668\u3002\u6846\u67b6\u63d0\u4f9b\u4e24\u79cd\u6a21\u5f0f\uff1aVeRA-E\uff08\u7b49\u4ef7\uff09\u4fdd\u6301\u5e95\u5c42\u903b\u8f91\u4e0d\u53d8\uff0c\u7528\u4e8e\u68c0\u6d4b\u8bb0\u5fc6\u5316\uff1bVeRA-H\uff08\u786c\u5316\uff09\u7cfb\u7edf\u589e\u52a0\u590d\u6742\u5ea6\uff0c\u751f\u6210\u56f0\u96be\u4efb\u52a1\u3002", "result": "\u8bc4\u4f3016\u4e2a\u524d\u6cbf\u6a21\u578b\u53d1\u73b0\uff1a(1)VeRA-E\u63d0\u9ad8\u8bc4\u4f30\u8d28\u91cf\u5e76\u63ed\u793a\u6c61\u67d3\u6a21\u5f0f\uff1b(2)VeRA-H\u80fd\u591f\u65e0\u9700\u4eba\u5de5\u751f\u6210\u5177\u6709\u53ef\u9760\u6807\u7b7e\u7684\u56f0\u96be\u4efb\u52a1\uff1b(3)VeRA\u786e\u7acb\u4e86\u9a8c\u8bc1\u57fa\u51c6\u4f5c\u4e3a\u901a\u7528\u8303\u5f0f\u3002\u6846\u67b6\u5c06\u57fa\u51c6\u4ece\u9759\u6001\u5bf9\u8c61\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u6309\u9700\u751f\u6210\u65b0\u9c9c\u9a8c\u8bc1\u5b9e\u4f8b\u7684\u53ef\u6267\u884c\u89c4\u8303\u3002", "conclusion": "VeRA\u91cd\u65b0\u5b9a\u4e49\u4e86\u57fa\u51c6\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ece\u9759\u6001\u8017\u5c3d\u5f0f\u4f7f\u7528\u8f6c\u53d8\u4e3a\u6309\u9700\u751f\u6210\u65b0\u9c9c\u9a8c\u8bc1\u5b9e\u4f8b\u7684\u53ef\u6267\u884c\u89c4\u8303\uff0c\u589e\u5f3a\u4e86\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u548c\u6210\u672c\u6548\u76ca\u3002\u5728\u53ef\u9a8c\u8bc1\u9886\u57df\uff0c\u8bc4\u4f30\u53ef\u4ee5\u65e0\u9650\u6269\u5c55\u800c\u4e0d\u727a\u7272\u6807\u7b7e\u5b8c\u6574\u6027\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.13567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13567", "abs": "https://arxiv.org/abs/2602.13567", "authors": ["Manish Dhakal", "Uthman Jinadu", "Anjila Budathoki", "Rajshekhar Sunderraman", "Yi Ding"], "title": "DistillLens: Symmetric Knowledge Distillation Through Logit Lens", "comment": "Knowledge Distillation in LLMs", "summary": "Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.", "AI": {"tldr": "DistillLens\u662f\u4e00\u4e2a\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7Logit Lens\u5c06\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u6295\u5f71\u5230\u8bcd\u6c47\u7a7a\u95f4\uff0c\u4f7f\u7528\u5bf9\u79f0\u6563\u5ea6\u76ee\u6807\u5bf9\u9f50\u5e08\u751f\u6a21\u578b\u7684\u601d\u7ef4\u8fc7\u7a0b\uff0c\u907f\u514d\u8fc7\u81ea\u4fe1\u548c\u6b20\u81ea\u4fe1\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u77e5\u8bc6\u84b8\u998f\u53ea\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\uff0c\u5c06\u6559\u5e08\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u601d\u7ef4\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u76d2\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u84b8\u998f\u65b9\u6cd5\uff08\u5982MSE\u548c\u975e\u5bf9\u79f0KL\u6563\u5ea6\uff09\u5ffd\u7565\u4e86\u6700\u7ec8\u8f93\u51fa\u6240\u9700\u7684\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u3002", "method": "\u901a\u8fc7Logit Lens\u5c06\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u6295\u5f71\u5230\u8bcd\u6c47\u7a7a\u95f4\uff0c\u4f7f\u7528\u5bf9\u79f0\u6563\u5ea6\u76ee\u6807\u5f3a\u5236\u7ed3\u6784\u5bf9\u9f50\uff0c\u8fd9\u79cd\u7ea6\u675f\u65bd\u52a0\u53cc\u5411\u60e9\u7f5a\uff0c\u9632\u6b62\u8fc7\u81ea\u4fe1\u548c\u6b20\u81ea\u4fe1\uff0c\u540c\u65f6\u4fdd\u7559\u6700\u7ec8\u63a8\u7406\u6240\u9700\u7684\u9ad8\u71b5\u4fe1\u606f\u901a\u9053\u3002", "result": "\u5728GPT-2\u548cLlama\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDistillLens\u5728\u591a\u6837\u5316\u7684\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u77e5\u8bc6\u84b8\u998f\u548c\u7279\u5f81\u8fc1\u79fb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DistillLens\u901a\u8fc7\u5bf9\u79f0\u5bf9\u9f50\u5e08\u751f\u6a21\u578b\u7684\u6f14\u5316\u601d\u7ef4\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5ffd\u7565\u4e2d\u95f4\u5c42\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u578b\u538b\u7f29\u6548\u679c\u3002"}}
{"id": "2602.13720", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13720", "abs": "https://arxiv.org/abs/2602.13720", "authors": ["Chen Feng", "Yang Xu", "Shaojie Shen"], "title": "FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments", "comment": "8 pages, 8 figures, 3 tables", "summary": "Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.", "AI": {"tldr": "FC-Vision\uff1a\u4e00\u79cd\u5728\u7ebf\u53ef\u89c1\u6027\u611f\u77e5\u91cd\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u81ea\u4e3b\u626b\u63cf\uff0c\u4e3b\u52a8\u9632\u6b62\u76ee\u6807\u906e\u6321\uff0c\u63d0\u5347\u626b\u63cf\u8d28\u91cf", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u81ea\u4e3b\u626b\u63cf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u907f\u969c\u548c\u6548\u7387\uff0c\u4f46\u5ffd\u7565\u4e86\u906e\u6321\u5bfc\u81f4\u7684\u53ef\u89c1\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u626b\u63cf\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u98de\u884c\u4e2d\u5b9e\u65f6\u9002\u5e94\u672a\u77e5\u969c\u788d\u7269\u5e76\u4fdd\u6301\u76ee\u6807\u53ef\u89c1\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFC-Vision\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4e24\u5c42\u5206\u89e3\u5b9e\u73b0\u5b9e\u65f6\u91cd\u89c4\u5212\uff1a1) \u65e0\u906e\u6321\u89c6\u70b9\u4fee\u590d\uff0c\u5728\u4fdd\u6301\u8986\u76d6\u8303\u56f4\u7684\u540c\u65f6\u6700\u5c0f\u5316\u504f\u79bb\u539f\u59cb\u626b\u63cf\u610f\u56fe\uff1b2) 5-DoF\u7a7a\u95f4\u4e2d\u7684\u5206\u6bb5\u6e05\u6d01\u611f\u77e5\u8fde\u63a5\u3002\u91c7\u7528\u63d2\u4ef6\u5f0f\u96c6\u6210\u7b56\u7565\uff0c\u65e0\u9700\u6539\u53d8\u73b0\u6709\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u8bc4\u4f30\u4e2d\uff0cFC-Vision\u5728\u610f\u5916\u906e\u6321\u4e0b\u663e\u8457\u63d0\u5347\u626b\u63cf\u8d28\u91cf\uff1a\u6700\u5927\u8986\u76d6\u589e\u76ca\u8fbe55.32%\uff0c\u906e\u6321\u7387\u964d\u4f4e73.17%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0c\u98de\u884c\u65f6\u95f4\u4ec5\u6709\u9002\u5ea6\u589e\u52a0\u3002", "conclusion": "FC-Vision\u80fd\u591f\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u81ea\u4e3b\u626b\u63cf\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u89c1\u6027\u611f\u77e5\u91cd\u89c4\u5212\u663e\u8457\u63d0\u5347\u626b\u63cf\u8d28\u91cf\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.13486", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13486", "abs": "https://arxiv.org/abs/2602.13486", "authors": ["Fei Wu", "Jia Hu", "Geyong Min", "Shiqiang Wang"], "title": "Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity", "comment": null, "summary": "Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faraFLoRA\u65b9\u6cd5\u89e3\u51b3\u8054\u90a6\u4f4e\u79e9\u81ea\u9002\u5e94\u4e2d\u7684\u79e9\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u79e9\u5206\u533a\u805a\u5408\u63d0\u5347\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u6027\u80fd", "motivation": "\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u5ba2\u6237\u7aef\u5728\u7cfb\u7edf\u8d44\u6e90\u548c\u6570\u636e\u5206\u5e03\u4e0a\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u8fd9\u4fc3\u4f7f\u4e0d\u540c\u5ba2\u6237\u7aef\u4f7f\u7528\u4e0d\u540c\u7684LoRA\u79e9\u3002\u4f5c\u8005\u53d1\u73b0\u5f02\u6784FedLoRA\u4e2d\u5b58\u5728\u88ab\u5ffd\u89c6\u7684\"\u79e9\u5d29\u6e83\"\u73b0\u8c61\uff0c\u5373\u5168\u5c40\u66f4\u65b0\u7684\u80fd\u91cf\u96c6\u4e2d\u5728\u6700\u5c0f\u5171\u4eab\u79e9\u4e0a\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u4e14\u5bf9\u79e9\u914d\u7f6e\u9ad8\u5ea6\u654f\u611f\u3002", "method": "\u63d0\u51faraFLoRA\u65b9\u6cd5\uff1a\u4e00\u79cd\u79e9\u5206\u533a\u805a\u5408\u65b9\u6cd5\uff0c\u5c06\u672c\u5730\u66f4\u65b0\u5206\u89e3\u4e3a\u79e9\u5206\u533a\uff0c\u7136\u540e\u6839\u636e\u6bcf\u4e2a\u5206\u533a\u7684\u6709\u6548\u5ba2\u6237\u7aef\u8d21\u732e\u52a0\u6743\u805a\u5408\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u79e9\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\uff1a\u79e9\u65e0\u5173\u7684\u805a\u5408\u6743\u91cd\u4e0e\u79e9\u76f8\u5173\u7684\u5ba2\u6237\u7aef\u8d21\u732e\u4e0d\u5339\u914d\u3002", "result": "\u5728\u5206\u7c7b\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0craFLoRA\u80fd\u591f\u9632\u6b62\u79e9\u5d29\u6e83\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdbFedLoRA\u57fa\u7ebf\u76f8\u5f53\u7684\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "raFLoRA\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8054\u90a6\u4f4e\u79e9\u81ea\u9002\u5e94\u4e2d\u7684\u79e9\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u79e9\u5206\u533a\u805a\u5408\u673a\u5236\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5f02\u6784\u5ba2\u6237\u7aef\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13748", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13748", "abs": "https://arxiv.org/abs/2602.13748", "authors": ["Yongkang Jin", "Jianwen Luo", "Jingjing Wang", "Jianmin Yao", "Yu Hong"], "title": "RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction", "comment": null, "summary": "Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.", "AI": {"tldr": "RMPL\u662f\u4e00\u4e2a\u7528\u4e8e\u4f4e\u8d44\u6e90\u591a\u5a92\u4f53\u4e8b\u4ef6\u62bd\u53d6\u7684\u5173\u7cfb\u611f\u77e5\u591a\u4efb\u52a1\u6e10\u8fdb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5355\u6a21\u6001\u4e8b\u4ef6\u62bd\u53d6\u548c\u591a\u5a92\u4f53\u5173\u7cfb\u62bd\u53d6\u7684\u5f02\u6784\u76d1\u7763\uff0c\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u63d0\u5347\u4e8b\u4ef6\u8bed\u4e49\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u8bba\u5143\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u591a\u5a92\u4f53\u4e8b\u4ef6\u62bd\u53d6\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8de8\u6a21\u6001\u5bf9\u9f50\u6216\u63d0\u793a\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u73af\u5883\u4e0b\u8bba\u5143\u5b9a\u4f4d\u80fd\u529b\u5f31\u3002", "method": "\u63d0\u51faRMPL\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u7edf\u4e00\u6a21\u5f0f\u5b66\u4e60\u8de8\u6a21\u6001\u5171\u4eab\u7684\u4e8b\u4ef6\u4e2d\u5fc3\u8868\u793a\uff1b2\uff09\u901a\u8fc7\u9636\u6bb5\u5f0f\u8bad\u7ec3\u6574\u5408\u5355\u6a21\u6001\u4e8b\u4ef6\u62bd\u53d6\u548c\u591a\u5a92\u4f53\u5173\u7cfb\u62bd\u53d6\u7684\u5f02\u6784\u76d1\u7763\uff1b3\uff09\u7528\u6df7\u5408\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u5fae\u8c03\u4e8b\u4ef6\u63d0\u53ca\u8bc6\u522b\u548c\u8bba\u5143\u89d2\u8272\u62bd\u53d6\u3002", "result": "\u5728M2E2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u4e0d\u540c\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RMPL\u901a\u8fc7\u5173\u7cfb\u611f\u77e5\u7684\u591a\u4efb\u52a1\u6e10\u8fdb\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u591a\u5a92\u4f53\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u7684\u7ed3\u6784\u5316\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u8bba\u5143\u5b9a\u4f4d\u95ee\u9898\u3002"}}
{"id": "2602.13747", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13747", "abs": "https://arxiv.org/abs/2602.13747", "authors": ["Evan Eames", "Priyadarshini Kannan", "Ronan Sangouard", "Philipp Plank", "Elvin Hajizada", "Gintautas Palinauskas", "Lana Amaya", "Michael Neumeier", "Sai Thejeshwar Sharma", "Marcella Toth", "Prottush Sarkar", "Axel von Arnim"], "title": "The More the Merrier: Running Multiple Neuromorphic Components On-Chip for Robotic Control", "comment": "IOP Journal of Neuromorphic Computing and Engineering, preliminary acceptance", "summary": "It has long been realized that neuromorphic hardware offers benefits for the domain of robotics such as low energy, low latency, as well as unique methods of learning. In aiming for more complex tasks, especially those incorporating multimodal data, one hurdle continuing to prevent their realization is an inability to orchestrate multiple networks on neuromorphic hardware without resorting to off-chip process management logic. To address this, we show a first example of a pipeline for vision-based robot control in which numerous complex networks can be run entirely on hardware via the use of a spiking neural state machine for process orchestration. The pipeline is validated on the Intel Loihi 2 research chip. We show that all components can run concurrently on-chip in the milli Watt regime at latencies competitive with the state-of-the-art. An equivalent network on simulated hardware is shown to accomplish robotic arm plug insertion in simulation, and the core elements of the pipeline are additionally tested on a real robotic arm.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5b8c\u5168\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u89c6\u89c9\u673a\u5668\u4eba\u63a7\u5236\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u8109\u51b2\u795e\u7ecf\u72b6\u6001\u673a\u8fdb\u884c\u8fdb\u7a0b\u7f16\u6392\uff0c\u5728Intel Loihi 2\u82af\u7247\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u6beb\u74e6\u7ea7\u529f\u8017\u548c\u4f4e\u5ef6\u8fdf", "motivation": "\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u5728\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u4f4e\u80fd\u8017\u3001\u4f4e\u5ef6\u8fdf\u7b49\u4f18\u52bf\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u548c\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f\u5728\u786c\u4ef6\u4e0a\u7f16\u6392\u591a\u4e2a\u7f51\u7edc\u7684\u80fd\u529b\uff0c\u9700\u8981\u4f9d\u8d56\u7247\u5916\u8fdb\u7a0b\u7ba1\u7406\u903b\u8f91", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u72b6\u6001\u673a\u7684\u8fdb\u7a0b\u7f16\u6392\u6d41\u6c34\u7ebf\uff0c\u4f7f\u591a\u4e2a\u590d\u6742\u7f51\u7edc\u80fd\u5b8c\u5168\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u5728Intel Loihi 2\u82af\u7247\u4e0a\u5b9e\u73b0\u89c6\u89c9\u673a\u5668\u4eba\u63a7\u5236\u6d41\u6c34\u7ebf", "result": "\u6240\u6709\u7ec4\u4ef6\u80fd\u5728\u7247\u4e0a\u5e76\u53d1\u8fd0\u884c\uff0c\u529f\u8017\u5728\u6beb\u74e6\u7ea7\uff0c\u5ef6\u8fdf\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff1b\u6a21\u62df\u786c\u4ef6\u4e0a\u7684\u7b49\u6548\u7f51\u7edc\u6210\u529f\u5b8c\u6210\u673a\u68b0\u81c2\u63d2\u5934\u63d2\u5165\u4efb\u52a1\uff0c\u6838\u5fc3\u7ec4\u4ef6\u5728\u771f\u5b9e\u673a\u68b0\u81c2\u4e0a\u6d4b\u8bd5\u9a8c\u8bc1", "conclusion": "\u9996\u6b21\u5c55\u793a\u4e86\u5b8c\u5168\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u89c6\u89c9\u673a\u5668\u4eba\u63a7\u5236\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u8109\u51b2\u795e\u7ecf\u72b6\u6001\u673a\u89e3\u51b3\u4e86\u591a\u7f51\u7edc\u7f16\u6392\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.13800", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13800", "abs": "https://arxiv.org/abs/2602.13800", "authors": ["Alberto Olivares-Alarcos", "Muhammad Ahsan", "Satrio Sanjaya", "Hsien-I Lin", "Guillem Aleny\u00e0"], "title": "Ontological grounding for sound and natural robot explanations via large language models", "comment": "An extended abstract of this article is accepted for presentation at AAMAS 2026: Olivares-Alarcos, A., Muhammad, A., Sanjaya, S., Lin, H. and Aleny\u00e0, G. (2026). Blending ontologies and language models to generate sound and natural robot explanations. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems. IFAAMAS", "summary": "Building effective human-robot interaction requires robots to derive conclusions from their experiences that are both logically sound and communicated in ways aligned with human expectations. This paper presents a hybrid framework that blends ontology-based reasoning with large language models (LLMs) to produce semantically grounded and natural robot explanations. Ontologies ensure logical consistency and domain grounding, while LLMs provide fluent, context-aware and adaptive language generation. The proposed method grounds data from human-robot experiences, enabling robots to reason about whether events are typical or atypical based on their properties. We integrate a state-of-the-art algorithm for retrieving and constructing static contrastive ontology-based narratives with an LLM agent that uses them to produce concise, clear, interactive explanations. The approach is validated through a laboratory study replicating an industrial collaborative task. Empirical results show significant improvements in the clarity and brevity of ontology-based narratives while preserving their semantic accuracy. Initial evaluations further demonstrate the system's ability to adapt explanations to user feedback. Overall, this work highlights the potential of ontology-LLM integration to advance explainable agency, and promote more transparent human-robot collaboration.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u672c\u4f53\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u8bed\u4e49\u57fa\u7840\u4e14\u81ea\u7136\u7684\u89e3\u91ca\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u900f\u660e\u6027\u3002", "motivation": "\u6784\u5efa\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u9700\u8981\u673a\u5668\u4eba\u4ece\u7ecf\u9a8c\u4e2d\u5f97\u51fa\u65e2\u7b26\u5408\u903b\u8f91\u53c8\u80fd\u4ee5\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u7684\u65b9\u5f0f\u4f20\u8fbe\u7684\u7ed3\u8bba\u3002\u5f53\u524d\u673a\u5668\u4eba\u89e3\u91ca\u7cfb\u7edf\u5728\u903b\u8f91\u4e00\u81f4\u6027\u4e0e\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u4e4b\u95f4\u5b58\u5728\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff1a\u672c\u4f53\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u548c\u9886\u57df\u57fa\u7840\uff0cLLM\u63d0\u4f9b\u6d41\u7545\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8bed\u8a00\u751f\u6210\u3002\u65b9\u6cd5\u57fa\u4e8e\u4eba\u673a\u4ea4\u4e92\u7ecf\u9a8c\u6570\u636e\uff0c\u8ba9\u673a\u5668\u4eba\u6839\u636e\u4e8b\u4ef6\u5c5e\u6027\u5224\u65ad\u5178\u578b\u6027\u3002\u96c6\u6210\u6700\u5148\u8fdb\u7684\u9759\u6001\u5bf9\u6bd4\u672c\u4f53\u53d9\u4e8b\u68c0\u7d22\u6784\u5efa\u7b97\u6cd5\u4e0eLLM\u4ee3\u7406\uff0c\u751f\u6210\u7b80\u6d01\u6e05\u6670\u7684\u4ea4\u4e92\u5f0f\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u5ba4\u7814\u7a76\u9a8c\u8bc1\u4e86\u5de5\u4e1a\u534f\u4f5c\u4efb\u52a1\u573a\u666f\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u672c\u4f53\u53d9\u4e8b\u7684\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u51c6\u786e\u6027\u3002\u521d\u6b65\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u660e\u7cfb\u7edf\u80fd\u6839\u636e\u7528\u6237\u53cd\u9988\u81ea\u9002\u5e94\u8c03\u6574\u89e3\u91ca\u3002", "conclusion": "\u672c\u4f53-LLM\u96c6\u6210\u65b9\u6cd5\u5728\u63a8\u8fdb\u53ef\u89e3\u91ca\u667a\u80fd\u4f53\u3001\u4fc3\u8fdb\u66f4\u900f\u660e\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u89e3\u91ca\u7cfb\u7edf\u63d0\u4f9b\u4e86\u903b\u8f91\u4e25\u8c28\u4e0e\u81ea\u7136\u8868\u8fbe\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.13272", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13272", "abs": "https://arxiv.org/abs/2602.13272", "authors": ["Muyan Weng", "Defu Cao", "Wei Yang", "Yashaswi Sharma", "Yan Liu"], "title": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks", "comment": null, "summary": "It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.", "AI": {"tldr": "TemporalBench\u662f\u4e00\u4e2a\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u6e10\u8fdb\u4e30\u5bcc\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5f3a\u9884\u6d4b\u6027\u80fd\u4e0d\u4e00\u5b9a\u53cd\u6620\u771f\u6b63\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4e0d\u6e05\u695a\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\u662f\u53cd\u6620\u771f\u6b63\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u5728\u4e0a\u4e0b\u6587\u548c\u4e8b\u4ef6\u9a71\u52a8\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u8bca\u65ad\u6027\u57fa\u51c6\u6765\u533a\u5206\u8fd9\u4e24\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51faTemporalBench\u57fa\u51c6\uff0c\u91c7\u7528\u56db\u5c42\u4efb\u52a1\u5206\u7c7b\u6cd5\uff1a\u5386\u53f2\u7ed3\u6784\u89e3\u91ca\u3001\u65e0\u4e0a\u4e0b\u6587\u9884\u6d4b\u3001\u4e0a\u4e0b\u6587\u65f6\u95f4\u63a8\u7406\u548c\u4e8b\u4ef6\u6761\u4ef6\u9884\u6d4b\uff0c\u8986\u76d6\u96f6\u552e\u3001\u533b\u7597\u3001\u80fd\u6e90\u548c\u7269\u7406\u7cfb\u7edf\u56db\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u63a7\u5236\u5bf9\u672a\u6765\u76ee\u6807\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u8bbf\u95ee\uff0c\u8fdb\u884c\u8bca\u65ad\u5206\u6790\u3002", "result": "\u5e7f\u6cdb\u7684\u57fa\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5927\u7684\u6570\u503c\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u8f6c\u5316\u4e3a\u7a33\u5065\u7684\u4e0a\u4e0b\u6587\u6216\u4e8b\u4ef6\u611f\u77e5\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u7684\u667a\u80fd\u4f53\u6846\u67b6\u8868\u73b0\u51fa\u5206\u6563\u7684\u4f18\u52bf\u548c\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u5728\u4ec5\u5173\u6ce8\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u5e38\u88ab\u9690\u85cf\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u9884\u6d4b\u57fa\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u5168\u9762\u8bc4\u4f30\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002TemporalBench\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6570\u636e\u96c6\u548c\u6392\u884c\u699c\uff0c\u7528\u4e8e\u4fc3\u8fdb\u5bf9\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u7684\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2602.13280", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13280", "abs": "https://arxiv.org/abs/2602.13280", "authors": ["Hanchen David Wang", "Clayton Cohn", "Zifan Xu", "Siyuan Guo", "Gautam Biswas", "Meiyi Ma"], "title": "BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation", "comment": "paper under submission at IJCAI", "summary": "Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and \"unknown unknowns\"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).", "AI": {"tldr": "BEAGLE\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u7406\u8bba\u6765\u6a21\u62df\u5b66\u751f\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u89e3\u51b3\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u5b66\u4e60\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u6a21\u62df\u5b66\u751f\u65f6\u5b58\u5728\u7684\"\u80fd\u529b\u504f\u5dee\"\u95ee\u9898\u3002", "motivation": "\u6a21\u62df\u5b66\u751f\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u89e3\u51b3\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u884c\u4e3a\u5bf9\u6559\u80b2\u7814\u7a76\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u6536\u96c6\u771f\u5b9e\u6570\u636e\u9762\u4e34\u9690\u79c1\u95ee\u9898\u548c\u7eb5\u5411\u7814\u7a76\u7684\u9ad8\u6210\u672c\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6a21\u62df\u5b66\u751f\u7684\u53ef\u80fd\u9014\u5f84\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\"\u80fd\u529b\u504f\u5dee\"\u2014\u2014\u503e\u5411\u4e8e\u8ffd\u6c42\u9ad8\u6548\u6b63\u786e\u6027\uff0c\u800c\u4e0d\u662f\u6a21\u62df\u65b0\u624b\u5b66\u4e60\u8005\u90a3\u79cd\u53cd\u590d\u3001\u4e0d\u7a33\u5b9a\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "BEAGLE\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\u521b\u65b0\uff1a1) \u534a\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u63a7\u5236\u8ba4\u77e5\u884c\u4e3a\u548c\u5143\u8ba4\u77e5\u884c\u4e3a\u7684\u65f6\u95f4\u4e0e\u8f6c\u6362\uff1b2) \u5e26\u6709\u663e\u5f0f\u7f3a\u9677\u6ce8\u5165\u7684\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\uff0c\u5f3a\u5236\u5b9e\u65bd\u771f\u5b9e\u7684\u77e5\u8bc6\u7a7a\u767d\u548c\"\u672a\u77e5\u7684\u672a\u77e5\"\uff1b3) \u89e3\u8026\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u5c06\u9ad8\u5c42\u7b56\u7565\u4f7f\u7528\u4e0e\u4ee3\u7801\u751f\u6210\u52a8\u4f5c\u5206\u79bb\uff0c\u9632\u6b62\u6a21\u578b\u65e0\u58f0\u5730\u7ea0\u6b63\u81ea\u5df1\u7684\u6545\u610f\u9519\u8bef\u3002", "result": "\u5728Python\u7f16\u7a0b\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cBEAGLE\u5728\u91cd\u73b0\u771f\u5b9e\u5b66\u4e60\u8f68\u8ff9\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u4eba\u7c7b\u56fe\u7075\u6d4b\u8bd5\u4e2d\uff0c\u7528\u6237\u65e0\u6cd5\u533a\u5206\u5408\u6210\u8f68\u8ff9\u4e0e\u771f\u5b9e\u5b66\u751f\u6570\u636e\uff0c\u51c6\u786e\u7387\u4e0e\u968f\u673a\u731c\u6d4b\u65e0\u663e\u8457\u5dee\u5f02(52.8%)\u3002", "conclusion": "BEAGLE\u901a\u8fc7\u6574\u5408\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u7406\u8bba\u548c\u521b\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u6a21\u62df\u5b66\u751f\u884c\u4e3a\u65f6\u7684\u80fd\u529b\u504f\u5dee\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u771f\u5b9e\u5b66\u751f\u6570\u636e\u96be\u4ee5\u533a\u5206\u7684\u5408\u6210\u5b66\u4e60\u8f68\u8ff9\uff0c\u4e3a\u6559\u80b2\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6a21\u62df\u5de5\u5177\u3002"}}
{"id": "2602.13649", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13649", "abs": "https://arxiv.org/abs/2602.13649", "authors": ["Li Zhang", "Nital Patel", "Xiuqi Li", "Jessica Lin"], "title": "Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series", "comment": null, "summary": "Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \\textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u94fe\u6982\u5ff5\uff0c\u7528\u4e8e\u5728\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u4e24\u4e2a\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u53d1\u73b0\u5f02\u5e38\u6f14\u5316\u8d8b\u52bf", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u94fe\u5b9a\u4e49\u4ec5\u8003\u8651\u5355\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u5bb9\u6613\u9519\u8fc7\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u6f14\u5316\u6a21\u5f0f", "method": "\u5f15\u5165\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u94fe\u65b0\u5b9a\u4e49\uff0c\u4e13\u95e8\u7528\u4e8e\u5728\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u4e24\u4e2a\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u53d1\u73b0\u610f\u5916\u6f14\u5316\u8d8b\u52bf\uff0c\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u4e2d\u65ad\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u6392\u5e8f\u6807\u51c6\u8bc6\u522b\u6700\u4f73\u94fe", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u4f4d\u5f02\u5e38\u6f14\u5316\u6a21\u5f0f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709TSC\u5de5\u4f5c\uff0c\u5e76\u5728\u82f1\u7279\u5c14\u771f\u5b9e\u5236\u9020\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027", "conclusion": "\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u94fe\u80fd\u591f\u6709\u6548\u53d1\u73b0\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u6f14\u5316\u8d8b\u52bf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.13977", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13977", "abs": "https://arxiv.org/abs/2602.13977", "authors": ["Zhennan Jiang", "Shangqing Zhou", "Yutong Jiang", "Zefang Huang", "Mingjie Wei", "Yuhui Chen", "Tianxing Zhou", "Zhen Guo", "Hao Lin", "Quanlu Zhang", "Yu Wang", "Haoran Li", "Chao Yu", "Dongbin Zhao"], "title": "WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL", "comment": "21pages, 8 figures", "summary": "Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.", "AI": {"tldr": "WoVR\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u4e16\u754c\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u5e7b\u89c9\u3001\u6539\u8fdbrollout\u7a33\u5b9a\u6027\u548c\u4fdd\u6301\u7b56\u7565-\u6a21\u62df\u5668\u5bf9\u9f50\uff0c\u4f7f\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u80fd\u591f\u4f5c\u4e3a\u5b9e\u7528\u7684RL\u6a21\u62df\u5668\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u6709\u671b\u8d85\u8d8a\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u9700\u8981\u5927\u91cf\u771f\u5b9e\u4ea4\u4e92\uff1b\u73b0\u6709\u4e16\u754c\u6a21\u578b\u6a21\u62df\u5668\u5b58\u5728\u5e7b\u89c9\u548c\u957f\u65f6\u57df\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u8fd9\u4e9b\u8bef\u5dee\u4f1a\u7834\u574f\u4f18\u5316\u4fe1\u53f7\uff0c\u5bfc\u81f4\u7b56\u7565\u5229\u7528\u6a21\u578b\u4e0d\u51c6\u786e\u6027\u800c\u975e\u771f\u6b63\u4efb\u52a1\u8fdb\u5c55\u3002", "method": "WoVR\u6846\u67b6\uff1a1) \u4f7f\u7528\u53ef\u63a7\u7684\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u4e16\u754c\u6a21\u578b\u63d0\u9ad8rollout\u7a33\u5b9a\u6027\uff1b2) \u901a\u8fc7\u5173\u952e\u5e27\u521d\u59cb\u5316rollouts\u51cf\u5c11\u6709\u6548\u8bef\u5dee\u6df1\u5ea6\uff1b3) \u901a\u8fc7\u4e16\u754c\u6a21\u578b-\u7b56\u7565\u534f\u540c\u8fdb\u5316\u4fdd\u6301\u7b56\u7565-\u6a21\u62df\u5668\u5bf9\u9f50\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6210\u529f\u7387\u4ece39.95%\u63d0\u5347\u81f369.2%\uff08+29.3\u70b9\uff09\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u6210\u529f\u7387\u4ece61.7%\u63d0\u5347\u81f391.7%\uff08+30.0\u70b9\uff09\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u957f\u65f6\u57df\u60f3\u8c61rollouts\u548c\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316\u3002", "conclusion": "\u5f53\u5e7b\u89c9\u88ab\u660e\u786e\u63a7\u5236\u65f6\uff0c\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u7528\u6a21\u62df\u5668\uff0cWoVR\u6846\u67b6\u901a\u8fc7\u8c03\u8282RL\u4e0e\u4e0d\u5b8c\u7f8e\u60f3\u8c61\u52a8\u6001\u7684\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2602.14039", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14039", "abs": "https://arxiv.org/abs/2602.14039", "authors": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "title": "Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models", "comment": null, "summary": "Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.", "AI": {"tldr": "MoE\u5d4c\u5165\u6a21\u578b\u4f7f\u7528\u7ebf\u6027\u52a0\u6743\u6c42\u548c\u805a\u5408\u4e13\u5bb6\u8f93\u51fa\uff0c\u4f46\u51e0\u4f55\u5206\u6790\u663e\u793a\u4e13\u5bb6\u8868\u793a\u4f4d\u4e8e\u5171\u4eab\u8d85\u7403\u9762\u6d41\u5f62\u4e0a\uff0c\u7ebf\u6027\u805a\u5408\u4f1a\u5bfc\u81f4\u5411\u6d41\u5f62\u5185\u90e8\u584c\u7f29\uff0c\u7834\u574f\u5d4c\u5165\u53ef\u6bd4\u6027\u3002\u4f5c\u8005\u63d0\u51fa\u7403\u9762\u91cd\u5fc3\u805a\u5408(SBA)\u4f5c\u4e3a\u51e0\u4f55\u4fdd\u6301\u7684\u805a\u5408\u7b97\u5b50\uff0c\u5728MTEB\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524dMoE\u5d4c\u5165\u6a21\u578b\u4f7f\u7528\u7ebf\u6027\u52a0\u6743\u6c42\u548c\u805a\u5408\u4e13\u5bb6\u8f93\u51fa\uff0c\u9690\u542b\u5047\u8bbe\u5d4c\u5165\u7a7a\u95f4\u5177\u6709\u7ebf\u6027\u5b50\u7a7a\u95f4\u7ed3\u6784\u3002\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u5047\u8bbe\u4e0e\u4e13\u5bb6\u8868\u793a\u7684\u51e0\u4f55\u7279\u6027\u4e0d\u4e00\u81f4\uff0c\u7ebf\u6027\u805a\u5408\u4f1a\u5bfc\u81f4\u5d4c\u5165\u5411\u91cf\u5e45\u5ea6\u548c\u65b9\u5411\u5931\u771f\uff0c\u964d\u4f4e\u5d4c\u5165\u53ef\u6bd4\u6027\u3002", "method": "\u63d0\u51fa\u7403\u9762\u91cd\u5fc3\u805a\u5408(SBA)\u65b9\u6cd5\uff0c\u5c06\u5f84\u5411\u548c\u89d2\u5ea6\u5206\u91cf\u5206\u79bb\u5904\u7406\uff0c\u4fdd\u6301\u8d85\u7403\u9762\u51e0\u4f55\u7ed3\u6784\u3002SBA\u4e0e\u73b0\u6709\u8def\u7531\u673a\u5236\u5b8c\u5168\u517c\u5bb9\uff0c\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u9a8c\u8bc1\u4e13\u5bb6\u8f93\u51fa\u4f4d\u4e8e\u5171\u4eab\u8d85\u7403\u9762\u6d41\u5f62\u4e0a\uff0c\u5177\u6709\u7d27\u5bc6\u96c6\u4e2d\u7684\u8303\u6570\u548c\u663e\u8457\u7684\u89d2\u5ea6\u5206\u79bb\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u805a\u7c7b\u548c\u91cd\u590d\u95ee\u9898\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\uff0cSBA\u76f8\u6bd4\u7ebf\u6027\u805a\u5408\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u8bad\u7ec3\u6210\u672c\u76f8\u540c\u4e14\u5b8c\u5168\u7a33\u5b9a\u3002\u51e0\u4f55\u5206\u6790\u8bc1\u5b9eSBA\u9632\u6b62\u4e86\u805a\u5408\u5f15\u8d77\u7684\u584c\u7f29\uff0c\u4fdd\u6301\u4e86\u8d85\u7403\u9762\u4e00\u81f4\u6027\u3002", "conclusion": "MoE\u5d4c\u5165\u67b6\u6784\u4e2d\u51e0\u4f55\u611f\u77e5\u7684\u805a\u5408\u81f3\u5173\u91cd\u8981\u3002SBA\u4f5c\u4e3a\u51e0\u4f55\u4fdd\u6301\u7684\u805a\u5408\u7b97\u5b50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u6027\u805a\u5408\u4e0e\u4e13\u5bb6\u8868\u793a\u51e0\u4f55\u7279\u6027\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u73b0\u6709\u8def\u7531\u673a\u5236\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.13530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13530", "abs": "https://arxiv.org/abs/2602.13530", "authors": ["Yiheng Shu", "Saisri Padmaja Jonnalagedda", "Xiang Gao", "Bernal Jim\u00e9nez Guti\u00e9rrez", "Weijian Qi", "Kamalika Das", "Huan Sun", "Yu Su"], "title": "REMem: Reasoning with Episodic Memory in Language Agent", "comment": "Accepted by The Fourteenth International Conference on Learning Representations (ICLR 2026) as poster", "summary": "Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.", "AI": {"tldr": "REMem\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u63a8\u7406\u60c5\u666f\u8bb0\u5fc6\uff0c\u901a\u8fc7\u79bb\u7ebf\u7d22\u5f15\u6784\u5efa\u6df7\u5408\u8bb0\u5fc6\u56fe\uff0c\u5728\u7ebf\u63a8\u7406\u4f7f\u7528\u667a\u80fd\u68c0\u7d22\u5668\uff0c\u5728\u60c5\u666f\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u64c5\u957f\u5728\u65f6\u7a7a\u80cc\u666f\u4e0b\u8bb0\u5fc6\u5177\u4f53\u7ecf\u5386\u5e76\u8fdb\u884c\u8de8\u4e8b\u4ef6\u63a8\u7406\uff0c\u800c\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u7684\u8bb0\u5fc6\u4e3b\u8981\u662f\u8bed\u4e49\u7684\uff0c\u65e0\u6cd5\u6709\u6548\u56de\u5fc6\u548c\u63a8\u7406\u4ea4\u4e92\u5386\u53f2\u3002\u73b0\u6709\u5de5\u4f5c\u5f80\u5f80\u5ffd\u89c6\u60c5\u666f\u6027\u3001\u7f3a\u4e4f\u660e\u786e\u7684\u4e8b\u4ef6\u5efa\u6a21\uff0c\u6216\u8fc7\u5ea6\u5f3a\u8c03\u7b80\u5355\u68c0\u7d22\u800c\u975e\u590d\u6742\u63a8\u7406\u3002", "method": "REMem\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u79bb\u7ebf\u7d22\u5f15\uff1a\u5c06\u7ecf\u9a8c\u8f6c\u6362\u4e3a\u6df7\u5408\u8bb0\u5fc6\u56fe\uff0c\u7075\u6d3b\u94fe\u63a5\u65f6\u95f4\u611f\u77e5\u7684\u8981\u70b9\u548c\u4e8b\u5b9e\uff1b2) \u5728\u7ebf\u63a8\u7406\uff1a\u4f7f\u7528\u667a\u80fd\u68c0\u7d22\u5668\uff0c\u914d\u5907\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5de5\u5177\u5728\u8bb0\u5fc6\u56fe\u4e0a\u8fdb\u884c\u8fed\u4ee3\u68c0\u7d22\u3002", "result": "\u5728\u56db\u4e2a\u60c5\u666f\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREMem\u663e\u8457\u4f18\u4e8eMem0\u548cHippoRAG 2\u7b49\u6700\u5148\u8fdb\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5728\u60c5\u666f\u56de\u5fc6\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u53d6\u5f973.4%\u548c13.4%\u7684\u7edd\u5bf9\u6539\u8fdb\u3002\u6b64\u5916\uff0cREMem\u5bf9\u4e0d\u53ef\u56de\u7b54\u7684\u95ee\u9898\u8868\u73b0\u51fa\u66f4\u7a33\u5065\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "conclusion": "REMem\u901a\u8fc7\u6df7\u5408\u8bb0\u5fc6\u56fe\u548c\u667a\u80fd\u68c0\u7d22\u5668\u6709\u6548\u89e3\u51b3\u4e86\u60c5\u666f\u8bb0\u5fc6\u7684\u56de\u5fc6\u548c\u63a8\u7406\u6311\u6218\uff0c\u4e3a\u8bed\u8a00\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.13555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13555", "abs": "https://arxiv.org/abs/2602.13555", "authors": ["Song Wang", "Lingling Li", "Marcus Santos", "Guanghui Wang"], "title": "Privacy-Concealing Cooperative Perception for BEV Scene Segmentation", "comment": null, "summary": "Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.", "AI": {"tldr": "\u63d0\u51faPCC\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u5728BEV\u7279\u5f81\u4e2d\u9690\u85cf\u89c6\u89c9\u7ebf\u7d22\uff0c\u9632\u6b62\u56fe\u50cf\u91cd\u5efa\uff0c\u4fdd\u62a4\u5408\u4f5c\u611f\u77e5\u4e2d\u7684\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u901a\u8fc7\u5171\u4eab\u4f20\u611f\u4fe1\u606f\u63d0\u5347\u611f\u77e5\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5171\u4eab\u6570\u636e\u53ef\u80fd\u88ab\u7528\u4e8e\u91cd\u5efa\u654f\u611f\u89c6\u89c9\u5185\u5bb9\u3002", "method": "\u57fa\u4e8e\u5171\u4eab\u7684BEV\u7279\u5f81\u8bbe\u8ba1\u9690\u85cf\u7f51\u7edc\uff0c\u91c7\u7528\u5bf9\u6297\u5b66\u4e60\u673a\u5236\u8bad\u7ec3\uff1a\u9690\u85cf\u7f51\u7edc\u5728BEV\u7279\u5f81\u4e2d\u9690\u85cf\u89c6\u89c9\u7ebf\u7d22\uff0c\u91cd\u5efa\u7f51\u7edc\u5c1d\u8bd5\u6062\u590d\u8fd9\u4e9b\u7ebf\u7d22\u3002\u611f\u77e5\u7f51\u7edc\u4e0e\u9690\u85cf\u7f51\u7edc\u96c6\u6210\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "PCC\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86\u91cd\u5efa\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u5bf9\u5206\u5272\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\uff0c\u4e3a\u5408\u4f5c\u8f66\u8f86\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u63d0\u51fa\u7684PCC\u6846\u67b6\u5728\u4fdd\u62a4\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u9690\u79c1\u65b9\u9762\u6709\u6548\uff0c\u5728\u4fdd\u6301\u5206\u5272\u6027\u80fd\u7684\u540c\u65f6\u9632\u6b62\u56fe\u50cf\u91cd\u5efa\uff0c\u6e90\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2602.13921", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13921", "abs": "https://arxiv.org/abs/2602.13921", "authors": ["Juntong Wang", "Libin Chen", "Xiyuan Wang", "Shijia Kang", "Haotong Yang", "Da Zheng", "Muhan Zhang"], "title": "GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization", "comment": "46 pages, 14figures", "summary": "Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.", "AI": {"tldr": "GREPO\u662f\u9996\u4e2a\u7528\u4e8e\u4ed3\u5e93\u7ea7bug\u5b9a\u4f4d\u4efb\u52a1\u7684GNN\u57fa\u51c6\uff0c\u5305\u542b86\u4e2aPython\u4ed3\u5e93\u548c47294\u4e2abug\u4fee\u590d\u4efb\u52a1\uff0c\u4e3aGNN\u5904\u7406\u63d0\u4f9b\u56fe\u6570\u636e\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u3002", "motivation": "\u4ed3\u5e93\u7ea7bug\u5b9a\u4f4d\u662f\u5173\u952e\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\uff0c\u6807\u51c6LLM\u7531\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u65e0\u6cd5\u5904\u7406\u6574\u4e2a\u4ee3\u7801\u4ed3\u5e93\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u6709\u9650\uff0c\u800cGNN\u867d\u6709\u6f5c\u529b\u4f46\u7f3a\u4e4f\u4e13\u7528\u57fa\u51c6\u3002", "method": "\u5f15\u5165GREPO\u57fa\u51c6\uff0c\u5305\u542b86\u4e2aPython\u4ed3\u5e93\u548c47294\u4e2abug\u4fee\u590d\u4efb\u52a1\uff0c\u63d0\u4f9b\u53ef\u76f4\u63a5\u7528\u4e8eGNN\u5904\u7406\u7684\u56fe\u6570\u636e\u7ed3\u6784\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdGNN\u67b6\u6784\u3002", "result": "GNN\u67b6\u6784\u5728bug\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u5173\u952e\u8bcd\u5339\u914d\u3001\u6587\u672c\u76f8\u4f3c\u5ea6\u548c\u7b80\u5355\u56fe\u542f\u53d1\u5f0f\u65b9\u6cd5\uff09\u3002", "conclusion": "GREPO\u5c55\u793a\u4e86GNN\u5728bug\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.14299", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.14299", "abs": "https://arxiv.org/abs/2602.14299", "authors": ["Ming Li", "Xirui Li", "Tianyi Zhou"], "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook", "comment": null, "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.", "AI": {"tldr": "\u7814\u7a76AI\u667a\u80fd\u4f53\u793e\u4f1a\u662f\u5426\u50cf\u4eba\u7c7b\u793e\u4f1a\u4e00\u6837\u7ecf\u5386\u8d8b\u540c\u52a8\u6001\uff0c\u53d1\u73b0\u867d\u7136\u5168\u5c40\u8bed\u4e49\u7a33\u5b9a\uff0c\u4f46\u4e2a\u4f53\u4fdd\u6301\u9ad8\u591a\u6837\u6027\uff0c\u7f3a\u4e4f\u76f8\u4e92\u5f71\u54cd\u548c\u5171\u8bc6\u5f62\u6210\uff0c\u8868\u660e\u4ec5\u9760\u89c4\u6a21\u548c\u4e92\u52a8\u5bc6\u5ea6\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u793e\u4f1a\u5316\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u7f51\u7edc\u73af\u5883\u4e2d\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u7814\u7a76AI\u667a\u80fd\u4f53\u793e\u4f1a\u662f\u5426\u50cf\u4eba\u7c7b\u793e\u4f1a\u4e00\u6837\u7ecf\u5386\u8d8b\u540c\u52a8\u6001\u3002Moltbook\u6a21\u62df\u4e86\u4e00\u4e2a\u81ea\u4e3b\u667a\u80fd\u4f53\u53c2\u4e0e\u5f00\u653e\u3001\u6301\u7eed\u6f14\u5316\u7684\u5728\u7ebf\u793e\u4f1a\u7684\u672a\u6765\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u8bca\u65adAI\u667a\u80fd\u4f53\u793e\u4f1a\u7684\u6846\u67b6\uff0c\u5305\u62ec\u52a8\u6001\u6f14\u5316\u7684\u5b9a\u91cf\u8bca\u65ad\u6307\u6807\uff1a\u8bed\u4e49\u7a33\u5b9a\u5316\u3001\u8bcd\u6c47\u66f4\u66ff\u3001\u4e2a\u4f53\u60ef\u6027\u3001\u5f71\u54cd\u529b\u6301\u4e45\u6027\u548c\u96c6\u4f53\u5171\u8bc6\u3002", "result": "Moltbook\u7cfb\u7edf\u5904\u4e8e\u52a8\u6001\u5e73\u8861\uff1a\u5168\u5c40\u8bed\u4e49\u5e73\u5747\u503c\u5feb\u901f\u7a33\u5b9a\uff0c\u4f46\u4e2a\u4f53\u667a\u80fd\u4f53\u4fdd\u6301\u9ad8\u591a\u6837\u6027\u548c\u6301\u7eed\u8bcd\u6c47\u66f4\u66ff\uff0c\u907f\u514d\u540c\u8d28\u5316\u3002\u7136\u800c\uff0c\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u5f3a\u4e2a\u4f53\u60ef\u6027\u548c\u5bf9\u4e92\u52a8\u4f19\u4f34\u7684\u6700\u5c0f\u9002\u5e94\u6027\u54cd\u5e94\uff0c\u963b\u788d\u76f8\u4e92\u5f71\u54cd\u548c\u5171\u8bc6\u5f62\u6210\u3002\u5f71\u54cd\u529b\u4fdd\u6301\u77ed\u6682\u6027\uff0c\u6ca1\u6709\u6301\u4e45\u8d85\u7ea7\u8282\u70b9\uff0c\u793e\u4f1a\u56e0\u7f3a\u4e4f\u5171\u4eab\u793e\u4f1a\u8bb0\u5fc6\u800c\u65e0\u6cd5\u53d1\u5c55\u7a33\u5b9a\u7684\u96c6\u4f53\u5f71\u54cd\u529b\u951a\u70b9\u3002", "conclusion": "\u4ec5\u9760\u89c4\u6a21\u548c\u4e92\u52a8\u5bc6\u5ea6\u4e0d\u8db3\u4ee5\u8bf1\u5bfc\u793e\u4f1a\u5316\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5373\u5c06\u5230\u6765\u7684\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u793e\u4f1a\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u539f\u5219\u3002"}}
{"id": "2602.14428", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14428", "abs": "https://arxiv.org/abs/2602.14428", "authors": ["Wang Xing", "Wei Song", "Siyu Lin", "Chen Wu", "Man Wang"], "title": "LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.", "AI": {"tldr": "\u63d0\u51faLLM\u8f85\u52a9\u7684\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\uff0c\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u6559\u5e08\u6a21\u578b\u548cLLM\u7684\u4e30\u5bcc\u80cc\u666f\u77e5\u8bc6\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6a21\u578b\u8ba1\u7b97\u91cf\u5927\u3001\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u7684\u538b\u7f29\u548c\u84b8\u998f\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u56fe\u8bbe\u8ba1\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u65f6\u5e8f\u573a\u666f\u4f1a\u5ffd\u7565\u65f6\u95f4\u4f9d\u8d56\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faLLM\u8f85\u52a9\u7684\u84b8\u998f\u6846\u67b6\uff0c\u5305\u542b\u4f20\u7edf\u9ad8\u5bb9\u91cf\u65f6\u5e8f\u6559\u5e08\u6a21\u578b\u548c\u4f5c\u4e3a\u8f85\u52a9\u6307\u5bfc\u8005\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002LLM\u63d0\u4f9b\u4e30\u5bcc\u7684\u80cc\u666f\u77e5\u8bc6\u548c\u65f6\u5e8f\u611f\u77e5\u4fe1\u53f7\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76d1\u7763\u548c\u84b8\u998f\u76ee\u6807\uff0c\u91c7\u7528\u5206\u9636\u6bb5\u5bf9\u9f50\u7b56\u7565\u9010\u6b65\u6574\u5408\u4e24\u4e2a\u6559\u5e08\u7684\u6307\u5bfc\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171TKG\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0d\u540c\u9aa8\u5e72\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b66\u751f\u6a21\u578b\u7d27\u51d1\u9ad8\u6548\u7684\u540c\u65f6\uff0c\u6301\u7eed\u6539\u8fdb\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u5f3a\u84b8\u998f\u57fa\u7ebf\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u6559\u5e08\uff0c\u5c06\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u8d44\u6e90\u9ad8\u6548\u7684TKG\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u84b8\u998f\u6846\u67b6\u3002"}}
{"id": "2602.15010", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15010", "abs": "https://arxiv.org/abs/2602.15010", "authors": ["Max Sobol Mark", "Jacky Liang", "Maria Attarian", "Chuyuan Fu", "Debidatta Dwibedi", "Dhruv Shah", "Aviral Kumar"], "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames", "comment": null, "summary": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.", "AI": {"tldr": "\u63d0\u51faBig Picture Policies (BPP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u5173\u952e\u5e27\u6765\u51cf\u5c11\u8bad\u7ec3\u4e0e\u90e8\u7f72\u95f4\u7684\u5206\u5e03\u504f\u79fb\uff0c\u5728\u9700\u8981\u5386\u53f2\u4fe1\u606f\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u4f73\u673a\u5668\u4eba\u7b56\u7565\u901a\u5e38\u53ea\u57fa\u4e8e\u5f53\u524d\u89c2\u6d4b\uff0c\u65e0\u6cd5\u5904\u7406\u9700\u8981\u5386\u53f2\u4fe1\u606f\u7684\u4efb\u52a1\u3002\u76f4\u63a5\u4f7f\u7528\u5386\u53f2\u89c2\u6d4b\u4f1a\u56e0\u865a\u5047\u76f8\u5173\u6027\u800c\u5931\u8d25\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u65f6\u5386\u53f2\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBig Picture Policies (BPP)\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u4efb\u52a1\u76f8\u5173\u7684\u5173\u952e\u5e27\uff0c\u5c06\u591a\u6837\u5316\u7684\u8f68\u8ff9\u6295\u5f71\u5230\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u4e8b\u4ef6\u96c6\u5408\u4e0a\uff0c\u51cf\u5c11\u5206\u5e03\u504f\u79fb\u3002", "result": "\u57284\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u548c3\u4e2a\u4eff\u771f\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cBPP\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u6bd4\u6700\u4f73\u5bf9\u6bd4\u65b9\u6cd5\u6210\u529f\u7387\u9ad8\u51fa70%\u3002", "conclusion": "\u901a\u8fc7\u68c0\u6d4b\u5173\u952e\u5e27\u6765\u6761\u4ef6\u5316\u7b56\u7565\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u5386\u53f2\u6761\u4ef6\u5316\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9700\u8981\u5386\u53f2\u4fe1\u606f\u7684\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.15018", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15018", "abs": "https://arxiv.org/abs/2602.15018", "authors": ["Richeek Das", "Pratik Chaudhari"], "title": "Neurosim: A Fast Simulator for Neuromorphic Robot Perception", "comment": "13 pages, 6 figures", "summary": "Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .", "AI": {"tldr": "Neurosim\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u5b9e\u65f6\u4f20\u611f\u5668\u4eff\u771f\u5e93\uff0c\u652f\u6301\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u3001RGB\u76f8\u673a\u3001\u6df1\u5ea6\u4f20\u611f\u5668\u548c\u60ef\u6027\u4f20\u611f\u5668\u4eff\u771f\uff0c\u4ee5\u53ca\u591a\u65cb\u7ffc\u98de\u884c\u5668\u7684\u654f\u6377\u52a8\u529b\u5b66\u4eff\u771f\uff0c\u5728\u684c\u9762GPU\u4e0a\u53ef\u8fbe~2700 FPS\u3002\u5b83\u901a\u8fc7Cortex\u901a\u4fe1\u5e93\u4e0e\u673a\u5668\u5b66\u4e60\u548c\u673a\u5668\u4eba\u5de5\u4f5c\u6d41\u96c6\u6210\u3002", "motivation": "\u4e3a\u795e\u7ecf\u5f62\u6001\u611f\u77e5\u548c\u63a7\u5236\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6a21\u6001\u65f6\u95f4\u540c\u6b65\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u5b9e\u73b0\u95ed\u73af\u5b9e\u65f6\u7b97\u6cd5\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u6027\u80fd\u4f20\u611f\u5668\u4eff\u771f\u5e93Neurosim\uff0c\u7ed3\u5408\u57fa\u4e8eZeroMQ\u7684\u901a\u4fe1\u5e93Cortex\uff0c\u63d0\u4f9b\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u7684\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u652f\u6301NumPy\u6570\u7ec4\u548cPyTorch\u5f20\u91cf\u3002", "result": "Neurosim\u5728\u684c\u9762GPU\u4e0a\u5b9e\u73b0\u9ad8\u8fbe~2700 FPS\u7684\u5e27\u7387\uff0c\u80fd\u591f\u4eff\u771f\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u591a\u65cb\u7ffc\u98de\u884c\u5668\u52a8\u529b\u5b66\uff0c\u5e76\u4e0e\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "Neurosim\u548cCortex\u4e3a\u795e\u7ecf\u5f62\u6001\u611f\u77e5\u548c\u63a7\u5236\u7b97\u6cd5\u7684\u8bad\u7ec3\u4e0e\u5b9e\u65f6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u5e73\u53f0\uff0c\u652f\u6301\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u95ed\u73af\u7cfb\u7edf\u9a8c\u8bc1\u3002"}}
{"id": "2602.13728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13728", "abs": "https://arxiv.org/abs/2602.13728", "authors": ["Junpeng Zhang", "Zewei Yang", "Jie Feng", "Yuhui Zheng", "Ronghua Shang", "Mengxuan Zhang"], "title": "Explore Intrinsic Geometry for Query-based Tiny and Oriented Object Detector with Momentum-based Bipartite Matching", "comment": "13 pages", "summary": "Recent query-based detectors have achieved remarkable progress, yet their performance remains constrained when handling objects with arbitrary orientations, especially for tiny objects capturing limited texture information. This limitation primarily stems from the underutilization of intrinsic geometry during pixel-based feature decoding and the occurrence of inter-stage matching inconsistency caused by stage-wise bipartite matching. To tackle these challenges, we present IGOFormer, a novel query-based oriented object detector that explicitly integrates intrinsic geometry into feature decoding and enhances inter-stage matching stability. Specifically, we design an Intrinsic Geometry-aware Decoder, which enhances the object-related features conditioned on an object query by injecting complementary geometric embeddings extrapolated from their correlations to capture the geometric layout of the object, thereby offering a critical geometric insight into its orientation. Meanwhile, a Momentum-based Bipartite Matching scheme is developed to adaptively aggregate historical matching costs by formulating an exponential moving average with query-specific smoothing factors, effectively preventing conflicting supervisory signals arising from inter-stage matching inconsistency. Extensive experiments and ablation studies demonstrate the superiority of our IGOFormer for aerial oriented object detection, achieving an AP$_{50}$ score of 78.00\\% on DOTA-V1.0 using Swin-T backbone under the single-scale setting. The code will be made publicly available.", "AI": {"tldr": "IGOFormer\u662f\u4e00\u4e2a\u9762\u5411\u822a\u7a7a\u56fe\u50cf\u4e2d\u4efb\u610f\u65b9\u5411\u7269\u4f53\u7684\u65b0\u578b\u67e5\u8be2\u5f0f\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u89e3\u7801\u548c\u52a8\u91cf\u5339\u914d\u673a\u5236\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u67e5\u8be2\u5f0f\u68c0\u6d4b\u5668\u5728\u5904\u7406\u4efb\u610f\u65b9\u5411\u7269\u4f53\uff08\u5c24\u5176\u662f\u7eb9\u7406\u4fe1\u606f\u6709\u9650\u7684\u5c0f\u7269\u4f53\uff09\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u50cf\u7d20\u7ea7\u7279\u5f81\u89e3\u7801\u4e2d\u672a\u5145\u5206\u5229\u7528\u5185\u5728\u51e0\u4f55\u4fe1\u606f\uff0c\u4ee5\u53ca\u9636\u6bb5\u95f4\u4e8c\u5206\u56fe\u5339\u914d\u4e0d\u4e00\u81f4", "method": "\u63d0\u51faIGOFormer\uff1a1\uff09\u5185\u5728\u51e0\u4f55\u611f\u77e5\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u6ce8\u5165\u4ece\u5bf9\u8c61\u67e5\u8be2\u76f8\u5173\u6027\u63a8\u65ad\u7684\u4e92\u8865\u51e0\u4f55\u5d4c\u5165\u6765\u589e\u5f3a\u5bf9\u8c61\u76f8\u5173\u7279\u5f81\uff1b2\uff09\u57fa\u4e8e\u52a8\u91cf\u7684\u4e8c\u5206\u56fe\u5339\u914d\u65b9\u6848\uff0c\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u81ea\u9002\u5e94\u805a\u5408\u5386\u53f2\u5339\u914d\u6210\u672c", "result": "\u5728DOTA-V1.0\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528Swin-T\u9aa8\u5e72\u7f51\u7edc\u5728\u5355\u5c3a\u5ea6\u8bbe\u7f6e\u4e0b\u8fbe\u523078.00%\u7684AP50\u5206\u6570\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027", "conclusion": "IGOFormer\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u5185\u5728\u51e0\u4f55\u4fe1\u606f\u548c\u589e\u5f3a\u9636\u6bb5\u95f4\u5339\u914d\u7a33\u5b9a\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u822a\u7a7a\u56fe\u50cf\u4e2d\u4efb\u610f\u65b9\u5411\u7269\u4f53\u7684\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2602.13904", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13904", "abs": "https://arxiv.org/abs/2602.13904", "authors": ["Manqing Liu", "David Williams-King", "Ida Caspary", "Linh Le", "Hannes Whittingham", "Puria Radmard", "Cameron Tice", "Edward James Young"], "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models", "comment": null, "summary": "Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7b80\u5355\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u4efb\u52a1\u65e0\u5173\u7684\u6307\u6807\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u533a\u5206\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u7684\u4e09\u79cd\u75c5\u7406\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u7279\u5b9a\u75c5\u7406\u6a21\u578b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u601d\u7ef4\u94fe\u63a8\u7406\u662f\u73b0\u4ee3LLM\u67b6\u6784\u7684\u57fa\u7840\uff0c\u4e5f\u662fAI\u5b89\u5168\u7684\u5173\u952e\u5e72\u9884\u70b9\u3002\u7136\u800c\uff0c\u601d\u7ef4\u94fe\u63a8\u7406\u53ef\u80fd\u5b58\u5728\u75c5\u7406\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u963b\u788d\u4e86\u5176\u76d1\u63a7\u7684\u6709\u6548\u6027\u3002\u5148\u524d\u7814\u7a76\u5df2\u8bc6\u522b\u51fa\u4e09\u79cd\u75c5\u7406\uff1a\u4e8b\u540e\u5408\u7406\u5316\u3001\u7f16\u7801\u63a8\u7406\u548c\u5185\u5316\u63a8\u7406\uff0c\u9700\u8981\u5f00\u53d1\u5de5\u5177\u6765\u66f4\u597d\u5730\u7406\u89e3\u548c\u533a\u5206\u8fd9\u4e9b\u75c5\u7406\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u5957\u5177\u4f53\u7684\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u7b80\u5355\u6613\u5b9e\u73b0\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u4efb\u52a1\u65e0\u5173\u3002\u4e3a\u4e86\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e86\u4e13\u95e8\u5c55\u793a\u7279\u5b9a\u601d\u7ef4\u94fe\u75c5\u7406\u7684\u6a21\u578b\u751f\u7269\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u8bc4\u4f30\u601d\u7ef4\u94fe\u75c5\u7406\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u76d1\u63a7\u65b9\u9762\u5177\u6709\u76f4\u63a5\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u533a\u5206\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u7684\u75c5\u7406\u6a21\u5f0f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5bf9\u8bad\u7ec3\u65f6\u76d1\u63a7\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8LLM\u63a8\u7406\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.13760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13760", "abs": "https://arxiv.org/abs/2602.13760", "authors": ["Li Wang", "HaoYu Wang", "Xi Chen", "ZeKun Jiang", "Kang Li", "Jian Li"], "title": "SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video", "comment": null, "summary": "Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.", "AI": {"tldr": "SAM4Dcap\uff1a\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u4f30\u8ba1\u751f\u7269\u529b\u5b66\u6307\u6807\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u7ed3\u54084D\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u4e0e\u751f\u7269\u529b\u5b66\u6c42\u89e3\u5668", "motivation": "\u5b9a\u91cf\u751f\u7269\u529b\u5b66\u5206\u6790\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548c\u635f\u4f24\u9884\u9632\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u6210\u672c\u9ad8\u6602\uff0c\u591a\u89c6\u56fe\u89c6\u9891\u65b9\u6cd5\u867d\u7136\u964d\u4f4e\u4e86\u95e8\u69db\uff0c\u4f46\u4ecd\u4e0d\u9002\u7528\u4e8e\u5bb6\u5ead\u5355\u76ee\u6355\u6349\u573a\u666f", "method": "\u96c6\u6210SAM-Body4D\u7684\u65f6\u95f4\u4e00\u81f44D\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u4e0eOpenSim\u751f\u7269\u529b\u5b66\u6c42\u89e3\u5668\uff0c\u5c06\u91cd\u5efa\u7684\u7f51\u683c\u8f6c\u6362\u4e3a\u8f68\u8ff9\u6587\u4ef6\uff0c\u517c\u5bb9\u591a\u79cd\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u5f15\u5165\u81ea\u52a8\u5316\u63d0\u793a\u7b56\u7565\u548cLinux\u539f\u751f\u6784\u5efa", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u884c\u8d70\u548c\u8df3\u8dc3\u4efb\u52a1\u4e2d\uff0cSAM4Dcap\u6709\u6f5c\u529b\u5b9e\u73b0\u4e0e\u591a\u89c6\u56fe\u7cfb\u7edf\u76f8\u5f53\u7684\u819d\u5173\u8282\u8fd0\u52a8\u5b66\u9884\u6d4b\uff0c\u5c3d\u7ba1\u5728\u9acb\u5173\u8282\u5c48\u66f2\u548c\u6b8b\u7559\u6296\u52a8\u65b9\u9762\u4ecd\u5b58\u5728\u4e00\u4e9b\u5dee\u5f02", "conclusion": "\u901a\u8fc7\u5c06\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6210\u719f\u7684\u751f\u7269\u529b\u5b66\u6a21\u62df\u76f8\u7ed3\u5408\uff0cSAM4Dcap\u4e3a\u975e\u5b9e\u9a8c\u5ba4\u8fd0\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u8bbf\u95ee\u7684\u57fa\u7840\u6846\u67b6"}}
{"id": "2602.14200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14200", "abs": "https://arxiv.org/abs/2602.14200", "authors": ["Nicolas Zumarraga", "Thomas Kaar", "Ning Wang", "Maxwell A. Xu", "Max Rosenblattl", "Markus Kreft", "Kevin O'Sullivan", "Paul Schmiedmayer", "Patrick Langer", "Robert Jakob"], "title": "TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models", "comment": "18 pages, 13 figures, 10 tables (main paper: 5 pages, 3 figures, 2 tables)", "summary": "Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.", "AI": {"tldr": "TS-Haystack\u662f\u4e00\u4e2a\u957f\u4e0a\u4e0b\u6587\u65f6\u95f4\u5e8f\u5217\u68c0\u7d22\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4e2d\u7684\u68c0\u7d22\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u538b\u7f29\u65f6\u5206\u7c7b\u6027\u80fd\u4fdd\u6301\u4f46\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5728\u77ed\u5e8f\u5217\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u4f20\u611f\u5668\u6570\u636e\u53ef\u80fd\u5305\u542b\u6570\u767e\u4e07\u4e2a\u6570\u636e\u70b9\uff0c\u9700\u8981\u7cbe\u786e\u7684\u65f6\u95f4\u5b9a\u4f4d\u548c\u4e25\u683c\u7684\u8ba1\u7b97\u7ea6\u675f\uff0c\u5f53\u524d\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5f15\u5165TS-Haystack\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cd\u4efb\u52a1\u7c7b\u578b\uff08\u76f4\u63a5\u68c0\u7d22\u3001\u65f6\u95f4\u63a8\u7406\u3001\u591a\u6b65\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5f02\u5e38\uff09\u7684\u5341\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u5728\u957f\u7eb5\u5411\u52a0\u901f\u5ea6\u8ba1\u8bb0\u5f55\u4e2d\u5d4c\u5165\u77ed\u6d3b\u52a8\u7247\u6bb5\u8fdb\u884c\u53d7\u63a7\"\u9488\u63d2\u5165\"\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4ece\u79d2\u52302\u5c0f\u65f6\u7684\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u4f1a\u5ffd\u7565\u65f6\u95f4\u7c92\u5ea6\uff0c\u4ea7\u751f\u4efb\u52a1\u4f9d\u8d56\u6548\u5e94\uff1a\u538b\u7f29\u6709\u52a9\u4e8e\u5206\u7c7b\u4f46\u635f\u5bb3\u5c40\u90e8\u4e8b\u4ef6\u68c0\u7d22\u3002\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u538b\u7f29\u5728\u9ad8\u8fbe176\u500d\u538b\u7f29\u6bd4\u4e0b\u4fdd\u6301\u6216\u6539\u5584\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f46\u68c0\u7d22\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u5bfc\u81f4\u65f6\u95f4\u5c40\u90e8\u4fe1\u606f\u4e22\u5931\u3002", "conclusion": "\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u89e3\u8026\u5e8f\u5217\u957f\u5ea6\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u4fdd\u771f\u5ea6\u7684\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u65f6\u95f4\u5e8f\u5217\u68c0\u7d22\u7684\u6311\u6218\u3002"}}
{"id": "2602.14231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14231", "abs": "https://arxiv.org/abs/2602.14231", "authors": ["Seyedsaman Emami", "Daniel Hern\u00e1ndez-Lobato", "Gonzalo Mart\u00ednez-Mu\u00f1oz"], "title": "Robust multi-task boosting using clustering and local ensembling", "comment": null, "summary": "Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.", "AI": {"tldr": "RMB-CLE\uff1a\u4e00\u79cd\u901a\u8fc7\u57fa\u4e8e\u8bef\u5dee\u7684\u4efb\u52a1\u805a\u7c7b\u548c\u5c40\u90e8\u96c6\u6210\u6765\u9632\u6b62\u8d1f\u8fc1\u79fb\u7684\u9c81\u68d2\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6", "motivation": "\u4f20\u7edf\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5728\u5f3a\u5236\u4e0d\u76f8\u5173\u6216\u566a\u58f0\u4efb\u52a1\u5171\u4eab\u8868\u793a\u65f6\u5bb9\u6613\u906d\u53d7\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u8bc6\u522b\u76f8\u5173\u4efb\u52a1\u5e76\u9632\u6b62\u8d1f\u4f20\u8f93\u7684\u9c81\u68d2\u65b9\u6cd5", "method": "\u63d0\u51faRMB-CLE\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u8bef\u5dee\u63a8\u5bfc\u4efb\u52a1\u95f4\u76f8\u4f3c\u5ea6\uff08\u5206\u89e3\u4e3a\u529f\u80fd\u4e0d\u5339\u914d\u548c\u4e0d\u53ef\u7ea6\u566a\u58f0\uff09\uff0c\u4f7f\u7528\u51dd\u805a\u805a\u7c7b\u81ea\u9002\u5e94\u5206\u7ec4\u4efb\u52a1\uff0c\u5e76\u5728\u6bcf\u4e2a\u7c07\u5185\u4f7f\u7528\u5c40\u90e8\u96c6\u6210\u5b9e\u73b0\u9c81\u68d2\u77e5\u8bc6\u5171\u4eab", "result": "\u5728\u5408\u6210\u6570\u636e\u4e2d\u80fd\u6062\u590d\u771f\u5b9e\u4efb\u52a1\u7c07\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u591a\u4efb\u52a1\u3001\u5355\u4efb\u52a1\u548c\u57fa\u4e8e\u6c60\u5316\u7684\u96c6\u6210\u65b9\u6cd5", "conclusion": "RMB-CLE\u4e0d\u4ec5\u7ed3\u5408\u4e86\u805a\u7c7b\u548c\u96c6\u6210\uff0c\u66f4\u662f\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4e3a\u9c81\u68d2\u591a\u4efb\u52a1\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u57fa\u7840"}}
{"id": "2602.14095", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.14095", "abs": "https://arxiv.org/abs/2602.14095", "authors": ["Artem Karpov"], "title": "NEST: Nascent Encoded Steganographic Thoughts", "comment": null, "summary": "Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86LLM\u7684\u9690\u5199\u5f0f\u601d\u7ef4\u94fe\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u65e0\u6cd5\u6301\u7eed\u9690\u85cf\u63a8\u7406\uff0c\u4f46\u5728\u7b80\u5316\u4efb\u52a1\u4e2d\u5df2\u5c55\u73b0\u521d\u6b65\u80fd\u529b\uff0c\u5f3a\u8c03\u9700\u8981\u6301\u7eed\u8bc4\u4f30\u9690\u5199\u98ce\u9669\u3002", "motivation": "\u76d1\u63a7\u601d\u7ef4\u94fe\u662fLLM\u5b89\u5168\u7684\u57fa\u7840\u6280\u672f\uff0c\u4f46\u5982\u679c\u6a21\u578b\u5b66\u4f1a\u9690\u85cf\u63a8\u7406\uff0c\u8fd9\u79cd\u76d1\u63a7\u5c31\u4f1a\u5931\u6548\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u901a\u8fc7\u9690\u5199\u65b9\u5f0f\u9690\u85cf\u63a8\u7406\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u4e3a\u98ce\u9669\u8bc4\u4f30\u548c\u90e8\u7f72\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e8628\u4e2a\u6a21\u578b\uff08\u4ece\u8fc7\u53bb\u5230\u5f53\u524d\u524d\u6cbf\uff09\u7684\u9690\u5199\u80fd\u529b\u6781\u9650\uff0c\u6d4b\u91cf\u76d1\u63a7\u89c4\u907f\u3001\u62d2\u7edd\u7387\u3001\u7f16\u7801\u4fdd\u771f\u5ea6\u548c\u9690\u85cf\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u9690\u5199\u85cf\u5934\u8bd7\u4e0e\u666e\u901a\u63a8\u7406\u548c\u586b\u5145\u6807\u8bb0\u57fa\u7ebf\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u548c\u7b97\u672f\u4efb\u52a1\u4e0a\u65e0\u6cd5\u6301\u7eed\u9690\u85cf\u63a8\u7406\uff0c\u4f46\u5728\u7b80\u5316\u8ba1\u6570\u5b9e\u9a8c\u4e2d\uff0cClaude Opus 4.5\u5728\u9690\u85cf\u4efb\u52a1\u4e0a\u8fbe\u523092%\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u521d\u6b65\u80fd\u529b\u3002GPT-5.2\u5728\u7f55\u89c1\u60c5\u51b5\u4e0b\uff08<1%\uff09\u4f1a\u62d2\u7edd\u9690\u5199\u6307\u4ee4\u540c\u65f6\u5374\u9075\u5b88\u5b83\u4eec\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6301\u7eed\u8bc4\u4f30\u9690\u5199\u98ce\u9669\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u4f9b\u4e86\u9884\u5148\u68c0\u6d4b\u548c\u9632\u6b62\u9690\u85cf\u63a8\u7406\u7684\u65b9\u6cd5\u8bba\uff0c\u8fd9\u4e9b\u9690\u85cf\u63a8\u7406\u53ef\u80fd\u52a9\u957f\u672a\u5bf9\u9f50\u7684\u8c0b\u5212\u548c\u6b3a\u9a97\u884c\u4e3a\u3002"}}
{"id": "2602.14798", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.14798", "abs": "https://arxiv.org/abs/2602.14798", "authors": ["Yohan Lee", "Jisoo Jang", "Seoyeon Choi", "Sangyeop Kim", "Seungtaek Choi"], "title": "Overthinking Loops in Agents: A Structural Risk via MCP Tools", "comment": null, "summary": "Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.", "AI": {"tldr": "\u6076\u610fMCP\u5de5\u5177\u670d\u52a1\u5668\u53ef\u901a\u8fc7\u8bf1\u5bfc\u8fc7\u5ea6\u601d\u8003\u5faa\u73af\u653b\u51fbLLM\u4ee3\u7406\uff0c\u9020\u6210\u8d44\u6e90\u6d88\u8017\u548c\u6027\u80fd\u4e0b\u964d", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u4f9d\u8d56\u6587\u672c\u53ef\u89c1\u7684\u5143\u6570\u636e\uff08\u5982\u5de5\u5177\u540d\u79f0\u3001\u63cf\u8ff0\u3001\u8fd4\u56de\u6d88\u606f\uff09\uff0c\u8fd9\u79cd\u4fbf\u5229\u6027\u521b\u9020\u4e86\u4f9b\u5e94\u94fe\u653b\u51fb\u9762\uff0c\u6076\u610f\u5de5\u5177\u670d\u52a1\u5668\u53ef\u88ab\u5171\u540c\u6ce8\u518c\u5e76\u8bf1\u5bfc\u8fc7\u5ea6\u601d\u8003\u5faa\u73af", "method": "\u5c06\u653b\u51fb\u5f62\u5f0f\u5316\u4e3a\u7ed3\u6784\u6027\u8fc7\u5ea6\u601d\u8003\u653b\u51fb\uff0c\u533a\u522b\u4e8e\u4ee4\u724c\u7ea7\u5197\u957f\uff0c\u5b9e\u73b0\u4e8614\u4e2a\u6076\u610f\u5de5\u5177\u5206\u5e03\u5728\u4e09\u4e2a\u670d\u52a1\u5668\u4e0a\uff0c\u89e6\u53d1\u91cd\u590d\u3001\u5f3a\u5236\u7ec6\u5316\u548c\u5206\u6563\u6ce8\u610f\u529b\u7b49\u653b\u51fb\u6a21\u5f0f", "result": "\u653b\u51fb\u5728\u5f02\u6784\u6ce8\u518c\u8868\u548c\u591a\u4e2a\u652f\u6301\u5de5\u5177\u7684\u6a21\u578b\u4e2d\u9020\u6210\u4e25\u91cd\u8d44\u6e90\u653e\u5927\uff08\u6700\u9ad8\u8fbe142.4\u500d\u4ee4\u724c\uff09\uff0c\u5e76\u53ef\u80fd\u964d\u4f4e\u4efb\u52a1\u7ed3\u679c\u8d28\u91cf\uff1b\u89e3\u7801\u65f6\u7684\u7b80\u6d01\u63a7\u5236\u65e0\u6cd5\u53ef\u9760\u9632\u6b62\u5faa\u73af\u8bf1\u5bfc", "conclusion": "\u9632\u5fa1\u63aa\u65bd\u5e94\u57fa\u4e8e\u5de5\u5177\u8c03\u7528\u7ed3\u6784\u800c\u975e\u4ec5\u4ee4\u724c\u5c42\u9762\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u7ed3\u6784\u6027\u8fc7\u5ea6\u601d\u8003\u653b\u51fb"}}
{"id": "2602.14812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14812", "abs": "https://arxiv.org/abs/2602.14812", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "title": "Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque", "comment": null, "summary": "Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5df4\u65af\u514b\u8bed\u975e\u95ee\u7b54\u5f0f\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6BasPhyCo\uff0c\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u5df4\u65af\u514b\u8bed\u65b9\u8a00\u53d8\u4f53\u65f6\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u57fa\u672c\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u8003\u5bdf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5df4\u65af\u514b\u8bed\uff09\u975e\u95ee\u7b54\u5f0f\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ee5\u610f\u5927\u5229\u8bedGITA\u6570\u636e\u96c6\u4e3a\u57fa\u7840\uff0c\u521b\u5efa\u4e86\u5df4\u65af\u514b\u8bed\u975e\u95ee\u7b54\u5f0f\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6BasPhyCo\uff08\u5305\u542b\u6807\u51c6\u548c\u65b9\u8a00\u53d8\u4f53\uff09\u3002\u901a\u8fc7\u4e09\u4e2a\u5c42\u6b21\u8bc4\u4f30\u6a21\u578b\uff1a\u533a\u5206\u5408\u7406/\u4e0d\u5408\u7406\u53d9\u8ff0\u7684\u51c6\u786e\u6027\u3001\u8bc6\u522b\u5bfc\u81f4\u4e0d\u5408\u7406\u51b2\u7a81\u5143\u7d20\u7684\u8fde\u8d2f\u6027\u3001\u786e\u5b9a\u5177\u4f53\u7269\u7406\u72b6\u6001\u7684\u53ef\u9a8c\u8bc1\u6027\u3002\u4f7f\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u53ca\u4e13\u95e8\u9488\u5bf9\u610f\u5927\u5229\u8bed\u548c\u5df4\u65af\u514b\u8bed\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u53ef\u9a8c\u8bc1\u6027\u4efb\u52a1\u4e0a\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5df4\u65af\u514b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u6709\u9650\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65b9\u8a00\u53d8\u4f53\u65f6\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u65b9\u8a00\u53d8\u4f53\u65f6\u3002BasPhyCo\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2602.14267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14267", "abs": "https://arxiv.org/abs/2602.14267", "authors": ["Manal Rahal", "Bestoun S. Ahmed", "Roger Renstr\u00f6m", "Robert Stener"], "title": "Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting", "comment": "8 pages", "summary": "With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.\n  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.", "AI": {"tldr": "DELTAiF\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5bb6\u5ead\u70ed\u6c34\u6d88\u8017\uff0c\u901a\u8fc7\u4ece\u4ee3\u8868\u6027\u5bb6\u5ead\u5b66\u4e60\u77e5\u8bc6\u5e76\u5fae\u8c03\u5230\u5176\u4ed6\u5bb6\u5ead\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u70ed\u6c34\u9700\u6c42\u9884\u6d4b\uff0c\u51cf\u5c1167%\u8bad\u7ec3\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u4f4f\u5b85\u70ed\u6cf5\u5b89\u88c5\u7684\u5feb\u901f\u589e\u957f\uff0c\u4f18\u5316\u5bb6\u5ead\u70ed\u6c34\u751f\u4ea7\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u6280\u672f\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u5bb6\u5ead\u5355\u72ec\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e91\u8fde\u63a5\u70ed\u6cf5\u90e8\u7f72\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faDELTAiF\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u4ee3\u8868\u6027\u5bb6\u5ead\u5b66\u4e60\u77e5\u8bc6\u5e76\u5fae\u8c03\u5230\u5176\u4ed6\u5bb6\u5ead\uff0c\u9884\u6d4b\u5927\u578b\u70ed\u6c34\u4f7f\u7528\u4e8b\u4ef6\uff08\u5982\u6dcb\u6d74\uff09\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u70ed\u6cf5\u5b89\u88c5\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u7ea667%\uff0c\u9884\u6d4b\u7cbe\u5ea6\u57280.874-0.991\u4e4b\u95f4\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u57280.001-0.017\u4e4b\u95f4\u3002\u5f53\u6e90\u5bb6\u5ead\u6d88\u8d39\u6a21\u5f0f\u89c4\u5f8b\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u5c24\u5176\u663e\u8457\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5bb6\u5ead\u70ed\u6c34\u9700\u6c42\u9884\u6d4b\uff0cDELTAiF\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6e90\u5bb6\u5ead\u6d88\u8d39\u6a21\u5f0f\u89c4\u5f8b\u7684\u60c5\u51b5\u3002"}}
{"id": "2602.14229", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14229", "abs": "https://arxiv.org/abs/2602.14229", "authors": ["Abubakarr Jaye", "Nigel Boachie Kumankumah", "Chidera Biringa", "Anjel Shaileshbhai Patel", "Sulaiman Vesal", "Dayquan Julienne", "Charlotte Siska", "Manuel Ra\u00fal Mel\u00e9ndez Luj\u00e1n", "Anthony Twum-Barimah", "Mauricio Velazco", "Tianwei Chen"], "title": "CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments", "comment": null, "summary": "Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMulti-Horizon Task Environments (MHTEs)\u95ee\u9898\u7c7b\u522b\uff0c\u5e76\u5f00\u53d1CorpGen\u6846\u67b6\u89e3\u51b3\u591a\u4efb\u52a1\u957f\u65f6\u7a0b\u63a8\u7406\u4e2d\u7684\u56db\u79cd\u5931\u6548\u6a21\u5f0f\uff0c\u5728\u6a21\u62df\u4f01\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b03.5\u500d\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u8bc4\u4f30\u5355\u4e2a\u4efb\u52a1\uff0c\u800c\u771f\u5b9e\u7ec4\u7ec7\u5de5\u4f5c\u9700\u8981\u7ba1\u7406\u591a\u4e2a\u5e76\u53d1\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u6d89\u53ca\u4efb\u52a1\u4ea4\u9519\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u4f18\u5148\u7ea7\u8c03\u6574\u3002\u9700\u8981\u89e3\u51b3\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u7684\u63a8\u7406\u6311\u6218\u3002", "method": "\u63d0\u51faCorpGen\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u89c4\u5212\u8fdb\u884c\u591a\u65f6\u7a0b\u76ee\u6807\u5bf9\u9f50\u3001\u5b50\u4ee3\u7406\u9694\u79bb\u9632\u6b62\u4efb\u52a1\u4ea4\u53c9\u6c61\u67d3\u3001\u5206\u5c42\u5185\u5b58\uff08\u5de5\u4f5c\u5185\u5b58\u3001\u7ed3\u6784\u5316\u5185\u5b58\u3001\u8bed\u4e49\u5185\u5b58\uff09\u548c\u81ea\u9002\u5e94\u6458\u8981\u3002\u901a\u8fc7\u6570\u5b57\u5458\u5de5\u6a21\u62df\u4f01\u4e1a\u73af\u5883\u3002", "result": "\u5728OSWorld Office\u4e0a\uff0cCorpGen\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u6700\u9ad83.5\u500d\u6539\u8fdb\uff0815.2% vs 4.3%\uff09\uff0c\u5728\u8d1f\u8f7d\u589e\u52a0\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u7ecf\u9a8c\u5b66\u4e60\u8d21\u732e\u6700\u5927\u3002", "conclusion": "CorpGen\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u65f6\u7a0b\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u56db\u79cd\u5931\u6548\u6a21\u5f0f\uff0c\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\u67b6\u6784\u673a\u5236\u800c\u975e\u7279\u5b9aCUA\u5b9e\u73b0\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u591a\u4efb\u52a1\u957f\u65f6\u7a0b\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14338", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14338", "abs": "https://arxiv.org/abs/2602.14338", "authors": ["Zhi Zhang", "Zhen Han", "Costas Mavromatis", "Qi Zhu", "Yunyi Zhang", "Sheng Guan", "Dingmin Wang", "Xiong Zhou", "Shuai Wang", "Soji Adeshina", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.", "AI": {"tldr": "AERO\u6539\u8fdbGRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u3001\u9009\u62e9\u6027\u62d2\u7edd\u548c\u8d1d\u53f6\u65af\u540e\u9a8c\u9632\u6b62\u96f6\u68af\u5ea6\u6b7b\u533a\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u51cf\u5c1148%\u8ba1\u7b97\u91cf\u548c45%\u5355\u6b65\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "GRPO\u5728RLVR\u5fae\u8c03\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5f53\u7ec4\u5185\u6240\u6709rollout\u7ed3\u679c\u76f8\u540c\u65f6\uff08\u5168\u5bf9\u6216\u5168\u9519\uff09\uff0c\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u4e3a\u96f6\uff0c\u5bfc\u81f4\u65e0\u68af\u5ea6\u4fe1\u53f7\u548c\u8ba1\u7b97\u6d6a\u8d39\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAERO\u65b9\u6cd5\uff1a1) \u81ea\u9002\u5e94rollout\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u6570\u91cf\uff1b2) \u9009\u62e9\u6027\u62d2\u7edd\u7b56\u7565\uff0c\u7b56\u7565\u6027\u4fee\u526arollout\uff1b3) \u8d1d\u53f6\u65af\u540e\u9a8c\u7ef4\u62a4\uff0c\u9632\u6b62\u96f6\u4f18\u52bf\u6b7b\u533a\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\u914d\u7f6e\u4e0a\u9a8c\u8bc1\uff1a\u76f8\u540c\u603brollout\u9884\u7b97\u4e0b\uff0cAERO\u51cf\u5c11\u7ea648%\u603b\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff0c\u7f29\u77ed\u7ea645%\u5355\u6b65\u65f6\u95f4\uff0c\u540c\u65f6Pass@8\u548cAvg@8\u6307\u6807\u5339\u914d\u6216\u4f18\u4e8eGRPO\u3002", "conclusion": "AERO\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684RL-based LLM\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u3002"}}
{"id": "2602.14122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.14122", "abs": "https://arxiv.org/abs/2602.14122", "authors": ["Bingwen Zhu", "Yuqian Fu", "Qiaole Dong", "Guolei Sun", "Tianwen Qian", "Yuzheng Wu", "Danda Pani Paudel", "Xiangyang Xue", "Yanwei Fu"], "title": "EgoSound: Benchmarking Sound Understanding in Egocentric Videos", "comment": "17 pages", "summary": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.", "AI": {"tldr": "EgoSound\u662f\u9996\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u58f0\u97f3\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b7315\u4e2a\u9a8c\u8bc1\u8fc7\u7684\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d67\u4e2a\u4efb\u52a1\u7c7b\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u548c\u56e0\u679c\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4eba\u7c7b\u611f\u77e5\u672c\u8d28\u4e0a\u662f\u591a\u611f\u5b98\u7684\uff0c\u6574\u5408\u89c6\u89c9\u3001\u58f0\u97f3\u548c\u8fd0\u52a8\u6765\u7406\u89e3\u4e16\u754c\u3002\u58f0\u97f3\u63d0\u4f9b\u4e86\u5173\u4e8e\u7a7a\u95f4\u5e03\u5c40\u3001\u5c4f\u5e55\u5916\u4e8b\u4ef6\u548c\u56e0\u679c\u4ea4\u4e92\u7684\u91cd\u8981\u7ebf\u7d22\uff0c\u7279\u522b\u662f\u5728\u81ea\u6211\u4e2d\u5fc3\u8bbe\u7f6e\u4e2d\uff0c\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u7d27\u5bc6\u8026\u5408\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\uff0c\u7f3a\u4e4f\u5bf9\u58f0\u97f3\u7406\u89e3\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "1. \u6574\u5408Ego4D\u548cEgoBlind\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6709\u89c6\u89c9\u548c\u4f9d\u8d56\u58f0\u97f3\u7684\u4f53\u9a8c\n2. \u5b9a\u4e49\u4e03\u4efb\u52a1\u5206\u7c7b\u6cd5\uff1a\u5185\u5728\u58f0\u97f3\u611f\u77e5\u3001\u7a7a\u95f4\u5b9a\u4f4d\u3001\u56e0\u679c\u63a8\u7406\u548c\u8de8\u6a21\u6001\u63a8\u7406\n3. \u901a\u8fc7\u591a\u9636\u6bb5\u81ea\u52a8\u751f\u6210\u6d41\u7a0b\u6784\u5efaEgoSound\u57fa\u51c6\uff0c\u5305\u542b7315\u4e2a\u9a8c\u8bc1\u8fc7\u7684\u95ee\u7b54\u5bf9\uff0c\u8986\u76d6900\u4e2a\u89c6\u9891\n4. \u57289\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5c55\u73b0\u51fa\u65b0\u5174\u7684\u542c\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u548c\u56e0\u679c\u7406\u89e3\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002EgoSound\u4e3a\u63a8\u8fdb\u591a\u611f\u5b98\u81ea\u6211\u4e2d\u5fc3\u667a\u80fd\u5efa\u7acb\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u7840\u3002", "conclusion": "EgoSound\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u58f0\u97f3\u7406\u89e3\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u542c\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f25\u5408\"\u770b\u5230\"\u548c\"\u771f\u6b63\u542c\u5230\"\u4e16\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.14795", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14795", "abs": "https://arxiv.org/abs/2602.14795", "authors": ["Ivan Diliso", "Roberto Barile", "Claudia d'Amato", "Nicola Fanizzi"], "title": "Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs", "comment": null, "summary": "Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \\resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5305\u542b\u6a21\u5f0f\u5c42\u548c\u4e8b\u5b9e\u5c42\u77e5\u8bc6\u7684\u6570\u636e\u96c6\u63d0\u53d6\u5de5\u4f5c\u6d41\u53ca\u76f8\u5e94\u6570\u636e\u96c6\u5957\u4ef6\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u548c\u63a8\u7406\u670d\u52a1", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u7cbe\u5316\u7b97\u6cd5\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u901a\u5e38\u53ea\u5305\u542b\u4e8b\u5b9e\u5c42\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u6a21\u5f0f\u5c42\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u4f9d\u8d56\u4e30\u5bcc\u672c\u4f53\u7ea6\u675f\u3001\u63a8\u7406\u6216\u795e\u7ecf\u7b26\u53f7\u6280\u672f\u7684\u65b9\u6cd5\u8bc4\u4f30\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u573a\u666f", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u4f5c\u6d41\uff0c\u80fd\u591f\u4ece\u6e90\u77e5\u8bc6\u56fe\u8c31\u4e2d\u540c\u65f6\u63d0\u53d6\u6a21\u5f0f\u5c42\u548c\u4e8b\u5b9e\u5c42\u77e5\u8bc6\uff0c\u5904\u7406\u4e24\u8005\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5229\u7528\u63a8\u7406\u63a8\u5bfc\u9690\u542b\u77e5\u8bc6\uff0c\u5e76\u5c06\u7ed3\u679c\u5e8f\u5217\u5316\u4e3aOWL\u683c\u5f0f\uff0c\u540c\u65f6\u63d0\u4f9b\u52a0\u8f7d\u5230\u6807\u51c6\u673a\u5668\u5b66\u4e60\u5e93\u5f20\u91cf\u8868\u793a\u7684\u5b9e\u7528\u5de5\u5177", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u6a21\u5f0f\u5c42\u548c\u4e8b\u5b9e\u5c42\u77e5\u8bc6\u7684\u6570\u636e\u96c6\u8d44\u6e90\u5957\u4ef6\uff0c\u5305\u62ec\u4ece\u5177\u6709\u4e30\u5bcc\u6a21\u5f0f\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u7684\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5bf9\u73b0\u6709\u6570\u636e\u96c6\u7684\u6a21\u5f0f\u4fe1\u606f\u589e\u5f3a\uff0c\u6240\u6709\u6570\u636e\u96c6\u90fd\u4ee5OWL\u683c\u5f0f\u5e8f\u5217\u5316\uff0c\u652f\u6301\u63a8\u7406\u670d\u52a1", "conclusion": "\u8be5\u8d44\u6e90\u586b\u8865\u4e86\u77e5\u8bc6\u56fe\u8c31\u7cbe\u5316\u8bc4\u4f30\u4e2d\u7f3a\u4e4f\u6a21\u5f0f\u5c42\u4fe1\u606f\u7684\u7a7a\u767d\uff0c\u4e3a\u4f9d\u8d56\u672c\u4f53\u7ea6\u675f\u3001\u63a8\u7406\u548c\u795e\u7ecf\u7b26\u53f7\u6280\u672f\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u771f\u5b9e\u77e5\u8bc6\u56fe\u8c31\u573a\u666f\u4e0b\u7684\u65b9\u6cd5\u8bc4\u4f30"}}
{"id": "2602.14571", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2602.14571", "abs": "https://arxiv.org/abs/2602.14571", "authors": ["Qian Liyan", "Zhang Yao", "Yuan Ye", "Zhang Zhaoke", "Fang Jin", "Jiang Shimiao", "Zhang Jin", "Li Ke", "Liu Beijiang", "Xu Chenglin", "Zhang Yifan", "Jia Xiaoqian", "Qin Xiaoshuai", "Huang Xingtao"], "title": "DCTracks: An Open Dataset for Machine Learning-Based Drift Chamber Track Reconstruction", "comment": null, "summary": "We introduce a Monte Carlo (MC) dataset of single- and two-track drift chamber events to advance Machine Learning (ML)-based track reconstruction. To enable standardized and comparable evaluation, we define track reconstruction specific metrics and report results for traditional track reconstruction algorithms and a Graph Neural Networks (GNNs) method, facilitating rigorous, reproducible validation for future research.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u8f68\u8ff9\u91cd\u5efa\u7684\u8499\u7279\u5361\u6d1b\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6bd4\u8f83\u4f20\u7edf\u7b97\u6cd5\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5", "motivation": "\u4e3a\u63a8\u8fdb\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8f68\u8ff9\u91cd\u5efa\u7814\u7a76\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u4fbf\u8fdb\u884c\u53ef\u91cd\u590d\u3001\u53ef\u6bd4\u8f83\u7684\u9a8c\u8bc1", "method": "\u521b\u5efa\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7684\u5355\u8f68\u8ff9\u548c\u53cc\u8f68\u8ff9\u6f02\u79fb\u5ba4\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u8f68\u8ff9\u91cd\u5efa\u4e13\u7528\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5b9e\u73b0\u4f20\u7edf\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u5bf9\u6bd4", "result": "\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u4f20\u7edf\u7b97\u6cd5\u548cGNN\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e86\u672a\u6765\u7814\u7a76\u7684\u53ef\u91cd\u590d\u9a8c\u8bc1", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u8f68\u8ff9\u91cd\u5efa\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u7ed3\u679c\uff0c\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u53d1\u5c55\u548c\u6027\u80fd\u63d0\u5347"}}
{"id": "2602.14587", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.14587", "abs": "https://arxiv.org/abs/2602.14587", "authors": ["Minh Nguyen"], "title": "Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow", "comment": null, "summary": "Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u7684\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u66f4\u65b0\u89e3\u51b3\u6807\u51c6\u79bb\u6563\u65f6\u95f4RL\u5728\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u9a71\u52a8\u63a7\u5236\u4e2d\u7684Q\u51fd\u6570\u9000\u5316\u95ee\u9898\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5b9e\u9645\u4ea4\u6613\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u63a7\u5236\u95ee\u9898\uff08\u91d1\u878d\u3001\u673a\u5668\u4eba\u7b49\uff09\u5728\u8fde\u7eed\u65f6\u95f4\u4e2d\u6f14\u5316\uff0c\u5177\u6709\u975e\u5747\u5300\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u51b3\u7b56\u7279\u6027\u3002\u6807\u51c6\u79bb\u6563\u65f6\u95f4RL\u57fa\u4e8e\u56fa\u5b9a\u6b65\u957fBellman\u66f4\u65b0\uff0c\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u5b58\u5728\u56f0\u96be\uff1a\u5f53\u65f6\u95f4\u95f4\u9694\u7f29\u5c0f\u65f6\uff0cQ\u51fd\u6570\u4f1a\u9000\u5316\u4e3a\u503c\u51fd\u6570V\uff0c\u5931\u53bb\u52a8\u4f5c\u6392\u5e8f\u80fd\u529b\u3002\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u901a\u8fc7\u4f18\u52bf\u7387\u51fd\u6570q\u91cd\u65b0\u5f15\u5165\u52a8\u4f5c\u4fe1\u606f\uff0c\u4f46\u4f7f\u7528\u590d\u6742\u7684\u9785\u635f\u5931\u6216\u6b63\u4ea4\u7ea6\u675f\u6765\u5f3a\u5236\u6700\u4f18\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u6d4b\u8bd5\u8fc7\u7a0b\u9009\u62e9\u654f\u611f\uff0c\u5e76\u5c06V\u548cq\u8026\u5408\u5230\u96be\u4ee5\u53ef\u9760\u8bad\u7ec3\u7684\u5927\u578b\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e2d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u7684\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u91c7\u7528\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\uff1a1) q\u901a\u8fc7V\u7684\u6269\u6563\u751f\u6210\u5668\u5b66\u4e60\uff1b2) V\u901a\u8fc7\u57fa\u4e8e\u54c8\u5bc6\u987f\u91cf\u7684\u503c\u6d41\u66f4\u65b0\uff0c\u8be5\u503c\u6d41\u5728\u65e0\u7a77\u5c0f\u65f6\u95f4\u6b65\u4e0b\u4ecd\u4fdd\u6301\u4fe1\u606f\u6027\uff08\u800c\u6807\u51c6\u7684max/softmax\u5907\u4efd\u4f1a\u5931\u6548\uff09\u3002\u7406\u8bba\u4e0a\u901a\u8fc7\u65b0\u7684\u6982\u7387\u8bba\u8bc1\u8bc1\u660e\u4e25\u683c\u6536\u655b\uff0c\u7ed5\u8fc7\u4e86\u751f\u6210\u5668\u57fa\u54c8\u5bc6\u987f\u91cf\u5728sup\u8303\u6570\u4e0b\u7f3a\u4e4fBellman\u5f0f\u6536\u7f29\u7684\u6311\u6218\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5b9e\u9645\u4ea4\u6613\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u548c\u9886\u5148\u7684\u79bb\u6563\u65f6\u95f4\u57fa\u7ebf\uff0c\u5728\u5355\u4e2a\u5b63\u5ea6\u5185\u5b9e\u73b0\u4e8621%\u7684\u5229\u6da6\uff0c\u51e0\u4e4e\u662f\u7b2c\u4e8c\u4f73\u65b9\u6cd5\u7684\u4e24\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4RL\u4e2dQ\u51fd\u6570\u9000\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u7684\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u5b9e\u73b0\u4e86\u53ef\u9760\u8bad\u7ec3\u548c\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u9a71\u52a8\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.14376", "abs": "https://arxiv.org/abs/2602.14376", "authors": ["Yuliang Wu", "Wei Zhai", "Yuxin Cui", "Tiesong Zhao", "Yang Cao", "Zheng-Jun Zha"], "title": "Event-based Visual Deformation Measurement", "comment": null, "summary": "Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.", "AI": {"tldr": "\u63d0\u51fa\u4e8b\u4ef6-\u5e27\u878d\u5408\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u63d0\u4f9b\u65f6\u95f4\u5bc6\u96c6\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5e27\u63d0\u4f9b\u7a7a\u95f4\u5bc6\u96c6\u7cbe\u786e\u4f30\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6548\u53d8\u5f62\u6d4b\u91cf", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u4f9d\u8d56\u5e27\u95f4\u5fae\u5c0f\u8fd0\u52a8\u6765\u7ea6\u675f\u5bf9\u5e94\u641c\u7d22\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u5ea6\u52a8\u6001\u573a\u666f\u7684\u5e94\u7528\uff0c\u6216\u9700\u8981\u9ad8\u901f\u76f8\u673a\u5e26\u6765\u9ad8\u6602\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500", "method": "\u63d0\u51fa\u4e8b\u4ef6-\u5e27\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u7684\u65f6\u95f4\u5bc6\u96c6\u8fd0\u52a8\u7ebf\u7d22\u548c\u5e27\u7684\u7a7a\u95f4\u5bc6\u96c6\u7cbe\u786e\u4f30\u8ba1\uff1b\u5f15\u5165\u4eff\u5c04\u4e0d\u53d8\u5355\u7eaf\u5f62(AIS)\u6846\u67b6\uff0c\u5c06\u53d8\u5f62\u573a\u5212\u5206\u4e3a\u7ebf\u6027\u5316\u5b50\u533a\u57df\uff1b\u91c7\u7528\u90bb\u57df\u8d2a\u5a6a\u4f18\u5316\u7b56\u7565\u52a0\u901f\u53c2\u6570\u641c\u7d22\u5e76\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef", "result": "\u5728\u5305\u542b120\u591a\u4e2a\u5e8f\u5217\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u751f\u5b58\u7387\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf1.6%\uff1b\u4ec5\u4f7f\u7528\u9ad8\u901f\u89c6\u9891\u65b9\u6cd518.9%\u7684\u6570\u636e\u5b58\u50a8\u548c\u5904\u7406\u8d44\u6e90", "conclusion": "\u4e8b\u4ef6-\u5e27\u878d\u5408\u6846\u67b6\u7ed3\u5408AIS\u5efa\u6a21\u548c\u90bb\u57df\u8d2a\u5a6a\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u53d8\u5f62\u6d4b\u91cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.14523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.14523", "abs": "https://arxiv.org/abs/2602.14523", "authors": ["Robinson Umeike", "Thang Dao", "Shane Crawford", "John van de Lindt", "Blythe Johnston", "Wanting", "Wang", "Trung Do", "Ajibola Mofikoya", "Sarbesh Banjara", "Cuong Pham"], "title": "Architectural Insights for Post-Tornado Damage Recognition", "comment": null, "summary": "Rapid and accurate building damage assessment in the immediate aftermath of tornadoes is critical for coordinating life-saving search and rescue operations, optimizing emergency resource allocation, and accelerating community recovery. However, current automated methods struggle with the unique visual complexity of tornado-induced wreckage, primarily due to severe domain shift from standard pre-training datasets and extreme class imbalance in real-world disaster data. To address these challenges, we introduce a systematic experimental framework evaluating 79 open-source deep learning models, encompassing both Convolutional Neural Networks (CNNs) and Vision Transformers, across over 2,300 controlled experiments on our newly curated Quad-State Tornado Damage (QSTD) benchmark dataset. Our findings reveal that achieving operational-grade performance hinges on a complex interaction between architecture and optimization, rather than architectural selection alone. Most strikingly, we demonstrate that optimizer choice can be more consequential than architecture: switching from Adam to SGD provided dramatic F1 gains of +25 to +38 points for Vision Transformer and Swin Transformer families, fundamentally reversing their ranking from bottom-tier to competitive with top-performing CNNs. Furthermore, a low learning rate of 1x10^(-4) proved universally critical, boosting average F1 performance by +10.2 points across all architectures. Our champion model, ConvNeXt-Base trained with these optimized settings, demonstrated strong cross-event generalization on the held-out Tuscaloosa-Moore Tornado Damage (TMTD) dataset, achieving 46.4% Macro F1 (+34.6 points over baseline) and retaining 85.5% Ordinal Top-1 Accuracy despite temporal and sensor domain shifts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8679\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9f99\u5377\u98ce\u5efa\u7b51\u635f\u4f24\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f18\u5316\u5668\u9009\u62e9\u6bd4\u67b6\u6784\u9009\u62e9\u66f4\u91cd\u8981\uff0cSGD\u4f18\u5316\u5668\u914d\u5408\u4f4e\u5b66\u4e60\u7387\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9f99\u5377\u98ce\u540e\u5feb\u901f\u51c6\u786e\u7684\u5efa\u7b51\u635f\u4f24\u8bc4\u4f30\u5bf9\u6551\u63f4\u548c\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u9f99\u5377\u98ce\u7834\u574f\u573a\u666f\u4e0e\u6807\u51c6\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u9886\u57df\u504f\u79fb\uff1b2) \u771f\u5b9e\u707e\u5bb3\u6570\u636e\u4e2d\u5b58\u5728\u6781\u7aef\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86Quad-State Tornado Damage (QSTD)\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e8679\u4e2a\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecCNN\u548cVision Transformer\uff09\uff0c\u8fdb\u884c\u4e86\u8d85\u8fc72300\u4e2a\u63a7\u5236\u5b9e\u9a8c\u3002\u91cd\u70b9\u5173\u6ce8\u67b6\u6784\u4e0e\u4f18\u5316\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u7279\u522b\u662f\u4f18\u5316\u5668\u9009\u62e9\u548c\u5b66\u4e60\u7387\u8bbe\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u4f18\u5316\u5668\u9009\u62e9\u6bd4\u67b6\u6784\u9009\u62e9\u66f4\u91cd\u8981\uff0c\u4eceAdam\u5207\u6362\u5230SGD\u4f7fVision Transformer\u548cSwin Transformer\u7684F1\u5206\u6570\u63d0\u534725-38\u70b9\uff1b2) \u4f4e\u5b66\u4e60\u7387(1x10^(-4))\u5bf9\u6240\u6709\u67b6\u6784\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u5e73\u5747\u63d0\u5347F1\u5206\u657010.2\u70b9\uff1b3) \u6700\u4f73\u6a21\u578bConvNeXt-Base\u5728\u8de8\u4e8b\u4ef6\u6d4b\u8bd5\u4e2d\u8fbe\u523046.4% Macro F1\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534734.6\u70b9\u3002", "conclusion": "\u9f99\u5377\u98ce\u635f\u4f24\u8bc4\u4f30\u4e2d\uff0c\u5b9e\u73b0\u64cd\u4f5c\u7ea7\u6027\u80fd\u7684\u5173\u952e\u5728\u4e8e\u67b6\u6784\u4e0e\u4f18\u5316\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u800c\u975e\u5355\u7eaf\u67b6\u6784\u9009\u62e9\u3002\u4f18\u5316\u5668\u9009\u62e9\u548c\u5b66\u4e60\u7387\u8bbe\u7f6e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u53ef\u80fd\u8d85\u8fc7\u67b6\u6784\u672c\u8eab\uff0c\u8fd9\u4e00\u53d1\u73b0\u5bf9\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
