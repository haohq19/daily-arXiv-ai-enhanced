<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程实现参数高效的领域自适应方法，仅需训练视觉提示层而不修改主干网络参数，显著减少了训练参数和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个源-目标对微调主干网络参数，导致训练参数和存储需求线性增长，且无法重用训练好的主干参数。

Method: 在主干网络前添加领域特定的视觉重编程层，生成视觉提示作为纹理偏置来适应目标域风格，使用多目标函数优化域内和域间分布差异。

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，优于现有参数高效UDA基线方法PDA，且参数使用量仅为46%。

Conclusion: VirDA实现了参数高效的领域自适应，在保持高性能的同时显著减少了训练参数需求，支持主干网络在不同域间的重用。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [2] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出了MemoryPack和Direct Forcing两个创新方法来解决长视频生成中的长期依赖建模和错误累积问题，显著提升了自回归视频模型的实用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕捉长期依赖关系，同时防止自回归解码中固有的错误累积。现有方法在保持时间一致性和计算效率方面存在不足。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖，实现分钟级时间一致性；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理过程中的错误传播。

Result: 提出的方法能够优雅地扩展视频长度，保持计算效率，维持线性复杂度，显著增强了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同推进了自回归视频模型的实用化，为长视频生成提供了有效的解决方案。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [3] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT是一个统一的多模态大语言模型范式，通过视觉参考令牌直接生成文本和多样化视觉输出，解决了现有方法依赖间接表示的限制。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉任务中依赖间接表示（如将坐标生成为文本），限制了性能并阻碍了密集预测任务（如分割）。

Method: 提出视觉参考令牌(VRTs)，从查询图像的视觉补丁嵌入中派生，与LLM的输出文本令牌无缝交错；使用轻量级解码器将LLM输出转换为检测、分割和定位预测；动态扩展嵌入表以改进定位和区分相似对象。

Result: 在四个视觉感知和理解任务上的实证研究表明，PaDT始终达到最先进的性能，甚至与显著更大的MLLM模型相比也是如此。

Conclusion: PaDT为MLLMs提供了一个统一的范式，能够直接生成文本和多样化视觉输出，在多个视觉任务上表现出色。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [4] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 提出F2C方法，通过选择关键片段而非单帧来保持时间连续性，结合自适应分辨率策略在固定计算预算下提升视频理解性能


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型面临'大海捞针'问题：原始视频帧产生的大量视觉标记耗尽模型上下文窗口。现有解决方案通过选择稀疏帧集来减少标记数量，但这种逐帧选择丢弃了基本的时间动态，导致对运动和事件连续性的推理效果不佳

Method: 提出F2C方法：1）从孤立关键帧扩展到关键片段（短时间相干段）；2）采用自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定

Result: 在三个长视频基准测试中，F2C方法优于均匀采样：Video-MME提升8.1%，LongVideoBench提升5.6%，MLVU提升10.3%

Conclusion: 研究强调了在帧选择中保持时间连贯性的重要性，并为视频大语言模型扩展到实际视频理解应用提供了实用途径

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 本文从变分互信息下界出发，推导出两种自监督表示学习范式：自蒸馏互信息（SDMI）和联合互信息（JMI），为现有方法的架构组件提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 自监督表示学习虽然取得了显著经验成功，但其基本原理仍不够清晰。现有工作往往将预测器网络、停止梯度操作等架构元素视为经验性添加，缺乏理论依据。

Method: 从变分互信息下界出发，推导出SDMI和JMI两种训练范式。SDMI需要交替优化，使停止梯度操作成为理论必需；JMI允许通过对称架构进行联合优化。

Result: 证明预测器网络在SDMI中、统计正则器在JMI中都是互信息目标的可行替代。许多现有自监督学习方法都是这两种范式的具体实例或近似。

Conclusion: 为现有自监督表示学习方法的架构组件选择提供了超越启发式便利的理论解释，统一了看似不同的方法。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [6] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出了一种量化数据集各向异性（对称性破缺）的度量方法，通过双样本神经网络分类器测试来区分原始数据集与其随机增强版本。研究发现多个基准点云数据集存在高度对齐，且分布对称性破缺会阻碍不变方法的最优性能。


<details>
  <summary>Details</summary>
Motivation: 对称感知方法（如数据增强和等变架构）假设变换后的数据点在测试分布中具有高概率或"重要性"。本文旨在批判性评估这一假设，开发量化数据集各向异性的方法。

Method: 提出基于双样本神经网络分类器的度量方法，通过区分原始数据集和随机增强数据集来量化各向异性。在合成数据集上验证该方法，并应用于多个基准点云数据集。

Result: 在多个基准点云数据集中发现了令人惊讶的高度对齐现象。理论分析表明，分布对称性破缺会阻止不变方法达到最优性能，即使底层标签确实是不变的。

Conclusion: 等变方法的有效性是数据集依赖的：在某些各向异性数据集上仍有益处，但在其他数据集上则不然。理解等变性需要重新思考数据中的对称性偏差。

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [7] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: 这篇综述系统性地分析了边缘人工智能(Edge AI)的发展历程、当前现状和未来方向，涵盖部署位置、处理能力、应用领域和硬件类型等多个维度，并探讨了关键技术、挑战和新兴机遇。


<details>
  <summary>Details</summary>
Motivation: 边缘AI通过在网络边缘设备中嵌入智能，实现近数据源的实时处理，具有改善隐私保护和降低延迟的优势。本文旨在为研究者和从业者提供该领域的全面框架。

Method: 采用PRISMA指南进行系统性分析，通过多维度分类法（包括部署位置、处理能力如TinyML和联邦学习、应用领域、硬件类型）来审视边缘AI的发展。

Result: 分析了边缘AI从早期内容分发网络和雾计算到现代设备端智能的演进历程，探讨了专用硬件加速器、优化软件和通信协议等核心技术，评估了资源限制、安全、模型管理、功耗和连接性等挑战。

Conclusion: 边缘AI领域存在巨大的发展潜力，特别是在神经形态硬件、持续学习算法、边缘-云协作和可信集成等新兴机遇方面，为未来研究和应用提供了重要方向。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [8] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 提出Flock模型，通过概率节点-关系等变性解决知识图谱零样本链接预测问题，在54个知识图谱上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱基础模型的确定性等变性限制了表达能力，无法区分结构相似但语义不同的关系

Method: 引入概率节点-关系等变性，通过随机游走采样、序列编码、序列模型嵌入和池化聚合来表示节点和关系

Result: 在新诊断数据集Petals上完美解决现有模型失败的问题，在54个知识图谱的实体和关系预测任务中达到最优性能

Conclusion: Flock模型通过概率等变性突破了传统模型的表达能力限制，是知识图谱同构不变链接级函数的通用逼近器

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [9] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: 本研究开发了一个预测框架，使用FDA兽医药物不良事件报告数据来分类动物用药安全结果（死亡vs康复），结合数据工程、机器学习和可解释AI方法，旨在早期识别高风险药物事件特征。


<details>
  <summary>Details</summary>
Motivation: 确保食品生产动物用药安全对保护动物福利和人类食品安全至关重要。不良事件可能预示意外的药代动力学或毒代动力学效应，增加食品链中违规残留风险，需要开发预测工具来支持风险评估和监管决策。

Method: 使用约128万份FDA兽医药物不良事件报告，通过数据预处理管道整合关系表并标准化不良事件。采用多种监督学习模型（随机森林、CatBoost、XGBoost、ExcelFormer、大语言模型），处理类别不平衡问题，并集成平均不确定性边际伪标注方法。

Result: 集成方法和CatBoost表现最佳，精确度、召回率和F1分数均达到0.95。结合AUM伪标注提高了少数类检测能力。SHAP可解释性分析识别出与致命结果相关的生物学合理预测因子。

Conclusion: 该框架表明，结合严格的数据工程、先进机器学习和可解释AI能够实现准确、可解释的兽医安全结果预测，支持FARAD任务，加强残留风险评估，为监管和临床决策提供信息。

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [10] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种绕过提示防护的新攻击方法，该方法能够持续越狱生产模型，同时保持响应质量，揭示了轻量级提示防护在现代LLM架构中的固有攻击面。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，确保AI安全和对齐至关重要。提示防护是一种流行的轻量级机制，旨在过滤恶意查询，但本文发现其存在局限性。

Method: 攻击方法利用提示防护与主LLM之间的资源不对称性，编码一个轻量级防护无法解码但主模型可以解码的越狱提示。

Result: 该方法成功越狱了Google Gemini、DeepSeek Chat、Grok和Mistral Le Chat等高度保护的聊天接口生产模型。

Conclusion: 研究揭示了轻量级提示防护在现代LLM架构中的固有攻击面，强调需要将防御重点从阻止恶意输入转向防止恶意输出，并识别了其他关键对齐问题。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [11] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: MIRA是一种无需训练、在推理时对齐扩散模型的方法，通过图像空间的KL代理正则化来防止奖励黑客问题，在保持提示一致性的同时提高奖励分数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像虽然符合文本提示，但往往无法满足用户特定的标量奖励标准（如美学评分）。现有的推理时对齐方法存在奖励黑客问题，即模型生成高评分但偏离原始提示的图像。

Method: 提出MIRA方法，引入基于分数的图像空间KL代理正则化，约束采样轨迹以防止奖励黑客。还提出了MIRA-DPO，将偏好优化映射到推理时，适用于不可微分奖励。

Result: 在SDv1.5和SDXL模型上，针对多种奖励（美学评分、HPSv2、PickScore）和公共数据集，MIRA相比强基线获得超过60%的胜率，同时保持提示一致性。机制图显示奖励增益接近零漂移，而DNO方法随计算增加而漂移。

Conclusion: MIRA是一种有效的推理时对齐方法，能够在不进行微调的情况下提高扩散模型的奖励分数，同时防止奖励黑客问题，保持生成图像与原始提示的一致性。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [12] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: 本文分析了RLHF中KL散度正则化的不同实现方式，证明了'k1 in reward'和'k2 as loss'在策略条件下梯度等价，都是Reverse KL正则化的理论正确实现，而'k3 as loss'只是一阶有偏近似。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法中KL正则化的实现存在混淆，有些方法将数学项作为数值估计系数而非优化损失函数，需要建立统一框架来分析不同实现方式的理论基础。

Method: 建立统一框架连接两种实现风格：将数学项作为策略得分函数的分离系数或作为直接损失函数。通过梯度分析证明不同实现方式的等价性和偏差。

Result: 证明'k1 in reward'是Reverse KL正则化的原则性损失，'k2 as loss'在策略条件下与其梯度等价，而'k3 as loss'只是一阶有偏近似。还发现离策略实现存在重要性采样偏差。

Conclusion: 为KL正则化的选择和正确实现提供了基于梯度的理论依据，有助于构建更稳健有效的RLHF系统。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [13] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fernández*

Main category: cs.LG

TL;DR: Poolformer是一种序列到序列模型，用循环层和池化操作替代自注意力机制，解决了长序列处理中自注意力二次复杂度的问题，在音频处理任务中表现优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长序列处理中存在二次复杂度问题，限制了其实际应用。研究者希望开发一种更高效的序列建模方法来处理长序列数据。

Method: 使用循环层替代自注意力，引入池化操作减少序列长度。模型采用递归定义的SkipBlocks结构，包含残差块、下采样池化层、嵌套SkipBlock、上采样池化层和额外残差块。

Result: 池化显著加速训练，改善感知指标（FID和IS），防止过拟合。在原始音频处理任务中，Poolformer超越了SaShiMi和Mamba等先进模型。深层处理长程依赖，浅层处理短期特征。

Conclusion: Poolformer为长序列处理提供了高效解决方案，未来可扩展到文本、视觉和多模态应用中，基于Poolformer的大语言模型可有效处理图像和视频的密集表示。

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [14] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 使用强化学习来对抗反应式干扰，通过自适应调整发射功率、调制方式和信道选择来优化吞吐量


<details>
  <summary>Details</summary>
Motivation: 研究如何应对采用动态策略选择信道和感知阈值的反应式干扰器，在缺乏信道条件和干扰策略先验知识的情况下避免干扰

Method: 采用Q-learning处理离散干扰事件状态，使用深度Q网络(DQN)处理基于接收功率的连续状态，通过不同的奖励函数和动作集进行学习

Result: 强化学习能够快速适应频谱动态变化，在信道和干扰策略随时间变化时保持高传输速率

Conclusion: 强化学习是应对反应式干扰的有效方法，能够自适应地优化传输性能

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个评估多平台代理环境中长期记忆和状态跟踪的基准测试，专注于企业环境中的动态工作流程，整合Slack、Linear和Git等平台的异步事件。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文和记忆基准测试主要关注对话场景，但企业环境中记忆评估的需求对于有效应用至关重要。

Method: 通过手动专家设计和基于代理的可扩展合成来构建数据集，模拟真实的软件开发流程，创建包含噪声、冲突和跨平台引用的时间线。

Result: 实验显示最先进的LLM和记忆后端在长时程记忆利用、跨平台依赖处理和矛盾解决方面面临挑战，表现最佳的GPT-5模型仅达到60%的正确性得分。

Conclusion: 该工作为记忆增强代理的评估研究提供了可扩展框架，超越了现有的对话设置焦点，为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [16] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个利用LLM将原始系统日志转换为基于本体的知识图谱的AI代理，通过RAG和迭代校正确保图谱质量，并能将日志证据映射到MITRE ATT&CK战术。


<details>
  <summary>Details</summary>
Motivation: 系统日志是重要的网络威胁情报来源，但由于缺乏结构、语义不一致和跨设备碎片化，其效用受限。需要方法将噪声异构数据转化为连贯可互操作的表示。

Method: 集成轻量级日志本体与RAG和迭代校正步骤，确保生成的知识图谱语法和语义有效；聚合图谱到会话级别，使用LLM预测MITRE ATT&CK战术。

Result: 在公共基准和真实蜜罐数据集上评估，展示了跨多个图谱后端的稳健生成能力，以及将对抗活动准确映射到ATT&CK战术的能力。检索和校正提高了精确率和召回率。

Conclusion: 基于本体的表示为可操作的网络威胁情报提取提供了价值，代码导向模型在结构化日志分析中有效，检索和校正对精度和召回率有益。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [17] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种基于稀疏自编码器和聚类技术的方法，用于分析大语言模型的内部token表示，并通过构建token转移图来指导数学推理任务的生成过程。


<details>
  <summary>Details</summary>
Motivation: 旨在通过分析LLM的内部表示来理解其推理过程，并找到在数学推理任务中平衡利用已知推理轨迹和探索新路径的方法，以提高推理质量。

Method: 首先训练稀疏自编码器生成稀疏向量表示，然后使用k-means聚类构建token转移图，基于边权重定义奖励函数来量化推理轨迹的遵循程度，同时通过聚类多样性评估探索程度。

Result: 研究发现平衡利用和探索对于数学推理任务的高准确性至关重要，稀疏自编码器可作为可扩展的奖励模型来指导生成过程。

Conclusion: 该方法能够防止极端行为，促进更高质量的推理过程，在数学推理任务中实现利用和探索之间的平衡权衡。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [18] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文提出了一种对抗性逆强化学习方法，用于学习密集的、令牌级别的推理奖励模型，该模型可直接从专家演示中学习，为语言模型推理提供过程监督。


<details>
  <summary>Details</summary>
Motivation: 重新构建和操作对抗性逆强化学习到大型语言模型推理中，旨在学习密集的令牌级别奖励模型，为过程监督提供直接反馈，而不是通过监督微调模仿风格。

Method: 使用对抗性逆强化学习方法学习推理奖励模型，该模型在训练期间提供步骤级反馈来优化推理策略，在推理时作为评论家对采样轨迹进行重排序。

Result: 在GSM8K数据集上使用Llama3和Qwen2.5骨干网络进行实验，证明密集推理奖励可作为学习信号引发推理，并且通过奖励引导的重排序提高了预测性能。

Conclusion: 通过将训练信号、推理时间选择和令牌级别诊断统一到单个推理奖励中，这项工作表明可重用的过程级别奖励具有增强语言模型中多步推理的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [19] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个用于表格异常检测的基准数据集，通过恢复文本语义来支持上下文感知的研究。它提供20个包含结构化文本元数据的表格数据集，并实现了多种异常检测算法，包括零样本LLM框架。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文，而实际应用中异常定义与领域特定语境密切相关。这限制了研究灵活性，阻碍模型充分利用领域知识进行检测。

Method: 提供20个精心策划的表格数据集，包含结构化文本元数据；实现包括经典、深度学习和基于LLM的先进异常检测算法；开发零样本LLM框架，无需任务特定训练即可利用语义上下文。

Result: 语义上下文提高了检测性能，并通过支持领域感知推理增强了可解释性。实验分析揭示了文本元数据在异常检测中的作用和效用。

Conclusion: ReTabAD为系统探索上下文感知异常检测建立了基准，证明语义上下文对提升检测性能和可解释性的重要性。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出基于置信度感知的路由系统，在生成前评估模型不确定性，根据可靠性估计重定向查询，显著减少幻觉并降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，生成看似合理但事实错误的内容。现有后生成修正方法计算成本高且无法预防不可靠内容生成

Method: 结合三种互补信号：内部表示与参考嵌入的语义对齐、跨模型层的内部收敛分析、学习置信度估计，统一置信度分数决定四种路由路径

Result: 在知识密集型QA基准测试中，幻觉检测显著提升（0.74 vs 0.42基线），计算成本降低40%，F1分数从0.61提升到0.82，假阳性率低至0.09

Conclusion: 从反应式修正转向主动评估的范式转变，为LLM可靠性增强提供了计算高效的方法

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [21] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob是一个基于LLM的语义合成管道，通过多智能体框架从复杂在线文本中提取时空相关文本，结合渐进融合架构将细粒度上下文与时空数据融合，显著提升人类移动性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型难以利用描述外部事件的文本信息来预测人类移动性的突然变化，需要一种能够有效整合文本语义和时空数据的方法。

Method: 使用LLM驱动的多智能体框架自动提取和推理时空相关文本，通过创新的渐进融合架构将文本上下文与时空数据结合，利用预训练的事件先验知识。

Result: 在构建的数据集上评估，SeMob相比纯时空模型在MAE上最大降低13.92%，在RMSE上最大降低11.12%，在事件发生时空邻近区域表现尤为突出。

Conclusion: SeMob框架通过有效整合文本语义和时空数据，显著提升了事件驱动的人类移动性预测性能，特别是在事件相关时空区域表现优越。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [22] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 提出了一种名为渐进式自我反思(PSR)的推理时技术，使大语言模型能够动态自我监控和修正输出，显著降低有害内容生成风险，同时保持良性任务性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成连贯和上下文相关文本方面表现出色，但其部署存在生成有害或不适当内容的重大风险，需要有效的安全防护机制。

Method: 采用渐进式自我反思技术，通过多轮自我评估动态修正输出；引入轻量级自我反思预测器，根据输入复杂度估计最优反思轮数，平衡安全性与计算效率。

Result: 在Llama-3.1-8B-Instruct上攻击成功率从77.5%降至5.9%，在Llama-3.1-8B基础模型上从89.7%降至5.6%，在Qwen2.5-7B-Instruct上从44.4%降至3.8%，且不影响良性任务性能。

Conclusion: 渐进式自我反思作为一种可扩展的测试时方法，通过根据输入风险特征动态分配计算资源，有效增强了大语言模型的安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [23] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA是一个提示框架，通过将因果事件图转换为自然语言陈述注入LLM输入，在事件问答任务中平均提升5%准确率，零样本设置下最高提升12%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在一般语言任务上表现出色，但在需要因果或时序推理的事件类问题上表现不佳。

Method: 提出TAG-EQA框架，将结构化因果事件图转换为自然语言陈述注入LLM输入，包含9种提示配置（3种策略×3种输入模态）。

Result: 在TORQUESTRA基准测试中，相比纯文本基线平均准确率提升5%，零样本设置下最高提升12%，图增强CoT提示有效时提升18%。

Conclusion: 因果图可以在不进行微调的情况下增强LLM的事件推理能力，为基于提示的问答提供了一种灵活的结构编码方式。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [24] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: 该研究从过程导向角度评估大语言模型的创造力，引入基于约束的决策作为作者创造力的分析框架，发现LLMs在创作中一致性地优先考虑风格而非其他元素。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型创造力评估主要关注输出质量而非创作过程，本研究旨在通过叙事学视角分析LLMs作为计算作者的创作过程。

Method: 使用受控提示分配作者角色，分析模型的创意偏好，并探究模型选择背后的推理过程。

Result: 研究发现LLMs在创作决策中一致性地强调风格元素，优先于角色、事件和场景等其他元素，不同模型展现出独特的创作偏好特征。

Conclusion: 该方法为分析AI作者创造力提供了新颖的系统性工具，能够揭示不同模型在创作过程中的独特偏好特征。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill是一个结合模仿学习和任务运动规划的框架，能够实现组合泛化和实时故障恢复，在动态环境中执行多步操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习缺乏组合泛化能力，以及任务运动规划延迟过高的问题，实现既具有组合性又能实时响应的多步操作系统。

Method: 从无标签、无分割的演示数据中联合学习谓词、操作符和技能，运行时使用符号规划器组合和重排序技能来实现符号目标，同时在运动和符号层面进行实时恢复。

Result: 在RoboCasa模拟中执行12个单步任务成功率85%，无需额外数据即可组合成最多需要6次技能重组的多步计划；在真实机器人上从5分钟无标签游戏数据学习后能通过目标指定执行多个任务。

Conclusion: SymSkill成功结合了模仿学习和任务运动规划的优势，实现了组合泛化和实时故障恢复，为动态环境中的多步操作提供了有效解决方案。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>
