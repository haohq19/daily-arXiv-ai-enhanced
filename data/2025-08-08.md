<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

TL;DR: 研究利用事件相机和深度学习（CNN_LSTM模型）实现低成本、高精度的眼球中心定位，准确率达81%，未来计划通过LRP提升模型可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决高速眼球运动（300度/秒）的精确追踪问题，降低对昂贵高速相机的依赖，应用于VR/AR设备以提升用户体验。

Method: 采用CNN_LSTM深度学习模型，结合事件相机输入，定位眼球中心位置（x, y）。

Result: 模型准确率约为81%，验证了方法的有效性。

Conclusion: 研究成功开发了低成本且高效的眼球追踪算法，未来将通过LRP进一步优化模型。

Abstract: This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>


### [2] [FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images](https://arxiv.org/abs/2508.05137)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: FedGIN是一种联邦学习框架，通过全局强度非线性增强模块实现多模态医学图像分割，无需共享原始数据，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中多模态数据共享的隐私问题和域偏移挑战，开发一种统一模型以提升泛化能力。

Method: 提出FedGIN框架，结合GIN模块在本地训练中协调多模态强度分布，评估了在有限数据和完整数据场景下的性能。

Result: 在有限数据场景下，FedGIN比无GIN的联邦学习性能提升12-18%；在完整数据场景下，接近集中式性能，Dice分数提升显著。

Conclusion: FedGIN在多模态医学图像分割中表现优异，兼顾隐私保护和性能提升，具有临床应用潜力。

Abstract: Medical image segmentation plays a crucial role in AI-assisted diagnostics,
surgical planning, and treatment monitoring. Accurate and robust segmentation
models are essential for enabling reliable, data-driven clinical decision
making across diverse imaging modalities. Given the inherent variability in
image characteristics across modalities, developing a unified model capable of
generalizing effectively to multiple modalities would be highly beneficial.
This model could streamline clinical workflows and reduce the need for
modality-specific training. However, real-world deployment faces major
challenges, including data scarcity, domain shift between modalities (e.g., CT
vs. MRI), and privacy restrictions that prevent data sharing. To address these
issues, we propose FedGIN, a Federated Learning (FL) framework that enables
multimodal organ segmentation without sharing raw patient data. Our method
integrates a lightweight Global Intensity Non-linear (GIN) augmentation module
that harmonizes modality-specific intensity distributions during local
training. We evaluated FedGIN using two types of datasets: an imputed dataset
and a complete dataset. In the limited dataset scenario, the model was
initially trained using only MRI data, and CT data was added to assess its
performance improvements. In the complete dataset scenario, both MRI and CT
data were fully utilized for training on all clients. In the limited-data
scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test
cases compared to FL without GIN and consistently outperformed local baselines.
In the complete dataset scenario, FedGIN demonstrated near-centralized
performance, with a 30% Dice score improvement over the MRI-only baseline and a
10% improvement over the CT-only baseline, highlighting its strong
cross-modality generalization under privacy constraints.

</details>


### [3] [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221)
*Xiao Wang,Liye Jin,Xufeng Lou,Shiao Wang,Lan Chen,Bo Jiang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于推理的视觉语言跟踪框架ReasoningTrack，结合预训练模型Qwen2.5-VL，通过SFT和GRPO优化推理与语言生成，并构建了大规模数据集TNLLT。


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合语言与视觉特征时性能有限，且未能充分利用大模型的优势，因此提出推理框架以提升跟踪效果。

Method: 使用Qwen2.5-VL预训练模型，结合SFT和GRPO优化推理与语言生成，嵌入更新的语言描述与视觉特征，通过跟踪头预测目标位置。

Result: 在多个基准数据集上验证了推理策略的有效性，并构建了包含200个视频序列的TNLLT数据集。

Conclusion: ReasoningTrack通过推理和语言生成优化显著提升了视觉语言跟踪性能，为后续研究提供了新方向。

Abstract: Vision-language tracking has received increasing attention in recent years,
as textual information can effectively address the inflexibility and inaccuracy
associated with specifying the target object to be tracked. Existing works
either directly fuse the fixed language with vision features or simply modify
using attention, however, their performance is still limited. Recently, some
researchers have explored using text generation to adapt to the variations in
the target during tracking, however, these works fail to provide insights into
the model's reasoning process and do not fully leverage the advantages of large
models, which further limits their overall performance. To address the
aforementioned issues, this paper proposes a novel reasoning-based
vision-language tracking framework, named ReasoningTrack, based on a
pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning)
and reinforcement learning GRPO are used for the optimization of reasoning and
language generation. We embed the updated language descriptions and feed them
into a unified tracking backbone network together with vision features. Then,
we adopt a tracking head to predict the specific location of the target object.
In addition, we propose a large-scale long-term vision-language tracking
benchmark dataset, termed TNLLT, which contains 200 video sequences. 20
baseline visual trackers are re-trained and evaluated on this dataset, which
builds a solid foundation for the vision-language visual tracking task.
Extensive experiments on multiple vision-language tracking benchmark datasets
fully validated the effectiveness of our proposed reasoning-based natural
language generation strategy. The source code of this paper will be released on
https://github.com/Event-AHU/Open_VLTrack

</details>


### [4] [Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events](https://arxiv.org/abs/2508.05507)
*Lin Zhu,Ruonan Liu,Xiao Wang,Lizhi Wang,Hua Huang*

Main category: cs.CV

TL;DR: 提出了一种自监督预训练框架，用于从稀疏且嘈杂的事件数据中提取潜在信息，包括边缘信息和纹理线索。


<details>
  <summary>Details</summary>
Motivation: 事件数据具有高时间分辨率和宽动态范围，但其稀疏性和噪声特性使得特征提取变得复杂。

Method: 框架分为三个阶段：差异引导掩码建模、主干固定的特征转换和聚焦对比学习。

Result: 在多个下游任务（如目标识别、语义分割和光流估计）中表现优于现有方法。

Conclusion: 该框架能有效提取事件数据中的潜在信息，并在多个任务中表现出色。

Abstract: Event camera, a novel neuromorphic vision sensor, records data with high
temporal resolution and wide dynamic range, offering new possibilities for
accurate visual representation in challenging scenarios. However, event data is
inherently sparse and noisy, mainly reflecting brightness changes, which
complicates effective feature extraction. To address this, we propose a
self-supervised pre-training framework to fully reveal latent information in
event data, including edge information and texture cues. Our framework consists
of three stages: Difference-guided Masked Modeling, inspired by the event
physical sampling process, reconstructs temporal intensity difference maps to
extract enhanced information from raw event data. Backbone-fixed Feature
Transition contrasts event and image features without updating the backbone to
preserve representations learned from masked modeling and stabilizing their
effect on contrastive learning. Focus-aimed Contrastive Learning updates the
entire model to improve semantic discrimination by focusing on high-value
regions. Extensive experiments show our framework is robust and consistently
outperforms state-of-the-art methods on various downstream tasks, including
object recognition, semantic segmentation, and optical flow estimation. The
code and dataset are available at https://github.com/BIT-Vision/EventPretrain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration](https://arxiv.org/abs/2508.04780)
*Lin Jiang,Dahai Yu,Rongchao Xu,Tian Tang,Guang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种公平感知的电力恢复策略EPOPR，通过预测-优化框架解决现有恢复方案中的不公平问题，显著减少了停电时间和社区间的不平等。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件频发，现有电力恢复方案因基于请求量决策而导致弱势社区恢复不足，亟需兼顾效率与公平的解决方案。

Method: 设计了EPOPR框架，包含Equity-Conformalized Quantile Regression（预测修复时间）和Spatial-Temporal Attentional RL（公平决策）。

Result: 实验显示EPOPR平均减少停电时间3.60%，降低社区间不平等14.19%。

Conclusion: EPOPR成功平衡了电力恢复的效率与公平，为未来灾害响应提供了新思路。

Abstract: The increasing frequency of extreme weather events, such as hurricanes,
highlights the urgent need for efficient and equitable power system
restoration. Many electricity providers make restoration decisions primarily
based on the volume of power restoration requests from each region. However,
our data-driven analysis reveals significant disparities in request submission
volume, as disadvantaged communities tend to submit fewer restoration requests.
This disparity makes the current restoration solution inequitable, leaving
these communities vulnerable to extended power outages. To address this, we aim
to propose an equity-aware power restoration strategy that balances both
restoration efficiency and equity across communities. However, achieving this
goal is challenging for two reasons: the difficulty of predicting repair
durations under dataset heteroscedasticity, and the tendency of reinforcement
learning agents to favor low-uncertainty actions, which potentially undermine
equity. To overcome these challenges, we design a predict-then-optimize
framework called EPOPR with two key components: (1) Equity-Conformalized
Quantile Regression for uncertainty-aware repair duration prediction, and (2)
Spatial-Temporal Attentional RL that adapts to varying uncertainty levels
across regions for equitable decision-making. Experimental results show that
our EPOPR effectively reduces the average power outage duration by 3.60% and
decreases inequity between different communities by 14.19% compared to
state-of-the-art baselines.

</details>


### [6] [Unified Flow Matching for Long Horizon Event Forecasting](https://arxiv.org/abs/2508.04843)
*Xiao Shou*

Main category: cs.LG

TL;DR: 提出了一种基于流匹配的统一框架，用于非自回归地联合建模标记事件序列的时间和类型，显著提升了长范围预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型在长范围事件序列预测中效率低和误差累积的问题。

Method: 采用连续和离散流匹配方法，联合建模事件间时间和事件类型，避免顺序解码。

Result: 在六个真实世界基准测试中，准确性和生成效率均优于自回归和基于扩散的基线模型。

Conclusion: 提出的流匹配框架为长范围标记事件序列建模提供了高效且准确的解决方案。

Abstract: Modeling long horizon marked event sequences is a fundamental challenge in
many real-world applications, including healthcare, finance, and user behavior
modeling. Existing neural temporal point process models are typically
autoregressive, predicting the next event one step at a time, which limits
their efficiency and leads to error accumulation in long-range forecasting. In
this work, we propose a unified flow matching framework for marked temporal
point processes that enables non-autoregressive, joint modeling of inter-event
times and event types, via continuous and discrete flow matching. By learning
continuous-time flows for both components, our method generates coherent long
horizon event trajectories without sequential decoding. We evaluate our model
on six real-world benchmarks and demonstrate significant improvements over
autoregressive and diffusion-based baselines in both accuracy and generation
efficiency.

</details>


### [7] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: 研究探讨AR在CPR中如何影响情境感知（SA），提出基于眼动的SA预测模型FixGraphPool，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AR系统可能引发认知隧道效应，降低SA，尤其在安全关键场景（如CPR）中需平衡虚拟指导与现实警觉。

Method: 开发AR应用提供CPR实时反馈，通过用户研究（模拟意外事件）收集SA数据，提出FixGraphPool模型分析眼动数据。

Result: 高SA与眼动特征（如扫视幅度、速度）相关；FixGraphPool模型预测SA准确率达83.0%，优于其他方法。

Conclusion: 眼动数据可用于AR中的SA建模，为设计兼顾安全与SA的AR系统提供依据。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [8] [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/abs/2508.05316)
*Yue Duan,Taicai Chen,Lei Qi,Yinghuan Shi*

Main category: cs.LG

TL;DR: USP框架通过分而治之的方法解决了半监督持续学习中的三个关键问题：学习塑性、无标签学习和记忆稳定性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 减少标注成本并处理持续到达的数据，同时解决无标签学习、记忆稳定性和学习塑性之间的平衡问题。

Method: USP框架包含三个策略：特征空间保留（FSR）用于学习塑性，分而治之伪标签（DCP）用于无标签学习，类均值锚定无标签蒸馏（CUD）用于记忆稳定性。

Result: USP在最后准确率上比之前的方法提升了5.94%，验证了其有效性。

Conclusion: USP框架通过协同增强三个关键方面，显著提升了半监督持续学习的性能。

Abstract: Semi-supervised continual learning (SSCL) seeks to leverage both labeled and
unlabeled data in a sequential learning setup, aiming to reduce annotation
costs while managing continual data arrival. SSCL introduces complex
challenges, including ensuring effective unlabeled learning (UL), while
balancing memory stability (MS) and learning plasticity (LP). Previous SSCL
efforts have typically focused on isolated aspects of the three, while this
work presents USP, a divide-and-conquer framework designed to synergistically
enhance these three aspects: (1) Feature Space Reservation (FSR) strategy for
LP, which constructs reserved feature locations for future classes by shaping
old classes into an equiangular tight frame; (2) Divide-and-Conquer
Pseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels
across both high- and low-confidence unlabeled data; and (3)
Class-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's
outputs to anchor unlabeled data to stable class means for distillation to
prevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL
methods, with gains up to 5.94% in the last accuracy, validating its
effectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.

</details>


### [9] [Competing Risks: Impact on Risk Estimation and Algorithmic Fairness](https://arxiv.org/abs/2508.05435)
*Vincent Jeanselme,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 论文指出将竞争风险视为截尾数据会引入显著偏差，导致风险高估并加剧不公平性，提出了量化误差的框架并验证了其对预测性能和公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 准确预测事件发生时间对决策至关重要，但现有生存分析常忽略竞争风险的影响，导致偏差和不公平。

Method: 形式化竞争风险误分类问题，开发量化误差的框架，并通过心血管管理数据验证。

Result: 忽略竞争风险会高估风险并加剧群体间差异，对高风险群体影响更大。

Conclusion: 生存模型需考虑竞争风险以提高准确性、减少不公平性，并为决策提供更好支持。

Abstract: Accurate time-to-event prediction is integral to decision-making, informing
medical guidelines, hiring decisions, and resource allocation. Survival
analysis, the quantitative framework used to model time-to-event data, accounts
for patients who do not experience the event of interest during the study
period, known as censored patients. However, many patients experience events
that prevent the observation of the outcome of interest. These competing risks
are often treated as censoring, a practice frequently overlooked due to a
limited understanding of its consequences. Our work theoretically demonstrates
why treating competing risks as censoring introduces substantial bias in
survival estimates, leading to systematic overestimation of risk and,
critically, amplifying disparities. First, we formalize the problem of
misclassifying competing risks as censoring and quantify the resulting error in
survival estimates. Specifically, we develop a framework to estimate this error
and demonstrate the associated implications for predictive performance and
algorithmic fairness. Furthermore, we examine how differing risk profiles
across demographic groups lead to group-specific errors, potentially
exacerbating existing disparities. Our findings, supported by an empirical
analysis of cardiovascular management, demonstrate that ignoring competing
risks disproportionately impacts the individuals most at risk of these events,
potentially accentuating inequity. By quantifying the error and highlighting
the fairness implications of the common practice of considering competing risks
as censoring, our work provides a critical insight into the development of
survival models: practitioners must account for competing risks to improve
accuracy, reduce disparities in risk assessment, and better inform downstream
decisions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的智能系统，用于工业机械的预测性维护，结合振动频率分析和多代理生成技术，提供可操作的维护建议。


<details>
  <summary>Details</summary>
Motivation: 工业机械维护需要及时干预以防止灾难性故障并优化运行效率，传统方法仅能检测异常，缺乏具体的维护建议。

Method: 系统将轴承振动数据（BPFO、BPFI、BSF、FTF频率）序列化为自然语言供LLM处理，结合多代理组件分析维护手册和网络搜索，生成结构化维护建议。

Result: 实验验证表明，系统能有效检测异常并提供上下文相关的维护指导，填补了状态监测与可操作维护计划之间的空白。

Conclusion: 该研究推动了LLM在工业维护中的应用，为跨机械组件和工业领域的预测性维护提供了可扩展框架。

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [11] [Graph-based Event Log Repair](https://arxiv.org/abs/2508.05145)
*Sebastiano Dissegna,Chiara Di Francescomarino,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 提出了一种基于异构图神经网络（HGNN）的方法，用于修复流程挖掘中事件日志的缺失属性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的事件日志常因数据采集问题导致信息缺失，现有方法依赖流程模型或机器学习，但HGNN能更自然地处理复杂多模态序列。

Method: 开发了一种HGNN模型，能够从不完整的事件轨迹中恢复所有缺失属性，并在合成和真实日志上进行了评估。

Result: 与基于自动编码器的先进方法相比，HGNN在所有事件属性的修复上表现出色。

Conclusion: HGNN在事件日志修复中具有显著优势，能够全面恢复缺失属性。

Abstract: The quality of event logs in Process Mining is crucial when applying any form
of analysis to them. In real-world event logs, the acquisition of data can be
non-trivial (e.g., due to the execution of manual activities and related manual
recording or to issues in collecting, for each event, all its attributes), and
often may end up with events recorded with some missing information. Standard
approaches to the problem of trace (or log) reconstruction either require the
availability of a process model that is used to fill missing values by
leveraging different reasoning techniques or employ a Machine Learning/Deep
Learning model to restore the missing values by learning from similar cases. In
recent years, a new type of Deep Learning model that is capable of handling
input data encoded as graphs has emerged, namely Graph Neural Networks. Graph
Neural Network models, and even more so Heterogeneous Graph Neural Networks,
offer the advantage of working with a more natural representation of complex
multi-modal sequences like the execution traces in Process Mining, allowing for
more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural
Network model that, given a trace containing some incomplete events, will
return the full set of attributes missing from those events. We evaluate our
work against a state-of-the-art approach leveraging autoencoders on two
synthetic logs and four real event logs, on different types of missing values.
Different from state-of-the-art model-free approaches, which mainly focus on
repairing a subset of event attributes, the proposed approach shows very good
performance in reconstructing all different event attributes.

</details>


### [12] [Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation](https://arxiv.org/abs/2508.05427)
*Kartar Kumar Lohana Tharwani,Rajesh Kumar,Sumita,Numan Ahmed,Yong Tang*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）正在改变有机合成中反应的设计与执行方式，结合多种技术加速化学发现，但仍需解决数据偏见和安全问题。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何从理论工具发展为实用的实验室助手，推动数据驱动的绿色化学。

Method: 结合图神经网络、量子计算和实时光谱技术，优化LLMs在合成化学中的应用。

Result: LLMs显著缩短发现周期，支持更环保的化学研究，但仍存在数据偏见和安全性挑战。

Conclusion: 通过开放基准和可解释界面等社区倡议，LLMs有望实现快速、可靠且包容的分子创新。

Abstract: Large language models (LLMs) are beginning to reshape how chemists plan and
run reactions in organic synthesis. Trained on millions of reported
transformations, these text-based models can propose synthetic routes, forecast
reaction outcomes and even instruct robots that execute experiments without
human supervision. Here we survey the milestones that turned LLMs from
speculative tools into practical lab partners. We show how coupling LLMs with
graph neural networks, quantum calculations and real-time spectroscopy shrinks
discovery cycles and supports greener, data-driven chemistry. We discuss
limitations, including biased datasets, opaque reasoning and the need for
safety gates that prevent unintentional hazards. Finally, we outline community
initiatives open benchmarks, federated learning and explainable interfaces that
aim to democratize access while keeping humans firmly in control. These
advances chart a path towards rapid, reliable and inclusive molecular
innovation powered by artificial intelligence and automation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 提出了一种基于视觉-语言聚类的框架，用于更全面地评估视觉活动识别系统，解决了动词语义和图像解释的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 标准精确匹配评估方法无法捕捉动词语义和图像解释的模糊性，导致模型性能评估不完整。

Method: 提出了一个视觉-语言聚类框架，构建动词意义簇，用于更稳健的评估。

Result: 在imSitu数据集上，每张图像平均映射到2.8个意义簇，每个簇代表图像的不同视角。聚类评估与人类判断更一致。

Conclusion: 聚类评估方法提供了更细致的模型性能评估，优于标准评估方法。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [14] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 提出了一种多阶段大型语言模型框架，用于从非结构化文本中提取社会健康决定因素（SDoH），提升自杀事件相关因素分析的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决自杀事件中SDoH因素分析面临的挑战，如长尾分布、关键压力源分析和模型可解释性不足。

Method: 采用多阶段语言模型框架，并与BioBERT、GPT-3.5-turbo和DeepSeek-R1等模型对比，结合自动化评估和用户研究。

Result: 框架在SDoH因素提取和相关上下文检索任务中表现优异，且小规模任务专用模型在降低推理成本的同时性能相当或更好。

Conclusion: 该方法提高了从非结构化文本中提取自杀相关SDoH的准确性和透明度，有助于早期风险识别和预防策略制定。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [15] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: CGRS方法通过动态抑制高置信度时的反思触发词，减少冗余推理步骤，降低推理成本，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理语言模型（LRLMs）中因反思行为导致的过度思考问题，如冗余推理步骤增加计算成本和降低实用性。

Method: 提出Certainty-Guided Reflection Suppression（CGRS），动态抑制高置信度时的反思触发词，无需重新训练或修改模型架构。

Result: 在四个推理基准测试中，CGRS平均减少18.5%至41.9%的token使用，同时保持准确性。

Conclusion: CGRS在模型架构和规模上均表现出高效性，为高效推理提供了实用价值。

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [16] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为LAG的新方法，通过逻辑分解和依赖感知推理增强大语言模型的知识密集型任务表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务中容易产生幻觉，现有检索增强生成方法在复杂推理场景中表现不足。

Method: LAG将复杂问题分解为原子子问题，按逻辑依赖顺序解决，并引入逻辑终止机制防止错误传播。

Result: 在四个基准数据集上的实验表明，LAG显著提高了推理鲁棒性，减少了幻觉。

Conclusion: LAG为现有RAG系统提供了一种更符合人类认知的替代方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [On the causality between affective impact and coordinated human-robot reactions](https://arxiv.org/abs/2508.04834)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 研究机器人主动与人类共享事件反应是否影响人类对其情感影响的感知，发现共享反应和近人类反应时间（约200ms）效果最佳。


<details>
  <summary>Details</summary>
Motivation: 提升机器人在社交环境中的功能，探索其反应行为对人类感知的影响。

Method: 设计两个实验：一个隔离机器人反应元素，另一个测试不同反应延迟时间（n=84和n=110）。

Result: 共享事件的机器人反应显著改变人类感知（p<0.05），近人类反应时间（200ms）效果最佳。

Conclusion: 200ms延迟对小型非人形机器人最有效，100ms延迟则让人类感觉对机器人影响最大。

Abstract: In an effort to improve how robots function in social contexts, this paper
investigates if a robot that actively shares a reaction to an event with a
human alters how the human perceives the robot's affective impact. To verify
this, we created two different test setups. One to highlight and isolate the
reaction element of affective robot expressions, and one to investigate the
effects of applying specific timing delays to a robot reacting to a physical
encounter with a human. The first test was conducted with two different groups
(n=84) of human observers, a test group and a control group both interacting
with the robot. The second test was performed with 110 participants using
increasingly longer reaction delays for the robot with every ten participants.
The results show a statistically significant change (p$<$.05) in perceived
affective impact for the robots when they react to an event shared with a human
observer rather than reacting at random. The result also shows for shared
physical interaction, the near-human reaction times from the robot are most
appropriate for the scenario. The paper concludes that a delay time around
200ms may render the biggest impact on human observers for small-sized
non-humanoid robots. It further concludes that a slightly shorter reaction time
around 100ms is most effective when the goal is to make the human observers
feel they made the biggest impact on the robot.

</details>
