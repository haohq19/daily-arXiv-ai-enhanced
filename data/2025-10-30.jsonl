{"id": "2510.25223", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25223", "abs": "https://arxiv.org/abs/2510.25223", "authors": ["Kun ouyang", "Haoyu Wang", "Dong Fang"], "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data", "comment": "14 pages, 11 figures", "summary": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments."}
{"id": "2510.24824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24824", "abs": "https://arxiv.org/abs/2510.24824", "authors": ["Bohong Wu", "Mengzhao Chen", "Xiang Luo", "Shen Yan", "Qifan Yu", "Fan Xia", "Tianqi Zhang", "Hongrui Zhan", "Zheng Zhong", "Xun Zhou", "Siyuan Qiao", "Xingyan Bin"], "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling", "comment": null, "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."}
{"id": "2510.25126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25126", "abs": "https://arxiv.org/abs/2510.25126", "authors": ["Yuen Chen", "Yulun Wu", "Samuel Sharpe", "Igor Melnyk", "Nam H. Nguyen", "Furong Huang", "C. Bayan Bruss", "Rizal Fathony"], "title": "Bridging the Divide: End-to-End Sequence-Graph Learning", "comment": null, "summary": "Many real-world datasets are both sequential and relational: each node\ncarries an event sequence while edges encode interactions. Existing methods in\nsequence modeling and graph modeling often neglect one modality or the other.\nWe argue that sequences and graphs are not separate problems but complementary\nfacets of the same dataset, and should be learned jointly. We introduce BRIDGE,\na unified end-to-end architecture that couples a sequence encoder with a GNN\nunder a single objective, allowing gradients to flow across both modules and\nlearning task-aligned representations. To enable fine-grained token-level\nmessage passing among neighbors, we add TOKENXATTN, a token-level\ncross-attention layer that passes messages between events in neighboring\nsequences. Across two settings, friendship prediction (Brightkite) and fraud\ndetection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph\nmethods, and sequence-only baselines on ranking and classification metrics."}
{"id": "2510.25174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25174", "abs": "https://arxiv.org/abs/2510.25174", "authors": ["Huadong Tang", "Youpeng Zhao", "Min Xu", "Jun Wang", "Qiang Wu"], "title": "Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation", "comment": "Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)", "summary": "Prevalent semantic segmentation methods generally adopt a vanilla classifier\nto categorize each pixel into specific classes.\n  Although such a classifier learns global information from the training data,\nthis information is represented by a set of fixed parameters (weights and\nbiases).\n  However, each image has a different class distribution, which prevents the\nclassifier from addressing the unique characteristics of individual images.\n  At the dataset level, class imbalance leads to segmentation results being\nbiased towards majority classes, limiting the model's effectiveness in\nidentifying and segmenting minority class regions.\n  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that\ndynamically adjusts the classifier using global (dataset-level) and local\n(image-level) contextual information.\n  Specifically, we leverage a memory bank to learn dataset-level contextual\ninformation of each class, incorporating the class-specific contextual\ninformation from the current image to improve the classifier for precise pixel\nlabeling.\n  Additionally, a teacher-student network paradigm is adopted, where the domain\nexpert (teacher network) dynamically adjusts contextual information with ground\ntruth and transfers knowledge to the student network.\n  Comprehensive experiments illustrate that the proposed ECAC can achieve\nstate-of-the-art performance across several datasets, including ADE20K,\nCOCO-Stuff10K, and Pascal-Context."}
{"id": "2510.25126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25126", "abs": "https://arxiv.org/abs/2510.25126", "authors": ["Yuen Chen", "Yulun Wu", "Samuel Sharpe", "Igor Melnyk", "Nam H. Nguyen", "Furong Huang", "C. Bayan Bruss", "Rizal Fathony"], "title": "Bridging the Divide: End-to-End Sequence-Graph Learning", "comment": null, "summary": "Many real-world datasets are both sequential and relational: each node\ncarries an event sequence while edges encode interactions. Existing methods in\nsequence modeling and graph modeling often neglect one modality or the other.\nWe argue that sequences and graphs are not separate problems but complementary\nfacets of the same dataset, and should be learned jointly. We introduce BRIDGE,\na unified end-to-end architecture that couples a sequence encoder with a GNN\nunder a single objective, allowing gradients to flow across both modules and\nlearning task-aligned representations. To enable fine-grained token-level\nmessage passing among neighbors, we add TOKENXATTN, a token-level\ncross-attention layer that passes messages between events in neighboring\nsequences. Across two settings, friendship prediction (Brightkite) and fraud\ndetection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph\nmethods, and sequence-only baselines on ranking and classification metrics."}
{"id": "2510.25696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25696", "abs": "https://arxiv.org/abs/2510.25696", "authors": ["Yesmine Abdennadher", "Eleonora Cicciarella", "Michele Rossi"], "title": "Convolutional Spiking-based GRU Cell for Spatio-temporal Data", "comment": "6 pages, 1 figure. Published in 2025 IEEE International Workshop On\n  Machine Learning for Signal Processing, Aug. 31-Sep. 3, 2025, Istanbul,\n  Turkey", "summary": "Spike-based temporal messaging enables SNNs to efficiently process both\npurely temporal and spatio-temporal time-series or event-driven data. Combining\nSNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,\ngives rise to a robust framework for sequential data processing; however,\ntraditional RNNs often lose local details when handling long sequences.\nPrevious approaches, such as SpikGRU, fail to capture fine-grained local\ndependencies in event-based spatio-temporal data. In this paper, we introduce\nthe Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional\noperations to preserve local structure and dependencies while integrating the\ntemporal precision of spiking neurons with the efficient gating mechanisms of\nGRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,\nSHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our\nexperiments show that CS-GRU outperforms state-of-the-art GRU variants by an\naverage of 4.35%, achieving over 90% accuracy on sequential tasks and up to\n99.31% on MNIST. It is worth noting that our solution achieves 69% higher\nefficiency compared to SpikGRU. The code is available at:\nhttps://github.com/YesmineAbdennadher/CS-GRU."}
