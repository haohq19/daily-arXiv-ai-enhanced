{"id": "2512.20056", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20056", "abs": "https://arxiv.org/abs/2512.20056", "authors": ["Hao Li", "Fabian Deuser", "Wenping Yin", "Steffen Knoblauch", "Wufan Zhao", "Filip Biljecki", "Yong Xue", "Wei Huang"], "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach", "comment": null, "summary": "As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC", "AI": {"tldr": "\u63d0\u51faProbGLC\u6982\u7387\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u6982\u7387\u548c\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u63d0\u9ad8\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u707e\u5bb3\u4e8b\u4ef6\u9891\u53d1\u52a0\u5267\uff0c\u5feb\u901f\u51c6\u786e\u7684\u707e\u5bb3\u4f4d\u7f6e\u8bc6\u522b\u5bf9\u5e94\u6025\u54cd\u5e94\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u63d0\u9ad8\u5730\u7406\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faProbGLC\u6982\u7387\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u6846\u67b6\uff0c\u5c06\u6982\u7387\u6a21\u578b\u548c\u786e\u5b9a\u6027\u6a21\u578b\u7edf\u4e00\u7ed3\u5408\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u5c40\u90e8\u5316\u8bc4\u5206\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u591a\u79cd\u707e\u5bb3\u7c7b\u578b\u7684\u8de8\u89c6\u89d2\u5b9a\u4f4d\u3002", "result": "\u5728\u4e24\u4e2a\u8de8\u89c6\u89d2\u707e\u5bb3\u6570\u636e\u96c6\uff08MultiIAN\u548cSAGAINDisaster\uff09\u4e0a\u9a8c\u8bc1\uff0c\u8fbe\u52300.86\u7684Acc@1km\u548c0.97\u7684Acc@25km\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u4f9b\u6982\u7387\u5206\u5e03\u548c\u5c40\u90e8\u5316\u8bc4\u5206\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ProbGLC\u65b9\u6cd5\u5728\u707e\u5bb3\u5730\u7406\u5b9a\u4f4d\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u8de8\u89c6\u89d2\u65b9\u6cd5\u589e\u5f3a\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\uff0c\u6709\u671b\u4fc3\u8fdb\u66f4\u5feb\u66f4\u597d\u7684\u707e\u5bb3\u54cd\u5e94\u3002"}}
{"id": "2512.20042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20042", "abs": "https://arxiv.org/abs/2512.20042", "authors": ["Nguyen Lam Phu Quy", "Pham Phu Hoa", "Tran Chi Nguyen", "Dao Sy Duy Minh", "Nguyen Hoang Minh Ngoc", "Huynh Trung Kiet"], "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva", "comment": "7 pages, 5 figures. System description for the EVENTA Grand Challenge (Track 1) at ACM MM'25", "summary": "Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u4f3c\u56fe\u50cf\u548c\u5916\u90e8\u6587\u672c\u77e5\u8bc6\u6765\u589e\u5f3a\u56fe\u50cf\u63cf\u8ff0\uff0c\u751f\u6210\u5305\u542b\u4e8b\u4ef6\u80cc\u666f\u3001\u65f6\u95f4\u7ebf\u7d22\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4e30\u5bcc\u63cf\u8ff0", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u63cf\u8ff0\u901a\u5e38\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u6df1\u5ea6\uff0c\u7701\u7565\u4e86\u4e8b\u4ef6\u80cc\u666f\u3001\u65f6\u95f4\u7ebf\u7d22\u3001\u7ed3\u679c\u548c\u547d\u540d\u5b9e\u4f53\u7b49\u5173\u952e\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u5728\u65b0\u95fb\u3001\u6559\u80b2\u7b49\u9886\u57df\u7684\u5e94\u7528\u6548\u679c", "method": "\u4f7f\u7528BEIT-3\u548cSigLIP\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u56fe\u50cf\uff0cORB\u548cSIFT\u8fdb\u884c\u51e0\u4f55\u5bf9\u9f50\u91cd\u6392\u5e8f\uff0c\u4ece\u76f8\u5173\u6587\u7ae0\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u5fae\u8c03\u7684Qwen3\u6a21\u578b\u6574\u5408\u4e0a\u4e0b\u6587\u4e0eInstruct BLIP\u751f\u6210\u7684\u57fa\u7840\u63cf\u8ff0", "result": "\u5728OpenEvents v1\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u751f\u6210\u4e86\u663e\u8457\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u63cf\u8ff0\uff0c\u663e\u793a\u51fa\u5728\u9700\u8981\u6df1\u5ea6\u89c6\u89c9-\u6587\u672c\u7406\u89e3\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u7ba1\u9053\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8f93\u5165\u548c\u5916\u90e8\u6587\u672c\u77e5\u8bc6\uff0c\u80fd\u591f\u751f\u6210\u4e8b\u4ef6\u4e30\u5bcc\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u50cf\u63cf\u8ff0\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u6df1\u5ea6\u7684\u95ee\u9898"}}
{"id": "2512.19737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19737", "abs": "https://arxiv.org/abs/2512.19737", "authors": ["Cl\u00e9ment Elliker", "Jesse Read", "Sonia Vanier", "Albert Bifet"], "title": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach", "comment": "8 body pages, 3 appendix pages, 8 tables, 2 figures, accepted at AAAI2026 Main Track", "summary": "Reliable prediction of train delays is essential for enhancing the robustness and efficiency of railway transportation systems. In this work, we reframe delay forecasting as a stochastic simulation task, modeling state-transition dynamics through imitation learning. We introduce Drift-Corrected Imitation Learning (DCIL), a novel self-supervised algorithm that extends DAgger by incorporating distance-based drift correction, thereby mitigating covariate shift during rollouts without requiring access to an external oracle or adversarial schemes. Our approach synthesizes the dynamical fidelity of event-driven models with the representational capacity of data-driven methods, enabling uncertainty-aware forecasting via Monte Carlo simulation. We evaluate DCIL using a comprehensive real-world dataset from \\textsc{Infrabel}, the Belgian railway infrastructure manager, which encompasses over three million train movements. Our results, focused on predictions up to 30 minutes ahead, demonstrate superior predictive performance of DCIL over traditional regression models and behavioral cloning on deep learning architectures, highlighting its effectiveness in capturing the sequential and uncertain nature of delay propagation in large-scale networks.", "AI": {"tldr": "\u63d0\u51faDCIL\u7b97\u6cd5\uff0c\u901a\u8fc7\u8ddd\u79bb\u6f02\u79fb\u6821\u6b63\u7684\u6a21\u4eff\u5b66\u4e60\u8fdb\u884c\u5217\u8f66\u5ef6\u8bef\u9884\u6d4b\uff0c\u5728\u6bd4\u5229\u65f6\u94c1\u8def\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u5217\u8f66\u5ef6\u8bef\u7684\u53ef\u9760\u9884\u6d4b\u5bf9\u63d0\u5347\u94c1\u8def\u8fd0\u8f93\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u5e8f\u5217\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u7279\u5f81", "method": "\u5c06\u5ef6\u8bef\u9884\u6d4b\u91cd\u6784\u4e3a\u968f\u673a\u6a21\u62df\u4efb\u52a1\uff0c\u63d0\u51faDCIL\u7b97\u6cd5\uff0c\u5728DAgger\u57fa\u7840\u4e0a\u52a0\u5165\u8ddd\u79bb\u6f02\u79fb\u6821\u6b63\uff0c\u65e0\u9700\u5916\u90e8\u9884\u8a00\u673a\u6216\u5bf9\u6297\u65b9\u6848", "result": "\u5728\u5305\u542b300\u591a\u4e07\u5217\u8f66\u8fd0\u884c\u7684\u6bd4\u5229\u65f6\u94c1\u8def\u6570\u636e\u96c6\u4e0a\uff0cDCIL\u572830\u5206\u949f\u9884\u6d4b\u8303\u56f4\u5185\u4f18\u4e8e\u4f20\u7edf\u56de\u5f52\u6a21\u578b\u548c\u884c\u4e3a\u514b\u9686\u65b9\u6cd5", "conclusion": "DCIL\u6210\u529f\u7ed3\u5408\u4e8b\u4ef6\u9a71\u52a8\u6a21\u578b\u7684\u52a8\u6001\u4fdd\u771f\u5ea6\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u8868\u793a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u5ef6\u8bef\u4f20\u64ad\u7684\u5e8f\u5217\u6027\u548c\u4e0d\u786e\u5b9a\u6027"}}
{"id": "2512.20520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20520", "abs": "https://arxiv.org/abs/2512.20520", "authors": ["Chehak Malhotra", "Mehak Gopal", "Akshaya Devadiga", "Pradeep Singh", "Ridam Pal", "Ritwik Kashyap", "Tavpritesh Sethi"], "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units", "comment": null, "summary": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.", "AI": {"tldr": "LLMs\u4e0eSLMs\u5728\u9884\u6d4bICU\u60a3\u8005\u4f11\u514b\u98ce\u9669\u4e0a\u7684\u6027\u80fd\u5bf9\u6bd4\u7814\u7a76\uff0c\u53d1\u73b0\u4e24\u8005\u8868\u73b0\u76f8\u5f53\uff0cGatorTron-Base\u8868\u73b0\u6700\u4f73\u4f46\u4f18\u52bf\u6709\u9650\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u7814\u7a76\u8f83\u5c11\u3002\u53ca\u65f6\u9884\u6d4b\u4f11\u514b\u80fd\u591f\u5b9e\u73b0\u65e9\u671f\u5e72\u9884\uff0c\u6539\u5584\u60a3\u8005\u9884\u540e\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30LLMs\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "method": "\u4f7f\u7528MIMIC III\u6570\u636e\u5e93\u4e2d17,294\u4f8bICU\u4f4f\u9662\u60a3\u8005\u7684\u6587\u672c\u6570\u636e\uff0c\u7b5b\u9009\u51faLOS>24\u5c0f\u65f6\u4e14\u4f11\u514b\u6307\u6570>0.7\u7684\u60a3\u8005\uff08\u6b63\u5e38\u7ec4355\u4f8b\uff0c\u5f02\u5e38\u7ec487\u4f8b\uff09\u3002\u6bd4\u8f83\u4e86GatorTron-Base\u3001Llama 8B\u3001Mistral 7B\u7b49LLMs\u4e0eBioBERT\u3001DocBERT\u3001BioClinicalBERT\u3001Word2Vec\u3001Doc2Vec\u7b49SLMs\u7684\u6027\u80fd\u3002\u4f7f\u7528focal loss\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "GatorTron-Base\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u52a0\u6743\u53ec\u56de\u738780.5%\uff0c\u4f46LLMs\u548cSLMs\u7684\u6574\u4f53\u6027\u80fd\u6307\u6807\u76f8\u5f53\u3002\u8fd9\u8868\u660eLLMs\u5728\u9884\u6d4b\u672a\u6765\u4e34\u5e8a\u4e8b\u4ef6\u65b9\u9762\u5e76\u4e0d\u5929\u7136\u4f18\u4e8eSLMs\u3002", "conclusion": "LLMs\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u5e76\u672a\u5c55\u73b0\u51fa\u5bf9SLMs\u7684\u660e\u663e\u4f18\u52bf\u3002\u672a\u6765\u8bad\u7ec3LLMs\u5e94\u91cd\u70b9\u5173\u6ce8\u5f00\u53d1\u80fd\u591f\u9884\u6d4b\u4e34\u5e8a\u8f68\u8ff9\u7684\u6a21\u578b\uff0c\u800c\u975e\u7b80\u5355\u7684NER\u6216\u8868\u578b\u8bc6\u522b\u4efb\u52a1\uff0c\u4ee5\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u4e34\u5e8a\u7ed3\u679c\u3002"}}
{"id": "2512.19805", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.19805", "abs": "https://arxiv.org/abs/2512.19805", "authors": ["Deepit Sapru"], "title": "Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy", "comment": null, "summary": "This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8425\u9500\u51b3\u7b56\u6846\u67b6\uff0c\u5c06\u5f02\u8d28\u6027\u5904\u7406\u63d0\u5347\u8f6c\u5316\u4e3a\u7ea6\u675f\u6027\u76ee\u6807\u7b56\u7565\uff0c\u5728\u9075\u5b88\u4e1a\u52a1\u62a4\u680f\u7684\u540c\u65f6\u6700\u5927\u5316\u6536\u5165\u548c\u7559\u5b58\u7387", "motivation": "\u9700\u8981\u5c06\u56e0\u679c\u63a8\u65ad\u7684\u5f02\u8d28\u6027\u5904\u7406\u6548\u679c\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u8425\u9500\u51b3\u7b56\uff0c\u5728\u9884\u7b97\u3001\u9500\u552e\u4e0b\u964d\u7b49\u4e1a\u52a1\u7ea6\u675f\u4e0b\u4f18\u5316\u76ee\u6807\u7b56\u7565", "method": "\u4f7f\u7528\u63d0\u5347\u5b66\u4e60\u5668\u4f30\u8ba1\u6761\u4ef6\u5e73\u5747\u5904\u7406\u6548\u679c(CATE)\uff0c\u7136\u540e\u901a\u8fc7\u7ea6\u675f\u5206\u914d\u4f18\u5316\u51b3\u5b9a\u76ee\u6807\u4eba\u7fa4\u548c\u4f18\u60e0\u7b56\u7565", "result": "\u5728\u7559\u5b58\u6d88\u606f\u3001\u4e8b\u4ef6\u5956\u52b1\u548c\u6d88\u8d39\u9608\u503c\u5206\u914d\u7b49\u5e94\u7528\u4e2d\uff0c\u6846\u67b6\u5728\u79bb\u7ebf\u8bc4\u4f30\u4e2d\u6301\u7eed\u4f18\u4e8e\u503e\u5411\u6027\u548c\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5728\u4fdd\u6301\u7528\u6237\u4f53\u9a8c\u7ea6\u675f\u7684\u540c\u65f6\u63d0\u5347\u6536\u5165\u548c\u5b8c\u6210\u7387", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8425\u9500\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u64cd\u4f5c\u624b\u518c\uff0c\u7528\u4e8e\u89c4\u6a21\u5316\u5b9e\u65bd\u56e0\u679c\u76ee\u6807\u7b56\u7565\u3001\u8bbe\u7f6e\u4e1a\u52a1\u62a4\u680f\uff0c\u5e76\u4f7f\u8425\u9500\u6d3b\u52a8\u4e0e\u6218\u7565KPI\u4fdd\u6301\u4e00\u81f4"}}
{"id": "2512.20257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20257", "abs": "https://arxiv.org/abs/2512.20257", "authors": ["Daniele Cardullo", "Simone Teglia", "Irene Amerini"], "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation", "comment": null, "summary": "With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.", "AI": {"tldr": "LADLE-MM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5668\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u548cBLIP\u591a\u6a21\u6001\u5d4c\u5165\uff0c\u5728DGM4\u548cVERITE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u591a\u5a92\u4f53\u751f\u6210\u5de5\u5177\u666e\u53ca\uff0c\u8de8\u6a21\u6001\u7684\u5408\u6210\u5185\u5bb9\u64cd\u7eb5\u6210\u4e3a\u5e7f\u6cdb\u5a01\u80c1\uff0c\u7528\u4e8e\u626d\u66f2\u91cd\u8981\u4e8b\u4ef6\u53d9\u4e8b\u548c\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u6216\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e94\u7528\u3002", "method": "\u63d0\u51faLADLE-MM\uff1a\u57fa\u4e8e\u6709\u9650\u6807\u6ce8\u7684\u5b66\u4e60\u96c6\u6210\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5668\u3002\u91c7\u7528\u6a21\u578b\u96c6\u6210\u521d\u59cb\u5316\uff0c\u5305\u542b\u4e24\u4e2a\u5355\u6a21\u6001\u5206\u652f\u548c\u4e00\u4e2a\u591a\u6a21\u6001\u5206\u652f\uff0c\u5229\u7528BLIP\u63d0\u53d6\u7684\u591a\u6a21\u6001\u5d4c\u5165\u4f5c\u4e3a\u56fa\u5b9a\u53c2\u8003\u7a7a\u95f4\u6765\u589e\u5f3a\u56fe\u50cf\u548c\u6587\u672c\u8868\u793a\u3002", "result": "\u5728DGM4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u6bd4\u5148\u524dSOTA\u6a21\u578b\u5c1160.3%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u4e8c\u5143\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff1b\u5728VERITE\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4f7f\u7528\u66f4\u590d\u6742\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5355\u6a21\u6001\u504f\u5dee\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LADLE-MM\u8bc1\u660e\u4e86\u5728\u6709\u9650\u6807\u6ce8\u8bbe\u7f6e\u4e0b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u6709\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u589e\u5f3a\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20006", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20006", "abs": "https://arxiv.org/abs/2512.20006", "authors": ["Sukumar Kishanthan", "Asela Hevapathige"], "title": "Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance", "comment": null, "summary": "Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.", "AI": {"tldr": "\u63d0\u51faOGAB\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u6b63\u4ea4\u6027\u548c\u5206\u7ec4\u611f\u77e5\u504f\u7f6e\u5b66\u4e60\u7f13\u89e3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u7b7e\u4fe1\u606f\u5373\u53ef\u589e\u5f3a\u7279\u5f81\u533a\u5206\u5ea6\u3002", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u662f\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u6316\u6398\u4e2d\u7684\u5e38\u89c1\u6311\u6218\uff0c\u4f1a\u5bfc\u81f4\u5206\u7c7b\u5668\u6027\u80fd\u4e0b\u964d\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0b\u6027\u80fd\u4ecd\u7136\u4f1a\u6076\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u9884\u5904\u7406\u6570\u636e\u4fee\u6539\u6216\u540e\u5904\u7406\u6821\u6b63\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4f46\u4f5c\u8005\u5e0c\u671b\u76f4\u63a5\u5728\u8bad\u7ec3\u9636\u6bb5\u5d4c\u5165\u5b66\u4e60\u5c42\u9762\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faOGAB\u6fc0\u6d3b\u51fd\u6570\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u6b63\u4ea4\u53d8\u6362\uff1a\u901a\u8fc7\u4fdd\u6301\u7279\u5f81\u72ec\u7acb\u6027\u6765\u4fdd\u7559\u5c11\u6570\u7c7b\u4fe1\u606f\uff0c\u9632\u6b62\u591a\u6570\u7c7b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff1b2\uff09\u5206\u7ec4\u611f\u77e5\u504f\u7f6e\u673a\u5236\uff1a\u81ea\u52a8\u8bc6\u522b\u6570\u636e\u805a\u7c7b\u5e76\u8c03\u6574\u5d4c\u5165\u4ee5\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u3002\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86OGAB\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff0c\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u6fc0\u6d3b\u51fd\u6570\u53ef\u4ee5\u5f15\u5165\u5f3a\u5f52\u7eb3\u504f\u7f6e\u6765\u89e3\u51b3\u590d\u6742\u6570\u636e\u6311\u6218\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u975e\u7ebf\u6027\u529f\u80fd\u3002OGAB\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u548c\u5206\u7ec4\u611f\u77e5\u504f\u7f6e\u5b66\u4e60\uff0c\u80fd\u591f\u5728\u65e0\u9700\u663e\u5f0f\u6807\u7b7e\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20432", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.20432", "abs": "https://arxiv.org/abs/2512.20432", "authors": ["Ji Song", "Xing Wang", "Jianguo Wu", "Xiaowei Yue"], "title": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images", "comment": null, "summary": "In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.", "AI": {"tldr": "\u63d0\u51faTBSD\u65b9\u6cd5\u7528\u4e8e\u7eb9\u7406\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u7eb9\u7406\u57fa\u51fd\u6570\u5b66\u4e60\u51c6\u5468\u671f\u7eb9\u7406\u6a21\u5f0f\uff0c\u51cf\u5c11\u8bef\u8bc6\u522b\uff0c\u964d\u4f4e\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u7eb9\u7406\u7f3a\u9677\u56fe\u50cf\u65f6\u5b58\u5728\u8bef\u8bc6\u522b\u3001\u9c81\u68d2\u6027\u4f4e\u3001\u8fc7\u5ea6\u4f9d\u8d56\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u6570\u636e\u96c6\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u9488\u5bf9\u7eb9\u7406\u56fe\u50cf\u80cc\u666f\u5e73\u6ed1\u3001\u5f02\u5e38\u7a00\u758f\u7684\u7279\u70b9\u5f00\u53d1\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7eb9\u7406\u57fa\u96c6\u6210\u5e73\u6ed1\u5206\u89e3(TBSD)\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u8fc7\u7a0b\uff1a1) \u5b66\u4e60\u7eb9\u7406\u57fa\u51fd\u6570\u4ee5\u6709\u6548\u63d0\u53d6\u51c6\u5468\u671f\u7eb9\u7406\u6a21\u5f0f\uff1b2) \u5229\u7528\u7eb9\u7406\u57fa\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u9632\u6b62\u7eb9\u7406\u8bef\u8bc6\u522b\u5e76\u9ad8\u7cbe\u5ea6\u6355\u6349\u6f5c\u5728\u5f02\u5e38\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u66f4\u5c11\u7684\u8bef\u8bc6\u522b\u3001\u66f4\u5c0f\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u9700\u6c42\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u90fd\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "TBSD\u65b9\u6cd5\u4e3a\u7eb9\u7406\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u51c6\u5468\u671f\u6027\u5e76\u5229\u7528\u7eb9\u7406\u57fa\u51fd\u6570\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5de5\u4e1a\u5236\u9020\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.20403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20403", "abs": "https://arxiv.org/abs/2512.20403", "authors": ["Xuan-An Le", "Minh-Nam Tran", "Son Nguyen"], "title": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples", "comment": null, "summary": "Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.", "AI": {"tldr": "BRIDGE\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u5927\u6a21\u578b\u5230\u5c0f\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u5c11\u91cf\u6570\u636e\u8bad\u7ec3\u4e2d\u7b49\u89c4\u6a21\u6559\u5e08\u52a9\u624b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528\u8be5\u52a9\u624b\u514d\u8d39\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u8bad\u7ec3\u5c0f\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4eAPI\u6210\u672c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4ece\u5927\u578b\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-4\uff09\u5411\u5c0f\u578b\u53ef\u90e8\u7f72\u6a21\u578b\uff08\u5c0f\u4e8e1B\u53c2\u6570\uff09\u84b8\u998f\u77e5\u8bc6\u9762\u4e34\u5bb9\u91cf\u9884\u7b97\u9677\u9631\uff1a\u5e08\u751f\u6a21\u578b1000\u500d\u5bb9\u91cf\u5dee\u8ddd\u963b\u788d\u76f4\u63a5\u8fc1\u79fb\uff0c\u800cAPI\u6210\u672c\u53c8\u9650\u5236\u6570\u636e\u6536\u96c6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u75283-5%\u6570\u636e\u8bad\u7ec3\u4e2d\u7b49\u89c4\u6a21\u6559\u5e08\u52a9\u624b\uff08\u7ea67B\uff09\uff0c\u901a\u8fc7\u96f6API\u6210\u672c\u7ba1\u9053\u9009\u62e9\u5e73\u8861\u96be\u5ea6\u548c\u591a\u6837\u6027\u7684\u6570\u636e\uff1b2\uff09\u5229\u7528\u6559\u5e08\u52a9\u624b\u514d\u8d39\u751f\u6210\u5b8c\u6574\u6570\u636e\u96c6\u7684\u5408\u6210\u63a8\u7406\u8fc7\u7a0b\u6765\u8bad\u7ec3\u5c0f\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u6307\u4ee4\u8c03\u4f18\u8bfe\u7a0b\u5efa\u7acb\u884c\u4e3a\u5bf9\u9f50\u3002", "result": "\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBRIDGE\u4f7f\u5b66\u751f\u6a21\u578b\u6027\u80fd\u63d0\u534728-41%\uff0c\u5c06\u80fd\u529b\u5dee\u8ddd\u7f29\u5c0f12-16%\uff0c\u540c\u65f6\u4f7f\u752810\u500d\u66f4\u5c11\u7684\u6559\u5e08\u67e5\u8be2\u3002\u4ec5\u75285%\u8d44\u6e90\u5c31\u8d85\u8d8a\u4e86\u4f7f\u7528100%\u9884\u7b97\u7684\u76f4\u63a5\u84b8\u998f\u57fa\u7ebf\u3002", "conclusion": "BRIDGE\u901a\u8fc7\u6218\u7565\u4e2d\u95f4\u5316\u548c\u9884\u7b97\u4e0d\u5bf9\u79f0\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6210\u672c-\u6027\u80fd\u8fb9\u754c\uff0c\u4e3a\u5927\u89c4\u6a21\u4e13\u6709\u6a21\u578b\u5230\u5c0f\u578b\u53ef\u90e8\u7f72\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20438", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20438", "abs": "https://arxiv.org/abs/2512.20438", "authors": ["Jibin Joseph"], "title": "Machine Learning to Predict Digital Frustration from Clickstream Data", "comment": "17 pages, 5 figures", "summary": "Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.", "AI": {"tldr": "\u4f7f\u7528\u7535\u5546\u70b9\u51fb\u6d41\u6570\u636e\u9884\u6d4b\u7528\u6237\u4f1a\u8bdd\u662f\u5426\u53d7\u632b\uff0cXGBoost\u548cLSTM\u6a21\u578b\u5747\u80fd\u8fbe\u523090%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0cLSTM\u5728\u4ec520-30\u6b21\u4ea4\u4e92\u540e\u5373\u53ef\u53ef\u9760\u9884\u6d4b", "motivation": "\u79fb\u52a8\u5e94\u7528\u548c\u7f51\u7ad9\u7684\u7528\u6237\u53d7\u632b\u4f1a\u5bfc\u81f4\u9500\u552e\u635f\u5931\u548c\u6295\u8bc9\uff0c\u4f01\u4e1a\u9700\u8981\u9884\u6d4b\u7528\u6237\u53d7\u632b\u4f1a\u8bdd\u4ee5\u6539\u5584\u7528\u6237\u4f53\u9a8c", "method": "\u57fa\u4e8e\u70b9\u51fb\u6d41\u6570\u636e\u5b9a\u4e49\u53d7\u632b\u89c4\u5219\uff08\u6124\u6012\u7206\u53d1\u3001\u6765\u56de\u5bfc\u822a\u3001\u8d2d\u7269\u8f66\u6d41\u5931\u3001\u641c\u7d22\u56f0\u96be\u3001\u957f\u65f6\u95f4\u5f98\u5f8a\uff09\uff0c\u6784\u5efa\u8868\u683c\u7279\u5f81\u8bad\u7ec3XGBoost\u7b49\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u4f7f\u7528\u5b8c\u6574\u4e8b\u4ef6\u5e8f\u5217\u8bad\u7ec3LSTM\u5206\u7c7b\u5668", "result": "XGBoost\u8fbe\u5230\u7ea690%\u51c6\u786e\u7387\u548c0.9579 ROC AUC\uff0cLSTM\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230\u7ea691%\u51c6\u786e\u7387\u548c0.9705 ROC AUC\uff0c\u4e14\u4ec5\u9700\u524d20-30\u6b21\u4ea4\u4e92\u5373\u53ef\u53ef\u9760\u9884\u6d4b\u53d7\u632b", "conclusion": "\u70b9\u51fb\u6d41\u6570\u636e\u80fd\u6709\u6548\u9884\u6d4b\u7528\u6237\u53d7\u632b\uff0cLSTM\u6a21\u578b\u5728\u65e9\u671f\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u52a9\u4e8e\u4f01\u4e1a\u53ca\u65f6\u5e72\u9884\u6539\u5584\u7528\u6237\u4f53\u9a8c"}}
