{"id": "2510.10035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10035", "abs": "https://arxiv.org/abs/2510.10035", "authors": ["Jusheng Zhang", "Kaitong Cai", "Qinglin Zeng", "Ningyuan Liu", "Stephen Fan", "Ziliang Chen", "Keze Wang"], "title": "Failure-Driven Workflow Refinement", "comment": null, "summary": "Optimizing LLM-based workflows is typically formulated as a global search,\nwhere candidate workflows are evaluated based on a scalar metric. This\nparadigm, however, suffers from a critical flaw: information collapse. By\nreducing rich, multi-step execution traces to simple success/failure signals,\nexisting methods are rendered blind to the underlying structure of failures,\nfundamentally preventing them from modeling the workflow's failure\ndistribution. We reconceptualize this challenge as a distributional problem. We\npropose a new paradigm where the optimization goal is not to maximize a scalar\nscore, but to directly minimize a workflow's Expected Failure Mass, i.e., the\nintegral of its failure probability density function defined over a\nhigh-dimensional Failure Signature Space (FSS). This distributional lens allows\nus to move from inefficient, zero-order optimization to a principled,\ngradient-like descent on the failure landscape itself. We introduce CE-Graph, a\nframework that operationalizes this paradigm through a novel, failure-driven\nrefinement process. CE-Graph approximates the failure distribution from a pool\nof counterexamples, identifies its densest regions as recurring failure modes,\nand applies targeted, operator-constrained graph edits via a Propose-and-Verify\nmechanism to greedily reduce the failure mass. On math, code, and QA\nbenchmarks, our CE-Graph achieves higher robustness at a significantly lower\ncost than strong baselines. This suggests that a system's reliability emerges\nnot from avoiding failures, but from systematically learning and reshaping the\ngeometric structure of its failure distributions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u8303\u5f0f\uff0c\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u5931\u8d25\u8d28\u91cf\u800c\u975e\u6700\u5927\u5316\u6807\u91cf\u5206\u6570\u6765\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u574d\u7f29\u95ee\u9898\uff0c\u5c06\u4e30\u5bcc\u7684\u591a\u6b65\u6267\u884c\u8f68\u8ff9\u7b80\u5316\u4e3a\u7b80\u5355\u7684\u6210\u529f/\u5931\u8d25\u4fe1\u53f7\uff0c\u65e0\u6cd5\u5efa\u6a21\u5de5\u4f5c\u6d41\u7684\u5931\u8d25\u5206\u5e03\u3002", "method": "\u63d0\u51faCE-Graph\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4f8b\u6c60\u8fd1\u4f3c\u5931\u8d25\u5206\u5e03\uff0c\u8bc6\u522b\u5bc6\u96c6\u533a\u57df\u4f5c\u4e3a\u91cd\u590d\u5931\u8d25\u6a21\u5f0f\uff0c\u4f7f\u7528\u63d0\u8bae-\u9a8c\u8bc1\u673a\u5236\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u56fe\u7f16\u8f91\u6765\u8d2a\u5a6a\u5730\u51cf\u5c11\u5931\u8d25\u8d28\u91cf\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCE-Graph\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u6bd4\u5f3a\u57fa\u7ebf\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u5e76\u975e\u6765\u81ea\u907f\u514d\u5931\u8d25\uff0c\u800c\u662f\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u548c\u91cd\u5851\u5176\u5931\u8d25\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u5b9e\u73b0\u3002"}}
{"id": "2510.09668", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5b50\u5d4c\u5165\u548c\u4e34\u5e8a\u77e5\u8bc6\uff0c\u4f7f\u7528\u5143\u542f\u53d1\u5f0f\u4f18\u5316\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u662f\u5bfc\u81f4\u53ef\u9884\u9632\u4e0d\u826f\u4e8b\u4ef6\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u540c\u65f6\u4e86\u89e3\u54ea\u4e9b\u836f\u7269\u4e0d\u76f8\u4e92\u4f5c\u7528\u5bf9\u4e8e\u5b89\u5168\u5904\u65b9\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u540c\u6837\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u7684\u5206\u5b50\u5d4c\u5165\uff08Mol2Vec\u548cSMILES-BERT\uff09\u548c\u65e0\u6cc4\u6f0f\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u4e34\u5e8a\u8bc4\u5206\uff08RBScore\uff09\uff0c\u4f7f\u7528\u4e09\u9636\u6bb5\u5143\u542f\u53d1\u5f0f\u7b56\u7565\uff08RSmpl-ACO-PSO\uff09\u4f18\u5316\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff08DrugBank\u4e0aROC-AUC 0.911\uff0cPR-AUC 0.867\uff09\uff0c\u5e76\u57282\u578b\u7cd6\u5c3f\u75c5\u961f\u5217\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u80fd\u591f\u652f\u6301\u66f4\u5b89\u5168\u7684\u836f\u7269\u6cbb\u7597\u548c\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2510.09867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09867", "abs": "https://arxiv.org/abs/2510.09867", "authors": ["Zhi Chen", "Xin Yu", "Xiaohui Tao", "Yan Li", "Zi Huang"], "title": "Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation", "comment": "Accepted to the journal Pattern Recognition in 2025", "summary": "Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across\nvarious tasks by pre-training on numerous image-text pairs. These models often\nbenefit from using an ensemble of context prompts to represent a class. Despite\nbeing effective, conventional prompt ensembling that averages textual features\nof context prompts often yields suboptimal results. This is because feature\naveraging shifts the class centroids away from the true class distribution. To\naddress this issue, we propose the Cluster-Aware Prompt Ensemble Learning\n(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL\nclassifies images into one of several class clusters, each represented by a\ndistinct prompt. Instead of ensembling prompts in the feature space, we perform\nensembling in the classification logits space, aligning better with the visual\nfeature distribution. To further optimize prompt fine-tuning while maintaining\ncluster-specific discriminative power, we introduce a cluster-preserving\nregularization term. This ensures that prompts remain distinct and specialized\nfor different clusters, preventing collapse into a uniform direction.\nAdditionally, we integrate an adaptive prompt weighting technique to\ndynamically adjust the attention weights for flawed or ambiguous prompts,\nensuring robust performance across diverse datasets and tasks.", "AI": {"tldr": "\u63d0\u51faCluster-Aware Prompt Ensemble Learning (CAPEL)\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5206\u7c7blogits\u7a7a\u95f4\u8fdb\u884c\u96c6\u6210\u800c\u975e\u7279\u5f81\u7a7a\u95f4\u5e73\u5747\uff0c\u66f4\u597d\u5730\u4fdd\u6301\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u805a\u7c7b\u7279\u6027\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u901a\u8fc7\u5e73\u5747\u6587\u672c\u7279\u5f81\u6765\u4ee3\u8868\u7c7b\u522b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u4f7f\u7c7b\u522b\u8d28\u5fc3\u504f\u79bb\u771f\u5b9e\u5206\u5e03\uff0c\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002", "method": "CAPEL\u6846\u67b6\u5c06\u56fe\u50cf\u5206\u7c7b\u5230\u591a\u4e2a\u7c7b\u522b\u805a\u7c7b\u4e2d\uff0c\u6bcf\u4e2a\u805a\u7c7b\u7531\u4e0d\u540c\u7684\u63d0\u793a\u8868\u793a\uff1b\u5728\u5206\u7c7blogits\u7a7a\u95f4\u8fdb\u884c\u96c6\u6210\uff1b\u5f15\u5165\u805a\u7c7b\u4fdd\u6301\u6b63\u5219\u5316\u9879\uff1b\u96c6\u6210\u81ea\u9002\u5e94\u63d0\u793a\u52a0\u6743\u6280\u672f\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5bf9\u9f50\u89c6\u89c9\u7279\u5f81\u5206\u5e03\uff0c\u4fdd\u6301\u63d0\u793a\u7684\u533a\u5206\u6027\uff0c\u5bf9\u5b58\u5728\u7f3a\u9677\u6216\u6a21\u7cca\u7684\u63d0\u793a\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "CAPEL\u6846\u67b6\u901a\u8fc7\u4fdd\u6301\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u805a\u7c7b\u7279\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.", "AI": {"tldr": "SwarmSys\u662f\u4e00\u4e2a\u53d7\u7fa4\u4f53\u667a\u80fd\u542f\u53d1\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u8005\u3001\u5de5\u4f5c\u8005\u548c\u9a8c\u8bc1\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u81ea\u7ec4\u7ec7\u534f\u8c03\uff0c\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7814\u7a76\u7efc\u5408\u548c\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u89d2\u8272\u6216\u96c6\u4e2d\u63a7\u5236\uff0c\u9650\u5236\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u96c6\u6210\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u548c\u4e8b\u4ef6\u6863\u6848\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u6982\u7387\u5339\u914d\u4ee5\u53ca\u4fe1\u606f\u7d20\u542f\u53d1\u7684\u5f3a\u5316\u673a\u5236\uff0c\u652f\u6301\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u65e0\u5168\u5c40\u76d1\u7763\u7684\u81ea\u7ec4\u7ec7\u6536\u655b\u3002", "result": "\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7814\u7a76\u7efc\u5408\u548c\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cSwarmSys\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7fa4\u4f53\u542f\u53d1\u7684\u534f\u8c03\u662f\u63a8\u8fdb\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u534f\u8c03\u6269\u5c55\u53ef\u80fd\u4e0e\u6a21\u578b\u6269\u5c55\u5728\u63a8\u8fdbLLM\u667a\u80fd\u65b9\u9762\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2510.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09854", "abs": "https://arxiv.org/abs/2510.09854", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "comment": null, "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks.", "AI": {"tldr": "NG-Router\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u7684\u8def\u7531\u5206\u5e03\uff0c\u89e3\u51b3\u8425\u517b\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u548c\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8425\u517b\u95ee\u7b54\u65b9\u6cd5\u9762\u4e34\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u63a8\u7406\u80fd\u529b\u6709\u9650\u3001\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u590d\u6742\u4ee5\u53ca\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u5f71\u54cd\u51b3\u7b56\u51c6\u786e\u6027\u7684\u6311\u6218\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u8282\u70b9\u96c6\u6210\u5230\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u8def\u7531\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u5b50\u56fe\u68c0\u7d22\u673a\u5236\u6765\u8bc6\u522b\u5173\u952e\u8bc1\u636e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNG-Router\u4e00\u81f4\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NG-Router\u4e3a\u590d\u6742\u8425\u517b\u5065\u5eb7\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2510.09869", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09869", "abs": "https://arxiv.org/abs/2510.09869", "authors": ["Sil Hamilton", "Matthew Wilkens", "Andrew Piper"], "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking", "comment": null, "summary": "We present NarraBench, a theory-informed taxonomy of narrative-understanding\ntasks, as well as an associated survey of 78 existing benchmarks in the area.\nWe find significant need for new evaluations covering aspects of narrative\nunderstanding that are either overlooked in current work or are poorly aligned\nwith existing metrics. Specifically, we estimate that only 27% of narrative\ntasks are well captured by existing benchmarks, and we note that some areas --\nincluding narrative events, style, perspective, and revelation -- are nearly\nabsent from current evaluations. We also note the need for increased\ndevelopment of benchmarks capable of assessing constitutively subjective and\nperspectival aspects of narrative, that is, aspects for which there is\ngenerally no single correct answer. Our taxonomy, survey, and methodology are\nof value to NLP researchers seeking to test LLM narrative understanding.", "AI": {"tldr": "NarraBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u7406\u8bba\u6784\u5efa\u7684\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u8c03\u67e5\u4e8678\u4e2a\u73b0\u6709\u57fa\u51c6\uff0c\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u5728\u53d9\u4e8b\u4e8b\u4ef6\u3001\u98ce\u683c\u3001\u89c6\u89d2\u548c\u63ed\u793a\u7b49\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u53ea\u670927%\u7684\u53d9\u4e8b\u4efb\u52a1\u88ab\u73b0\u6709\u57fa\u51c6\u5145\u5206\u8986\u76d6\u3002", "motivation": "\u5f53\u524d\u53d9\u4e8b\u7406\u89e3\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u7a7a\u767d\uff0c\u8bb8\u591a\u91cd\u8981\u65b9\u9762\u88ab\u5ffd\u89c6\u6216\u4e0e\u73b0\u6709\u6307\u6807\u4e0d\u5339\u914d\uff0c\u7279\u522b\u662f\u6784\u6210\u6027\u4e3b\u89c2\u548c\u89c6\u89d2\u6027\u65b9\u9762\u7f3a\u4e4f\u9002\u5f53\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u7406\u8bba\u6307\u5bfc\u7684\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5bf978\u4e2a\u73b0\u6709\u57fa\u51c6\u8fdb\u884c\u7cfb\u7edf\u6027\u8c03\u67e5\u548c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4ec527%\u7684\u53d9\u4e8b\u4efb\u52a1\u88ab\u73b0\u6709\u57fa\u51c6\u5145\u5206\u8986\u76d6\uff0c\u53d9\u4e8b\u4e8b\u4ef6\u3001\u98ce\u683c\u3001\u89c6\u89d2\u548c\u63ed\u793a\u7b49\u5173\u952e\u65b9\u9762\u51e0\u4e4e\u5b8c\u5168\u7f3a\u5931\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u591a\u80fd\u591f\u8bc4\u4f30\u6784\u6210\u6027\u4e3b\u89c2\u548c\u89c6\u89d2\u6027\u65b9\u9762\u7684\u57fa\u51c6\uff0c\u8be5\u5206\u7c7b\u6cd5\u3001\u8c03\u67e5\u548c\u65b9\u6cd5\u8bba\u5bf9\u6d4b\u8bd5LLM\u53d9\u4e8b\u7406\u89e3\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u5c06\u504f\u5dee\u7f13\u89e3\u548c\u7279\u5f81\u9009\u62e9\u6574\u5408\u5230\u5355\u4e00\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u6765\u9632\u6b62\u9690\u85cf\u4f9d\u8d56\u5173\u7cfb\u5bfc\u81f4\u7684\u504f\u89c1\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u7279\u5f81\u6392\u9664\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u9632\u6b62\u9690\u85cf\u4f9d\u8d56\u5173\u7cfb\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u504f\u89c1\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5e73\u8861\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6784\u5efa\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\uff0c\u5b9a\u4e49\u7279\u5f81\u5b50\u96c6\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u4e0e\u96c6\u6210\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u540c\u65f6\u8003\u8651\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9009\u62e9\u7279\u5f81\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861\u6cdb\u5316\u80fd\u529b\u3001\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\uff0c\u907f\u514d\u4ec5\u4f9d\u8d56\u9884\u5904\u7406\u8c03\u6574\u6216\u4e8b\u540e\u4fee\u6b63\u673a\u5236\u3002", "conclusion": "\u8be5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u901a\u7528\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9884\u6d4b\u53d8\u91cf\u76f8\u5173\u4e14\u504f\u89c1\u53ef\u80fd\u65e0\u610f\u4e2d\u91cd\u65b0\u51fa\u73b0\u7684\u73af\u5883\u3002"}}
{"id": "2510.10455", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10455", "abs": "https://arxiv.org/abs/2510.10455", "authors": ["Jiayu Ding", "Xulin Chen", "Garrett E. Katz", "Zhenyu Gan"], "title": "Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds", "comment": null, "summary": "Quadrupedal robots exhibit a wide range of viable gaits, but generating\nspecific footfall sequences often requires laborious expert tuning of numerous\nvariables, such as touch-down and lift-off events and holonomic constraints for\neach leg. This paper presents a unified reinforcement learning framework for\ngenerating versatile quadrupedal gaits by leveraging the intrinsic symmetries\nand velocity-period relationship of dynamic legged systems. We propose a\nsymmetry-guided reward function design that incorporates temporal,\nmorphological, and time-reversal symmetries. By focusing on preserved\nsymmetries and natural dynamics, our approach eliminates the need for\npredefined trajectories, enabling smooth transitions between diverse locomotion\npatterns such as trotting, bounding, half-bounding, and galloping. Implemented\non the Unitree Go2 robot, our method demonstrates robust performance across a\nrange of speeds in both simulations and hardware tests, significantly improving\ngait adaptability without extensive reward tuning or explicit foot placement\ncontrol. This work provides insights into dynamic locomotion strategies and\nunderscores the crucial role of symmetries in robotic gait design.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u8f68\u8ff9\u5373\u53ef\u751f\u6210\u591a\u79cd\u56db\u8db3\u673a\u5668\u4eba\u6b65\u6001\uff0c\u5305\u62ec\u5c0f\u8dd1\u3001\u8df3\u8dc3\u3001\u534a\u8df3\u8dc3\u548c\u75be\u9a70\u7b49\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e13\u5bb6\u624b\u52a8\u8c03\u6574\u5927\u91cf\u53d8\u91cf\u6765\u751f\u6210\u7279\u5b9a\u6b65\u6001\u5e8f\u5217\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u8017\u65f6\u3002", "method": "\u5229\u7528\u52a8\u6001\u817f\u5f0f\u7cfb\u7edf\u7684\u5185\u5728\u5bf9\u79f0\u6027\u548c\u901f\u5ea6-\u5468\u671f\u5173\u7cfb\uff0c\u8bbe\u8ba1\u5bf9\u79f0\u6027\u5f15\u5bfc\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5305\u542b\u65f6\u95f4\u3001\u5f62\u6001\u548c\u65f6\u95f4\u53cd\u8f6c\u5bf9\u79f0\u6027\u3002", "result": "\u5728Unitree Go2\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\uff0c\u5728\u4eff\u771f\u548c\u786c\u4ef6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6b65\u6001\u9002\u5e94\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86\u52a8\u6001\u8fd0\u52a8\u7b56\u7565\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u5728\u673a\u5668\u4eba\u6b65\u6001\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories.", "AI": {"tldr": "Traj-CoA\u662f\u4e00\u4e2a\u7528\u4e8e\u60a3\u8005\u8f68\u8ff9\u5efa\u6a21\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u94fe\u5f0f\u667a\u80fd\u4f53\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u4fdd\u6301\u5b8c\u6574\u65f6\u95f4\u7ebf\uff0c\u5728\u96f6\u6837\u672c\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u65f6\u9762\u4e34\u6570\u636e\u957f\u4e14\u566a\u58f0\u591a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ec\u5de5\u4f5c\u667a\u80fd\u4f53\u94fe\u5f0f\u5904\u7406EHR\u6570\u636e\u5757\uff0c\u5c06\u5173\u952e\u4e8b\u4ef6\u63d0\u53d6\u5230\u5171\u4eab\u957f\u671f\u8bb0\u5fc6\u6a21\u5757EHRMem\u4e2d\uff0c\u6700\u540e\u7531\u7ba1\u7406\u667a\u80fd\u4f53\u7efc\u5408\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u4e8e\u4e94\u5e74EHR\u6570\u636e\u7684\u96f6\u6837\u672c\u4e00\u5e74\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cTraj-CoA\u4f18\u4e8e\u56db\u7c7b\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u4e34\u5e8a\u5bf9\u9f50\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Traj-CoA\u4e3a\u590d\u6742\u60a3\u8005\u8f68\u8ff9\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.10516", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10516", "abs": "https://arxiv.org/abs/2510.10516", "authors": ["Kanishkha Jaisankar", "Xiaoyang Jiang", "Feifan Liao", "Jeethu Sreenivas Amuthan"], "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control", "comment": null, "summary": "Energy-efficient and high-performance motor control remains a critical\nchallenge in robotics, particularly for high-dimensional continuous control\ntasks with limited onboard resources. While Deep Reinforcement Learning (DRL)\nhas achieved remarkable results, its computational demands and energy\nconsumption limit deployment in resource-constrained environments. This paper\nintroduces a novel framework combining population-coded Spiking Neural Networks\n(SNNs) with DRL to address these challenges. Our approach leverages the\nevent-driven, asynchronous computation of SNNs alongside the robust policy\noptimization capabilities of DRL, achieving a balance between energy efficiency\nand control performance. Central to this framework is the Population-coded\nSpiking Actor Network (PopSAN), which encodes high-dimensional observations\ninto neuronal population activities and enables optimal policy learning through\ngradient-based updates. We evaluate our method on the Isaac Gym platform using\nthe PixMC benchmark with complex robotic manipulation tasks. Experimental\nresults on the Franka robotic arm demonstrate that our approach achieves energy\nsavings of up to 96.10% compared to traditional Artificial Neural Networks\n(ANNs) while maintaining comparable control performance. The trained SNN\npolicies exhibit robust finger position tracking with minimal deviation from\ncommanded trajectories and stable target height maintenance during\npick-and-place operations. These results position population-coded SNNs as a\npromising solution for energy-efficient, high-performance robotic control in\nresource-constrained applications, paving the way for scalable deployment in\nreal-world robotics systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7fa4\u4f53\u7f16\u7801\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u7684\u65b0\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe96.10%\u7684\u80fd\u8017\u8282\u7701\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u80fd\u8017\u6548\u7387\u4e0e\u9ad8\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u4f20\u7edfDRL\u65b9\u6cd5\u8ba1\u7b97\u9700\u6c42\u5927\u3001\u80fd\u8017\u9ad8\u3002", "method": "\u4f7f\u7528\u7fa4\u4f53\u7f16\u7801\u8109\u51b2\u6f14\u5458\u7f51\u7edc(PopSAN)\uff0c\u5c06\u9ad8\u7ef4\u89c2\u6d4b\u7f16\u7801\u4e3a\u795e\u7ecf\u5143\u7fa4\u4f53\u6d3b\u52a8\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u66f4\u65b0\u5b9e\u73b0\u6700\u4f18\u7b56\u7565\u5b66\u4e60\uff0c\u7ed3\u5408SNN\u7684\u4e8b\u4ef6\u9a71\u52a8\u5f02\u6b65\u8ba1\u7b97\u548cDRL\u7684\u9c81\u68d2\u7b56\u7565\u4f18\u5316\u80fd\u529b\u3002", "result": "\u5728Isaac Gym\u5e73\u53f0\u7684PixMC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFranka\u673a\u68b0\u81c2\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc(ANN)\u8282\u770196.10%\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u63a7\u5236\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u624b\u6307\u4f4d\u7f6e\u8ddf\u8e2a\u548c\u76ee\u6807\u9ad8\u5ea6\u7ef4\u6301\u3002", "conclusion": "\u7fa4\u4f53\u7f16\u7801SNN\u4e3a\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u4e2d\u7684\u8282\u80fd\u9ad8\u6027\u80fd\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.09732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09732", "abs": "https://arxiv.org/abs/2510.09732", "authors": ["P. van Oerle", "R. H. Bemthuis", "F. A. Bukhsh"], "title": "Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction", "comment": "12 pages, 2 figures, 3 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Large Language Models (LLMs) are increasingly used to generate textual\nexplanations of process models discovered from event logs. Producing\nexplanations from large behavioral abstractions (e.g., directly-follows graphs\nor Petri nets) can be computationally expensive. This paper reports an\nexploratory evaluation of explanation quality under progressive\nbehavioral-input reduction, where models are discovered from progressively\nsmaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple\ninput sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a\nsecond LLM to assess completeness, bottleneck identification, and suggested\nimprovements. On synthetic logs, explanation quality is largely preserved under\nmoderate reduction, indicating a practical cost-quality trade-off. The study is\nexploratory, as the scores are LLM-based (comparative signals rather than\nground truth) and the data are synthetic. The results suggest a path toward\nmore computationally efficient, LLM-assisted process analysis in\nresource-constrained settings.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fc7\u7a0b\u6a21\u578b\u89e3\u91ca\u4e2d\u7684\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u6743\u8861\uff0c\u901a\u8fc7\u9010\u6b65\u51cf\u5c11\u884c\u4e3a\u8f93\u5165\u6765\u6d4b\u8bd5\u89e3\u91ca\u8d28\u91cf\u4fdd\u6301\u60c5\u51b5", "motivation": "\u76f4\u63a5\u4ece\u5927\u578b\u884c\u4e3a\u62bd\u8c61\uff08\u5982\u76f4\u63a5\u8ddf\u968f\u56fe\u6216Petri\u7f51\uff09\u751f\u6210\u89e3\u91ca\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u63a2\u7d22\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u66f4\u9ad8\u6548\u7684\u8fc7\u7a0b\u5206\u6790\u65b9\u6cd5", "method": "\u5efa\u7acb\u7ba1\u9053\uff1a(i)\u4ece\u56fa\u5b9a\u65e5\u5fd7\u7684\u9010\u6b65\u7f29\u5c0f\u524d\u7f00\u53d1\u73b0\u6a21\u578b\uff0c(ii)\u7528LLM\u751f\u6210\u89e3\u91ca\uff0c(iii)\u7528\u7b2c\u4e8c\u4e2aLLM\u8bc4\u4f30\u5b8c\u6574\u6027\u3001\u74f6\u9888\u8bc6\u522b\u548c\u6539\u8fdb\u5efa\u8bae", "result": "\u5728\u5408\u6210\u65e5\u5fd7\u4e0a\uff0c\u9002\u5ea6\u51cf\u5c11\u8f93\u5165\u65f6\u89e3\u91ca\u8d28\u91cf\u57fa\u672c\u4fdd\u6301\uff0c\u8868\u660e\u5b58\u5728\u5b9e\u7528\u7684\u6210\u672c-\u8d28\u91cf\u6743\u8861", "conclusion": "\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u8f85\u52a9\u8fc7\u7a0b\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u9700\u6ce8\u610f\u8bc4\u4f30\u57fa\u4e8eLLM\u8bc4\u5206\u4e14\u6570\u636e\u4e3a\u5408\u6210\u7684\u63a2\u7d22\u6027\u9650\u5236"}}
{"id": "2510.10602", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10602", "abs": "https://arxiv.org/abs/2510.10602", "authors": ["Zhuoheng Gao", "Jiyao Zhang", "Zhiyong Xie", "Hao Dong", "Zhaofei Yu", "Rongmei Chen", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "comment": null, "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects.", "AI": {"tldr": "SpikeGrasp\u662f\u4e00\u4e2a\u53d7\u795e\u7ecf\u542f\u53d1\u76846-DoF\u6293\u53d6\u68c0\u6d4b\u6846\u67b6\uff0c\u76f4\u63a5\u5904\u7406\u6765\u81ea\u7acb\u4f53\u8109\u51b2\u76f8\u673a\u7684\u539f\u59cb\u5f02\u6b65\u4e8b\u4ef6\uff0c\u65e0\u9700\u91cd\u5efa3D\u70b9\u4e91\uff0c\u5728\u6742\u4e71\u548c\u65e0\u7eb9\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u6293\u53d6\u7cfb\u7edf\u4f9d\u8d56\u5c06\u4f20\u611f\u5668\u6570\u636e\u8f6c\u6362\u4e3a\u663e\u5f0f3D\u70b9\u4e91\uff0c\u8fd9\u4e00\u8ba1\u7b97\u6b65\u9aa4\u5728\u751f\u7269\u667a\u80fd\u4e2d\u4e0d\u5b58\u5728\u3002\u672c\u6587\u63a2\u7d22\u4e00\u79cd\u6839\u672c\u4e0a\u4e0d\u540c\u7684\u3001\u53d7\u795e\u7ecf\u542f\u53d1\u7684\u8303\u5f0f\u3002", "method": "\u5f15\u5165SpikeGrasp\u6846\u67b6\uff0c\u6a21\u62df\u751f\u7269\u89c6\u89c9\u8fd0\u52a8\u901a\u8def\uff0c\u5904\u7406\u6765\u81ea\u7acb\u4f53\u8109\u51b2\u76f8\u673a\u7684\u539f\u59cb\u5f02\u6b65\u4e8b\u4ef6\uff0c\u4f7f\u7528\u5faa\u73af\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8fed\u4ee3\u4f18\u5316\u6293\u53d6\u5047\u8bbe\uff0c\u65e0\u9700\u91cd\u5efa\u70b9\u4e91\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSpikeGrasp\u8d85\u8d8a\u4f20\u7edf\u57fa\u4e8e\u70b9\u4e91\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6742\u4e71\u548c\u65e0\u7eb9\u7406\u573a\u666f\u4e2d\uff0c\u5e76\u663e\u793a\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u9a8c\u8bc1\u8fd9\u79cd\u7aef\u5230\u7aef\u3001\u53d7\u795e\u7ecf\u542f\u53d1\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0cSpikeGrasp\u4e3a\u672a\u6765\u80fd\u591f\u5b9e\u73b0\u81ea\u7136\u754c\u4e2d\u6d41\u7545\u9ad8\u6548\u64cd\u4f5c\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u7279\u522b\u662f\u9488\u5bf9\u52a8\u6001\u7269\u4f53\u3002"}}
{"id": "2510.10082", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10082", "abs": "https://arxiv.org/abs/2510.10082", "authors": ["Parthiv Chatterjee", "Shivam Sonawane", "Amey Hengle", "Aditya Tanna", "Sourish Dasgupta", "Tanmoy Chakraborty"], "title": "Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers", "comment": null, "summary": "Document summarization enables efficient extraction of user-relevant content\nbut is inherently shaped by individual subjectivity, making it challenging to\nidentify subjective salient information in multifaceted documents. This\ncomplexity underscores the necessity for personalized summarization. However,\ntraining models for personalized summarization has so far been challenging,\nparticularly because diverse training data containing both user preference\nhistory (i.e., click-skip trajectory) and expected (gold-reference) summaries\nare scarce. The MS/CAS PENS dataset is a valuable resource but includes only\npreference history without target summaries, preventing end-to-end supervised\nlearning, and its limited topic-transition diversity further restricts\ngeneralization. To address this, we propose $\\mathrm{PerAugy}$, a novel\ncross-trajectory shuffling and summary-content perturbation based data\naugmentation technique that significantly boosts the accuracy of four\nstate-of-the-art baseline (SOTA) user-encoders commonly used in personalized\nsummarization frameworks (best result: $\\text{0.132}$$\\uparrow$ w.r.t AUC). We\nselect two such SOTA summarizer frameworks as baselines and observe that when\naugmented with their corresponding improved user-encoders, they consistently\nshow an increase in personalization (avg. boost: $\\text{61.2\\%}\\uparrow$ w.r.t.\nPSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the\naugmented dataset by \\peraugy, we introduce three dataset diversity metrics --\n$\\mathrm{TP}$, $\\mathrm{RTC}$, and \\degreed\\ to quantify the induced diversity.\nWe find that $\\mathrm{TP}$ and $\\mathrm{DegreeD}$ strongly correlate with\nuser-encoder performance on the PerAugy-generated dataset across all accuracy\nmetrics, indicating that increased dataset diversity is a key factor driving\nperformance gains.", "AI": {"tldr": "\u63d0\u51faPerAugy\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u8de8\u8f68\u8ff9\u6df7\u6d17\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u6458\u8981\u4e2d\u7528\u6237\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u6570\u636e\u96c6\u591a\u6837\u6027\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u4e2a\u6027\u5316\u6458\u8981\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5982MS/CAS PENS\u7f3a\u4e4f\u76ee\u6807\u6458\u8981\u4e14\u4e3b\u9898\u8f6c\u6362\u591a\u6837\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faPerAugy\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5305\u542b\u8de8\u8f68\u8ff9\u6df7\u6d17\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u6570\u636e\u96c6\u591a\u6837\u6027\u6307\u6807(TP\u3001RTC\u3001DegreeD)\u6765\u91cf\u5316\u589e\u5f3a\u6548\u679c\u3002", "result": "PerAugy\u663e\u8457\u63d0\u5347\u4e86\u56db\u79cdSOTA\u7528\u6237\u7f16\u7801\u5668\u7684\u51c6\u786e\u6027(\u6700\u4f73AUC\u63d0\u53470.132)\uff0c\u589e\u5f3a\u540e\u7684\u6458\u8981\u6846\u67b6\u4e2a\u6027\u5316\u7a0b\u5ea6\u5e73\u5747\u63d0\u534761.2%(PSE-SU4\u6307\u6807)\u3002", "conclusion": "\u6570\u636e\u96c6\u591a\u6837\u6027\u662f\u63d0\u5347\u4e2a\u6027\u5316\u6458\u8981\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0cTP\u548cDegreeD\u6307\u6807\u4e0e\u7528\u6237\u7f16\u7801\u5668\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86PerAugy\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10781", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.10781", "abs": "https://arxiv.org/abs/2510.10781", "authors": ["Douglas Hutchings", "Luai Abuelsamen", "Karthik Rajgopal"], "title": "Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis", "comment": "23 pages, 7 figures. Technical report with complete implementation\n  details and open-source code", "summary": "We present a comprehensive two-layer Voronoi coverage control approach for\ncoordinating hybrid aerial-ground robot teams in hazardous material emergency\nresponse scenarios. Traditional Voronoi coverage control methods face three\ncritical limitations in emergency contexts: heterogeneous agent capabilities\nwith vastly different velocities, clustered initial deployment configurations,\nand urgent time constraints requiring rapid response rather than eventual\nconvergence. Our method addresses these challenges through a decoupled\ntwo-layer architecture that separately optimizes aerial and ground robot\npositioning, with aerial agents delivering ground sensors via airdrop to\nhigh-priority locations. We provide detailed implementation of bounded Voronoi\ncell computation, efficient numerical integration techniques for\nimportance-weighted centroids, and robust control strategies that prevent agent\ntrapping. Simulation results demonstrate an 88% reduction in response time,\nachieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds\ncompared to 220 seconds for ground-only deployment. Complete implementation\ncode is available at https://github.com/dHutchings/ME292B.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42Voronoi\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u534f\u8c03\u6df7\u5408\u7a7a\u4e2d-\u5730\u9762\u673a\u5668\u4eba\u56e2\u961f\u5728\u5371\u9669\u6750\u6599\u5e94\u6025\u54cd\u5e94\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u4f18\u5316\u7a7a\u4e2d\u548c\u5730\u9762\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfVoronoi\u8986\u76d6\u63a7\u5236\u5728\u5e94\u6025\u573a\u666f\u4e2d\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5f02\u6784\u4ee3\u7406\u80fd\u529b\uff08\u901f\u5ea6\u5dee\u5f02\u5927\uff09\u3001\u96c6\u7fa4\u521d\u59cb\u90e8\u7f72\u914d\u7f6e\u3001\u4ee5\u53ca\u7d27\u6025\u65f6\u95f4\u7ea6\u675f\u9700\u8981\u5feb\u901f\u54cd\u5e94\u800c\u975e\u6e10\u8fdb\u6536\u655b\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7684\u53cc\u5c42\u67b6\u6784\uff0c\u5206\u522b\u4f18\u5316\u7a7a\u4e2d\u548c\u5730\u9762\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u901a\u8fc7\u7a7a\u6295\u65b9\u5f0f\u5c06\u5730\u9762\u4f20\u611f\u5668\u90e8\u7f72\u5230\u9ad8\u4f18\u5148\u7ea7\u4f4d\u7f6e\u3002\u5305\u62ec\u6709\u754cVoronoi\u5355\u5143\u8ba1\u7b97\u3001\u91cd\u8981\u6027\u52a0\u6743\u8d28\u5fc3\u7684\u9ad8\u6548\u6570\u503c\u79ef\u5206\u6280\u672f\u4ee5\u53ca\u9632\u6b62\u4ee3\u7406\u9677\u5165\u56f0\u5883\u7684\u9c81\u68d2\u63a7\u5236\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u54cd\u5e94\u65f6\u95f4\u51cf\u5c11\u4e8688%\uff0c\u572825\u79d2\u5185\u5b9e\u73b0\u76ee\u6807\u4f20\u611f\u5668\u8986\u76d6\uff08\u521d\u59cb\u4f20\u611f\u5668\u635f\u593118.5%\uff09\uff0c\u800c\u4ec5\u5730\u9762\u90e8\u7f72\u9700\u8981220\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e94\u6025\u54cd\u5e94\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6df7\u5408\u673a\u5668\u4eba\u56e2\u961f\u7684\u90e8\u7f72\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002"}}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AuraGen\u6570\u636e\u751f\u6210\u5f15\u64ce\u548cSafiron\u9884\u6267\u884c\u5b89\u5168\u62a4\u680f\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u51b3\u6570\u636e\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u5b9e\u73b0\u5728\u8ba1\u5212\u9636\u6bb5\u800c\u975e\u6267\u884c\u540e\u68c0\u6d4b\u548c\u9884\u9632AI\u4ee3\u7406\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u62a4\u680f\u5927\u591a\u5728\u6267\u884c\u540e\u64cd\u4f5c\uff0c\u96be\u4ee5\u6269\u5c55\u4e14\u65e0\u6cd5\u5728\u8ba1\u5212\u7ea7\u522b\u8fdb\u884c\u53ef\u63a7\u76d1\u7763\u3002\u67d0\u4e9b\u98ce\u9669\u4e00\u65e6\u6267\u884c\u4f1a\u9020\u6210\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u5728\u8ba1\u5212\u9636\u6bb5\u5e72\u9884\u662f\u6700\u5b89\u5168\u7684\u9884\u9632\u65b9\u5f0f\u3002", "method": "1) AuraGen\uff1a\u53ef\u63a7\u6570\u636e\u751f\u6210\u5f15\u64ce\uff0c\u5408\u6210\u826f\u6027\u8f68\u8ff9\u3001\u6ce8\u5165\u5206\u7c7b\u98ce\u9669\u3001\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8fc7\u6ee4\u8f93\u51fa\uff1b2) Safiron\uff1a\u57fa\u7840\u62a4\u680f\u6a21\u578b\uff0c\u7ed3\u5408\u8de8\u89c4\u5212\u5668\u9002\u914d\u5668\u548c\u7d27\u51d1\u5b88\u62a4\u6a21\u578b\uff0c\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1b3) Pre-Exec Bench\uff1a\u6db5\u76d6\u591a\u6837\u5316\u5de5\u5177\u548c\u5206\u652f\u8f68\u8ff9\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u62a4\u680f\u5728Pre-Exec Bench\u4e0a\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\uff0c\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u63d0\u70bc\u51fa\u53ef\u64cd\u4f5c\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u5b89\u5168\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6a21\u677f\uff0c\u901a\u8fc7\u9884\u6267\u884c\u5b89\u5168\u673a\u5236\u6709\u6548\u9884\u9632\u98ce\u9669\u3002"}}
{"id": "2510.09784", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09784", "abs": "https://arxiv.org/abs/2510.09784", "authors": ["Richard John", "Yunrui Qiu", "Lukas Herron", "Pratyush Tiwary"], "title": "Combined Representation and Generation with Diffusive State Predictive Information Bottleneck", "comment": null, "summary": "Generative modeling becomes increasingly data-intensive in high-dimensional\nspaces. In molecular science, where data collection is expensive and important\nevents are rare, compression to lower-dimensional manifolds is especially\nimportant for various downstream tasks, including generation. We combine a\ntime-lagged information bottleneck designed to characterize molecular important\nrepresentations and a diffusion model in one joint training objective. The\nresulting protocol, which we term Diffusive State Predictive Information\nBottleneck (D-SPIB), enables the balancing of representation learning and\ngeneration aims in one flexible architecture. Additionally, the model is\ncapable of combining temperature information from different molecular\nsimulation trajectories to learn a coherent and useful internal representation\nof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase\nits potential for exploring physical conditions outside the training set.", "AI": {"tldr": "\u63d0\u51faD-SPIB\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u5ef6\u8fdf\u4fe1\u606f\u74f6\u9888\u548c\u6269\u6563\u6a21\u578b\uff0c\u5728\u5206\u5b50\u79d1\u5b66\u4e2d\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\u76ee\u6807\uff0c\u80fd\u591f\u4ece\u4e0d\u540c\u6e29\u5ea6\u6a21\u62df\u8f68\u8ff9\u4e2d\u5b66\u4e60\u8fde\u8d2f\u7684\u70ed\u529b\u5b66\u8868\u793a\u3002", "motivation": "\u5206\u5b50\u79d1\u5b66\u4e2d\u6570\u636e\u6536\u96c6\u6602\u8d35\u4e14\u91cd\u8981\u4e8b\u4ef6\u7f55\u89c1\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u9700\u8981\u538b\u7f29\u5230\u4f4e\u7ef4\u6d41\u5f62\u4ee5\u652f\u6301\u751f\u6210\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u5ef6\u8fdf\u4fe1\u606f\u74f6\u9888\u548c\u6269\u6563\u6a21\u578b\u7684\u8054\u5408\u8bad\u7ec3\u76ee\u6807\uff0c\u6784\u5efaD-SPIB\u534f\u8bae\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u540c\u6e29\u5ea6\u6a21\u62df\u8f68\u8ff9\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u5206\u5b50\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u63a2\u7d22\u8bad\u7ec3\u96c6\u5916\u7269\u7406\u6761\u4ef6\u7684\u6f5c\u529b\u3002", "conclusion": "D-SPIB\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u67b6\u6784\uff0c\u80fd\u591f\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\u76ee\u6807\uff0c\u5e76\u5b66\u4e60\u8fde\u8d2f\u6709\u7528\u7684\u70ed\u529b\u5b66\u5185\u90e8\u8868\u793a\u3002"}}
{"id": "2510.09891", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09891", "abs": "https://arxiv.org/abs/2510.09891", "authors": ["Parsa Gooya", "Reinel Sospedra-Alfonso"], "title": "Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration", "comment": null, "summary": "Seasonal forecast of Arctic sea ice concentration is key to mitigate the\nnegative impact and assess potential opportunities posed by the rapid decline\nof sea ice coverage. Seasonal prediction systems based on climate models often\nshow systematic biases and complex spatio-temporal errors that grow with the\nforecasts. Consequently, operational predictions are routinely bias corrected\nand calibrated using retrospective forecasts. For predictions of Arctic sea ice\nconcentration, error corrections are mainly based on one-to-one post-processing\nmethods including climatological mean or linear regression correction and, more\nrecently, machine learning. Such deterministic adjustments are confined at best\nto the limited number of costly-to-run ensemble members of the raw forecast.\nHowever, decision-making requires proper quantification of uncertainty and\nlikelihood of events, particularly of extremes. We introduce a probabilistic\nerror correction framework based on a conditional Variational Autoencoder model\nto map the conditional distribution of observations given the biased model\nprediction. This method naturally allows for generating large ensembles of\nadjusted forecasts. We evaluate our model using deterministic and probabilistic\nmetrics and show that the adjusted forecasts are better calibrated, closer to\nthe observational distribution, and have smaller errors than climatological\nmean adjusted forecasts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6982\u7387\u8bef\u5dee\u6821\u6b63\u6846\u67b6\uff0c\u7528\u4e8e\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u7684\u5b63\u8282\u6027\u9884\u6d4b\uff0c\u80fd\u591f\u751f\u6210\u5927\u91cf\u8c03\u6574\u540e\u7684\u9884\u6d4b\u96c6\u5408\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u7684\u5b63\u8282\u6027\u9884\u6d4b\u5bf9\u7f13\u89e3\u6d77\u51b0\u5feb\u901f\u51cf\u5c11\u7684\u8d1f\u9762\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8e\u6c14\u5019\u6a21\u578b\u7684\u9884\u6d4b\u7cfb\u7edf\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u590d\u6742\u7684\u65f6\u7a7a\u8bef\u5dee\uff0c\u9700\u8981\u504f\u5dee\u6821\u6b63\u548c\u6821\u51c6\u3002\u51b3\u7b56\u5236\u5b9a\u9700\u8981\u9002\u5f53\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6781\u7aef\u4e8b\u4ef6\u6982\u7387\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6a21\u578b\u6784\u5efa\u6982\u7387\u8bef\u5dee\u6821\u6b63\u6846\u67b6\uff0c\u5c06\u89c2\u6d4b\u503c\u7684\u6761\u4ef6\u5206\u5e03\u6620\u5c04\u5230\u6709\u504f\u5dee\u7684\u6a21\u578b\u9884\u6d4b\u4e0a\uff0c\u81ea\u7136\u751f\u6210\u5927\u91cf\u8c03\u6574\u540e\u7684\u9884\u6d4b\u96c6\u5408\u3002", "result": "\u8c03\u6574\u540e\u7684\u9884\u6d4b\u5728\u6821\u51c6\u6027\u3001\u4e0e\u89c2\u6d4b\u5206\u5e03\u7684\u63a5\u8fd1\u7a0b\u5ea6\u548c\u8bef\u5dee\u5927\u5c0f\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e\u6c14\u5019\u5e73\u5747\u503c\u7684\u8c03\u6574\u9884\u6d4b\u3002", "conclusion": "\u8be5\u6982\u7387\u8bef\u5dee\u6821\u6b63\u6846\u67b6\u80fd\u591f\u6709\u6548\u6539\u5584\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u5b63\u8282\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6982\u7387\u4fe1\u606f\u3002"}}
{"id": "2510.10257", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10257", "abs": "https://arxiv.org/abs/2510.10257", "authors": ["Abdelrhman Elrawy", "Emad A. Mohammed"], "title": "Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its\nstandard adaptive density control (ADC) can lead to overfitting and bloated\nreconstructions. While state-of-the-art methods like FSGS improve quality, they\noften do so by significantly increasing the primitive count. This paper\npresents a framework that revises the core 3DGS optimization to prioritize\nefficiency. We replace the standard positional gradient heuristic with a novel\ndensification trigger that uses the opacity gradient as a lightweight proxy for\nrendering error. We find this aggressive densification is only effective when\npaired with a more conservative pruning schedule, which prevents destructive\noptimization cycles. Combined with a standard depth-correlation loss for\ngeometric guidance, our framework demonstrates a fundamental improvement in\nefficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k\nvs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a\nreduction of approximately 70%. This dramatic gain in compactness is achieved\nwith a modest trade-off in reconstruction metrics, establishing a new\nstate-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view\nsynthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u6548\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7684\u81f4\u5bc6\u5316\u89e6\u53d1\u673a\u5236\u548c\u4fdd\u5b88\u4fee\u526a\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u57fa\u5143\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5b58\u5728\u8fc7\u62df\u5408\u548c\u91cd\u5efa\u81a8\u80c0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u8d28\u91cf\u4f46\u663e\u8457\u589e\u52a0\u57fa\u5143\u6570\u91cf\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u7528\u4e0d\u900f\u660e\u5ea6\u68af\u5ea6\u4f5c\u4e3a\u6e32\u67d3\u8bef\u5dee\u7684\u8f7b\u91cf\u7ea7\u4ee3\u7406\u66ff\u4ee3\u4f4d\u7f6e\u68af\u5ea6\u542f\u53d1\u5f0f\uff0c\u7ed3\u5408\u4fdd\u5b88\u4fee\u526a\u7b56\u7565\u9632\u6b62\u7834\u574f\u6027\u4f18\u5316\u5faa\u73af\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u6df1\u5ea6\u76f8\u5173\u635f\u5931\u63d0\u4f9b\u51e0\u4f55\u6307\u5bfc\u3002", "result": "\u57283\u89c6\u56feLLFF\u6570\u636e\u96c6\u4e0a\u6bd4FSGS\u7d27\u51d140%\u4ee5\u4e0a\uff0832k vs 57k\u57fa\u5143\uff09\uff0c\u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u7ea670%\u57fa\u5143\uff0c\u5728\u8d28\u91cf-\u6548\u7387\u5e15\u7d2f\u6258\u524d\u6cbf\u8fbe\u5230\u65b0\u6700\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u6838\u5fc3\u4f18\u5316\u7b56\u7565\uff0c\u5728\u5c11\u6837\u672c\u89c6\u56fe\u5408\u6210\u4e2d\u5b9e\u73b0\u4e86\u57fa\u5143\u6570\u91cf\u7684\u5927\u5e45\u51cf\u5c11\uff0c\u4ec5\u4ee5\u9002\u5ea6\u7684\u91cd\u5efa\u6307\u6807\u635f\u5931\u4e3a\u4ee3\u4ef7\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u8d28\u91cf-\u6548\u7387\u5e73\u8861\u6807\u51c6\u3002"}}
{"id": "2510.09930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09930", "abs": "https://arxiv.org/abs/2510.09930", "authors": ["Ching Chang", "Ming-Chih Lo", "Chiao-Tung Chan", "Wen-Chih Peng", "Tien-Fu Chen"], "title": "MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation", "comment": "This paper is currently under review. The code will be made available\n  upon acceptance", "summary": "Web platforms, mobile applications, and connected sensing systems generate\nmultivariate time series with states at multiple levels of granularity, from\ncoarse regimes to fine-grained events. Effective segmentation in these settings\nrequires integrating across granularities while supporting iterative refinement\nthrough sparse prompt signals, which provide a compact mechanism for injecting\ndomain knowledge. Yet existing prompting approaches for time series\nsegmentation operate only within local contexts, so the effect of a prompt\nquickly fades and cannot guide predictions across the entire sequence. To\novercome this limitation, we propose MemPromptTSS, a framework for iterative\nmulti-granularity segmentation that introduces persistent prompt memory. A\nmemory encoder transforms prompts and their surrounding subsequences into\nmemory tokens stored in a bank. This persistent memory enables each new\nprediction to condition not only on local cues but also on all prompts\naccumulated across iterations, ensuring their influence persists across the\nentire sequence. Experiments on six datasets covering wearable sensing and\nindustrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy\nimprovements over the best baseline in single- and multi-granularity\nsegmentation under single iteration inference, and provides stronger refinement\nin iterative inference with average per-iteration gains of 2.66 percentage\npoints compared to 1.19 for PromptTSS. These results highlight the importance\nof persistent memory for prompt-guided segmentation, establishing MemPromptTSS\nas a practical and effective framework for real-world applications.", "AI": {"tldr": "MemPromptTSS\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u7c92\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6301\u4e45\u63d0\u793a\u8bb0\u5fc6\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u63d0\u793a\u5f71\u54cd\u5feb\u901f\u8870\u51cf\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u65b9\u6cd5\u4e2d\u7684\u63d0\u793a\u673a\u5236\u53ea\u80fd\u5728\u5c40\u90e8\u4e0a\u4e0b\u6587\u4e2d\u8d77\u4f5c\u7528\uff0c\u63d0\u793a\u7684\u5f71\u54cd\u4f1a\u5feb\u901f\u6d88\u5931\uff0c\u65e0\u6cd5\u5728\u6574\u4e2a\u5e8f\u5217\u4e2d\u6307\u5bfc\u9884\u6d4b\u3002", "method": "\u63d0\u51faMemPromptTSS\u6846\u67b6\uff0c\u4f7f\u7528\u8bb0\u5fc6\u7f16\u7801\u5668\u5c06\u63d0\u793a\u53ca\u5176\u5468\u56f4\u5b50\u5e8f\u5217\u8f6c\u6362\u4e3a\u8bb0\u5fc6\u4ee4\u724c\u5b58\u50a8\u5728\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u4f7f\u6bcf\u4e2a\u65b0\u9884\u6d4b\u90fd\u80fd\u57fa\u4e8e\u6240\u6709\u7d2f\u79ef\u7684\u63d0\u793a\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMemPromptTSS\u5728\u5355\u6b21\u8fed\u4ee3\u63a8\u7406\u4e2d\u6bd4\u6700\u4f73\u57fa\u7ebf\u5728\u5355\u7c92\u5ea6\u548c\u591a\u7c92\u5ea6\u5206\u5272\u4e0a\u5206\u522b\u63d0\u534723%\u548c85%\u7684\u51c6\u786e\u7387\uff0c\u5728\u8fed\u4ee3\u63a8\u7406\u4e2d\u5e73\u5747\u6bcf\u6b21\u8fed\u4ee3\u589e\u76ca\u4e3a2.66\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u6301\u4e45\u8bb0\u5fc6\u5bf9\u4e8e\u63d0\u793a\u5f15\u5bfc\u7684\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0cMemPromptTSS\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2510.11604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11604", "abs": "https://arxiv.org/abs/2510.11604", "authors": ["Sanjula De Alwis", "Indrajith Ekanayake"], "title": "Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce", "comment": null, "summary": "In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u53ef\u89e3\u91caAI\u3001\u751f\u5b58\u5206\u6790\u548cRFM\u5206\u6790\u7684\u4e09\u7ec4\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u5ba2\u6237\u6d41\u5931\u5206\u6790\uff0c\u4ece\u5355\u7eaf\u9884\u6d4b\u8f6c\u5411\u8bbe\u8ba1\u4e2a\u6027\u5316\u7559\u5b58\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u6d41\u5931\u6a21\u578b\u591a\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u5bf9\u6d41\u5931\u539f\u56e0\u3001\u5e72\u9884\u65f6\u673a\u548c\u9ad8\u98ce\u9669\u5ba2\u6237\u7fa4\u4f53\u7684\u6d1e\u5bdf\uff0c\u9700\u8981\u8f6c\u5411\u57fa\u4e8e\u53ef\u89e3\u91ca\u8bc1\u636e\u7684\u4e2a\u6027\u5316\u7559\u5b58\u7b56\u7565\u8bbe\u8ba1\u3002", "method": "\u96c6\u6210\u53ef\u89e3\u91caAI\u91cf\u5316\u7279\u5f81\u8d21\u732e\u3001\u751f\u5b58\u5206\u6790\u5efa\u6a21\u6d41\u5931\u65f6\u95f4\u98ce\u9669\u3001RFM\u5206\u6790\u6309\u4ea4\u6613\u884c\u4e3a\u7ec6\u5206\u5ba2\u6237\u7684\u4e09\u7ec4\u4ef6\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u6d41\u5931\u9a71\u52a8\u56e0\u7d20\u3001\u4f30\u8ba1\u5e72\u9884\u7a97\u53e3\u3001\u4f18\u5148\u8003\u8651\u76ee\u6807\u7ec6\u5206\u5e02\u573a\uff0c\u652f\u6301\u51cf\u5c11\u6d41\u5931\u548c\u589e\u5f3a\u5ba2\u6237\u5fe0\u8bda\u5ea6\u7684\u7b56\u7565\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u4e3a\u8bbe\u8ba1\u57fa\u4e8e\u8bc1\u636e\u7684\u4e2a\u6027\u5316\u5ba2\u6237\u7559\u5b58\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u5355\u7eaf\u9884\u6d4b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.10395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10395", "abs": "https://arxiv.org/abs/2510.10395", "authors": ["Xinlong Chen", "Yue Ding", "Weihong Lin", "Jingyun Hua", "Linli Yao", "Yang Shi", "Bozhou Li", "Yuanxing Zhang", "Qiang Liu", "Pengfei Wan", "Liang Wang", "Tieniu Tan"], "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration", "comment": "Project webpage: https://avocado-captioner.github.io/", "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.", "AI": {"tldr": "AVoCaDO\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u89c6\u542c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u5668\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7684\u65f6\u95f4\u7f16\u6392\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89c6\u542c\u89c6\u9891\u5b57\u5e55\u65e8\u5728\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u63cf\u8ff0\uff0c\u5b9e\u73b0\u89c6\u89c9\u548c\u542c\u89c9\u4e8b\u4ef6\u7684\u65f6\u95f4\u5bf9\u9f50\uff0c\u4ece\u800c\u6709\u76ca\u4e8e\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1aAVoCaDO SFT\u572810.7\u4e07\u9ad8\u8d28\u91cf\u65f6\u95f4\u5bf9\u9f50\u7684\u89c6\u542c\u5b57\u5e55\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\uff1bAVoCaDO GRPO\u5229\u7528\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5bf9\u8bdd\u51c6\u786e\u6027\uff0c\u540c\u65f6\u89c4\u8303\u5316\u5b57\u5e55\u957f\u5ea6\u548c\u51cf\u5c11\u5d29\u6e83\u3002", "result": "AVoCaDO\u5728\u56db\u4e2a\u89c6\u542c\u89c6\u9891\u5b57\u5e55\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u4ec5\u89c6\u89c9\u8bbe\u7f6e\u7684VDC\u548cDREAM-1K\u57fa\u51c6\u4e0a\u4e5f\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "AVoCaDO\u901a\u8fc7\u65f6\u95f4\u7f16\u6392\u9a71\u52a8\u7684\u89c6\u542c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.10658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10658", "abs": "https://arxiv.org/abs/2510.10658", "authors": ["Guy Mor-Lan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News", "comment": null, "summary": "While media bias is widely studied, the epistemic strategies behind factual\nreporting remain computationally underexplored. This paper analyzes these\nstrategies through a large-scale comparison of CNN and Fox News. To isolate\nreporting style from topic selection, we employ an article matching strategy to\ncompare reports on the same events and apply the FactAppeal framework to a\ncorpus of over 470K articles covering two highly politicized periods: the\nCOVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting\ncontains more factual statements and is more likely to ground them in external\nsources. The outlets also exhibit sharply divergent sourcing patterns: CNN\nbuilds credibility by citing Experts} and Expert Documents, constructing an\nappeal to formal authority, whereas Fox News favors News Reports and direct\nquotations. This work quantifies how partisan outlets use systematically\ndifferent epistemic strategies to construct reality, adding a new dimension to\nthe study of media bias.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u6bd4\u8f83CNN\u548c\u798f\u514b\u65af\u65b0\u95fb\uff0c\u5206\u6790\u4e86\u5a92\u4f53\u7684\u4e8b\u5b9e\u62a5\u9053\u7b56\u7565\uff0c\u53d1\u73b0CNN\u4f7f\u7528\u66f4\u591a\u4e8b\u5b9e\u9648\u8ff0\u5e76\u66f4\u4f9d\u8d56\u5916\u90e8\u6765\u6e90\uff0c\u800c\u798f\u514b\u65af\u65b0\u95fb\u504f\u597d\u65b0\u95fb\u62a5\u9053\u548c\u76f4\u63a5\u5f15\u7528\uff0c\u63ed\u793a\u4e86\u515a\u6d3e\u5a92\u4f53\u4f7f\u7528\u4e0d\u540c\u8ba4\u77e5\u7b56\u7565\u6784\u5efa\u73b0\u5b9e\u3002", "motivation": "\u867d\u7136\u5a92\u4f53\u504f\u89c1\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u4e8b\u5b9e\u62a5\u9053\u80cc\u540e\u7684\u8ba4\u77e5\u7b56\u7565\u5728\u8ba1\u7b97\u5c42\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u91cf\u5316\u5206\u6790\u4e0d\u540c\u5a92\u4f53\u5982\u4f55\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u8ba4\u77e5\u7b56\u7565\u6765\u6784\u5efa\u73b0\u5b9e\u3002", "method": "\u91c7\u7528\u6587\u7ae0\u5339\u914d\u7b56\u7565\u6bd4\u8f83\u540c\u4e00\u4e8b\u4ef6\u7684\u62a5\u9053\uff0c\u5e94\u7528FactAppeal\u6846\u67b6\u5206\u6790\u8d85\u8fc747\u4e07\u7bc7\u6587\u7ae0\uff0c\u6db5\u76d6COVID-19\u5927\u6d41\u884c\u548c\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u4e24\u4e2a\u9ad8\u5ea6\u653f\u6cbb\u5316\u65f6\u671f\u3002", "result": "CNN\u7684\u62a5\u9053\u5305\u542b\u66f4\u591a\u4e8b\u5b9e\u9648\u8ff0\u4e14\u66f4\u503e\u5411\u4e8e\u57fa\u4e8e\u5916\u90e8\u6765\u6e90\uff0cCNN\u901a\u8fc7\u5f15\u7528\u4e13\u5bb6\u548c\u4e13\u5bb6\u6587\u4ef6\u6784\u5efa\u6b63\u5f0f\u6743\u5a01\u7684\u5438\u5f15\u529b\uff0c\u800c\u798f\u514b\u65af\u65b0\u95fb\u504f\u597d\u65b0\u95fb\u62a5\u9053\u548c\u76f4\u63a5\u5f15\u7528\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u91cf\u5316\u4e86\u515a\u6d3e\u5a92\u4f53\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u4f7f\u7528\u4e0d\u540c\u7684\u8ba4\u77e5\u7b56\u7565\u6765\u6784\u5efa\u73b0\u5b9e\uff0c\u4e3a\u5a92\u4f53\u504f\u89c1\u7814\u7a76\u589e\u6dfb\u4e86\u65b0\u7684\u7ef4\u5ea6\u3002"}}
{"id": "2510.10140", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10140", "abs": "https://arxiv.org/abs/2510.10140", "authors": ["Yue Deng", "Francisco Santos", "Pang-Ning Tan", "Lifeng Luo"], "title": "Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction", "comment": null, "summary": "Deep learning based weather forecasting (DLWF) models leverage past weather\nobservations to generate future forecasts, supporting a wide range of\ndownstream tasks, including tropical cyclone (TC) trajectory prediction. In\nthis paper, we investigate their vulnerability to adversarial attacks, where\nsubtle perturbations to the upstream weather forecasts can alter the downstream\nTC trajectory predictions. Although research on adversarial attacks in DLWF\nmodels has grown recently, generating perturbed upstream forecasts that\nreliably steer downstream output toward attacker-specified trajectories remains\na challenge. First, conventional TC detection systems are opaque,\nnon-differentiable black boxes, making standard gradient-based attacks\ninfeasible. Second, the extreme rarity of TC events leads to severe class\nimbalance problem, making it difficult to develop efficient attack methods that\nwill produce the attacker's target trajectories. Furthermore, maintaining\nphysical consistency in adversarially generated forecasts presents another\nsignificant challenge. To overcome these limitations, we propose Cyc-Attack, a\nnovel method that perturbs the upstream forecasts of DLWF models to generate\nadversarial trajectories. First, we pre-train a differentiable surrogate model\nto approximate the TC detector's output, enabling the construction of\ngradient-based attacks. Cyc-Attack also employs skewness-aware loss function\nwith kernel dilation strategy to address the imbalance problem. Finally, a\ndistance-based gradient weighting scheme and regularization are used to\nconstrain the perturbations and eliminate spurious trajectories to ensure the\nadversarial forecasts are realistic and not easily detectable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Cyc-Attack\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684\u4e0a\u6e38\u9884\u62a5\u6765\u751f\u6210\u5bf9\u6297\u6027\u70ed\u5e26\u6c14\u65cb\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u653b\u51fb\u65b9\u6cd5\u5728\u975e\u53ef\u5faeTC\u68c0\u6d4b\u7cfb\u7edf\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u5fae\u6270\u4e0a\u6e38\u5929\u6c14\u9884\u6d4b\u6765\u6539\u53d8\u4e0b\u6e38\u70ed\u5e26\u6c14\u65cb\u8f68\u8ff9\u9884\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u53ef\u5faeTC\u68c0\u6d4b\u7cfb\u7edf\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faCyc-Attack\u65b9\u6cd5\uff1a\u9884\u8bad\u7ec3\u53ef\u5fae\u66ff\u4ee3\u6a21\u578b\u8fd1\u4f3cTC\u68c0\u6d4b\u5668\u8f93\u51fa\uff0c\u4f7f\u7528\u504f\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u6838\u81a8\u80c0\u7b56\u7565\u89e3\u51b3\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u68af\u5ea6\u52a0\u6743\u65b9\u6848\u548c\u6b63\u5219\u5316\u786e\u4fdd\u5bf9\u6297\u9884\u62a5\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "Cyc-Attack\u80fd\u591f\u6709\u6548\u751f\u6210\u5bf9\u6297\u6027\u8f68\u8ff9\uff0c\u6210\u529f\u6270\u52a8\u4e0a\u6e38\u5929\u6c14\u9884\u62a5\u4ee5\u4ea7\u751f\u653b\u51fb\u8005\u6307\u5b9a\u7684\u70ed\u5e26\u6c14\u65cb\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "conclusion": "Cyc-Attack\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u5728\u5929\u6c14\u9884\u62a5\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5bf9\u4e0a\u6e38\u6270\u52a8\u7684\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u5bf9\u6c14\u8c61\u5b89\u5168\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.10546", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10546", "abs": "https://arxiv.org/abs/2510.10546", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Sidra Sultana", "Ayesha Kanwal", "Nazia Perwaiz"], "title": "GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction", "comment": null, "summary": "Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high\nmountain regions, yet predictive research is hindered by fragmented and\nunimodal data. Most prior efforts emphasize post-event mapping, whereas\nforecasting requires harmonized datasets that combine visual indicators with\nphysical precursors. We present GLOFNet, a multimodal dataset for GLOF\nmonitoring and prediction, focused on the Shisper Glacier in the Karakoram. It\nintegrates three complementary sources: Sentinel-2 multispectral imagery for\nspatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and\nMODIS Land Surface Temperature records spanning over two decades. Preprocessing\nincluded cloud masking, quality filtering, normalization, temporal\ninterpolation, augmentation, and cyclical encoding, followed by harmonization\nacross modalities. Exploratory analysis reveals seasonal glacier velocity\ncycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in\ncryospheric conditions. The resulting dataset, GLOFNet, is publicly available\nto support future research in glacial hazard prediction. By addressing\nchallenges such as class imbalance, cloud contamination, and coarse resolution,\nGLOFNet provides a structured foundation for benchmarking multimodal deep\nlearning approaches to rare hazard prediction.", "AI": {"tldr": "GLOFNet\u662f\u4e00\u4e2a\u7528\u4e8e\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34\u76d1\u6d4b\u548c\u9884\u6d4b\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86Sentinel-2\u591a\u5149\u8c31\u5f71\u50cf\u3001NASA\u51b0\u5ddd\u901f\u5ea6\u6570\u636e\u548cMODIS\u5730\u8868\u6e29\u5ea6\u6570\u636e\uff0c\u4e13\u6ce8\u4e8e\u5580\u5587\u6606\u4ed1\u5c71\u8109\u7684Shisper\u51b0\u5ddd\u3002", "motivation": "\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34\u662f\u9ad8\u5c71\u5730\u533a\u7f55\u89c1\u4f46\u7834\u574f\u6027\u5f3a\u7684\u707e\u5bb3\uff0c\u73b0\u6709\u7814\u7a76\u53d7\u9650\u4e8e\u788e\u7247\u5316\u548c\u5355\u6a21\u6001\u6570\u636e\uff0c\u7f3a\u4e4f\u9884\u6d4b\u80fd\u529b\u3002\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u6307\u6807\u548c\u7269\u7406\u524d\u5146\u7684\u534f\u8c03\u6570\u636e\u96c6\u3002", "method": "\u6574\u5408\u4e09\u79cd\u4e92\u8865\u6570\u636e\u6e90\uff1aSentinel-2\u591a\u5149\u8c31\u5f71\u50cf\u7528\u4e8e\u7a7a\u95f4\u76d1\u6d4b\uff0cNASA ITS_LIVE\u901f\u5ea6\u4ea7\u54c1\u7528\u4e8e\u51b0\u5ddd\u8fd0\u52a8\u5b66\uff0cMODIS\u5730\u8868\u6e29\u5ea6\u8bb0\u5f55\u8986\u76d6\u4e8c\u5341\u591a\u5e74\u3002\u9884\u5904\u7406\u5305\u62ec\u4e91\u63a9\u819c\u3001\u8d28\u91cf\u8fc7\u6ee4\u3001\u5f52\u4e00\u5316\u3001\u65f6\u95f4\u63d2\u503c\u3001\u6570\u636e\u589e\u5f3a\u548c\u5468\u671f\u6027\u7f16\u7801\uff0c\u7136\u540e\u8fdb\u884c\u591a\u6a21\u6001\u534f\u8c03\u3002", "result": "\u63a2\u7d22\u6027\u5206\u6790\u63ed\u793a\u4e86\u51b0\u5ddd\u901f\u5ea6\u7684\u5b63\u8282\u6027\u5468\u671f\u3001\u6bcf\u5341\u5e74\u7ea60.8K\u7684\u957f\u671f\u5347\u6e29\u8d8b\u52bf\uff0c\u4ee5\u53ca\u51b0\u51bb\u5708\u6761\u4ef6\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u3002GLOFNet\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "GLOFNet\u901a\u8fc7\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4e91\u6c61\u67d3\u548c\u7c97\u5206\u8fa8\u7387\u7b49\u6311\u6218\uff0c\u4e3a\u7f55\u89c1\u707e\u5bb3\u9884\u6d4b\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u51c6\u57fa\u7840\u3002"}}
{"id": "2510.10150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10150", "abs": "https://arxiv.org/abs/2510.10150", "authors": ["Zhezheng Hao", "Hong Wang", "Haoyang Liu", "Jian Luo", "Jiarui Yu", "Hande Dong", "Qiang Lin", "Can Wang", "Jiawei Chen"], "title": "Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective", "comment": null, "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM\nreasoning, its training process poses a critical risk: entropy collapse. This\nphenomenon is a rapid loss of policy diversity, stemming from the\nexploration-exploitation imbalance and leading to a lack of generalization.\nRecent entropy-intervention methods aim to prevent \\coloredtext{entropy\ncollapse}, yet their underlying mechanisms remain unclear. In this paper, we\nconduct a quantitative analysis to reveal token-level entropy changes and how\nexisting entropy intervention methods help avoid entropy collapse. Our findings\npoint out a fundamental limitation of existing methods: they attempt to control\nentropy dynamics indirectly. By only affecting related factors, such as the\nadvantage signal and generation probability, their effectiveness is inherently\nlimited and could potentially fail. To address this limitation, we introduce an\nentropy-change-aware reweighting scheme, namely Stabilizing Token-level\nEntropy-changE via Reweighting (STEER), that adaptively stabilizes entropy\ndynamics through fine-grained token-level adjustments. Our approach mitigates\nover-exploitation while fostering robust exploration. Extensive experiments\ndemonstrate that STEER significantly mitigates entropy collapse, stabilizes\nentropy dynamics, and achieves stronger downstream performance across various\nmathematical reasoning benchmarks \\footnote{Our code is available at\nhttps://github.com/zz-haooo/STEER.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1(RLVR)\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u71b5\u53d8\u5316\u611f\u77e5\u91cd\u52a0\u6743\u65b9\u6cd5STEER\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684token\u7ea7\u8c03\u6574\u6765\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u6709\u6548\u7f13\u89e3\u71b5\u5d29\u6e83\u5e76\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "RLVR\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b58\u5728\u71b5\u5d29\u6e83\u98ce\u9669\uff0c\u5373\u7b56\u7565\u591a\u6837\u6027\u5feb\u901f\u4e27\u5931\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u71b5\u5e72\u9884\u65b9\u6cd5\u673a\u5236\u4e0d\u660e\u786e\u4e14\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u6709\u6548\u7684\u71b5\u52a8\u6001\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTEER\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u53d8\u5316\u611f\u77e5\u7684\u91cd\u52a0\u6743\u65b9\u6848\uff0c\u5728token\u7ea7\u522b\u81ea\u9002\u5e94\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u7f13\u89e3\u8fc7\u5ea6\u5229\u7528\u540c\u65f6\u4fc3\u8fdb\u7a33\u5065\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTEER\u663e\u8457\u7f13\u89e3\u4e86\u71b5\u5d29\u6e83\uff0c\u7a33\u5b9a\u4e86\u71b5\u52a8\u6001\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u5f3a\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "STEER\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u71b5\u52a8\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aRLVR\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u6709\u6548\u7684\u71b5\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2510.10577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10577", "abs": "https://arxiv.org/abs/2510.10577", "authors": ["Haonan Wang", "Hanyu Zhou", "Haoyue Liu", "Luxin Yan"], "title": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes", "comment": null, "summary": "Optical flow estimation has achieved promising results in conventional scenes\nbut faces challenges in high-speed and low-light scenes, which suffer from\nmotion blur and insufficient illumination. These conditions lead to weakened\ntexture and amplified noise and deteriorate the appearance saturation and\nboundary completeness of frame cameras, which are necessary for motion feature\nmatching. In degraded scenes, the frame camera provides dense appearance\nsaturation but sparse boundary completeness due to its long imaging time and\nlow dynamic range. In contrast, the event camera offers sparse appearance\nsaturation, while its short imaging time and high dynamic range gives rise to\ndense boundary completeness. Traditionally, existing methods utilize feature\nfusion or domain adaptation to introduce event to improve boundary\ncompleteness. However, the appearance features are still deteriorated, which\nseverely affects the mostly adopted discriminative models that learn the\nmapping from visual features to motion fields and generative models that\ngenerate motion fields based on given visual features. So we introduce\ndiffusion models that learn the mapping from noising flow to clear flow, which\nis not affected by the deteriorated visual features. Therefore, we propose a\nnovel optical flow estimation framework Diff-ABFlow based on diffusion models\nwith frame-event appearance-boundary fusion.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548c\u5e27-\u4e8b\u4ef6\u5916\u89c2-\u8fb9\u754c\u878d\u5408\u7684\u5149\u6d41\u4f30\u8ba1\u6846\u67b6Diff-ABFlow\uff0c\u89e3\u51b3\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5149\u6d41\u4f30\u8ba1\u5728\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e2d\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u7eb9\u7406\u51cf\u5f31\u3001\u566a\u58f0\u653e\u5927\u3001\u5916\u89c2\u9971\u548c\u5ea6\u548c\u8fb9\u754c\u5b8c\u6574\u6027\u6076\u5316\u3002\u5e27\u76f8\u673a\u63d0\u4f9b\u5bc6\u96c6\u5916\u89c2\u4f46\u8fb9\u754c\u7a00\u758f\uff0c\u4e8b\u4ef6\u76f8\u673a\u63d0\u4f9b\u7a00\u758f\u5916\u89c2\u4f46\u8fb9\u754c\u5bc6\u96c6\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u878d\u5408\u6216\u57df\u9002\u5e94\u5f15\u5165\u4e8b\u4ef6\u6570\u636e\u6539\u5584\u8fb9\u754c\u5b8c\u6574\u6027\uff0c\u4f46\u5916\u89c2\u7279\u5f81\u4ecd\u7136\u6076\u5316\uff0c\u5f71\u54cd\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ece\u566a\u58f0\u6d41\u5230\u6e05\u6670\u6d41\u7684\u6620\u5c04\uff0c\u4e0d\u53d7\u6076\u5316\u89c6\u89c9\u7279\u5f81\u5f71\u54cd\u3002\u63d0\u51faDiff-ABFlow\u6846\u67b6\uff0c\u7ed3\u5408\u5e27-\u4e8b\u4ef6\u5916\u89c2-\u8fb9\u754c\u878d\u5408\uff0c\u5229\u7528\u5e27\u76f8\u673a\u7684\u5916\u89c2\u9971\u548c\u5ea6\u548c\u4e8b\u4ef6\u76f8\u673a\u7684\u8fb9\u754c\u5b8c\u6574\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6269\u6563\u6a21\u578b\u907f\u514d\u4e86\u6076\u5316\u89c6\u89c9\u7279\u5f81\u5bf9\u5149\u6d41\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u5229\u7528\u5e27-\u4e8b\u4ef6\u878d\u5408\u540c\u65f6\u83b7\u5f97\u826f\u597d\u7684\u5916\u89c2\u548c\u8fb9\u754c\u4fe1\u606f\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e27-\u4e8b\u4ef6\u5916\u89c2-\u8fb9\u754c\u878d\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u7684\u5149\u6d41\u4f30\u8ba1\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u6076\u5316\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.10232", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10232", "abs": "https://arxiv.org/abs/2510.10232", "authors": ["Xuening Wu", "Shenqin Yin", "Yanlan Kang", "Xinhang Zhang", "Qianya Xu", "Zeping Chen", "Wenqiang Zhang"], "title": "SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification", "comment": null, "summary": "Recursive self-modification is increasingly central in AutoML, neural\narchitecture search, and adaptive optimization, yet no existing framework\nensures that such changes are made safely. Godel machines offer a principled\nsafeguard by requiring formal proofs of improvement before rewriting code;\nhowever, such proofs are unattainable in stochastic, high-dimensional settings.\nWe introduce the Statistical Godel Machine (SGM), the first statistical safety\nlayer for recursive edits. SGM replaces proof-based requirements with\nstatistical confidence tests (e-values, Hoeffding bounds), admitting a\nmodification only when superiority is certified at a chosen confidence level,\nwhile allocating a global error budget to bound cumulative risk across\nrounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which\nindexes spending by confirmation events rather than rounds, concentrating the\nerror budget on promising edits while preserving familywise\nvalidity.Experiments across supervised learning, reinforcement learning, and\nblack-box optimization validate this role: SGM certifies genuine gains on\nCIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates\nrobustness on RL and optimization benchmarks.Together, these results position\nSGM as foundational infrastructure for continual, risk-aware self-modification\nin learning systems.Code is available at:\nhttps://github.com/gravitywavelet/sgm-anon.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u8ba1\u54e5\u5fb7\u5c14\u673a(SGM)\uff0c\u9996\u4e2a\u7528\u4e8e\u9012\u5f52\u7f16\u8f91\u7684\u7edf\u8ba1\u5b89\u5168\u5c42\uff0c\u7528\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5\u66ff\u4ee3\u8bc1\u660e\u8981\u6c42\uff0c\u5728\u9009\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u8ba4\u8bc1\u6539\u8fdb\u540e\u624d\u5141\u8bb8\u4fee\u6539\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u9519\u8bef\u9884\u7b97\u63a7\u5236\u7d2f\u79ef\u98ce\u9669\u3002", "motivation": "\u9012\u5f52\u81ea\u4fee\u6539\u5728AutoML\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u81ea\u9002\u5e94\u4f18\u5316\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u786e\u4fdd\u6b64\u7c7b\u66f4\u6539\u7684\u5b89\u5168\u6027\u3002\u54e5\u5fb7\u5c14\u673a\u867d\u7136\u63d0\u4f9b\u539f\u5219\u6027\u4fdd\u969c\uff0c\u4f46\u5728\u968f\u673a\u9ad8\u7ef4\u73af\u5883\u4e2d\u65e0\u6cd5\u83b7\u5f97\u5f62\u5f0f\u5316\u8bc1\u660e\u3002", "method": "SGM\u4f7f\u7528\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5(e\u503c\u3001Hoeffding\u754c)\u66ff\u4ee3\u8bc1\u660e\u8981\u6c42\uff0c\u4ec5\u5728\u6539\u8fdb\u4ee5\u9009\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u88ab\u8ba4\u8bc1\u65f6\u624d\u5141\u8bb8\u4fee\u6539\uff0c\u540c\u65f6\u5206\u914d\u5168\u5c40\u9519\u8bef\u9884\u7b97\u6765\u9650\u5236\u591a\u8f6e\u7d2f\u79ef\u98ce\u9669\u3002\u63d0\u51fa\u786e\u8ba4\u89e6\u53d1\u8c03\u548c\u652f\u51fa(CTHS)\u65b9\u6cd5\uff0c\u6309\u786e\u8ba4\u4e8b\u4ef6\u800c\u975e\u8f6e\u6b21\u7d22\u5f15\u652f\u51fa\uff0c\u5c06\u9519\u8bef\u9884\u7b97\u96c6\u4e2d\u5728\u6709\u5e0c\u671b\u7684\u7f16\u8f91\u4e0a\u3002", "result": "\u5728\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u9ed1\u76d2\u4f18\u5316\u7684\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86SGM\u7684\u4f5c\u7528\uff1a\u5728CIFAR-100\u4e0a\u8ba4\u8bc1\u771f\u5b9e\u589e\u76ca\uff0c\u5728ImageNet-100\u4e0a\u62d2\u7edd\u865a\u5047\u6539\u8fdb\uff0c\u5728RL\u548c\u4f18\u5316\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "SGM\u4e3a\u5b66\u4e60\u7cfb\u7edf\u4e2d\u6301\u7eed\u3001\u98ce\u9669\u611f\u77e5\u7684\u81ea\u4fee\u6539\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.10653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10653", "abs": "https://arxiv.org/abs/2510.10653", "authors": ["Sebastian Schmidt", "Julius K\u00f6rner", "Stephan G\u00fcnnemann"], "title": "A Machine Learning Perspective on Automated Driving Corner Cases", "comment": null, "summary": "For high-stakes applications, like autonomous driving, a safe operation is\nnecessary to prevent harm, accidents, and failures. Traditionally, difficult\nscenarios have been categorized into corner cases and addressed individually.\nHowever, this example-based categorization is not scalable and lacks a data\ncoverage perspective, neglecting the generalization to training data of machine\nlearning models. In our work, we propose a novel machine learning approach that\ntakes the underlying data distribution into account. Based on our novel\nperspective, we present a framework for effective corner case recognition for\nperception on individual samples. In our evaluation, we show that our approach\n(i) unifies existing scenario-based corner case taxonomies under a\ndistributional perspective, (ii) achieves strong performance on corner case\ndetection tasks across standard benchmarks for which we extend established\nout-of-distribution detection benchmarks, and (iii) enables analysis of\ncombined corner cases via a newly introduced fog-augmented Lost & Found\ndataset. These results provide a principled basis for corner case recognition,\nunderlining our manual specification-free definition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u5206\u5e03\u89c6\u89d2\u7684\u89d2\u70b9\u6848\u4f8b\u8bc6\u522b\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u57fa\u4e8e\u573a\u666f\u7684\u89d2\u70b9\u6848\u4f8b\u5206\u7c7b\u6cd5\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u652f\u6301\u7ec4\u5408\u89d2\u70b9\u6848\u4f8b\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u793a\u4f8b\u7684\u89d2\u70b9\u6848\u4f8b\u5206\u7c7b\u65b9\u6cd5\u4e0d\u53ef\u6269\u5c55\u4e14\u7f3a\u4e4f\u6570\u636e\u8986\u76d6\u89c6\u89d2\uff0c\u65e0\u6cd5\u8003\u8651\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u8003\u8651\u5e95\u5c42\u6570\u636e\u5206\u5e03\u7684\u65b0\u65b9\u6cd5\u6765\u5b9e\u73b0\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8003\u8651\u5e95\u5c42\u6570\u636e\u5206\u5e03\uff0c\u5f00\u53d1\u4e86\u7528\u4e8e\u4e2a\u4f53\u6837\u672c\u611f\u77e5\u7684\u6709\u6548\u89d2\u70b9\u6848\u4f8b\u8bc6\u522b\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5(i)\u7edf\u4e00\u4e86\u73b0\u6709\u57fa\u4e8e\u573a\u666f\u7684\u89d2\u70b9\u6848\u4f8b\u5206\u7c7b\u6cd5\uff0c(ii)\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5f3a\u5927\u7684\u89d2\u70b9\u6848\u4f8b\u68c0\u6d4b\u6027\u80fd\uff0c(iii)\u901a\u8fc7\u65b0\u5f15\u5165\u7684\u96fe\u589e\u5f3aLost & Found\u6570\u636e\u96c6\u652f\u6301\u7ec4\u5408\u89d2\u70b9\u6848\u4f8b\u5206\u6790\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u89d2\u70b9\u6848\u4f8b\u8bc6\u522b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u65e0\u9700\u624b\u52a8\u89c4\u8303\u7684\u5b9a\u4e49\u65b9\u6cd5\u3002"}}
{"id": "2510.10750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10750", "abs": "https://arxiv.org/abs/2510.10750", "authors": ["Laura Weihl", "Nejc Novak", "Stefan H. Bengtson", "Malte Pedersen"], "title": "Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection", "comment": null, "summary": "Underwater video monitoring is a promising strategy for assessing marine\nbiodiversity, but the vast volume of uneventful footage makes manual inspection\nhighly impractical. In this work, we explore the use of visual anomaly\ndetection (VAD) based on deep neural networks to automatically identify\ninteresting or anomalous events. We introduce AURA, the first multi-annotator\nbenchmark dataset for underwater VAD, and evaluate four VAD models across two\nmarine scenes. We demonstrate the importance of robust frame selection\nstrategies to extract meaningful video segments. Our comparison against\nmultiple annotators reveals that VAD performance of current models varies\ndramatically and is highly sensitive to both the amount of training data and\nthe variability in visual content that defines \"normal\" scenes. Our results\nhighlight the value of soft and consensus labels and offer a practical approach\nfor supporting scientific exploration and scalable biodiversity monitoring.", "AI": {"tldr": "\u63d0\u51faAURA\u9996\u4e2a\u591a\u6807\u6ce8\u8005\u6c34\u4e0b\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cdVAD\u6a21\u578b\u5728\u4e24\u79cd\u6d77\u6d0b\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u6b63\u5e38\u573a\u666f\u5b9a\u4e49\u9ad8\u5ea6\u654f\u611f\u3002", "motivation": "\u6c34\u4e0b\u89c6\u9891\u76d1\u6d4b\u662f\u8bc4\u4f30\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u4f46\u6d77\u91cf\u65e0\u4e8b\u4ef6\u89c6\u9891\u4f7f\u5f97\u4eba\u5de5\u68c0\u67e5\u6781\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u81ea\u52a8\u8bc6\u522b\u6709\u8da3\u6216\u5f02\u5e38\u4e8b\u4ef6\u3002", "method": "\u5f15\u5165AURA\u591a\u6807\u6ce8\u8005\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u91c7\u7528\u9c81\u68d2\u7684\u5e27\u9009\u62e9\u7b56\u7565\u63d0\u53d6\u6709\u610f\u4e49\u7684\u89c6\u9891\u7247\u6bb5\u3002", "result": "\u5f53\u524dVAD\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u6b63\u5e38\u573a\u666f\u7684\u89c6\u89c9\u5185\u5bb9\u53d8\u5f02\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u8f6f\u6807\u7b7e\u548c\u5171\u8bc6\u6807\u7b7e\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u652f\u6301\u79d1\u5b66\u63a2\u7d22\u548c\u53ef\u6269\u5c55\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u591a\u6807\u6ce8\u8005\u57fa\u51c6\u548c\u9c81\u68d2\u5e27\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.11277", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11277", "abs": "https://arxiv.org/abs/2510.11277", "authors": ["Guangyu Wei", "Ke Han", "Yueming Lyu", "Yu Luo", "Yue Jiang", "Caifeng Shan", "Nicu Sebe"], "title": "Towards Real-Time Fake News Detection under Evidence Scarcity", "comment": null, "summary": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE.", "AI": {"tldr": "EASE\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u6765\u81ea\u9002\u5e94\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5305\u542b\u8bc1\u636e\u8bc4\u4f30\u3001\u63a8\u7406\u8bc4\u4f30\u548c\u60c5\u611f\u56de\u9000\u4e09\u4e2a\u89c6\u89d2\uff0c\u5728\u8bc1\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u8bc1\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u8bc1\u636e\uff0c\u5728\u8bc1\u636e\u4e0d\u8db3\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faEASE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u72ec\u7acb\u8bc4\u4f30\u89c6\u89d2\uff1a\u8bc1\u636e\u8bc4\u4f30\u3001\u63a8\u7406\u8bc4\u4f30\u548c\u60c5\u611f\u56de\u9000\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u548c\u4f2a\u6807\u7b7e\u589e\u5f3a\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u8bc4\u4f30\u611f\u77e5\u7684\u51b3\u7b56\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u5b9e\u65f6\u65b0\u95fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86RealTimeNews-25\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "EASE\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u548c\u591a\u89c6\u89d2\u51b3\u7b56\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u65f6\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u8bc1\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2510.11328", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11328", "abs": "https://arxiv.org/abs/2510.11328", "authors": ["Chenxi Wang", "Yixuan Zhang", "Ruiji Yu", "Yufei Zheng", "Lang Gao", "Zirui Song", "Zixiang Xu", "Gus Xia", "Huishuai Zhang", "Dongyan Zhao", "Xiuying Chen"], "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control", "comment": "19 pages, 8 figures, 8 tables. Code and dataset available at\n  https://github.com/Aurora-cx/EmotionCircuits-LLM", "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u63ed\u793a\u5e76\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u60c5\u611f\u7535\u8def\uff0c\u901a\u8fc7\u8bc6\u522b\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5934\u6784\u5efa\u5168\u5c40\u60c5\u611f\u7535\u8def\uff0c\u5b9e\u73b0\u4e8699.65%\u7684\u60c5\u611f\u8868\u8fbe\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u60c5\u611f\u667a\u80fd\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u7406\u89e3\u60c5\u611f\u8868\u8fbe\u7684\u5185\u90e8\u673a\u5236\u5e76\u63a7\u5236\u751f\u6210\u6587\u672c\u4e2d\u7684\u60c5\u611f\u3002", "method": "\u6784\u5efaSEV\u6570\u636e\u96c6\u63d0\u53d6\u4e0a\u4e0b\u6587\u65e0\u5173\u60c5\u611f\u65b9\u5411\uff0c\u901a\u8fc7\u5206\u6790\u5206\u89e3\u548c\u56e0\u679c\u5206\u6790\u8bc6\u522b\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5934\uff0c\u6574\u5408\u4e3a\u5168\u5c40\u60c5\u611f\u7535\u8def\uff0c\u5e76\u8fdb\u884c\u8c03\u5236\u5e72\u9884\u3002", "result": "\u8bc6\u522b\u51fa\u60c5\u611f\u8ba1\u7b97\u7684\u5177\u4f53\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5934\uff0c\u6784\u5efa\u4e86\u9a71\u52a8\u60c5\u611f\u8868\u8fbe\u7684\u5168\u5c40\u7535\u8def\uff0c\u76f4\u63a5\u8c03\u5236\u8fd9\u4e9b\u7535\u8def\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523099.65%\u7684\u60c5\u611f\u8868\u8fbe\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u63ed\u793a\u548c\u9a8c\u8bc1LLM\u4e2d\u60c5\u611f\u7535\u8def\u7684\u7814\u7a76\uff0c\u4e3a\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u60c5\u611f\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.11370", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11370", "abs": "https://arxiv.org/abs/2510.11370", "authors": ["Wenhan Ma", "Hailin Zhang", "Liang Zhao", "Yifan Song", "Yudong Wang", "Zhifang Sui", "Fuli Luo"], "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.", "AI": {"tldr": "\u63d0\u51fa\u4e86Rollout Routing Replay (R3)\u65b9\u6cd5\u6765\u89e3\u51b3MoE\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u8def\u7531\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8bb0\u5f55\u63a8\u7406\u9636\u6bb5\u7684\u8def\u7531\u5206\u5e03\u5e76\u5728\u8bad\u7ec3\u65f6\u91cd\u653e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565\u5dee\u5f02\u3002", "motivation": "MoE\u6a21\u578b\u7684\u8def\u7531\u673a\u5236\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u751a\u81f3\u707e\u96be\u6027\u8bad\u7ec3\u5d29\u6e83\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u8def\u7531\u884c\u4e3a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4ee5\u53ca\u76f8\u540c\u6761\u4ef6\u4e0b\u591a\u6b21\u524d\u5411\u4f20\u64ad\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u4e13\u5bb6\u9009\u62e9\u3002", "method": "\u63d0\u51faR3\u65b9\u6cd5\uff1a\u8bb0\u5f55\u63a8\u7406\u5f15\u64ce\u4e2d\u7684\u8def\u7531\u5206\u5e03\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u91cd\u653e\u8fd9\u4e9b\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565KL\u6563\u5ea6\uff0c\u7f13\u89e3\u6781\u7aef\u5dee\u5f02\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\uff0cR3\u6210\u529f\u7a33\u5b9a\u4e86RL\u8bad\u7ec3\uff0c\u9632\u6b62\u4e86\u5d29\u6e83\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GSPO\u548cTIS\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7a33\u5b9aMoE\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11005", "abs": "https://arxiv.org/abs/2510.11005", "authors": ["Kai Han", "Siqi Ma", "Chengxuan Qian", "Jun Chen", "Chongwen Lyu", "Yuqing Song", "Zhe Liu"], "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation", "comment": null, "summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images\nis essential for surgical planning and tumor staging. Although foundation\nmodels generally perform well in segmentation tasks, they often struggle to\nfocus on foreground areas in complex, low-contrast backgrounds, where some\nmalignant tumors closely resemble normal organs, complicating contextual\ndifferentiation. To address these challenges, we propose the Foreground-Aware\nSpectrum Segmentation (FASS) framework. First, we introduce a foreground-aware\nmodule to amplify the distinction between background and the entire volume\nspace, allowing the model to concentrate more effectively on target areas.\nNext, a feature-level frequency enhancement module, based on wavelet transform,\nextracts discriminative high-frequency features to enhance boundary recognition\nand detail perception. Eventually, we introduce an edge constraint module to\npreserve geometric continuity in segmentation boundaries. Extensive experiments\non multiple medical datasets demonstrate superior performance across all\nmetrics, validating the effectiveness of our framework, particularly in\nrobustness under complex conditions and fine structure recognition. Our\nframework significantly enhances segmentation of low-contrast images, paving\nthe way for applications in more diverse and complex medical imaging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86FASS\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u666f\u611f\u77e5\u6a21\u5757\u3001\u5c0f\u6ce2\u53d8\u6362\u9891\u7387\u589e\u5f3a\u6a21\u5757\u548c\u8fb9\u7f18\u7ea6\u675f\u6a21\u5757\uff0c\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u4f4e\u5bf9\u6bd4\u5ea6\u80bf\u7624\u5206\u5272\u7684\u6311\u6218\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u4f4e\u5bf9\u6bd4\u5ea6\u80cc\u666f\u4e0b\u96be\u4ee5\u805a\u7126\u524d\u666f\u533a\u57df\uff0c\u7279\u522b\u662f\u5f53\u6076\u6027\u80bf\u7624\u4e0e\u6b63\u5e38\u5668\u5b98\u76f8\u4f3c\u65f6\uff0c\u4e0a\u4e0b\u6587\u533a\u5206\u53d8\u5f97\u56f0\u96be\u3002", "method": "1. \u524d\u666f\u611f\u77e5\u6a21\u5757\u653e\u5927\u80cc\u666f\u4e0e\u6574\u4e2a\u4f53\u79ef\u7a7a\u95f4\u7684\u533a\u5206\uff1b2. \u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u7279\u5f81\u7ea7\u9891\u7387\u589e\u5f3a\u6a21\u5757\u63d0\u53d6\u5224\u522b\u6027\u9ad8\u9891\u7279\u5f81\uff1b3. \u8fb9\u7f18\u7ea6\u675f\u6a21\u5757\u4fdd\u6301\u5206\u5272\u8fb9\u754c\u7684\u51e0\u4f55\u8fde\u7eed\u6027\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u6709\u6307\u6807\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u7ec6\u7ed3\u6784\u8bc6\u522b\u65b9\u9762\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5bf9\u6bd4\u5ea6\u56fe\u50cf\u7684\u5206\u5272\u6548\u679c\uff0c\u4e3a\u66f4\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u533b\u5b66\u6210\u50cf\u573a\u666f\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.11063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11063", "abs": "https://arxiv.org/abs/2510.11063", "authors": ["Chang Liu", "Henghui Ding", "Kaining Ying", "Lingyi Hong", "Ning Xu", "Linjie Yang", "Yuchen Fan", "Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han", "Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Chang Soo Lim", "Joonyoung Moon", "Donghyeon Cho", "Tingmin Li", "Yixuan Li", "Yang Yang", "An Yan", "Leilei Cao", "Feng Lu", "Ran Hong", "Youhai Jiang", "Fengjie Zhu", "Yujie Xie", "Hongyang Zhang", "Zhihui Liu", "Shihai Ruan", "Quanzhu Niu", "Dengxian Gong", "Shihao Chen", "Tao Zhang", "Yikang Zhou", "Haobo Yuan", "Lu Qi", "Xiangtai Li", "Shunping Ji", "Ran Hong", "Feng Lu", "Leilei Cao", "An Yan", "Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation", "comment": "16 pages, 9 figures", "summary": "This report presents an overview of the 7th Large-scale Video Object\nSegmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the\ntwo traditional tracks of LSVOS that jointly target robustness in realistic\nvideo scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition\nfeatures a newly introduced track, Complex VOS (MOSEv2). Building upon prior\ninsights, MOSEv2 substantially increases difficulty, introducing more\nchallenging but realistic scenarios including denser small objects, frequent\ndisappear/reappear events, severe occlusions, adverse weather and lighting,\netc., pushing long-term consistency and generalization beyond curated\nbenchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for\nVOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric\nto better evaluate objects across scales and disappearance cases. We summarize\ndatasets and protocols, highlight top-performing solutions, and distill\nemerging trends, such as the growing role of LLM/MLLM components and\nmemory-aware propagation, aiming to chart future directions for resilient,\nlanguage-aware video segmentation in the wild.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ICCV 2025\u7b2c7\u5c4a\u5927\u89c4\u6a21\u89c6\u9891\u76ee\u6807\u5206\u5272\u6311\u6218\u8d5b\uff0c\u5305\u542b\u4f20\u7edfVOS\u548cRVOS\u8d5b\u9053\uff0c\u4ee5\u53ca\u65b0\u589e\u7684\u590d\u6742VOS\u8d5b\u9053MOSEv2\uff0c\u540e\u8005\u5f15\u5165\u4e86\u66f4\u73b0\u5b9e\u7684\u6311\u6218\u573a\u666f\u3002", "motivation": "\u63a8\u52a8\u89c6\u9891\u76ee\u6807\u5206\u5272\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u589e\u52a0\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u6765\u6d4b\u8bd5\u957f\u671f\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4fdd\u7559\u4f20\u7edfVOS\u548cRVOS\u8d5b\u9053\uff0c\u65b0\u589eMOSEv2\u8d5b\u9053\uff0c\u91c7\u7528\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6307\u6807J&F\u548cJ&F\u0307\u6765\u8861\u91cf\u4e0d\u540c\u5c3a\u5ea6\u548c\u6d88\u5931\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u548c\u534f\u8bae\uff0c\u7a81\u51fa\u4e86\u9876\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u70bc\u51faLLM/MLLM\u7ec4\u4ef6\u548c\u5185\u5b58\u611f\u77e5\u4f20\u64ad\u7b49\u65b0\u5174\u8d8b\u52bf\u3002", "conclusion": "\u4e3a\u5728\u91ce\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u5f39\u6027\u7684\u3001\u8bed\u8a00\u611f\u77e5\u7684\u89c6\u9891\u5206\u5272\u6307\u660e\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.11618", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11618", "abs": "https://arxiv.org/abs/2510.11618", "authors": ["Zehao Chen", "Rong Pan", "Haoran Li"], "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models", "comment": "Project: https://storyboxproject.github.io", "summary": "Human writers often begin their stories with an overarching mental scene,\nwhere they envision the interactions between characters and their environment.\nInspired by this creative process, we propose a novel approach to long-form\nstory generation, termed hybrid bottom-up long-form story generation, using\nmulti-agent simulations. In our method, agents interact within a dynamic\nsandbox environment, where their behaviors and interactions with one another\nand the environment generate emergent events. These events form the foundation\nfor the story, enabling organic character development and plot progression.\nUnlike traditional top-down approaches that impose rigid structures, our hybrid\nbottom-up approach allows for the natural unfolding of events, fostering more\nspontaneous and engaging storytelling. The system is capable of generating\nstories exceeding 10,000 words while maintaining coherence and consistency,\naddressing some of the key challenges faced by current story generation models.\nWe achieve state-of-the-art performance across several metrics. This approach\noffers a scalable and innovative solution for creating dynamic, immersive\nlong-form stories that evolve organically from agent-driven interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u81ea\u5e95\u5411\u4e0a\u7684\u957f\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6765\u751f\u6210\u8d85\u8fc710,000\u5b57\u7684\u957f\u7bc7\u6545\u4e8b\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6c99\u76d2\u73af\u5883\u4e2d\u7684\u4e92\u52a8\u4ea7\u751f\u6d8c\u73b0\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u6709\u673a\u7684\u89d2\u8272\u53d1\u5c55\u548c\u60c5\u8282\u63a8\u8fdb\u3002", "motivation": "\u53d7\u4eba\u7c7b\u4f5c\u5bb6\u521b\u4f5c\u8fc7\u7a0b\u4e2d\u5148\u6784\u5efa\u6574\u4f53\u5fc3\u7406\u573a\u666f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u65b9\u6cd5\u7ed3\u6784\u50f5\u5316\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u81ea\u53d1\u7684\u6545\u4e8b\u751f\u6210\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6a21\u62df\u65b9\u6cd5\uff0c\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6c99\u76d2\u73af\u5883\u4e2d\u4e92\u52a8\uff0c\u5176\u884c\u4e3a\u548c\u4e0e\u73af\u5883\u53ca\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\u4ea7\u751f\u6d8c\u73b0\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u6784\u6210\u6545\u4e8b\u57fa\u7840\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8d85\u8fc710,000\u5b57\u7684\u957f\u7bc7\u6545\u4e8b\uff0c\u540c\u65f6\u4fdd\u6301\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u521b\u5efa\u52a8\u6001\u3001\u6c89\u6d78\u5f0f\u7684\u957f\u7bc7\u6545\u4e8b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u6545\u4e8b\u4ece\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u4e92\u52a8\u4e2d\u6709\u673a\u6f14\u5316\u800c\u6765\u3002"}}
{"id": "2510.10952", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10952", "abs": "https://arxiv.org/abs/2510.10952", "authors": ["Xi Mao", "Zhendong Wang", "Jingyu Li", "Lingchao Mao", "Utibe Essien", "Hairong Wang", "Xuelei Sherry Ni"], "title": "Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant", "comment": null, "summary": "Early detection of Alzheimer's disease (AD) is crucial because its\nneurodegenerative effects are irreversible, and neuropathologic and\nsocial-behavioral risk factors accumulate years before diagnosis. Identifying\nhigher-risk individuals earlier enables prevention, timely care, and equitable\nresource allocation. We predict cognitive performance from social determinants\nof health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset\nderived from the nationally representative Mex-Cog cohort of the 2003 and 2012\nMexican Health and Aging Study (MHAS).\n  Data: The target is a validated composite cognitive score across seven\ndomains-orientation, memory, attention, language, constructional praxis, and\nexecutive function-derived from the 2016 and 2021 MHAS waves. Predictors span\ndemographic, socioeconomic, health, lifestyle, psychosocial, and healthcare\naccess factors.\n  Methodology: Missingness was addressed with a singular value decomposition\n(SVD)-based imputation pipeline treating continuous and categorical variables\nseparately. This approach leverages latent feature correlations to recover\nmissing values while balancing reliability and scalability. After evaluating\nmultiple methods, XGBoost was chosen for its superior predictive performance.\n  Results and Discussion: The framework outperformed existing methods and the\ndata challenge leaderboard, demonstrating high accuracy, robustness, and\ninterpretability. SHAP-based post hoc analysis identified top contributing SDOH\nfactors and age-specific feature patterns. Notably, flooring material emerged\nas a strong predictor, reflecting socioeconomic and environmental disparities.\nOther influential factors, age, SES, lifestyle, social interaction, sleep,\nstress, and BMI, underscore the multifactorial nature of cognitive aging and\nthe value of interpretable, data-driven SDOH modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u9884\u6d4b\u8ba4\u77e5\u8868\u73b0\uff0c\u901a\u8fc7XGBoost\u6a21\u578b\u5728\u58a8\u897f\u54e5\u8001\u9f84\u5316\u5065\u5eb7\u7814\u7a76\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc6\u522b\u51fa\u5730\u677f\u6750\u6599\u7b49\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5176\u795e\u7ecf\u9000\u884c\u6027\u6548\u5e94\u4e0d\u53ef\u9006\u8f6c\uff0c\u4e14\u795e\u7ecf\u75c5\u7406\u548c\u793e\u4f1a\u884c\u4e3a\u98ce\u9669\u56e0\u7d20\u5728\u8bca\u65ad\u524d\u591a\u5e74\u5c31\u5df2\u79ef\u7d2f\u3002\u65e9\u671f\u8bc6\u522b\u9ad8\u98ce\u9669\u4e2a\u4f53\u6709\u52a9\u4e8e\u9884\u9632\u3001\u53ca\u65f6\u62a4\u7406\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u7684\u63d2\u503c\u7ba1\u9053\u5904\u7406\u7f3a\u5931\u503c\uff0c\u5206\u522b\u5904\u7406\u8fde\u7eed\u548c\u5206\u7c7b\u53d8\u91cf\u3002\u8bc4\u4f30\u591a\u79cd\u65b9\u6cd5\u540e\u9009\u62e9XGBoost\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\uff0c\u56e0\u5176\u9884\u6d4b\u6027\u80fd\u6700\u4f18\u3002", "result": "\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u6570\u636e\u6311\u6218\u6392\u884c\u699c\uff0c\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002SHAP\u5206\u6790\u8bc6\u522b\u51fa\u5730\u677f\u6750\u6599\u3001\u5e74\u9f84\u3001\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u3001\u751f\u6d3b\u65b9\u5f0f\u7b49\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u5728\u8ba4\u77e5\u8001\u5316\u4e2d\u7684\u591a\u56e0\u7d20\u6027\u8d28\uff0c\u4ee5\u53ca\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9a71\u52a8\u7684SDOH\u5efa\u6a21\u7684\u4ef7\u503c\uff0c\u4e3a\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.10982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10982", "abs": "https://arxiv.org/abs/2510.10982", "authors": ["Zihan Wang", "Zhiyong Ma", "Zhongkui Ma", "Shuofeng Liu", "Akide Liu", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "title": "Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization", "comment": null, "summary": "Recent AI regulations call for data that remain useful for innovation while\nresistant to misuse, balancing utility with protection at the model level.\nExisting approaches either perturb data to make it unlearnable or retrain\nmodels to suppress transfer, but neither governs inference by unknown models,\nand both typically require control over training. We propose non-transferable\nexamples (NEs), a training-free and data-agnostic input-side usage-control\nmechanism. We recode inputs within a model-specific low-sensitivity subspace,\npreserving outputs for the authorized model while reducing performance on\nunauthorized models through subspace misalignment. We establish formal bounds\nthat guarantee utility for the authorized model and quantify deviation for\nunauthorized ones, with the Hoffman-Wielandt inequality linking degradation to\nspectral differences. Empirically, NEs retain performance on diverse vision\nbackbones and state-of-the-art vision-language models under common\npreprocessing, whereas non-target models collapse even with reconstruction\nattempts. These results establish NEs as a practical means to preserve intended\ndata utility while preventing unauthorized exploitation. Our project is\navailable at https://trusted-system-lab.github.io/model-specificity", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u975e\u53ef\u8f6c\u79fb\u793a\u4f8b(NEs)\u7684\u8f93\u5165\u4fa7\u4f7f\u7528\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u8f93\u5165\u91cd\u65b0\u7f16\u7801\u5230\u6a21\u578b\u7279\u5b9a\u7684\u4f4e\u654f\u611f\u5b50\u7a7a\u95f4\uff0c\u5728\u4fdd\u6301\u6388\u6743\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u672a\u6388\u6743\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u8fc7\u6270\u52a8\u6570\u636e\u4f7f\u5176\u4e0d\u53ef\u5b66\u4e60\uff0c\u8981\u4e48\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u4ee5\u6291\u5236\u8fc1\u79fb\uff0c\u4f46\u90fd\u65e0\u6cd5\u63a7\u5236\u672a\u77e5\u6a21\u578b\u7684\u63a8\u7406\uff0c\u4e14\u901a\u5e38\u9700\u8981\u63a7\u5236\u8bad\u7ec3\u8fc7\u7a0b\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6570\u636e\u65e0\u5173\u7684\u8f93\u5165\u4fa7\u4f7f\u7528\u63a7\u5236\u673a\u5236\u3002", "method": "\u5728\u6a21\u578b\u7279\u5b9a\u7684\u4f4e\u654f\u611f\u5b50\u7a7a\u95f4\u4e2d\u5bf9\u8f93\u5165\u8fdb\u884c\u91cd\u65b0\u7f16\u7801\uff0c\u4fdd\u6301\u6388\u6743\u6a21\u578b\u7684\u8f93\u51fa\u540c\u65f6\u901a\u8fc7\u5b50\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u6765\u964d\u4f4e\u672a\u6388\u6743\u6a21\u578b\u7684\u6027\u80fd\u3002\u4f7f\u7528Hoffman-Wielandt\u4e0d\u7b49\u5f0f\u5c06\u6027\u80fd\u4e0b\u964d\u4e0e\u8c31\u5dee\u5f02\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNEs\u5728\u591a\u79cd\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u4fdd\u6301\u6027\u80fd\uff0c\u800c\u975e\u76ee\u6807\u6a21\u578b\u5373\u4f7f\u5c1d\u8bd5\u91cd\u5efa\u4e5f\u4f1a\u6027\u80fd\u5d29\u6e83\u3002", "conclusion": "NEs\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u9884\u671f\u6570\u636e\u6548\u7528\u7684\u540c\u65f6\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u5229\u7528\u3002"}}
{"id": "2510.11232", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11232", "abs": "https://arxiv.org/abs/2510.11232", "authors": ["Neilansh Chauhan", "Piyush Kumar Gupta", "Faraz Doja"], "title": "LightPneumoNet: Lightweight Pneumonia Classifier", "comment": "13 pages (including references), 5 figures", "summary": "Effective pneumonia diagnosis is often challenged by the difficulty of\ndeploying large, computationally expensive deep learning models in\nresource-limited settings. This study introduces LightPneumoNet, an efficient,\nlightweight convolutional neural network (CNN) built from scratch to provide an\naccessible and accurate diagnostic solution for pneumonia detection from chest\nX-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.\nPreprocessing included image resizing to 224x224, grayscale conversion, and\npixel normalization, with data augmentation (rotation, zoom, shear) to prevent\noverfitting. The custom architecture features four blocks of stacked\nconvolutional layers and contains only 388,082 trainable parameters, resulting\nin a minimal 1.48 MB memory footprint. On the independent test set, our model\ndelivered exceptional performance, achieving an overall accuracy of 0.942,\nprecision of 0.92, and an F1-Score of 0.96. Critically, it obtained a\nsensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify\ntrue pneumonia cases and minimize clinically significant false negatives.\nNotably, LightPneumoNet achieves this high recall on the same dataset where\nexisting approaches typically require significantly heavier architectures or\nfail to reach comparable sensitivity levels. The model's efficiency enables\ndeployment on low-cost hardware, making advanced computer-aided diagnosis\naccessible in underserved clinics and serving as a reliable second-opinion tool\nto improve patient outcomes.", "AI": {"tldr": "LightPneumoNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u80ba\u708eX\u5149\u7247\u8bca\u65ad\uff0c\u4ec538.8\u4e07\u53c2\u6570\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u80ba\u708e\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u80ba\u708e\u8bca\u65ad\u7684\u56f0\u96be\uff0c\u63d0\u4f9b\u53ef\u8bbf\u95ee\u4e14\u51c6\u786e\u7684\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u6784\u5efa\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\uff0c\u5305\u542b4\u4e2a\u5377\u79ef\u5757\uff0c\u4ec5388,082\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u4f7f\u75285,856\u5f20\u80f8\u90e8X\u5149\u7247\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u56fe\u50cf\u9884\u5904\u7406\uff08224x224\u8c03\u6574\u3001\u7070\u5ea6\u8f6c\u6362\u3001\u50cf\u7d20\u5f52\u4e00\u5316\uff09\u548c\u6570\u636e\u589e\u5f3a\uff08\u65cb\u8f6c\u3001\u7f29\u653e\u3001\u526a\u5207\uff09\u3002", "result": "\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u51c6\u786e\u73870.942\uff0c\u7cbe\u786e\u73870.92\uff0cF1\u5206\u65700.96\uff0c\u654f\u611f\u5ea6\uff08\u53ec\u56de\u7387\uff090.99\uff0c\u5185\u5b58\u5360\u7528\u4ec51.48MB\u3002", "conclusion": "LightPneumoNet\u5728\u4fdd\u6301\u9ad8\u654f\u611f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\uff0c\u53ef\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u90e8\u7f72\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bca\u6240\u63d0\u4f9b\u53ef\u9760\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2510.11302", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11302", "abs": "https://arxiv.org/abs/2510.11302", "authors": ["Samer Al-Hamadani"], "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models", "comment": "23 pages, 4 figures, 4 tables", "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u76d1\u7763\u5b66\u4e60\u68c0\u6d4b\u5668(YOLO)\u4e0e\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(Gemini)\u8fdb\u884c\u6210\u672c\u6548\u76ca\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u67b6\u6784\u9009\u62e9\u7684\u5b9a\u91cf\u5e73\u8861\u70b9\u9608\u503c\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u68c0\u6d4b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6210\u672c\uff0c\u800c\u96f6\u6837\u672cVLM\u65e0\u9700\u6807\u6ce8\u4f46\u7cbe\u5ea6\u8f83\u4f4e\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e24\u8005\u7684\u6210\u672c\u6548\u76ca\u3002", "method": "\u57281000\u5f20\u5206\u5c42COCO\u56fe\u50cf\u548c200\u5f20\u591a\u6837\u5316\u4ea7\u54c1\u56fe\u50cf\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7ed3\u5408\u8be6\u7ec6\u7684\u603b\u62e5\u6709\u6210\u672c\u5efa\u6a21\uff0c\u5206\u6790\u51c6\u786e\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "result": "\u76d1\u7763YOLO\u51c6\u786e\u738791.2%\uff0c\u96f6\u6837\u672cGemini\u51c6\u786e\u738768.5%\uff0cYOLO\u4f18\u52bf\u9700\u89815500\u4e07\u6b21\u63a8\u7406\u624d\u5408\u7406\uff1bGemini\u5728\u591a\u6837\u5316\u4ea7\u54c1\u7c7b\u522b\u51c6\u786e\u738752.3%\uff0cYOLO\u4e3a0%\uff1bGemini\u6bcf\u6b21\u6b63\u786e\u68c0\u6d4b\u6210\u672c$0.00050\u8fdc\u4f4e\u4e8eYOLO\u7684$0.143\u3002", "conclusion": "\u6700\u4f18\u67b6\u6784\u9009\u62e9\u53d6\u51b3\u4e8e\u90e8\u7f72\u91cf\u3001\u7c7b\u522b\u7a33\u5b9a\u6027\u3001\u9884\u7b97\u7ea6\u675f\u548c\u7cbe\u5ea6\u8981\u6c42\uff0c\u800c\u975e\u7eaf\u7cb9\u6280\u672f\u6027\u80fd\u6307\u6807\u3002"}}
{"id": "2510.11305", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.11305", "abs": "https://arxiv.org/abs/2510.11305", "authors": ["Jean-Paul Travert", "C\u00e9dric Goeury", "S\u00e9bastien Boyaval", "Vito Bacchi", "Fabrice Zaoui"], "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation", "comment": null, "summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)\nimagery are crucial for calibrating and validating hydraulic models. This study\nuses SAR imagery to evaluate various preprocessing (especially speckle noise\nreduction), flood mapping, and water depth estimation methods. The impact of\nthe choice of method at different steps and its hyperparameters is studied by\nconsidering an ensemble of preprocessed images, flood maps, and water depth\nfields. The evaluation is conducted for two flood events on the Garonne River\n(France) in 2019 and 2021, using hydrodynamic simulations and in-situ\nobservations as reference data. Results show that the choice of speckle filter\nalters flood extent estimations with variations of several square kilometers.\nFurthermore, the selection and tuning of flood mapping methods also affect\nperformance. While supervised methods outperformed unsupervised ones, tuned\nunsupervised approaches (such as local thresholding or change detection) can\nachieve comparable results. The compounded uncertainty from preprocessing and\nflood mapping steps also introduces high variability in the water depth field\nestimates. This study highlights the importance of considering the entire\nprocessing pipeline, encompassing preprocessing, flood mapping, and water depth\nestimation methods and their associated hyperparameters. Rather than relying on\na single configuration, adopting an ensemble approach and accounting for\nmethodological uncertainty should be privileged. For flood mapping, the method\nchoice has the most influence. For water depth estimation, the most influential\nprocessing step was the flood map input resulting from the flood mapping step\nand the hyperparameters of the methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86SAR\u56fe\u50cf\u9884\u5904\u7406\u3001\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u65b9\u6cd5\u5bf9\u6d2a\u6c34\u6a21\u62df\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65b9\u6cd5\u9009\u62e9\u548c\u53c2\u6570\u8c03\u6574\u4f1a\u663e\u8457\u5f71\u54cd\u7ed3\u679c\u7cbe\u5ea6\uff0c\u5efa\u8bae\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u800c\u975e\u5355\u4e00\u914d\u7f6e\u3002", "motivation": "\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u5bf9\u6821\u51c6\u548c\u9a8c\u8bc1\u6c34\u529b\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46SAR\u56fe\u50cf\u5904\u7406\u4e2d\u4e0d\u540c\u65b9\u6cd5\u548c\u53c2\u6570\u9009\u62e9\u7684\u5f71\u54cd\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f7f\u7528SAR\u56fe\u50cf\u8bc4\u4f30\u591a\u79cd\u9884\u5904\u7406\uff08\u7279\u522b\u662f\u6591\u70b9\u566a\u58f0\u53bb\u9664\uff09\u3001\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u9884\u5904\u7406\u56fe\u50cf\u3001\u6d2a\u6c34\u56fe\u548c\u6c34\u6df1\u573a\u7684\u96c6\u6210\u6765\u7814\u7a76\u65b9\u6cd5\u548c\u8d85\u53c2\u6570\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u6591\u70b9\u6ee4\u6ce2\u5668\u7684\u9009\u62e9\u4f1a\u6539\u53d8\u6570\u5e73\u65b9\u516c\u91cc\u7684\u6d2a\u6c34\u8303\u56f4\u4f30\u8ba1\uff1b\u76d1\u7763\u65b9\u6cd5\u4f18\u4e8e\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4f46\u8c03\u4f18\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff08\u5982\u5c40\u90e8\u9608\u503c\u6216\u53d8\u5316\u68c0\u6d4b\uff09\u53ef\u83b7\u5f97\u76f8\u5f53\u7ed3\u679c\uff1b\u9884\u5904\u7406\u548c\u6d2a\u6c34\u5236\u56fe\u6b65\u9aa4\u7684\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u6c34\u6df1\u4f30\u8ba1\u9ad8\u5ea6\u53ef\u53d8\u3002", "conclusion": "\u5fc5\u987b\u8003\u8651\u6574\u4e2a\u5904\u7406\u6d41\u7a0b\uff08\u9884\u5904\u7406\u3001\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u65b9\u6cd5\u53ca\u5176\u8d85\u53c2\u6570\uff09\uff0c\u5efa\u8bae\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u5e76\u8003\u8651\u65b9\u6cd5\u4e0d\u786e\u5b9a\u6027\uff1b\u6d2a\u6c34\u5236\u56fe\u4e2d\u65b9\u6cd5\u9009\u62e9\u5f71\u54cd\u6700\u5927\uff0c\u6c34\u6df1\u4f30\u8ba1\u4e2d\u6d2a\u6c34\u56fe\u8f93\u5165\u548c\u65b9\u6cd5\u8d85\u53c2\u6570\u6700\u5177\u5f71\u54cd\u529b\u3002"}}
{"id": "2510.11498", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11498", "abs": "https://arxiv.org/abs/2510.11498", "authors": ["Yuhang Li", "Chenchen Zhang", "Ruilin Lv", "Ao Liu", "Ken Deng", "Yuanxing Zhang", "Jiaheng Liu", "Wiggin Zhou", "Bo Zhou"], "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding", "comment": null, "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.", "AI": {"tldr": "ReLook\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\uff0c\u5b9e\u73b0\u524d\u7aef\u4ee3\u7801\u751f\u6210\u7684\u751f\u6210-\u8bca\u65ad-\u4f18\u5316\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u524d\u7aef\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u524d\u7aef\u5f00\u53d1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u524d\u7aef\u4ee3\u7801\u7684\u6b63\u786e\u6027\u9700\u8981\u901a\u8fc7\u6e32\u67d3\u50cf\u7d20\u548c\u4ea4\u4e92\u6765\u5224\u65ad\uff0c\u9700\u8981\u89c6\u89c9\u53cd\u9988\u6765\u8bc4\u4f30\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u6279\u8bc4\u5bb6\uff0c\u5bf9\u4ee3\u7801\u8fdb\u884c\u622a\u56fe\u8bc4\u5206\u5e76\u63d0\u4f9b\u57fa\u4e8e\u89c6\u89c9\u7684\u53cd\u9988\uff1b\u5f15\u5165\u5f3a\u5236\u4f18\u5316\u7b56\u7565\uff0c\u53ea\u63a5\u53d7\u6539\u8fdb\u7684\u4fee\u8ba2\u7248\u672c\uff1b\u5728\u63a8\u7406\u9636\u6bb5\u89e3\u8026\u6279\u8bc4\u5bb6\uff0c\u8fd0\u884c\u8f7b\u91cf\u7ea7\u7684\u81ea\u7f16\u8f91\u5faa\u73af\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReLook\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u524d\u7aef\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ReLook\u6846\u67b6\u5c55\u793a\u4e86\u667a\u80fd\u611f\u77e5\u3001\u89c6\u89c9\u5956\u52b1\u4ee5\u53ca\u8bad\u7ec3-\u63a8\u7406\u89e3\u8026\u5728\u524d\u7aef\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u89c6\u89c9\u57fa\u7840\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11257", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.11257", "abs": "https://arxiv.org/abs/2510.11257", "authors": ["Davide Borghini", "Davide Marchi", "Angelo Nardone", "Giordano Scerra", "Silvia Giulia Galfr\u00e8", "Alessandro Pingitore", "Giuseppe Prencipe", "Corrado Priami", "Alina S\u00eerbu"], "title": "MIEO: encoding clinical data to enhance cardiovascular event prediction", "comment": "Presented in the Poster Session of Computational Intelligence methods\n  for Bioinformatics and Biostatistics (CIBB) 2025", "summary": "As clinical data are becoming increasingly available, machine learning\nmethods have been employed to extract knowledge from them and predict clinical\nevents. While promising, approaches suffer from at least two main issues: low\navailability of labelled data and data heterogeneity leading to missing values.\nThis work proposes the use of self-supervised auto-encoders to efficiently\naddress these challenges. We apply our methodology to a clinical dataset from\npatients with ischaemic heart disease. Patient data is embedded in a latent\nspace, built using unlabelled data, which is then used to train a neural\nnetwork classifier to predict cardiovascular death. Results show improved\nbalanced accuracy compared to applying the classifier directly to the raw data,\ndemonstrating that this solution is promising, especially in conditions where\navailability of unlabelled data could increase.", "AI": {"tldr": "\u4f7f\u7528\u81ea\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u5904\u7406\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u548c\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u6570\u636e\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u63d0\u9ad8\u5fc3\u8840\u7ba1\u6b7b\u4ea1\u9884\u6d4b\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u4e34\u5e8a\u6570\u636e\u5b58\u5728\u6807\u7b7e\u7a00\u7f3a\u548c\u6570\u636e\u5f02\u6784\u5bfc\u81f4\u7f3a\u5931\u503c\u7684\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u8be5\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u9884\u6d4b\u5fc3\u8840\u7ba1\u6b7b\u4ea1\u3002", "result": "\u76f8\u6bd4\u76f4\u63a5\u5728\u539f\u59cb\u6570\u636e\u4e0a\u5e94\u7528\u5206\u7c7b\u5668\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u51c6\u786e\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u6807\u7b7e\u6570\u636e\u53ef\u7528\u6027\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u826f\u597d\u524d\u666f\u3002"}}
{"id": "2510.11565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11565", "abs": "https://arxiv.org/abs/2510.11565", "authors": ["Aniket Gupta", "Hanhui Wang", "Charles Saunders", "Aruni RoyChowdhury", "Hanumant Singh", "Huaizu Jiang"], "title": "SNAP: Towards Segmenting Anything in Any Point Cloud", "comment": "Project Page, https://neu-vi.github.io/SNAP/", "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/", "AI": {"tldr": "SNAP\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4ea4\u4e92\u5f0f3D\u70b9\u4e91\u5206\u5272\u6a21\u578b\uff0c\u652f\u6301\u8de8\u57df\uff08\u5ba4\u5185\u3001\u5ba4\u5916\u3001\u822a\u7a7a\uff09\u7684\u70b9\u57fa\u548c\u6587\u672c\u57fa\u63d0\u793a\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u548c\u57df\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u57df\u548c\u5355\u4e00\u4ea4\u4e92\u5f62\u5f0f\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u5bfc\u81f4\u7684\u8d1f\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u57287\u4e2a\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u7528\u57df\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u9632\u6b62\u8d1f\u8fc1\u79fb\uff1b\u5bf9\u4e8e\u6587\u672c\u63d0\u793a\u5206\u5272\uff0c\u81ea\u52a8\u751f\u6210\u63a9\u7801\u63d0\u6848\u5e76\u4e0eCLIP\u6587\u672c\u67e5\u8be2\u5d4c\u5165\u5339\u914d\uff0c\u652f\u6301\u5168\u666f\u548c\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u3002", "result": "\u57289\u4e2a\u96f6\u6837\u672c\u7a7a\u95f4\u63d0\u793a\u5206\u5272\u57fa\u51c6\u4e2d\u76848\u4e2a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u57285\u4e2a\u6587\u672c\u63d0\u793a\u57fa\u51c6\u4e2d\u5747\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u8bc1\u660e\u7edf\u4e00\u6a21\u578b\u53ef\u5339\u654c\u6216\u8d85\u8d8a\u4e13\u7528\u57df\u7279\u5b9a\u65b9\u6cd5\u3002", "conclusion": "SNAP\u5c55\u793a\u4e86\u7edf\u4e00\u6a21\u578b\u5728\u8de8\u57df3D\u70b9\u4e91\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u76843D\u6807\u6ce8\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.11579", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11579", "abs": "https://arxiv.org/abs/2510.11579", "authors": ["Hongyu Zhu", "Lin Chen", "Mounim A. El-Yacoubi", "Mingsheng Shang"], "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis", "comment": "Under Review", "summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.", "AI": {"tldr": "MS-Mix\u662f\u4e00\u4e2a\u60c5\u611f\u611f\u77e5\u7684\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u6837\u672c\u9009\u62e9\u3001\u60c5\u611f\u5f3a\u5ea6\u5f15\u5bfc\u7684\u6df7\u5408\u6bd4\u4f8b\u8ba1\u7b97\u548c\u60c5\u611f\u5bf9\u9f50\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMixup\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4f20\u7edfMixup\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u4f1a\u4ea7\u751f\u6807\u7b7e\u6a21\u7cca\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u60c5\u611f\u611f\u77e5\u7684\u6df7\u5408\u673a\u5236\u3002", "method": "\u63d0\u51faMS-Mix\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u60c5\u611f\u611f\u77e5\u6837\u672c\u9009\u62e9\u7b56\u7565\u9632\u6b62\u60c5\u611f\u77db\u76fe\u6837\u672c\u6df7\u5408\uff1b\u60c5\u611f\u5f3a\u5ea6\u5f15\u5bfc\u6a21\u5757\u52a8\u6001\u8ba1\u7b97\u6a21\u6001\u7279\u5b9a\u6df7\u5408\u6bd4\u4f8b\uff1b\u60c5\u611f\u5bf9\u9f50\u635f\u5931\u901a\u8fc7KL\u6563\u5ea6\u6b63\u5219\u5316\u8054\u5408\u8bad\u7ec3\u60c5\u611f\u5f3a\u5ea6\u9884\u6d4b\u5668\u548c\u4e3b\u5e72\u7f51\u7edc\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u516d\u4e2a\u6700\u5148\u8fdb\u4e3b\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMS-Mix\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u9c81\u68d2\u7684\u591a\u6a21\u6001\u60c5\u611f\u589e\u5f3a\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "MS-Mix\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u7684\u6df7\u5408\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.11339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11339", "abs": "https://arxiv.org/abs/2510.11339", "authors": ["Xingtong Yu", "Ruijuan Liang", "Xinming Zhang", "Yuan Fang"], "title": "Event-Aware Prompt Learning for Dynamic Graphs", "comment": "Under review", "summary": "Real-world graph typically evolve via a series of events, modeling dynamic\ninteractions between objects across various domains. For dynamic graph\nlearning, dynamic graph neural networks (DGNNs) have emerged as popular\nsolutions. Recently, prompt learning methods have been explored on dynamic\ngraphs. However, existing methods generally focus on capturing the relationship\nbetween nodes and time, while overlooking the impact of historical events. In\nthis paper, we propose EVP, an event-aware dynamic graph prompt learning\nframework that can serve as a plug-in to existing methods, enhancing their\nability to leverage historical events knowledge. First, we extract a series of\nhistorical events for each node and introduce an event adaptation mechanism to\nalign the fine-grained characteristics of these events with downstream tasks.\nSecond, we propose an event aggregation mechanism to effectively integrate\nhistorical knowledge into node representations. Finally, we conduct extensive\nexperiments on four public datasets to evaluate and analyze EVP.", "AI": {"tldr": "\u63d0\u51fa\u4e86EVP\u6846\u67b6\uff0c\u4e00\u79cd\u4e8b\u4ef6\u611f\u77e5\u7684\u52a8\u6001\u56fe\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u63d2\u4ef6\uff0c\u589e\u5f3a\u5176\u5229\u7528\u5386\u53f2\u4e8b\u4ef6\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8282\u70b9\u4e0e\u65f6\u95f4\u7684\u5173\u7cfb\uff0c\u4f46\u5ffd\u89c6\u4e86\u5386\u53f2\u4e8b\u4ef6\u7684\u5f71\u54cd\u3002", "method": "1. \u4e3a\u6bcf\u4e2a\u8282\u70b9\u63d0\u53d6\u5386\u53f2\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5f15\u5165\u4e8b\u4ef6\u9002\u5e94\u673a\u5236\u5bf9\u9f50\u4e8b\u4ef6\u7279\u5f81\u4e0e\u4e0b\u6e38\u4efb\u52a1\uff1b2. \u63d0\u51fa\u4e8b\u4ef6\u805a\u5408\u673a\u5236\u5c06\u5386\u53f2\u77e5\u8bc6\u6574\u5408\u5230\u8282\u70b9\u8868\u793a\u4e2d\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u6765\u8bc4\u4f30\u548c\u5206\u6790EVP\u3002", "conclusion": "EVP\u6846\u67b6\u80fd\u591f\u6709\u6548\u589e\u5f3a\u73b0\u6709\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u5bf9\u5386\u53f2\u4e8b\u4ef6\u77e5\u8bc6\u7684\u5229\u7528\u80fd\u529b\u3002"}}
{"id": "2510.11717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11717", "abs": "https://arxiv.org/abs/2510.11717", "authors": ["Takuya Nakabayashi", "Navami Kairanda", "Hideo Saito", "Vladislav Golyanik"], "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams", "comment": null, "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.", "AI": {"tldr": "Ev4DGS\u662f\u9996\u4e2a\u4ec5\u4ece\u5355\u76ee\u4e8b\u4ef6\u6d41\u4e2d\u6e32\u67d3\u975e\u521a\u6027\u53d8\u5f62\u7269\u4f53\u65b0\u89c6\u89d2\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u65e0\u9700RGB\u8f93\u5165\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5904\u7406\u975e\u521a\u6027\u7269\u4f53\u65f6\u9700\u8981\u7a00\u758fRGB\u8f93\u5165\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002\u672c\u6587\u63a2\u7d22\u662f\u5426\u4ec5\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u5b66\u4e60\u7c7b\u4f3c\u6a21\u578b\u3002", "method": "\u901a\u8fc71) \u5c06\u4f30\u8ba1\u6a21\u578b\u8f93\u51fa\u4e0e2D\u4e8b\u4ef6\u89c2\u6d4b\u7a7a\u95f4\u5173\u8054\u7684\u635f\u5931\u51fd\u6570\uff0c2) \u4ece\u4e8b\u4ef6\u751f\u6210\u7684\u4e8c\u503c\u63a9\u7801\u8bad\u7ec3\u7684\u7c97\u7cd93D\u53d8\u5f62\u6a21\u578b\uff0c\u56de\u5f52\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Ev4DGS\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u591a\u79cd\u6734\u7d20\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Ev4DGS\u8bc1\u660e\u4e86\u4ec5\u4ece\u4e8b\u4ef6\u6d41\u6e32\u67d3\u975e\u521a\u6027\u53d8\u5f62\u7269\u4f53\u65b0\u89c6\u89d2\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.16444", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16444", "abs": "https://arxiv.org/abs/2509.16444", "authors": ["Chenhan Lyu", "Yutong Song", "Pengfei Zhang", "Amir M. Rahmani"], "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots", "comment": null, "summary": "Mental health applications have emerged as a critical area in computational\nhealth, driven by rising global rates of mental illness, the integration of AI\nin psychological care, and the need for scalable solutions in underserved\ncommunities. These include therapy chatbots, crisis detection, and wellness\nplatforms handling sensitive data, requiring specialized AI safety beyond\ngeneral safeguards due to emotional vulnerability, risks like misdiagnosis or\nsymptom exacerbation, and precise management of vulnerable states to avoid\nsevere outcomes such as self-harm or loss of trust. Despite AI safety advances,\ngeneral safeguards inadequately address mental health-specific challenges,\nincluding crisis intervention accuracy to avert escalations, therapeutic\nguideline adherence to prevent misinformation, scale limitations in\nresource-constrained settings, and adaptation to nuanced dialogues where\ngenerics may introduce biases or miss distress signals. We introduce an\napproach to apply Constitutional AI training with domain-specific mental health\nprinciples for safe, domain-adapted CAI systems in computational mental health\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5baa\u6cd5AI\u8bad\u7ec3\u7684\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u6784\u5efa\u5b89\u5168\u3001\u9886\u57df\u9002\u5e94\u7684AI\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u901a\u7528AI\u5b89\u5168\u63aa\u65bd\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0cAI\u5728\u5fc3\u7406\u62a4\u7406\u4e2d\u7684\u6574\u5408\u4ee5\u53ca\u670d\u52a1\u4e0d\u8db3\u793e\u533a\u5bf9\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u7684\u53d1\u5c55\u3002\u8fd9\u4e9b\u5e94\u7528\u5904\u7406\u654f\u611f\u6570\u636e\uff0c\u9700\u8981\u4e13\u95e8\u7684AI\u5b89\u5168\u63aa\u65bd\uff0c\u56e0\u4e3a\u901a\u7528\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9\u5fc3\u7406\u5065\u5eb7\u7279\u6709\u7684\u6311\u6218\uff0c\u5982\u60c5\u7eea\u8106\u5f31\u6027\u3001\u8bef\u8bca\u98ce\u9669\u3001\u75c7\u72b6\u6076\u5316\u4ee5\u53ca\u5371\u673a\u5e72\u9884\u51c6\u786e\u6027\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5baa\u6cd5AI\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u5fc3\u7406\u5065\u5eb7\u539f\u5219\uff0c\u4e3a\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u5f00\u53d1\u5b89\u5168\u3001\u9886\u57df\u9002\u5e94\u7684CAI\u7cfb\u7edf\u3002", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u5371\u673a\u5e72\u9884\u51c6\u786e\u6027\u3001\u6cbb\u7597\u6307\u5357\u9075\u5b88\u3001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5bf9\u7ec6\u5fae\u5bf9\u8bdd\u7684\u9002\u5e94\u6027\u65b9\u9762\u3002", "conclusion": "\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u5baa\u6cd5AI\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u9002\u5408\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u9700\u6c42\u7684AI\u7cfb\u7edf\uff0c\u6709\u6548\u5e94\u5bf9\u901a\u7528AI\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u89e3\u51b3\u7684\u9886\u57df\u7279\u6709\u6311\u6218\u3002"}}
