<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification](https://arxiv.org/abs/2512.21476)
*Suncheng Xiang,Xiaoyang Wang,Junjie Jiang,Hejia Wang,Dahong Qian*

Main category: cs.CV

TL;DR: 提出Gated Progressive Fusion网络，通过门控机制选择性融合多层级特征，用于结肠镜息肉重识别，提升小目标识别性能


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉重识别在结直肠癌计算机辅助诊断中很重要，但现有方法使用高层特征的粗粒度分辨率对小目标识别效果不佳，因为小目标需要更详细的信息

Method: 提出Gated Progressive Fusion网络，使用门控机制在全连接方式下选择性融合多层级特征，采用门控渐进融合策略通过多层级特征交互实现语义信息的逐层细化

Result: 在标准基准测试中，该方法优于最先进的单模态重识别模型，特别是结合专门的多模态融合策略时效果更佳

Conclusion: 提出的门控渐进融合网络通过有效融合多层级特征，显著提升了结肠镜息肉重识别性能，特别是在小目标识别方面

Abstract: Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras, which plays an important role in the prevention and treatment of colorectal cancer in computer-aided diagnosis. However, the coarse resolution of high-level features of a specific polyp often leads to inferior results for small objects where detailed information is important. To address this challenge, we propose a novel architecture, named Gated Progressive Fusion network, to selectively fuse features from multiple levels using gates in a fully connected way for polyp ReID. On the basis of it, a gated progressive fusion strategy is introduced to achieve layer-wise refinement of semantic information through multi-level feature interactions. Experiments on standard benchmarks show the benefits of the multimodal setting over state-of-the-art unimodal ReID models, especially when combined with the specialized multimodal fusion strategy.

</details>


### [2] [EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal](https://arxiv.org/abs/2512.21545)
*Sanghyun Jo,Donghwan Lee,Eunji Jung,Seong Je Oh,Kyungsu Kim*

Main category: cs.CV

TL;DR: EraseLoRA：无需数据集的物体移除框架，通过背景感知推理和测试时适配解决现有方法中非目标前景被误判为背景导致物体重现的问题，以及注意力操作破坏细节的问题。


<details>
  <summary>Details</summary>
Motivation: 物体移除与普通修复不同，需要防止被掩码的目标重新出现，并以结构和上下文保真度重建被遮挡的背景。现有无数据集方法通过重定向掩码内的自注意力存在两个问题：非目标前景常被误判为背景导致不需要的物体重新生成；直接的注意力操作破坏细节并阻碍背景线索的连贯整合。

Method: 提出EraseLoRA框架：1) 背景感知前景排除(BFE)：使用多模态大语言模型从单张图像-掩码对中分离目标前景、非目标前景和干净背景，无需配对监督；2) 背景感知重建与子类型聚合(BRSA)：进行测试时优化，将推断的背景子类型视为互补片段，通过重建和对齐目标强制其一致整合，无需显式注意力干预。

Result: EraseLoRA作为预训练扩散模型的插件，在物体移除基准测试中表现出对无数据集基线的持续改进，并与数据集驱动方法竞争的结果。代码将在发表后提供。

Conclusion: EraseLoRA通过背景感知推理和测试时适配，解决了现有无数据集物体移除方法的关键问题，实现了更可靠的物体移除效果，无需依赖大规模数据集训练。

Abstract: Object removal differs from common inpainting, since it must prevent the masked target from reappearing and reconstruct the occluded background with structural and contextual fidelity, rather than merely filling a hole plausibly. Recent dataset-free approaches that redirect self-attention inside the mask fail in two ways: non-target foregrounds are often misinterpreted as background, which regenerates unwanted objects, and direct attention manipulation disrupts fine details and hinders coherent integration of background cues. We propose EraseLoRA, a novel dataset-free framework that replaces attention surgery with background-aware reasoning and test-time adaptation. First, Background-aware Foreground Exclusion (BFE), uses a multimodal large-language models to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without paired supervision, producing reliable background cues while excluding distractors. Second, Background-aware Reconstruction with Subtype Aggregation (BRSA), performs test-time optimization that treats inferred background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives, preserving local detail and global structure without explicit attention intervention. We validate EraseLoRA as a plug-in to pretrained diffusion models and across benchmarks for object removal, demonstrating consistent improvements over dataset-free baselines and competitive results against dataset-driven methods. The code will be made available upon publication.

</details>


### [3] [Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints](https://arxiv.org/abs/2512.21637)
*Mutiara Shabrina,Nova Kurnia Putri,Jefri Satria Ferdiansyah,Sabita Khansa Dewi,Novanto Yudistira*

Main category: cs.CV

TL;DR: 本文分析了PPE框架，发现其正则化策略存在潜在更新密集导致语义泄漏的问题，提出使用L1正则化进行稀疏约束，实现更聚焦和可控的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 文本驱动的图像编辑常面临属性纠缠问题，即修改目标属性（如添加刘海）会无意中改变其他语义属性（如身份或外貌）。PPE框架虽然尝试解决此问题，但其正则化策略存在潜在更新密集、易导致语义泄漏的局限性。

Method: 1. 分析PPE框架的架构组件（基于BERT的属性预测和基于StyleGAN2的图像生成）；2. 识别原始正则化策略的局限性；3. 提出基于L1正则化的稀疏约束方法，对潜在空间操作施加稀疏性约束。

Result: 实验结果表明，提出的方法能够实现更聚焦和可控的编辑，有效减少非目标属性的意外改变，同时更好地保留面部身份特征。

Conclusion: 通过引入稀疏约束，改进了PPE框架的正则化策略，解决了潜在更新密集导致的语义泄漏问题，实现了更精确的文本驱动图像编辑。

Abstract: Text-driven image manipulation often suffers from attribute entanglement, where modifying a target attribute (e.g., adding bangs) unintentionally alters other semantic properties such as identity or appearance. The Predict, Prevent, and Evaluate (PPE) framework addresses this issue by leveraging pre-trained vision-language models for disentangled editing. In this work, we analyze the PPE framework, focusing on its architectural components, including BERT-based attribute prediction and StyleGAN2-based image generation on the CelebA-HQ dataset. Through empirical analysis, we identify a limitation in the original regularization strategy, where latent updates remain dense and prone to semantic leakage. To mitigate this issue, we introduce a sparsity-based constraint using L1 regularization on latent space manipulation. Experimental results demonstrate that the proposed approach enforces more focused and controlled edits, effectively reducing unintended changes in non-target attributes while preserving facial identity.

</details>


### [4] [RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention](https://arxiv.org/abs/2512.21710)
*Zhan Chen,Zile Guo,Enze Zhu,Peirong Zhang,Xiaoxuan Liu,Lei Wang,Yidan Zhang*

Main category: cs.CV

TL;DR: RAPTOR是一个实时高分辨率视频预测架构，通过创新的Efficient Video Attention模块和3阶段训练课程，在保持高质量的同时实现实时性能，特别适用于无人机等边缘设备应用。


<details>
  <summary>Details</summary>
Motivation: 视频预测面临分辨率、质量和速度的三难困境，现有方法无法满足自主无人机在密集城市环境中对高分辨率实时预测的严格要求，这限制了在延迟关键应用中的使用。

Method: 提出RAPTOR架构，采用单次前向设计避免迭代方法的误差累积和延迟。核心创新是Efficient Video Attention模块，通过将时空建模分解为空间和时间轴交替操作，将复杂度从O(ST²)降低到O(S+T)，实现全局上下文建模。配合3阶段训练课程从粗到细逐步优化预测。

Result: RAPTOR是首个在Jetson AGX Orin上对512²分辨率视频超过30 FPS的预测器，在UAVid、KTH和自定义高分辨率数据集上PSNR、SSIM和LPIPS指标达到新SOTA。在实际无人机导航任务中将任务成功率提升18%。

Conclusion: RAPTOR打破了视频预测中长期存在的权衡，实现了实时高分辨率性能，为更安全、更具预见性的具身智能体铺平了道路，特别适用于边缘硬件上的延迟关键应用。

Abstract: Video prediction is plagued by a fundamental trilemma: achieving high-resolution and perceptual quality typically comes at the cost of real-time speed, hindering its use in latency-critical applications. This challenge is most acute for autonomous UAVs in dense urban environments, where foreseeing events from high-resolution imagery is non-negotiable for safety. Existing methods, reliant on iterative generation (diffusion, autoregressive models) or quadratic-complexity attention, fail to meet these stringent demands on edge hardware. To break this long-standing trade-off, we introduce RAPTOR, a video prediction architecture that achieves real-time, high-resolution performance. RAPTOR's single-pass design avoids the error accumulation and latency of iterative approaches. Its core innovation is Efficient Video Attention (EVA), a novel translator module that factorizes spatiotemporal modeling. Instead of processing flattened spacetime tokens with $O((ST)^2)$ or $O(ST)$ complexity, EVA alternates operations along the spatial (S) and temporal (T) axes. This factorization reduces the time complexity to $O(S + T)$ and memory complexity to $O(max(S, T))$, enabling global context modeling at $512^2$ resolution and beyond, operating directly on dense feature maps with a patch-free design. Complementing this architecture is a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details. Experiments show RAPTOR is the first predictor to exceed 30 FPS on a Jetson AGX Orin for $512^2$ video, setting a new state-of-the-art on UAVid, KTH, and a custom high-resolution dataset in PSNR, SSIM, and LPIPS. Critically, RAPTOR boosts the mission success rate in a real-world UAV navigation task by 18/%, paving the way for safer and more anticipatory embodied agents.

</details>


### [5] [S&P 500 Stock's Movement Prediction using CNN](https://arxiv.org/abs/2512.21804)
*Rahul Gupta*

Main category: cs.CV

TL;DR: 使用卷积神经网络(CNN)处理多维原始股市数据预测S&P 500指数股票走势，将历史数据矩阵视为"图像"进行分类预测


<details>
  <summary>Details</summary>
Motivation: 传统数学方法在算法交易中已有应用，但现有深度学习研究大多使用单维数据且未充分考虑金融数据的复杂性。本文旨在使用包含股票分割/股息事件的多维原始市场数据，而非人工设计的金融数据

Method: 采用卷积神经网络(CNN)处理多维股票数据，将历史数据矩阵视为向量化的"图像"，用于预测单个股票、行业板块或投资组合的走势

Result: 模型取得了有前景的结果，能够有效预测股票走势

Conclusion: CNN能够成功应用于多维原始金融数据预测，为这一相对新兴领域的研究奠定了基础，支持从单只股票到投资组合的多层次预测

Abstract: This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].
  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.

</details>


### [6] [Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening](https://arxiv.org/abs/2512.21861)
*Md Rafid Islam,Rafsan Jany,Akib Ahmed,Mohammad Ashrafuzzaman Khan*

Main category: cs.CV

TL;DR: 该研究探索了通过特征级融合多个CNN骨干网络来提升糖尿病视网膜病变（DR）二分类筛查的准确性和效率，在五个公共数据集上验证了融合模型优于单一模型，其中EfficientNet-B0 + DenseNet121融合在准确性和延迟之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是可预防性失明的主要原因，但大规模筛查受到专家资源有限以及不同设备和人群图像质量差异的制约。需要开发能够处理异质数据、同时保证准确性和效率的自动化筛查方案。

Method: 使用来自五个公共数据集（APTOS、EyePACS、IDRiD、Messidor、ODIR）的11,156张眼底图像，将DR检测作为二分类任务。比较了三种预训练模型（ResNet50、EfficientNet-B0、DenseNet121）及其两两融合和三者融合的变体。通过五次独立运行评估性能，并进行推理时间分析。

Result: 融合模型在所有运行中均优于单一骨干网络。EfficientNet-B0 + DenseNet121融合模型获得最佳平均性能（准确率82.89%），正常病例和糖尿病病例的F1分数平衡（分别为83.60%和82.60%）。三模型融合虽然具有竞争力，但计算成本显著更高。推理分析显示EfficientNet-B0最快（约1.16毫秒/图像），而Eff+Den融合在准确性和延迟之间提供了有利的平衡。

Conclusion: 轻量级特征融合可以增强模型在异质数据集上的泛化能力，支持在准确性和吞吐量都至关重要的可扩展DR二分类筛查工作流程。融合策略在保持计算效率的同时提升了性能，为实际部署提供了实用方案。

Abstract: Diabetic retinopathy (DR) remains a leading cause of preventable blindness, yet large-scale screening is constrained by limited specialist availability and variable image quality across devices and populations. This work investigates whether feature-level fusion of complementary convolutional neural network (CNN) backbones can deliver accurate and efficient binary DR screening on globally sourced fundus images. Using 11,156 images pooled from five public datasets (APTOS, EyePACS, IDRiD, Messidor, and ODIR), we frame DR detection as a binary classification task and compare three pretrained models (ResNet50, EfficientNet-B0, and DenseNet121) against pairwise and tri-fusion variants. Across five independent runs, fusion consistently outperforms single backbones. The EfficientNet-B0 + DenseNet121 (Eff+Den) fusion model achieves the best overall mean performance (accuracy: 82.89\%) with balanced class-wise F1-scores for normal (83.60\%) and diabetic (82.60\%) cases. While the tri-fusion is competitive, it incurs a substantially higher computational cost. Inference profiling highlights a practical trade-off: EfficientNet-B0 is the fastest (approximately 1.16 ms/image at batch size 1000), whereas the Eff+Den fusion offers a favorable accuracy--latency balance. These findings indicate that lightweight feature fusion can enhance generalization across heterogeneous datasets, supporting scalable binary DR screening workflows where both accuracy and throughput are critical.

</details>


### [7] [Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models](https://arxiv.org/abs/2512.22046)
*Zongmin Zhang,Zhen Sun,Yifan Liao,Wenhan Dong,Xinlei He,Xingshuo Han,Shengmin Xu,Xinyi Huang*

Main category: cs.CV

TL;DR: BadVSFM：首个针对提示驱动视频分割基础模型（VSFM）的后门攻击框架，通过两阶段训练实现高成功率攻击，同时保持正常分割质量


<details>
  <summary>Details</summary>
Motivation: 随着SAM2等提示驱动视频分割基础模型在自动驾驶、数字病理学等关键领域的应用，后门安全威胁日益凸显。研究发现传统后门攻击对VSFMs几乎无效（成功率低于5%），需要专门针对VSFM特性的攻击方法

Method: 提出BadVSFM两阶段攻击框架：1）引导图像编码器，使触发帧映射到目标嵌入，干净帧保持与参考编码器对齐；2）训练掩码解码器，使所有提示类型的触发帧-提示对产生共享目标掩码，同时保持干净输出接近参考解码器

Result: 在两个数据集和五个VSFM上的实验表明，BadVSFM在不同触发器和提示下实现了强大可控的后门效果，同时保持干净分割质量。攻击成功率显著提升，且对四种代表性防御方法基本无效

Conclusion: BadVSFM揭示了当前VSFMs中未被充分探索的安全漏洞，通过分离触发和干净表示、将注意力转移到触发区域，实现了有效的后门攻击，现有防御方法难以应对

Abstract: Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.

</details>


### [8] [Yume-1.5: A Text-Controlled Interactive World Generation Model](https://arxiv.org/abs/2512.22096)
*Xiaofeng Mao,Zhen Li,Chuanhao Li,Xiaojie Xu,Kaining Ying,Tong He,Jiangmiao Pang,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出名为\method的新框架，通过单张图像或文本提示生成逼真、交互式和连续的世界，支持键盘探索，解决了现有扩散模型参数过大、推理步骤长、历史上下文增长快等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型生成交互式可探索世界的方法面临参数规模过大、依赖冗长推理步骤、历史上下文快速增长等关键挑战，严重限制了实时性能，且缺乏文本控制生成能力。

Method: 框架包含三个核心组件：1) 集成统一上下文压缩与线性注意力的长视频生成框架；2) 基于双向注意力蒸馏和增强文本嵌入方案的实时流式加速策略；3) 用于生成世界事件的文本控制方法。

Result: 提出了一个能够从单张图像或文本提示生成逼真、交互式、连续世界的完整框架，支持键盘探索，并在补充材料中提供了代码库。

Conclusion: \method框架有效解决了现有扩散模型在生成交互式世界时面临的实时性能和文本控制问题，为从单张图像或文本提示生成逼真、交互式、连续世界提供了可行的解决方案。

Abstract: Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data](https://arxiv.org/abs/2512.21516)
*Hongqing He,Jie Xu,Wenyuan Yang,Yonghua Zhu,Guoqiu Wen,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 提出了一种统一的对比学习多视图聚类框架，通过全局图引导和局部图加权对比学习，解决不完整和噪声多视图数据中的样本配对稀少和错误配对问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多视图数据常存在不完整或噪声问题，导致样本配对稀少或错误配对，这严重影响了基于对比学习的多视图聚类效果。稀少配对问题阻碍了多视图互补信息的充分提取，而错误配对问题则导致对比学习在错误方向上优化模型。

Method: 1. 全局图引导对比学习：所有视图样本构建全局视图亲和图，形成新的样本对以充分探索互补信息，解决稀少配对问题。2. 局部图加权对比学习：利用局部邻居生成配对权重，自适应地加强或减弱配对对比学习，缓解错误配对问题。该方法无需插补，可集成到统一的全局-局部图引导对比学习框架中。

Result: 在不完整和噪声设置下的多视图数据上进行大量实验，证明该方法相比现有最先进方法取得了优越性能。

Conclusion: 提出的统一对比学习多视图聚类框架有效解决了不完整和噪声多视图数据中的样本配对问题，通过全局图引导和局部图加权机制显著提升了聚类效果。

Abstract: Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.

</details>


### [10] [Synthetic Financial Data Generation for Enhanced Financial Modelling](https://arxiv.org/abs/2512.21791)
*Christophe D. Hounwanou,Yae Ulrich Gaba,Pierre Ntakirutimana*

Main category: cs.LG

TL;DR: 本文提出了一个统一的合成金融数据评估框架，应用于三种生成模型：ARIMA-GARCH、VAE和TimeGAN，通过S&P 500数据评估其保真度、时间结构和下游任务实用性。


<details>
  <summary>Details</summary>
Motivation: 金融领域的数据稀缺性和保密性限制了模型开发和稳健测试，需要有效的合成数据生成方法，但缺乏统一的评估标准。

Method: 提出多标准评估框架，包括保真度（MMD）、时间结构（自相关和波动率聚类）和下游任务实用性（均值-方差投资组合优化和波动率预测），应用于三种生成模型：统计ARIMA-GARCH、VAE和TimeGAN。

Result: ARIMA-GARCH能捕捉线性趋势和条件波动率但无法复制非线性动态；VAE产生平滑轨迹但低估极端事件；TimeGAN在真实性和时间一致性方面达到最佳平衡（MMD最低：1.84e-3）。

Conclusion: 提出了根据应用需求和计算约束选择生成模型的实用指南，统一的评估协议和可复现代码库旨在标准化合成金融数据研究的基准测试。

Abstract: Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.

</details>


### [11] [Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers](https://arxiv.org/abs/2512.21801)
*Krishna Chaitanya Sunkara,Rambabu Konakanchi*

Main category: cs.LG

TL;DR: 基于LSTM和随机森林的智能物联网监控系统，用于预测和检测数据中心液体冷却泄漏，可提前2-4小时预警并在1分钟内识别突发泄漏，减少能源浪费。


<details>
  <summary>Details</summary>
Motivation: AI数据中心采用液体冷却处理高热量，但冷却剂泄漏会导致计划外停机和大修期间大量能源损失，需要智能监控系统来预防这些问题。

Method: 结合LSTM神经网络进行概率泄漏预测和随机森林分类器进行即时检测的智能物联网监控系统，使用MQTT流传输、InfluxDB存储和Streamlit仪表板。

Result: 在符合ASHRAE 2021标准的合成数据测试中，达到96.5%的检测准确率和87%的预测准确率（90%概率，±30分钟窗口），湿度、压力和流量提供强预测信号。

Conclusion: 该系统可提前2-4小时预测泄漏并在1分钟内识别突发事件，对于典型47机架设施，每年可防止约1,500 kWh能源浪费，为可持续数据中心运营提供可行性方案。

Abstract: AI data centers which are GPU centric, have adopted liquid cooling to handle extreme heat loads, but coolant leaks result in substantial energy loss through unplanned shutdowns and extended repair periods. We present a proof-of-concept smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting with Random Forest classifiers for instant detection. Testing on synthetic data aligned with ASHRAE 2021 standards, our approach achieves 96.5% detection accuracy and 87% forecasting accuracy at 90% probability within plus or minus 30-minute windows. Analysis demonstrates that humidity, pressure, and flow rate deliver strong predictive signals, while temperature exhibits minimal immediate response due to thermal inertia in server hardware. The system employs MQTT streaming, InfluxDB storage, and Streamlit dashboards, forecasting leaks 2-4 hours ahead while identifying sudden events within 1 minute. For a typical 47-rack facility, this approach could prevent roughly 1,500 kWh annual energy waste through proactive maintenance rather than reactive emergency procedures. While validation remains synthetic-only, results establish feasibility for future operational deployment in sustainable data center operations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech](https://arxiv.org/abs/2512.21706)
*Shuchang Pan,Siddharth Banerjee,Dhruv Hebbar,Siddhant Patel,Akshaj Gupta,Kan Jen Cheng,Hanjo Kim,Zeyi Austin Li,Martin Q. Ma,Tingle Li,Gopala Anumanchipalli,Jiachen Lian*

Main category: cs.CL

TL;DR: 提出基于因果推理和图思维（GoT）的对话行为推理框架，通过层次化标注预测沟通意图和言语行为，建立因果时序依赖，实现全双工会话系统的自然交互。


<details>
  <summary>Details</summary>
Motivation: 人类对话由隐含的思维链组织，表现为时序言语行为。捕捉这种因果路径是构建自然全双工交互系统的关键，但现有方法缺乏对这种因果关系的建模。

Method: 1. 将对话过程建模为图思维（GoT）中的因果推理；2. 采用层次化标注方案形式化意图到行为的路径，预测高层沟通意图和低层言语行为；3. 使用混合语料库训练，结合可控模拟数据和真实对话数据；4. 将流式预测构建为演化图，通过多模态Transformer预测下一个言语行为并生成决策理由。

Result: 在合成和真实全双工对话上的实验表明，该框架实现了稳健的行为检测，产生了可解释的推理链，并为全双工语音对话系统的对话推理基准建立了基础。

Conclusion: GoT框架通过建模对话中的因果推理过程，能够有效预测言语行为、生成解释性理由，为构建更自然、可解释的全双工会话系统提供了理论基础和实用框架。

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [13] [Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers](https://arxiv.org/abs/2512.21709)
*Md. Rakibul Islam,Most. Sharmin Sultana Samu,Md. Zahid Hossain,Farhad Uz Zaman,Md. Kamrozzaman Bhuiyan*

Main category: cs.CL

TL;DR: 该研究探索了孟加拉语中AI生成文本的检测，测试了五种基于Transformer的模型，发现零样本评估效果接近随机，但经过微调后XLM-RoBERTa、mDeBERTa和MultilingualBERT能达到约91%的准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能生成类似人类写作的文本，这引发了关于虚假信息和内容操纵的担忧。虽然已有研究涉及多语言检测，但孟加拉语领域仍未被充分探索。孟加拉语丰富的词汇和复杂结构使得区分人类写作和AI生成文本特别具有挑战性。

Method: 研究调查了五种基于Transformer的模型：XLMRoBERTa-Large、mDeBERTaV3-Base、BanglaBERT-Base、IndicBERT-Base和MultilingualBERT-Base。首先进行零样本评估，然后进行任务特定的微调。

Result: 零样本评估显示所有模型表现接近随机水平（约50%准确率）。微调后性能显著提升：XLM-RoBERTa、mDeBERTa和MultilingualBERT在准确率和F1分数上都达到约91%。IndicBERT表现相对较弱，表明其在该任务微调中的有效性有限。

Conclusion: 该研究推进了孟加拉语中AI生成文本的检测，为构建对抗AI生成内容的强大系统奠定了基础。研究强调了任务特定微调的重要性，并识别出最适合该任务的模型架构。

Abstract: Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.

</details>
