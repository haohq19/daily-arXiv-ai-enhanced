{"id": "2512.08979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08979", "abs": "https://arxiv.org/abs/2512.08979", "authors": ["Daechul Ahn", "Yura Choi", "Hyeonbeom Choi", "Seongwon Cho", "San Kim", "Jonghyun Choi"], "title": "What Happens When: Learning Temporal Orders of Events in Videos", "comment": "WACV 2026", "summary": "Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVECTOR\u57fa\u51c6\u6d4b\u8bd5VLMMs\u7684\u4e8b\u4ef6\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51faMECOT\u65b9\u6cd5\u901a\u8fc7\u4e8b\u4ef6\u7ea7\u6307\u4ee4\u5fae\u8c03\u548c\u601d\u7ef4\u94fe\u63d0\u5347\u65f6\u5e8f\u7406\u89e3\uff0c\u5728VECTOR\u548c\u73b0\u6709\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5bf9\u591a\u4e2a\u4e8b\u4ef6\u65f6\u5e8f\u987a\u5e8f\u7684\u51c6\u786e\u6355\u6349\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u89c6\u9891\u5e27\u88ab\u6253\u4e71\uff0c\u6a21\u578b\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u4ecd\u8868\u73b0\u826f\u597d\uff0c\u8868\u660e\u6a21\u578b\u53ef\u80fd\u4f9d\u8d56\u573a\u666f\u5148\u9a8c\u77e5\u8bc6\u800c\u975e\u51c6\u786e\u7684\u65f6\u5e8f\u5904\u7406\u3002", "method": "\u63d0\u51faVECTOR\u57fa\u51c6\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u7684\u4e8b\u4ef6\u65f6\u5e8f\u8bc6\u522b\u80fd\u529b\uff1b\u63d0\u51faMECOT\u65b9\u6cd5\uff1a1\uff09\u5728\u8be6\u7ec6\u7684\u4e8b\u4ef6\u7ea7\u89c6\u9891\u63cf\u8ff0\u4e0a\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\uff1b2\uff09\u63a8\u7406\u65f6\u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u589e\u5f3a\u65f6\u5e8f\u610f\u8bc6\u3002", "result": "\u591a\u79cdVLMMs\u5728VECTOR\u57fa\u51c6\u4e0a\u7ecf\u5e38\u65e0\u6cd5\u7406\u89e3\u4e8b\u4ef6\u987a\u5e8f\uff1bMECOT\u65b9\u6cd5\u5728VECTOR\u57fa\u51c6\u4e0a\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e0a\u4e5f\u6709\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u65f6\u5e8f\u7406\u89e3\u7684\u6709\u6548\u6027\u3002", "conclusion": "VLMMs\u7684\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\uff1bMECOT\u901a\u8fc7\u4e8b\u4ef6\u7ea7\u5fae\u8c03\u548c\u601d\u7ef4\u94fe\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.08957", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08957", "abs": "https://arxiv.org/abs/2512.08957", "authors": ["Dhruv Nigam"], "title": "LUMOS: Large User MOdels for User Behavior Prediction", "comment": null, "summary": "User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like \"how will upcoming holidays affect user engagement?\" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.\n  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\\% increase in Daily Active Users.", "AI": {"tldr": "LUMOS\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5927\u89c4\u6a21\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1\u5e76\u5229\u7528\u539f\u59cb\u7528\u6237\u6d3b\u52a8\u6570\u636e\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548c\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u7684\u9700\u6c42\u3002", "motivation": "\u5728\u7ebfB2C\u5e73\u53f0\u9762\u4e34\u5927\u89c4\u6a21\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u7279\u5f81\u5de5\u7a0b\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u8ba1\u7b97\u6602\u8d35\uff0c\u4e14\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "LUMOS\u91c7\u7528Transformer\u67b6\u6784\uff0c\u5f15\u5165\u65b0\u9896\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u9884\u6d4b\u6761\u4ef6\u5316\u4e8e\u672a\u6765\u5df2\u77e5\u4e8b\u4ef6\uff08\u5982\u8282\u5047\u65e5\u3001\u4fc3\u9500\u7b49\uff09\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u6001\u6807\u8bb0\u5316\u6280\u672f\uff0c\u5c06\u7528\u6237\u4ea4\u6613\u3001\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u548c\u9759\u6001\u7528\u6237\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u901a\u8fc7\u4e13\u95e8\u7684\u5d4c\u5165\u8def\u5f84\u5904\u7406\u3002", "result": "\u5728\u5305\u542b2750\u4ebf\u7528\u6237\u6d3b\u52a8\u6807\u8bb0\u548c2.5\u4ebf\u7528\u6237\u7684\u751f\u4ea7\u6570\u636e\u96c6\u4e0a\uff0cLUMOS\u57285\u4e2a\u4efb\u52a1\u4e2d\u76f8\u6bd4\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\uff0c\u4e8c\u5206\u7c7b\u4efb\u52a1ROC-AUC\u5e73\u5747\u63d0\u53470.025\uff0c\u56de\u5f52\u4efb\u52a1MAPE\u964d\u4f4e4.6%\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u6bcf\u65e5\u6d3b\u8dc3\u7528\u6237\u589e\u52a03.15%\u3002", "conclusion": "LUMOS\u901a\u8fc7\u6d88\u9664\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u548c\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u5927\u89c4\u6a21\u7528\u6237\u884c\u4e3a\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u5e76\u5e26\u6765\u4e86\u53ef\u8861\u91cf\u7684\u4e1a\u52a1\u5f71\u54cd\u3002"}}
{"id": "2512.08987", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08987", "abs": "https://arxiv.org/abs/2512.08987", "authors": ["Yuze Hao", "Linchao Zhu", "Yi Yang"], "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization", "comment": "Accepted at NeurIPS 2025", "summary": "Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.", "AI": {"tldr": "\u63d0\u51fa3DID\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u4e0e\u7269\u7406\u611f\u77e5\u4f18\u5316\u7b56\u7565\u76f4\u63a5\u63a2\u7d223D\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u51e0\u4f55\u751f\u6210", "motivation": "\u73b0\u6709\u9006\u8bbe\u8ba1\u65b9\u6cd5\u57283D\u9886\u57df\u5b58\u5728\u5c40\u9650\uff1a\u7f51\u683c\u641c\u7d22\u4e0d\u53ef\u884c\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u591a\u4f7f\u75282D\u6295\u5f71\u6216\u5fae\u8c03\u73b0\u67093D\u5f62\u72b6\uff0c\u727a\u7272\u4f53\u79ef\u7ec6\u8282\u5e76\u9650\u5236\u8bbe\u8ba1\u63a2\u7d22\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u4ece\u96f6\u5f00\u59cb3D\u8bbe\u8ba1", "method": "1. \u5b66\u4e60\u7edf\u4e00\u7269\u7406-\u51e0\u4f55\u5d4c\u5165\uff0c\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7d27\u51d1\u6355\u6349\u5f62\u72b6\u548c\u7269\u7406\u573a\u6570\u636e\uff1b2. \u4e24\u9636\u6bb5\u7269\u7406\u611f\u77e5\u4f18\u5316\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u68af\u5ea6\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u5668\u63a2\u7d22\u5168\u5c40\u6f5c\u5728\u6d41\u5f62\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u76ee\u6807\u9a71\u52a8\u3001\u62d3\u6251\u4fdd\u6301\u7684\u7ec6\u5316\u96d5\u523b", "result": "3DID\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f3D\u51e0\u4f55\uff0c\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8bbe\u8ba1\u591a\u6837\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u76843DID\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u5bfc\u822a3D\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u4ece\u96f6\u5f00\u59cb3D\u9006\u8bbe\u8ba1\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2512.09016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09016", "abs": "https://arxiv.org/abs/2512.09016", "authors": ["Haiqian Han", "Lingdong Kong", "Jianing Li", "Ao Liang", "Chengtao Zhu", "Jiacheng Lyu", "Lai Xing Ng", "Xiangyang Ji", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "Learning to Remove Lens Flare in Event Camera", "comment": "Preprint; 29 pages, 14 figures, 4 tables; Project Page at https://e-flare.github.io/", "summary": "Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.", "AI": {"tldr": "E-Deflare\uff1a\u9996\u4e2a\u7cfb\u7edf\u6027\u7684\u53bb\u9664\u4e8b\u4ef6\u76f8\u673a\u955c\u5934\u5149\u6655\u6846\u67b6\uff0c\u5305\u542b\u7269\u7406\u6a21\u578b\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548cSOTA\u6062\u590d\u7f51\u7edc", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u52a8\u6001\u8303\u56f4\u7684\u4f18\u52bf\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u955c\u5934\u5149\u6655\u7684\u5f71\u54cd\uff0c\u8fd9\u79cd\u5149\u5b66\u4f2a\u5f71\u5728\u4e8b\u4ef6\u6d41\u4e2d\u5f62\u6210\u590d\u6742\u7684\u65f6\u7a7a\u5931\u771f\uff0c\u76ee\u524d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76", "method": "1. \u5efa\u7acb\u7269\u7406\u57fa\u7840\uff1a\u63a8\u5bfc\u57fa\u4e8e\u7269\u7406\u7684\u975e\u7ebf\u6027\u6291\u5236\u673a\u5236\u524d\u5411\u6a21\u578b\uff1b2. \u521b\u5efaE-Deflare Benchmark\uff1a\u5305\u542b\u5927\u89c4\u6a21\u6a21\u62df\u8bad\u7ec3\u96c6E-Flare-2.7K\u548c\u9996\u4e2a\u914d\u5bf9\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u96c6E-Flare-R\uff1b3. \u8bbe\u8ba1E-DeflareNet\u5b9e\u73b0SOTA\u6062\u590d\u6027\u80fd", "result": "E-DeflareNet\u5728\u53bb\u9664\u955c\u5934\u5149\u6655\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6062\u590d\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u660e\u663e\u76ca\u5904", "conclusion": "E-Deflare\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u7684\u53bb\u9664\u4e8b\u4ef6\u76f8\u673a\u955c\u5934\u5149\u6655\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u6a21\u578b\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e13\u7528\u7f51\u7edc\u89e3\u51b3\u4e86\u8fd9\u4e00\u957f\u671f\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00"}}
{"id": "2512.09074", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09074", "abs": "https://arxiv.org/abs/2512.09074", "authors": ["Shangqing Xu", "Zhiyuan Zhao", "Megha Sharma", "Jos\u00e9 Mar\u00eda Mart\u00edn-Olalla", "Alexander Rodr\u00edguez", "Gregory A. Wellenius", "B. Aditya Prakash"], "title": "Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction", "comment": null, "summary": "Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.", "AI": {"tldr": "DeepTherm\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u81f4\u547d\u70ed\u6d6a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u65e0\u9700\u70ed\u76f8\u5173\u6b7b\u4ea1\u7387\u5386\u53f2\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5206\u79bb\u57fa\u7ebf\u6b7b\u4ea1\u7387\u4e0e\u70ed\u6d6a\u5f71\u54cd\uff0c\u5b9e\u73b0\u51c6\u786e\u9884\u8b66\u3002", "motivation": "\u57ce\u5e02\u4e25\u91cd\u70ed\u6d6a\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u9700\u8981\u5efa\u7acb\u65e9\u671f\u9884\u8b66\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9884\u6d4b\u5373\u5c06\u5230\u6765\u7684\u81f4\u547d\u70ed\u6d6a\uff0c\u4e3b\u8981\u56e0\u4e3a\u70ed\u76f8\u5173\u6b7b\u4ea1\u7387\u96be\u4ee5\u5b9a\u4e49\u548c\u4f30\u8ba1\uff0c\u4e14\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u9700\u8981\u6ee1\u8db3\u6570\u636e\u53ef\u7528\u6027\u3001\u65f6\u7a7a\u9c81\u68d2\u6027\u548c\u51b3\u7b56\u6210\u672c\u7b49\u989d\u5916\u8981\u6c42\u3002", "method": "\u63d0\u51faDeepTherm\u6a21\u5757\u5316\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u53cc\u9884\u6d4b\u7ba1\u9053\uff0c\u4ece\u5168\u56e0\u6b7b\u4ea1\u7387\u4e2d\u5206\u79bb\u51fa\u65e0\u70ed\u6d6a\u548c\u5176\u4ed6\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u7684\u57fa\u7ebf\u6b7b\u4ea1\u7387\uff0c\u65e0\u9700\u70ed\u76f8\u5173\u6b7b\u4ea1\u7387\u5386\u53f2\u6570\u636e\u3002", "result": "\u5728\u897f\u73ed\u7259\u771f\u5b9e\u6570\u636e\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cDeepTherm\u5728\u4e0d\u540c\u5730\u533a\u3001\u65f6\u95f4\u6bb5\u548c\u4eba\u7fa4\u7fa4\u4f53\u4e2d\u8868\u73b0\u4e00\u81f4\u3001\u9c81\u68d2\u4e14\u51c6\u786e\uff0c\u540c\u65f6\u5141\u8bb8\u5728\u6f0f\u62a5\u548c\u8bef\u62a5\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "conclusion": "DeepTherm\u6210\u529f\u89e3\u51b3\u4e86\u9884\u6d4b\u81f4\u547d\u70ed\u6d6a\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u5386\u53f2\u6b7b\u4ea1\u7387\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u9884\u6d4b\u81f4\u547d\u70ed\u6d6a\uff0c\u4e3a\u516c\u5171\u536b\u751f\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2512.09841", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.09841", "abs": "https://arxiv.org/abs/2512.09841", "authors": ["Yijing Chen", "Yihan Wu", "Kaisi Guan", "Yuchen Ren", "Yuyue Wang", "Ruihua Song", "Liyun Ru"], "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models", "comment": "Code available at https://github.com/YJCX330/Chronus/", "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.", "AI": {"tldr": "ChronusOmni\u662f\u4e00\u4e2a\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u589e\u5f3a\u97f3\u9891\u89c6\u9891\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u901a\u8fc7\u65f6\u95f4\u6233\u6807\u8bb0\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u8de8\u6a21\u6001\u65f6\u95f4\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u573a\u666f\uff0c\u4e13\u6ce8\u4e8e\u663e\u5f0f\u65f6\u95f4\u5b9a\u4f4d\u95ee\u9898\uff0c\u4f46\u5bf9\u97f3\u9891\u6a21\u6001\u5229\u7528\u4e0d\u8db3\uff0c\u4e14\u5ffd\u7565\u4e86\u8de8\u6a21\u6001\u7684\u9690\u5f0f\u65f6\u95f4\u5173\u7cfb\uff08\u5982\u89c6\u89c9\u5185\u5bb9\u4e0e\u8bed\u97f3\u7684\u5bf9\u5e94\u5173\u7cfb\uff09\uff0c\u800c\u8fd9\u4e9b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "1. \u5728\u6bcf\u4e2a\u65f6\u95f4\u5355\u5143\u4e2d\u5c06\u57fa\u4e8e\u6587\u672c\u7684\u65f6\u95f4\u6233\u6807\u8bb0\u4e0e\u89c6\u89c9\u548c\u97f3\u9891\u8868\u793a\u4ea4\u9519\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u65f6\u95f4\u5efa\u6a21\uff1b2. \u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u5f3a\u5236\u6b63\u786e\u7684\u65f6\u95f4\u987a\u5e8f\u5e76\u589e\u5f3a\u7ec6\u7c92\u5ea6\u65f6\u95f4\u63a8\u7406\uff1b3. \u6784\u5efaChronusAV\u6570\u636e\u96c6\uff0c\u652f\u6301\u97f3\u9891\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "ChronusOmni\u5728ChronusAV\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u5176\u4ed6\u65f6\u95f4\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u7684\u5927\u591a\u6570\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f73\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u7684\u89c6\u9891\u548c\u97f3\u9891\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "ChronusOmni\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u663e\u5f0f\u548c\u9690\u5f0f\u7684\u65f6\u95f4\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09269", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09269", "abs": "https://arxiv.org/abs/2512.09269", "authors": ["Yixuan Wang", "Dan P. Guralnik", "Warren E. Dixon"], "title": "Goal inference with Rao-Blackwellized Particle Filters", "comment": "9 pages, 2 figures", "summary": "Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRao-Blackwellized\u7c92\u5b50\u6ee4\u6ce2\u7684\u610f\u56fe\u63a8\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u8f68\u8ff9\u89c2\u6d4b\u4e2d\u63a8\u65ad\u79fb\u52a8\u4ee3\u7406\u7684\u6700\u7ec8\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u4fe1\u606f\u8bba\u6cc4\u9732\u5ea6\u91cf\u6765\u91cf\u5316\u63a8\u65ad\u6548\u679c\u3002", "motivation": "\u4ece\u566a\u58f0\u8f68\u8ff9\u89c2\u6d4b\u4e2d\u63a8\u65ad\u79fb\u52a8\u4ee3\u7406\u7684\u6700\u7ec8\u76ee\u6807\u662f\u4e00\u4e2a\u57fa\u672c\u4f30\u8ba1\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u610f\u56fe\u63a8\u65ad\u6280\u672f\uff0c\u540c\u65f6\u91cf\u5316\u63a8\u65ad\u6548\u679c\u3002", "method": "\u4f7f\u7528Rao-Blackwellized\u7c92\u5b50\u6ee4\u6ce2\u53d8\u4f53\uff0c\u5229\u7528\u4ee3\u7406\u7684\u95ed\u73af\u884c\u4e3a\u5047\u8bbe\u548c\u95ed\u5f0f\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u89e3\u6790\u5730\u8fb9\u7f18\u5316\u7ebf\u6027\u9ad8\u65af\u5b50\u7ed3\u6784\uff0c\u4ec5\u66f4\u65b0\u7c92\u5b50\u6743\u91cd\u3002\u5f15\u5165\u4e24\u79cd\u5dee\u5206\u4f30\u8ba1\u5668\uff1a\u57fa\u4e8eRBPF\u6743\u91cd\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u9650\u5236\u5728\u6709\u6548\u6837\u672c\u4e0a\u7684\u7b80\u5316\u7248\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u51c6\u786e\u5730\u6062\u590d\u5408\u89c4\u4ee3\u7406\u7684\u610f\u56fe\u3002\u901a\u8fc7\u9ad8\u65af\u6df7\u5408KL\u754c\u9650\u63d0\u4f9b\u4e86\u771f\u5b9e\u610f\u56fe\u5206\u5e03\u4e0eRBPF\u4f30\u8ba1\u4e4b\u95f4KL\u6563\u5ea6\u7684\u53ef\u8ba1\u7b97\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u7b80\u5316\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u51e0\u4e4e\u4e0e\u5b8c\u6574\u7248\u672c\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u610f\u56fe\u63a8\u65ad\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8bbe\u8ba1\u610f\u56fe\u6df7\u6dc6\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1\u80fd\u9690\u85cf\u610f\u56fe\u7684\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2512.09172", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09172", "abs": "https://arxiv.org/abs/2512.09172", "authors": ["Sauda Maryam", "Sara Nadeem", "Faisal Qureshi", "Mohsen Ali"], "title": "Prompt-Based Continual Compositional Zero-Shot Learning", "comment": null, "summary": "We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u6301\u7eed\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6PromptCCZSL\uff0c\u901a\u8fc7\u591a\u6559\u5e08\u84b8\u998f\u548c\u4f1a\u8bdd\u611f\u77e5\u63d0\u793a\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u5c5e\u6027\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u5047\u8bbe\u7c7b\u522b\u4e92\u65a5\uff0c\u4f46\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u5c5e\u6027\u548c\u5bf9\u8c61\u53ef\u80fd\u5728\u4e0d\u540c\u4f1a\u8bdd\u4e2d\u91cd\u590d\u51fa\u73b0\uff0c\u800c\u7ec4\u5408\u4fdd\u6301\u552f\u4e00\uff0c\u8fd9\u5e26\u6765\u4e86\u66f4\u590d\u6742\u7684\u707e\u96be\u6027\u9057\u5fd8\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u77e5\u8bc6\u7684\u540c\u65f6\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\u3002", "method": "\u57fa\u4e8e\u51bb\u7ed3\u7684VLM\u9aa8\u5e72\uff0c\u63d0\u51faPromptCCZSL\u6846\u67b6\uff1a1) \u901a\u8fc7\u6700\u8fd1\u52a0\u6743\u591a\u6559\u5e08\u84b8\u998f\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\uff1b2) \u4f7f\u7528\u4f1a\u8bdd\u611f\u77e5\u7ec4\u5408\u63d0\u793a\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff1b3) \u901a\u8fc7\u4f1a\u8bdd\u65e0\u5173\u878d\u5408\u5b66\u4e60\u5c5e\u6027\u548c\u5bf9\u8c61\u63d0\u793a\u4ee5\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\uff1b4) \u4f59\u5f26\u951a\u70b9\u635f\u5931\u7a33\u5b9a\u5148\u524d\u77e5\u8bc6\uff1b5) \u6b63\u4ea4\u6295\u5f71\u635f\u5931\u786e\u4fdd\u65b0\u5d4c\u5165\u4e0e\u5148\u524d\u5d4c\u5165\u533a\u5206\uff1b6) \u4f1a\u8bdd\u5185\u591a\u6837\u6027\u635f\u5931\u4fc3\u8fdb\u5f53\u524d\u4f1a\u8bdd\u5d4c\u5165\u7684\u591a\u6837\u6027\u3002", "result": "\u5728UT-Zappos\u548cC-GQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPromptCCZSL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VLM\u548c\u975eVLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u5c01\u95ed\u4e16\u754c\u8bbe\u7f6e\u4e0b\u7684CCZSL\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u89e3\u51b3\u4e86\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6301\u7eed\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u7684PromptCCZSL\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u5e73\u8861\u4e86\u65b0\u77e5\u8bc6\u5b66\u4e60\u548c\u5148\u524d\u77e5\u8bc6\u4fdd\u7559\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u7ec4\u5408\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09333", "abs": "https://arxiv.org/abs/2512.09333", "authors": ["Yutong Du", "Zicheng Liu", "Bo Wu", "Jingwei Kou", "Hang Li", "Changyou Li", "Yali Zong", "Bo Qi"], "title": "Improved Physics-Driven Neural Network to Solve Inverse Scattering Problems", "comment": null, "summary": "This paper presents an improved physics-driven neural network (IPDNN) framework for solving electromagnetic inverse scattering problems (ISPs). A new Gaussian-localized oscillation-suppressing window (GLOW) activation function is introduced to stabilize convergence and enable a lightweight yet accurate network architecture. A dynamic scatter subregion identification strategy is further developed to adaptively refine the computational domain, preventing missed detections and reducing computational cost. Moreover, transfer learning is incorporated to extend the solver's applicability to practical scenarios, integrating the physical interpretability of iterative algorithms with the real-time inference capability of neural networks. Numerical simulations and experimental results demonstrate that the proposed solver achieves superior reconstruction accuracy, robustness, and efficiency compared with existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u7269\u7406\u9a71\u52a8\u795e\u7ecf\u7f51\u7edc\u6846\u67b6IPDNN\uff0c\u7528\u4e8e\u7535\u78c1\u9006\u6563\u5c04\u95ee\u9898\u6c42\u89e3\uff0c\u5f15\u5165\u65b0\u7684\u6fc0\u6d3b\u51fd\u6570GLOW\u548c\u52a8\u6001\u6563\u5c04\u5b50\u533a\u57df\u8bc6\u522b\u7b56\u7565\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7535\u78c1\u9006\u6563\u5c04\u95ee\u9898\u5728\u533b\u5b66\u6210\u50cf\u3001\u65e0\u635f\u68c0\u6d4b\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u7b49\u95ee\u9898\u3002\u9700\u8981\u7ed3\u5408\u7269\u7406\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u63a8\u7406\u80fd\u529b\u3002", "method": "1. \u63d0\u51faGLOW\u6fc0\u6d3b\u51fd\u6570\uff08\u9ad8\u65af\u5c40\u90e8\u5316\u632f\u8361\u6291\u5236\u7a97\u53e3\uff09\u7a33\u5b9a\u6536\u655b\u5e76\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\uff1b2. \u5f00\u53d1\u52a8\u6001\u6563\u5c04\u5b50\u533a\u57df\u8bc6\u522b\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u7ec6\u5316\u8ba1\u7b97\u57df\uff1b3. \u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u6269\u5c55\u6c42\u89e3\u5668\u9002\u7528\u6027\uff1b4. \u5c06\u8fed\u4ee3\u7b97\u6cd5\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u76f8\u7ed3\u5408\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6c42\u89e3\u5668\u5728\u91cd\u5efa\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u7684\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "IPDNN\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7535\u78c1\u9006\u6563\u5c04\u95ee\u9898\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6fc0\u6d3b\u51fd\u6570\u3001\u81ea\u9002\u5e94\u533a\u57df\u8bc6\u522b\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e0e\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u7ed3\u5408\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09592", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09592", "abs": "https://arxiv.org/abs/2512.09592", "authors": ["Zhe Wang", "Qijin Song", "Yucen Peng", "Weibang Bai"], "title": "CS3D: An Efficient Facial Expression Recognition via Event Vision", "comment": null, "summary": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device.", "AI": {"tldr": "CS3D\u6846\u67b6\u901a\u8fc7\u5206\u89e33D\u5377\u79ef\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u80fd\u8017\uff0c\u7ed3\u5408\u8f6f\u8109\u51b2\u795e\u7ecf\u5143\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4e8b\u4ef6\u76f8\u673a\u4e0a\u8fdb\u884c\u9ad8\u6548\u51c6\u786e\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u4f4e\u5ef6\u8fdf\u7b49\u4f18\u52bf\uff0c\u4f46\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u8017\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u9650\u5236\u4e86\u4e8b\u4ef6\u89c6\u89c9\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faCS3D\u6846\u67b6\uff1a1) \u5206\u89e3Convolutional 3D\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u80fd\u8017\uff1b2) \u4f7f\u7528\u8f6f\u8109\u51b2\u795e\u7ecf\u5143\u589e\u5f3a\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\uff1b3) \u5f15\u5165\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "CS3D\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4RNN\u3001Transformer\u548cC3D\u7b49\u67b6\u6784\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u80fd\u8017\u4ec5\u4e3a\u539f\u59cbC3D\u768421.97%\u3002", "conclusion": "CS3D\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u7684\u80fd\u8017\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2512.09368", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09368", "abs": "https://arxiv.org/abs/2512.09368", "authors": ["Mingyuan Li", "Chunyu Liu", "Zhuojun Li", "Xiao Liu", "Guangsheng Yu", "Bo Du", "Jun Shen", "Qiang Wu"], "title": "CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning", "comment": null, "summary": "Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.", "AI": {"tldr": "\u63d0\u51faCFLight\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5b66\u4e60\u589e\u5f3a\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u78b0\u649e\u4e8b\u6545", "motivation": "\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f18\u5148\u8003\u8651\u6548\u7387\u800c\u5ffd\u89c6\u5b89\u5168\u6027\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u5e73\u8861\u5b89\u5168\u4e0e\u6548\u7387\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u9884\u6d4b\u4e0d\u540c\u884c\u52a8\u7ed3\u679c\uff0c\u96c6\u6210CF\u6a21\u5757\u548c\"X\"\u6a21\u5757\uff0c\u5f00\u53d1CFLight\u7b97\u6cd5\u5b9e\u73b0\u8fd1\u96f6\u78b0\u649e\u63a7\u5236\u7b56\u7565", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCFLight\u76f8\u6bd4\u4f20\u7edfRL\u65b9\u6cd5\u548c\u8fd1\u671f\u5b89\u5168RL\u6a21\u578b\uff0c\u80fd\u51cf\u5c11\u78b0\u649e\u5e76\u63d0\u5347\u6574\u4f53\u4ea4\u901a\u6027\u80fd", "conclusion": "CFLight\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u5b89\u5168\u7684RL\u6846\u67b6\uff0c\u80fd\u6709\u6548\u5904\u7406\u4ea4\u901a\u5b89\u5168\u4e8b\u4ef6\uff0c\u4e3a\u5176\u4ed6\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027"}}
{"id": "2512.09706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09706", "abs": "https://arxiv.org/abs/2512.09706", "authors": ["Kaichen He", "Zihao Wang", "Muyao Li", "Anji Liu", "Yitao Liang"], "title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning", "comment": null, "summary": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "CrossAgent\u662f\u4e00\u4e2a\u7edf\u4e00\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u80fd\u591f\u638c\u63e1\u5f02\u6784\u52a8\u4f5c\u7a7a\u95f4\u5e76\u81ea\u4e3b\u9009\u62e9\u6700\u4f18\u4ea4\u4e92\u63a5\u53e3\uff0c\u5728Minecraft\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u901a\u5e38\u5c40\u9650\u4e8e\u9759\u6001\u9884\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\uff08\u5982API\u3001GUI\u4e8b\u4ef6\u6216\u673a\u5668\u4eba\u547d\u4ee4\uff09\uff0c\u8fd9\u79cd\u521a\u6027\u9650\u5236\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u56e0\u4e3a\u6700\u4f18\u4ea4\u4e92\u7c92\u5ea6\u4f1a\u968f\u4e0a\u4e0b\u6587\u53d8\u5316", "method": "\u63d0\u51fa\u7edf\u4e00\u667a\u80fd\u4f53\u6a21\u578bCrossAgent\uff0c\u91c7\u7528\u5305\u542b\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\u548c\u591a\u8f6e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7b97\u6cd5\u7684\u7efc\u5408\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u81ea\u9002\u5e94\u52a8\u4f5c\u5207\u6362\uff0c\u65e0\u9700\u4eba\u5de5\u6307\u5b9a\u89c4\u5219", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u73af\u5883\u7684800\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cCrossAgent\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u5229\u7528\u4e0d\u540c\u52a8\u4f5c\u7a7a\u95f4\u7684\u4f18\u52bf\uff0c\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u52a8\u4f5c\u57fa\u7ebf\uff0c\u5728\u957f\u65f6\u7a0b\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387", "conclusion": "CrossAgent\u901a\u8fc7\u638c\u63e1\u5f02\u6784\u52a8\u4f5c\u7a7a\u95f4\u548c\u81ea\u4e3b\u9009\u62e9\u6700\u4f18\u63a5\u53e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9002\u5e94\u6027\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53AI\u4ece\u5de5\u7a0b\u5316\u590d\u6742\u5de5\u4f5c\u6d41\u5411\u540e\u8bad\u7ec3\u539f\u751f\u6a21\u578b\u7684\u8303\u5f0f\u8f6c\u53d8\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2512.09924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09924", "abs": "https://arxiv.org/abs/2512.09924", "authors": ["Xinyu Liu", "Hangjie Yuan", "Yujie Wei", "Jiazheng Xing", "Yujin Han", "Jiahao Pan", "Yanbiao Ma", "Chi-Min Chan", "Kang Zhao", "Shiwei Zhang", "Wenhan Luo", "Yike Guo"], "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning", "comment": null, "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faRVE\u4efb\u52a1\u548cReViSE\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u53cd\u601d\u63a8\u7406\u7edf\u4e00\u751f\u6210\u4e0e\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7f16\u8f91\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf", "motivation": "\u73b0\u6709\u89c6\u9891\u7edf\u4e00\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5728\u63a8\u7406\u611f\u77e5\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u8fd9\u4e3b\u8981\u7531\u4e8e\u4e24\u4e2a\u539f\u56e0\uff1a1) \u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u63a8\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91\u7684\u6570\u636e\u96c6\uff1b2) \u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u7f16\u8f91\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u4e30\u5bcc\u7684\u7406\u89e3\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u7f16\u8f91\u8fc7\u7a0b\u3002", "method": "1) \u63d0\u51fa\u63a8\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91(RVE)\u4efb\u52a1\uff0c\u8981\u6c42\u7f16\u8f91\u65f6\u8003\u8651\u7269\u7406\u5408\u7406\u6027\u548c\u56e0\u679c\u52a8\u6001\uff1b2) \u6784\u5efaRVE-Bench\u57fa\u51c6\uff0c\u5305\u542b\u63a8\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u4e24\u4e2a\u4e92\u8865\u5b50\u96c6\uff1b3) \u63d0\u51faReViSE\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u53cd\u601d\u63a8\u7406(SRF)\u67b6\u6784\uff0c\u5c06\u751f\u6210\u4e0e\u8bc4\u4f30\u7edf\u4e00\u5728\u5355\u4e00\u67b6\u6784\u4e2d\uff0c\u5229\u7528\u5185\u90e8VLM\u63d0\u4f9b\u5185\u5728\u53cd\u9988\u6765\u4f18\u5316\u751f\u6210\u5668\u7684\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u5728RVE-Bench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cReViSE\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5728\u63a8\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91\u5b50\u96c6\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e8632%\u7684\u603b\u4f53\u5206\u6570\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165RVE\u4efb\u52a1\u3001\u6784\u5efa\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\u4ee5\u53ca\u63d0\u51fa\u7edf\u4e00\u7684ReViSE\u6846\u67b6\uff0c\u6210\u529f\u5f25\u5408\u4e86\u89c6\u9891\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u7f16\u8f91\u80fd\u529b\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u63a8\u7406\u611f\u77e5\u7684\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
