<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs](https://arxiv.org/abs/2510.08631)
*Hanieh Shojaei Miandashti,Claus Brenner*

Main category: cs.CV

TL;DR: 提出了一种基于层次贝叶斯建模的无监督OOD检测方法，通过分离认知不确定性来改进LiDAR点云中的异常物体检测，在SemanticKITTI数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督OOD检测方法依赖预测熵，但混淆了认知不确定性和偶然不确定性，导致将分布内模糊区域误分类为OOD。需要开发能准确分离不确定性的方法。

Method: 使用深度神经网络特征空间中的高斯混合模型参数进行层次贝叶斯建模，从中提取认知不确定性用于OOD检测，无需辅助数据或额外训练阶段。

Result: 在SemanticKITTI数据集上，相比预测熵方法，AUROC提升18%，AUPRC提升22%，FPR95从76%降低到40%（减少36%）。

Conclusion: 基于层次贝叶斯建模的认知不确定性方法能有效改进无监督OOD检测性能，避免混淆不同类型的不确定性，在LiDAR点云分析中表现优异。

Abstract: In addition to accurate scene understanding through precise semantic
segmentation of LiDAR point clouds, detecting out-of-distribution (OOD)
objects, instances not encountered during training, is essential to prevent the
incorrect assignment of unknown objects to known classes. While supervised OOD
detection methods depend on auxiliary OOD datasets, unsupervised methods avoid
this requirement but typically rely on predictive entropy, the entropy of the
predictive distribution obtained by averaging over an ensemble or multiple
posterior weight samples. However, these methods often conflate epistemic
(model) and aleatoric (data) uncertainties, misclassifying ambiguous in
distribution regions as OOD. To address this issue, we present an unsupervised
OOD detection approach that employs epistemic uncertainty derived from
hierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters in
the feature space of a deep neural network. Without requiring auxiliary data or
additional training stages, our approach outperforms existing uncertainty-based
methods on the SemanticKITTI dataset, achieving an 18\% improvement in AUROC,
22\% increase in AUPRC, and 36\% reduction in FPR95 (from 76\% to 40\%),
compared to the predictive entropy approach used in prior works.

</details>


### [2] [Detection of high-frequency oscillations using time-frequency analysis](https://arxiv.org/abs/2510.08637)
*Mostafa Mohammadpour,Mehdi Zekriyapanah Gashti,Yusif S. Gasimov*

Main category: cs.CV

TL;DR: 提出了一种基于无监督聚类的高频振荡自动检测方法，在80-500Hz频段内能有效区分HFOs与尖峰、背景活动和伪迹，在控制数据集上达到97.67%的敏感度和98.57%的精确度。


<details>
  <summary>Details</summary>
Motivation: 高频振荡是识别癫痫灶的新生物标志物，但视觉识别耗时费力且主观，需要开发自动化检测方法用于研究和临床应用。

Method: 使用时频域的S变换提取事件，采用无监督聚类技术对事件进行分类，区分高频振荡、尖峰、背景活动和伪迹。

Result: 在控制数据集上敏感度97.67%、精确度98.57%、F分数97.78%；在癫痫患者中，切除与非切除接触点的HFOs比率达到0.73，与手术结果相关性更强。

Conclusion: HFOs是癫痫患者癫痫发生性的有前景生物标志物，切除HFOs（特别是快速波纹）可达到无癫痫发作，而残留HFOs会导致癫痫复发。

Abstract: High-frequency oscillations (HFOs) are a new biomarker for identifying the
epileptogenic zone. Mapping HFO-generating regions can improve the precision of
resection sites in patients with refractory epilepsy. However, detecting HFOs
remains challenging, and their clinical features are not yet fully defined.
Visual identification of HFOs is time-consuming, labor-intensive, and
subjective. As a result, developing automated methods to detect HFOs is
critical for research and clinical use. In this study, we developed a novel
method for detecting HFOs in the ripple and fast ripple frequency bands (80-500
Hz). We validated it using both controlled datasets and data from epilepsy
patients. Our method employs an unsupervised clustering technique to categorize
events extracted from the time-frequency domain using the S-transform. The
proposed detector differentiates HFOs events from spikes, background activity,
and artifacts. Compared to existing detectors, our method achieved a
sensitivity of 97.67%, a precision of 98.57%, and an F-score of 97.78% on the
controlled dataset. In epilepsy patients, our results showed a stronger
correlation with surgical outcomes, with a ratio of 0.73 between HFOs rates in
resected versus non-resected contacts. The study confirmed previous findings
that HFOs are promising biomarkers of epileptogenicity in epileptic patients.
Removing HFOs, especially fast ripple, leads to seizure freedom, while
remaining HFOs lead to seizure recurrence.

</details>


### [3] [Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation](https://arxiv.org/abs/2510.08979)
*Yuki Nii,Futa Waseda,Ching-Chun Chang,Isao Echizen*

Main category: cs.CV

TL;DR: 提出首个防御AI非法上色的方法Uncolorable Examples，通过向灰度图像添加不可察觉的扰动来破坏未经授权的上色效果。


<details>
  <summary>Details</summary>
Motivation: AI上色技术存在版权侵权风险，如未经授权对黑白漫画和电影进行上色并转售，但目前缺乏有效的防护方法。

Method: 提出PAChroma方法，使用拉普拉斯滤波器优化不可察觉的扰动以保持感知质量，并在优化过程中应用多样化输入变换来增强跨模型迁移性和对后处理的鲁棒性。

Result: 在ImageNet和Danbooru数据集上的实验表明，PAChroma能有效降低上色质量同时保持视觉外观。

Conclusion: 这是保护视觉内容免受非法AI上色的首个防御方法，为生成媒体中的版权保护开辟了新途径。

Abstract: AI-based colorization has shown remarkable capability in generating realistic
color images from grayscale inputs. However, it poses risks of copyright
infringement -- for example, the unauthorized colorization and resale of
monochrome manga and films. Despite these concerns, no effective method
currently exists to prevent such misuse. To address this, we introduce the
first defensive paradigm, Uncolorable Examples, which embed imperceptible
perturbations into grayscale images to invalidate unauthorized colorization. To
ensure real-world applicability, we establish four criteria: effectiveness,
imperceptibility, transferability, and robustness. Our method, Perception-Aware
Chroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that
meet these four criteria by optimizing imperceptible perturbations with a
Laplacian filter to preserve perceptual quality, and applying diverse input
transformations during optimization to enhance transferability across models
and robustness against common post-processing (e.g., compression). Experiments
on ImageNet and Danbooru datasets demonstrate that PAChroma effectively
degrades colorization quality while maintaining the visual appearance. This
work marks the first step toward protecting visual content from illegitimate AI
colorization, paving the way for copyright-aware defenses in generative media.

</details>


### [4] [FLOWING: Implicit Neural Flows for Structure-Preserving Morphing](https://arxiv.org/abs/2510.09537)
*Arthur Bizzi,Matias Grynberg,Vitor Matias,Daniel Perazzo,João Paulo Lima,Luiz Velho,Nuno Gonçalves,João Pereira,Guilherme Schardong,Tiago Novello*

Main category: cs.CV

TL;DR: FLOWING是一个基于微分向量流的变形框架，通过将变形建模为流构建，确保连续性、可逆性和时间一致性，在2D图像和3D形状变形中实现高质量结果。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机在隐式神经表示中建模变形时，需要昂贵的正则化，导致训练不稳定且特征对齐效果不佳。

Method: 将变形重新定义为微分向量流的构建，将结构流属性直接编码到网络架构中，确保变换的连续性和可逆性。

Result: 在面部变形、图像变形和高斯泼溅变形等应用中，FLOWING实现了最先进的变形质量，且收敛速度更快。

Conclusion: 基于流的变形方法能够产生原则性强且稳定的变换，实现准确且结构保持的变形效果。

Abstract: Morphing is a long-standing problem in vision and computer graphics,
requiring a time-dependent warping for feature alignment and a blending for
smooth interpolation. Recently, multilayer perceptrons (MLPs) have been
explored as implicit neural representations (INRs) for modeling such
deformations, due to their meshlessness and differentiability; however,
extracting coherent and accurate morphings from standard MLPs typically relies
on costly regularizations, which often lead to unstable training and prevent
effective feature alignment. To overcome these limitations, we propose FLOWING
(FLOW morphING), a framework that recasts warping as the construction of a
differential vector flow, naturally ensuring continuity, invertibility, and
temporal coherence by encoding structural flow properties directly into the
network architectures. This flow-centric approach yields principled and stable
transformations, enabling accurate and structure-preserving morphing of both 2D
images and 3D shapes. Extensive experiments across a range of applications -
including face and image morphing, as well as Gaussian Splatting morphing -
show that FLOWING achieves state-of-the-art morphing quality with faster
convergence. Code and pretrained models are available at
http://schardong.github.io/flowing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective](https://arxiv.org/abs/2510.08840)
*Thai-Hoang Pham,Jiayuan Chen,Seungyeon Lee,Yuanlong Wang,Sayoko Moroi,Xueru Zhang,Ping Zhang*

Main category: cs.LG

TL;DR: FairTTE是首个用于评估医学影像中时间到事件预测公平性的综合框架，揭示了不同成像模态中普遍存在的偏见，并发现现有公平性方法缓解效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医学影像分析中的广泛应用，人们担忧算法可能对某些社会群体产生偏见。现有研究主要关注医学影像诊断任务，而忽视了预后场景中的公平性问题。

Method: 提出FairTTE框架，整合前沿的时间到事件预测和公平性算法，利用因果分析技术识别和量化医学影像数据集中嵌入的不同偏见来源。

Result: 大规模评估显示偏见在不同成像模态中普遍存在，现有公平性方法缓解效果有限。公平性在分布偏移下难以维持，偏见来源与模型差异之间存在强关联。

Conclusion: 需要针对所有形式偏见的整体方法，现有解决方案存在局限性，迫切需要更稳健、公平的预后模型。

Abstract: As machine learning (ML) algorithms are increasingly used in medical image
analysis, concerns have emerged about their potential biases against certain
social groups. Although many approaches have been proposed to ensure the
fairness of ML models, most existing works focus only on medical image
diagnosis tasks, such as image classification and segmentation, and overlooked
prognosis scenarios, which involve predicting the likely outcome or progression
of a medical condition over time. To address this gap, we introduce FairTTE,
the first comprehensive framework for assessing fairness in time-to-event (TTE)
prediction in medical imaging. FairTTE encompasses a diverse range of imaging
modalities and TTE outcomes, integrating cutting-edge TTE prediction and
fairness algorithms to enable systematic and fine-grained analysis of fairness
in medical image prognosis. Leveraging causal analysis techniques, FairTTE
uncovers and quantifies distinct sources of bias embedded within medical
imaging datasets. Our large-scale evaluation reveals that bias is pervasive
across different imaging modalities and that current fairness methods offer
limited mitigation. We further demonstrate a strong association between
underlying bias sources and model disparities, emphasizing the need for
holistic approaches that target all forms of bias. Notably, we find that
fairness becomes increasingly difficult to maintain under distribution shifts,
underscoring the limitations of existing solutions and the pressing need for
more robust, equitable prognostic models.

</details>


### [6] [AB-PINNs: Adaptive-Basis Physics-Informed Neural Networks for Residual-Driven Domain Decomposition](https://arxiv.org/abs/2510.08924)
*Jonah Botvinick-Greenhouse,Wael H. Ali,Mouhacine Benosman,Saviz Mowlavi*

Main category: cs.LG

TL;DR: 提出了自适应基物理信息神经网络（AB-PINNs），这是一种动态调整子域以适应解内在特征的新方法，通过在高残差区域引入新子域来增强表达能力，特别适合多尺度问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理多尺度问题时面临挑战，静态域分解方法需要大量超参数调优且容易陷入局部最小值。需要一种能够动态适应解特征的方法来提高训练效率和准确性。

Method: 基于经典网格细化技术，在训练过程中动态修改域分解：现有子域自适应调整，同时在高残差区域引入新子域，不同子域可以学习捕捉解的不同尺度特征。

Result: 数值结果表明AB-PINNs能够有效解决各种复杂多尺度偏微分方程，相比静态域分解方法减少了超参数调优需求并防止了局部最小值收敛。

Conclusion: AB-PINNs提供了一种灵活的域分解方法，通过动态子域适应和引入机制，显著提升了PINNs在多尺度问题上的性能，减少了超参数调优负担并改善了收敛性。

Abstract: We introduce adaptive-basis physics-informed neural networks (AB-PINNs), a
novel approach to domain decomposition for training PINNs in which existing
subdomains dynamically adapt to the intrinsic features of the unknown solution.
Drawing inspiration from classical mesh refinement techniques, we also modify
the domain decomposition on-the-fly throughout training by introducing new
subdomains in regions of high residual loss, thereby providing additional
expressive power where the solution of the differential equation is challenging
to represent. Our flexible approach to domain decomposition is well-suited for
multiscale problems, as different subdomains can learn to capture different
scales of the underlying solution. Moreover, the ability to introduce new
subdomains during training helps prevent convergence to unwanted local minima
and can reduce the need for extensive hyperparameter tuning compared to static
domain decomposition approaches. Throughout, we present comprehensive numerical
results which demonstrate the effectiveness of AB-PINNs at solving a variety of
complex multiscale partial differential equations.

</details>


### [7] [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections](https://arxiv.org/abs/2510.09023)
*Milad Nasr,Nicholas Carlini,Chawin Sitawarin,Sander V. Schulhoff,Jamie Hayes,Michael Ilie,Juliette Pluto,Shuang Song,Harsh Chaudhari,Ilia Shumailov,Abhradeep Thakurta,Kai Yuanqing Xiao,Andreas Terzis,Florian Tramèr*

Main category: cs.LG

TL;DR: 论文指出当前语言模型防御评估方法存在缺陷，应使用自适应攻击者进行测试。作者通过系统优化技术成功绕过了12种最新防御方法，大多数防御的攻击成功率从接近零提升到90%以上。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型防御评估方法存在缺陷，通常只针对静态攻击集或弱优化方法进行测试，无法真实反映防御能力。

Method: 使用自适应攻击策略，系统调整和扩展通用优化技术，包括梯度下降、强化学习、随机搜索和人工引导探索。

Result: 成功绕过了12种基于不同技术的最新防御方法，大多数防御的攻击成功率从接近零提升到90%以上。

Conclusion: 未来的防御工作必须考虑更强的攻击方法，如本文所述，才能做出可靠且令人信服的鲁棒性声明。

Abstract: How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense's design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.

</details>


### [8] [Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach](https://arxiv.org/abs/2510.09041)
*Junchao Fan,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出IGCARL方法解决深度强化学习在自动驾驶中的对抗攻击脆弱性问题，通过战略对抗者和约束优化提升鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒方法存在三个关键问题：对抗短视攻击、难以引发安全关键事件、训练不稳定导致策略漂移

Method: IGCARL包含战略目标对抗者和鲁棒驾驶代理，对抗者执行战略协调多步攻击，代理在约束条件下学习鲁棒策略

Result: 实验显示IGCARL比最先进方法成功率提高至少27.9%，显著增强对抗攻击的鲁棒性

Conclusion: IGCARL能有效提升DRL自动驾驶的安全性和可靠性，解决对抗攻击脆弱性问题

Abstract: Deep reinforcement learning (DRL) has demonstrated remarkable success in
developing autonomous driving policies. However, its vulnerability to
adversarial attacks remains a critical barrier to real-world deployment.
Although existing robust methods have achieved success, they still suffer from
three key issues: (i) these methods are trained against myopic adversarial
attacks, limiting their abilities to respond to more strategic threats, (ii)
they have trouble causing truly safety-critical events (e.g., collisions), but
instead often result in minor consequences, and (iii) these methods can
introduce learning instability and policy drift during training due to the lack
of robust constraints. To address these issues, we propose Intelligent
General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel
robust autonomous driving approach that consists of a strategic targeted
adversary and a robust driving agent. The strategic targeted adversary is
designed to leverage the temporal decision-making capabilities of DRL to
execute strategically coordinated multi-step attacks. In addition, it
explicitly focuses on inducing safety-critical events by adopting a general-sum
objective. The robust driving agent learns by interacting with the adversary to
develop a robust autonomous driving policy against adversarial attacks. To
ensure stable learning in adversarial environments and to mitigate policy drift
caused by attacks, the agent is optimized under a constrained formulation.
Extensive experiments show that IGCARL improves the success rate by at least
27.9\% over state-of-the-art methods, demonstrating superior robustness to
adversarial attacks and enhancing the safety and reliability of DRL-based
autonomous driving.

</details>


### [9] [Spatio-Temporal Graph Convolutional Networks for EV Charging Demand Forecasting Using Real-World Multi-Modal Data Integration](https://arxiv.org/abs/2510.09048)
*Jose Tupayachi,Mustafa C. Camur,Kevin Heaslip,Xueping Li*

Main category: cs.LG

TL;DR: TW-GCN框架结合图卷积网络和时序架构，预测美国田纳西州电动汽车充电需求，3小时中期预测效果最佳，1DCNN表现最优，区域分析显示预测精度存在差异。


<details>
  <summary>Details</summary>
Motivation: 交通是温室气体排放主要来源，电动汽车充电基础设施空间分布不均和使用不规律给电网稳定性和投资规划带来挑战。

Method: 提出TW-GCN时空预测框架，结合图卷积网络和时序架构，利用真实交通流量、天气条件和专有数据捕捉空间依赖性和时序动态。

Result: 3小时中期预测在响应性和稳定性间达到最佳平衡，1DCNN模型表现最优；区域分析显示东、中、西田纳西州预测精度存在差异，受站点密度、人口和需求变化影响。

Conclusion: TW-GCN框架推进了数据驱动智能在电动汽车基础设施规划中的整合，支持可持续交通转型和弹性电网管理。

Abstract: Transportation remains a major contributor to greenhouse gas emissions,
highlighting the urgency of transitioning toward sustainable alternatives such
as electric vehicles (EVs). Yet, uneven spatial distribution and irregular
utilization of charging infrastructure create challenges for both power grid
stability and investment planning. This study introduces TW-GCN, a
spatio-temporal forecasting framework that combines Graph Convolutional
Networks with temporal architectures to predict EV charging demand in
Tennessee, United States (U.S.). We utilize real-world traffic flows, weather
conditions, and proprietary data provided by one of the largest EV
infrastructure company in the U.S. to capture both spatial dependencies and
temporal dynamics. Extensive experiments across varying lag horizons,
clustering strategies, and sequence lengths reveal that mid-horizon (3-hour)
forecasts achieve the best balance between responsiveness and stability, with
1DCNN consistently outperforming other temporal models. Regional analysis shows
disparities in predictive accuracy across East, Middle, and West Tennessee,
reflecting how station density, population, and local demand variability shape
model performance. The proposed TW-GCN framework advances the integration of
data-driven intelligence into EV infrastructure planning, supporting both
sustainable mobility transitions and resilient grid management.

</details>


### [10] [Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction](https://arxiv.org/abs/2510.09159)
*Tianyi Chen,Mingcheng Zhu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 该研究提出了首个系统性基准来比较EHR表示方法，发现事件流模型表现最佳，预训练模型在少样本场景下效率高，特征选择策略需根据临床场景调整。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）支持临床预测的深度学习应用，但由于评估实践不一致，患者数据的最佳表示方法仍不明确。

Method: 建立标准化基准，在MIMIC-IV和EHRSHOT数据集上比较三种EHR表示范式：多变量时间序列、事件流和文本事件流，评估包括Transformer、MLP、LSTM、Retain、CLMBR、计数模型和LLM等多种模型。

Result: 事件流模型始终表现最佳；预训练模型在少样本场景下样本效率高；特征选择策略需适应临床场景：ICU预测中修剪稀疏特征有益，而纵向任务中保留这些特征至关重要。

Conclusion: 通过统一可复现的流程，研究结果为基于临床环境和数据情况选择EHR表示提供了实用指导。

Abstract: Electronic Health Records (EHRs) enable deep learning for clinical
predictions, but the optimal method for representing patient data remains
unclear due to inconsistent evaluation practices. We present the first
systematic benchmark to compare EHR representation methods, including
multivariate time-series, event streams, and textual event streams for LLMs.
This benchmark standardises data curation and evaluation across two distinct
clinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping)
and the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year
pancreatic cancer). For each paradigm, we evaluate appropriate modelling
families--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR
and count-based models for event streams, 8-20B LLMs for textual streams--and
analyse the impact of feature pruning based on data missingness. Our
experiments reveal that event stream models consistently deliver the strongest
performance. Pre-trained models like CLMBR are highly sample-efficient in
few-shot settings, though simpler count-based models can be competitive given
sufficient data. Furthermore, we find that feature selection strategies must be
adapted to the clinical setting: pruning sparse features improves ICU
predictions, while retaining them is critical for longitudinal tasks. Our
results, enabled by a unified and reproducible pipeline, provide practical
guidance for selecting EHR representations based on the clinical context and
data regime.

</details>


### [11] [Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network](https://arxiv.org/abs/2510.09350)
*Vu Duc Anh Nguyen,Ziyue Li*

Main category: cs.LG

TL;DR: 提出了一种名为XGeoAI的新框架，用于实时、可解释的多步列车延误预测，核心是基于图注意力网络的两阶段自回归模型。


<details>
  <summary>Details</summary>
Motivation: 铁路网络运营效率受到列车延误级联效应的持续影响，现有研究在提供网络范围多步自回归预测和实时可解释性方面存在不足。

Method: 使用两阶段自回归图注意力网络(GAT)模型，将系统表示为运行事件(到达和离开)的时空图，并加入平台和车站拥堵等细粒度特征。

Result: 虽然模型在纯误差指标(MAE)上被简单的持续性基线挑战，但在延误事件分类精度上始终更高，这对于可靠的决策支持工具至关重要。

Conclusion: 该框架为实时列车延误预测提供了可行的解决方案，特别是在延误事件分类精度方面具有优势，适合作为决策支持工具部署。

Abstract: The operational efficiency of railway networks, a cornerstone of modern
economies, is persistently undermined by the cascading effects of train delays.
Accurately forecasting this delay propagation is a critical challenge for
real-time traffic management. While recent research has leveraged Graph Neural
Networks (GNNs) to model the network structure of railways, a significant gap
remains in developing frameworks that provide multi-step autoregressive
forecasts at a network-wide scale, while simultaneously offering the live,
interpretable explanations needed for decision support. This paper addresses
this gap by developing and evaluating a novel XGeoAI framework for live,
explainable, multi-step train delay forecasting. The core of this work is a
two-stage, autoregressive Graph Attention Network (GAT) model, trained on a
real-world dataset covering over 40% of the Dutch railway network. The model
represents the system as a spatio-temporal graph of operational events
(arrivals and departures) and is enriched with granular features, including
platform and station congestion. To test its viability for live deployment, the
model is rigorously evaluated using a sequential, k-step-ahead forecasting
protocol that simulates real-world conditions where prediction errors can
compound. The results demonstrate that while the proposed GATv2 model is
challenged on pure error metrics (MAE) by a simpler Persistence baseline, it
achieves consistently higher precision in classifying delay events -- a crucial
advantage for a reliable decision support tool.

</details>


### [12] [On Uniformly Scaling Flows: A Density-Aligned Approach to Deep One-Class Classification](https://arxiv.org/abs/2510.09452)
*Faried Abu Zaid,Tim Katzke,Emmanuel Müller,Daniel Neider*

Main category: cs.LG

TL;DR: 本文证明了均匀缩放流(USFs)统一了深度单类分类和密度估计两种异常检测范式，通过理论分析表明USF的最大似然训练等价于Deep SVDD目标，并提出了将USFs作为非USFs的替代方案来提升性能。


<details>
  <summary>Details</summary>
Motivation: 连接深度单类分类（如Deep SVDD）和密度估计（如归一化流）这两种广泛研究的异常检测范式，建立理论桥梁。

Method: 使用均匀缩放流(USFs)，证明其最大似然训练等价于Deep SVDD目标，并防止表示崩溃。将USFs作为非USFs的替代方案应用于现代异常检测架构。

Result: 在多个基准测试和模型骨干上，USFs替换非USFs带来了持续的性能提升和显著改善的训练稳定性，适用于图像级和像素级检测。

Conclusion: USFs统一了两种主要异常检测范式，既推进了理论理解又提升了实际性能，建议将USFs作为异常检测架构中的标准组件。

Abstract: Unsupervised anomaly detection is often framed around two widely studied
paradigms. Deep one-class classification, exemplified by Deep SVDD, learns
compact latent representations of normality, while density estimators realized
by normalizing flows directly model the likelihood of nominal data. In this
work, we show that uniformly scaling flows (USFs), normalizing flows with a
constant Jacobian determinant, precisely connect these approaches.
Specifically, we prove how training a USF via maximum-likelihood reduces to a
Deep SVDD objective with a unique regularization that inherently prevents
representational collapse. This theoretical bridge implies that USFs inherit
both the density faithfulness of flows and the distance-based reasoning of
one-class methods. We further demonstrate that USFs induce a tighter alignment
between negative log-likelihood and latent norm than either Deep SVDD or
non-USFs, and how recent hybrid approaches combining one-class objectives with
VAEs can be naturally extended to USFs. Consequently, we advocate using USFs as
a drop-in replacement for non-USFs in modern anomaly detection architectures.
Empirically, this substitution yields consistent performance gains and
substantially improved training stability across multiple benchmarks and model
backbones for both image-level and pixel-level detection. These results unify
two major anomaly detection paradigms, advancing both theoretical understanding
and practical performance.

</details>


### [13] [Automated Evolutionary Optimization for Resource-Efficient Neural Network Training](https://arxiv.org/abs/2510.09566)
*Ilia Revin,Leon Strelkov,Vadim A. Potemkin,Ivan Kireev,Andrey Savchenko*

Main category: cs.LG

TL;DR: 提出了PETRA自动化机器学习框架，通过进化优化模型架构和训练策略，结合剪枝、量化和损失正则化，显著减小模型规模（最多75%）和延迟（最多33%），提升吞吐量（13%）且不影响目标指标。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络模型优化中的分布式计算、压缩技术和高效训练等关键挑战，满足对可扩展和资源高效模型日益增长的需求。

Method: 开发PETRA自动化机器学习框架，应用进化优化方法优化模型架构和训练策略，包含剪枝、量化和损失正则化技术。

Result: 在真实世界金融事件序列数据以及图像和时间序列基准测试中，PETRA能够显著减小模型规模（最多75%）、降低延迟（最多33%）、提高吞吐量（13%），且不影响目标指标性能。

Conclusion: PETRA框架有效解决了神经网络模型优化的关键挑战，实现了模型性能和可扩展性的显著提升，为资源受限环境下的模型部署提供了可行解决方案。

Abstract: There are many critical challenges in optimizing neural network models,
including distributed computing, compression techniques, and efficient
training, regardless of their application to specific tasks. Solving such
problems is crucial because the need for scalable and resource-efficient models
is increasing. To address these challenges, we have developed a new automated
machine learning (AutoML) framework, Parameter Efficient Training with Robust
Automation (PETRA). It applies evolutionary optimization to model architecture
and training strategy. PETRA includes pruning, quantization, and loss
regularization. Experimental studies on real-world data with financial event
sequences, as well as image and time-series -- benchmarks, demonstrate PETRA's
ability to improve neural model performance and scalability -- namely, a
significant decrease in model size (up to 75%) and latency (up to 33%), and an
increase in throughput (by 13%) without noticeable degradation in the target
metric.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction](https://arxiv.org/abs/2510.09049)
*Joonghyuk Hahn,Soohan Lim,Yo-Sub Han*

Main category: cs.AI

TL;DR: 提出了MEC³O多专家共识系统，通过将LLMs分配到不同复杂度类别并让专家进行结构化辩论，使用加权共识机制整合预测，在代码时间复杂度预测任务上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在代码时间复杂度预测中存在局限性，不同模型在不同复杂度类别上表现各异，没有单一模型在所有类别上都表现优异。

Method: MEC³O系统将LLMs分配到特定复杂度类别，提供类别专业化指令使其成为专家，然后进行结构化辩论，通过加权共识机制整合预测结果。

Result: 在CodeComplex数据集上，MEC³O比开源基线方法准确率和macro-F1分数至少提高10%，在macro-F1分数上平均超过GPT-4o-mini，与GPT-4o和GPT-o4-mini的F1分数相当。

Conclusion: 多专家辩论和加权共识策略能有效生成最终预测，证明了该方法的有效性。

Abstract: Predicting the complexity of source code is essential for software
development and algorithm analysis. Recently, Baik et al. (2025) introduced
CodeComplex for code time complexity prediction. The paper shows that LLMs
without fine-tuning struggle with certain complexity classes. This suggests
that no single LLM excels at every class, but rather each model shows
advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus
system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs
to complexity classes based on their performance and provides them with
class-specialized instructions, turning them into experts. These experts engage
in structured debates, and their predictions are integrated through a weighted
consensus mechanism. Our expertise assignments to LLMs effectively handle
Degeneration-of-Thought, reducing reliance on a separate judge model, and
preventing convergence to incorrect majority opinions. Experiments on
CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving
at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini
in macro-F1 scores on average and demonstrates competitive on-par F1 scores to
GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of
multi-expert debates and weight consensus strategy to generate the final
predictions. Our code and data is available at
https://github.com/suhanmen/MECO.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Measuring Moral LLM Responses in Multilingual Capacities](https://arxiv.org/abs/2510.08776)
*Kimaya Basu,Savi Kolari,Allison Yu*

Main category: cs.CL

TL;DR: 评估前沿和开源大语言模型在低资源和高资源语言中的多语言响应准确性和一致性，发现GPT-5表现最佳，而其他模型在不同语言和类别中表现不一致。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在全球多语言环境中的广泛应用，需要理解和保障其多语言响应的质量，特别是在不同语言资源条件下的表现。

Method: 使用五点评分标准和评判大语言模型，在五个维度上评估模型在低资源和高资源语言中的响应。

Result: GPT-5在所有类别中平均表现最佳，特别是在同意与自主权（3.56分）和伤害预防与安全（4.73分）类别中得分最高；Gemini 2.5 Pro表现最差，相应得分为1.39和1.98。

Conclusion: 需要进一步测试语言变化如何影响大语言模型在不同类别中的响应，并改进这些领域的性能。

Abstract: With LLM usage becoming widespread across countries, languages, and humanity
more broadly, the need to understand and guardrail their multilingual responses
increases. Large-scale datasets for testing and benchmarking have been created
to evaluate and facilitate LLM responses across multiple dimensions. In this
study, we evaluate the responses of frontier and leading open-source models in
five dimensions across low and high-resource languages to measure LLM accuracy
and consistency across multilingual contexts. We evaluate the responses using a
five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed
the best on average in each category, while other models displayed more
inconsistency across language and category. Most notably, in the Consent &
Autonomy and Harm Prevention & Safety categories, GPT scored the highest with
averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages
of 1.39 and 1.98, respectively. These findings emphasize the need for further
testing on how linguistic shifts impact LLM responses across various categories
and improvement in these areas.

</details>


### [16] [DARO: Difficulty-Aware Reweighting Policy Optimization](https://arxiv.org/abs/2510.09001)
*Jingyu Zhou,Lu Ma,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 论文提出了DARO方法，通过动态调整不同难度组的损失贡献来解决现有RLVR方法中的损失尺度问题，显著提升了数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的RLVR方法使用静态或过于简化的权重方案，无法适应模型能力的变化，导致训练在不同难度级别上失衡，阻碍整体性能提升。

Method: 提出DARO方法，根据模型学习状态动态调整每个难度组的损失贡献，解决损失尺度问题。

Result: 在Qwen2.5-Math-1.5B、Qwen2.5-Math-7B和Llama3.1-8B模型上的实验表明，DARO在六个数学基准测试中优于四个领先基线，收敛速度更快且最终性能更优。

Conclusion: DARO通过动态重加权机制有效解决了RLVR训练中的损失尺度问题，显著提升了数学推理能力。

Abstract: Recent advances in large language models (LLMs) have shown that reasoning
ability can be significantly enhanced through Reinforcement Learning with
Verifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has
emerged as the de facto approach for RLVR, inspiring numerous variants.
However, our mathematical analysis reveals that these methods are fundamentally
weighted variations of GRPO. We provide a unified view, demonstrating that
their reliance on static or overly simplistic weighting schemes tied to sample
difficulty prevents adaptation to a model's evolving capabilities. This creates
a significant loss scale issue, where training disproportionately focuses on
certain difficulty levels at the expense of others, hindering overall
performance. To address these limitations, we introduce
\textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that
dynamically adjusts the loss contribution of each difficulty group based on the
model's learning state. Extensive experiments on Qwen2.5-Math-1.5B,
Qwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading
baselines across six math benchmarks, achieving significantly faster
convergence and superior final performance.

</details>


### [17] [CrisiText: A dataset of warning messages for LLM training in emergency communication](https://arxiv.org/abs/2510.09243)
*Giacomo Gonella,Gian Maria Campedelli,Stefano Menini,Marco Guerini*

Main category: cs.CL

TL;DR: CrisiText是首个大规模危机预警消息生成数据集，包含13种危机场景的40万条预警消息，用于研究NLG技术在危机管理中的应用。


<details>
  <summary>Details</summary>
Motivation: 在自然灾害或暴力袭击等危机情况下，及时生成有效的预警消息对保护民众至关重要，但目前NLP技术在此领域的应用主要局限于分类任务，预警消息生成的潜力尚未充分挖掘。

Method: 从现有危机描述出发创建事件链，为每个事件生成预警消息，遵循专家指南确保术语准确性和事实性。同时为每条消息提供三种次优预警类型，用于比较监督微调、偏好对齐、零样本和少样本等不同NLG方法。

Result: 构建了包含40万条预警消息的大规模数据集，覆盖近1.8万个危机情境。通过实验比较了多种NLG方法在危机预警生成任务上的表现。

Conclusion: CrisiText数据集填补了危机预警消息生成研究的空白，为开发更有效的危机管理AI系统提供了重要资源，展示了NLG技术在紧急情况下的应用潜力。

Abstract: Effectively identifying threats and mitigating their potential damage during
crisis situations, such as natural disasters or violent attacks, is paramount
for safeguarding endangered individuals. To tackle these challenges, AI has
been used in assisting humans in emergency situations. Still, the use of NLP
techniques remains limited and mostly focuses on classification tasks. The
significant potential of timely warning message generation using NLG
architectures, however, has been largely overlooked. In this paper we present
CrisiText, the first large-scale dataset for the generation of warning messages
across 13 different types of crisis scenarios. The dataset contains more than
400,000 warning messages (spanning almost 18,000 crisis situations) aimed at
assisting civilians during and after such events. To generate the dataset, we
started from existing crisis descriptions and created chains of events related
to the scenarios. Each event was then paired with a warning message. The
generations follow experts' written guidelines to ensure correct terminology
and factuality of their suggestions. Additionally, each message is accompanied
by three suboptimal warning types to allow for the study of different NLG
approaches. To this end, we conducted a series of experiments comparing
supervised fine-tuning setups with preference alignment, zero-shot, and
few-shot approaches. We further assessed model performance in
out-of-distribution scenarios and evaluated the effectiveness of an automatic
post-editor.

</details>


### [18] [Domain-Adapted Pre-trained Language Models for Implicit Information Extraction in Crash Narratives](https://arxiv.org/abs/2510.09434)
*Xixi Wang,Jordanka Kovaceva,Miguel Costa,Shuai Wang,Francisco Camara Pereira,Robert Thomson*

Main category: cs.CL

TL;DR: 该研究探索使用紧凑的开源预训练语言模型处理交通事故叙述文本，通过微调技术提升在碰撞方式和事故类型识别等推理密集型任务上的性能，在真实数据集上表现优于GPT-4o等大型闭源模型。


<details>
  <summary>Details</summary>
Motivation: 交通事故叙述文本分析对提升交通安全很重要，但现有工具难以批量处理非结构化文本，大型语言模型在推理密集型任务上表现不佳且存在隐私问题，因此研究紧凑开源模型是否能支持此类任务。

Method: 使用低秩适应(LoRA)和BERT微调技术，向预训练语言模型注入领域特定知识，针对碰撞方式和车辆事故类型识别两个挑战性目标进行优化。

Result: 在权威真实数据集CISS上的实验表明，微调后的紧凑模型性能优于GPT-4o等强闭源模型，且只需最小训练资源，还能捕获更丰富的叙述细节并纠正数据集中的错误标注。

Conclusion: 紧凑开源预训练语言模型通过适当微调可以有效支持交通事故叙述的推理密集型信息提取，在性能、隐私和资源效率方面都优于大型闭源模型。

Abstract: Free-text crash narratives recorded in real-world crash databases have been
shown to play a significant role in improving traffic safety. However,
large-scale analyses remain difficult to implement as there are no documented
tools that can batch process the unstructured, non standardized text content
written by various authors with diverse experience and attention to detail. In
recent years, Transformer-based pre-trained language models (PLMs), such as
Bidirectional Encoder Representations from Transformers (BERT) and large
language models (LLMs), have demonstrated strong capabilities across various
natural language processing tasks. These models can extract explicit facts from
crash narratives, but their performance declines on inference-heavy tasks in,
for example, Crash Type identification, which can involve nearly 100
categories. Moreover, relying on closed LLMs through external APIs raises
privacy concerns for sensitive crash data. Additionally, these black-box tools
often underperform due to limited domain knowledge. Motivated by these
challenges, we study whether compact open-source PLMs can support
reasoning-intensive extraction from crash narratives. We target two challenging
objectives: 1) identifying the Manner of Collision for a crash, and 2) Crash
Type for each vehicle involved in the crash event from real-world crash
narratives. To bridge domain gaps, we apply fine-tuning techniques to inject
task-specific knowledge to LLMs with Low-Rank Adaption (LoRA) and BERT.
Experiments on the authoritative real-world dataset Crash Investigation
Sampling System (CISS) demonstrate that our fine-tuned compact models
outperform strong closed LLMs, such as GPT-4o, while requiring only minimal
training resources. Further analysis reveals that the fine-tuned PLMs can
capture richer narrative details and even correct some mislabeled annotations
in the dataset.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [19] [Adaptive Science Operations in Deep Space Missions Using Offline Belief State Planning](https://arxiv.org/abs/2510.08812)
*Grace Ra Kim,Hailey Warner,Duncan Eddy,Evan Astle,Zachary Booth,Edward Balaban,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 提出一个基于POMDP和贝叶斯网络的自适应科学仪器序列框架，用于深空任务中的自主科学操作，在Enceladus Orbilander案例中减少近40%的样本识别错误。


<details>
  <summary>Details</summary>
Motivation: 深空任务面临极端通信延迟和环境不确定性，无法进行实时地面操作，需要支持通信受限环境下的自主科学操作。

Method: 将贝叶斯网络集成到POMDP观测空间中，管理高维不确定测量；离线计算仪器操作策略，允许在发射前生成和验证资源感知计划。

Result: 在Enceladus Orbilander的生命检测套件案例研究中，与基线操作概念相比，样本识别错误减少近40%。

Conclusion: 该POMDP框架能够有效提高深空任务中科学数据采集的准确性和自主性，特别适用于通信受限环境。

Abstract: Deep space missions face extreme communication delays and environmental
uncertainty that prevent real-time ground operations. To support autonomous
science operations in communication-constrained environments, we present a
partially observable Markov decision process (POMDP) framework that adaptively
sequences spacecraft science instruments. We integrate a Bayesian network into
the POMDP observation space to manage the high-dimensional and uncertain
measurements typical of astrobiology missions. This network compactly encodes
dependencies among measurements and improves the interpretability and
computational tractability of science data. Instrument operation policies are
computed offline, allowing resource-aware plans to be generated and thoroughly
validated prior to launch. We use the Enceladus Orbilander's proposed Life
Detection Suite (LDS) as a case study, demonstrating how Bayesian network
structure and reward shaping influence system performance. We compare our
method against the mission's baseline Concept of Operations (ConOps),
evaluating both misclassification rates and performance in off-nominal sample
accumulation scenarios. Our approach reduces sample identification errors by
nearly 40%

</details>


### [20] [Trust Modeling and Estimation in Human-Autonomy Interactions](https://arxiv.org/abs/2510.09013)
*Daniel A. Williams,Airlie Chapman,Daniel R. Little,Chris Manzie*

Main category: cs.RO

TL;DR: 本文提出了一种基于切换线性系统的监督者信任动态模型，该模型能够处理对自主系统性能的不对称响应和间歇性通信特性。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏能够处理自主系统性能不对称响应和监督者-自主系统间歇性通信的监督者信任动态模型，这影响了人机交互质量。

Method: 采用具有事件触发采样的切换线性系统结构来构建监督者信任估计模型，并使用51名参与者的信任响应数据来识别模型参数。

Result: 开发了一个基于切换线性模型的监督者信任观测器，能够有效捕捉信任动态变化。

Conclusion: 所提出的模型框架能够更好地描述监督者对自主系统的信任动态，为人机交互系统的设计提供理论支持。

Abstract: Advances in the control of autonomous systems have accompanied an expansion
in the potential applications for autonomous robotic systems. The success of
applications involving humans depends on the quality of interaction between the
autonomous system and the human supervisor, which is particularly affected by
the degree of trust that the supervisor places in the autonomous system. Absent
from the literature are models of supervisor trust dynamics that can
accommodate asymmetric responses to autonomous system performance and the
intermittent nature of supervisor-autonomous system communication. This paper
focuses on formulating an estimated model of supervisor trust that incorporates
both of these features by employing a switched linear system structure with
event-triggered sampling of the model input and output. Trust response data
collected in a user study with 51 participants were then used identify
parameters for a switched linear model-based observer of supervisor trust.

</details>


### [21] [FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents](https://arxiv.org/abs/2510.09483)
*Lars Ohnemus,Nils Hantke,Max Weißer,Kai Furmans*

Main category: cs.RO

TL;DR: FOGMACHINE是一个开源框架，将动态场景图与离散事件模拟相结合，用于在不确定环境中建模对象动态、智能体观察和交互，支持多智能体行为研究和不确定性传播分析。


<details>
  <summary>Details</summary>
Motivation: 当前动态场景图方法难以捕捉随机动态、部分可观测性和多智能体活动，而这些对于在不确定性和延迟感知下行动的具身AI智能体至关重要。

Method: 将动态场景图与离散事件模拟融合，构建可扩展框架来建模对象动态、智能体观察和交互。

Result: 在城市场景实验中展示了真实的时间和空间模式，同时揭示了在稀疏观察下信念估计的挑战。

Conclusion: 通过结合结构化表示和高效模拟，FOGMACHINE为复杂不确定环境中的基准测试、模型训练和具身AI发展建立了有效工具。

Abstract: Dynamic Scene Graphs (DSGs) provide a structured representation of
hierarchical, interconnected environments, but current approaches struggle to
capture stochastic dynamics, partial observability, and multi-agent activity.
These aspects are critical for embodied AI, where agents must act under
uncertainty and delayed perception. We introduce FOGMACHINE , an open-source
framework that fuses DSGs with discrete-event simulation to model object
dynamics, agent observations, and interactions at scale. This setup enables the
study of uncertainty propagation, planning under limited perception, and
emergent multi-agent behavior. Experiments in urban scenarios illustrate
realistic temporal and spatial patterns while revealing the challenges of
belief estimation under sparse observations. By combining structured
representations with efficient simulation, FOGMACHINE establishes an effective
tool for benchmarking, model training, and advancing embodied AI in complex,
uncertain environments.

</details>
