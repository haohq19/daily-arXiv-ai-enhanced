<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models](https://arxiv.org/abs/2601.14330)
*Mengyu Sun,Ziyuan Yang,Andrew Beng Jin Teoh,Junxu Liu,Haibo Hu,Yi Zhang*

Main category: cs.CV

TL;DR: 论文提出LURE方法，通过重构潜在空间和引导采样轨迹来重新唤醒被擦除的概念，解决了现有概念擦除方法的漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在漏洞，被擦除的概念仍能被重新唤醒。现有重新唤醒方法主要依赖提示级优化，忽略了其他生成因素，限制了对其底层动态的全面理解。

Method: 将生成过程建模为隐函数进行理论分析，提出LURE方法：1）语义重新绑定机制重构潜在空间；2）梯度场正交化防止多概念场景中的梯度冲突；3）潜在语义识别引导采样确保重新唤醒过程的稳定性。

Result: 实验证明LURE能够在多种擦除任务和方法中，同时高质量地重新唤醒多个被擦除的概念。

Conclusion: 通过全面分析生成过程的多个因素，提出的LURE方法有效解决了概念擦除方法的漏洞问题，实现了稳定、高质量的概念重新唤醒。

Abstract: Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.

</details>


### [2] [PAS-Mamba: Phase-Amplitude-Spatial State Space Model for MRI Reconstruction](https://arxiv.org/abs/2601.14530)
*Xiaoyan Kui,Zijie Fan,Zexin Ji,Qinsong Li,Hao Xu,Weixin Si,Haodong Xu,Beiji Zou*

Main category: cs.CV

TL;DR: PAS-Mamba：一种用于MRI重建的相位-幅度-空间状态空间模型，通过在频域解耦相位和幅度建模，并结合图像域特征，实现更好的重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有MRI重建方法通常将频域视为整体处理，忽略了其内部相位和幅度分量携带的不同信息类型。相位主要控制图像结构，幅度反映像素级强度，统一建模会导致特征学习干扰。

Method: 提出PAS-Mamba框架：1）图像域使用LocalMamba保持空间局部性；2）频域将幅度和相位解耦为两个专门分支；3）提出圆形频域扫描（CFDS）按频率顺序序列化特征；4）双域互补融合模块（DDCFM）自适应融合幅度相位表示并实现频域与图像域双向交换。

Result: 在IXI和fastMRI膝关节数据集上的大量实验表明，PAS-Mamba在MRI重建任务中持续优于最先进的重建方法。

Conclusion: 通过解耦频域相位和幅度建模，并结合图像域特征，PAS-Mamba能够更有效地利用频域信息，实现更优的MRI重建质量，证明了分离处理相位和幅度特征的重要性。

Abstract: Joint feature modeling in both the spatial and frequency domains has become a mainstream approach in MRI reconstruction. However, existing methods generally treat the frequency domain as a whole, neglecting the differences in the information carried by its internal components. According to Fourier transform theory, phase and amplitude represent different types of information in the image. Our spectrum swapping experiments show that magnitude mainly reflects pixel-level intensity, while phase predominantly governs image structure. To prevent interference between phase and magnitude feature learning caused by unified frequency-domain modeling, we propose the Phase-Amplitude-Spatial State Space Model (PAS-Mamba) for MRI Reconstruction, a framework that decouples phase and magnitude modeling in the frequency domain and combines it with image-domain features for better reconstruction. In the image domain, LocalMamba preserves spatial locality to sharpen fine anatomical details. In frequency domain, we disentangle amplitude and phase into two specialized branches to avoid representational coupling. To respect the concentric geometry of frequency information, we propose Circular Frequency Domain Scanning (CFDS) to serialize features from low to high frequencies. Finally, a Dual-Domain Complementary Fusion Module (DDCFM) adaptively fuses amplitude phase representations and enables bidirectional exchange between frequency and image domains, delivering superior reconstruction. Extensive experiments on the IXI and fastMRI knee datasets show that PAS-Mamba consistently outperforms state of the art reconstruction methods.

</details>


### [3] [LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning](https://arxiv.org/abs/2601.14594)
*Lianying Chao,Linfeng Yin,Peiyu Ren,Yifan Jiang,Qiaoyu Ren,Dingcheng Shan,Jing-cheng Pang,Sijie Wu,Xubin Li,Kai Zhang*

Main category: cs.CV

TL;DR: 论文提出可学习帧选择器(LFS)，通过选择时间多样且事件相关的帧来改进视频描述生成，并引入新基准ICH-CC来评估人类一致性理解。


<details>
  <summary>Details</summary>
Motivation: 传统视频描述方法使用均匀采样帧，但忽略了视频事件分布不均的问题，导致无法有效捕捉重要事件。同时现有基准与人类认知存在差距。

Method: 提出LFS框架：1) 显式建模时间重要性以平衡时间多样性和事件相关性；2) 采用分层策略确保时间覆盖并避免聚类；3) 利用冻结视频LLM的caption反馈来学习直接优化下游描述质量的帧选择。

Result: LFS在两个代表性社区基准和ICH-CC上一致改进详细视频描述，在VDC上提升达2.0%，在ICH-CC上提升超4%。增强的描述还提高了视频问答性能。

Conclusion: LFS为详细视频描述提供了有效且易于集成的解决方案，通过智能帧选择显著提升描述质量，同时新基准ICH-CC更好地反映了人类一致性理解。

Abstract: Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.

</details>


### [4] [Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption](https://arxiv.org/abs/2601.14738)
*Liqin Wang,Qianyue Hu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: VoidFace是一种针对扩散模型人脸交换攻击的系统性防御方法，通过注入扰动破坏身份信息通路，在保持视觉质量的同时有效防御多种人脸交换模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的快速发展使得人脸交换技术普及，但引发了隐私和身份安全问题。现有的主动防御方法通常是从图像编辑攻击中改编而来，在人脸交换场景中效果不佳，主要原因是忽视了人脸交换系统的结构韧性和独特的静态条件引导机制。

Method: VoidFace将人脸交换视为耦合的身份通路，通过在关键瓶颈处注入扰动引发级联破坏。具体包括：1）定位破坏和身份擦除以降低物理回归和语义嵌入；2）通过解耦注意力机制切断身份注入；3）破坏中间扩散特征防止源身份重建。为了保持视觉不可感知性，在潜在流形中进行对抗搜索，采用感知自适应策略平衡攻击效果和图像质量。

Result: 大量实验表明，VoidFace在各种基于扩散的人脸交换模型中优于现有防御方法，同时生成的对抗人脸具有更优的视觉质量。

Conclusion: VoidFace通过系统性破坏人脸交换的身份通路，提供了一种有效且视觉质量高的防御方案，解决了现有防御方法在人脸交换场景中的不足。

Abstract: The rapid evolution of diffusion models has democratized face swapping but also raises concerns about privacy and identity security. Existing proactive defenses, often adapted from image editing attacks, prove ineffective in this context. We attribute this failure to an oversight of the structural resilience and the unique static conditional guidance mechanism inherent in face swapping systems. To address this, we propose VoidFace, a systemic defense method that views face swapping as a coupled identity pathway. By injecting perturbations at critical bottlenecks, VoidFace induces cascading disruption throughout the pipeline. Specifically, we first introduce localization disruption and identity erasure to degrade physical regression and semantic embeddings, thereby impairing the accurate modeling of the source face. We then intervene in the generative domain by decoupling attention mechanisms to sever identity injection, and corrupting intermediate diffusion features to prevent the reconstruction of source identity. To ensure visual imperceptibility, we perform adversarial search in the latent manifold, guided by a perceptual adaptive strategy to balance attack potency with image quality. Extensive experiments show that VoidFace outperforms existing defenses across various diffusion-based swapping models, while producing adversarial faces with superior visual quality.

</details>


### [5] [UBATrack: Spatio-Temporal State Space Model for General Multi-Modal Tracking](https://arxiv.org/abs/2601.14799)
*Qihua Liang,Liang Chen,Yaozong Zheng,Jian Nong,Zhiyi Mo,Bineng Zhong*

Main category: cs.CV

TL;DR: 提出UBATrack，一种基于Mamba状态空间模型的多模态跟踪框架，通过时空Mamba适配器和动态多模态特征混合器有效捕捉时空线索，无需全参数微调，在多个RGB-T、RGB-D、RGB-E跟踪基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前通用多模态跟踪器主要通过提示学习统一各种模态跟踪任务，但忽视了时空线索的有效捕捉。需要一种能同时建模跨模态依赖和时空视觉线索的高效方法。

Method: 提出UBATrack框架，包含两个模块：1) 时空Mamba适配器(STMA)，利用Mamba的长序列建模能力以适配器调优方式联合建模跨模态依赖和时空视觉线索；2) 动态多模态特征混合器，跨多个特征维度增强多模态表示能力以提高跟踪鲁棒性。

Result: 在RGB-T、RGB-D、RGB-E跟踪基准上超越SOTA方法，在LasHeR、RGBT234、RGBT210、DepthTrack、VOT-RGBD22和VisEvent数据集上取得优异结果。

Conclusion: UBATrack通过Mamba模型有效捕捉时空线索，无需全参数微调，提高了多模态跟踪的训练效率和性能，为多模态跟踪提供了新思路。

Abstract: Multi-modal object tracking has attracted considerable attention by integrating multiple complementary inputs (e.g., thermal, depth, and event data) to achieve outstanding performance. Although current general-purpose multi-modal trackers primarily unify various modal tracking tasks (i.e., RGB-Thermal infrared, RGB-Depth or RGB-Event tracking) through prompt learning, they still overlook the effective capture of spatio-temporal cues. In this work, we introduce a novel multi-modal tracking framework based on a mamba-style state space model, termed UBATrack. Our UBATrack comprises two simple yet effective modules: a Spatio-temporal Mamba Adapter (STMA) and a Dynamic Multi-modal Feature Mixer. The former leverages Mamba's long-sequence modeling capability to jointly model cross-modal dependencies and spatio-temporal visual cues in an adapter-tuning manner. The latter further enhances multi-modal representation capacity across multiple feature dimensions to improve tracking robustness. In this way, UBATrack eliminates the need for costly full-parameter fine-tuning, thereby improving the training efficiency of multi-modal tracking algorithms. Experiments show that UBATrack outperforms state-of-the-art methods on RGB-T, RGB-D, and RGB-E tracking benchmarks, achieving outstanding results on the LasHeR, RGBT234, RGBT210, DepthTrack, VOT-RGBD22, and VisEvent datasets.

</details>


### [6] [Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD](https://arxiv.org/abs/2601.15061)
*Qiwei Ma,Jun Zhang*

Main category: cs.CV

TL;DR: 提出一个基于差分隐私的图像生成框架，使用误差反馈随机梯度下降(EFSGD)方法，在相同隐私预算下生成更高质量和可用性的图像。


<details>
  <summary>Details</summary>
Motivation: 传统数据掩码技术（如匿名化）在隐私保护机器学习中无法同时实现预期的隐私保护和数据效用。合成数据在生成大量训练样本和防止真实数据信息泄露方面越来越重要，但现有方法在隐私和效用之间存在重复权衡过程。

Method: 提出一个新颖的差分隐私生成框架，采用误差反馈随机梯度下降(EFSGD)方法，在训练过程中引入重构损失和噪声注入机制。

Result: 在相同隐私预算下生成比相关工作更高质量和可用性的图像。在三个基准数据集（MNIST、Fashion-MNIST、CelebA）上几乎所有指标都达到了最先进的结果。

Conclusion: 提出的框架在灰度和RGB图像上都表现出有效性和泛化能力，解决了隐私保护机器学习中隐私与效用的权衡问题。

Abstract: Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.

</details>


### [7] [StableWorld: Towards Stable and Consistent Long Interactive Video Generation](https://arxiv.org/abs/2601.15281)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出StableWorld方法解决交互式视频生成中的稳定性和时间一致性问题，通过动态帧淘汰机制过滤退化帧，防止累积漂移


<details>
  <summary>Details</summary>
Motivation: 当前交互式视频生成方法在长时间交互中面临严重的不稳定性和时间退化问题，导致空间漂移和场景崩溃，需要解决这些挑战以实现更稳定的交互生成

Method: 提出StableWorld方法，采用动态帧淘汰机制，持续过滤退化帧同时保留几何一致的帧，从源头上防止累积漂移

Result: 在多个交互式视频模型（Matrix-Game、Open-Oasis、Hunyuan-GameCraft）上验证了StableWorld的有效性，显著提高了稳定性、时间一致性和泛化能力

Conclusion: StableWorld是一种模型无关的简单有效方法，可应用于不同的交互式视频生成框架，为解决交互视频生成中的稳定性和时间一致性问题提供了有效方案

Abstract: In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation](https://arxiv.org/abs/2601.14473)
*Danny Butvinik,Nana Boateng,Achi Hackmon*

Main category: cs.LG

TL;DR: 提出一种在线自适应方法，将风险评分流转换为多个审核队列，无需标签且能实时运行，通过核密度估计和尾质量曲线满足容量约束，减少阈值抖动。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如top-K或手动调整阈值）在将风险评分流转换为审核队列时存在不足，需要一种标签无关、能适应动态变化、支持多队列路由且满足明确容量约束的实时解决方案。

Method: 对评分流拟合在线自适应核密度估计，将密度转换为尾质量曲线以满足容量约束，并将结果"捕捉"到跨带宽检测到的持久密度谷值，支持滑动窗口或指数遗忘的实时操作。

Result: 在合成、漂移、多模态流上，该方法实现了竞争性的容量遵守能力，同时减少了阈值抖动，每次事件更新成本为O(G)，每个活动占用恒定内存。

Conclusion: 该方法提供了一种有效的标签无关实时解决方案，用于将风险评分流转换为审核队列，在满足容量约束的同时保持稳定的阈值决策。

Abstract: We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity

</details>


### [9] [How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness](https://arxiv.org/abs/2601.14519)
*Giulio Rossolini*

Main category: cs.LG

TL;DR: 研究提出概率框架评估对抗攻击能否代表随机噪声下的鲁棒性，发现对抗扰动并非总是随机噪声鲁棒性的有效代理


<details>
  <summary>Details</summary>
Motivation: 对抗攻击被广泛用于评估模型鲁棒性，但其作为随机扰动鲁棒性代理的有效性存在争议。需要探究对抗扰动是否能代表相同幅度随机噪声下的鲁棒性，还是仅反映最坏情况的异常事件

Method: 引入概率度量量化方向性偏置扰动分布下的噪声风险，参数κ在均匀噪声和对抗方向之间插值。提出在统计上更接近均匀噪声的对抗攻击策略，在ImageNet和CIFAR-10上系统评估常用攻击方法

Result: 实验系统评估了广泛使用的攻击方法，揭示了对抗攻击成功何时有意义地反映噪声风险，何时会失败，为安全导向的评估提供指导

Conclusion: 对抗扰动并非总是随机噪声鲁棒性的有效估计器，需要更谨慎地使用对抗攻击进行安全评估，提出的框架能更准确地评估模型在统计相关扰动下的鲁棒性

Abstract: Adversarial attacks are widely used to evaluate model robustness, yet their validity as proxies for robustness to random perturbations remains debated. We ask whether an adversarial perturbation provides a representative estimate of robustness under random noise of the same magnitude, or instead reflects an atypical worst-case event. To this end, we introduce a probabilistic metric that quantifies noisy risk with respect to directionally biased perturbation distributions, parameterized by a concentration factor $κ$ that interpolates between isotropic noise and adversarial direction. Using this framework, we study the limits of adversarial perturbations as estimators of noisy risk by proposing an attack strategy designed to operate in regimes statistically closer to uniform noise. Experiments on ImageNet and CIFAR-10 systematically benchmark widely used attacks, highlighting when adversarial success meaningfully reflects noisy risk and when it fails, thereby informing their use in safety-oriented evaluation.

</details>


### [10] [Place with Intention: An Empirical Attendance Predictive Study of Expo 2025 Osaka, Kansai, Japan](https://arxiv.org/abs/2601.14570)
*Xiaojie Yang,Dizhi Huang,Hangli Ge,Masahiro Sano,Takeaki Ohdake,Kazuma Hatano,Noboru Koshizuka*

Main category: cs.LG

TL;DR: 提出基于Transformer的框架，利用预约动态（门票预订及更新）作为参观者出席意图的代理，用于大规模国际活动（如大阪世博会）的出席预测，避免多源外部数据依赖。


<details>
  <summary>Details</summary>
Motivation: 大规模国际活动（如2025大阪世博会）的准确出席预测对交通、人流和服务管理至关重要。现有方法依赖多源外部数据（天气、交通、社交媒体）来提高准确性，但在历史数据不足时可能导致不可靠结果。

Method: 提出Transformer框架，利用预约动态（门票预订及时间窗口内的更新）作为参观者出席意图的代理。构建包含入场记录和预约动态的数据集，在单通道（总出席）和双通道（东、西门分开）设置下评估模型。采用编码器-解码器结构、逆风格嵌入和自适应融合模块。

Result: 结果显示，分别建模东、西门能持续提高预测准确性，特别是对短期和中期预测。消融研究证实了编码器-解码器结构、逆风格嵌入和自适应融合模块的重要性。

Conclusion: 预约动态为大规模国际活动的出席预测提供了实用且信息丰富的基础，避免了多源数据整合的复杂性，同时通过预约模式隐式捕捉了天气、促销等外部影响。

Abstract: Accurate forecasting of daily attendance is vital for managing transportation, crowd flows, and services at large-scale international events such as Expo 2025 Osaka, Kansai, Japan. However, existing approaches often rely on multi-source external data (such as weather, traffic, and social media) to improve accuracy, which can lead to unreliable results when historical data are insufficient. To address these challenges, we propose a Transformer-based framework that leverages reservation dynamics, i.e., ticket bookings and subsequent updates within a time window, as a proxy for visitors' attendance intentions, under the assumption that such intentions are eventually reflected in reservation patterns. This design avoids the complexity of multi-source integration while still capturing external influences like weather and promotions implicitly embedded in reservation dynamics. We construct a dataset combining entrance records and reservation dynamics and evaluate the model under both single-channel (total attendance) and two-channel (separated by East and West gates) settings. Results show that separately modeling East and West gates consistently improves accuracy, particularly for short- and medium-term horizons. Ablation studies further confirm the importance of the encoder-decoder structure, inverse-style embedding, and adaptive fusion module. Overall, our findings indicate that reservation dynamics offer a practical and informative foundation for attendance forecasting in large-scale international events.

</details>


### [11] [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 该研究评估了使用大语言模型（LLMs）生成反事实解释（CFEs）在临床数据集上的表现，发现微调后的LLMs能产生高质量、临床可操作的反事实，可用于干预设计和数据增强。


<details>
  <summary>Details</summary>
Motivation: 反事实解释能提供以人为中心的解释性，识别改变机器学习模型预测所需的最小、可操作的改变。然而，传统优化方法生成的CFEs在临床场景中可能缺乏语义连贯性和可操作性，需要探索LLMs在生成高质量反事实方面的潜力。

Method: 使用多种LLMs（GPT-4零样本/少样本、BioMistral-7B、LLaMA-3.1-8B）在预训练和微调配置下生成反事实解释。基于多模态AI-READI临床数据集，从干预质量、特征多样性和增强效果三个维度评估CFs。与DiCE、CFNOW、NICE等优化基线方法进行比较。

Result: 微调后的LLMs（特别是LLaMA-3.1-8B）能产生高可信度（达99%）、强有效性（达0.99）且具有现实可修改特征调整的反事实。在标签稀缺场景下，LLM生成的CFs用于数据增强能显著恢复分类器性能，在三种稀缺场景下平均F1恢复率达20%。相比传统优化方法，LLMs能生成更临床可操作、语义更连贯的反事实。

Conclusion: LLM驱动的反事实解释在可解释干预设计和传感器数字健康领域的数据高效模型训练方面具有广阔前景。SenseCF框架通过微调LLM生成有效的反事实解释，并补充不平衡数据集中的少数类，能提升模型训练效果、增强模型鲁棒性和预测性能。

Abstract: Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.
  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance

</details>


### [12] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 本文提出一种基于深度学习的个性化血糖预测方法，通过患者特定数据提高预测准确性，相比传统通用模型能更好地处理个体差异，显著改善不良事件预测能力。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴血糖监测设备和移动健康应用的普及，准确的血糖预测对于增强自动化胰岛素输送和决策支持系统至关重要。传统通用模型无法充分处理个体差异，需要个性化方法来提高预测准确性和实时响应能力。

Method: 采用深度学习框架，结合患者特定数据进行个性化建模。比较留一受试者交叉验证与微调策略，评估其对患者特定动态的建模能力。实验对比多模态患者特定方法与传统的仅使用连续血糖监测数据方法，并进行消融研究确定有效个性化所需的最小数据量。

Result: 个性化模型显著改善不良事件预测，实现更精确及时的干预。多模态患者特定方法优于传统CGM-only方法。消融研究确定了有效个性化所需的最小训练数据量，这对实际应用中数据收集受限的情况具有重要意义。

Conclusion: 自适应个性化血糖预测模型在下一代糖尿病管理中具有重要潜力，特别是在可穿戴和移动健康平台中，能够增强面向消费者的糖尿病护理解决方案。

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [13] [Fine-Grained Traceability for Transparent ML Pipelines](https://arxiv.org/abs/2601.14971)
*Liping Chen,Mujie Liu,Haytham Fayek*

Main category: cs.LG

TL;DR: FG-Trac是一个模型无关的框架，为机器学习流水线提供可验证的细粒度样本级追溯能力，通过加密承诺确保数据使用历史的完整性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统多为多阶段流水线，但现有透明度机制通常只在模型层面运作，缺乏可验证的样本级追溯能力。这使得实践者和用户无法确定特定样本是否被使用、何时被处理，以及相应记录是否随时间保持完整。

Method: FG-Trac定义了一个明确的机制来捕获和验证预处理和训练过程中的样本生命周期事件，基于训练检查点计算贡献分数，并将这些追溯记录锚定到防篡改的加密承诺中。该框架无需修改模型架构或训练目标即可集成。

Result: 在卷积神经网络和多模态图学习流水线上的实验表明，FG-Trac在保持预测性能的同时，使机器学习系统能够提供关于单个样本在模型执行过程中如何被使用和传播的可验证证据。

Conclusion: FG-Trac为机器学习流水线建立了可验证的细粒度样本级追溯能力，通过加密承诺确保数据使用历史的完整性和可审计性，为机器学习系统的透明度和问责制提供了重要工具。

Abstract: Modern machine learning systems are increasingly realised as multistage pipelines, yet existing transparency mechanisms typically operate at a model level: they describe what a system is and why it behaves as it does, but not how individual data samples are operationally recorded, tracked, and verified as they traverse the pipeline. This absence of verifiable, sample-level traceability leaves practitioners and users unable to determine whether a specific sample was used, when it was processed, or whether the corresponding records remain intact over time. We introduce FG-Trac, a model-agnostic framework that establishes verifiable, fine-grained sample-level traceability throughout machine learning pipelines. FG-Trac defines an explicit mechanism for capturing and verifying sample lifecycle events across preprocessing and training, computes contribution scores explicitly grounded in training checkpoints, and anchors these traces to tamper-evident cryptographic commitments. The framework integrates without modifying model architectures or training objectives, reconstructing complete and auditable data-usage histories with practical computational overhead. Experiments on a canonical convolutional neural network and a multimodal graph learning pipeline demonstrate that FG-Trac preserves predictive performance while enabling machine learning systems to furnish verifiable evidence of how individual samples were used and propagated during model execution.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution](https://arxiv.org/abs/2601.15075)
*Chen Qian,Peng Wang,Dongrui Liu,Junyao Yang,Dadi Guo,Ling Tang,Jilin Mei,Qihan Ren,Shuai Shao,Yong Liu,Jie Fu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 提出一个通用智能体归因框架，用于识别驱动智能体行为的内在因素，而不仅仅是失败归因


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在客服、网页导航、软件工程等实际应用中广泛部署，理解智能体为何采取特定行动对于问责和治理变得越来越重要。现有研究主要关注失败归因，不足以解释智能体行为背后的推理过程。

Method: 提出分层框架：在组件级别使用时序似然动态识别关键交互步骤；在句子级别使用基于扰动的分析来精确定位特定文本证据

Result: 在包括标准工具使用和内存诱导偏差等可靠性风险在内的多样化智能体场景中验证，实验结果表明该框架能可靠地识别驱动智能体行为的关键历史事件和句子

Conclusion: 该框架为构建更安全、更可问责的智能体系统迈出了关键一步，能够识别驱动智能体行为的内在因素，而不仅仅是失败归因

Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding](https://arxiv.org/abs/2601.14304)
*Juncheng Wang,Zhe Hu,Chao Xu,Siyue Ren,Yuxiang Feng,Yang Liu,Baigui Sun,Shujun Wang*

Main category: cs.CL

TL;DR: AR音频生成器早期前缀token隐含编码全局语义属性，作者提出Plan-Critic模型通过早期评估候选前缀来引导生成，在保持计算效率的同时显著提升指令跟随能力


<details>
  <summary>Details</summary>
Motivation: 自回归音频生成模型虽然能生成时序连贯的音频，但在遵循复杂文本提示（特别是描述复杂声音事件）方面表现不佳。作者发现AR模型的早期前缀token隐含编码了最终输出的全局语义属性，这揭示了一种隐式规划能力

Method: 提出Plan-Critic：一个轻量级辅助模型，使用GAE启发的目标函数训练，从部分生成中预测最终指令跟随质量。在推理时，Plan-Critic评估候选前缀，剪枝低质量轨迹，将计算资源重新分配给高潜力规划种子

Result: Plan-Critic引导的采样在CLAP分数上比AR基线提升高达10分，在AR文本到音频生成中建立了新的SOTA，同时保持与标准best-of-N解码相同的计算复杂度

Conclusion: 这项工作弥合了因果生成与全局语义对齐之间的差距，证明即使是严格的自回归模型也能进行前瞻性规划。Plan-Critic通过早期评估候选前缀实现了引导式探索，显著提升了AR音频生成器的指令跟随能力

Abstract: Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual prompts, especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10-point improvement in CLAP score over the AR baseline-establishing a new state of the art in AR text-to-audio generation-while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.

</details>


### [16] [CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents](https://arxiv.org/abs/2601.14914)
*Tianxiang Fei,Cheng Chen,Yue Pan,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: CodeDelegator：一个多智能体框架，通过角色专业化将规划与实现分离，使用持久规划者和临时编码者，引入EPSS状态分离机制防止上下文污染


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要战略规划和详细实现，但单一智能体同时处理两者会导致调试痕迹和中间失败污染上下文，损害长期性能

Method: 提出CodeDelegator多智能体框架：1) 持久Delegator负责战略监督、任务分解、规范编写和进度监控；2) 为每个子任务实例化新的Coder智能体，提供干净的上下文；3) 引入EPSS机制隔离每个Coder的执行状态同时保持全局一致性

Result: 在多个基准测试上的实验证明了CodeDelegator在不同场景下的有效性

Conclusion: 通过将规划与实现分离，CodeDelegator解决了单一智能体上下文污染问题，提高了长期任务的性能

Abstract: Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.

</details>


### [17] [RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)](https://arxiv.org/abs/2601.15129)
*Yishu Wei,Adam E. Flanders,Errol Colak,John Mongan,Luciano M Prevedello,Po-Hao Chen,Henrique Min Ho Lee,Gilberto Szarf,Hamilton Shoji,Jason Sho,Katherine Andriole,Tessa Cook,Lisa C. Adams,Linda C. Chu,Maggie Chung,Geraldine Brusca-Augello,Djeven P. Deva,Navneet Singh,Felipe Sanchez Tijmes,Jeffrey B. Alpert,Elsie T. Nguyen,Drew A. Torigian,Kate Hanneman,Lauren K Groner,Alexander Phan,Ali Islam,Matias F. Callejas,Gustavo Borges da Silva Teles,Faisal Jamal,Maryam Vazirabad,Ali Tejani,Hari Trivedi,Paulo Kuriki,Rajesh Bhayana,Elana T. Benishay,Yi Lin,Yifan Peng,George Shih*

Main category: cs.CL

TL;DR: 开发了一个包含200个胸部X光片、12个标注标签的基准数据集，采用AI辅助专家标注流程，旨在为临床有用的多模态LLM工具提供高质量评估标准


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在放射学考试中表现与放射科住院医师相当，但开发临床有用的工具需要由领域专家策划的高质量基准数据集

Method: 使用GPT-4o从报告中提取异常发现，通过本地LLM映射到12个基准标签；采用AI辅助标注流程，让17名放射科专家评估AI建议的标签，每个X光片由3名专家评审

Result: 创建了包含200个胸部X光片、12个基准标签的公开数据集，其中381个X光片获得至少2名专家"完全同意"AI标签；分为100个公开样本和100个保留样本用于独立评估

Conclusion: 成功开发了高质量胸部X光基准数据集和AI辅助标注流程，支持放射科医生高效标注，为多模态LLM工具的临床评估提供可靠标准

Abstract: Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked "Agree all", "Agree mostly" or "Disagree" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected "Agree All" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [18] [TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks](https://arxiv.org/abs/2601.14550)
*Tailai Cheng,Kejia Chen,Lingyun Chen,Liding Zhang,Yue Zhang,Yao Ling,Mahdi Hamad,Zhenshan Bing,Fan Wu,Karan Sharma,Alois Knoll*

Main category: cs.RO

TL;DR: TacUMI：集成多模态传感器的手持演示系统，用于接触丰富任务的演示数据采集与分割


<details>
  <summary>Details</summary>
Motivation: 复杂长时程操作任务需要任务分解，仅依赖视觉和本体感知信息难以揭示事件转换，需要高质量多模态数据和鲁棒分割方法

Method: 基于UMI手持演示设备，集成ViTac传感器、力扭矩传感器和姿态跟踪器，构建紧凑的机器人兼容夹爪设计；提出多模态分割框架，利用时序模型检测语义有意义的事件边界

Result: 在电缆安装任务上评估，分割准确率超过90%，多模态集成带来显著改进

Conclusion: TacUMI为接触丰富任务的多模态演示数据可扩展采集和分割建立了实用基础

Abstract: Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.

</details>


### [19] [A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control](https://arxiv.org/abs/2601.14628)
*Weiyu Guo,He Zhang,Pengteng Li,Tiefu Cai,Ziyang Chen,Yandong Guo,Xiao He,Yongkui Yang,Ying Sun,Hui Xiong*

Main category: cs.RO

TL;DR: NeuroVLA：首个在物理机器人上部署的神经形态视觉-语言-动作框架，模仿生物神经系统结构，实现动态稳定性、反射响应和时间记忆能力


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略难以复制生物运动的动态稳定性、反射响应和时间记忆能力，而生物系统能从稀疏经验中快速学习技能

Method: 采用系统级仿生设计：高层模型规划目标，自适应小脑模块使用高频传感器反馈稳定运动，仿生脊髓层执行快速动作生成

Result: 实现最先进性能，在神经形态处理器上仅消耗0.4w能量，消除机械臂抖动，展现时间记忆能力，安全反射响应时间小于20毫秒

Conclusion: NeuroVLA框架成功模仿生物神经系统结构，在物理机器人上实现了生物运动特性，为机器人控制提供了新的仿生解决方案

Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.

</details>


### [20] [Risk Estimation for Automated Driving](https://arxiv.org/abs/2601.15018)
*Leon Tolksdorf,Arturo Tejada,Jonas Bauernfeind,Christian Birkner,Nathan van de Wouw*

Main category: cs.RO

TL;DR: 提出一种结合碰撞概率估计与碰撞严重性的通用风险估计方法，用于自动驾驶车辆的运动规划和安全评估


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全评估需要准确的风险估计，现有方法要么依赖经验模型，要么采用严重近似，缺乏通用性和准确性

Method: 结合最新的碰撞概率估计技术与碰撞严重性概念，开发通用风险估计方法，可为不同碰撞类型分配个体化严重性函数

Result: 提出的方法计算效率高，适用于实时运动规划应用，并为高斯不确定性提供了示例实现代码

Conclusion: 该方法为自动驾驶提供了准确、通用的风险估计框架，能够根据不确定性水平和潜在碰撞严重性保持安全距离

Abstract: Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.

</details>


### [21] [V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks](https://arxiv.org/abs/2601.15164)
*Yaru Liu,Ao-bo Wang,Nanyang Ye*

Main category: cs.RO

TL;DR: V-CAGE是一个闭环框架，用于生成物理合理、语义对齐的大规模操作数据集，通过上下文感知实例化、分层指令分解和VLM验证循环来解决合成数据中的物理不现实和语义不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法存在三个主要问题：生成的场景物理上不合理；语言驱动程序经常"成功"但不满足任务语义；高级指令需要接地到可执行动作序列。这些限制了从合成数据学习长时程具身行为的效果。

Method: 1. 上下文感知实例化机制：通过动态维护禁止空间区域地图，防止物体穿插，确保在杂乱环境中可达、无冲突的配置。2. 分层指令分解模块：将高级目标分解为组合动作基元，促进连贯的长时程规划。3. VLM验证循环：作为视觉批评者，在每个子任务后执行严格拒绝采样，过滤"静默失败"。

Result: 实验表明，V-CAGE生成的数​​据集具有优越的物理和语义保真度，与非验证基线相比，显著提高了下游策略的成功率和泛化能力。

Conclusion: V-CAGE通过结合几何一致性强制、分层规划和语义验证，有效解决了合成数据生成中的关键挑战，为大规模、高质量的具身操作数据集创建提供了可行方案。

Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently "succeed" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., "get ready for work") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out "silent failures" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.

</details>
