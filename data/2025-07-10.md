<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: 提出了一种事件自动编码器架构，用于高效压缩和重建事件数据，提升实时边缘计算中的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统帧式视觉系统在高动态环境中存在运动模糊、高延迟和数据冗余问题，而事件相机虽能异步捕捉亮度变化，但稀疏和噪声事件流对目标检测构成挑战。

Method: 采用卷积编码的事件自动编码器架构，结合自适应阈值选择和轻量级分类器，以降低计算复杂度并提高识别精度。

Result: 在SEFD数据集上，模型精度与YOLO-v4相当，但参数减少35.5倍；在嵌入式平台上实现8至44.8 FPS的高帧率，性能优于现有技术87.84倍。

Conclusion: 该模型显著提升了事件视觉性能，适用于低功耗、高速的实时边缘计算应用。

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [2] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为KSCU的新方法，专注于扩散模型的关键步骤进行概念遗忘，以平衡遗忘效果与生成能力的保留。


<details>
  <summary>Details</summary>
Motivation: 现有概念遗忘方法在平衡遗忘效果与生成能力保留方面存在不足，需要一种更有效的方法来解决这一问题。

Method: KSCU方法通过分析扩散模型的逐步采样特性，专注于对最终结果影响最大的关键步骤进行微调，减少参数更新次数。

Result: 实验表明，KSCU能有效防止生成不良图像，同时更好地保留模型的生成能力。

Conclusion: KSCU方法在概念遗忘任务中表现出色，为扩散模型的安全使用提供了新思路。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出一种基于网格数据结构的过拟合检测与预防方法，通过拉普拉斯算子二阶导数计算熵损失，优化超参数以减少振荡。


<details>
  <summary>Details</summary>
Motivation: 解决回归任务中数据过拟合问题，特别是在网格数据结构中，避免训练数据的振荡和噪声干扰。

Method: 利用原始训练网格计算导数作为真实标签，在交错网格上计算训练数据的导数以检测振荡，通过拉普拉斯算子损失优化超参数。

Result: 通过最小化熵损失，有效减少了训练模型的振荡，无需从训练数据中拆分测试点。

Conclusion: 该方法在网格数据回归中有效预防过拟合，通过拉普拉斯算子作为替代测试指标，简化了训练过程。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [4] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 论文提出两种新的损失函数MV-InfoNCE和MV-DHEL，解决多视图对比学习中存在的四个关键问题，并在实验中验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习方法在处理多视图时存在优化冲突、视图交互不足、对齐与均匀性耦合等问题，限制了多视图学习的潜力。

Method: 提出MV-InfoNCE和MV-DHEL两种损失函数，前者同时建模所有视图交互，后者解耦对齐与均匀性并支持多模态扩展。

Result: 在ImageNet1K等数据集上表现优于现有方法，且能有效利用多视图和高维嵌入空间。

Conclusion: 新方法显著提升了多视图对比学习的性能，并解决了维度崩溃问题。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [5] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed是一个生成多表时间序列电子健康记录（EHR）数据的框架，首次模拟原始EHR的复杂结构和时间动态，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和监管限制，真实EHR数据难以共享，需要生成合成数据。现有方法仅生成专家选择的特征，无法模拟原始EHR的复杂性。

Method: RawMed采用基于文本的表示和压缩技术，最小化预处理，生成多表时间序列数据。

Result: 在两个开源EHR数据集上验证，RawMed在保真度和实用性上优于基线模型。

Conclusion: RawMed为合成EHR数据提供了更接近原始数据的解决方案，并提出了新的评估框架。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [6] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: PERK是一种参数高效的方法，通过测试时学习将长上下文编码到轻量级适配器中，显著提升了长上下文推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文推理中信息检索的挑战，同时避免传统元学习方法的高内存消耗问题。

Method: 采用双嵌套优化循环：内循环将上下文编码到低秩适配器（LoRA），外循环学习如何利用适配器进行推理。

Result: 在多个长上下文推理任务中，PERK显著优于基线方法，性能提升最高达90%（小模型）和27%（大模型）。

Conclusion: PERK在推理时更高效且鲁棒性强，适用于复杂推理和长上下文场景。

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [7] [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
*Xin Su,Sungduk Yu,Phillip Howard,Steven Bethard*

Main category: cs.CL

TL;DR: 论文提出了一种基于SCATE框架的时间归一化方法，通过代码生成任务实现，利用大语言模型生成可执行代码，并通过数据增强提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于ISO-TimeML的系统表达能力有限，难以处理复杂时间表达式，如组合式、事件相关和多跨度表达。

Method: 将时间归一化任务重新定义为基于SCATE框架的代码生成任务，开发了可执行的SCATE Python库，并利用大语言模型生成代码。通过数据增强合成大规模标注数据。

Result: 实验表明，基于增强数据训练的小型本地部署模型性能优于大语言模型，实现了高效、准确且可解释的时间归一化。

Conclusion: 该方法为时间归一化提供了新的解决方案，兼具实用性和性能优势。

Abstract: Time normalization is the task of converting natural language temporal
expressions into machine-readable representations. It underpins many downstream
applications in information retrieval, question answering, and clinical
decision-making. Traditional systems based on the ISO-TimeML schema limit
expressivity and struggle with complex constructs such as compositional,
event-relative, and multi-span time expressions. In this work, we introduce a
novel formulation of time normalization as a code generation task grounded in
the SCATE framework, which defines temporal semantics through symbolic and
compositional operators. We implement a fully executable SCATE Python library
and demonstrate that large language models (LLMs) can generate executable SCATE
code. Leveraging this capability, we develop an automatic data augmentation
pipeline using LLMs to synthesize large-scale annotated data with code-level
validation. Our experiments show that small, locally deployable models trained
on this augmented data can achieve strong performance, outperforming even their
LLM parents and enabling practical, accurate, and interpretable time
normalization.

</details>


### [8] [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
*Gennadii Iakovlev*

Main category: cs.CL

TL;DR: 该研究提出了一种通过人工智能检测政治人物互动的新方法，用于衡量精英极化，并分析了英国、匈牙利和意大利的数据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析政治人物在议会演讲中的互动和情感表达，量化精英极化现象。

Method: 利用人工智能检测演讲中的发言者和被提及者，评估情感温度，构建精英极化指数。

Result: 生成的指数对选举活动、政党危机等事件反应良好，可用于欧盟范围内的长期研究。

Conclusion: 该方法为精英极化的量化研究提供了新工具，适用于跨国比较和时间序列分析。

Abstract: This project introduces a new measure of elite polarization via actor and
subject detection using artificial intelligence. I identify when politicians
mention one another in parliamentary speeches, note who is speaking and who is
being addressed, and assess the emotional temperature behind these evaluations.
This maps how elites evaluate their various out-parties, allowing us to create
an index of mutual out-party hostility, that is, elite polarization. While I
analyzed polarization data over the past four decades for the UK, and two
decades for Hungary and Italy, my approach lays the groundwork for a
twenty-year, EU-wide time-series dataset on elite polarization. I obtain the
results that can be aggregated by party and quarter. The resulting index
demonstrates a good face validity: it reacts to events such as electoral
campaigns, country- and party-level crises, and to parties losing and assuming
power.

</details>


### [9] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在处理确定和不确定预测时，其隐藏状态的动态变化基本一致，表明不确定性并未显著影响推理过程。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何内部表示和处理不确定性，以检测不确定性和防止幻觉。

Method: 使用Tuned Lens（Logit Lens的变体）分析11个数据集和5个模型的层间概率轨迹，比较确定和不确定预测的动态。

Result: 确定和不确定预测的轨迹一致，均在相似层出现置信度突然增加的现象。但能力更强的模型可能学会以不同方式处理不确定性。

Conclusion: 研究挑战了简单方法检测不确定性的可行性，并展示了可解释性方法在研究不确定性影响推理中的应用。

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [10] [SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments](https://arxiv.org/abs/2507.06564)
*Tianshun Li,Tianyi Huai,Zhen Li,Yichun Gao,Haoang Li,Xinhu Zheng*

Main category: cs.RO

TL;DR: SkyVLN框架结合视觉-语言导航（VLN）和非线性模型预测控制（NMPC），提升无人机在复杂城市环境中的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法在动态3D环境中表现不足，SkyVLN通过结合大型语言模型（LLMs）和NMPC，提高导航的准确性和鲁棒性。

Method: SkyVLN使用多模态导航代理，包括细粒度空间语言化器和历史路径记忆机制，结合NMPC模块实现动态避障。

Result: 实验表明，SkyVLN在新环境中显著提高了导航成功率和效率。

Conclusion: SkyVLN为无人机在复杂城市环境中的自主导航提供了有效解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across
various sectors, driven by their mobility and adaptability. This paper
introduces SkyVLN, a novel framework integrating vision-and-language navigation
(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in
complex urban environments. Unlike traditional navigation methods, SkyVLN
leverages Large Language Models (LLMs) to interpret natural language
instructions and visual observations, enabling UAVs to navigate through dynamic
3D spaces with improved accuracy and robustness. We present a multimodal
navigation agent equipped with a fine-grained spatial verbalizer and a history
path memory mechanism. These components allow the UAV to disambiguate spatial
contexts, handle ambiguous instructions, and backtrack when necessary. The
framework also incorporates an NMPC module for dynamic obstacle avoidance,
ensuring precise trajectory tracking and collision prevention. To validate our
approach, we developed a high-fidelity 3D urban simulation environment using
AirSim, featuring realistic imagery and dynamic urban elements. Extensive
experiments demonstrate that SkyVLN significantly improves navigation success
rates and efficiency, particularly in new and unseen environments.

</details>


### [11] [AI Space Cortex: An Experimental System for Future Era Space Exploration](https://arxiv.org/abs/2507.06574)
*Thomas Touma,Ersin Daş,Erica Tevere,Martin Feather,Ksenia Kolcio,Maurice Prather,Alberto Candela,Ashish Goel,Erik Kramer,Hari Nayar,Lorraine Fesq,Joel W. Burdick*

Main category: cs.RO

TL;DR: REASIMO项目旨在为NASA的COLDTech计划开发AI辅助的自主系统，以应对冰卫星任务中的通信延迟和恶劣环境挑战。


<details>
  <summary>Details</summary>
Motivation: 冰卫星任务（如欧罗巴和恩克拉多斯）面临通信延迟、能源限制和辐射等挑战，需要自主系统以应对异常并完成任务目标。

Method: 结合AI技术和预训练行为，开发了一个智能框架，用于检测和恢复异常，并在NASA喷气推进实验室的测试平台上进行自主采样操作测试。

Result: 成功展示了AI辅助自主系统在模拟冰卫星表面条件下的操作能力。

Conclusion: REASIMO框架为未来冰卫星任务提供了强大的自主性解决方案，能够在不依赖地球通信的情况下完成任务。

Abstract: Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)
effort contributes to NASA's Concepts for Ocean worlds Life Detection
Technology (COLDTech) program, which explores science platform technologies for
ocean worlds such as Europa and Enceladus. Ocean world missions pose
significant operational challenges. These include long communication lags,
limited power, and lifetime limitations caused by radiation damage and hostile
conditions. Given these operational limitations, onboard autonomy will be vital
for future Ocean world missions. Besides the management of nominal lander
operations, onboard autonomy must react appropriately in the event of
anomalies. Traditional spacecraft rely on a transition into 'safe-mode' in
which non-essential components and subsystems are powered off to preserve
safety and maintain communication with Earth. For a severely time-limited Ocean
world mission, resolutions to these anomalies that can be executed without
Earth-in-the-loop communication and associated delays are paramount for
completion of the mission objectives and science goals. To address these
challenges, the REASIMO effort aims to demonstrate a robust level of
AI-assisted autonomy for such missions, including the ability to detect and
recover from anomalies, and to perform missions based on pre-trained behaviors
rather than hard-coded, predetermined logic like all prior space missions. We
developed an AI-assisted, personality-driven, intelligent framework for control
of an Ocean world mission by combining a mix of advanced technologies. To
demonstrate the capabilities of the framework, we perform tests of autonomous
sampling operations on a lander-manipulator testbed at the NASA Jet Propulsion
Laboratory, approximating possible surface conditions such a mission might
encounter.

</details>


### [12] [Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments](https://arxiv.org/abs/2507.06750)
*Tohid Kargar Tasooji,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 提出了一种分布式容错协同定位框架，通过自适应事件触发通信策略提升多机器人系统在对抗环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失或通信受限环境中，传统定位方法易受传感器操纵和通信干扰等对抗攻击影响，需提升系统鲁棒性。

Method: 采用自适应事件触发通信策略，动态调整通信阈值，并结合实时感知和通信质量优化性能。

Result: 实验证明该算法在对抗环境中显著优于传统方法，定位精度和通信效率更高。

Conclusion: 该方法提升了多机器人系统的可扩展性、可靠性和容错能力，适用于现实复杂环境。

Abstract: In multi-robot systems (MRS), cooperative localization is a crucial task for
enhancing system robustness and scalability, especially in GPS-denied or
communication-limited environments. However, adversarial attacks, such as
sensor manipulation, and communication jamming, pose significant challenges to
the performance of traditional localization methods. In this paper, we propose
a novel distributed fault-tolerant cooperative localization framework to
enhance resilience against sensor and communication disruptions in adversarial
environments. We introduce an adaptive event-triggered communication strategy
that dynamically adjusts communication thresholds based on real-time sensing
and communication quality. This strategy ensures optimal performance even in
the presence of sensor degradation or communication failure. Furthermore, we
conduct a rigorous analysis of the convergence and stability properties of the
proposed algorithm, demonstrating its resilience against bounded adversarial
zones and maintaining accurate state estimation. Robotarium-based experiment
results show that our proposed algorithm significantly outperforms traditional
methods in terms of localization accuracy and communication efficiency,
particularly in adversarial settings. Our approach offers improved scalability,
reliability, and fault tolerance for MRS, making it suitable for large-scale
deployments in real-world, challenging environments.

</details>
