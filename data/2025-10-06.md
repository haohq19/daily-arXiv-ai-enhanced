<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的视频蒸馏方法，通过输入人体姿态条件引导注意力和损失函数，将多步扩散视频模型蒸馏为少步学生模型，实现实时性能并提升视觉质量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法由于大量去噪步骤和昂贵的注意力机制导致速度缓慢，无法实现实时部署

Method: 利用输入人体姿态关键点的准确对应关系来引导注意力到相关区域，提出输入感知稀疏注意力和输入感知蒸馏损失

Result: 实现了实时性能，相比最近的音频驱动和输入驱动方法，视觉质量得到改善

Conclusion: 通过整合输入感知稀疏注意力和蒸馏损失，在保持视觉质量的同时显著提升了推理效率

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [2] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出FunLoRA方法，使用秩1矩阵和函数增强的动态条件机制来解决生成模型持续学习中的灾难性遗忘问题，无需依赖合成数据重训练。


<details>
  <summary>Details</summary>
Motivation: 解决深度生成模型持续学习中的两个关键问题：训练时间随任务增加而不可控，以及依赖合成数据导致性能长期退化。

Method: 设计基于低秩适应(LoRA)的新型条件机制，专门使用秩1矩阵，通过精心选择的函数增强重参数化矩阵的秩，称为FunLoRA。

Result: 在从头训练的流匹配模型中，FunLoRA超越了基于扩散模型的最先进方法，达到更高的分类准确率，同时只需要一小部分内存成本和采样时间。

Conclusion: FunLoRA通过动态条件机制保证生成模型避免灾难性遗忘，只需在当前任务数据上训练，实现了参数高效微调。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [3] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: AdaRD-Key是一个无需训练的关键帧采样模块，通过最大化相关性-多样性目标来提升长视频理解性能，在弱对齐情况下能自动切换到多样性模式。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型依赖均匀采样会忽略关键时刻，而现有关键帧选择方法要么采用严格的时间间隔错过细粒度线索，要么只关注视觉多样性而忽略查询相关性。

Method: 提出统一的相关性-多样性最大体积目标，结合查询条件相关性评分和对数行列式多样性组件，并采用轻量级相关性感知门控机制处理弱对齐情况。

Result: 在LongVideoBench和Video-MME上的广泛实验展示了最先进的性能，特别是在长视频上表现优异，且计算高效，可在单个GPU上实时运行。

Conclusion: AdaRD-Key是一个无需训练、即插即用的高效关键帧采样方法，能显著提升长视频理解能力，解决了现有方法的局限性。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [4] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 提出Med-K2N方法解决医学图像多模态合成中的三个关键挑战：异质模态贡献建模、融合质量控制和模态身份一致性保持，通过三个协作模块和因果模态身份模块实现K到N的灵活模态重建。


<details>
  <summary>Details</summary>
Motivation: 临床诊断需要灵活的多模态重建能力，但面临三个挑战：不同模态对目标任务的异质贡献如何建模？如何控制融合质量防止噪声信息退化？如何在多输出生成中保持模态身份一致性？

Method: 将多模态医学数据视为序列帧，设计三个协作模块：PreWeightNet进行全局贡献评估，ThresholdNet进行自适应过滤，EffiWeightNet计算有效权重；同时提出因果模态身份模块(CMIM)通过视觉语言建模建立生成图像与目标模态描述间的因果约束。

Result: 在多个基准测试中，Med-K2N显著优于现有最先进方法。

Conclusion: 所提出的Med-K2N方法通过渐进增强和因果约束机制，有效解决了医学图像多模态合成中的关键挑战，为临床诊断提供了更灵活的模态重建能力。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [5] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 该论文研究了多事件文本到视频生成中的事件转换控制问题，提出了MEve评估套件，并系统分析了OpenSora和CogVideoX模型，发现早期干预去噪步骤和块级模型层对多事件视频生成至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展到多事件生成时忽略了事件转换的内在因素，本文旨在回答多事件提示何时何地控制事件转换这一核心问题。

Method: 引入MEve自建提示套件评估多事件文本到视频生成，对OpenSora和CogVideoX两个代表性模型家族进行系统研究。

Result: 广泛实验证明了在去噪步骤早期干预和块级模型层的重要性，揭示了多事件视频生成的关键因素。

Conclusion: 该研究突出了未来模型中多事件调节的可能性，为多事件文本到视频生成提供了重要见解。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文提出了一种改进的在线共形预测方法，通过使用更广泛的函数来量化预测区间的相关性，而不是简单的二元评估，从而产生更窄的预测区间同时保持覆盖有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的在线共形预测方法在阈值更新步骤中只关注预测区间的有效性（真实值是否在区间内），而忽略了区间的相关性。本文旨在利用这个被忽视的方面来改进方法。

Method: 提出在阈值更新步骤中用更广泛的函数类替代二元评估（内部/外部），这些函数使用真实值来量化预测区间的相关性，有助于防止阈值突变，可能产生更窄的预测区间。

Result: 在真实世界数据集上的实验结果表明，这些函数能够产生比现有OCP方法更紧的区间，同时保持覆盖有效性。

Conclusion: 通过量化预测区间相关性的函数来增强阈值更新步骤，可以产生更窄的预测区间，同时维持覆盖保证，改进了在线共形预测方法的性能。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [7] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: 提出一种基于校准误差估计的主动学习采集函数，通过查询校准误差最高的样本来改进模型的不确定性校准，从而降低泛化误差和校准误差。


<details>
  <summary>Details</summary>
Motivation: 传统基于不确定性的主动学习方法在深度神经网络中效果不佳，因为模型的不确定性通常未校准，这会影响采集函数的有效性，导致泛化性能差和校准误差高。

Method: 提出新的采集函数，在利用DNN不确定性之前先估计校准误差并查询校准误差最高的样本。使用核校准误差估计器处理协变量偏移，并理论证明该方法能保证未标注池和测试数据上的校准误差有界。

Result: 实验表明，该方法在池式主动学习设置中优于其他采集函数基线，具有更低的校准误差和泛化误差。

Conclusion: 通过主动学习校准误差高的样本可以有效改进模型的不确定性校准，从而提升模型的泛化性能和校准性能。

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [8] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Wahlström*

Main category: cs.LG

TL;DR: 使用机器学习和可穿戴传感器数据（惯性传感器和肌电图）来估计单臂哑铃弯举时的自觉用力程度评分，随机森林分类器表现最佳，准确率达到41.4%，±1 RPE准确率为85.9%。


<details>
  <summary>Details</summary>
Motivation: 准确估计自觉用力程度评分可以增强抗阻训练，通过个性化反馈和预防伤害来改善训练效果。

Method: 收集了69组超过1000次重复的自定义数据集，使用可穿戴惯性传感器和肌电图传感器数据，提取统计特征训练机器学习模型。

Result: 随机森林分类器表现最好，精确准确率为41.4%，±1 RPE准确率为85.9%。肌电图数据的加入略微提高了模型准确性，但受到数据质量和放置敏感性的限制。离心重复时间是最强的RPE预测因子。

Conclusion: 研究证明了基于可穿戴传感器的RPE估计的可行性，并指出了提高模型泛化能力的关键挑战。

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 提出基于多模态大语言模型的框架，处理视觉感知、定位和车辆数据，为驾驶员生成自然语言警报，提升电动汽车在城市驾驶中的安全性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车与智能电网的集成需要确保驾驶员、车辆与环境之间安全可解释的交互，但目前缺乏有效的多模态数据处理和自然语言警报生成方法。

Method: 结合YOLOv8视觉感知、地理编码定位和CAN总线遥测数据，使用多模态大语言模型处理传感器数据并生成自然语言驾驶员警报。

Result: 使用真实世界数据验证，框架能有效生成针对行人、自行车和其他车辆接近等关键情境的上下文感知警报。

Conclusion: 大语言模型在电动出行中具有作为辅助工具的潜力，通过可扩展的车队协调、电动汽车负载预测和交通感知能源规划，同时惠及交通系统和电网。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [10] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 提出了一种新的可能性概念作为Zadeh传统可能性理论的替代方案，专注于现实世界事件的可能性计算，基于事件的先决条件和约束条件。


<details>
  <summary>Details</summary>
Motivation: 替代1978年Zadeh提出的标准可能性概念，专注于现实事件的可能性评估，特别适用于规划问题。

Method: 将事件视为具有促成其发生的先决条件和阻碍其发生的约束条件，可能性计算基于先决条件成立概率和约束条件不成立概率的函数。

Result: 开发了一种新的可能性理论，可用于规划问题中评估不同计划的可行性，通过车辆路线规划示例进行了说明。

Conclusion: 这种可能性模型可能正确捕捉了人类关于计划推理的正常思维方式，并具有未来应用的潜力。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 该研究首次将生存分析应用于对话AI鲁棒性评估，分析36,951个对话轮次发现：突发语义漂移会灾难性增加对话失败风险，而渐进语义漂移则显著降低失败风险并延长对话寿命。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注静态基准和单轮评估，无法捕捉真实对话中的时间动态退化特征，需要新的方法来理解LLM在多轮对话中的鲁棒性。

Method: 采用生存分析框架，包括Cox比例风险模型、加速失效时间模型和随机生存森林方法，对9个最先进LLM的对话数据进行建模，将失败视为时间到事件的过程。

Result: AFT交互模型表现最佳，具有优异的区分度和校准能力。研究发现突发语义漂移会急剧增加对话失败风险，而渐进语义漂移则具有保护作用，能显著延长对话持续时间。

Conclusion: 生存分析是评估LLM鲁棒性的强大范式，为设计弹性对话代理提供了具体见解，并挑战了对话AI系统中语义一致性必要性的普遍假设。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [12] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 开发了一个基于检索增强生成(RAG)的系统，用于查询英国NICE临床指南，该系统通过混合嵌入机制实现高精度检索，显著提升了回答的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: NICE临床指南篇幅冗长、数量庞大，在时间紧迫的医疗环境中难以有效利用，需要开发能够快速提供精确信息的自然语言查询系统。

Method: 采用检索增强生成(RAG)架构，结合混合嵌入机制，从10,195个文本块中检索相关信息，使用LLMs生成回答。

Result: 系统检索性能优异：平均倒数排名0.814，首块召回率81%，前10块召回率99.1%；RAG增强模型在忠实度方面提升64.7个百分点至99.5%，显著优于医疗专用模型Meditron3-8B(43%)。

Conclusion: RAG是医疗领域应用生成式AI的有效、可靠且可扩展的方法，能够以成本效益高的方式提供医疗指南访问。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>
