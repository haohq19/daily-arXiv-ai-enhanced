<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoccerMaster: A Vision Foundation Model for Soccer Understanding](https://arxiv.org/abs/2512.11016)
*Haolin Yang,Jiayuan Rao,Haoning Wu,Weidi Xie*

Main category: cs.CV

TL;DR: SoccerMaster：首个足球视觉基础模型，通过监督多任务预训练统一处理多种足球理解任务，在各项下游任务中超越专用模型


<details>
  <summary>Details</summary>
Motivation: 足球理解具有领域特定的复杂性和独特挑战，现有方法通常依赖孤立的、任务特定的专家模型，缺乏统一框架来处理从细粒度感知到语义推理的多样化任务

Method: 提出SoccerMaster足球视觉基础模型，通过监督多任务预训练统一框架；开发自动数据标注管道生成可扩展的空间标注，整合现有数据集构建SoccerFactory预训练数据资源

Result: SoccerMaster在多种下游任务中持续超越任务特定的专家模型，展示了其广度和优越性

Conclusion: SoccerMaster作为首个足球特定视觉基础模型，成功统一了多样化理解任务，为足球视觉理解提供了全面解决方案，数据、代码和模型将公开可用

Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.

</details>


### [2] [E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring](https://arxiv.org/abs/2512.11076)
*Jack Brady,Andrew Dailey,Kristen Schang,Zo Vic Shong*

Main category: cs.CV

TL;DR: 本文综述了基于事件相机在城市动态研究中的应用，分析了其优势、挑战及机器学习应用，并提出了多传感器融合方案。


<details>
  <summary>Details</summary>
Motivation: 传统城市监测方法存在局限性，需要更好的技术来理解城市动态。事件相机作为一种新兴传感器，具有独特优势（如低光工作能力），有望改善城市动态研究。

Method: 通过文献综述方法，分析事件相机的工作原理、应用场景、优势挑战以及机器学习应用，并提出多传感器融合方案（事件相机+红外、LiDAR、振动传感器等）。

Result: 事件相机能够捕捉重要信息同时保护隐私，适合城市动态研究。多传感器融合可以增强事件相机能力并克服其现有挑战。

Conclusion: 事件相机是研究城市动态的有前景媒介，结合多传感器融合技术可以进一步提升城市监测能力。

Abstract: Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.

</details>


### [3] [HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning](https://arxiv.org/abs/2512.11534)
*Yiqing Yang,Kin-Man Lam*

Main category: cs.CV

TL;DR: 提出端到端可训练的帧选择框架，通过链式思维生成任务特定查询向量，结合集合级优化和师生互学习，显著提升视频理解性能


<details>
  <summary>Details</summary>
Motivation: 传统视频关键帧选择方法存在两个主要问题：1）独立评分方法导致选择帧在时间上聚集且视觉冗余；2）使用MLLM生成的静态伪标签训练轻量级选择器，无法动态适应任务目标

Method: 1）使用链式思维引导小型语言模型生成任务特定隐式查询向量；2）定义包含相关性、覆盖率和冗余度的连续集合级目标函数，通过Gumbel-Softmax进行可微分优化；3）采用师生互学习，通过KL散度对齐学生选择器（SLM）和教师推理器（MLLM）的帧重要性分布

Result: 在Video-MME、LongVideoBench、MLVU和NExT-QA等多个基准测试中，该方法显著优于现有方法

Conclusion: 提出的端到端可训练框架解决了传统帧选择方法的局限性，通过动态适应任务目标、集合级优化和师生互学习，实现了更优的关键帧选择性能

Abstract: Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.

</details>


### [4] [DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation](https://arxiv.org/abs/2512.11465)
*Mohamed Abdelsamad,Michael Ulrich,Bin Yang,Miao Zhang,Yakov Miron,Abhinav Valada*

Main category: cs.CV

TL;DR: DOS是一个自监督学习框架，通过仅在可观察点蒸馏语义相关性软映射来学习3D点云表示，避免信息泄露并提供比离散token分配更丰富的监督。


<details>
  <summary>Details</summary>
Motivation: 3D点云的自监督学习面临几何不规则、重建易走捷径和语义分布不平衡等挑战，需要一种更有效的表示学习方法。

Method: 提出DOS框架：1）仅在可观察点蒸馏语义相关性软映射，避免掩码区域信息泄露；2）引入Zipfian原型和Zipf-Sinkhorn算法，强制原型使用遵循幂律分布并调节目标软映射的锐度。

Result: 在nuScenes、Waymo、SemanticKITTI、ScanNet和ScanNet200等多个基准测试中，DOS在语义分割和3D目标检测任务上超越了当前最先进方法，且无需额外数据或标注。

Conclusion: 可观察点软映射蒸馏为学习鲁棒的3D表示提供了一个可扩展且有效的范式，解决了自监督学习中语义不平衡和信息泄露等关键问题。

Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.

</details>


### [5] [Reconstruction as a Bridge for Event-Based Visual Question Answering](https://arxiv.org/abs/2512.11510)
*Hanyue Lou,Jiayi Zhou,Yang Zhang,Boyu Li,Yi Wang,Guangnan Ye,Boxin Shi*

Main category: cs.CV

TL;DR: 提出FRT和ART方法，通过重建将事件相机数据与多模态大语言模型结合，并创建EvQA基准进行评估，在事件视觉理解上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机在挑战性视觉条件下具有独特优势，但需要与基于帧的模型兼容。当前缺乏将事件数据与多模态大语言模型有效结合的方法，且缺少客观评估基准。

Method: 提出两种方法：1) 基于帧的重建与标记化(FRT)方法，通过重建作为桥梁；2) 自适应重建与标记化(ART)方法，利用事件稀疏性提高效率。同时创建EvQA基准，包含来自22个公共数据集的1000个事件-Q&A对。

Result: 实验表明，提出的方法在EvQA基准上实现了最先进的性能，证明了多模态大语言模型在事件视觉理解中的巨大潜力。

Conclusion: 通过重建作为桥梁，成功将事件相机与多模态大语言模型结合，提出的方法在事件视觉理解任务上表现出色，为事件视觉与语言模型的融合提供了有效解决方案。

Abstract: Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.

</details>


### [6] [Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection](https://arxiv.org/abs/2512.11683)
*Qiushi Guo*

Main category: cs.CV

TL;DR: 提出Depth Copy Paste，一种多模态深度感知的数据增强框架，通过复制全身人物实例并粘贴到语义兼容的场景中，生成多样且物理一致的人脸检测训练样本。


<details>
  <summary>Details</summary>
Motivation: 传统复制粘贴增强方法由于前景提取不准确、场景几何不一致和背景语义不匹配，往往产生不真实的合成图像。需要解决这些限制来提升人脸检测系统在遮挡、光照变化和复杂环境下的鲁棒性。

Method: 1) 使用BLIP和CLIP联合评估语义和视觉一致性，自动检索最适合给定前景人物的背景图像；2) 集成SAM3进行精确分割，使用Depth-Anything提取非遮挡可见人物区域；3) 引入深度引导滑动窗口放置机制，在背景深度图上搜索具有最佳深度连续性和尺度对齐的粘贴位置。

Result: 实验表明Depth Copy Paste能提供更多样和真实的训练数据，相比传统复制粘贴和无深度增强方法，在下游人脸检测任务中带来显著性能提升。

Conclusion: Depth Copy Paste通过多模态语义匹配、高质量前景提取和深度感知放置，生成了物理一致且视觉逼真的合成图像，有效提升了人脸检测系统的鲁棒性。

Abstract: Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [On the failure of ReLU activation for physics-informed machine learning](https://arxiv.org/abs/2512.11184)
*Conor Rowan*

Main category: cs.LG

TL;DR: 本文分析了ReLU激活函数在物理信息机器学习中表现不佳的原因，发现自动微分无法正确处理不连续场的导数，导致梯度计算错误。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习使用控制微分方程训练神经网络，激活函数的选择影响性能。已有研究表明ReLU在基准微分方程上表现不佳，但具体原因尚不清楚，本文旨在诊断ReLU表现差的原因。

Method: 通过分析ReLU的分段线性特性，研究其在二阶微分方程中的局限性，并进一步分析其在仅含一阶导数的变分问题中的失败。重点考察自动微分（PyTorch）如何处理不连续场的导数，以及这如何影响物理信息损失的梯度计算。

Result: 发现ReLU失败的根本原因在于其二阶导数问题：虽然损失函数可能只涉及一阶导数，但训练过程中的自动微分需要计算二阶导数。PyTorch的自动微分无法正确表征不连续场的导数，导致物理信息损失的梯度被错误指定。

Conclusion: ReLU在物理信息机器学习中表现不佳的主要原因是其分段线性特性导致的不连续性，使得自动微分无法正确处理导数计算，从而影响训练梯度。这解释了为什么sigmoid、tanh和swish等平滑激活函数表现更好。

Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.

</details>


### [8] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 提出梯度投影框架，用于在扩散模型训练中系统性地排除敏感概念级特征，防止记忆化风险


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的记忆化带来安全和知识产权风险，传统方法只能防止特定训练样本的过拟合，无法系统性地阻止概念级特征的内部化

Method: 梯度投影框架：在反向传播过程中识别并切除与禁止属性嵌入对齐的训练信号，将梯度更新投影到敏感特征嵌入空间的正交补空间

Result: 框架显著减少记忆化，同时严格保持生成质量和语义保真度，可无缝集成到标准扩散模型训练流程

Conclusion: 通过将记忆控制重新定义为选择性学习，为IP安全和隐私保护的生成AI建立了新范式

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [9] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态暂停低重要性token的KV更新，减少KV缓存大小55-67%而不损失生成质量


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文生成时KV缓存内存占用过高的问题，为内存受限的部署提供实用解决方案

Method: 基于滑动注意力窗口识别低重要性token，采用可逆软冻结机制暂停其KV更新，所有token保存在GPU外存储中可按需恢复；引入亚线性冻结调度，冻结时长随重复检测次数亚线性增长

Result: 在LLaMA-3 8B上实验显示，主动KV缓存大小减少55-67%，保持生成质量并通过needle-in-haystack检索测试

Conclusion: ASR-KF-EGR是一种无需训练、架构无关的推理时框架，为长上下文LLM的内存受限部署提供了实用解决方案

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [10] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 本文提出了一种基于对称性的扩散策略引导框架，将等变扩散策略与强化学习结合，通过利用几何对称性提高样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 等变扩散策略结合了扩散模型的生成能力和几何对称性的泛化优势，但直接用标准强化学习进行微调会忽略对称性，导致样本效率低和不稳定。需要开发能够利用对称性的引导框架。

Method: 理论证明等变扩散过程的等变性，推导出群不变潜在噪声MDP，提出基于对称性的引导框架，比较标准、等变和近似等变强化学习策略。

Result: 实验表明利用对称性进行引导能显著提高样本效率、防止价值发散，即使在演示数据极少的情况下也能实现强大的策略改进，同时确定了严格等变性在对称性破坏下的实际边界。

Conclusion: 对称性感知的引导框架为等变扩散策略的强化学习微调提供了理论基础和实践指导，在保持对称性优势的同时实现了高效的策略改进。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [11] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分短期异常（应忽略）和持久性异常（应响应）来改进时间序列预测，在异常条件下提升性能的同时保持正常数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列预测中，有些异常事件具有持久影响需要响应，有些则是短期噪声应该忽略。传统预测模型无法区分这两类异常，要么对噪声过度反应，要么错过重要的分布变化。

Method: 提出Co-TSFA对比学习正则化框架：1）生成仅输入增强和输入-输出增强，分别建模预测无关和预测相关的异常；2）引入潜在输出对齐损失，将表示变化与预测变化关联；3）鼓励对无关扰动保持不变性，同时对有意义的分布变化保持敏感性。

Result: 在Traffic和Electricity基准数据集以及真实现金需求数据集上的实验表明，Co-TSFA在异常条件下显著提升预测性能，同时在正常数据上保持准确性。

Conclusion: Co-TSFA通过对比学习框架有效区分预测相关和无关的异常，解决了传统模型在异常处理上的不足，为时间序列预测中的异常鲁棒性提供了新方法。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 该论文首次系统比较了经典优化方法（整数线性规划ILP）与AI方法（PatternBoost变压器学习和PPO强化学习）在No-Three-In-Line问题上的表现，发现ILP在19×19网格内可保证最优解，而AI方法在较小网格上具有竞争力，混合方法是最有前景的方向。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line问题是组合几何中的著名问题，经典方法如整数线性规划（ILP）虽然能保证最优解，但随着网格规模增大面临指数级复杂度。机器学习方法为模式近似提供了有前景的替代方案，但缺乏系统比较。

Method: 首次将PatternBoost变压器学习和PPO强化学习应用于该问题，并与传统ILP方法进行比较。ILP用于获取可证明的最优解，AI方法用于探索模式近似。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格上获得完美解，但在11×11网格上因约束违反而失败。

Conclusion: 经典优化方法对于精确解仍然必不可少，而AI方法在较小实例上具有竞争力。混合方法结合两者优势，是扩展到更大问题规模的最有前景方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise](https://arxiv.org/abs/2512.11282)
*Qingsen Ma,Dianyun Wang,Ran Jing,Yujun Sun,Zhenbo Xu*

Main category: cs.CL

TL;DR: CIP是一种轻量级即插即用的因果提示框架，通过构建实体、动作和事件之间的因果关系序列来减少大语言模型在处理长噪声检索上下文时的幻觉问题，提高事实基础和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长而嘈杂的检索上下文时经常产生幻觉，因为它们依赖虚假相关性而非真正的因果关系。需要一种方法来改善模型的事实基础和可解释性。

Method: CIP框架构建实体、动作和事件之间的因果关系序列，并将其注入提示中引导推理朝向因果相关证据。通过因果干预和反事实推理，抑制非因果推理路径。

Result: 在包括GPT-4o、Gemini 2.0 Flash和Llama 3.1在内的七个主流语言模型上，CIP将可归因率提高2.6分，因果一致性得分提高0.38，有效信息密度提高四倍，并将端到端响应延迟降低高达55.1%。

Conclusion: 因果推理可能成为提高大语言模型可解释性、稳定性和效率的有前景范式。CIP框架通过因果引导有效减少幻觉并提升推理质量。

Abstract: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.

</details>


### [14] [CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare](https://arxiv.org/abs/2512.11437)
*Akash Ghosh,Srivarshinee Sridhar,Raghav Kaushik Ravi,Muhsin Muhsin,Sriparna Saha,Chirag Agarwal*

Main category: cs.CL

TL;DR: CLINIC是一个全面的多语言医疗基准测试，用于评估语言模型在医疗领域的可信度，涵盖5个关键维度、18个任务和15种语言，揭示了模型在事实准确性、偏见、隐私等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型主要基于高资源语言训练，难以应对中低资源语言的医疗查询复杂性，阻碍了其在全球多语言医疗环境中的实际应用，需要可靠的可信度评估方法。

Method: 开发CLINIC基准测试，系统评估语言模型在医疗领域的5个可信度维度：真实性、公平性、安全性、鲁棒性和隐私性，通过18个多样化任务覆盖15种语言和多个医疗主题。

Result: 评估显示语言模型在事实正确性方面存在困难，在不同人口统计和语言群体中表现出偏见，容易受到隐私泄露和对抗攻击的影响。

Conclusion: CLINIC基准测试为提升语言模型在全球多语言医疗环境中的覆盖范围和安全奠定了基础，揭示了当前模型的局限性并指出了改进方向。

Abstract: Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.

</details>


### [15] [Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction](https://arxiv.org/abs/2512.11502)
*Kai Golan Hashiloni,Brenda Kasabe Nokai,Michal Shevach,Esthy Shemesh,Ronit Bartin,Anna Bergrin,Liran Harel,Nachum Dershowitz,Liat Nadai Arad,Kfir Bar*

Main category: cs.CL

TL;DR: 提出一个基于DictaBERT 2.0的希伯来语医疗语言模型，用于从电子健康记录中提取结构化临床时间线，构建患者旅程。模型通过500万份去标识化医院记录持续预训练，在两个新数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 需要从希伯来语电子健康记录中提取结构化临床时间线来构建患者旅程，但缺乏专门针对希伯来语医疗文本的模型。

Method: 基于DictaBERT 2.0架构，在超过500万份去标识化医院记录上进行持续预训练。引入词汇适应技术提高标记效率，并创建了两个新的数据集（内科/急诊科和肿瘤科）用于评估事件时间关系提取。

Result: 模型在两个新数据集上都取得了强劲性能。词汇适应提高了标记效率，去标识化处理不会影响下游任务性能，支持隐私保护的模型开发。

Conclusion: 成功开发了一个有效的希伯来语医疗语言模型，能够从电子健康记录中提取临床时间线。模型在伦理限制下可供研究使用，证明了隐私保护与模型性能可以兼顾。

Abstract: We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control](https://arxiv.org/abs/2512.11047)
*Haoran Jiang,Jin Chen,Qingwen Bu,Li Chen,Modi Shi,Yanjie Zhang,Delong Li,Chuanzhe Suo,Chuang Wang,Zhihui Peng,Hongyang Li*

Main category: cs.RO

TL;DR: 提出WholeBodyVLA框架，通过视觉-语言-动作统一学习从低成本无动作视频获取全身运动知识，结合专门的运动控制策略，实现人形机器人大空间运动操作任务


<details>
  <summary>Details</summary>
Motivation: 现有方法在运动操作任务中缺乏操作感知的运动能力，限制了机器人的工作空间，无法执行大空间运动操作。这源于两个挑战：1) 人形机器人遥操作数据稀缺导致运动操作知识获取困难；2) 现有RL控制器精度和稳定性有限导致运动命令执行不可靠

Method: 1) 提出统一潜在学习框架，使视觉-语言-动作系统能从低成本无动作的自我中心视频中学习；2) 设计高效人类数据收集管道扩展数据集；3) 提出专门的运动操作导向RL策略，针对前进、转向、下蹲等核心运动进行精确稳定控制；4) 整合为WholeBodyVLA统一框架

Result: 在AgiBot X2人形机器人上验证，相比先前基线提升21.3%。表现出强大的泛化能力和高扩展性，能够执行广泛的任务范围

Conclusion: WholeBodyVLA是首个实现大空间人形机器人运动操作的统一框架，通过结合视频学习和专门控制策略，解决了现有方法的局限性，在人形机器人运动操作领域取得了显著进展

Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.

</details>


### [17] [Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance](https://arxiv.org/abs/2512.11173)
*Tzu-Hsien Lee,Fidan Mahmudova,Karthik Desingh*

Main category: cs.RO

TL;DR: 提出基于物体中心的模仿学习框架，让四足移动机械臂仅使用机载RGB相机实现最后一米导航，达到可操作的精确定位，无需深度、激光雷达或地图先验。


<details>
  <summary>Details</summary>
Motivation: 现有RGB导航系统通常只有米级精度，不适合移动机械臂操作前的精确定位需求，导致操作策略超出训练分布而频繁失败。

Method: 使用目标图像、多视角RGB观测和文本提示作为导航策略输入，通过语言驱动分割模块和空间评分矩阵解码器提供显式物体定位和相对位姿推理。

Result: 在未见过的目标物体上，边缘对齐成功率73.47%，物体对齐成功率96.94%，证明仅用类别级数据和RGB观测即可实现精确最后一米导航。

Conclusion: 无需深度、激光雷达或地图先验即可实现类别级的精确最后一米导航，为统一移动操作提供了可扩展的途径。

Abstract: Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/

</details>


### [18] [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](https://arxiv.org/abs/2512.11773)
*Britton Jordan,Jordan Thompson,Jesse F. d'Almeida,Hao Li,Nithesh Kumar,Susheela Sharma Stern,Ipek Oguz,Robert J. Webster,Daniel Brown,Alan Kuntz,James Ferguson*

Main category: cs.RO

TL;DR: ProbeMDE：一种用于单目深度估计的成本感知主动感知框架，通过结合RGB图像和稀疏本体感知测量，利用集成模型预测密集深度图，并通过SVGD选择最优测量位置以减少不确定性。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在手术等挑战性环境中存在不确定性和不准确性问题，因为纹理缺失表面、镜面反射和遮挡常见。需要一种方法在保持准确性的同时最小化所需的测量次数。

Method: 提出ProbeMDE框架：1）使用MDE模型集成，基于RGB图像和稀疏本体感知测量预测密集深度图；2）通过集成方差量化预测不确定性；3）利用SVGD在梯度图上选择最优测量位置，避免模式崩溃。

Result: 在模拟和物理实验中验证，在中央气道阻塞手术模型上优于基线方法，在标准深度估计指标上实现更高准确性，同时最小化所需的本体感知测量次数。

Conclusion: ProbeMDE通过主动感知和不确定性量化，有效提高了单目深度估计在手术等挑战性环境中的准确性，为机器人感知提供了实用工具。

Abstract: Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.

</details>


### [19] [AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797)
*Junjie Ye,Rong Xue,Basile Van Hoorick,Pavel Tokmakov,Muhammad Zubair Irshad,Yue Wang,Vitor Guizilini*

Main category: cs.RO

TL;DR: AnchorDream：基于预训练视频扩散模型的机器人数据合成方法，通过机器人运动渲染锚定实体约束，从少量演示生成大规模多样化数据集，显著提升模仿学习性能


<details>
  <summary>Details</summary>
Motivation: 大规模多样化机器人演示数据收集是模仿学习的主要瓶颈，真实数据采集成本高，模拟器多样性和保真度有限且存在明显的模拟到真实差距。现有生成方法要么只改变视觉外观而不创造新行为，要么存在实体不一致导致运动不自然的问题

Method: 引入AnchorDream，一种实体感知的世界模型，重新利用预训练视频扩散模型进行机器人数据合成。该方法通过机器人运动渲染条件化扩散过程，锚定实体约束防止幻觉，同时合成与机器人运动学一致的对象和环境。仅需少量人类遥操作演示即可扩展为大规模、多样化、高质量数据集，无需显式环境建模

Result: 实验表明生成的数据在下游策略学习中带来一致改进：在模拟器基准测试中相对提升36.4%，在真实世界研究中性能提升近一倍。这些结果证明将生成世界模型基于机器人运动为扩展模仿学习提供了实用路径

Conclusion: 将生成世界模型基于机器人运动为扩展模仿学习提供了实用路径。AnchorDream通过实体感知的数据合成方法，有效解决了机器人演示数据稀缺的问题，显著提升了模仿学习的性能表现

Abstract: The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.

</details>
