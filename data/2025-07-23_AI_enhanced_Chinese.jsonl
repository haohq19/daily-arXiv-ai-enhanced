{"id": "2507.15874", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15874", "abs": "https://arxiv.org/abs/2507.15874", "authors": ["Yin Wu", "Daniel Slieter", "Vivek Subramanian", "Ahmed Abouelazm", "Robin Bohn", "J. Marius Z\u00f6llner"], "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM", "comment": null, "summary": "The growing number of ADAS-equipped vehicles has led to a dramatic increase\nin driving data, yet most of them capture routine driving behavior. Identifying\nand understanding safety-critical corner cases within this vast dataset remains\na significant challenge. Braking events are particularly indicative of\npotentially hazardous situations, motivating the central question of our\nresearch: Why does a vehicle brake? Existing approaches primarily rely on\nrule-based heuristics to retrieve target scenarios using predefined condition\nfilters. While effective in simple environments such as highways, these methods\nlack generalization in complex urban settings. In this paper, we propose a\nnovel framework that leverages Large Language Model (LLM) for scenario\nunderstanding and reasoning. Our method bridges the gap between low-level\nnumerical signals and natural language descriptions, enabling LLM to interpret\nand classify driving scenarios. We propose a dual-path scenario retrieval that\nsupports both category-based search for known scenarios and embedding-based\nretrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate\nevaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.\nExperimental results show that our method outperforms rule-based baselines and\ngeneralizes well to OOD scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u5206\u6790\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u7279\u522b\u662f\u5236\u52a8\u4e8b\u4ef6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u68c0\u7d22\u65b9\u6cd5\u5904\u7406\u5df2\u77e5\u548c\u672a\u77e5\u573a\u666f\uff0c\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740ADAS\u8f66\u8f86\u6570\u91cf\u589e\u957f\uff0c\u9a7e\u9a76\u6570\u636e\u6025\u5267\u589e\u52a0\uff0c\u4f46\u5927\u591a\u6570\u6570\u636e\u6355\u83b7\u7684\u662f\u5e38\u89c4\u9a7e\u9a76\u884c\u4e3a\u3002\u5728\u6d77\u91cf\u6570\u636e\u4e2d\u8bc6\u522b\u548c\u7406\u89e3\u5b89\u5168\u5173\u952e\u7684\u6781\u7aef\u60c5\u51b5\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7406\u89e3\"\u8f66\u8f86\u4e3a\u4ec0\u4e48\u5236\u52a8\"\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\u7684\u65b0\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c06\u4f4e\u7ea7\u6570\u503c\u4fe1\u53f7\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u4f7fLLM\u80fd\u591f\u89e3\u91ca\u548c\u5206\u7c7b\u9a7e\u9a76\u573a\u666f\u3002\u8bbe\u8ba1\u4e86\u53cc\u8def\u5f84\u573a\u666f\u68c0\u7d22\u673a\u5236\uff1a\u652f\u6301\u57fa\u4e8e\u7c7b\u522b\u7684\u5df2\u77e5\u573a\u666f\u641c\u7d22\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u672a\u77e5\u5206\u5e03\u5916(OOD)\u573a\u666f\u68c0\u7d22\u3002", "result": "\u5728Argoverse 2\u4f20\u611f\u5668\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6784\u5efa\u4e86\u573a\u666f\u6807\u6ce8\u6570\u636e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728OOD\u573a\u666f\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u5206\u6790\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u7279\u522b\u662f\u5236\u52a8\u4e8b\u4ef6\u3002\u53cc\u8def\u5f84\u68c0\u7d22\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u5df2\u77e5\u548c\u672a\u77e5\u573a\u666f\u7684\u5904\u7406\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16151", "abs": "https://arxiv.org/abs/2507.16151", "authors": ["Yasser Ashraf", "Ahmed Sharshar", "Velibor Bojkovic", "Bin Gu"], "title": "SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities", "comment": null, "summary": "Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by\naccumulating light intensities at each pixel, offering ultra-high energy\nefficiency and exceptional temporal resolution. Unlike event cameras, which\nrecord changes in light intensity to capture motion, spike cameras provide even\nfiner spatiotemporal resolution and a more precise representation of continuous\nchanges. In this paper, we introduce the first video action recognition (VAR)\ndataset using spike camera, alongside synchronized RGB and thermal modalities,\nto enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By\npreserving the inherent sparsity and temporal precision of spiking data, our\nthree datasets offer a unique platform for exploring multimodal video\nunderstanding and serve as a valuable resource for directly comparing spiking,\nthermal, and RGB modalities. This work contributes a novel dataset that will\ndrive research in energy-efficient, ultra-low-power video understanding,\nspecifically for action recognition tasks using spike-based data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u57fa\u4e8e\u8109\u51b2\u76f8\u673a\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u540c\u65f6\u5305\u542bRGB\u548c\u70ed\u6210\u50cf\u6a21\u6001\uff0c\u4e3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u8d85\u4f4e\u529f\u8017\u89c6\u9891\u7406\u89e3\u7814\u7a76\u3002", "motivation": "\u8109\u51b2\u76f8\u673a\u4f5c\u4e3a\u751f\u7269\u542f\u53d1\u7684\u89c6\u89c9\u4f20\u611f\u5668\u5177\u6709\u8d85\u9ad8\u80fd\u6548\u548c\u5353\u8d8a\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u6765\u652f\u6301\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7814\u7a76\u548c\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u7684\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u8109\u51b2\u76f8\u673a\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u540c\u65f6\u5305\u542b\u540c\u6b65\u7684RGB\u548c\u70ed\u6210\u50cf\u6a21\u6001\u6570\u636e\uff0c\u4fdd\u6301\u8109\u51b2\u6570\u636e\u56fa\u6709\u7684\u7a00\u758f\u6027\u548c\u65f6\u95f4\u7cbe\u5ea6\uff0c\u4e3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u5e73\u53f0\uff0c\u80fd\u591f\u76f4\u63a5\u6bd4\u8f83\u8109\u51b2\u3001\u70ed\u6210\u50cf\u548cRGB\u4e09\u79cd\u6a21\u6001\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u63a8\u52a8\u57fa\u4e8e\u8109\u51b2\u6570\u636e\u7684\u8282\u80fd\u3001\u8d85\u4f4e\u529f\u8017\u89c6\u9891\u7406\u89e3\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u65b9\u9762\uff0c\u4e3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u63d0\u4f9b\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2507.16191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16191", "abs": "https://arxiv.org/abs/2507.16191", "authors": ["Fansheng Zeng", "Bineng Zhong", "Haiying Xia", "Yufei Tan", "Xiantao Hu", "Liangtao Shi", "Shuxiang Song"], "title": "Explicit Context Reasoning with Supervision for Visual Tracking", "comment": null, "summary": "Contextual reasoning with constraints is crucial for enhancing temporal\nconsistency in cross-frame modeling for visual tracking. However, mainstream\ntracking algorithms typically associate context by merely stacking historical\ninformation without explicitly supervising the association process, making it\ndifficult to effectively model the target's evolving dynamics. To alleviate\nthis problem, we propose RSTrack, which explicitly models and supervises\ncontext reasoning via three core mechanisms. \\textit{1) Context Reasoning\nMechanism}: Constructs a target state reasoning pipeline, converting\nunconstrained contextual associations into a temporal reasoning process that\npredicts the current representation based on historical target states, thereby\nenhancing temporal consistency. \\textit{2) Forward Supervision Strategy}:\nUtilizes true target features as anchors to constrain the reasoning pipeline,\nguiding the predicted output toward the true target distribution and\nsuppressing drift in the context reasoning process. \\textit{3) Efficient State\nModeling}: Employs a compression-reconstruction mechanism to extract the core\nfeatures of the target, removing redundant information across frames and\npreventing ineffective contextual associations. These three mechanisms\ncollaborate to effectively alleviate the issue of contextual association\ndivergence in traditional temporal modeling. Experimental results show that\nRSTrack achieves state-of-the-art performance on multiple benchmark datasets\nwhile maintaining real-time running speeds. Our code is available at\nhttps://github.com/GXNU-ZhongLab/RSTrack.", "AI": {"tldr": "RSTrack\u662f\u4e00\u4e2a\u89c6\u89c9\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff08\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u524d\u5411\u76d1\u7763\u3001\u9ad8\u6548\u72b6\u6001\u5efa\u6a21\uff09\u89e3\u51b3\u4f20\u7edf\u8ddf\u8e2a\u4e2d\u4e0a\u4e0b\u6587\u5173\u8054\u53d1\u6563\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8ddf\u8e2a\u7b97\u6cd5\u4ec5\u901a\u8fc7\u5806\u53e0\u5386\u53f2\u4fe1\u606f\u6765\u5173\u8054\u4e0a\u4e0b\u6587\uff0c\u7f3a\u4e4f\u5bf9\u5173\u8054\u8fc7\u7a0b\u7684\u663e\u5f0f\u76d1\u7763\uff0c\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u76ee\u6807\u7684\u6f14\u5316\u52a8\u6001\uff0c\u5bfc\u81f4\u8de8\u5e27\u5efa\u6a21\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRSTrack\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u4e0a\u4e0b\u6587\u63a8\u7406\u673a\u5236\uff1a\u6784\u5efa\u76ee\u6807\u72b6\u6001\u63a8\u7406\u7ba1\u9053\uff0c\u5c06\u65e0\u7ea6\u675f\u7684\u4e0a\u4e0b\u6587\u5173\u8054\u8f6c\u6362\u4e3a\u57fa\u4e8e\u5386\u53f2\u72b6\u6001\u9884\u6d4b\u5f53\u524d\u8868\u793a\u7684\u65f6\u95f4\u63a8\u7406\u8fc7\u7a0b\uff1b2\uff09\u524d\u5411\u76d1\u7763\u7b56\u7565\uff1a\u5229\u7528\u771f\u5b9e\u76ee\u6807\u7279\u5f81\u4f5c\u4e3a\u951a\u70b9\u7ea6\u675f\u63a8\u7406\u7ba1\u9053\uff0c\u6307\u5bfc\u9884\u6d4b\u8f93\u51fa\u8d8b\u5411\u771f\u5b9e\u76ee\u6807\u5206\u5e03\uff1b3\uff09\u9ad8\u6548\u72b6\u6001\u5efa\u6a21\uff1a\u91c7\u7528\u538b\u7f29-\u91cd\u6784\u673a\u5236\u63d0\u53d6\u76ee\u6807\u6838\u5fc3\u7279\u5f81\uff0c\u53bb\u9664\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u8fd0\u884c\u901f\u5ea6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4f20\u7edf\u65f6\u95f4\u5efa\u6a21\u4e2d\u4e0a\u4e0b\u6587\u5173\u8054\u53d1\u6563\u7684\u95ee\u9898\u3002", "conclusion": "RSTrack\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u76d1\u7763\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e09\u4e2a\u673a\u5236\u7684\u534f\u540c\u4f5c\u7528\u6709\u6548\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u8ddf\u8e2a\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16229", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16229", "abs": "https://arxiv.org/abs/2507.16229", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8bed\u97f3\u7684AI\u4ee3\u7406\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5f00\u53d1Agent PULSE\u7cfb\u7edf\u5e76\u8fdb\u884c\u8bd5\u70b9\u7814\u7a76\uff0c\u8bc1\u660e\u4e86AI\u8bed\u97f3\u52a9\u624b\u5728\u9884\u9632\u62a4\u7406\u548c\u6301\u7eed\u60a3\u8005\u76d1\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u4eba\u7fa4\u4e2d\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u53ef\u53ca\u6027\u4f18\u52bf\u3002", "motivation": "\u533b\u7597\u4fdd\u5065\u4e2d\u5b58\u5728\u7ecf\u6d4e\u548c\u53ef\u53ca\u6027\u5dee\u8ddd\uff0c\u4f20\u7edf\u7684\u6570\u5b57\u5065\u5eb7\u670d\u52a1\u96be\u4ee5\u8986\u76d6\u670d\u52a1\u4e0d\u8db3\u7684\u4eba\u7fa4\u3002\u9700\u8981\u63a2\u7d22\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3AI\u4ee3\u7406\u5982\u4f55\u5728\u4eba\u5de5\u5e72\u9884\u7ecf\u6d4e\u4e0a\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u7684\u533b\u7597\u670d\u52a1\uff0c\u7279\u522b\u662f\u5728\u9884\u9632\u62a4\u7406\u548c\u6301\u7eed\u60a3\u8005\u76d1\u6d4b\u65b9\u9762\u3002", "method": "\u5f00\u53d1\u4e86Agent PULSE\uff08\u60a3\u8005\u7406\u89e3\u548c\u8054\u7edc\u652f\u6301\u5f15\u64ce\uff09\u7cfb\u7edf\uff0c\u8fd9\u662fIBM\u7814\u7a76\u9662\u3001\u514b\u5229\u592b\u5170\u8bca\u6240\u57fa\u91d1\u4f1a\u548c\u83ab\u5c14\u8c6a\u65af\u533b\u5b66\u9662\u7684\u5408\u4f5c\u9879\u76ee\u3002\u5bf933\u540d\u708e\u75c7\u6027\u80a0\u75c5\u60a3\u8005\u8fdb\u884c\u8bd5\u70b9\u7814\u7a76\uff0c\u5206\u6790\u60a3\u8005\u5bf9AI\u9a71\u52a8\u76d1\u6d4b\u7684\u63a5\u53d7\u5ea6\u3002\u5efa\u7acb\u7ecf\u6d4e\u6a21\u578b\u5206\u6790\u6210\u672c\u6548\u76ca\uff0c\u5e76\u8bc4\u4f30\u6280\u672f\u6311\u6218\u3001\u653f\u7b56\u8003\u8651\u548c\u4f26\u7406\u6846\u67b6\u3002", "result": "\u8bd5\u70b9\u7814\u7a76\u663e\u793a70%\u7684\u60a3\u8005\u63a5\u53d7AI\u9a71\u52a8\u7684\u76d1\u6d4b\uff0c37%\u66f4\u504f\u597dAI\u76d1\u6d4b\u800c\u975e\u4f20\u7edf\u65b9\u5f0f\u3002\u6210\u672c\u6548\u7528\u5206\u6790\u663e\u793a\u5728\u5e38\u89c4\u76d1\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5c\u5728\u8282\u7ea6\u3002\u6280\u672f\u65b9\u9762\u8bc6\u522b\u4e86\u5b9e\u65f6\u5bf9\u8bddAI\u5904\u7406\u3001\u533b\u7597\u7cfb\u7edf\u96c6\u6210\u548c\u9690\u79c1\u5408\u89c4\u7b49\u6311\u6218\u3002AI\u8bed\u97f3\u4ee3\u7406\u63d0\u9ad8\u4e86\u533b\u7597\u4fdd\u5065\u7684\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u3001\u60a3\u8005\u53c2\u4e0e\u5ea6\u548c\u53ef\u53ca\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u97f3\u7684AI\u4ee3\u7406\u53ef\u4ee5\u4f5c\u4e3a\u516c\u5e73\u3001\u53ef\u6301\u7eed\u6570\u5b57\u533b\u7597\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u5165\u53e3\u70b9\u3002\u901a\u8fc7\u89e3\u51b3\u5f53\u524d\u9650\u5236\u5e76\u4f7fAI\u5f00\u53d1\u4e0e\u4f26\u7406\u548c\u76d1\u7ba1\u6846\u67b6\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u591f\u5f25\u5408\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u7ecf\u6d4e\u548c\u53ef\u53ca\u6027\u5dee\u8ddd\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u9632\u62a4\u7406\u548c\u60a3\u8005\u76d1\u6d4b\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.16228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16228", "abs": "https://arxiv.org/abs/2507.16228", "authors": ["Shreelekha Revankar", "Utkarsh Mall", "Cheng Perng Phoo", "Kavita Bala", "Bharath Hariharan"], "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing", "comment": "17 pages, 9 figures, 4 tables", "summary": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS", "AI": {"tldr": "\u7814\u7a76\u8005\u6784\u5efa\u4e86MONITRS\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710,000\u4e2aFEMA\u707e\u5bb3\u4e8b\u4ef6\u7684\u65f6\u5e8f\u536b\u661f\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\uff0c\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u7684\u707e\u5bb3\u76d1\u6d4b\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u707e\u5bb3\u54cd\u5e94\u7cfb\u7edf\u53d7\u9650\u4e8e\u96be\u4ee5\u8fdb\u5165\u53d7\u707e\u5730\u533a\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u7279\u5b9a\u707e\u5bb3\u7c7b\u578b\u3001\u4f9d\u8d56\u4eba\u5de5\u4e13\u5bb6\u89e3\u91ca\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u5177\u6709\u8db3\u591f\u65f6\u95f4\u7c92\u5ea6\u548c\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u7684\u6570\u636e\u96c6\u6765\u8ddf\u8e2a\u707e\u5bb3\u8fdb\u5c55", "method": "\u6784\u5efaMONITRS\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710,000\u4e2aFEMA\u707e\u5bb3\u4e8b\u4ef6\uff0c\u7ed3\u5408\u65f6\u5e8f\u536b\u661f\u56fe\u50cf\u3001\u65b0\u95fb\u6587\u7ae0\u7684\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u3001\u5730\u7406\u6807\u8bb0\u4f4d\u7f6e\u548c\u95ee\u7b54\u5bf9\uff0c\u5e76\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)", "result": "\u5728MONITRS\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u707e\u5bb3\u76d1\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u707e\u5bb3\u54cd\u5e94\u7cfb\u7edf\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6", "conclusion": "MONITRS\u6570\u636e\u96c6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u707e\u5bb3\u76d1\u6d4b\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u5316\u707e\u5bb3\u54cd\u5e94\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u548c\u65b0\u7684\u6027\u80fd\u57fa\u51c6"}}
{"id": "2507.16363", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.16363", "abs": "https://arxiv.org/abs/2507.16363", "authors": ["Hailin Yue", "Hulin Kuang", "Jin Liu", "Junjian Li", "Lanlan Wang", "Mengshen He", "Jianxin Wang"], "title": "Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction", "comment": null, "summary": "Accurately predicting the survival of cancer patients is crucial for\npersonalized treatment. However, existing studies focus solely on the\nrelationships between samples with known survival risks, without fully\nleveraging the value of censored samples. Furthermore, these studies may suffer\nperformance degradation in modality-missing scenarios and even struggle during\nthe inference process. In this study, we propose a bipartite patient-modality\ngraph learning with event-conditional modelling of censoring for cancer\nsurvival prediction (CenSurv). Specifically, we first use graph structure to\nmodel multimodal data and obtain representation. Then, to alleviate performance\ndegradation in modality-missing scenarios, we design a bipartite graph to\nsimulate the patient-modality relationship in various modality-missing\nscenarios and leverage a complete-incomplete alignment strategy to explore\nmodality-agnostic features. Finally, we design a plug-and-play\nevent-conditional modeling of censoring (ECMC) that selects reliable censored\ndata using dynamic momentum accumulation confidences, assigns more accurate\nsurvival times to these censored data, and incorporates them as uncensored data\ninto training. Comprehensive evaluations on 5 publicly cancer datasets showcase\nthe superiority of CenSurv over the best state-of-the-art by 3.1% in terms of\nthe mean C-index, while also exhibiting excellent robustness under various\nmodality-missing scenarios. In addition, using the plug-and-play ECMC module,\nthe mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of\nCenSurv is available at https://github.com/yuehailin/CenSurv.", "AI": {"tldr": "\u63d0\u51fa\u4e86CenSurv\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5206\u56fe\u60a3\u8005-\u6a21\u6001\u5b66\u4e60\u548c\u4e8b\u4ef6\u6761\u4ef6\u5220\u5931\u5efa\u6a21\u6765\u9884\u6d4b\u764c\u75c7\u60a3\u8005\u751f\u5b58\u671f\uff0c\u5728\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747C-index\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u53473.1%", "motivation": "\u73b0\u6709\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u7814\u7a76\u4ec5\u5173\u6ce8\u5df2\u77e5\u751f\u5b58\u98ce\u9669\u6837\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u672a\u5145\u5206\u5229\u7528\u5220\u5931\u6837\u672c\u7684\u4ef7\u503c\uff1b\u5728\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u56f0\u96be", "method": "\u8bbe\u8ba1\u53cc\u5206\u56fe\u7ed3\u6784\u5efa\u6a21\u591a\u6a21\u6001\u6570\u636e\u83b7\u53d6\u8868\u5f81\uff1b\u4f7f\u7528\u5b8c\u6574-\u4e0d\u5b8c\u6574\u5bf9\u9f50\u7b56\u7565\u63a2\u7d22\u6a21\u6001\u65e0\u5173\u7279\u5f81\uff1b\u8bbe\u8ba1\u5373\u63d2\u5373\u7528\u7684\u4e8b\u4ef6\u6761\u4ef6\u5220\u5931\u5efa\u6a21(ECMC)\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u52a8\u91cf\u7d2f\u79ef\u7f6e\u4fe1\u5ea6\u9009\u62e9\u53ef\u9760\u5220\u5931\u6570\u636e\u5e76\u5206\u914d\u66f4\u51c6\u786e\u7684\u751f\u5b58\u65f6\u95f4", "result": "\u57285\u4e2a\u516c\u5f00\u764c\u75c7\u6570\u636e\u96c6\u4e0a\uff0cCenSurv\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747C-index\u63d0\u53473.1%\uff1b\u5728\u5404\u79cd\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u9c81\u68d2\u6027\uff1bECMC\u6a21\u5757\u4f7f8\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747C-index\u63d0\u53471.3%", "conclusion": "CenSurv\u901a\u8fc7\u53cc\u5206\u56fe\u5b66\u4e60\u548c\u4e8b\u4ef6\u6761\u4ef6\u5220\u5931\u5efa\u6a21\u6709\u6548\u63d0\u5347\u4e86\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0cECMC\u6a21\u5757\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5373\u63d2\u5373\u7528\u7279\u6027"}}
{"id": "2507.16419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16419", "abs": "https://arxiv.org/abs/2507.16419", "authors": ["Ivona Krchova", "Michael Platzer", "Paul Tiwald"], "title": "Improving Predictions on Highly Unbalanced Data Using Open Source Synthetic Data Upsampling", "comment": null, "summary": "Unbalanced tabular data sets present significant challenges for predictive\nmodeling and data analysis across a wide range of applications. In many\nreal-world scenarios, such as fraud detection, medical diagnosis, and rare\nevent prediction, minority classes are vastly underrepresented, making it\ndifficult for traditional machine learning algorithms to achieve high accuracy.\nThese algorithms tend to favor the majority class, leading to biased models\nthat struggle to accurately represent minority classes. Synthetic data holds\npromise for addressing the under-representation of minority classes by\nproviding new, diverse, and highly realistic samples. This paper presents a\nbenchmark study on the use of AI-generated synthetic data for upsampling highly\nunbalanced tabular data sets.\n  We evaluate the effectiveness of an open-source solution, the Synthetic Data\nSDK by MOSTLY AI, which provides a flexible and user-friendly approach to\nsynthetic upsampling for mixed-type data. We compare predictive models trained\non data sets upsampled with synthetic records to those using standard methods,\nsuch as naive oversampling and SMOTE-NC. Our results demonstrate that synthetic\ndata can improve predictive accuracy for minority groups by generating diverse\ndata points that fill gaps in sparse regions of the feature space. We show that\nupsampled synthetic training data consistently results in top-performing\npredictive models, particularly for mixed-type data sets containing very few\nminority samples.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u4e0d\u5e73\u8861\u8868\u683c\u6570\u636e\u96c6\u7684\u9884\u6d4b\u5efa\u6a21\u6311\u6218\uff0c\u8bc4\u4f30\u4e86AI\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u5c11\u6570\u7c7b\u4e0a\u91c7\u6837\u4e2d\u7684\u6548\u679c\u3002\u901a\u8fc7\u5bf9\u6bd4\u5408\u6210\u6570\u636e\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6734\u7d20\u8fc7\u91c7\u6837\u548cSMOTE-NC\uff09\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u80fd\u591f\u663e\u8457\u63d0\u5347\u5c11\u6570\u7c7b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u7c7b\u578b\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u8bb8\u591a\u5e94\u7528\u573a\u666f\uff08\u5982\u6b3a\u8bc8\u68c0\u6d4b\u3001\u533b\u7597\u8bca\u65ad\u3001\u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b\uff09\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c11\u6570\u7c7b\u6837\u672c\u6781\u5ea6\u7f3a\u4e4f\uff0c\u5bfc\u81f4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u504f\u5411\u591a\u6570\u7c7b\uff0c\u96be\u4ee5\u51c6\u786e\u8868\u793a\u5c11\u6570\u7c7b\u3002\u9700\u8981\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\u6765\u89e3\u51b3\u5c11\u6570\u7c7b\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528MOSTLY AI\u7684\u5f00\u6e90Synthetic Data SDK\u8fdb\u884c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u8be5\u5de5\u5177\u4e3a\u6df7\u5408\u7c7b\u578b\u6570\u636e\u63d0\u4f9b\u7075\u6d3b\u4e14\u7528\u6237\u53cb\u597d\u7684\u5408\u6210\u4e0a\u91c7\u6837\u65b9\u6cd5\u3002\u5c06\u4f7f\u7528\u5408\u6210\u6570\u636e\u4e0a\u91c7\u6837\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u9884\u6d4b\u6a21\u578b\u4e0e\u4f7f\u7528\u6807\u51c6\u65b9\u6cd5\uff08\u6734\u7d20\u8fc7\u91c7\u6837\u548cSMOTE-NC\uff09\u7684\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5408\u6210\u6570\u636e\u80fd\u591f\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u7684\u7a00\u758f\u533a\u57df\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u70b9\u6765\u586b\u8865\u7a7a\u7f3a\uff0c\u4ece\u800c\u6539\u5584\u5c11\u6570\u7fa4\u4f53\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u4e0a\u91c7\u6837\u7684\u8bad\u7ec3\u6570\u636e\u6301\u7eed\u4ea7\u751f\u8868\u73b0\u6700\u4f73\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5305\u542b\u6781\u5c11\u6570\u5c11\u6570\u7c7b\u6837\u672c\u7684\u6df7\u5408\u7c7b\u578b\u6570\u636e\u96c6\u3002", "conclusion": "AI\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u662f\u89e3\u51b3\u9ad8\u5ea6\u4e0d\u5e73\u8861\u8868\u683c\u6570\u636e\u96c6\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u4e0a\u91c7\u6837\u6280\u672f\uff0c\u5408\u6210\u6570\u636e\u5728\u63d0\u5347\u5c11\u6570\u7c7b\u9884\u6d4b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16260", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16260", "abs": "https://arxiv.org/abs/2507.16260", "authors": ["Haoyue Zhang", "Jie Zhang", "Song Guo"], "title": "ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference", "comment": null, "summary": "Although vision transformers (ViT) have shown remarkable success in various\nvision tasks, their computationally expensive self-attention hinder their\ndeployment on resource-constrained devices. Token reduction, which discards\nless important tokens during forward propagation, has been proposed to enhance\nthe efficiency of transformer models. However, existing methods handle\nunimportant tokens irreversibly, preventing their reuse in subsequent blocks.\nConsidering that transformers focus on different information among blocks,\ntokens reduced in early blocks might be useful later. Furthermore, to adapt\ntransformer models for resource-constrained devices, it is crucial to strike a\nbalance between model performance and computational overhead. To address these\nchallenges, in this paper, we introduce a novel Token Freezing and Reusing\n(ToFe) framework, where we identify important tokens at each stage and\ntemporarily freeze the unimportant ones, allowing their lagged reusing at a\nlater stage. Specifically, we design a prediction module for token\nidentification and an approximate module for recovery of the frozen tokens. By\njointly optimizing with the backbone through computation budget-aware\nend-to-end training, ToFe can adaptively process the necessary tokens at each\nblock, thereby reducing computational cost while maintaining performance.\nExtensive experiments demonstrate that ToFe reduces the computational cost of\nLV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a\nbetter trade-off between performance and complexity compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faToFe\uff08Token Freezing and Reusing\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u9636\u6bb5\u8bc6\u522b\u91cd\u8981token\u5e76\u4e34\u65f6\u51bb\u7ed3\u4e0d\u91cd\u8981token\uff0c\u5141\u8bb8\u540e\u7eed\u91cd\u7528\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u89c6\u89c9Transformer\u7684\u8ba1\u7b97\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u5728LV-ViT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0cTop-1\u51c6\u786e\u7387\u4ec5\u4e0b\u964d\u4e0d\u52302%\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9Transformer\u867d\u7136\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u73b0\u6709token\u7f29\u51cf\u65b9\u6cd5\u4e0d\u53ef\u9006\u5730\u4e22\u5f03\u4e0d\u91cd\u8981token\uff0c\u65e0\u6cd5\u5728\u540e\u7eed\u5757\u4e2d\u91cd\u7528\uff0c\u800c\u8003\u8651\u5230Transformer\u5728\u4e0d\u540c\u5757\u4e2d\u5173\u6ce8\u4e0d\u540c\u4fe1\u606f\uff0c\u65e9\u671f\u88ab\u7f29\u51cf\u7684token\u53ef\u80fd\u5728\u540e\u7eed\u9636\u6bb5\u6709\u7528\u3002", "method": "\u63d0\u51faToken Freezing and Reusing\uff08ToFe\uff09\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u5728\u6bcf\u4e2a\u9636\u6bb5\u8bc6\u522b\u91cd\u8981token\u5e76\u4e34\u65f6\u51bb\u7ed3\u4e0d\u91cd\u8981token\uff1b2\uff09\u8bbe\u8ba1\u9884\u6d4b\u6a21\u5757\u8fdb\u884ctoken\u8bc6\u522b\uff1b3\uff09\u8bbe\u8ba1\u8fd1\u4f3c\u6a21\u5757\u6062\u590d\u51bb\u7ed3\u7684token\uff1b4\uff09\u901a\u8fc7\u8ba1\u7b97\u9884\u7b97\u611f\u77e5\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u4e0e\u9aa8\u5e72\u7f51\u7edc\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5728\u6bcf\u4e2a\u5757\u5904\u7406\u5fc5\u8981\u7684token\u3002", "result": "\u5728LV-ViT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0cTop-1\u51c6\u786e\u7387\u4ec5\u4e0b\u964d\u4e0d\u52302%\u3002\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u6027\u80fd\u548c\u590d\u6742\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6743\u8861\u3002", "conclusion": "ToFe\u6846\u67b6\u901a\u8fc7\u4e34\u65f6\u51bb\u7ed3\u548c\u91cd\u7528token\u7684\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709token\u7f29\u51cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u89c6\u89c9Transformer\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16279", "abs": "https://arxiv.org/abs/2507.16279", "authors": ["Junhao Su", "Feiyu Zhu", "Hengyu Shi", "Tianyang Han", "Yurui Qiu", "Junfeng Luo", "Xiaoming Wei", "Jialin Gao"], "title": "MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning in Vision Tasks", "comment": "14 pages", "summary": "Deep learning typically relies on end-to-end backpropagation for training, a\nmethod that inherently suffers from issues such as update locking during\nparameter optimization, high GPU memory consumption, and a lack of biological\nplausibility. In contrast, supervised local learning seeks to mitigate these\nchallenges by partitioning the network into multiple local blocks and designing\nindependent auxiliary networks to update each block separately. However,\nbecause gradients are propagated solely within individual local blocks,\nperformance degradation occurs, preventing supervised local learning from\nsupplanting end-to-end backpropagation. To address these limitations and\nfacilitate inter-block information flow, we propose the Momentum Auxiliary\nNetwork++ (MAN++). MAN++ introduces a dynamic interaction mechanism by\nemploying the Exponential Moving Average (EMA) of parameters from adjacent\nblocks to enhance communication across the network. The auxiliary network,\nupdated via EMA, effectively bridges the information gap between blocks.\nNotably, we observed that directly applying EMA parameters can be suboptimal\ndue to feature discrepancies between local blocks. To resolve this issue, we\nintroduce a learnable scaling bias that balances feature differences, thereby\nfurther improving performance. We validate MAN++ through extensive experiments\non tasks that include image classification, object detection, and image\nsegmentation, utilizing multiple network architectures. The experimental\nresults demonstrate that MAN++ achieves performance comparable to end-to-end\ntraining while significantly reducing GPU memory usage. Consequently, MAN++\noffers a novel perspective for supervised local learning and presents a viable\nalternative to conventional training methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Momentum Auxiliary Network++ (MAN++)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\u673a\u5236\u548c\u53ef\u5b66\u4e60\u7f29\u653e\u504f\u7f6e\u6765\u6539\u8fdb\u76d1\u7763\u5c40\u90e8\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u5b58\u5728\u53c2\u6570\u4f18\u5316\u65f6\u7684\u66f4\u65b0\u9501\u5b9a\u3001\u9ad8GPU\u5185\u5b58\u6d88\u8017\u548c\u7f3a\u4e4f\u751f\u7269\u5408\u7406\u6027\u7b49\u95ee\u9898\u3002\u867d\u7136\u76d1\u7763\u5c40\u90e8\u5b66\u4e60\u8bd5\u56fe\u901a\u8fc7\u5c06\u7f51\u7edc\u5206\u5272\u4e3a\u591a\u4e2a\u5c40\u90e8\u5757\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u68af\u5ea6\u4ec5\u5728\u5355\u4e2a\u5c40\u90e8\u5757\u5185\u4f20\u64ad\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u6cd5\u66ff\u4ee3\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u3002", "method": "\u63d0\u51faMomentum Auxiliary Network++ (MAN++)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u76f8\u90bb\u5757\u53c2\u6570\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\u5f15\u5165\u52a8\u6001\u4ea4\u4e92\u673a\u5236\uff0c\u589e\u5f3a\u7f51\u7edc\u95f4\u7684\u901a\u4fe1\u3002\u8f85\u52a9\u7f51\u7edc\u901a\u8fc7EMA\u66f4\u65b0\uff0c\u6709\u6548\u5f25\u5408\u5757\u95f4\u4fe1\u606f\u5dee\u8ddd\u3002\u540c\u65f6\u5f15\u5165\u53ef\u5b66\u4e60\u7f29\u653e\u504f\u7f6e\u6765\u5e73\u8861\u5c40\u90e8\u5757\u95f4\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u591a\u79cd\u7f51\u7edc\u67b6\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMAN++\u5728\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "MAN++\u4e3a\u76d1\u7763\u5c40\u90e8\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u662f\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5c40\u90e8\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.16635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16635", "abs": "https://arxiv.org/abs/2507.16635", "authors": ["Ali Mohamed Ali", "Luca Tirel", "Hashim A. Hashim"], "title": "Novel Multi-Agent Action Masked Deep Reinforcement Learning for General Industrial Assembly Lines Balancing Problems", "comment": null, "summary": "Efficient planning of activities is essential for modern industrial assembly\nlines to uphold manufacturing standards, prevent project constraint violations,\nand achieve cost-effective operations. While exact solutions to such challenges\ncan be obtained through Integer Programming (IP), the dependence of the search\nspace on input parameters often makes IP computationally infeasible for\nlarge-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also\nbe applied, but they frequently produce suboptimal solutions in extensive\ncases. This paper introduces a novel mathematical model of a generic industrial\nassembly line formulated as a Markov Decision Process (MDP), without imposing\nassumptions on the type of assembly line a notable distinction from most\nexisting models. The proposed model is employed to create a virtual environment\nfor training Deep Reinforcement Learning (DRL) agents to optimize task and\nresource scheduling. To enhance the efficiency of agent training, the paper\nproposes two innovative tools. The first is an action-masking technique, which\nensures the agent selects only feasible actions, thereby reducing training\ntime. The second is a multi-agent approach, where each workstation is managed\nby an individual agent, as a result, the state and action spaces were reduced.\nA centralized training framework with decentralized execution is adopted,\noffering a scalable learning architecture for optimizing industrial assembly\nlines. This framework allows the agents to learn offline and subsequently\nprovide real-time solutions during operations by leveraging a neural network\nthat maps the current factory state to the optimal action. The effectiveness of\nthe proposed scheme is validated through numerical simulations, demonstrating\nsignificantly faster convergence to the optimal solution compared to a\ncomparable model-based approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u4e1a\u88c5\u914d\u7ebf\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u548c\u8d44\u6e90\u8c03\u5ea6\u7684\u9ad8\u6548\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u6574\u6570\u89c4\u5212\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u9057\u4f20\u7b97\u6cd5\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u5f80\u5f80\u4ea7\u751f\u6b21\u4f18\u89e3\u3002\u73b0\u6709\u7684\u88c5\u914d\u7ebf\u6a21\u578b\u5927\u591a\u5bf9\u88c5\u914d\u7ebf\u7c7b\u578b\u6709\u7279\u5b9a\u5047\u8bbe\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u5de5\u4e1a\u88c5\u914d\u7ebf\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5de5\u4e1a\u88c5\u914d\u7ebf\u6570\u5b66\u6a21\u578b\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(MDP)\uff0c\u4e0d\u5bf9\u88c5\u914d\u7ebf\u7c7b\u578b\u65bd\u52a0\u5047\u8bbe\u3002\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\u8fdb\u884c\u4efb\u52a1\u548c\u8d44\u6e90\u8c03\u5ea6\u4f18\u5316\u3002\u5f15\u5165\u4e24\u79cd\u521b\u65b0\u5de5\u5177\uff1a1)\u52a8\u4f5c\u63a9\u7801\u6280\u672f\u786e\u4fdd\u667a\u80fd\u4f53\u53ea\u9009\u62e9\u53ef\u884c\u52a8\u4f5c\uff1b2)\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5de5\u4f5c\u7ad9\u7531\u72ec\u7acb\u667a\u80fd\u4f53\u7ba1\u7406\uff0c\u51cf\u5c11\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u7684\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u4e0e\u53ef\u6bd4\u8f83\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u6536\u655b\u5230\u6700\u4f18\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u66f4\u5feb\u7684\u901f\u5ea6\u3002\u8be5\u6846\u67b6\u80fd\u591f\u8ba9\u667a\u80fd\u4f53\u79bb\u7ebf\u5b66\u4e60\u5e76\u5728\u8fd0\u884c\u671f\u95f4\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u67b6\u6784\u6765\u4f18\u5316\u5de5\u4e1a\u88c5\u914d\u7ebf\uff0c\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u52a8\u4f5c\u63a9\u7801\u548c\u591a\u667a\u80fd\u4f53\u6280\u672f\uff0c\u5728\u4fdd\u8bc1\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5de5\u4e1a\u88c5\u914d\u7ebf\u7684\u667a\u80fd\u5316\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.16672", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16672", "abs": "https://arxiv.org/abs/2507.16672", "authors": ["Yushang Zhao", "Huijie Shen", "Dannier Li", "Lu Chang", "Chengrui Zhou", "Yinuo Yang"], "title": "Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs", "comment": null, "summary": "Generative, explainable, and flexible recommender systems, derived using\nLarge Language Models (LLM) are promising and poorly adapted to the cold-start\nuser situation, where there is little to no history of interaction. The current\nsolutions i.e. supervised fine-tuning and collaborative filtering are\ndense-user-item focused and would be expensive to maintain and update. This\npaper introduces a meta-learning framework, that can be used to perform\nparameter-efficient prompt-tuning, to effectively personalize LLM-based\nrecommender systems quickly at cold-start. The model learns soft prompt\nembeddings with first-order (Reptile) and second-order (MAML) optimization by\ntreating each of the users as the tasks. As augmentations to the input tokens,\nthese learnable vectors are the differentiable control variables that represent\nuser behavioral priors. The prompts are meta-optimized through episodic\nsampling, inner-loop adaptation, and outer-loop generalization. On\nMovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model\noutperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in\nreal-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization\nis also supported by this scalable solution, and its 275 ms rate of adaptation\nallows successful real-time risk profiling of financial systems by shortening\ndetection latency and improving payment network stability. Crucially, the 275\nms adaptation capability can enable real-time risk profiling for financial\ninstitutions, reducing systemic vulnerability detection latency significantly\nversus traditional compliance checks. By preventing contagion in payment\nnetworks (e.g., Fedwire), the framework strengthens national financial\ninfrastructure resilience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u63d0\u793a\u8c03\u4f18\u6765\u5feb\u901f\u4e2a\u6027\u5316LLM\u63a8\u8350\u7cfb\u7edf\uff0c\u7279\u522b\u89e3\u51b3\u51b7\u542f\u52a8\u7528\u6237\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86275\u6beb\u79d2\u7684\u5b9e\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u53ef\u5e94\u7528\u4e8e\u91d1\u878d\u98ce\u9669\u5206\u6790\u7b49\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u51b7\u542f\u52a8\u7528\u6237\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5f53\u524d\u7684\u76d1\u7763\u5fae\u8c03\u548c\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u7ef4\u62a4\u66f4\u65b0\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u7528\u6237\u7684\u4e2a\u6027\u5316\u63a8\u8350\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u4e00\u9636\u4f18\u5316\uff08Reptile\uff09\u548c\u4e8c\u9636\u4f18\u5316\uff08MAML\uff09\u6765\u5b66\u4e60\u8f6f\u63d0\u793a\u5d4c\u5165\uff0c\u5c06\u6bcf\u4e2a\u7528\u6237\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\u3002\u901a\u8fc7\u60c5\u8282\u91c7\u6837\u3001\u5185\u5faa\u73af\u9002\u5e94\u548c\u5916\u5faa\u73af\u6cdb\u5316\u8fdb\u884c\u5143\u4f18\u5316\uff0c\u5b66\u4e60\u53ef\u5fae\u5206\u7684\u63a7\u5236\u53d8\u91cf\u6765\u8868\u793a\u7528\u6237\u884c\u4e3a\u5148\u9a8c\u3002", "result": "\u5728MovieLens-1M\u3001Amazon Reviews\u548cRecbole\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u81ea\u9002\u5e94\u6a21\u578b\u5728NDCG@10\u3001HR@10\u548cMRR\u6307\u6807\u4e0a\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u884c\uff08\u4f4e\u4e8e300\u6beb\u79d2\uff09\u3002\u652f\u6301\u96f6\u5386\u53f2\u4e2a\u6027\u5316\uff0c275\u6beb\u79d2\u7684\u9002\u5e94\u901f\u5ea6\u53ef\u7528\u4e8e\u91d1\u878d\u7cfb\u7edf\u5b9e\u65f6\u98ce\u9669\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e2a\u6027\u5316\u548c\u5b9e\u65f6\u9002\u5e94\u80fd\u529b\u3002275\u6beb\u79d2\u7684\u9002\u5e94\u901f\u5ea6\u4f7f\u5176\u80fd\u591f\u5e94\u7528\u4e8e\u91d1\u878d\u98ce\u9669\u5206\u6790\uff0c\u7f29\u77ed\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u652f\u4ed8\u7f51\u7edc\u7a33\u5b9a\u6027\uff0c\u589e\u5f3a\u56fd\u5bb6\u91d1\u878d\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u3002"}}
{"id": "2507.16397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16397", "abs": "https://arxiv.org/abs/2507.16397", "authors": ["Kahim Wong", "Jicheng Zhou", "Haiwei Wu", "Yain-Whar Si", "Jiantao Zhou"], "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement", "comment": null, "summary": "The advancement of image editing tools has enabled malicious manipulation of\nsensitive document images, underscoring the need for robust document image\nforgery detection.Though forgery detectors for natural images have been\nextensively studied, they struggle with document images, as the tampered\nregions can be seamlessly blended into the uniform document background (BG) and\nstructured text. On the other hand, existing document-specific methods lack\nsufficient robustness against various degradations, which limits their\npractical deployment. This paper presents ADCD-Net, a robust document forgery\nlocalization model that adaptively leverages the RGB/DCT forensic traces and\nintegrates key characteristics of document images. Specifically, to address the\nDCT traces' sensitivity to block misalignment, we adaptively modulate the DCT\nfeature contribution based on a predicted alignment score, resulting in much\nimproved resilience to various distortions, including resizing and cropping.\nAlso, a hierarchical content disentanglement approach is proposed to boost the\nlocalization performance via mitigating the text-BG disparities. Furthermore,\nnoticing the predominantly pristine nature of BG regions, we construct a\npristine prototype capturing traces of untampered regions, and eventually\nenhance both the localization accuracy and robustness. Our proposed ADCD-Net\ndemonstrates superior forgery localization performance, consistently\noutperforming state-of-the-art methods by 20.79\\% averaged over 5 types of\ndistortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ADCD-Net\uff0c\u4e00\u79cd\u9c81\u68d2\u7684\u6587\u6863\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u5b9a\u4f4d\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5229\u7528RGB/DCT\u53d6\u8bc1\u75d5\u8ff9\u548c\u5206\u5c42\u5185\u5bb9\u89e3\u8026\u6765\u63d0\u5347\u6587\u6863\u7be1\u6539\u68c0\u6d4b\u6027\u80fd\uff0c\u57285\u79cd\u5931\u771f\u7c7b\u578b\u4e0a\u5e73\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd520.79%\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u5668\u96be\u4ee5\u5904\u7406\u6587\u6863\u56fe\u50cf\uff0c\u56e0\u4e3a\u7be1\u6539\u533a\u57df\u53ef\u4ee5\u65e0\u7f1d\u878d\u5165\u7edf\u4e00\u7684\u6587\u6863\u80cc\u666f\u548c\u7ed3\u6784\u5316\u6587\u672c\u4e2d\uff1b\u800c\u73b0\u6709\u7684\u6587\u6863\u7279\u5b9a\u65b9\u6cd5\u5bf9\u5404\u79cd\u9000\u5316\u7f3a\u4e4f\u8db3\u591f\u7684\u9c81\u68d2\u6027\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u5e94\u7528\u3002", "method": "\u63d0\u51faADCD-Net\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1)\u57fa\u4e8e\u9884\u6d4b\u5bf9\u9f50\u5206\u6570\u81ea\u9002\u5e94\u8c03\u8282DCT\u7279\u5f81\u8d21\u732e\uff0c\u89e3\u51b3DCT\u75d5\u8ff9\u5bf9\u5757\u9519\u4f4d\u7684\u654f\u611f\u6027\uff1b2)\u5206\u5c42\u5185\u5bb9\u89e3\u8026\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f13\u89e3\u6587\u672c-\u80cc\u666f\u5dee\u5f02\u6765\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\uff1b3)\u6784\u5efa\u539f\u59cb\u539f\u578b\u6355\u83b7\u672a\u7be1\u6539\u533a\u57df\u7684\u75d5\u8ff9\uff0c\u589e\u5f3a\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "ADCD-Net\u5728\u6587\u6863\u7be1\u6539\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u57285\u79cd\u5931\u771f\u7c7b\u578b\u4e0a\u5e73\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd520.79%\uff0c\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u5b9a\u4f4d\u6027\u80fd\u548c\u5bf9\u5404\u79cd\u5931\u771f\u7684\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u8c03\u6574\u5927\u5c0f\u548c\u88c1\u526a\u7b49\u3002", "conclusion": "ADCD-Net\u6210\u529f\u89e3\u51b3\u4e86\u6587\u6863\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u9002\u5e94DCT\u7279\u5f81\u8c03\u8282\u3001\u5206\u5c42\u5185\u5bb9\u89e3\u8026\u548c\u539f\u59cb\u539f\u578b\u6784\u5efa\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6587\u6863\u7be1\u6539\u7684\u51c6\u786e\u5b9a\u4f4d\u548c\u9c81\u68d2\u68c0\u6d4b\uff0c\u4e3a\u6587\u6863\u56fe\u50cf\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16406", "abs": "https://arxiv.org/abs/2507.16406", "authors": ["Tanveer Younis", "Zhanglin Cheng"], "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges", "comment": "30 pages, 6 figures", "summary": "Sparse-view 3D reconstruction is essential for applications in which dense\nimage acquisition is impractical, such as robotics, augmented/virtual reality\n(AR/VR), and autonomous systems. In these settings, minimal image overlap\nprevents reliable correspondence matching, causing traditional methods, such as\nstructure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey\nreviews the latest advances in neural implicit models (e.g., NeRF and its\nregularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian\nSplatting), and hybrid frameworks that leverage priors from diffusion and\nvision foundation models (VFMs).We analyze how geometric regularization,\nexplicit shape modeling, and generative inference are used to mitigate\nartifacts such as floaters and pose ambiguities in sparse-view settings.\nComparative results on standard benchmarks reveal key trade-offs between the\nreconstruction accuracy, efficiency, and generalization. Unlike previous\nreviews, our survey provides a unified perspective on geometry-based, neural\nimplicit, and generative (diffusion-based) methods. We highlight the persistent\nchallenges in domain generalization and pose-free reconstruction and outline\nfuture directions for developing 3D-native generative priors and achieving\nreal-time, unconstrained sparse-view reconstruction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u7684\u7efc\u8ff0\uff0c\u56de\u987e\u4e86\u795e\u7ecf\u9690\u5f0f\u6a21\u578b\u3001\u663e\u5f0f\u70b9\u4e91\u65b9\u6cd5\u548c\u6df7\u5408\u6846\u67b6\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u51e0\u4f55\u6b63\u5219\u5316\u3001\u663e\u5f0f\u5f62\u72b6\u5efa\u6a21\u548c\u751f\u6210\u63a8\u7406\u5982\u4f55\u7f13\u89e3\u7a00\u758f\u89c6\u89d2\u8bbe\u7f6e\u4e2d\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6280\u672f\u3001AR/VR\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7b49\u5e94\u7528\u4e2d\uff0c\u5bc6\u96c6\u56fe\u50cf\u91c7\u96c6\u4e0d\u5207\u5b9e\u9645\uff0c\u6700\u5c0f\u56fe\u50cf\u91cd\u53e0\u5bfc\u81f4\u4f20\u7edf\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784(SfM)\u548c\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9(MVS)\u65b9\u6cd5\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u7efc\u8ff0\u4e86\u4e09\u7c7b\u4e3b\u8981\u65b9\u6cd5\uff1a1)\u795e\u7ecf\u9690\u5f0f\u6a21\u578b(\u5982NeRF\u53ca\u5176\u6b63\u5219\u5316\u7248\u672c)\uff1b2)\u663e\u5f0f\u57fa\u4e8e\u70b9\u4e91\u7684\u65b9\u6cd5(\u59823D\u9ad8\u65af\u6563\u5c04)\uff1b3)\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u7684\u6df7\u5408\u6846\u67b6\u3002\u5206\u6790\u4e86\u51e0\u4f55\u6b63\u5219\u5316\u3001\u663e\u5f0f\u5f62\u72b6\u5efa\u6a21\u548c\u751f\u6210\u63a8\u7406\u6280\u672f\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5bf9\u6bd4\u7ed3\u679c\u63ed\u793a\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002\u63d0\u4f9b\u4e86\u51e0\u4f55\u65b9\u6cd5\u3001\u795e\u7ecf\u9690\u5f0f\u65b9\u6cd5\u548c\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u65b9\u6cd5\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u7a81\u51fa\u4e86\u9886\u57df\u6cdb\u5316\u548c\u65e0\u59ff\u6001\u91cd\u5efa\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6301\u7eed\u6311\u6218\uff0c\u5305\u62ec\u9886\u57df\u6cdb\u5316\u548c\u65e0\u59ff\u6001\u91cd\u5efa\u95ee\u9898\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5305\u62ec\u5f00\u53d13D\u539f\u751f\u751f\u6210\u5148\u9a8c\u548c\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u7ea6\u675f\u7684\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u3002"}}
{"id": "2507.16612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16612", "abs": "https://arxiv.org/abs/2507.16612", "authors": ["Haoyang Su", "Shaohao Rui", "Jinyi Xiang", "Lianming Wu", "Xiaosong Wang"], "title": "CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast Cardiac Risk Prediction Using Cine MRIs", "comment": "Accepted at MICCAI 2025", "summary": "Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction\nfrom Cine MRI sequences remains a critical challenge. Existing methods\ntypically necessitate supervised learning based on human-refined masks in the\nventricular myocardium, which become impractical without contrast agents. We\nintroduce a self-supervised framework, namely Codebook-based Temporal-Spatial\nLearning (CTSL), that learns dynamic, spatiotemporal representations from raw\nCine data without requiring segmentation masks. CTSL decouples temporal and\nspatial features through a multi-view distillation strategy, where the teacher\nmodel processes multiple Cine views, and the student model learns from\nreduced-dimensional Cine-SA sequences. By leveraging codebook-based feature\nrepresentations and dynamic lesion self-detection through motion cues, CTSL\ncaptures intricate temporal dependencies and motion patterns. High-confidence\nMACE risk predictions are achieved through our model, providing a rapid,\nnon-invasive solution for cardiac risk assessment that outperforms traditional\ncontrast-dependent methods, thereby enabling timely and accessible heart\ndisease diagnosis in clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7801\u672c\u7684\u65f6\u7a7a\u5b66\u4e60\u6846\u67b6(CTSL)\uff0c\u65e0\u9700\u9020\u5f71\u5242\u548c\u5206\u5272\u63a9\u7801\u5373\u53ef\u4eceCine MRI\u5e8f\u5217\u4e2d\u51c6\u786e\u9884\u6d4b\u4e3b\u8981\u4e0d\u826f\u5fc3\u810f\u4e8b\u4ef6(MACE)\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u591a\u89c6\u56fe\u84b8\u998f\u7b56\u7565\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u5fc3\u810f\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684MACE\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u57fa\u4e8e\u4eba\u5de5\u7cbe\u7ec6\u5316\u5fc3\u5ba4\u5fc3\u808c\u63a9\u7801\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u6ca1\u6709\u9020\u5f71\u5242\u7684\u60c5\u51b5\u4e0b\u53d8\u5f97\u4e0d\u5b9e\u7528\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u5206\u5272\u63a9\u7801\u3001\u53ef\u76f4\u63a5\u4ece\u539f\u59cbCine\u6570\u636e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u65e0\u9020\u5f71\u5242\u7684\u5fc3\u810f\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7801\u672c\u7684\u65f6\u7a7a\u5b66\u4e60(CTSL)\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u84b8\u998f\u7b56\u7565\u89e3\u8026\u65f6\u95f4\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u5176\u4e2d\u6559\u5e08\u6a21\u578b\u5904\u7406\u591a\u4e2aCine\u89c6\u56fe\uff0c\u5b66\u751f\u6a21\u578b\u4ece\u964d\u7ef4\u7684Cine-SA\u5e8f\u5217\u4e2d\u5b66\u4e60\u3002\u5229\u7528\u57fa\u4e8e\u7801\u672c\u7684\u7279\u5f81\u8868\u793a\u548c\u901a\u8fc7\u8fd0\u52a8\u7ebf\u7d22\u8fdb\u884c\u52a8\u6001\u75c5\u53d8\u81ea\u68c0\u6d4b\u6765\u6355\u83b7\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "CTSL\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u7684MACE\u98ce\u9669\u9884\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u4f9d\u8d56\u9020\u5f71\u5242\u7684\u65b9\u6cd5\uff0c\u4e3a\u5fc3\u810f\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u65e0\u521b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u65e0\u9700\u9020\u5f71\u5242\u548c\u5206\u5272\u63a9\u7801\u7684MACE\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u73af\u5883\u4e2d\u53ca\u65f6\u548c\u4fbf\u6377\u7684\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
