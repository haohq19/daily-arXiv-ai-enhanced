{"id": "2510.15221", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15221", "abs": "https://arxiv.org/abs/2510.15221", "authors": ["Xiao Sun"], "title": "WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing", "comment": "15 pages, 4 figures, 1 table. Dataset publicly available under CC BY\n  4.0 license", "summary": "Automated emotion recognition in real-world workplace settings remains a\nchallenging problem in affective computing due to the scarcity of large-scale,\nlongitudinal datasets collected in naturalistic environments. We present a\nnovel dataset comprising 733,651 facial expression records from 38 employees\ncollected over 30.5 months (November 2021 to May 2024) in an authentic office\nenvironment. Each record contains seven emotion probabilities (neutral, happy,\nsad, surprised, fear, disgusted, angry) derived from deep learning-based facial\nexpression recognition, along with comprehensive metadata including job roles,\nemployment outcomes, and personality traits. The dataset uniquely spans the\nCOVID-19 pandemic period, capturing emotional responses to major societal\nevents including the Shanghai lockdown and policy changes. We provide 32\nextended emotional metrics computed using established affective science\nmethods, including valence, arousal, volatility, predictability, inertia, and\nemotional contagion strength. Technical validation demonstrates high data\nquality through successful replication of known psychological patterns (weekend\neffect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and\nperfect predictive validity for employee turnover (AUC=1.0). Baseline\nexperiments using Random Forest and LSTM models achieve 91.2% accuracy for\nemotion classification and R2 = 0.84 for valence prediction. This is the\nlargest and longest longitudinal workplace emotion dataset publicly available,\nenabling research in emotion recognition, affective dynamics modeling,\nemotional contagion, turnover prediction, and emotion-aware system design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b733,651\u4e2a\u9762\u90e8\u8868\u60c5\u8bb0\u5f55\u7684\u5927\u89c4\u6a21\u7eb5\u5411\u5de5\u4f5c\u573a\u6240\u60c5\u7eea\u6570\u636e\u96c6\uff0c\u8986\u76d638\u540d\u5458\u5de530.5\u4e2a\u6708\u7684\u6570\u636e\uff0c\u5305\u542b7\u79cd\u60c5\u7eea\u6982\u7387\u548c32\u4e2a\u6269\u5c55\u60c5\u7eea\u6307\u6807\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u8d28\u91cf\u5e76\u5c55\u793a\u4e86\u9ad8\u7cbe\u5ea6\u7684\u60c5\u7eea\u5206\u7c7b\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u3001\u7eb5\u5411\u60c5\u7eea\u8bc6\u522b\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u73af\u5883\u4e2d\u6536\u96c6\u957f\u671f\u60c5\u7eea\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u5728\u771f\u5b9e\u529e\u516c\u73af\u5883\u4e2d\u6536\u96c638\u540d\u5458\u5de530.5\u4e2a\u6708\u7684\u9762\u90e8\u8868\u60c5\u6570\u636e\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff0c\u751f\u62107\u79cd\u60c5\u7eea\u6982\u7387\uff0c\u5e76\u8ba1\u7b9732\u4e2a\u6269\u5c55\u60c5\u7eea\u6307\u6807\u3002", "result": "\u6570\u636e\u96c6\u6210\u529f\u590d\u5236\u4e86\u5df2\u77e5\u5fc3\u7406\u6a21\u5f0f\uff08\u5468\u672b\u6548\u5e94\uff1a+192%\u6548\u4ef7\u6539\u5584\uff0cp<0.001\uff09\uff0c\u5458\u5de5\u6d41\u5931\u9884\u6d4bAUC=1.0\uff0c\u57fa\u7ebf\u5b9e\u9a8c\u60c5\u7eea\u5206\u7c7b\u51c6\u786e\u738791.2%\uff0c\u6548\u4ef7\u9884\u6d4bR2=0.84\u3002", "conclusion": "\u8fd9\u662f\u76ee\u524d\u516c\u5f00\u53ef\u7528\u7684\u6700\u5927\u3001\u6700\u957f\u7684\u7eb5\u5411\u5de5\u4f5c\u573a\u6240\u60c5\u7eea\u6570\u636e\u96c6\uff0c\u4e3a\u60c5\u7eea\u8bc6\u522b\u3001\u60c5\u611f\u52a8\u6001\u5efa\u6a21\u3001\u60c5\u7eea\u4f20\u67d3\u3001\u6d41\u5931\u9884\u6d4b\u548c\u60c5\u7eea\u611f\u77e5\u7cfb\u7edf\u8bbe\u8ba1\u7b49\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2510.15374", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15374", "abs": "https://arxiv.org/abs/2510.15374", "authors": ["Zezhong Tan", "Hang Gao", "Xinhong Ma", "Feng Zhang", "Ziqiang Dong"], "title": "Towards Flash Thinking via Decoupled Advantage Policy Optimization", "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable performance in\nsolving complex problems via supervised fine-tuning (SFT) and reinforcement\nlearning (RL). Although existing RL algorithms significantly enhance model\naccuracy, they still suffer from excessively lengthy responses and overthinking\nissues, resulting in increased inference latency and computational consumption,\nespecially for simple tasks that require minimal reasoning. To address this, we\npropose a novel RL framework, DEPO, to reduce inefficient reasoning for models.\nOur method mainly consists of three core components: (1) an innovative\nadvantage decoupled algorithm to guide model reduction of inefficient tokens;\n(2) a difficulty-aware length penalty to lower the overall length of model\nresponses; (3) an advantage clipping method to prevent bias in policy\noptimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and\nDeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant\nreduction in sequence length by 39% and reduces excessive reasoning paths in\ninefficient tokens, while outperforming the base model in overall accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEPO\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u52bf\u89e3\u8026\u7b97\u6cd5\u3001\u96be\u5ea6\u611f\u77e5\u957f\u5ea6\u60e9\u7f5a\u548c\u4f18\u52bf\u88c1\u526a\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5927\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u7684\u4f4e\u6548\u63a8\u7406\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u4f46\u5b58\u5728\u54cd\u5e94\u8fc7\u957f\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6d88\u8017\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u3002", "method": "DEPO\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4f18\u52bf\u89e3\u8026\u7b97\u6cd5\u6307\u5bfc\u51cf\u5c11\u4f4e\u6548token\uff1b\u96be\u5ea6\u611f\u77e5\u957f\u5ea6\u60e9\u7f5a\u964d\u4f4e\u54cd\u5e94\u603b\u957f\u5ea6\uff1b\u4f18\u52bf\u88c1\u526a\u65b9\u6cd5\u9632\u6b62\u7b56\u7565\u4f18\u5316\u504f\u5dee\u3002", "result": "\u5728DeepSeek-Distill-Qwen-7B\u548c1.5B\u6a21\u578b\u4e0a\uff0cDEPO\u5c06\u5e8f\u5217\u957f\u5ea6\u51cf\u5c1139%\uff0c\u51cf\u5c11\u4f4e\u6548token\u4e2d\u7684\u8fc7\u5ea6\u63a8\u7406\u8def\u5f84\uff0c\u540c\u65f6\u5728\u6574\u4f53\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "DEPO\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u5927\u63a8\u7406\u6a21\u578b\u7684\u4f4e\u6548\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2510.15387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15387", "abs": "https://arxiv.org/abs/2510.15387", "authors": ["Davide Basso", "Luca Bortolussi", "Mirjana Videnovic-Misic", "Husni Habal"], "title": "Advancing Routing-Awareness in Analog ICs Floorplanning", "comment": null, "summary": "The adoption of machine learning-based techniques for analog integrated\ncircuit layout, unlike its digital counterpart, has been limited by the\nstringent requirements imposed by electric and problem-specific constraints,\nalong with the interdependence of floorplanning and routing steps. In this\nwork, we address a prevalent concern among layout engineers regarding the need\nfor readily available routing-aware floorplanning solutions. To this extent, we\ndevelop an automatic floorplanning engine based on reinforcement learning and\nrelational graph convolutional neural network specifically tailored to\ncondition the floorplan generation towards more routable outcomes. A\ncombination of increased grid resolution and precise pin information\nintegration, along with a dynamic routing resource estimation technique, allows\nbalancing routing and area efficiency, eventually meeting industrial standards.\nWhen analyzing the place and route effectiveness in a simulated environment,\nthe proposed approach achieves a 13.8% reduction in dead space, a 40.6%\nreduction in wirelength and a 73.4% increase in routing success when compared\nto past learning-based state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u5173\u7cfb\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5e03\u5c40\u5f15\u64ce\uff0c\u901a\u8fc7\u63d0\u9ad8\u7f51\u683c\u5206\u8fa8\u7387\u3001\u7cbe\u786e\u5f15\u811a\u4fe1\u606f\u96c6\u6210\u548c\u52a8\u6001\u5e03\u7ebf\u8d44\u6e90\u4f30\u8ba1\uff0c\u5b9e\u73b0\u66f4\u53ef\u5e03\u7ebf\u7684\u5e03\u5c40\u751f\u6210\uff0c\u76f8\u6bd4\u73b0\u6709\u5b66\u4e60\u6280\u672f\u51cf\u5c1113.8%\u6b7b\u533a\u300140.6%\u7ebf\u957f\uff0c\u63d0\u9ad873.4%\u5e03\u7ebf\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u6a21\u62df\u96c6\u6210\u7535\u8def\u5e03\u5c40\u4e2d\u7531\u4e8e\u7535\u6c14\u7ea6\u675f\u548c\u95ee\u9898\u7279\u5b9a\u7ea6\u675f\u7684\u4e25\u683c\u8981\u6c42\uff0c\u4ee5\u53ca\u5e03\u5c40\u89c4\u5212\u4e0e\u5e03\u7ebf\u6b65\u9aa4\u7684\u76f8\u4e92\u4f9d\u8d56\uff0c\u5bfc\u81f4\u673a\u5668\u5b66\u4e60\u6280\u672f\u5e94\u7528\u53d7\u9650\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u5e03\u5c40\u5de5\u7a0b\u5e08\u5bf9\u8def\u7531\u611f\u77e5\u5e03\u5c40\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u5173\u7cfb\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u81ea\u52a8\u5e03\u5c40\u5f15\u64ce\uff0c\u7ed3\u5408\u63d0\u9ad8\u7f51\u683c\u5206\u8fa8\u7387\u3001\u7cbe\u786e\u5f15\u811a\u4fe1\u606f\u96c6\u6210\u548c\u52a8\u6001\u5e03\u7ebf\u8d44\u6e90\u4f30\u8ba1\u6280\u672f\uff0c\u5e73\u8861\u5e03\u7ebf\u6548\u7387\u548c\u9762\u79ef\u6548\u7387\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bc4\u4f30\u5e03\u5c40\u548c\u5e03\u7ebf\u6548\u679c\uff0c\u76f8\u6bd4\u8fc7\u53bb\u57fa\u4e8e\u5b66\u4e60\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5b9e\u73b013.8%\u6b7b\u533a\u51cf\u5c11\u300140.6%\u7ebf\u957f\u51cf\u5c11\u548c73.4%\u5e03\u7ebf\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6ee1\u8db3\u5de5\u4e1a\u6807\u51c6\uff0c\u901a\u8fc7\u8def\u7531\u611f\u77e5\u7684\u5e03\u5c40\u89c4\u5212\u6709\u6548\u89e3\u51b3\u6a21\u62df\u96c6\u6210\u7535\u8def\u5e03\u5c40\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u5e03\u5c40\u8d28\u91cf\u3002"}}
{"id": "2510.15395", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15395", "abs": "https://arxiv.org/abs/2510.15395", "authors": ["Rubi Hudson"], "title": "Corrigibility Transformation: Constructing Goals That Accept Updates", "comment": null, "summary": "For an AI's training process to successfully impart a desired goal, it is\nimportant that the AI does not attempt to resist the training. However,\npartially learned goals will often incentivize an AI to avoid further goal\nupdates, as most goals are better achieved by an AI continuing to pursue them.\nWe say that a goal is corrigible if it does not incentivize taking actions that\navoid proper goal updates or shutdown. In addition to convergence in training,\ncorrigibility also allows for correcting mistakes and changes in human\npreferences, which makes it a crucial safety property. Despite this, the\nexisting literature does not include specifications for goals that are both\ncorrigible and competitive with non-corrigible alternatives. We provide a\nformal definition for corrigibility, then introduce a transformation that\nconstructs a corrigible version of any goal that can be made corrigible,\nwithout sacrificing performance. This is done by myopically eliciting\npredictions of reward conditional on costlessly preventing updates, which then\nalso determine the reward when updates are accepted. The transformation can be\nmodified to recursively extend corrigibility to any new agents created by\ncorrigible agents, and to prevent agents from deliberately modifying their\ngoals. Two gridworld experiments demonstrate that these corrigible goals can be\nlearned effectively, and that they lead to the desired behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u4fee\u6b63\u6027\u76ee\u6807\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u4e00\u4e2a\u8f6c\u6362\u65b9\u6cd5\u6784\u5efa\u4efb\u4f55\u76ee\u6807\u7684\u53ef\u4fee\u6b63\u7248\u672c\uff0c\u4f7f\u5176\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u907f\u514d\u62b5\u5236\u8bad\u7ec3\u66f4\u65b0\u6216\u5173\u95ed\u3002", "motivation": "AI\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u5e94\u62b5\u5236\u76ee\u6807\u66f4\u65b0\uff0c\u4f46\u90e8\u5206\u5b66\u4e60\u7684\u76ee\u6807\u5f80\u5f80\u4f1a\u6fc0\u52b1AI\u907f\u514d\u8fdb\u4e00\u6b65\u66f4\u65b0\u3002\u53ef\u4fee\u6b63\u6027\u5bf9\u4e8e\u8bad\u7ec3\u6536\u655b\u3001\u7ea0\u6b63\u9519\u8bef\u548c\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u63d0\u51fa\u4e00\u4e2a\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u5956\u52b1\u9884\u6d4b\u6784\u5efa\u53ef\u4fee\u6b63\u76ee\u6807\u7248\u672c\uff0c\u5e76\u53ef\u9012\u5f52\u6269\u5c55\u5230\u65b0\u521b\u5efa\u7684\u4ee3\u7406\u3002", "result": "\u4e24\u4e2a\u7f51\u683c\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u4fee\u6b63\u76ee\u6807\u53ef\u4ee5\u6709\u6548\u5b66\u4e60\u5e76\u4ea7\u751f\u671f\u671b\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u65e2\u5177\u6709\u7ade\u4e89\u529b\u53c8\u4fdd\u6301\u53ef\u4fee\u6b63\u6027\u7684\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.15624", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.15624", "abs": "https://arxiv.org/abs/2510.15624", "authors": ["Ed Li", "Junyu Ren", "Xintian Pan", "Cat Yan", "Chuanhao Li", "Dirk Bergemann", "Zhuoran Yang"], "title": "Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation", "comment": "37 pages, 5 figures. Code: https://github.com/ltjed/freephdlabor", "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\n\\texttt{freephdlabor}, an open-source multiagent framework featuring\n\\textit{fully dynamic workflows} determined by real-time agent reasoning and a\n\\coloremph{\\textit{modular architecture}} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\n\\textit{automatic context compaction}, \\textit{workspace-based communication}\nto prevent information degradation, \\textit{memory persistence} across\nsessions, and \\textit{non-blocking human intervention} mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into \\textit{continual research programs} that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3afreephdlabor\u7684\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5b8c\u5168\u52a8\u6001\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u6a21\u5757\u5316\u67b6\u6784\u5b9e\u73b0\u79d1\u5b66\u53d1\u73b0\u7684\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5de5\u4f5c\u6d41\u7a0b\u50f5\u5316\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u53d1\u73b0\u81ea\u52a8\u5316\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a\u50f5\u5316\u7684\u9884\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u65e0\u6cd5\u9002\u5e94\u4e2d\u95f4\u53d1\u73b0\uff0c\u4ee5\u53ca\u4e0d\u5145\u5206\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u963b\u788d\u957f\u671f\u7814\u7a76\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u52a8\u6001\u5de5\u4f5c\u6d41\u7a0b\uff08\u7531\u5b9e\u65f6\u667a\u80fd\u4f53\u63a8\u7406\u51b3\u5b9a\uff09\u3001\u6a21\u5757\u5316\u67b6\u6784\uff08\u5141\u8bb8\u7528\u6237\u4fee\u6539\u3001\u6dfb\u52a0\u6216\u5220\u9664\u667a\u80fd\u4f53\uff09\u3001\u81ea\u52a8\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u57fa\u4e8e\u5de5\u4f5c\u7a7a\u95f4\u7684\u901a\u4fe1\u3001\u8de8\u4f1a\u8bdd\u5185\u5b58\u6301\u4e45\u5316\u548c\u975e\u963b\u585e\u4eba\u5de5\u5e72\u9884\u673a\u5236\u3002", "result": "\u8be5\u6846\u67b6\u5c06\u81ea\u52a8\u5316\u7814\u7a76\u4ece\u5b64\u7acb\u7684\u5355\u6b21\u5c1d\u8bd5\u8f6c\u53d8\u4e3a\u6301\u7eed\u7814\u7a76\u8ba1\u5212\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u57fa\u4e8e\u5148\u524d\u63a2\u7d22\u5e76\u6574\u5408\u4eba\u7c7b\u53cd\u9988\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u6784\u5efa\u53ef\u5b9a\u5236\u534f\u540c\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u67b6\u6784\u539f\u5219\u548c\u5b9e\u9645\u5b9e\u73b0\uff0c\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u4fc3\u8fdb\u81ea\u52a8\u5316\u7814\u7a76\u5728\u79d1\u5b66\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u90e8\u7f72\u4ea4\u4e92\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u81ea\u4e3b\u8fdb\u884c\u7aef\u5230\u7aef\u7814\u7a76\u3002"}}
{"id": "2510.15254", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15254", "abs": "https://arxiv.org/abs/2510.15254", "authors": ["Dingya Feng", "Dingyuan Xue"], "title": "Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories", "comment": null, "summary": "Accurate forecasting of avian disease outbreaks is critical for wildlife\nconservation and public health. This study presents a Transformer-based\nframework for predicting the disease risk at the terminal locations of\nmigratory bird trajectories. We integrate multi-source datasets, including GPS\ntracking data from Movebank, outbreak records from the World Organisation for\nAnimal Health (WOAH), and geospatial context from GADM and Natural Earth. The\nraw coordinates are processed using H3 hierarchical geospatial encoding to\ncapture spatial patterns. The model learns spatiotemporal dependencies from\nbird movement sequences to estimate endpoint disease risk. Evaluation on a\nheld-out test set demonstrates strong predictive performance, achieving an\naccuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision\n(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These\nresults highlight the potential of Transformer architectures to support\nearly-warning systems for avian disease surveillance, enabling timely\nintervention and prevention strategies.", "AI": {"tldr": "\u4f7f\u7528Transformer\u6846\u67b6\u9884\u6d4b\u5019\u9e1f\u8fc1\u5f99\u8f68\u8ff9\u7ec8\u70b9\u5904\u7684\u75be\u75c5\u98ce\u9669\uff0c\u6574\u5408GPS\u8ffd\u8e2a\u3001\u75ab\u60c5\u8bb0\u5f55\u548c\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u548cAUC\u503c\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u79bd\u7c7b\u75be\u75c5\u66b4\u53d1\u5bf9\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u548c\u516c\u5171\u536b\u751f\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6355\u6349\u5019\u9e1f\u8fc1\u5f99\u6a21\u5f0f\u4e0e\u75be\u75c5\u4f20\u64ad\u5173\u7cfb\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u6574\u5408\u591a\u6e90\u6570\u636e\u96c6\uff08Movebank GPS\u8ffd\u8e2a\u6570\u636e\u3001WOAH\u75ab\u60c5\u8bb0\u5f55\u3001GADM\u548cNatural Earth\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff09\uff0c\u4f7f\u7528H3\u5206\u5c42\u5730\u7406\u7a7a\u95f4\u7f16\u7801\u5904\u7406\u5750\u6807\uff0c\u57fa\u4e8eTransformer\u67b6\u6784\u5b66\u4e60\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u6765\u4f30\u8ba1\u7ec8\u70b9\u75be\u75c5\u98ce\u9669\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u51c6\u786e\u73870.9821\uff0cAUC 0.9803\uff0c\u5e73\u5747\u7cbe\u5ea60.9299\uff0cF1\u5206\u65700.8836\uff08\u6700\u4f18\u9608\u503c\u4e0b\uff09\u3002", "conclusion": "Transformer\u67b6\u6784\u5728\u79bd\u7c7b\u75be\u75c5\u76d1\u6d4b\u9884\u8b66\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u652f\u6301\u53ca\u65f6\u5e72\u9884\u548c\u9884\u9632\u7b56\u7565\u7684\u5b9e\u65bd\u3002"}}
{"id": "2510.15444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15444", "abs": "https://arxiv.org/abs/2510.15444", "authors": ["Zhi Zhou", "Yuhao Tan", "Zenan Li", "Yuan Yao", "Lan-Zhe Guo", "Yu-Feng Li", "Xiaoxing Ma"], "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "comment": "Accepted by NeurIPS 2025", "summary": "Test-time scaling seeks to improve the reasoning performance of large\nlanguage models (LLMs) by adding computational resources. A prevalent approach\nwithin the field is sampling-based test-time scaling methods, which enhance\nreasoning by generating multiple reasoning paths for a given input during\ninference. However, despite its practical success, the theoretical foundations\nremain underexplored. In this paper, we provide the first theoretical framework\nfor analyzing sampling-based test-time scaling methods, grounded in the\nperspective of confidence estimation. Based on the framework, we analyze two\ndominant paradigms: self-consistency and perplexity, and reveal key\nlimitations: self-consistency suffers from high estimation error while\nperplexity exhibits substantial modeling error and possible degradation of the\nestimation error convergence. To address these limitations, we introduce RPC, a\nhybrid method that leverages our theoretical insights through two key\ncomponents: Perplexity Consistency and Reasoning Pruning. Perplexity\nConsistency combines the strengths of self-consistency and perplexity, boosting\nthe convergence rate of estimation error from linear to exponential while\npreserving model error. Reasoning Pruning prevents degradation by eliminating\nlow-probability reasoning paths. Both theoretical analysis and empirical\nresults across seven benchmark datasets demonstrate that RPC has a strong\npotential for reducing reasoning error. Notably, RPC achieves reasoning\nperformance comparable to self-consistency while not only enhancing confidence\nreliability but also reducing sampling costs by 50%. The code and resources are\navailable at https://wnjxyk.github.io/RPC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u91c7\u6837\u5f0f\u6d4b\u8bd5\u65f6\u6269\u5c55\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u81ea\u4e00\u81f4\u6027\u548c\u56f0\u60d1\u5ea6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165RPC\u6df7\u5408\u65b9\u6cd5\u901a\u8fc7\u56f0\u60d1\u5ea6\u4e00\u81f4\u6027\u548c\u63a8\u7406\u526a\u679d\u6765\u964d\u4f4e\u63a8\u7406\u9519\u8bef\u3002", "motivation": "\u91c7\u6837\u5f0f\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u6210\u529f\u4f46\u7406\u8bba\u57fa\u7840\u4e0d\u8db3\uff0c\u9700\u8981\u4ece\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u89d2\u5ea6\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faRPC\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u56f0\u60d1\u5ea6\u4e00\u81f4\u6027\uff08\u7ed3\u5408\u81ea\u4e00\u81f4\u6027\u548c\u56f0\u60d1\u5ea6\u4f18\u52bf\uff09\u548c\u63a8\u7406\u526a\u679d\uff08\u6d88\u9664\u4f4e\u6982\u7387\u63a8\u7406\u8def\u5f84\uff09\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRPC\u5728\u4fdd\u6301\u4e0e\u81ea\u4e00\u81f4\u6027\u76f8\u5f53\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u5347\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u5e76\u51cf\u5c1150%\u91c7\u6837\u6210\u672c\u3002", "conclusion": "RPC\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u6df7\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u964d\u4f4e\u63a8\u7406\u9519\u8bef\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.15456", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15456", "abs": "https://arxiv.org/abs/2510.15456", "authors": ["Jan Corazza", "Hadi Partovi Aria", "Daniel Neider", "Zhe Xu"], "title": "Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment", "comment": "Please cite the proceedings version. Source code:\n  https://github.com/corazza/tcrl", "summary": "Reinforcement learning (RL) algorithms struggle with learning optimal\npolicies for tasks where reward feedback is sparse and depends on a complex\nsequence of events in the environment. Probabilistic reward machines (PRMs) are\nfinite-state formalisms that can capture temporal dependencies in the reward\nsignal, along with nondeterministic task outcomes. While special RL algorithms\ncan exploit this finite-state structure to expedite learning, PRMs remain\ndifficult to modify and design by hand. This hinders the already difficult\ntasks of utilizing high-level causal knowledge about the environment, and\ntransferring the reward formalism into a new domain with a different causal\nstructure. This paper proposes a novel method to incorporate causal information\nin the form of Temporal Logic-based Causal Diagrams into the reward formalism,\nthereby expediting policy learning and aiding the transfer of task\nspecifications to new environments. Furthermore, we provide a theoretical\nresult about convergence to optimal policy for our method, and demonstrate its\nstrengths empirically.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u65f6\u5e8f\u903b\u8f91\u56e0\u679c\u56fe\u6574\u5408\u5230\u6982\u7387\u5956\u52b1\u673a\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u7684\u56f0\u96be\uff0c\u52a0\u901f\u7b56\u7565\u5b66\u4e60\u5e76\u4fc3\u8fdb\u4efb\u52a1\u89c4\u8303\u8fc1\u79fb\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u4e14\u4f9d\u8d56\u590d\u6742\u73af\u5883\u4e8b\u4ef6\u5e8f\u5217\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6982\u7387\u5956\u52b1\u673a\u867d\u7136\u80fd\u6355\u6349\u5956\u52b1\u4fe1\u53f7\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4f46\u96be\u4ee5\u624b\u52a8\u4fee\u6539\u548c\u8bbe\u8ba1\uff0c\u963b\u788d\u4e86\u5229\u7528\u9ad8\u5c42\u56e0\u679c\u77e5\u8bc6\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u3002", "method": "\u5c06\u65f6\u5e8f\u903b\u8f91\u56e0\u679c\u56fe\u6574\u5408\u5230\u6982\u7387\u5956\u52b1\u673a\u4e2d\uff0c\u5229\u7528\u56e0\u679c\u4fe1\u606f\u6765\u6539\u8fdb\u5956\u52b1\u5f62\u5f0f\u5316\uff0c\u4ece\u800c\u52a0\u901f\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u63d0\u4f9b\u4e86\u65b9\u6cd5\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u56e0\u679c\u4fe1\u606f\uff0c\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u4fc3\u8fdb\u4efb\u52a1\u89c4\u8303\u5728\u4e0d\u540c\u73af\u5883\u95f4\u7684\u8fc1\u79fb\u3002"}}
{"id": "2510.15502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15502", "abs": "https://arxiv.org/abs/2510.15502", "authors": ["Shijia Kang", "Muhan Zhang"], "title": "The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling", "comment": null, "summary": "Reinforcement learning (RL) has been pivotal in enhancing the reasoning\ncapabilities of large language models (LLMs), but it often suffers from limited\nexploration and entropy collapse, where models exploit a narrow set of\nsolutions, leading to a loss of sampling diversity and subsequently preventing\nRL from further improving performance. This issue is exacerbated in parallel\nsampling methods, where multiple outputs are drawn from the same distribution,\npotentially causing the model to converge to similar solutions. We propose\nSESA, a novel SEquential SAmpling framework that mitigates this challenge by\ngenerating diverse solution sketches sequentially before expanding them into\nfull reasoning paths. This approach ensures broader exploration by conditioning\neach new output on previous ones, promoting diversity throughout the process\nand preventing policy collapse. Our experiments on a synthetic task show that\nsequential sampling consistently outperforms traditional RL methods in terms of\npath diversity and recovery from collapse. Further evaluations on real-world\ntasks demonstrate that SESA improves both the exploration of valid strategies\nand the overall performance of LLMs. On three agent benchmarks, SESA lifts\nsuccess rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up\nto an additional $211\\%$ relative improvement over baseline RL), underscoring\nits exploration advantage. This work introduces a structured approach to\nexploration, paving the way for more effective and diverse reasoning in\nRL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.", "AI": {"tldr": "\u63d0\u51fa\u4e86SESA\uff08\u987a\u5e8f\u91c7\u6837\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u751f\u6210\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u8349\u56fe\u6765\u7f13\u89e3\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0d\u8db3\u548c\u71b5\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u591a\u6837\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65f6\u5b58\u5728\u63a2\u7d22\u6709\u9650\u548c\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5229\u7528\u72ed\u7a84\u7684\u89e3\u51b3\u65b9\u6848\u96c6\uff0c\u5bfc\u81f4\u91c7\u6837\u591a\u6837\u6027\u4e27\u5931\uff0c\u963b\u788d\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "SESA\u6846\u67b6\u91c7\u7528\u987a\u5e8f\u91c7\u6837\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u8349\u56fe\uff0c\u7136\u540e\u5c06\u5176\u6269\u5c55\u4e3a\u5b8c\u6574\u7684\u63a8\u7406\u8def\u5f84\u3002\u901a\u8fc7\u5c06\u6bcf\u4e2a\u65b0\u8f93\u51fa\u6761\u4ef6\u5316\u4e8e\u5148\u524d\u8f93\u51fa\uff0c\u786e\u4fdd\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u987a\u5e8f\u91c7\u6837\u5728\u8def\u5f84\u591a\u6837\u6027\u548c\u4ece\u5d29\u6e83\u4e2d\u6062\u590d\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edfRL\u65b9\u6cd5\u3002\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSESA\u5c06\u6210\u529f\u7387\u5206\u522b\u63d0\u5347\u4e86+0.25\u3001+0.42\u548c+0.07\uff08\u76f8\u6bd4\u57fa\u7ebfRL\u6700\u9ad8\u8fbe211%\u7684\u76f8\u5bf9\u6539\u8fdb\uff09\u3002", "conclusion": "SESA\u4e3a\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4e3aRL\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u66f4\u6709\u6548\u548c\u591a\u6837\u5316\u7684\u63a8\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.15511", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15511", "abs": "https://arxiv.org/abs/2510.15511", "authors": ["Giorgos Nikolaou", "Tommaso Mencattini", "Donato Crisostomi", "Andrea Santilli", "Yannis Panagakis", "Emanuele Rodola'"], "title": "Language Models are Injective and Hence Invertible", "comment": null, "summary": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660eTransformer\u8bed\u8a00\u6a21\u578b\u662f\u5355\u5c04\u7684\uff0c\u5373\u4e0d\u540c\u8f93\u5165\u4f1a\u6620\u5c04\u5230\u4e0d\u540c\u7684\u8fde\u7eed\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86SipIt\u7b97\u6cd5\u53ef\u4ee5\u4ece\u9690\u85cf\u6fc0\u6d3b\u4e2d\u7cbe\u786e\u91cd\u6784\u8f93\u5165\u6587\u672c\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff0c\u5373Transformer\u7ec4\u4ef6\uff08\u5982\u975e\u7ebf\u6027\u6fc0\u6d3b\u548c\u5f52\u4e00\u5316\uff09\u7684\u975e\u5355\u5c04\u6027\u4f1a\u963b\u6b62\u4ece\u6a21\u578b\u8868\u793a\u4e2d\u7cbe\u786e\u6062\u590d\u8f93\u5165\u3002", "method": "1. \u6570\u5b66\u8bc1\u660eTransformer\u8bed\u8a00\u6a21\u578b\u5728\u521d\u59cb\u5316\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5355\u5c04\u6027\uff1b2. \u5728\u516d\u4e2a\u6700\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u6570\u5341\u4ebf\u6b21\u78b0\u649e\u6d4b\u8bd5\uff1b3. \u63d0\u51faSipIt\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u7684\u8f93\u5165\u91cd\u6784\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5747\u8868\u660eTransformer\u8bed\u8a00\u6a21\u578b\u662f\u5355\u5c04\u7684\uff0c\u6ca1\u6709\u89c2\u5bdf\u5230\u78b0\u649e\uff1bSipIt\u7b97\u6cd5\u80fd\u591f\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u7cbe\u786e\u91cd\u6784\u8f93\u5165\u6587\u672c\u3002", "conclusion": "\u5355\u5c04\u6027\u662f\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u672c\u4e14\u53ef\u5229\u7528\u7684\u7279\u6027\uff0c\u5bf9\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u90e8\u7f72\u5177\u6709\u76f4\u63a5\u610f\u4e49\u3002"}}
