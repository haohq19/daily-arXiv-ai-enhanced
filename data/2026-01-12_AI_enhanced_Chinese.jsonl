{"id": "2601.05353", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.05353", "abs": "https://arxiv.org/abs/2601.05353", "authors": ["Shovito Barua Soumma", "Hassan Ghasemzadeh"], "title": "GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting", "comment": null, "summary": "Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.", "AI": {"tldr": "GlyRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8840\u7cd6\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528LLM\u4eceCGM\u6570\u636e\u4e2d\u63d0\u53d6\u4e34\u5e8a\u8bed\u4e49\uff0c\u7ed3\u5408\u591a\u6a21\u6001Transformer\u548c\u68c0\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u8840\u7cd6\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8840\u7cd6\u9884\u6d4b\u6a21\u578b\u5c06CGM\u6570\u636e\u89c6\u4e3a\u6570\u503c\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6216\u4f9d\u8d56\u96be\u4ee5\u5927\u89c4\u6a21\u6536\u96c6\u7684\u989d\u5916\u4f20\u611f\u5668\u3002LLM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u7cd6\u5c3f\u75c5\u62a4\u7406\u4e2d\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u53d6\u5668\u7684\u89d2\u8272\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faGlyRAG\u6846\u67b6\uff1a1) \u4f7f\u7528LLM\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u53d6\u5668\u4eceCGM\u8f68\u8ff9\u751f\u6210\u4e34\u5e8a\u603b\u7ed3\uff1b2) \u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u603b\u7ed3\u6587\u672c\uff1b3) \u5728\u591a\u6a21\u6001Transformer\u67b6\u6784\u4e2d\u5c06\u6587\u672c\u5d4c\u5165\u4e0e\u57fa\u4e8epatch\u7684\u8840\u7cd6\u8868\u793a\u878d\u5408\uff0c\u4f7f\u7528\u4ea4\u53c9\u7ffb\u8bd1\u635f\u5931\u5bf9\u9f50\u6587\u672c\u548c\u751f\u7406\u5b66\u5d4c\u5165\uff1b4) \u68c0\u7d22\u6a21\u5757\u5728\u5b66\u4e60\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bc6\u522b\u76f8\u4f3c\u5386\u53f2\u4e8b\u4ef6\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6574\u5408\u8fd9\u4e9b\u6848\u4f8b\u7c7b\u6bd4\u540e\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2aT1D\u961f\u5217\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a1) GlyRAG\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cRMSE\u964d\u4f4e\u8fbe39%\uff0c\u6bd4\u57fa\u7ebf\u8fdb\u4e00\u6b65\u964d\u4f4e1.7%\uff1b2) \u4e34\u5e8a\u8bc4\u4f30\u663e\u793a85%\u7684\u9884\u6d4b\u4f4d\u4e8e\u5b89\u5168\u533a\u57df\uff0c\u9884\u6d4b\u8840\u7cd6\u5f02\u5e38\u4e8b\u4ef6\u7684\u6027\u80fd\u63d0\u534751%\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u4e0a\u4e0b\u6587\u63d0\u53d6\u548cCGM\u8f68\u8ff9\u68c0\u7d22\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u957f\u671f\u8840\u7cd6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u7cd6\u5c3f\u75c5\u7ba1\u7406\u7684\u667a\u80fd\u51b3\u7b56\u652f\u6301\u5de5\u5177\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2601.05810", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05810", "abs": "https://arxiv.org/abs/2601.05810", "authors": ["ChunTeng Chen", "YiChen Hsu", "YiWen Liu", "WeiFang Sun", "TsaiChing Ni", "ChunYi Lee", "Min Sun", "YuanFu Yang"], "title": "SceneFoundry: Generating Interactive Infinite 3D Worlds", "comment": "15 pages", "summary": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.", "AI": {"tldr": "SceneFoundry\u662f\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u516c\u5bd3\u89c4\u6a21\u76843D\u4e16\u754c\uff0c\u5305\u542b\u529f\u80fd\u53ef\u52a8\u5bb6\u5177\u548c\u8bed\u4e49\u591a\u6837\u5e03\u5c40\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u7684\u529f\u80fd\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5305\u542b\u5bf9\u673a\u5668\u4eba\u7269\u4f53\u64cd\u4f5c\u548c\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u7684\u53ef\u52a8\u90e8\u4ef6\u7684\u7269\u4f53\u3002\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u3001\u4ea4\u4e92\u5f0f\u3001\u7269\u7406\u771f\u5b9e\u76843D\u73af\u5883\u5bf9\u63a8\u8fdb\u673a\u5668\u4eba\u5b66\u4e60\u548c\u5177\u8eab\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff1aLLM\u6a21\u5757\u63a7\u5236\u697c\u5c42\u5e03\u5c40\u751f\u6210\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u540e\u9a8c\u91c7\u6837\u4ece\u5927\u89c4\u6a213D\u8d44\u6e90\u5e93\u4e2d\u9ad8\u6548\u586b\u5145\u53ef\u52a8\u8d44\u4ea7\u3002\u91c7\u7528\u53ef\u5fae\u5206\u6307\u5bfc\u51fd\u6570\u6765\u8c03\u8282\u7269\u4f53\u6570\u91cf\u3001\u9632\u6b62\u5173\u8282\u78b0\u649e\u3001\u4fdd\u6301\u8db3\u591f\u7684\u673a\u5668\u4eba\u53ef\u901a\u884c\u7a7a\u95f4\u3002", "result": "\u6846\u67b6\u80fd\u591f\u751f\u6210\u7ed3\u6784\u6709\u6548\u3001\u8bed\u4e49\u8fde\u8d2f\u3001\u529f\u80fd\u4ea4\u4e92\u7684\u73af\u5883\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u7c7b\u578b\u548c\u6761\u4ef6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u5177\u8eabAI\u7814\u7a76\u3002", "conclusion": "SceneFoundry\u80fd\u591f\u751f\u6210\u5305\u542b\u529f\u80fd\u53ef\u52a8\u5bb6\u5177\u7684\u5927\u89c4\u6a213D\u73af\u5883\uff0c\u4e3a\u673a\u5668\u4eba\u8bad\u7ec3\u548c\u5177\u8eab\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05520", "abs": "https://arxiv.org/abs/2601.05520", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems", "comment": "22 pages, 13 figures, 7 tables", "summary": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.", "AI": {"tldr": "\u63d0\u51faCHisAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5206\u89e3\u65b9\u6cd5\u81ea\u52a8\u6784\u5efa\u4e2d\u56fd\u53e4\u4ee3\u5386\u53f2\u4e8b\u4ef6\u5206\u7c7b\u4f53\u7cfb\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u6587\u5316\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u6587\u5316\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u8bed\u5883\u5982\u4e2d\u56fd\u5386\u53f2\u4e2d\u3002\u5206\u7c7b\u7ed3\u6784\u80fd\u6709\u6548\u7ec4\u7ec7\u5386\u53f2\u77e5\u8bc6\uff0c\u4f46\u624b\u52a8\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55", "method": "CHisAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5c06\u5206\u7c7b\u6784\u5efa\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u81ea\u4e0b\u800c\u4e0a\u7684\u5f52\u7eb3\u5668\u4ece\u539f\u59cb\u5386\u53f2\u8bed\u6599\u63a8\u5bfc\u521d\u59cb\u5c42\u6b21\u7ed3\u6784\uff0c\u81ea\u4e0a\u800c\u4e0b\u7684\u6269\u5c55\u5668\u5229\u7528LLM\u4e16\u754c\u77e5\u8bc6\u5f15\u5165\u7f3a\u5931\u7684\u4e2d\u5c42\u6982\u5ff5\uff0c\u8bc1\u636e\u5f15\u5bfc\u7684\u4e30\u5bcc\u5668\u6574\u5408\u5916\u90e8\u7ed3\u6784\u5316\u5386\u53f2\u8d44\u6e90\u786e\u4fdd\u5fe0\u5b9e\u6027", "result": "\u57fa\u4e8e\u300a\u4e8c\u5341\u56db\u53f2\u300b\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u3001\u9886\u57df\u611f\u77e5\u7684\u4e2d\u56fd\u53e4\u4ee3\u4e8b\u4ef6\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\u653f\u6cbb\u3001\u519b\u4e8b\u3001\u5916\u4ea4\u548c\u793e\u4f1a\u751f\u6d3b\u3002\u8bc4\u4f30\u663e\u793a\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u8986\u76d6\u8303\u56f4\u5f97\u5230\u6539\u5584\uff0c\u5206\u7c7b\u4f53\u7cfb\u652f\u6301\u8de8\u6587\u5316\u5bf9\u9f50", "conclusion": "CHisAgent\u6846\u67b6\u80fd\u6709\u6548\u81ea\u52a8\u6784\u5efa\u5386\u53f2\u5206\u7c7b\u4f53\u7cfb\uff0c\u89e3\u51b3LLM\u5728\u5386\u53f2\u6587\u5316\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u8de8\u6587\u5316\u77e5\u8bc6\u5bf9\u9f50"}}
{"id": "2601.05495", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05495", "abs": "https://arxiv.org/abs/2601.05495", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "title": "MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding", "comment": "13 pages, 11 figures", "summary": "Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.", "AI": {"tldr": "MMViR\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u3001\u591a\u7c92\u5ea6\u7ed3\u6784\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u4fe1\u606f\u5197\u4f59\u95ee\u9898\uff0c\u901a\u8fc7\u5173\u952e\u8f6c\u6298\u70b9\u5206\u5272\u548c\u4e09\u7ea7\u63cf\u8ff0\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u548c\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u9762\u4e34\u6311\u6218\uff1a\u76f4\u63a5\u7f16\u7801\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u7b80\u5355\u7684\u89c6\u9891\u8f6c\u6587\u672c\u65b9\u6cd5\u4f1a\u4ea7\u751f\u5197\u4f59\u6216\u788e\u7247\u5316\u5185\u5bb9\uff0c\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u4e2d\u7684\u590d\u6742\u4e8b\u4ef6\u3001\u591a\u6837\u573a\u666f\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "MMViR\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u8f6c\u6298\u70b9\u6765\u5206\u5272\u89c6\u9891\uff0c\u6784\u5efa\u4e09\u7ea7\u63cf\u8ff0\u7ed3\u6784\uff1a\u5168\u5c40\u53d9\u4e8b\u4e0e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u57fa\u4e8e\u67e5\u8be2\u7684\u9ad8\u6548\u68c0\u7d22\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u591a\u79cd\u573a\u666f\u3002", "result": "\u5728\u95ee\u7b54\u3001\u6458\u8981\u548c\u68c0\u7d22\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMMViR\u8d85\u8d8a\u4e86\u5148\u524d\u6700\u5f3a\u65b9\u6cd5\uff0c\u5728\u5c0f\u65f6\u7ea7\u89c6\u9891\u7406\u89e3\u4e0a\u5b9e\u73b0\u4e8619.67%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u5904\u7406\u5ef6\u8fdf\u964d\u4f4e\u5230\u539f\u6765\u768445.4%\u3002", "conclusion": "MMViR\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u3001\u591a\u7c92\u5ea6\u7ed3\u6784\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7406\u89e3\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.05521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05521", "abs": "https://arxiv.org/abs/2601.05521", "authors": ["Jiayu Fang", "Zhiqi Shao", "Haoning Xi", "Boris Choy", "Junbin Gao"], "title": "Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management", "comment": "38pages, 18figures", "summary": "The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.", "AI": {"tldr": "MLA-STNet\uff1a\u57fa\u4e8eMamba\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u57ce\u5e02\u4e8b\u6545\u98ce\u9669\u9884\u6d4b\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6a21\u5757\u8bbe\u8ba1\u5904\u7406\u57ce\u5e02\u6570\u636e\u5f02\u8d28\u6027\u548c\u65f6\u7a7a\u6ce2\u52a8\uff0c\u5728\u7ebd\u7ea6\u548c\u829d\u52a0\u54e5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57ce\u5e02\u4e8b\u6545\u6570\u636e\u5177\u6709\u5f02\u8d28\u6027\u3001\u62a5\u544a\u4e0d\u4e00\u81f4\u3001\u805a\u7c7b\u7a00\u758f\u3001\u5468\u671f\u6027\u548c\u566a\u58f0\u7b49\u56fa\u6709\u7279\u6027\uff0c\u52a0\u4e0a\u6cbb\u7406\u788e\u7247\u5316\u548c\u62a5\u544a\u6807\u51c6\u4e0d\u517c\u5bb9\uff0c\u963b\u788d\u4e86\u8de8\u57ce\u5e02\u4e8b\u6545\u9884\u9632\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u63d0\u51faMLA-STNet\u7edf\u4e00\u7cfb\u7edf\uff0c\u5c06\u4e8b\u6545\u98ce\u9669\u9884\u6d4b\u6784\u5efa\u4e3a\u591a\u57ce\u5e02\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\u3002\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1aSTG-MA\uff08\u65f6\u7a7a\u5730\u7406Mamba\u6ce8\u610f\u529b\uff09\u6291\u5236\u4e0d\u7a33\u5b9a\u65f6\u7a7a\u6ce2\u52a8\u5e76\u589e\u5f3a\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\uff1bSTS-MA\uff08\u65f6\u7a7a\u8bed\u4e49Mamba\u6ce8\u610f\u529b\uff09\u901a\u8fc7\u5171\u4eab\u53c2\u6570\u8bbe\u8ba1\u7f13\u89e3\u8de8\u57ce\u5e02\u5f02\u8d28\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e2a\u4f53\u8bed\u4e49\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5728\u7ebd\u7ea6\u548c\u829d\u52a0\u54e5\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c75\u4e2a\u5b9e\u9a8c\uff0c\u6db5\u76d6\u5168\u5929\u548c\u9ad8\u9891\u4e8b\u6545\u65f6\u6bb5\u4e24\u79cd\u9884\u6d4b\u573a\u666f\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\uff0cMLA-STNet\u5b9e\u73b0RMSE\u964d\u4f4e6%\u3001Recall\u63d0\u9ad88%\u3001MAP\u63d0\u9ad85%\uff0c\u572850%\u8f93\u5165\u566a\u58f0\u4e0b\u6027\u80fd\u53d8\u5316\u5c0f\u4e8e1%\u3002", "conclusion": "MLA-STNet\u6709\u6548\u7edf\u4e00\u4e86\u5f02\u6784\u57ce\u5e02\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u8de8\u57ce\u5e02\u4e8b\u6545\u9884\u9632\u7cfb\u7edf\uff0c\u4e3a\u534f\u8c03\u548c\u6570\u636e\u9a71\u52a8\u7684\u57ce\u5e02\u5b89\u5168\u7ba1\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.05537", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05537", "abs": "https://arxiv.org/abs/2601.05537", "authors": ["Wei Zhou", "Hong Huang", "Ruize Shi", "Bang Liu"], "title": "Scalable Heterogeneous Graph Learning via Heterogeneous-aware Orthogonal Prototype Experts", "comment": null, "summary": "Heterogeneous Graph Neural Networks(HGNNs) have advanced mainly through better encoders, yet their decoding/projection stage still relies on a single shared linear head, assuming it can map rich node embeddings to labels. We call this the Linear Projection Bottleneck: in heterogeneous graphs, contextual diversity and long-tail shifts make a global head miss fine semantics, overfit hub nodes, and underserve tail nodes. While Mixture-of-Experts(MoE) could help, naively applying it clashes with structural imbalance and risks expert collapse. We propose a Heterogeneous-aware Orthogonal Prototype Experts framework named HOPE, a plug-and-play replacement for the standard prediction head. HOPE uses learnable prototype-based routing to assign instances to experts by similarity, letting expert usage follow the natural long-tail distribution, and adds expert orthogonalization to encourage diversity and prevent collapse. Experiments on four real datasets show consistent gains across SOTA HGNN backbones with minimal overhead.", "AI": {"tldr": "HOPE\u6846\u67b6\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u539f\u578b\u8def\u7531\u548c\u4e13\u5bb6\u6b63\u4ea4\u5316\u89e3\u51b3\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u7ebf\u6027\u6295\u5f71\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709HGNNs\u7684\u89e3\u7801/\u6295\u5f71\u9636\u6bb5\u4ecd\u4f7f\u7528\u5355\u4e00\u7684\u5171\u4eab\u7ebf\u6027\u5934\uff0c\u5047\u8bbe\u5b83\u80fd\u5c06\u4e30\u5bcc\u7684\u8282\u70b9\u5d4c\u5165\u6620\u5c04\u5230\u6807\u7b7e\u3002\u4f46\u5728\u5f02\u8d28\u56fe\u4e2d\uff0c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u548c\u957f\u5c3e\u5206\u5e03\u4f7f\u5f97\u5168\u5c40\u5934\u4f1a\u9519\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u8fc7\u62df\u5408\u4e2d\u5fc3\u8282\u70b9\uff0c\u800c\u5bf9\u5c3e\u90e8\u8282\u70b9\u670d\u52a1\u4e0d\u8db3", "method": "\u63d0\u51faHOPE\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u57fa\u4e8e\u539f\u578b\u7684\u8def\u7531\u673a\u5236\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u5c06\u5b9e\u4f8b\u5206\u914d\u7ed9\u4e13\u5bb6\uff0c\u8ba9\u4e13\u5bb6\u4f7f\u7528\u9075\u5faa\u81ea\u7136\u7684\u957f\u5c3e\u5206\u5e03\uff1b2\uff09\u6dfb\u52a0\u4e13\u5bb6\u6b63\u4ea4\u5316\u6765\u9f13\u52b1\u591a\u6837\u6027\u5e76\u9632\u6b62\u4e13\u5bb6\u5d29\u6e83", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHOPE\u5728\u591a\u4e2aSOTA HGNN\u9aa8\u5e72\u7f51\u7edc\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u5f00\u9500\u6700\u5c0f", "conclusion": "HOPE\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9884\u6d4b\u5934\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u7ebf\u6027\u6295\u5f71\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u539f\u578b\u8def\u7531\u548c\u4e13\u5bb6\u6b63\u4ea4\u5316\u673a\u5236\u66f4\u597d\u5730\u5904\u7406\u4e86\u5f02\u8d28\u56fe\u4e2d\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u548c\u957f\u5c3e\u5206\u5e03\u6311\u6218"}}
{"id": "2601.05633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05633", "abs": "https://arxiv.org/abs/2601.05633", "authors": ["Nuoyan Lyu", "Bingbing Xu", "Weihao Meng", "Yige Yuan", "Yang Zhang", "Zhiyong Huang", "Tat-Seng Chua", "Huawei Shen"], "title": "GIFT: Games as Informal Training for Generalizable LLMs", "comment": null, "summary": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u6e38\u620f\u4f5c\u4e3aLLM\u975e\u6b63\u5f0f\u5b66\u4e60\u73af\u5883\uff0c\u901a\u8fc7\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "LLM\u5728\u5f62\u5f0f\u5316\u5b66\u4e60\u4efb\u52a1\uff08\u5982\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\"\u5b9e\u8df5\u667a\u6167\"\u548c\u6cdb\u5316\u667a\u80fd\uff08\u5982\u6218\u7565\u521b\u9020\u529b\u548c\u793e\u4ea4\u63a8\u7406\uff09\u3002\u8fd9\u79cd\u5dee\u8ddd\u6e90\u4e8e\u7f3a\u4e4f\u975e\u6b63\u5f0f\u5b66\u4e60\uff0c\u800c\u6e38\u620f\u73af\u5883\u53ef\u4ee5\u63d0\u4f9b\u4e92\u52a8\u53cd\u9988\u548c\u62bd\u8c61\u590d\u6742\u6027\u6765\u57f9\u517b\u591a\u6837\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5c06\u6e38\u620f\u4f5c\u4e3aLLM\u975e\u6b63\u5f0f\u5b66\u4e60\u7684\u4e3b\u8981\u73af\u5883\uff0c\u5229\u7528\u5176\u5185\u5728\u5956\u52b1\u4fe1\u53f7\u548c\u62bd\u8c61\u590d\u6742\u6027\u3002\u4e3a\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u4efb\u52a1\u7ec4\u5408\u5f3a\u5236\u6267\u884c\u660e\u786e\u7684\"AND\"\u76ee\u6807\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u4efb\u52a1\u6df7\u5408\u4f18\u5316\u9690\u542b\u7684\"OR\"\u76ee\u6807\u3002", "result": "\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u5728Matrix Games\u3001TicTacToe\u548cWho's the Spy\u6e38\u620f\u4e2d\u9a8c\u8bc1\uff0c\u6e38\u620f\u57fa\u7840\u7684\u975e\u6b63\u5f0f\u5b66\u4e60\u4e0d\u4ec5\u80fd\u9632\u6b62\u4efb\u52a1\u5e72\u6270\uff0c\u8fd8\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5728\u5e7f\u6cdb\u80fd\u529b\u5bfc\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6e38\u620f\u73af\u5883\u4e3aLLM\u975e\u6b63\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7efc\u5408\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.05607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05607", "abs": "https://arxiv.org/abs/2601.05607", "authors": ["Zijun Min", "Bingshuai Liu", "Ante Wang", "Long Zhang", "Anxiang Zeng", "Haibo Zhang", "Jinsong Su"], "title": "Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.", "AI": {"tldr": "DHPO\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86GRPO\u7684token\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u548cGSPO\u7684\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u901a\u8fc7\u52a0\u6743\u673a\u5236\u548c\u5206\u652f\u7279\u5b9a\u88c1\u526a\u7b56\u7565\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RLVR\u7b97\u6cd5\u5728\u7c92\u5ea6\u4e0a\u5404\u6709\u4f18\u7f3a\u70b9\uff1aGRPO\u4f7f\u7528token\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u4fdd\u7559\u4e86\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u4f46\u65b9\u5dee\u9ad8\u4e0d\u7a33\u5b9a\uff1bGSPO\u4f7f\u7528\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u5339\u914d\u5e8f\u5217\u7ea7\u5956\u52b1\u4f46\u727a\u7272\u4e86token\u7ea7\u4fe1\u7528\u5206\u914d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faDHPO\uff08\u52a8\u6001\u6df7\u5408\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5728\u5355\u4e00\u88c1\u526a\u4ee3\u7406\u76ee\u6807\u4e2d\u6865\u63a5GRPO\u548cGSPO\u3002\u7ed3\u5408token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u63a2\u7d22\u4e86\u5e73\u5747\u6df7\u5408\u548c\u71b5\u5f15\u5bfc\u6df7\u5408\u4e24\u79cd\u53d8\u4f53\u3002\u91c7\u7528\u5206\u652f\u7279\u5b9a\u88c1\u526a\u7b56\u7565\uff0c\u5728\u6df7\u5408\u524d\u5206\u522b\u7ea6\u675f\u4e24\u79cd\u6bd4\u7387\u5728\u5404\u81ea\u7684\u4fe1\u4efb\u533a\u57df\u5185\u3002", "result": "\u5728\u4e03\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728Qwen3\u7cfb\u5217\u7684\u5bc6\u96c6\u548cMoE\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDHPO\u4e00\u81f4\u4f18\u4e8eGRPO\u548cGSPO\u3002", "conclusion": "DHPO\u6210\u529f\u7ed3\u5408\u4e86token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u673a\u5236\u548c\u7a33\u5b9a\u5316\u7b56\u7565\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
