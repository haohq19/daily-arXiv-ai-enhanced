<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/abs/2511.16166)
*Zeting Liu,Zida Yang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EvoVLA是一个自监督的视觉语言动作框架，通过三个阶段对齐的组件解决长时程机器人操作中的阶段幻觉问题，在模拟和真实环境中都显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在阶段幻觉问题，即智能体利用粗粒度评估信号在多步骤任务中走捷径，报告高进度但未真正完成任务。

Method: 提出三个互补组件：阶段对齐奖励（SAR）使用三元组对比学习和Gemini生成的难负样本来防止视觉捷径；基于姿态的对象探索（POE）将好奇心建立在相对对象-夹爪姿态而非原始像素上；长时程记忆使用选择性上下文保留和门控融合来稳定内在塑造。

Result: 在Discoverse-L基准测试中，EvoVLA比最强基线（OpenVLA-OFT）平均任务成功率提高10.2个百分点，达到69.2%。样本效率提高1.5倍，阶段幻觉从38.5%降至14.8%。真实世界部署在四个操作任务中平均成功率达54.6%，比OpenVLA-OFT高11个百分点。

Conclusion: EvoVLA有效解决了VLA模型中的阶段幻觉问题，实现了从模拟到真实环境的有效迁移和强泛化能力。

Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

</details>


### [2] [Graph Neural Networks for Surgical Scene Segmentation](https://arxiv.org/abs/2511.16430)
*Yihan Li,Nikhil Churamani,Maria Robu,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 提出两种结合Vision Transformer和Graph Neural Networks的分割模型，通过图结构增强对手术场景中解剖结构的空间和语义理解，在肝胆囊解剖识别任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在手术场景分割中面临遮挡、长距离依赖和罕见结构几何特征捕捉困难的问题，需要增强空间和语义理解能力。

Method: 提出两种模型：(1) 静态k近邻图+GCNII网络实现稳定的长距离信息传播；(2) 动态可微图生成器+GAT网络支持自适应拓扑学习，均基于ViT特征编码器。

Result: 在Endoscapes-Seg50和CholecSeg8k基准测试中，mIoU提升7-8%，mDice提升6%，在薄、罕见和安全关键结构上表现优异。

Conclusion: 图基分割方法结合ViT全局上下文和图基关系推理，提升了手术场景分割的性能和解剖一致性，为更安全的腹腔镜手术提供精确的解剖特征识别。

Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.

</details>


### [3] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 本文提出了Video-Next-Event Prediction (VNEP)任务，将传统文本回答的下一事件预测扩展为视频回答，并开发了VANS模型，通过强化学习将视觉语言模型与视频扩散模型对齐，在程序性和预测性基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 视频具有传达物理世界信息的独特能力，而语言模型在视频生成方面主要局限于娱乐应用。作者发现将视频作为下一事件预测的新回答模态的未充分利用机会，能够提供更直观和定制化的程序学习和创意探索答案。

Method: 提出VANS模型，使用强化学习将视觉语言模型(VLM)与视频扩散模型(VDM)对齐。核心是Joint-GRPO方法，通过共享奖励机制优化VLM生成准确且易于可视化的描述，同时指导VDM生成忠实于描述和输入视觉上下文的视频。构建了VANS-Data-100K专用数据集。

Result: 在程序性和预测性基准测试中，VANS在视频事件预测和可视化方面都达到了最先进的性能水平。

Conclusion: VNEP任务将下一事件预测从"讲述"转变为"展示"，VANS模型通过强化学习有效解决了多模态理解、指令条件推理和视觉语义一致性视频生成的挑战，为程序学习和创意应用开辟了新途径。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty](https://arxiv.org/abs/2511.16225)
*Victor Croisfelt,João Henrique Inacio de Souza,Shashi Raj Pandey,Beatriz Soret,Petar Popovski*

Main category: cs.LG

TL;DR: 提出了一种基于自适应时间窗口整合的神经启发式非阻塞推理范式，能够动态适应异构流中的随机延迟模式，无需依赖参考模态要求。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的非阻塞推理方法依赖参考模态范式，需要一个模态输入完全接收后才能处理，且需要昂贵的离线分析。不确定的通信延迟挑战了推理过程的时间流。

Method: 采用自适应时间窗口整合(TWIs)来动态调整异构流中的随机延迟模式，放松参考模态要求，构建通信延迟感知框架。

Result: 在音频-视觉事件定位任务上的实验表明，相比最先进方法，对网络动态具有更好的适应性。

Conclusion: 该框架实现了鲁棒的实时推理，并在准确性-延迟权衡方面提供了更精细的控制。

Abstract: Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.

</details>


### [5] [Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning](https://arxiv.org/abs/2511.16333)
*Mohammad Areeb Qazi,Maryam Nadeem,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 本文综述了医疗领域的世界模型，重点分析了其在医学影像、电子健康记录和机器人手术三个领域的应用，并提出了从L1到L4的能力评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型缺乏物理基础和时序推理能力，无法满足临床决策支持的需求。世界模型能够学习多模态、时序一致且动作条件化的表示，反映医疗过程的物理和因果结构。

Method: 通过能力评估框架（L1时序预测、L2动作条件预测、L3反事实推演、L4规划控制）系统性地综述了医疗世界模型的研究进展，涵盖医学影像诊断、疾病进展建模和手术规划三个应用领域。

Result: 大多数系统达到L1-L2级别，少数达到L3，极少数达到L4级别。识别出行动空间定义不清、安全约束不足、干预验证薄弱等限制临床可靠性的关键问题。

Conclusion: 提出了将生成骨干网络（Transformer、扩散模型、VAE）与因果/机械基础相结合的研究议程，旨在开发临床可靠、预测优先的世界模型，为医疗安全决策提供支持。

Abstract: Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.

</details>


### [6] [Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin](https://arxiv.org/abs/2511.16523)
*Ming-Lun Lee,Fu-Shiang Yang,Cheng-Kuan Lin,Yan-Ann Chen,Chih-Yu Lin,Yu-Chee Tseng*

Main category: cs.LG

TL;DR: 提出了首个专门用于动态客户端参与联邦学习(DPFL)的开源基准框架，揭示了动态参与导致的性能显著下降，并提出了KPFL解决方案来缓解不稳定性和知识丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究大多假设客户端持续参与，忽视了实际中客户端动态加入/离开的动态参与场景，且缺乏专门针对DPFL挑战的系统性基准框架。

Method: 开发了可配置数据分布、参与模式和评估指标的DPFL基准框架；提出了KPFL方法，通过维护共享知识池、双年龄和数据偏差加权以及生成知识蒸馏来缓解动态参与带来的问题。

Result: 基准测试显示动态参与导致FL模型性能显著下降；KPFL能有效提高模型鲁棒性和泛化能力。

Conclusion: 动态参与对联邦学习性能有重大影响，KPFL是解决这一问题的有效通用插件方法。

Abstract: Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.

</details>


### [7] [Toward Valid Generative Clinical Trial Data with Survival Endpoints](https://arxiv.org/abs/2511.16551)
*Perrine Chassat,Van Tuan Nguyen,Lucas Ducrot,Emilie Lanoy,Agathe Guilloux*

Main category: cs.LG

TL;DR: 提出一种变分自编码器方法，用于生成合成控制臂，解决临床试验中患者群体碎片化、招募缓慢和高成本的问题，特别是在肿瘤学和罕见病领域。


<details>
  <summary>Details</summary>
Motivation: 临床试验面临患者群体碎片化、招募缓慢和成本不可持续等挑战，特别是在肿瘤学和罕见病后期试验中。虽然已探索使用真实世界数据构建外部控制臂，但生成AI生成合成控制臂是一个有前景的替代方案。

Method: 引入变分自编码器，在统一的潜变量框架内联合生成混合类型协变量和生存结果，不假设独立删失。在合成和真实试验数据集上评估模型，包括隐私约束下的数据共享和控制臂增强两种场景。

Result: 在保真度、实用性和隐私指标上优于GAN基线方法，同时揭示了I类错误和功效的系统性误校准。提出了后生成选择程序来改善校准。

Conclusion: 该方法在生成生存建模方面取得了进展，但也凸显了开放挑战，特别是在校准改进方面。

Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation](https://arxiv.org/abs/2511.15825)
*Tuan-Anh Le,Anh Mai Vu,David Yang,Akash Awasthi,Hien Van Nguyen*

Main category: cs.AI

TL;DR: IMACT-CXR是一个基于多智能体对话的交互式胸部X光解读教学系统，整合了空间标注、视线分析、知识检索和图像推理功能，通过AutoGen工作流为学员提供实时辅导。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时处理学员的边界框标注、视线样本和自由文本观察的智能教学系统，帮助放射科培训生提高胸部X光解读能力，解决传统教学中缺乏实时反馈和个性化指导的问题。

Method: 采用多智能体架构，包含评估定位质量、生成苏格拉底式辅导、检索PubMed证据、推荐相似病例等专门智能体；使用贝叶斯知识追踪维护技能掌握度估计；集成肺叶分割模块进行解剖感知的视线反馈；通过安全提示防止过早泄露真实标签。

Result: 系统展示了响应迅速的辅导流程，具有有限的延迟、精确的答案泄露控制，以及向实时住院医师部署的可扩展性。初步评估显示在定位和诊断推理方面相比基线方法有所改进。

Conclusion: IMACT-CXR成功构建了一个功能全面的胸部X光解读教学系统，证明了多智能体对话辅导在医学影像教育中的有效性，为未来在真实住院医师培训中的部署奠定了基础。

Abstract: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

</details>


### [9] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: 提出了FOOTPASS数据集，这是首个用于足球比赛全程多模态、多智能体战术背景下逐场比赛动作识别的基准，结合计算机视觉输出和足球战术知识来生成可靠的逐场比赛数据流。


<details>
  <summary>Details</summary>
Motivation: 当前动作识别方法在构建可靠的逐场比赛数据方面仍不足，通常只能辅助而非完全自动化标注。同时战术建模、轨迹预测等研究需要基于比赛状态和逐场比赛数据，因此需要利用战术知识作为先验来支持基于计算机视觉的预测。

Method: 引入FOOTPASS数据集，支持开发利用计算机视觉任务输出（如跟踪、识别）和足球战术知识（包括长期战术规律）的球员中心动作识别方法。

Result: 创建了首个用于足球比赛全程多模态、多智能体战术背景下逐场比赛动作识别的基准数据集。

Conclusion: FOOTPASS数据集为数据驱动的体育分析提供了重要的输入，能够生成可靠的逐场比赛数据流，支持更自动化和可靠的逐场比赛数据提取。

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [10] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite是一个轻量级贝叶斯增强框架，用于提升基于Transformer的临床语言模型的不确定性感知能力，无需重新训练或架构修改，参数开销低于3%。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在临床决策支持中表现出潜力，但在模糊医疗案例中容易过度自信，需要校准的不确定性来确保可靠性。

Method: 集成三个组件：贝叶斯嵌入校准（使用蒙特卡洛dropout）、不确定性加权注意力（对token可靠性进行边际化）、置信度引导决策塑造（受临床风险最小化启发）。

Result: 在生物医学QA和临床预测基准测试中，持续改善校准和可信度，将过度自信降低32-48%；在模拟临床环境中，可通过标记不确定预测防止高达41%的诊断错误。

Conclusion: 该框架有效实现了医疗AI系统中可靠的不确定性传播，并提高了可解释性。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [11] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: SEG框架通过将支持结构优化集成到3D生成过程中，直接优化模型以减少支撑材料使用，显著降低3D打印的材料浪费和生产时间。


<details>
  <summary>Details</summary>
Motivation: 当前3D打印切片技术主要关注后处理优化，而非在模型生成阶段解决支撑效率问题。为减少支撑材料浪费和提高打印效率，需要在生成过程中直接优化模型设计。

Method: 提出SEG框架，将带偏移的直接偏好优化(ODPO)集成到3D生成流程中，在训练过程中融入支撑结构模拟，鼓励生成需要较少支撑的几何形状。

Result: 在Thingi10k-Val和GPT-3DP-Val基准数据集上的实验表明，SEG在支撑体积减少和可打印性方面显著优于TRELLIS、DPO和DRO等基线模型，同时保持对输入提示的高保真度。

Conclusion: SEG通过在生成过程中直接优化模型，为更可持续和高效的数字制造实践铺平了道路，具有改变3D打印方式的潜力。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>
