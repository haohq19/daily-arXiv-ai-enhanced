<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 开发基于卷积神经网络的OCSCC检测系统，结合图像采集硬件，研究图像分辨率对检测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌早期症状隐蔽、生长缓慢且发生在深部区域，常被漏诊导致可预防的死亡，需要有效的早期检测方法。

Method: 训练CNN模型识别OCSCC，使用4293张训练图像（良性/恶性肿瘤和阴性样本），设计图像采集硬件，测试不同分辨率图像的检测准确性。

Result: 图像分辨率越高，检测准确性越高，但呈现对数增长趋势，高像素数带来的收益递减。

Conclusion: CNN结合图像采集硬件可有效检测OCSCC，图像分辨率对检测准确性有重要影响，但高分辨率带来的边际效益递减。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [2] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一种轻量级的点和线段匹配器，通过注意力线消息传递(ALMP)组件显式暴露线的连通性，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将点和线匹配视为独立任务，GlueStick虽然实现了联合匹配但架构较重，无法满足实时应用或边缘设备部署需求。

Method: 提出轻量级匹配器LightGlueStick，核心创新是注意力线消息传递(ALMP)组件，显式暴露线的连通性，实现节点间高效通信。

Result: 在多个基准测试中建立了新的最先进性能，代码已开源。

Conclusion: LightGlueStick通过轻量级架构和ALMP组件，在保持高性能的同时实现了高效的点线联合匹配。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [3] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出了EMRRG框架，使用参数高效方法微调预训练的Mamba网络进行X射线报告生成，在三个基准数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成模型主要依赖LLM，对预训练视觉基础模型和先进微调技术探索有限，主流框架要么避免微调要么使用简单方法如LoRA，忽略了增强跨注意力机制的潜力。

Method: 将X射线图像分割为补丁并标记化，通过SSM基础的视觉骨干进行特征提取，使用Partial LoRA获得最优性能，结合混合解码器的LLM生成医疗报告，实现端到端训练。

Result: 在三个广泛使用的基准数据集上进行了广泛实验，充分验证了所提策略对X射线医疗报告生成的有效性。

Conclusion: EMRRG框架通过参数高效方法微调预训练Mamba网络，在X射线医疗报告生成任务中取得了优异性能，为医疗AI应用提供了新思路。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [4] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型的语义先验和经典机器学习算法，将视频理解重新定义为高维语义特征空间中的自监督时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在静态图像上的零样本推理能力尚未完全转化到视频领域，传统视频理解模型依赖大量标注数据和任务特定训练，成本高且可扩展性有限。

Method: 使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹，采用Kernel Temporal Segmentation进行时序分割，然后通过无监督密度聚类识别重复场景和主题，最后选择代表性关键帧并利用VLM生成文本描述。

Result: 该框架能够自动生成结构化的多模态视频内容摘要，实现了零样本的自动化视频结构分析。

Conclusion: 该方法提供了一种有效、可解释且模型无关的途径，用于视频内容的零样本自动化结构分析。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [5] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出了CARE框架，通过序列-图像对比对齐方法解决ADL识别中序列和图像表示的对齐问题，在三个CASAS数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有ADL识别方法存在表示级限制：序列方法保持时间顺序但对噪声敏感且缺乏空间感知，图像方法捕获全局模式但压缩时间动态和扭曲传感器布局，简单融合方法无法实现表示对齐。

Method: CARE框架包含：(1)时间感知、噪声弹性的序列编码；(2)空间感知和频率敏感的图像表示；(3)联合对比-分类目标进行端到端学习，通过序列-图像对比对齐确保跨表示对齐和任务特定可区分性。

Result: 在三个CASAS数据集上达到最先进性能：Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%，并展示了对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过对比对齐有效整合序列和图像表示的互补优势，为智能家居中可靠的ADL识别提供了潜力。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [6] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 基于TimeSformer视频识别模型，通过分析VALORANT游戏小地图中的战术特征（角色位置和游戏事件）来预测回合结果，在增强数据集上达到约81%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多基于比赛日志和统计数据，而VALORANT作为需要复杂策略的FPS游戏，其小地图信息包含丰富的战术特征，可用于提高预测准确性。

Method: 使用TimeSformer视频识别模型，从小地图信息中提取详细战术特征（角色位置和游戏事件），构建回合结果预测模型。

Result: 在增强战术事件标签的数据集上训练模型，达到约81%的预测准确率，特别是在回合中期阶段显著优于仅使用小地图信息的模型。

Conclusion: 利用比赛视频中的战术特征对于预测VALORANT回合结果非常有效，小地图中的战术信息能显著提升预测性能。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [7] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是第一个专门评估模型理解长视频能力的基准测试，重点关注人类语言、观点、动作等上下文元素，并整合视觉、音频和文本多模态。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估模型对长视频的理解能力方面存在不足，特别是在处理信息密集的长视频和多模态融合方面。

Method: 从FineVideo数据集中精选约1000个长视频，设计6个挑战性任务场景，开发三步半自动化数据质量保证流程。

Result: 实验结果显示全模态模型在精确时间定位和长距离因果推理任务中仍面临挑战，多模态融合存在信息丢失和处理偏差。

Conclusion: LongInsightBench为评估模型长视频理解能力提供了有效基准，揭示了当前全模态模型在多模态融合方面的局限性。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [8] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出了Semantic-E2VID框架，通过跨模态特征对齐和语义感知特征融合，将视觉语义知识从帧模态转移到事件模态，显著提升了事件到视频重建的质量和语义信息恢复能力。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然具有低延迟、高动态范围等优势，但由于只捕捉强度变化而忽略静态物体和背景，导致事件模态缺乏语义信息。现有的事件到视频重建方法往往忽视了语义信息的重要性。

Method: 1. 跨模态特征对齐模块：将Segment Anything Model的视觉语义知识转移到事件编码器；2. 语义感知特征融合块：整合学习到的语义特征形成富含语义的事件表示；3. 语义感知E2V监督：利用SAM生成的类别标签帮助模型重建语义细节。

Result: 在多个基准测试中，Semantic-E2VID显著提升了帧质量，优于最先进的事件到视频重建方法。

Conclusion: 通过引入视觉语义知识，Semantic-E2VID有效解决了事件模态缺乏语义信息的问题，为事件到视频重建任务提供了新的解决方案。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [9] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究通过系统评估25个合成人脸识别数据集，证明合成数据可替代真实数据集，最佳合成数据集识别准确率达95.67%，超越真实数据集CASIA-WebFace，同时提供隐私保护和偏见控制能力。


<details>
  <summary>Details</summary>
Motivation: 真实人脸数据集存在隐私侵犯和法律责任风险，合成数据作为隐私保护替代方案缺乏实证证据，需要填补这一关键空白。

Method: 系统文献回顾识别25个合成人脸数据集，结合实验验证，评估七个隐私保护合成数据关键要求，涉及超过1000万合成样本和五个标准基准测试。

Result: 最佳合成数据集VariFace和VIGFace识别准确率分别达95.67%和94.91%，超越真实数据集CASIA-WebFace(94.70%)，同时确保类内变异性和身份可分性，提供前所未有的偏见控制能力。

Conclusion: 合成人脸数据是科学可行且伦理必要的面部识别研究替代方案，能够平衡高精度与隐私保护需求。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [10] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 开发了一个基于视觉的马匹行为监控系统原型，使用YOLOv11和BoT-SORT进行目标检测与跟踪，能够自动识别马厩中的马匹和人员行为事件。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监控方法劳动密集且耗时，需要自动化系统来早期检测健康与福利问题。

Method: 结合目标检测和多目标跟踪技术，利用YOLOv11和BoT-SORT，基于目标轨迹和空间关系推断事件状态，使用CLIP和GroundingDINO辅助构建自定义数据集。

Result: 系统能区分五种事件类型并考虑摄像头盲区，定性评估显示对马相关事件检测可靠，但人员检测因数据稀缺存在局限。

Conclusion: 为马场实时行为监控奠定了基础，对动物福利和厩舍管理具有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种针对大型推理模型的死锁攻击方法，通过训练恶意对抗嵌入诱导模型陷入无限推理循环，阻止其输出最终答案。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的链式思维推理机制引入了新的安全漏洞，需要研究如何利用这种迭代思维机制进行攻击。

Method: 使用优化的对抗嵌入鼓励推理步骤后的过渡性标记，结合后门植入策略确保攻击可靠激活，克服连续到离散的投影间隙问题。

Result: 在四种先进LRM和三个数学推理基准测试中实现100%攻击成功率，迫使模型生成达到最大标记限制，同时保持对良性输入的效用损失可忽略。

Conclusion: 该研究揭示了大型推理模型在推理效率方面的关键安全漏洞，这种攻击对现有缓解策略具有鲁棒性。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [12] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出了STAR（STate-aware AdapteR）模块，用于增强时间序列基础模型在多元时间序列异常检测中对状态变量的建模能力，解决了现有模型忽视状态变量分类特性导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中时间序列包含数值变量和离散状态变量，现有时间序列基础模型通常忽视状态变量的分类特性，将其与数值变量统一处理，导致无法充分利用状态信息甚至性能下降。

Method: STAR包含三个核心组件：身份引导的状态编码器、条件瓶颈适配器和数值-状态匹配模块，通过可学习状态记忆捕获状态变量的分类语义，动态生成条件适配参数，有效检测状态变量本身的异常。

Result: 在真实世界数据集上的广泛实验表明，STAR能够显著提升现有时间序列基础模型在多元时间序列异常检测中的性能。

Conclusion: STAR作为即插即用模块，成功解决了时间序列基础模型对状态变量建模不足的问题，有效提升了异常检测性能。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [13] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出IB-FT方法解决LLMs在代码生成任务中的记忆障碍问题，通过信息瓶颈压缩记忆特征，提升泛化性能


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域微调时存在记忆障碍问题，即模型对下游代码数据的强记忆会阻碍其学习新的可泛化知识

Method: 提出基于信息瓶颈的微调方法(IB-FT)，在代码数据的隐藏表示上应用IB惩罚，压缩虚假记忆特征同时保留任务相关信息

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上，IB-FT显著缓解记忆障碍，提升Top-1性能，并在更严格的多样本指标Pass@k^(m)下获得更稳定的增益

Conclusion: IB-FT方法能有效克服LLMs在代码生成中的记忆障碍，相比传统微调方法具有更好的泛化性能和稳定性

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [14] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 该论文探讨了智能交通系统中的交通预测技术，重点分析了图神经网络在捕捉复杂空间依赖性和动态时间演化模式方面的应用，并指出了现有方法在事件信息处理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要可靠且响应迅速的交通预测模型来支持智能交通系统的建设，改善城市资源分配和出行体验。

Method: 使用图神经网络（GNNs）作为主流方法，通过复杂的图卷积结构和时间建模机制来捕捉道路网络拓扑中的空间依赖性和交通流数据的时间演化模式。早期方法主要依赖手动设计的事件特征。

Result: 基础模型如STGCN、GraphWaveNet以及更新的STWave和D2STGNN在标准交通数据集上表现出色，特别擅长捕捉具有周期性规律的交通模式。

Conclusion: 虽然现有方法在周期性交通模式预测方面有效，但在处理事件信息时仍存在局限性，主要依赖于专家先验知识，难以泛化到多样复杂的未知事件，且手动特征可能导致语义细节丢失。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [15] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习算法扩展到离散和连续复值波系统，在弱耗散状态下有效，适用于多种物理系统，通过可训练局部势能替代节点间连接，在激子-极化激子凝聚体中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法难以在物理神经网络中实现，平衡传播(EP)作为替代方案具有相似效率和原位训练潜力，但需要扩展到更广泛的物理系统。

Method: 扩展EP学习到离散和连续复值波系统，在弱耗散状态下有效，用可训练局部势能替代节点间连接，在激子-极化激子凝聚体中测试。

Result: 在标准基准测试中（包括逻辑任务和手写数字识别）展示了稳定收敛，验证了方法的有效性。

Conclusion: 为系统控制仅限于局部参数的物理系统中的原位学习建立了实用路径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [16] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE：基于GRU的简单高效模型，用于处理不规则采样的多变量时间序列，在预测任务中达到或超越SOTA方法，同时具有实现简单、调参少、计算开销低的优势。


<details>
  <summary>Details</summary>
Motivation: 现有复杂架构在处理不规则采样时间序列时的真正优势不明确，需要验证简单高效的RNN架构是否仍具有竞争力。

Method: 基于GRU架构，引入两种重置机制：观测触发重置和时间触发重置，使用可学习的指数衰减来支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在下一观测和下一事件预测任务上达到竞争性甚至优于SOTA方法的性能。

Conclusion: GRUwE证明了简单RNN架构的持续竞争力，提供了实现简单、调参少、计算开销低的实用优势。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [17] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于运行时监控语言(RML)的新型语言奖励机，能够表示非正则、非马尔可夫任务的奖励函数，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数通常被视为黑盒映射，缺乏解释性，且传统奖励机只能表示正则语言，无法处理计数或参数化条件等复杂行为。

Method: 基于运行时监控语言(RML)构建语言奖励机，利用RML的内置内存机制来指定非正则、非马尔可夫任务的奖励函数。

Result: 实验证明了该方法在表达能力上的优势，在灵活事件处理和任务规范方面优于现有的奖励机方法。

Conclusion: 语言奖励机扩展了奖励函数的表达能力，能够处理更复杂的非正则行为，为强化学习提供了更强大的任务规范工具。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [18] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 本文质疑LLM在因果发现中的真实能力，指出现有评估存在数据泄露问题，提出基于新科学研究的评估方法和结合LLM与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM在因果发现中的虚假表现，揭示现有评估因预训练数据泄露而不可靠，探索LLM在真实科学发现中的可信度。

Method: 开发基于新科学研究的评估协议防止数据泄露；设计混合方法将LLM预测作为经典PC算法的先验知识。

Result: 在BNLearn基准上LLM表现接近完美，但在新构建的科学图谱上表现较差；将LLM预测作为PC算法先验可显著提高准确性。

Conclusion: 呼吁采用基于科学、抗泄露的基准测试，并投资开发适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [19] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现风洞验证精度低于1.04%，比现有数据集提升5倍，为数据驱动的空气动力学优化建立了新标准。


<details>
  <summary>Details</summary>
Motivation: 传统汽车空气动力学优化面临计算成本高与精度不足的权衡，现有机器学习数据集存在网格分辨率不足、组件缺失和验证误差超过5%等问题，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用精炼网格策略和严格的壁面y+控制。

Result: 数据集实现风洞验证精度低于1.04%，比现有数据集提升5倍；基于该数据训练的模型在实现生产级精度的同时，将计算成本从数周减少到几分钟。

Conclusion: DrivAerStar是首个连接学术机器学习研究和工业CFD实践的数据集，为汽车开发中的数据驱动空气动力学优化建立了新标准，展示了将高保真物理模拟与AI整合到工程领域的范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [20] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级评估框架来评估LLM生成的数学优化公式，超越了传统的整体评估方法，通过多个精细指标分析结构性和数值性错误。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在自然语言到数学优化公式转换的评估往往采用整体方法，依赖粗略指标如解精度或运行时间，无法揭示结构或数值错误。

Method: 开发了包含决策变量和约束的精确率与召回率、约束和目标函数均方根误差、基于令牌使用和延迟的效率指标的综合评估框架，评估了GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同复杂度优化问题下的六种提示策略。

Result: GPT-5表现最优，思维链、自一致性和模块化提示最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精度和决策变量指标次之，简洁输出提升计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级精度、简洁输出提高计算效率，为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [21] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专门的专家网络实现内置可解释性，解决了金融异常检测中机制不透明、静态图结构无法适应市场变化、统一检测机制无法识别特定类型异常等问题。


<details>
  <summary>Details</summary>
Motivation: 现有金融异常检测器将所有异常统一处理，产生标量分数而不揭示具体失效机制、风险集中点或干预方法，这种不透明性阻碍了有针对性的监管响应。

Method: 通过BiLSTM与自注意力捕获多尺度时间依赖，跨模态注意力融合时空信息，神经多源插值学习动态图，应力调制融合自适应平衡学习动态与结构先验，将异常路由到四个机制特定专家，产生双级可解释归因。

Result: 在100只美国股票（2017-2024）上实现了92.3%的13个主要事件检测率，提前3.8天，比最佳基线提高30.8个百分点。硅谷银行案例研究展示了异常演化跟踪能力。

Conclusion: 该框架通过架构嵌入而非事后应用的方式实现可解释性，能够自动识别时间机制而无需标注监督，为金融异常检测提供了更精准和可操作的解决方案。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [22] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 应用可解释AI技术（Grad-CAM和SHAP）解释PhaseNet地震检测模型的决策过程，并基于SHAP值开发了门控推理方案，在9000个波形测试集上F1分数达到0.98，优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在检测微地震事件时具有高精度，但其黑盒特性在关键应用中存在担忧，需要提高模型的可解释性和可靠性。

Method: 使用Grad-CAM和SHAP等可解释AI技术分析PhaseNet模型；开发SHAP门控推理方案，将模型输出与基于解释的指标结合以减少错误。

Result: Grad-CAM显示网络注意力与P波和S波到达时间一致；SHAP值量化特征贡献，确认垂直分量幅度驱动P相位拾取，水平分量主导S相位拾取；SHAP门控模型在测试集上F1分数0.98（精度0.99，召回率0.97），优于基准PhaseNet（F1分数0.97），对噪声具有更强鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [23] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 在低信噪比(SNR)数据中，通过向梯度下降训练过程添加标签噪声可以抑制噪声记忆化，改善神经网络泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易在训练中记忆噪声，特别是在低SNR数据中导致泛化能力下降。受标签噪声具有正则化效果的启发，研究是否可以通过引入标签噪声来提升神经网络在低SNR场景下的性能。

Method: 使用两层神经网络，在理想化的信号-噪声数据设置中，采用带标签噪声的梯度下降算法进行训练。

Result: 标签噪声GD能够抑制噪声记忆化，促进信号快速学习，同时控制过拟合，在低SNR下实现良好泛化。相比之下，标准GD容易过拟合噪声，测试误差存在非零下界。

Conclusion: 在基于梯度的训练中引入标签噪声可以有效改善神经网络在低SNR数据上的泛化能力。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [24] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出CEPerFed方法，通过客户端历史风险梯度和历史平均梯度协调本地与全局优化，并使用分层SVD策略减少通信开销，解决多脉冲MRI分类中的联邦学习数据异构性和通信效率问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI在临床实践中广泛应用，但训练鲁棒模型需要大量多样化数据且需保护隐私。联邦学习虽然可行，但面临数据异构性导致的模型收敛问题和大量参数传输带来的通信开销挑战。

Method: CEPerFed方法：1) 使用客户端历史风险梯度加权其他客户端贡献，增强本地更新可靠性；2) 使用历史平均梯度确保本地更新与全局优化方向一致；3) 采用分层SVD策略仅传输模型更新所需的关键信息。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed通过协调本地与全局优化以及减少通信开销，成功解决了联邦学习在多脉冲MRI分类中的数据异构性和通信效率问题。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [25] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）来生成合成数据，解决自杀预测中数据不平衡问题，并在真实测试数据上取得了良好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 自杀预测是预防的关键，但真实数据中阳性样本稀少，导致极端的类别不平衡问题，需要数据增强来解决。

Method: 使用机器学习模型（逻辑回归、随机森林、支持向量机）和深度学习技术（生成对抗网络GAN）生成合成数据样本来增强数据集。

Result: 在真实测试数据上，逻辑回归的加权精度0.99、召回率0.85、F1分数0.91；随机森林分别为0.98、0.99、0.99；支持向量机为0.99、0.76、0.86。LR和SVM能正确识别自杀尝试案例，但RF未能识别。

Conclusion: 这些结果突出了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [26] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 本文研究如何提高黑盒大语言模型作为分类器的操作粒度，通过分析其低基数数值输出的原因，并提出有效方法在不损失性能的情况下显著增加可用操作点。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs在需要特定指标约束的应用中表现不佳，因为其数值输出基数低，限制了操作点的精细调整能力。

Method: 首先分析LLMs低基数输出的原因，发现其偏向生成四舍五入但有信息量的语言化概率；然后实验标准提示工程、不确定性估计和置信度激发技术；最后提出有效方法来显著增加可用操作点的数量和多样性。

Result: 提出的方法在11个数据集和3个LLMs上提供了更细粒度的操作点，并实现了与基准方法相当或更好的性能。

Conclusion: 通过提出的方法，可以显著提高黑盒LLMs作为分类器的操作粒度，同时保持或提升性能，解决了其在约束指标应用中的局限性。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法基于不规则时间事件样本学习连续时间进展动态，且难以处理患者异质性（不同进展速率和路径）。

Method: 使用时间详细超图表示临床认可的进展轨迹，通过神经ODE框架学习连续时间进展动态，包含可学习的TD-超图拉普拉斯算子捕捉并发症标记的相互依赖关系。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在2型糖尿病和相关心血管疾病进展建模方面优于多个基线方法。

Conclusion: TD-HNODE能够有效建模疾病进展的连续时间动态，克服了现有方法在处理不规则时间数据和患者异质性方面的局限性。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [28] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型作为预测工具的能力，发现LLMs已展现出有前景的预测能力，但也存在事件召回不准确、数据源误解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，研究利用LLMs预测现实世界未来事件的潜力，这一新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务并将每个任务分解为不同的流水线阶段，以支持受控的大规模实验。

Result: 综合评估显示许多LLMs已展现出令人印象深刻的预测能力，如较小的校准误差、一致的预测置信度和有前景的市场回报。

Conclusion: 虽然LLMs具有预测潜力，但在实现卓越预测智能方面存在关键瓶颈，包括事件召回不准确、数据源误解以及接近解决时信息聚合速度慢于市场等问题。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [29] [Advancing Routing-Awareness in Analog ICs Floorplanning](https://arxiv.org/abs/2510.15387)
*Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: 提出一种基于强化学习和关系图卷积神经网络的自动布局引擎，专门针对模拟集成电路布局问题，通过提高网格分辨率、精确引脚信息集成和动态布线资源估计技术，在布线成功率和面积效率方面显著优于现有学习型方法。


<details>
  <summary>Details</summary>
Motivation: 解决模拟集成电路布局工程师对布线感知的布局解决方案的需求，克服传统方法在电气约束、特定问题要求和布局布线步骤相互依赖方面的限制。

Method: 使用强化学习和关系图卷积神经网络构建自动布局引擎，结合高网格分辨率、精确引脚信息集成和动态布线资源估计技术。

Result: 与现有学习型最先进技术相比，实现了13.8%的死区减少、40.6%的线长减少和73.4%的布线成功率提升。

Conclusion: 该方法能够平衡布线和面积效率，满足工业标准，为模拟集成电路布局提供了有效的布线感知解决方案。

Abstract: The adoption of machine learning-based techniques for analog integrated
circuit layout, unlike its digital counterpart, has been limited by the
stringent requirements imposed by electric and problem-specific constraints,
along with the interdependence of floorplanning and routing steps. In this
work, we address a prevalent concern among layout engineers regarding the need
for readily available routing-aware floorplanning solutions. To this extent, we
develop an automatic floorplanning engine based on reinforcement learning and
relational graph convolutional neural network specifically tailored to
condition the floorplan generation towards more routable outcomes. A
combination of increased grid resolution and precise pin information
integration, along with a dynamic routing resource estimation technique, allows
balancing routing and area efficiency, eventually meeting industrial standards.
When analyzing the place and route effectiveness in a simulated environment,
the proposed approach achieves a 13.8% reduction in dead space, a 40.6%
reduction in wirelength and a 73.4% increase in routing success when compared
to past learning-based state-of-the-art techniques.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [30] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 该论文提出了TUuD框架，用于评估大语言模型在动态变化的"现在"参考点下对时间-事件和事件-事件关系的理解能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过基于感官运动经验的空间隐喻来概念化时间体验，而大语言模型在时间理解和推理方面仍存在局限，需要评估它们如何解释随时间动态变化的参考框架。

Method: 引入TUuD框架，让LLMs对当前时刻与目标事件之间的相似性进行评分（0.00-1.00），量化两个时间点之间的感知时间对齐程度。

Result: 四个评估的LLMs显示出对指示性时间参考框架的可测量适应，相似性评分在现在附近达到峰值，并向过去和未来事件递减，但这种适应在超出近期情境时会减弱。

Conclusion: LLMs表现出部分类人的时间认知，但它们的时序推理仍然对参考框架变化和时间距离敏感。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [31] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述系统回顾了2014-2025年间自然语言处理技术在心脏病学领域的应用研究，分析了265篇相关文献，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病日益普遍且复杂，相关信息分散在患者叙述、医疗记录和科学文献等非结构化文本数据中。NLP技术能够分析这些数据，帮助医疗专业人员深入了解心脏病学，从而革新心脏问题的诊断、治疗和预防方法。

Method: 查询了六个文献数据库，通过严格筛选过程识别了265篇相关文章，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行分析，并进行时间分析以展示方法演变趋势。

Result: 分析显示这些维度内存在相当大的多样性，证明了NLP研究在该领域的广度。时间分析揭示了所涵盖过去十年中NLP方法的演变和变化趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心血管疾病研究中的广泛应用和发展趋势。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [32] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了VideoBiasEval框架，用于评估视频生成中的社会偏见，发现在对齐调优过程中会放大并稳定化社会偏见。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型通过基于人类偏好的奖励模型进行对齐调优，虽然提升了视觉质量，但可能无意中编码和放大社会偏见。

Method: 引入VideoBiasEval诊断框架，基于社会偏见分类学，采用基于事件的提示策略，分离语义内容和演员属性，并引入多粒度指标进行评估。

Result: 对齐调优不仅强化了表征偏见，还使其在时间上更加稳定，产生更平滑但更刻板的描绘。

Conclusion: 需要在对齐过程中进行偏见感知的评估和缓解，以确保公平和社会责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [33] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过大规模情感分析发现孟加拉语新闻存在明显的负面情绪主导，特别是愤怒、恐惧和失望，并提出了可视化情感线索的新闻聚合器设计方案。


<details>
  <summary>Details</summary>
Motivation: 研究新闻媒体如何通过情感框架影响公众情绪，以及负面标题如何获得更多关注和传播，从而鼓励媒体使用更强情感反应的报道方式。

Method: 使用Gemma-3 4B模型对30万条孟加拉语新闻标题和内容进行零样本推理，识别每篇文章的主要情绪和整体基调。

Result: 发现负面情绪明显占主导地位，特别是愤怒、恐惧和失望，不同媒体对相似新闻的情感描述存在显著差异。

Conclusion: 基于研究结果，提出了以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中的隐藏情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [34] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 本研究开发了多种深度上下文嵌入模型，用于提升心脏病学领域的临床命名实体识别，在英语、西班牙语和意大利语的临床病例报告中提取疾病和药物提及，在BioASQ MultiCardioNER共享任务中取得了优于平均水平的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据快速增长，需要从非结构化临床文本中解锁生物医学知识以支持数据驱动的临床系统发展。虽然上下文语言模型在英语语料库的命名实体识别中表现出色，但针对低资源语言临床文本的研究仍然稀缺。

Method: 探索了在通用领域文本上训练的单语和多语BERT模型的有效性，用于从英语、西班牙语和意大利语的临床病例报告中提取疾病和药物提及。

Result: 在西班牙语疾病识别中获得77.88%的F1分数，西班牙语药物识别92.09%，英语药物识别91.74%，意大利语药物识别88.9%，在所有子任务的测试排行榜中均优于平均和中位数F1分数。

Conclusion: 开发的深度上下文嵌入模型成功提升了多语言临床命名实体识别性能，特别是在低资源语言环境中，为数据驱动的临床系统提供了有效支持。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [35] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 提出了SimpleVSF框架，通过融合视觉语言模型和轨迹评分技术来增强端到端自动驾驶规划性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景中决策仍存在不足，需要更好的认知能力和决策机制

Method: 结合传统评分器和VLM增强评分器，使用权重融合器进行定量聚合，以及基于VLM的融合器进行定性上下文感知决策

Result: 在ICCV 2025 NAVSIM v2端到端驾驶挑战中取得领先地位，展示了最先进的性能

Conclusion: SimpleVSF框架在安全性、舒适性和效率之间实现了优越的平衡

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [36] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机的新型自定位系统，集成可见光通信(VLC)和可见光定位(VLP)，可在GPS失效环境（如隧道）中为车辆提供位置估计。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机的高时间分辨率和高动态范围特性，在GPS失效环境下实现车辆定位，解决隧道等场景中的定位问题。

Method: 使用Walsh-Hadamard码为多个LED分配唯一导频序列，通过事件相机识别单个LED，实现VLC高容量通信和基于相位相关(POC)的精确距离估计。

Result: 在30km/h车速下进行实地测试，距离估计均方根误差小于0.75米（范围达100米），误码率低于0.01。

Conclusion: 这是首个使用单个事件相机同时实现VLC和VLP功能的车辆系统，展示了在真实环境中的鲁棒性能。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>
