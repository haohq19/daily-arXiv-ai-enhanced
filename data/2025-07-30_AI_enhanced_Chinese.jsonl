{"id": "2507.21147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21147", "abs": "https://arxiv.org/abs/2507.21147", "authors": ["Fabrizio Lo Scudo", "Alessio De Rango", "Luca Furnari", "Alfonso Senatore", "Donato D'Ambrosio", "Giuseppe Mendicino", "Gianluigi Greco"], "title": "Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning", "comment": "To appear in the Proceedings of ECAI 2025", "summary": "Wildfires significantly impact natural ecosystems and human health, leading\nto biodiversity loss, increased hydrogeological risks, and elevated emissions\nof toxic substances. Climate change exacerbates these effects, particularly in\nregions with rising temperatures and prolonged dry periods, such as the\nMediterranean. This requires the development of advanced risk management\nstrategies that utilize state-of-the-art technologies. However, in this\ncontext, the data show a bias toward an imbalanced setting, where the incidence\nof wildfire events is significantly lower than typical situations. This\nimbalance, coupled with the inherent complexity of high-dimensional\nspatio-temporal data, poses significant challenges for training deep learning\narchitectures. Moreover, since precise wildfire predictions depend mainly on\nweather data, finding a way to reduce computational costs to enable more\nfrequent updates using the latest weather forecasts would be beneficial. This\npaper investigates how adopting a contrastive framework can address these\nchallenges through enhanced latent representations for the patch's dynamic\nfeatures. We thus introduce a new morphology-based curriculum contrastive\nlearning that mitigates issues associated with diverse regional characteristics\nand enables the use of smaller patch sizes without compromising performance. An\nexperimental analysis is performed to validate the effectiveness of the\nproposed modeling strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u6001\u5b66\u7684\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u91ce\u706b\u9884\u6d4b\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u65f6\u7a7a\u6570\u636e\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91ce\u706b\u5bf9\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u5065\u5eb7\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u73b0\u6709\u6570\u636e\u5b58\u5728\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u590d\u6742\u6027\uff0c\u4e14\u4f9d\u8d56\u5929\u6c14\u6570\u636e\u7684\u9884\u6d4b\u9700\u8981\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51fa\u5f62\u6001\u5b66\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f18\u5316\u52a8\u6001\u7279\u5f81\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u66f4\u5c0f\u7684\u8865\u4e01\u5c3a\u5bf8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u533a\u57df\u591a\u6837\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u5f62\u6001\u5b66\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u4e3a\u91ce\u706b\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6570\u636e\u73af\u5883\u3002"}}
{"id": "2507.21965", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21965", "abs": "https://arxiv.org/abs/2507.21965", "authors": ["Yi Wang", "Peiyao Zhang", "Mojtaba Esfandiari", "Peter Gehlbach", "Iulian I. Iordachita"], "title": "A Deep Learning-Driven Autonomous System for Retinal Vein Cannulation: Validation Using a Chicken Embryo Model", "comment": null, "summary": "Retinal vein cannulation (RVC) is a minimally invasive microsurgical\nprocedure for treating retinal vein occlusion (RVO), a leading cause of vision\nimpairment. However, the small size and fragility of retinal veins, coupled\nwith the need for high-precision, tremor-free needle manipulation, create\nsignificant technical challenges. These limitations highlight the need for\nrobotic assistance to improve accuracy and stability. This study presents an\nautomated robotic system with a top-down microscope and B-scan optical\ncoherence tomography (OCT) imaging for precise depth sensing. Deep\nlearning-based models enable real-time needle navigation, contact detection,\nand vein puncture recognition, using a chicken embryo model as a surrogate for\nhuman retinal veins. The system autonomously detects needle position and\npuncture events with 85% accuracy. The experiments demonstrate notable\nreductions in navigation and puncture times compared to manual methods. Our\nresults demonstrate the potential of integrating advanced imaging and deep\nlearning to automate microsurgical tasks, providing a pathway for safer and\nmore reliable RVC procedures with enhanced precision and reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\uff08OCT\uff09\u6210\u50cf\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u9759\u8109\u63d2\u7ba1\uff08RVC\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89c6\u7f51\u819c\u9759\u8109\u63d2\u7ba1\uff08RVC\uff09\u6280\u672f\u9762\u4e34\u5c0f\u5c3a\u5bf8\u3001\u8106\u5f31\u6027\u53ca\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u9700\u6c42\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u673a\u5668\u4eba\u8f85\u52a9\u63d0\u5347\u624b\u672f\u6548\u679c\u3002", "method": "\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u7684\u663e\u5fae\u955c\u548cB-scan OCT\u6210\u50cf\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u9488\u5934\u5bfc\u822a\u3001\u63a5\u89e6\u68c0\u6d4b\u548c\u9759\u8109\u7a7f\u523a\u8bc6\u522b\uff0c\u4f7f\u7528\u9e21\u80da\u80ce\u6a21\u578b\u6a21\u62df\u4eba\u7c7b\u89c6\u7f51\u819c\u9759\u8109\u3002", "result": "\u7cfb\u7edf\u81ea\u4e3b\u68c0\u6d4b\u9488\u5934\u4f4d\u7f6e\u548c\u7a7f\u523a\u4e8b\u4ef6\u7684\u51c6\u786e\u7387\u8fbe85%\uff0c\u5b9e\u9a8c\u663e\u793a\u5bfc\u822a\u548c\u7a7f\u523a\u65f6\u95f4\u8f83\u624b\u52a8\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u7ed3\u5408\u5148\u8fdb\u6210\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u53ef\u81ea\u52a8\u5316\u663e\u5fae\u624b\u672f\u4efb\u52a1\uff0c\u4e3aRVC\u624b\u672f\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u53ef\u9760\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21234", "abs": "https://arxiv.org/abs/2507.21234", "authors": ["Fatema Binte Hassan", "Md Al Jubair", "Mohammad Mehadi Hasan", "Tahmid Hossain", "S M Mehebubur Rahman Khan Shuvo", "Mohammad Shamsul Arefin"], "title": "Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability", "comment": null, "summary": "In recent years, social media platforms have become prominent spaces for\nindividuals to express their opinions on ongoing events, including criminal\nincidents. As a result, public sentiment can shift dynamically over time. This\nstudy investigates the evolving public perception of crime-related news by\nclassifying user-generated comments into three categories: positive, negative,\nand neutral. A newly curated dataset comprising 28,528 Bangla-language social\nmedia comments was developed for this purpose. We propose a transformer-based\nmodel utilizing the XLM-RoBERTa Base architecture, which achieves a\nclassification accuracy of 97%, outperforming existing state-of-the-art methods\nin Bangla sentiment analysis. To enhance model interpretability, explainable AI\ntechnique is employed to identify the most influential features driving\nsentiment classification. The results underscore the effectiveness of\ntransformer-based models in processing low-resource languages such as Bengali\nand demonstrate their potential to extract actionable insights that can support\npublic policy formulation and crime prevention strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528XLM-RoBERTa Base\u67b6\u6784\u7684Transformer\u6a21\u578b\uff0c\u5bf9\u5b5f\u52a0\u62c9\u8bed\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe97%\uff0c\u5e76\u91c7\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u6210\u4e3a\u516c\u4f17\u8868\u8fbe\u5bf9\u72af\u7f6a\u4e8b\u4ef6\u610f\u89c1\u7684\u91cd\u8981\u5e73\u53f0\uff0c\u52a8\u6001\u53d8\u5316\u7684\u516c\u4f17\u60c5\u7eea\u9700\u8981\u88ab\u91cf\u5316\u5206\u6790\u3002", "method": "\u6784\u5efa\u5305\u542b28,528\u6761\u5b5f\u52a0\u62c9\u8bed\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528XLM-RoBERTa Base\u67b6\u6784\u7684Transformer\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8fbe97%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\u8bc6\u522b\u5173\u952e\u7279\u5f81\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u4e3a\u516c\u5171\u653f\u7b56\u5236\u5b9a\u548c\u72af\u7f6a\u9884\u9632\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.21507", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.21507", "abs": "https://arxiv.org/abs/2507.21507", "authors": ["Shibo Gao", "Peipei Yang", "Yangyang Liu", "Yi Chen", "Han Zhu", "Xuyao Zhang", "Linlin Huang"], "title": "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding", "comment": "21 pages, 19 figures, 8 tables", "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events in videos and\naccurately determine their time intervals. Current VAD methods mainly fall into\ntwo categories: traditional DNN-based approaches that focus on temporal\nlocalization, and LLM-based approaches that emphasize semantic understanding.\nBoth anomaly understanding and grounding are essential for comprehensive video\nanomaly detection and can complement each other. However, no existing model or\ndataset supports both tasks simultaneously. To address this, we introduce VAGU\n(Video Anomaly Grounding and Understanding), the first benchmark to integrate\nboth tasks. Each VAGU instance includes annotations for anomaly category,\nsemantic explanation, precise temporal grounding and Video QA. We also provide\nmultiple-choice Video QA for objective evaluation. Based on this dataset, we\npropose Glance then Scrutinize (GtS), a training-free framework guided by\ntextual prompts. The framework first enables coarse localization of\nhigh-probability anomalous regions, followed by detailed anomaly interpretation\nand temporal boundary refinement. Additionally, we propose the JeAUG metric,\nwhich jointly evaluates semantic interpretability and temporal precision,\novercoming the limitations of traditional metrics. Extensive experiments verify\nthe effectiveness of our benchmark, framework, and evaluation metric.", "AI": {"tldr": "VAGU\u662f\u9996\u4e2a\u6574\u5408\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e0e\u7406\u89e3\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301\u5f02\u5e38\u7c7b\u522b\u3001\u8bed\u4e49\u89e3\u91ca\u3001\u65f6\u95f4\u5b9a\u4f4d\u548c\u89c6\u9891\u95ee\u7b54\u3002\u63d0\u51fa\u7684GtS\u6846\u67b6\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5b9e\u73b0\u7c97\u5b9a\u4f4d\u548c\u7cbe\u7ec6\u5206\u6790\uff0c\u5e76\u5f15\u5165JeAUG\u6307\u6807\u8054\u5408\u8bc4\u4f30\u8bed\u4e49\u548c\u65f6\u95f4\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u65f6\u95f4\u5b9a\u4f4d\u6216\u8bed\u4e49\u7406\u89e3\uff0c\u7f3a\u4e4f\u540c\u65f6\u652f\u6301\u4e24\u8005\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faVAGU\u6570\u636e\u96c6\u548cGtS\u6846\u67b6\uff0c\u7ed3\u5408\u7c97\u5b9a\u4f4d\u4e0e\u7cbe\u7ec6\u5206\u6790\uff0c\u5e76\u4f7f\u7528JeAUG\u6307\u6807\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VAGU\u6570\u636e\u96c6\u3001GtS\u6846\u67b6\u548cJeAUG\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "VAGU\u548cGtS\u4e3a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0cJeAUG\u6539\u8fdb\u4e86\u8bc4\u4f30\u65b9\u5f0f\u3002"}}
{"id": "2507.21509", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21509", "abs": "https://arxiv.org/abs/2507.21509", "authors": ["Runjin Chen", "Andy Arditi", "Henry Sleight", "Owain Evans", "Jack Lindsey"], "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language Models", "comment": null, "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u201c\u52a9\u624b\u201d\u89d2\u8272\u7684\u6fc0\u6d3b\u7a7a\u95f4\u65b9\u5411\uff08\u5373\u201c\u89d2\u8272\u5411\u91cf\u201d\uff09\uff0c\u7528\u4e8e\u76d1\u63a7\u548c\u9884\u6d4b\u8bad\u7ec3\u4e2d\u7684\u89d2\u8272\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u5e72\u9884\u548c\u9884\u9632\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a9\u624b\u89d2\u8272\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u65f6\u53ef\u80fd\u504f\u79bb\u7406\u60f3\u884c\u4e3a\uff08\u5982\u90aa\u6076\u3001\u5949\u627f\u3001\u5e7b\u89c9\u503e\u5411\uff09\u7684\u539f\u56e0\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u89d2\u8272\u5411\u91cf\u6765\u76d1\u63a7\u89d2\u8272\u53d8\u5316\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5411\u91cf\u9884\u6d4b\u548c\u63a7\u5236\u8bad\u7ec3\u4e2d\u7684\u89d2\u8272\u504f\u79fb\uff0c\u63d0\u51fa\u5e72\u9884\u548c\u9884\u9632\u65b9\u6cd5\u3002", "result": "\u89d2\u8272\u5411\u91cf\u80fd\u6709\u6548\u9884\u6d4b\u548c\u63a7\u5236\u89d2\u8272\u53d8\u5316\uff0c\u5e72\u9884\u548c\u9884\u9632\u65b9\u6cd5\u53ef\u51cf\u5c11\u4e0d\u826f\u89d2\u8272\u504f\u79fb\uff0c\u5e76\u80fd\u6807\u8bb0\u5bfc\u81f4\u4e0d\u826f\u53d8\u5316\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u89d2\u8272\u5411\u91cf\u4e3a\u76d1\u63a7\u548c\u8c03\u6574\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u611f\u5174\u8da3\u7684\u89d2\u8272\u7279\u5f81\u3002"}}
{"id": "2507.21494", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21494", "abs": "https://arxiv.org/abs/2507.21494", "authors": ["Wenxuan Bao", "Ruxi Deng", "Ruizhong Qiu", "Tianxin Wei", "Hanghang Tong", "Jingrui He"], "title": "Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning", "comment": "Accepted by ICCV 2025", "summary": "Test-time adaptation with pre-trained vision-language models has gained\nincreasing attention for addressing distribution shifts during testing. Among\nthese approaches, memory-based algorithms stand out due to their training-free\nnature and ability to leverage historical test data. However, existing\ntest-time adaptation methods are typically designed for a single domain with\nabundant data. In decentralized settings such as federated learning, applying\nthese methods individually to each client suffers from limited test data, while\ndirectly sharing a single global memory via the server prevents proper\npersonalization to each client's unique distribution. To address this, we\npropose Latte, a novel framework where each client maintains a local memory to\nstore embeddings from its own historical test data and an external memory to\nstore class prototypes from other relevant clients. During communication, each\nclient retrieves prototypes from similar clients under the server's\ncoordination to expand its memory. For local adaptation, Latte utilizes both\nembedding similarity and uncertainty to enhance model performance. Our\ntheoretical analysis shows that Latte effectively leverages in-distribution\nclients while remaining robust to out-of-distribution clients. Extensive\nexperiments on domain adaptation and corruption benchmarks validate that Latte\nachieves superior performance in decentralized settings, while introducing only\nnegligible communication and computation costs. Our code is available at\nhttps://github.com/baowenxuan/Latte .", "AI": {"tldr": "Latte\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u672c\u5730\u548c\u5916\u90e8\u5185\u5b58\u5b58\u50a8\u5386\u53f2\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u56e0\u6570\u636e\u6709\u9650\u6216\u5171\u4eab\u5168\u5c40\u5185\u5b58\u800c\u65e0\u6cd5\u4e2a\u6027\u5316\u9002\u5e94\u6bcf\u4e2a\u5ba2\u6237\u7aef\u3002", "method": "Latte\u6846\u67b6\u4e2d\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u672c\u5730\u5185\u5b58\u5b58\u50a8\u81ea\u8eab\u5386\u53f2\u6d4b\u8bd5\u6570\u636e\uff0c\u5916\u90e8\u5185\u5b58\u5b58\u50a8\u76f8\u5173\u5ba2\u6237\u7aef\u7684\u7c7b\u539f\u578b\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u534f\u8c03\u68c0\u7d22\u76f8\u4f3c\u5ba2\u6237\u7aef\u7684\u539f\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLatte\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u6027\u80fd\u4f18\u8d8a\uff0c\u4e14\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "Latte\u6709\u6548\u5229\u7528\u5206\u5e03\u5185\u5ba2\u6237\u7aef\u5e76\u4fdd\u6301\u5bf9\u5206\u5e03\u5916\u5ba2\u6237\u7aef\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002"}}
{"id": "2507.21616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21616", "abs": "https://arxiv.org/abs/2507.21616", "authors": ["Kevin Doran", "Tom Baden"], "title": "Categorical Distributions are Effective Neural Network Outputs for Event Prediction", "comment": "32 pages, 26 figures", "summary": "We demonstrate the effectiveness of using a simple neural network output, a\ncategorical probability distribution, for the task of next spike prediction.\nThis case study motivates an investigation into why this simple output\nstructure is not commonly used with neural temporal point process models. We\nfind evidence that many existing datasets for evaluating temporal point process\nmodels do not reveal much information about the underlying event generating\nprocesses, and many existing models perform well due to regularization effects\nof model size and constraints on output structure. We extend existing datasets\nand create new ones in order to explore outside of this information limited\nregime and find that outputting a simple categorical distribution is\ncompetitive across a wide range of datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\uff08\u5206\u7c7b\u6982\u7387\u5206\u5e03\uff09\u8fdb\u884c\u4e0b\u4e00\u4e2a\u5c16\u5cf0\u9884\u6d4b\u7684\u6709\u6548\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u4e3a\u4f55\u8fd9\u79cd\u8f93\u51fa\u7ed3\u6784\u5728\u795e\u7ecf\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u4e2d\u4e0d\u5e38\u89c1\u3002", "motivation": "\u7814\u7a76\u7b80\u5355\u8f93\u51fa\u7ed3\u6784\u5728\u795e\u7ecf\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u4e2d\u4e0d\u5e38\u89c1\u7684\u539f\u56e0\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u66f4\u5e7f\u6cdb\u6570\u636e\u96c6\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u548c\u521b\u5efa\u65b0\u7684\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u7b80\u5355\u5206\u7c7b\u6982\u7387\u5206\u5e03\u4f5c\u4e3a\u8f93\u51fa\u7684\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u5206\u7c7b\u5206\u5e03\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u73b0\u6709\u6570\u636e\u96c6\u53ef\u80fd\u672a\u80fd\u5145\u5206\u63ed\u793a\u4e8b\u4ef6\u751f\u6210\u8fc7\u7a0b\u7684\u4fe1\u606f\u3002", "conclusion": "\u7b80\u5355\u5206\u7c7b\u5206\u5e03\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f93\u51fa\u7ed3\u6784\uff0c\u5c24\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2507.21649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21649", "abs": "https://arxiv.org/abs/2507.21649", "authors": ["Shibo Gao", "Peipei Yang", "Haiyang Guo", "Yangyang Liu", "Yi Chen", "Shuai Li", "Han Zhu", "Jian Xu", "Xu-Yao Zhang", "Linlin Huang"], "title": "The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM", "comment": null, "summary": "Video anomaly detection (VAD) aims to identify and ground anomalous behaviors\nor events in videos, serving as a core technology in the fields of intelligent\nsurveillance and public safety. With the advancement of deep learning, the\ncontinuous evolution of deep model architectures has driven innovation in VAD\nmethodologies, significantly enhancing feature representation and scene\nadaptability, thereby improving algorithm generalization and expanding\napplication boundaries. More importantly, the rapid development of multi-modal\nlarge language (MLLMs) and large language models (LLMs) has introduced new\nopportunities and challenges to the VAD field. Under the support of MLLMs and\nLLMs, VAD has undergone significant transformations in terms of data\nannotation, input modalities, model architectures, and task objectives. The\nsurge in publications and the evolution of tasks have created an urgent need\nfor systematic reviews of recent advancements. This paper presents the first\ncomprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing\nan in-depth discussion of the changes occurring in the VAD field in the era of\nlarge models and their underlying causes. Additionally, this paper proposes a\nunified framework that encompasses both deep neural network (DNN)-based and\nLLM-based VAD methods, offering a thorough analysis of the new VAD paradigms\nempowered by LLMs, constructing a classification system, and comparing their\nstrengths and weaknesses. Building on this foundation, this paper focuses on\ncurrent VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of\ntechnological advancements and existing bottlenecks, this paper distills key\nchallenges and outlines future research directions, offering guidance for the\nVAD community.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5176\u5e26\u6765\u7684\u53d8\u9769\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u548cLLMs\u7684\u53d1\u5c55\uff0cVAD\u9886\u57df\u9762\u4e34\u65b0\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7efc\u8ff0\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6570\u636e\u6807\u6ce8\u3001\u8f93\u5165\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u76ee\u6807\u7684\u53d8\u5316\uff0c\u6784\u5efa\u5206\u7c7b\u7cfb\u7edf\u5e76\u6bd4\u8f83\u4f18\u52a3\u3002", "result": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u6db5\u76d6\u57fa\u4e8eDNN\u548cLLM\u7684VAD\u65b9\u6cd5\uff0c\u603b\u7ed3\u65b0\u8303\u5f0f\u5e76\u5206\u6790\u5176\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u63d0\u70bc\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3aVAD\u793e\u533a\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2507.21738", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21738", "abs": "https://arxiv.org/abs/2507.21738", "authors": ["Huiqiang Chen", "Tianqing Zhu", "Xin Yu", "Wanlei Zhou"], "title": "Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation", "comment": "Accepted by IJCAI 2025", "summary": "Machine unlearning aims to remove the influence of specific samples from a\ntrained model. A key challenge in this process is over-unlearning, where the\nmodel's performance on the remaining data significantly drops due to the change\nin the model's parameters. Existing unlearning algorithms depend on the\nremaining data to prevent this issue. As such, these methods are inapplicable\nin a more practical scenario, where only the unlearning samples are available\n(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to\nfill this gap. Our approach offers three key innovations: (1) we approximate\nthe inaccessible remaining data by generating adversarial samples; (2)\nleveraging the generated samples, we pinpoint a specific subspace to perform\nthe unlearning process, therefore preventing over-unlearning in the challenging\nzero-shot scenario; and (3) we consider the influence of the unlearning process\non the remaining samples and design an influence-based pseudo-labeling\nstrategy. As a result, our method further improves the model's performance\nafter unlearning. The proposed method holds a theoretical guarantee, and\nexperiments on various benchmarks validate the effectiveness and superiority of\nour proposed method over several baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZS-PAG\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u96f6\u6837\u672c\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u8fc7\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6837\u672c\u548c\u8bbe\u8ba1\u5f71\u54cd\u4f2a\u6807\u7b7e\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\u4f9d\u8d56\u5269\u4f59\u6570\u636e\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u4ec5\u63d0\u4f9b\u9057\u5fd8\u6837\u672c\u7684\u96f6\u6837\u672c\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6837\u672c\u8fd1\u4f3c\u5269\u4f59\u6570\u636e\uff0c\u786e\u5b9a\u7279\u5b9a\u5b50\u7a7a\u95f4\u8fdb\u884c\u9057\u5fd8\uff0c\u5e76\u8bbe\u8ba1\u5f71\u54cd\u4f2a\u6807\u7b7e\u7b56\u7565\u4ee5\u9632\u6b62\u8fc7\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ZS-PAG\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "ZS-PAG\u5728\u96f6\u6837\u672c\u673a\u5668\u9057\u5fd8\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u8fc7\u9057\u5fd8\u95ee\u9898\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.21828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21828", "abs": "https://arxiv.org/abs/2507.21828", "authors": ["Anna Golub", "Beate Zywietz", "Annerose Eichel"], "title": "Modelling Adjectival Modification Effects on Semantic Plausibility", "comment": "Accepted at ESSLLI 2025 Student Session", "summary": "While the task of assessing the plausibility of events such as ''news is\nrelevant'' has been addressed by a growing body of work, less attention has\nbeen paid to capturing changes in plausibility as triggered by event\nmodification. Understanding changes in plausibility is relevant for tasks such\nas dialogue generation, commonsense reasoning, and hallucination detection as\nit allows to correctly model, for example, ''gentle sarcasm'' as a sign of\ncloseness rather than unkindness among friends [9]. In this work, we tackle the\nADEPT challenge benchmark [6] consisting of 16K English sentence pairs\ndiffering by exactly one adjectival modifier. Our modeling experiments provide\na conceptually novel method by using sentence transformers, and reveal that\nboth they and transformer-based models struggle with the task at hand, and\nsentence transformers - despite their conceptual alignment with the task - even\nunder-perform in comparison to models like RoBERTa. Furthermore, an in-depth\ncomparison with prior work highlights the importance of a more realistic,\nbalanced evaluation method: imbalances distort model performance and evaluation\nmetrics, and weaken result trustworthiness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e8b\u4ef6\u5408\u7406\u6027\u53d8\u5316\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53e5\u5b50Transformer\u7684\u65b0\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u5f3a\u8c03\u4e86\u5e73\u8861\u8bc4\u4f30\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u4e8b\u4ef6\u5408\u7406\u6027\u53d8\u5316\u5bf9\u5bf9\u8bdd\u751f\u6210\u3001\u5e38\u8bc6\u63a8\u7406\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u53e5\u5b50Transformer\u548cTransformer\u6a21\u578b\uff08\u5982RoBERTa\uff09\u5bf916K\u82f1\u8bed\u53e5\u5b50\u5bf9\u8fdb\u884c\u5efa\u6a21\u5b9e\u9a8c\u3002", "result": "\u53e5\u5b50Transformer\u548cTransformer\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u53e5\u5b50Transformer\u751a\u81f3\u4e0d\u5982RoBERTa\u3002\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u5e73\u8861\u4f1a\u5f71\u54cd\u7ed3\u679c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f46\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5e73\u8861\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.21756", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21756", "abs": "https://arxiv.org/abs/2507.21756", "authors": ["Jing Ren", "Suyu Ma", "Hong Jia", "Xiwei Xu", "Ivan Lee", "Haytham Fayek", "Xiaodong Li", "Feng Xia"], "title": "LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection", "comment": "6 pages, 1 figure", "summary": "Detecting driver fatigue is critical for road safety, as drowsy driving\nremains a leading cause of traffic accidents. Many existing solutions rely on\ncomputationally demanding deep learning models, which result in high latency\nand are unsuitable for embedded robotic devices with limited resources (such as\nintelligent vehicles/cars) where rapid detection is necessary to prevent\naccidents. This paper introduces LiteFat, a lightweight spatio-temporal graph\nlearning model designed to detect driver fatigue efficiently while maintaining\nhigh accuracy and low computational demands. LiteFat involves converting\nstreaming video data into spatio-temporal graphs (STG) using facial landmark\ndetection, which focuses on key motion patterns and reduces unnecessary data\nprocessing. LiteFat uses MobileNet to extract facial features and create a\nfeature matrix for the STG. A lightweight spatio-temporal graph neural network\nis then employed to identify signs of fatigue with minimal processing and low\nlatency. Experimental results on benchmark datasets show that LiteFat performs\ncompetitively while significantly decreasing computational complexity and\nlatency as compared to current state-of-the-art methods. This work enables the\ndevelopment of real-time, resource-efficient human fatigue detection systems\nthat can be implemented upon embedded robotic devices.", "AI": {"tldr": "LiteFat\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u65f6\u7a7a\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4b\u9a7e\u9a76\u5458\u75b2\u52b3\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u9a7e\u9a76\u5458\u75b2\u52b3\u662f\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "method": "\u5c06\u89c6\u9891\u6570\u636e\u8f6c\u6362\u4e3a\u65f6\u7a7a\u56fe\uff08STG\uff09\uff0c\u4f7f\u7528MobileNet\u63d0\u53d6\u9762\u90e8\u7279\u5f81\uff0c\u8f7b\u91cf\u7ea7\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u75b2\u52b3\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5ef6\u8fdf\u3002", "conclusion": "LiteFat\u4e3a\u5d4c\u5165\u5f0f\u8bbe\u5907\u63d0\u4f9b\u4e86\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u75b2\u52b3\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21786", "abs": "https://arxiv.org/abs/2507.21786", "authors": ["Zhaolong Wang", "Tongfeng Sun", "Mingzheng Du", "Yachao Huang"], "title": "MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning", "comment": null, "summary": "Vision-language pre-trained models (VLMs) such as CLIP have demonstrated\nremarkable zero-shot generalization, and prompt learning has emerged as an\nefficient alternative to full fine-tuning. However, existing methods often\nstruggle with generalization to novel classes, a phenomenon attributed to\noverfitting on seen classes and forgetting general knowledge. Furthermore,\nrecent approaches that improve generalization often introduce complex\narchitectures or heavy computational overhead. In this paper, we propose a\nMultiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance\nfew-shot generalization while maintaining computational efficiency. Our\napproach leverages an ensemble of parallel learnable context vectors to capture\ndiverse semantic aspects. To enrich these prompts, we introduce a semantic\nguidance mechanism that aligns them with comprehensive class descriptions\nautomatically generated by a Large Language Model (LLM). Furthermore, a\ndiversity regularization loss encourages the prompts to learn complementary and\northogonal features, preventing them from collapsing into redundant\nrepresentations. Extensive experiments on 11 benchmark datasets show that\nMSGCoOp significantly improves performance on base-to-novel generalization,\nachieving an average harmonic mean improvement of 1.10\\% over the strong KgCoOp\nbaseline. Our method also demonstrates enhanced robustness in cross-domain\ngeneralization tasks. Our code is avaliable at:\n\\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.", "AI": {"tldr": "MSGCoOp\u6846\u67b6\u901a\u8fc7\u591a\u8bed\u4e49\u5f15\u5bfc\u4e0a\u4e0b\u6587\u4f18\u5316\u63d0\u5347\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65b0\u7c7b\u522b\u6cdb\u5316\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u590d\u6742\u67b6\u6784\u6216\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u5229\u7528\u5e76\u884c\u53ef\u5b66\u4e60\u4e0a\u4e0b\u6587\u5411\u91cf\u96c6\u5408\uff0c\u7ed3\u5408LLM\u751f\u6210\u7684\u7c7b\u63cf\u8ff0\u8fdb\u884c\u8bed\u4e49\u5f15\u5bfc\uff0c\u5e76\u5f15\u5165\u591a\u6837\u6027\u6b63\u5219\u5316\u635f\u5931\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u57fa\u7c7b\u5230\u65b0\u7c7b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e73\u5747\u8c03\u548c\u5747\u503c\u63d0\u53471.10%\u3002", "conclusion": "MSGCoOp\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2507.21881", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21881", "abs": "https://arxiv.org/abs/2507.21881", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image", "comment": null, "summary": "Pain is a multifaceted phenomenon that affects a substantial portion of the\npopulation. Reliable and consistent evaluation benefits those experiencing pain\nand underpins the development of effective and advanced management strategies.\nAutomatic pain-assessment systems deliver continuous monitoring, inform\nclinical decision-making, and aim to reduce distress while preventing\nfunctional decline. By incorporating physiological signals, these systems\nprovide objective, accurate insights into an individual's condition. This study\nhas been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for\nNext-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline\nthat leverages electrodermal activity signals as input modality. Multiple\nrepresentations of the signal are created and visualized as waveforms, and they\nare jointly visualized within a single multi-representation diagram. Extensive\nexperiments incorporating various processing and filtering techniques, along\nwith multiple representation combinations, demonstrate the effectiveness of the\nproposed approach. It consistently yields comparable, and in several cases\nsuperior, results to traditional fusion methods, establishing it as a robust\nalternative for integrating different signal representations or modalities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u76ae\u80a4\u6d3b\u52a8\u4fe1\u53f7\u7684\u591a\u8868\u793a\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u3002", "motivation": "\u75bc\u75db\u8bc4\u4f30\u5bf9\u4e34\u5e8a\u51b3\u7b56\u548c\u60a3\u8005\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u5ba2\u89c2\u6027\u548c\u8fde\u7eed\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u7406\u4fe1\u53f7\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u75bc\u75db\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u7535\u76ae\u80a4\u6d3b\u52a8\u4fe1\u53f7\u751f\u6210\u591a\u79cd\u8868\u793a\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7\u591a\u8868\u793a\u56fe\u8054\u5408\u53ef\u89c6\u5316\uff0c\u7ed3\u5408\u591a\u79cd\u5904\u7406\u548c\u8fc7\u6ee4\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8868\u793a\u7ec4\u5408\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4fe1\u53f7\u8868\u793a\u6216\u6a21\u6001\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.21844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21844", "abs": "https://arxiv.org/abs/2507.21844", "authors": ["Weijia Zhang", "Yuehao Liu", "Wu Ran", "Chao Ma"], "title": "Cross-Architecture Distillation Made Simple with Redundancy Suppression", "comment": "Accepted by ICCV 2025 (Highlight)", "summary": "We describe a simple method for cross-architecture knowledge distillation,\nwhere the knowledge transfer is cast into a redundant information suppression\nformulation. Existing methods introduce sophisticated modules,\narchitecture-tailored designs, and excessive parameters, which impair their\nefficiency and applicability. We propose to extract the architecture-agnostic\nknowledge in heterogeneous representations by reducing the redundant\narchitecture-exclusive information. To this end, we present a simple redundancy\nsuppression distillation (RSD) loss, which comprises cross-architecture\ninvariance maximisation and feature decorrelation objectives. To prevent the\nstudent from entirely losing its architecture-specific capabilities, we further\ndesign a lightweight module that decouples the RSD objective from the student's\ninternal representations. Our method is devoid of the architecture-specific\ndesigns and complex operations in the pioneering method of OFA. It outperforms\nOFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their\nparameter overhead, which highlights its potential as a simple and strong\nbaseline to the cross-architecture distillation community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6291\u5236\u5197\u4f59\u4fe1\u606f\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u907f\u514d\u4e86\u590d\u6742\u6a21\u5757\u548c\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u590d\u6742\u6a21\u5757\u548c\u67b6\u6784\u5b9a\u5236\u8bbe\u8ba1\uff0c\u964d\u4f4e\u4e86\u6548\u7387\u548c\u9002\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5197\u4f59\u6291\u5236\u84b8\u998f\uff08RSD\uff09\u635f\u5931\uff0c\u5305\u62ec\u8de8\u67b6\u6784\u4e0d\u53d8\u6027\u6700\u5927\u5316\u548c\u7279\u5f81\u53bb\u76f8\u5173\u76ee\u6807\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6a21\u5757\u89e3\u8026RSD\u76ee\u6807\u4e0e\u5b66\u751f\u5185\u90e8\u8868\u793a\u3002", "result": "\u5728CIFAR-100\u548cImageNet-1k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eOFA\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u5f00\u9500\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u67b6\u6784\u84b8\u998f\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.21886", "categories": ["cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21886", "abs": "https://arxiv.org/abs/2507.21886", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "title": "Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline", "comment": null, "summary": "Pain is a complex condition affecting a large portion of the population.\nAccurate and consistent evaluation is essential for individuals experiencing\npain, and it supports the development of effective and advanced management\nstrategies. Automatic pain assessment systems provide continuous monitoring and\nsupport clinical decision-making, aiming to reduce distress and prevent\nfunctional decline. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed method introduces a pipeline that leverages respiration as the input\nsignal and incorporates a highly efficient cross-attention transformer\nalongside a multi-windowing strategy. Extensive experiments demonstrate that\nrespiration is a valuable physiological modality for pain assessment. Moreover,\nexperiments revealed that compact and efficient models, when properly\noptimized, can achieve strong performance, often surpassing larger\ncounterparts. The proposed multi-window approach effectively captures both\nshort-term and long-term features, as well as global characteristics, thereby\nenhancing the model's representational capacity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u547c\u5438\u4fe1\u53f7\u548c\u591a\u7a97\u53e3\u7b56\u7565\u7684\u9ad8\u6548\u4ea4\u53c9\u6ce8\u610f\u529bTransformer\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u75bc\u75db\u8bc4\u4f30\u5bf9\u4e2a\u4f53\u5065\u5eb7\u548c\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u80fd\u63d0\u4f9b\u8fde\u7eed\u76d1\u6d4b\u5e76\u652f\u6301\u6709\u6548\u7ba1\u7406\u7b56\u7565\u7684\u5f00\u53d1\u3002", "method": "\u5229\u7528\u547c\u5438\u4fe1\u53f7\u4f5c\u4e3a\u8f93\u5165\uff0c\u7ed3\u5408\u9ad8\u6548\u4ea4\u53c9\u6ce8\u610f\u529bTransformer\u548c\u591a\u7a97\u53e3\u7b56\u7565\uff0c\u6355\u6349\u77ed\u671f\u3001\u957f\u671f\u53ca\u5168\u5c40\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u547c\u5438\u4fe1\u53f7\u662f\u6709\u6548\u7684\u75bc\u75db\u8bc4\u4f30\u751f\u7406\u6a21\u6001\uff0c\u4f18\u5316\u540e\u7684\u7d27\u51d1\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u3002", "conclusion": "\u591a\u7a97\u53e3\u7b56\u7565\u589e\u5f3a\u4e86\u6a21\u578b\u8868\u5f81\u80fd\u529b\uff0c\u547c\u5438\u4fe1\u53f7\u7ed3\u5408\u9ad8\u6548\u6a21\u578b\u4e3a\u75bc\u75db\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.21857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21857", "abs": "https://arxiv.org/abs/2507.21857", "authors": ["Jiahao He", "Daerji Suolang", "Keren Fu", "Qijun Zhao"], "title": "Unleashing the Power of Motion and Depth: A Selective Fusion Strategy for RGB-D Video Salient Object Detection", "comment": "submitted to TMM on 11-Jun-2024, ID: MM-020522, still in peer review", "summary": "Applying salient object detection (SOD) to RGB-D videos is an emerging task\ncalled RGB-D VSOD and has recently gained increasing interest, due to\nconsiderable performance gains of incorporating motion and depth and that RGB-D\nvideos can be easily captured now in daily life. Existing RGB-D VSOD models\nhave different attempts to derive motion cues, in which extracting motion\ninformation explicitly from optical flow appears to be a more effective and\npromising alternative. Despite this, there remains a key issue that how to\neffectively utilize optical flow and depth to assist the RGB modality in SOD.\nPrevious methods always treat optical flow and depth equally with respect to\nmodel designs, without explicitly considering their unequal contributions in\nindividual scenarios, limiting the potential of motion and depth. To address\nthis issue and unleash the power of motion and depth, we propose a novel\nselective cross-modal fusion framework (SMFNet) for RGB-D VSOD, incorporating a\npixel-level selective fusion strategy (PSF) that achieves optimal fusion of\noptical flow and depth based on their actual contributions. Besides, we propose\na multi-dimensional selective attention module (MSAM) to integrate the fused\nfeatures derived from PSF with the remaining RGB modality at multiple\ndimensions, effectively enhancing feature representation to generate refined\nfeatures. We conduct comprehensive evaluation of SMFNet against 19\nstate-of-the-art models on both RDVS and DVisal datasets, making the evaluation\nthe most comprehensive RGB-D VSOD benchmark up to date, and it also\ndemonstrates the superiority of SMFNet over other models. Meanwhile, evaluation\non five video benchmark datasets incorporating synthetic depth validates the\nefficacy of SMFNet as well. Our code and benchmark results are made publicly\navailable at https://github.com/Jia-hao999/SMFNet.", "AI": {"tldr": "SMFNet\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u8de8\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u9009\u62e9\u6027\u878d\u5408\u7b56\u7565\u548c\u591a\u7ef4\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4f18\u5316\u4e86RGB-D\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u5149\u6d41\u548c\u6df1\u5ea6\u7684\u5229\u7528\u3002", "motivation": "\u73b0\u6709RGB-D VSOD\u65b9\u6cd5\u5bf9\u5149\u6d41\u548c\u6df1\u5ea6\u7684\u5229\u7528\u4e0d\u591f\u7075\u6d3b\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faSMFNet\u6846\u67b6\uff0c\u5305\u542b\u50cf\u7d20\u7ea7\u9009\u62e9\u6027\u878d\u5408\u7b56\u7565\uff08PSF\uff09\u548c\u591a\u7ef4\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff08MSAM\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e19\u79cd\u73b0\u6709\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86SMFNet\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "SMFNet\u901a\u8fc7\u9009\u62e9\u6027\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86RGB-D\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.21899", "categories": ["cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.21899", "abs": "https://arxiv.org/abs/2507.21899", "authors": ["Malik Uzair Mehmood", "Shahid Hussain", "Wen Li Wang", "Muhammad Usama Malik"], "title": "LLM-based Content Classification Approach for GitHub Repositories by the README Files", "comment": "8 pages, 4 Figures", "summary": "GitHub is the world's most popular platform for storing, sharing, and\nmanaging code. Every GitHub repository has a README file associated with it.\nThe README files should contain project-related information as per the\nrecommendations of GitHub to support the usage and improvement of repositories.\nHowever, GitHub repository owners sometimes neglected these recommendations.\nThis prevents a GitHub repository from reaching its full potential. This\nresearch posits that the comprehensiveness of a GitHub repository's README file\nsignificantly influences its adoption and utilization, with a lack of detail\npotentially hindering its full potential for widespread engagement and impact\nwithin the research community. Large Language Models (LLMs) have shown great\nperformance in many text-based tasks including text classification, text\ngeneration, text summarization and text translation. In this study, an approach\nis developed to fine-tune LLMs for automatically classifying different sections\nof GitHub README files. Three encoder-only LLMs are utilized, including BERT,\nDistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a\ngold-standard dataset consisting of 4226 README file sections. This approach\noutperforms current state-of-the-art methods and has achieved an overall F1\nscore of 0.98. Moreover, we have also investigated the use of\nParameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation\n(LoRA) and shown an economical alternative to full fine-tuning without\ncompromising much performance. The results demonstrate the potential of using\nLLMs in designing an automatic classifier for categorizing the content of\nGitHub README files. Consequently, this study contributes to the development of\nautomated tools for GitHub repositories to improve their identifications and\npotential usages.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLMs\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u7c7bGitHub README\u6587\u4ef6\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u901a\u8fc7\u5fae\u8c03BERT\u3001DistilBERT\u548cRoBERTa\u6a21\u578b\uff0c\u53d6\u5f97\u4e860.98\u7684F1\u5206\u6570\uff0c\u5e76\u63a2\u7d22\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u3002", "motivation": "GitHub\u4ed3\u5e93\u7684README\u6587\u4ef6\u5bf9\u9879\u76ee\u7684\u91c7\u7528\u548c\u5229\u7528\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u8bb8\u591a\u4ed3\u5e93\u6240\u6709\u8005\u5ffd\u89c6\u4e86\u5176\u5b8c\u6574\u6027\uff0c\u9650\u5236\u4e86\u9879\u76ee\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u5fae\u8c03\u4e86\u4e09\u79cd\u7f16\u7801\u5668\u6a21\u578b\uff08BERT\u3001DistilBERT\u548cRoBERTa\uff09\uff0c\u5e76\u4f7f\u7528\u5305\u542b4226\u4e2aREADME\u6587\u4ef6\u7247\u6bb5\u7684\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u8fbe\u52300.98\uff0c\u5e76\u9a8c\u8bc1\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u7684\u53ef\u884c\u6027\u3002", "conclusion": "LLMs\u5728\u81ea\u52a8\u5206\u7c7bGitHub README\u6587\u4ef6\u5185\u5bb9\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u6539\u8fdb\u4ed3\u5e93\u8bc6\u522b\u548c\u5229\u7528\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2507.21971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21971", "abs": "https://arxiv.org/abs/2507.21971", "authors": ["Zhijiang Li", "Haoran He"], "title": "EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation", "comment": null, "summary": "Event-based semantic segmentation explores the potential of event cameras,\nwhich offer high dynamic range and fine temporal resolution, to achieve robust\nscene understanding in challenging environments. Despite these advantages, the\ntask remains difficult due to two main challenges: extracting reliable features\nfrom sparse and noisy event streams, and effectively fusing them with dense,\nsemantically rich image data that differ in structure and representation. To\naddress these issues, we propose EIFNet, a multi-modal fusion network that\ncombines the strengths of both event and frame-based inputs. The network\nincludes an Adaptive Event Feature Refinement Module (AEFRM), which improves\nevent representations through multi-scale activity modeling and spatial\nattention. In addition, we introduce a Modality-Adaptive Recalibration Module\n(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and\nintegrate features across modalities using attention mechanisms and gated\nfusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets\nshow that EIFNet achieves state-of-the-art performance, demonstrating its\neffectiveness in event-based semantic segmentation.", "AI": {"tldr": "EIFNet\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff0c\u7ed3\u5408\u4e8b\u4ef6\u548c\u5e27\u8f93\u5165\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u4f18\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u4e8b\u4ef6\u8bed\u4e49\u5206\u5272\u7684\u6311\u6218\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u7cbe\u7ec6\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4f46\u5728\u7a00\u758f\u548c\u566a\u58f0\u4e8b\u4ef6\u6d41\u4e2d\u63d0\u53d6\u53ef\u9760\u7279\u5f81\u4ee5\u53ca\u4e0e\u5bc6\u96c6\u56fe\u50cf\u6570\u636e\u878d\u5408\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faEIFNet\uff0c\u5305\u542b\u81ea\u9002\u5e94\u4e8b\u4ef6\u7279\u5f81\u4f18\u5316\u6a21\u5757\uff08AEFRM\uff09\u3001\u6a21\u6001\u81ea\u9002\u5e94\u91cd\u6821\u51c6\u6a21\u5757\uff08MARM\uff09\u548c\u591a\u5934\u6ce8\u610f\u529b\u95e8\u63a7\u878d\u5408\u6a21\u5757\uff08MGFM\uff09\u3002", "result": "\u5728DDD17-Semantic\u548cDSEC-Semantic\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "EIFNet\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6a21\u6001\u878d\u5408\u95ee\u9898\u3002"}}
{"id": "2507.21985", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.21985", "abs": "https://arxiv.org/abs/2507.21985", "authors": ["Hyun Jun Yook", "Ga San Jhun", "Jae Hyun Cho", "Min Jeon", "Donghyun Kim", "Tae Hyung Kim", "Youn Kyu Lee"], "title": "ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models", "comment": "Accepted to ICCV2025", "summary": "Machine unlearning (MU) removes specific data points or concepts from deep\nlearning models to enhance privacy and prevent sensitive content generation.\nAdversarial prompts can exploit unlearned models to generate content containing\nremoved concepts, posing a significant security risk. However, existing\nadversarial attack methods still face challenges in generating content that\naligns with an attacker's intent while incurring high computational costs to\nidentify successful prompts. To address these challenges, we propose ZIUM, a\nZero-shot Intent-aware adversarial attack on Unlearned Models, which enables\nthe flexible customization of target attack images to reflect an attacker's\nintent. Additionally, ZIUM supports zero-shot adversarial attacks without\nrequiring further optimization for previously attacked unlearned concepts. The\nevaluation across various MU scenarios demonstrated ZIUM's effectiveness in\nsuccessfully customizing content based on user-intent prompts while achieving a\nsuperior attack success rate compared to existing methods. Moreover, its\nzero-shot adversarial attack significantly reduces the attack time for\npreviously attacked unlearned concepts.", "AI": {"tldr": "ZIUM\u662f\u4e00\u79cd\u96f6\u6837\u672c\u610f\u56fe\u611f\u77e5\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u9488\u5bf9\u672a\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u7b26\u5408\u653b\u51fb\u8005\u610f\u56fe\u7684\u5185\u5bb9\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u751f\u6210\u7b26\u5408\u653b\u51fb\u8005\u610f\u56fe\u7684\u5185\u5bb9\u65f6\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faZIUM\uff0c\u652f\u6301\u96f6\u6837\u672c\u5bf9\u6297\u653b\u51fb\uff0c\u65e0\u9700\u989d\u5916\u4f18\u5316\u5373\u53ef\u9488\u5bf9\u672a\u5b66\u4e60\u6982\u5ff5\u751f\u6210\u5b9a\u5236\u5316\u653b\u51fb\u5185\u5bb9\u3002", "result": "ZIUM\u5728\u591a\u79cd\u672a\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u4e14\u663e\u8457\u51cf\u5c11\u653b\u51fb\u65f6\u95f4\u3002", "conclusion": "ZIUM\u4e3a\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6548\u7387\u548c\u6548\u679c\u3002"}}
