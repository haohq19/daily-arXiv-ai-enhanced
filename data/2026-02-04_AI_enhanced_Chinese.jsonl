{"id": "2602.02502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02502", "abs": "https://arxiv.org/abs/2602.02502", "authors": ["Min Zeng", "Xi Chen", "Haiqin Yang", "Yike Guo"], "title": "Sparse Adapter Fusion for Continual Learning in NLP", "comment": "This paper has been accepted to EACL 2026", "summary": "Continual learning in natural language processing plays a crucial role in adapting to evolving data and preventing catastrophic forgetting. Despite significant progress, existing methods still face challenges, such as inefficient parameter reuse across tasks, risking catastrophic forgetting when tasks are dissimilar, and the unnecessary introduction of new parameters for each task, which hampers knowledge sharing among similar tasks. To tackle these issues, we propose a Sparse Adapter Fusion Method (SAFM), which dynamically fuses old and new adapters to address these challenges. SAFM operates in two stages: the decision stage and the tuning stage. In the decision stage, SAFM determines whether to incorporate a new adapter, reuse an existing one, or add an empty adapter. The architecture search procedure, designed to prioritize reusing or adding empty adapters, minimizes parameter consumption and maximizes reuse. In the tuning stage, SAFM especially facilitates a layer-wise loss to encourage differentiation between adapters, effectively capturing knowledge within the same task. Experimental results consistently show that SAFM outperforms state-of-the-art (SOTA) methods, achieving comparable performance while utilizing less than 60% of the parameters.", "AI": {"tldr": "SAFM\u662f\u4e00\u79cd\u7a00\u758f\u9002\u914d\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u878d\u5408\u65b0\u65e7\u9002\u914d\u5668\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u53c2\u6570\u91cd\u7528\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u6027\u80fd\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u5c11\u4e8e60%\u7684\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u8de8\u4efb\u52a1\u53c2\u6570\u91cd\u7528\u6548\u7387\u4f4e\uff1b2\uff09\u4efb\u52a1\u4e0d\u76f8\u4f3c\u65f6\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\uff1b3\uff09\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u4e0d\u5fc5\u8981\u5730\u5f15\u5165\u65b0\u53c2\u6570\uff0c\u963b\u788d\u76f8\u4f3c\u4efb\u52a1\u95f4\u7684\u77e5\u8bc6\u5171\u4eab\u3002", "method": "SAFM\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u51b3\u7b56\u9636\u6bb5\u51b3\u5b9a\u662f\u5f15\u5165\u65b0\u9002\u914d\u5668\u3001\u91cd\u7528\u73b0\u6709\u9002\u914d\u5668\u8fd8\u662f\u6dfb\u52a0\u7a7a\u9002\u914d\u5668\uff0c\u901a\u8fc7\u67b6\u6784\u641c\u7d22\u4f18\u5148\u91cd\u7528\u6216\u6dfb\u52a0\u7a7a\u9002\u914d\u5668\u6765\u6700\u5c0f\u5316\u53c2\u6570\u6d88\u8017\uff1b\u8c03\u4f18\u9636\u6bb5\u91c7\u7528\u5206\u5c42\u635f\u5931\u9f13\u52b1\u9002\u914d\u5668\u95f4\u5dee\u5f02\u5316\uff0c\u6709\u6548\u6355\u83b7\u540c\u4e00\u4efb\u52a1\u5185\u7684\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSAFM\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u5c11\u4e8e60%\u7684\u53c2\u6570\u3002", "conclusion": "SAFM\u901a\u8fc7\u52a8\u6001\u878d\u5408\u9002\u914d\u5668\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u53c2\u6570\u91cd\u7528\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u53c2\u6570\u5229\u7528\u3002"}}
{"id": "2602.02731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02731", "abs": "https://arxiv.org/abs/2602.02731", "authors": ["Rohan Pandey", "Haijuan Yan", "Hong Yu", "Jack Tsai"], "title": "Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors", "comment": null, "summary": "Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.", "AI": {"tldr": "\u5229\u7528\u9000\u4f0d\u519b\u4eba\u4e8b\u52a1\u90e8\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u9000\u4f0d\u519b\u4eba\u9996\u6b21\u65e0\u5bb6\u53ef\u5f52\u98ce\u9669\uff0c\u53d1\u73b0\u7ed3\u5408\u793e\u4f1a\u884c\u4e3a\u56e0\u7d20\u7684\u7eb5\u5411\u6a21\u578b\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u9000\u4f0d\u519b\u4eba\u65e0\u5bb6\u53ef\u5f52\u662f\u7f8e\u56fd\u91cd\u8981\u7684\u516c\u5171\u536b\u751f\u6311\u6218\uff0c\u9700\u8981\u4e3b\u52a8\u5e72\u9884\u7b56\u7565\u3002\u98ce\u9669\u9884\u6d4b\u4e3a\u9884\u9632\u6027\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u80fd\u9014\u5f84\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u9884\u6d4b\u65e0\u5bb6\u53ef\u5f52\u98ce\u9669\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u56de\u987e\u6027\u9884\u540e\u7814\u7a76\uff0c\u5206\u67904,276,403\u540d\u9000\u4f0d\u519b\u4eba\u4e8b\u52a1\u90e8\u60a3\u8005\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u3002\u6784\u5efa\u9759\u6001\u548c\u65f6\u53d8\u6570\u636e\u8868\u793a\uff0c\u4f7f\u7528\u4e34\u5e8a\u533b\u751f\u6307\u5bfc\u7684\u903b\u8f91\u5efa\u6a21\u4e34\u5e8a\u72b6\u51b5\u548c\u793e\u4f1a\u98ce\u9669\u7684\u6301\u7eed\u6027\u3002\u6bd4\u8f83\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u3001\u57fa\u4e8eTransformer\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u548c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5c06\u793e\u4f1a\u548c\u884c\u4e3a\u56e0\u7d20\u7eb3\u5165\u7eb5\u5411\u6a21\u578b\u4f7f\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u66f2\u7ebf\u4e0b\u9762\u79ef\u63d0\u9ad815-30%\u3002\u5728\u524d1%\u98ce\u9669\u5c42\uff0c\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u9633\u6027\u9884\u6d4b\u503c\u57283.93-13.80%\u8303\u56f4\u5185\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533a\u5206\u6027\u80fd\u4e0a\u4e0d\u5982\u7f16\u7801\u5668\u6a21\u578b\uff0c\u4f46\u5728\u4e0d\u540c\u79cd\u65cf\u7fa4\u4f53\u95f4\u8868\u73b0\u5dee\u5f02\u8f83\u5c0f\u3002", "conclusion": "\u7eb5\u5411\u3001\u793e\u4f1a\u4fe1\u606f\u4e30\u5bcc\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5efa\u6a21\u80fd\u5c06\u65e0\u5bb6\u53ef\u5f52\u98ce\u9669\u96c6\u4e2d\u5230\u53ef\u64cd\u4f5c\u7684\u5c42\u7ea7\uff0c\u4e3a\u9ad8\u5371\u9000\u4f0d\u519b\u4eba\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u9884\u9632\u7b56\u7565\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4e0d\u5982\u4e13\u95e8\u8bbe\u8ba1\u7684\u7f16\u7801\u5668\u6a21\u578b\u3002"}}
{"id": "2602.02536", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02536", "abs": "https://arxiv.org/abs/2602.02536", "authors": ["Tianle Gu", "Kexin Huang", "Lingyu Li", "Ruilin Luo", "Shiyang Huang", "Zongqi Wang", "Yujiu Yang", "Yan Teng", "Yingchun Wang"], "title": "From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation", "comment": null, "summary": "Safety moderation is pivotal for identifying harmful content. Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision. Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination. Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces. By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process. This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts. To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM). UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage. Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning. Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal benchmark using less than 40\\% of the training data used by leading baselines. Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation. Supplementary materials are available at \\href{https://trustworthylab.github.io/UniMod/}{project website}.", "AI": {"tldr": "UniMod\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5b89\u5168\u5ba1\u6838\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\u66ff\u4ee3\u7a00\u758f\u4e8c\u5143\u6807\u7b7e\uff0c\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u6377\u5f84\uff0c\u4f7f\u7528\u591a\u7ef4\u5ea6\u5956\u52b1\u6a21\u578b\u548c\u76d1\u7763\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5b89\u5168\u5ba1\u6838\u9762\u4e34\u6570\u636e\u548c\u76d1\u7763\u7a00\u758f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u4e8c\u5143\u6807\u7b7e\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u6377\u5f84\uff0c\u65e0\u6cd5\u5b66\u4e60\u6709\u6548\u7684\u5206\u7c7b\u8fb9\u754c\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faUniMod\u5b66\u4e60\u8303\u5f0f\uff0c\u6784\u5efa\u5305\u542b\u8bc1\u636e\u5b9a\u4f4d\u3001\u6a21\u6001\u8bc4\u4f30\u3001\u98ce\u9669\u6620\u5c04\u3001\u7b56\u7565\u51b3\u7b56\u548c\u54cd\u5e94\u751f\u6210\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\uff1b\u5f00\u53d1\u591a\u5934\u90e8\u6807\u91cf\u5956\u52b1\u6a21\u578bUniRM\u63d0\u4f9b\u591a\u7ef4\u5ea6\u76d1\u7763\uff1b\u5f15\u5165\u4e13\u95e8\u4f18\u5316\u7b56\u7565\u89e3\u8026\u4efb\u52a1\u53c2\u6570\u5e76\u5e73\u8861\u8bad\u7ec3\u52a8\u6001\u3002", "result": "UniMod\u5728\u6587\u672c\u5ba1\u6838\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5728\u591a\u6a21\u6001\u5ba1\u6838\u4e0a\u5efa\u7acb\u65b0\u57fa\u51c6\uff0c\u4ec5\u4f7f\u7528\u9886\u5148\u57fa\u7ebf\u4e0d\u523040%\u7684\u8bad\u7ec3\u6570\u636e\uff1b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u5c5e\u6027\u8f68\u8ff9\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "UniMod\u4e3a\u591a\u6a21\u6001\u5ba1\u6838\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u96c6\u63a8\u7406\u8f68\u8ff9\u66ff\u4ee3\u7a00\u758f\u51b3\u7b56\uff0c\u9632\u6b62\u6377\u5f84\u5b66\u4e60\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.03039", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03039", "abs": "https://arxiv.org/abs/2602.03039", "authors": ["Geonhui Son", "Jeong Ryong Lee", "Dosik Hwang"], "title": "HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency", "comment": "Accepted manuscript. This is the accepted version of the article published in Neural Networks", "summary": "Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fr\u00e9chet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: https://github.com/higun2/HP-GAN.", "AI": {"tldr": "HP-GAN\u901a\u8fc7FakeTwins\u81ea\u76d1\u7763\u635f\u5931\u548c\u5224\u522b\u5668\u4e00\u81f4\u6027\u673a\u5236\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u63d0\u5347GAN\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u8ba1\u7b97\u611f\u77e5\u635f\u5931\u6216\u7279\u5f81\u7a7a\u95f4\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5148\u9a8c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u5224\u522b\u5668\u4e00\u81f4\u6027\u673a\u5236\uff0c\u66f4\u6709\u6548\u5730\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u63d0\u5347GAN\u6027\u80fd\u3002", "method": "\u63d0\u51faHP-GAN\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a1) FakeTwins\uff1a\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u4f5c\u4e3a\u7f16\u7801\u5668\u8ba1\u7b97\u81ea\u76d1\u7763\u635f\u5931\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u50cf\u8bad\u7ec3\u751f\u6210\u5668\uff1b2) \u5224\u522b\u5668\u4e00\u81f4\u6027\uff1a\u5728\u57fa\u4e8eCNN\u548cViT\u7279\u5f81\u7f51\u7edc\u7684\u5224\u522b\u5668\u4e4b\u95f4\u5efa\u7acb\u4e00\u81f4\u6027\u673a\u5236\uff0c\u4fc3\u8fdb\u534f\u540c\u5b66\u4e60\u5e76\u589e\u5f3a\u8bad\u7ec3\u9c81\u68d2\u6027\u3002", "result": "\u572817\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u5927\u89c4\u6a21\u3001\u5c0f\u89c4\u6a21\u548c\u6709\u9650\u6570\u636e\u573a\u666f\uff0c\u8986\u76d6\u591a\u79cd\u56fe\u50cf\u9886\u57df\uff09\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cHP-GAN\u5728Fr\u00e9chet Inception Distance\uff08FID\uff09\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u591a\u6837\u6027\u548c\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "HP-GAN\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u5224\u522b\u5668\u4e00\u81f4\u6027\u673a\u5236\uff0c\u6709\u6548\u5229\u7528\u4e86\u795e\u7ecf\u7f51\u7edc\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86GAN\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.02547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02547", "abs": "https://arxiv.org/abs/2602.02547", "authors": ["Hankyeol Kim", "Pilsung Kang"], "title": "naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement", "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.", "AI": {"tldr": "naPINN\u662f\u4e00\u79cd\u566a\u58f0\u81ea\u9002\u5e94\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5d4c\u5165\u80fd\u91cf\u6a21\u578b\u5b66\u4e60\u9884\u6d4b\u6b8b\u5dee\u7684\u6f5c\u5728\u5206\u5e03\uff0c\u4f7f\u7528\u53ef\u8bad\u7ec3\u53ef\u9760\u6027\u95e8\u81ea\u9002\u5e94\u8fc7\u6ee4\u9ad8\u80fd\u91cf\u6570\u636e\u70b9\uff0c\u6709\u6548\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u548c\u5f02\u5e38\u503c\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfPINN\u5728\u590d\u6742\u6d4b\u91cf\u566a\u58f0\u548c\u4e25\u91cd\u5f02\u5e38\u503c\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u566a\u58f0\u5206\u5e03\u77e5\u8bc6\u5c31\u80fd\u4ece\u635f\u574f\u6d4b\u91cf\u4e2d\u7a33\u5065\u6062\u590d\u7269\u7406\u89e3\u7684\u65b9\u6cd5\u3002", "method": "naPINN\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u5d4c\u5165\u80fd\u91cf\u6a21\u578b\u5b66\u4e60\u9884\u6d4b\u6b8b\u5dee\u7684\u6f5c\u5728\u5206\u5e03\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u80fd\u91cf\u666f\u89c2\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u53ef\u9760\u6027\u95e8\u81ea\u9002\u5e94\u8fc7\u6ee4\u9ad8\u80fd\u91cf\u6570\u636e\u70b9\uff0c\u540c\u65f6\u4f7f\u7528\u62d2\u7edd\u6210\u672c\u6b63\u5219\u5316\u9632\u6b62\u6709\u6548\u6570\u636e\u88ab\u4e22\u5f03\u7684\u5e73\u51e1\u89e3\u3002", "result": "\u5728\u591a\u79cd\u53d7\u975e\u9ad8\u65af\u566a\u58f0\u548c\u4e0d\u540c\u5f02\u5e38\u503c\u7387\u635f\u574f\u7684\u57fa\u51c6\u504f\u5fae\u5206\u65b9\u7a0b\u4e0a\uff0cnaPINN\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9c81\u68d2PINN\u57fa\u7ebf\uff0c\u6210\u529f\u9694\u79bb\u5f02\u5e38\u503c\u5e76\u5728\u4e25\u91cd\u6570\u636e\u635f\u574f\u4e0b\u51c6\u786e\u91cd\u5efa\u52a8\u529b\u5b66\u3002", "conclusion": "naPINN\u80fd\u591f\u4ece\u635f\u574f\u6d4b\u91cf\u4e2d\u7a33\u5065\u6062\u590d\u7269\u7406\u89e3\uff0c\u65e0\u9700\u5148\u9a8c\u566a\u58f0\u5206\u5e03\u77e5\u8bc6\uff0c\u4e3a\u5904\u7406\u590d\u6742\u566a\u58f0\u548c\u5f02\u5e38\u503c\u7684\u9006\u95ee\u9898\u548c\u63a7\u5236\u65b9\u7a0b\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02995", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02995", "abs": "https://arxiv.org/abs/2602.02995", "authors": ["Sizhe Tang", "Rongqian Chen", "Tian Lan"], "title": "Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents", "comment": null, "summary": "While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\\sim 77\\%$, significantly outperforming trajectory-level baselines under equivalent compute.", "AI": {"tldr": "Agent Alpha\u662f\u4e00\u4e2aGUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u7ea7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7ed3\u5408\u751f\u6210\u3001\u63a2\u7d22\u548c\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e3b\u52a8\u89c4\u5212\u3001\u65e9\u671f\u526a\u679d\u548c\u524d\u7f00\u91cd\u7528\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u5230\u7ea677%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u901a\u8fc7\u8f68\u8ff9\u7ea7\u91c7\u6837\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff0c\u4f46\u7f3a\u4e4f\u56de\u5f52\u80fd\u529b\uff0c\u65e0\u6cd5\u91cd\u7528\u90e8\u5206\u6210\u529f\u7ed3\u679c\u6216\u4ece\u65e9\u671f\u9519\u8bef\u4e2d\u6062\u590d\u3002", "method": "\u63d0\u51faAgent Alpha\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u7ea7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7ed3\u5408\u751f\u6210\u3001\u63a2\u7d22\u548c\u8bc4\u4f30\uff0c\u91c7\u7528alpha-UCT\u5f15\u5bfc\u641c\u7d22\u3001\u6bd4\u8f83\u9a71\u52a8\u8bc4\u4f30\u548c\u591a\u6837\u6027\u7ea6\u675f\u6269\u5c55\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u5230\u7ea677%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u8ba1\u7b97\u6761\u4ef6\u4e0b\u7684\u8f68\u8ff9\u7ea7\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Agent Alpha\u901a\u8fc7\u6b65\u7ea7MCTS\u5b9e\u73b0\u4e86\u6709\u6548\u7684GUI\u4ee3\u7406\u89c4\u5212\uff0c\u80fd\u591f\u4e3b\u52a8\u5efa\u6a21\u89c4\u5212\u7a7a\u95f4\u7ed3\u6784\uff0c\u652f\u6301\u65e9\u671f\u526a\u679d\u548c\u524d\u7f00\u91cd\u7528\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.03084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03084", "abs": "https://arxiv.org/abs/2602.03084", "authors": ["Zhitao Gao", "Jie Ma", "Xuhong Li", "Pengyu Li", "Ning Qu", "Yaqiang Wu", "Hui Liu", "Jun Liu"], "title": "AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback", "comment": null, "summary": "Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \\underline{A}utonomous \\underline{E}volutionary \\underline{R}easoning \\underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \\textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\\% on Qwen3-4B-Base and 5.10\\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.", "AI": {"tldr": "AERO\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u81ea\u4e3b\u63a8\u7406\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u7cfb\u7edf\u548c\u71b5\u57fa\u5b9a\u4f4d\u89e3\u51b3LLM\u81ea\u6211\u8fdb\u5316\u4e2d\u7684\u5b66\u4e60\u533a\u57df\u9009\u62e9\u548c\u96c6\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728\u590d\u6742\u63a8\u7406\u4e2d\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u548c\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u800c\u73b0\u6709\u7684\u81ea\u6211\u8fdb\u5316\u8303\u5f0f\u5f80\u5f80\u65e0\u6cd5\u786e\u5b9a\u6700\u4f73\u5b66\u4e60\u533a\u57df\uff0c\u5e76\u4e14\u901a\u8fc7\u6709\u7f3a\u9677\u7684\u5185\u90e8\u53cd\u9988\u5f3a\u5316\u96c6\u4f53\u5e7b\u89c9\u548c\u9519\u8bef\u5148\u9a8c\u3002", "method": "\u63d0\u51faAERO\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eZPD\u7406\u8bba\u4f7f\u7528\u71b5\u57fa\u5b9a\u4f4d\u6765\u9488\u5bf9\"\u53ef\u89e3\u6027\u5dee\u8ddd\"\uff1b2\uff09\u91c7\u7528\u72ec\u7acb\u53cd\u4e8b\u5b9e\u6821\u6b63\u8fdb\u884c\u9c81\u68d2\u9a8c\u8bc1\uff1b3\uff09\u5f15\u5165\u4ea4\u9519\u8bad\u7ec3\u7b56\u7565\u540c\u6b65\u529f\u80fd\u89d2\u8272\u80fd\u529b\u589e\u957f\u5e76\u9632\u6b62\u8bfe\u7a0b\u5d29\u6e83\uff1b4\uff09\u5728\u534f\u540c\u53cc\u5faa\u73af\u7cfb\u7edf\u4e2d\u5185\u5316\u81ea\u6211\u63d0\u95ee\u3001\u56de\u7b54\u548c\u6279\u8bc4\u3002", "result": "\u5728\u8de8\u8d8a\u4e09\u4e2a\u9886\u57df\u7684\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAERO\u5728Qwen3-4B-Base\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53474.57%\uff0c\u5728Qwen3-8B-Base\u4e0a\u63d0\u53475.10%\uff0c\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u3002", "conclusion": "AERO\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u81ea\u4e3b\u63a8\u7406\u8fdb\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u81ea\u6211\u8fdb\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3aLLM\u7684\u81ea\u4e3b\u80fd\u529b\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.03219", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03219", "abs": "https://arxiv.org/abs/2602.03219", "authors": ["Guhong Chen", "Chenghao Sun", "Cheng Fu", "Qiyao Wang", "Zhihong Huang", "Chaopeng Wei", "Guangxu Chen", "Feiteng Fang", "Ahmadreza Argha", "Bing Zhao", "Xander Xu", "Qi Han", "Hamid Alinejad-Rokny", "Qiang Qu", "Binhua Li", "Shiwen Ni", "Min Yang", "Hu Wei", "Yongbin Li"], "title": "Beyond Quantity: Trajectory Diversity Scaling for Code Agents", "comment": null, "summary": "As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.", "AI": {"tldr": "TDScaling\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u591a\u6837\u6027\u6269\u5c55\u7684\u4ee3\u7801\u4ee3\u7406\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u52a0\u8f68\u8ff9\u591a\u6837\u6027\u800c\u975e\u6570\u91cf\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5305\u542b\u4e1a\u52a1\u96c6\u7fa4\u3001\u84dd\u56fe\u9a71\u52a8\u591a\u4ee3\u7406\u3001\u81ea\u9002\u5e94\u8fdb\u5316\u548c\u6c99\u76d2\u4ee3\u7801\u5de5\u5177\u56db\u5927\u521b\u65b0\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7MCP\u6f14\u5316\u4e3a\u5de5\u5177\u4ea4\u4e92\u4ee3\u7406\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u5230\u4f4e\u8d28\u91cf\u5408\u6210\u6570\u636e\u548c\u6570\u91cf\u6269\u5c55\u6536\u76ca\u9012\u51cf\u7684\u9650\u5236\u3002\u6570\u91cf\u4e3a\u4e2d\u5fc3\u7684\u6269\u5c55\u5b58\u5728\u65e9\u671f\u74f6\u9888\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8f68\u8ff9\u6570\u636e\u3002", "method": "\u63d0\u51faTDScaling\u6846\u67b6\uff1a1)\u4e1a\u52a1\u96c6\u7fa4\u673a\u5236\u6355\u6349\u771f\u5b9e\u670d\u52a1\u903b\u8f91\u4f9d\u8d56\uff1b2)\u84dd\u56fe\u9a71\u52a8\u591a\u4ee3\u7406\u8303\u5f0f\u786e\u4fdd\u8f68\u8ff9\u8fde\u8d2f\u6027\uff1b3)\u57fa\u4e8e\u9886\u57df\u71b5\u3001\u63a8\u7406\u6a21\u5f0f\u71b5\u548c\u7d2f\u79ef\u52a8\u4f5c\u590d\u6742\u5ea6\u7684\u81ea\u9002\u5e94\u8fdb\u5316\u673a\u5236\u5f15\u5bfc\u5408\u6210\u8d70\u5411\u957f\u5c3e\u573a\u666f\uff1b4)\u6c99\u76d2\u4ee3\u7801\u5de5\u5177\u9632\u6b62\u5185\u5728\u7f16\u7801\u80fd\u529b\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u901a\u7528\u5de5\u5177\u4f7f\u7528\u57fa\u51c6(BFCL, tau^2-Bench)\u548c\u4ee3\u7801\u4ee3\u7406\u4efb\u52a1(RebenchT, CodeCI, BIRD)\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTDScaling\u5b9e\u73b0\u4e86\u53cc\u8d62\uff1a\u65e2\u63d0\u5347\u4e86\u5de5\u5177\u4f7f\u7528\u6cdb\u5316\u80fd\u529b\uff0c\u53c8\u589e\u5f3a\u4e86\u5185\u5728\u7f16\u7801\u719f\u7ec3\u5ea6\u3002\u5c06\u53d1\u5e03\u5305\u542b30,000+\u5de5\u5177\u96c6\u7fa4\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "conclusion": "TDScaling\u901a\u8fc7\u8f68\u8ff9\u591a\u6837\u6027\u6269\u5c55\u800c\u975e\u6570\u91cf\u6269\u5c55\uff0c\u5728\u56fa\u5b9a\u8bad\u7ec3\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6210\u672c\u6743\u8861\uff0c\u89e3\u51b3\u4e86\u4ee3\u7801\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u548c\u6cdb\u5316\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.03338", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03338", "abs": "https://arxiv.org/abs/2602.03338", "authors": ["Rakshith Vasudev", "Melisa Russak", "Dan Bikel", "Waseem Alshikh"], "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention", "comment": null, "summary": "Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.\n  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.", "AI": {"tldr": "LLM\u6279\u8bc4\u6a21\u578b\u7684\u4e3b\u52a8\u5e72\u9884\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u4f7f\u79bb\u7ebf\u51c6\u786e\u7387\u5f88\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u5e72\u9884\u5b58\u5728\"\u7834\u574f-\u6062\u590d\"\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u90e8\u7f72\u524d\u6d4b\u8bd5\u6765\u9884\u6d4b\u5e72\u9884\u6548\u679c\uff0c\u907f\u514d\u6027\u80fd\u9000\u5316\u3002", "motivation": "\u5c3d\u7ba1LLM\u6279\u8bc4\u6a21\u578b\u88ab\u8ba4\u4e3a\u80fd\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u5176\u5728\u90e8\u7f72\u65f6\u7684\u5b9e\u9645\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u5177\u6709\u9ad8\u79bb\u7ebf\u51c6\u786e\u7387\u7684\u6279\u8bc4\u6a21\u578b\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u8fd9\u8868\u660e\u4ec5\u51ed\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u5224\u65ad\u5e72\u9884\u662f\u5426\u5b89\u5168\u3002", "method": "\u63d0\u51fa\"\u7834\u574f-\u6062\u590d\"\u6743\u8861\u6846\u67b6\uff1a\u5e72\u9884\u53ef\u80fd\u6062\u590d\u5931\u8d25\u7684\u8f68\u8ff9\uff0c\u4f46\u4e5f\u53ef\u80fd\u7834\u574f\u539f\u672c\u4f1a\u6210\u529f\u7684\u8f68\u8ff9\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u90e8\u7f72\u524d\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ec5\u970050\u4e2a\u4efb\u52a1\u7684\u8bd5\u70b9\u5c31\u80fd\u4f30\u8ba1\u5e72\u9884\u53ef\u80fd\u5e26\u6765\u7684\u5e2e\u52a9\u6216\u635f\u5bb3\uff0c\u65e0\u9700\u5b8c\u5168\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u5728\u9ad8\u6210\u529f\u7387\u4efb\u52a1\u4e0a\uff0c\u5e72\u9884\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d0\u5230-26\u4e2a\u767e\u5206\u70b9\uff1b\u800c\u5728\u9ad8\u5931\u8d25\u7387\u7684ALFWorld\u57fa\u51c6\u4e0a\uff0c\u5e72\u9884\u5e26\u6765+2.8\u4e2a\u767e\u5206\u70b9\u7684\u9002\u5ea6\u6539\u8fdb(p=0.014)\u3002\u90e8\u7f72\u524d\u6d4b\u8bd5\u80fd\u6b63\u786e\u9884\u6d4b\u8fd9\u4e9b\u7ed3\u679c\u3002", "conclusion": "LLM\u6279\u8bc4\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u51b3\u5b9a\u5e72\u9884\u662f\u5426\u5b89\u5168\u3002\u63d0\u51fa\u7684\u90e8\u7f72\u524d\u6d4b\u8bd5\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u4f55\u65f6\u4e0d\u5e94\u5e72\u9884\uff0c\u4ece\u800c\u5728\u90e8\u7f72\u524d\u9632\u6b62\u4e25\u91cd\u7684\u6027\u80fd\u9000\u5316\u3002\u8be5\u6846\u67b6\u7684\u4e3b\u8981\u4ef7\u503c\u5728\u4e8e\u907f\u514d\u6709\u5bb3\u5e72\u9884\u3002"}}
{"id": "2602.03249", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03249", "abs": "https://arxiv.org/abs/2602.03249", "authors": ["Zhicheng Yang", "Zhijiang Guo", "Yinya Huang", "Yongxin Wang", "Wenlei Shi", "Yiwei Wang", "Xiaodan Liang", "Jing Tang"], "title": "Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning", "comment": null, "summary": "Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.", "AI": {"tldr": "Accordion-Thinking\u6846\u67b6\u8ba9LLM\u5b66\u4f1a\u52a8\u6001\u603b\u7ed3\u63a8\u7406\u6b65\u9aa4\uff0c\u901a\u8fc7\u6298\u53e0\u6a21\u5f0f\u51cf\u5c11KV\u7f13\u5b58\u4f9d\u8d56\uff0c\u5b9e\u73b03\u500d\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\u4e14\u4fdd\u6301\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u65b9\u6cd5\u9762\u4e34KV\u7f13\u5b58\u7ebf\u6027\u589e\u957f\u548c\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u4e8c\u6b21\u65b9\u589e\u957f\u7684\u5b9e\u8df5\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u673a\u5236", "method": "\u5f15\u5165Accordion-Thinking\u6846\u67b6\uff0cLLM\u5b66\u4e60\u901a\u8fc7\u52a8\u6001\u603b\u7ed3\u6765\u81ea\u6211\u8c03\u8282\u63a8\u7406\u6b65\u9aa4\u7c92\u5ea6\uff0c\u91c7\u7528\u6298\u53e0\u63a8\u7406\u6a21\u5f0f\u5b9a\u671f\u603b\u7ed3\u601d\u7ef4\u8fc7\u7a0b\u5e76\u4e22\u5f03\u5386\u53f2token\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u6fc0\u52b1\u8fd9\u79cd\u80fd\u529b", "result": "\u6a21\u578b\u5b66\u4f1a\u5c06\u5173\u952e\u63a8\u7406\u4fe1\u606f\u7f16\u7801\u5230\u7d27\u51d1\u603b\u7ed3\u4e2d\uff0c\u6298\u53e0\u6a21\u5f0f\u4e0e\u5b8c\u6574\u5c55\u5f00\u6a21\u5f0f\u7684\u7cbe\u5ea6\u5dee\u8ddd\u5728\u8bad\u7ec3\u4e2d\u9010\u6e10\u7f29\u5c0f\u76f4\u81f3\u6d88\u5931\uff0c\u572848GB GPU\u5185\u5b58\u914d\u7f6e\u4e0b\u5b9e\u73b03\u500d\u541e\u5410\u91cf\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u81ea\u6211\u538b\u7f29\uff0cLLM\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u5927\u91cf\u5386\u53f2token\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u7ed3\u6784\u5316\u6b65\u9aa4\u603b\u7ed3\u63d0\u4f9b\u4e86\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u8fc7\u7a0b\u8bb0\u5f55"}}
{"id": "2602.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03442", "abs": "https://arxiv.org/abs/2602.03442", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Shaohan Wang", "Pengyu Wang", "Xiaorui Wang", "Zhendong Mao"], "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces", "comment": "18 pages, 8 figures", "summary": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.", "AI": {"tldr": "A-RAG\u662f\u4e00\u4e2a\u4ee3\u7406\u5f0fRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u6a21\u578b\u66b4\u9732\u5206\u5c42\u68c0\u7d22\u63a5\u53e3\uff0c\u8ba9\u6a21\u578b\u53c2\u4e0e\u68c0\u7d22\u51b3\u7b56\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5229\u7528\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5229\u7528\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u957f\u65f6\u7a0b\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002\u5b83\u4eec\u8981\u4e48\u91c7\u7528\u5355\u6b21\u68c0\u7d22\u7b97\u6cd5\uff0c\u8981\u4e48\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7a0b\uff0c\u90fd\u4e0d\u8ba9\u6a21\u578b\u53c2\u4e0e\u68c0\u7d22\u51b3\u7b56\uff0c\u9650\u5236\u4e86\u6a21\u578b\u80fd\u529b\u7684\u53d1\u6325\u3002", "method": "A-RAG\u6846\u67b6\u5411\u6a21\u578b\u63d0\u4f9b\u4e09\u79cd\u5206\u5c42\u68c0\u7d22\u5de5\u5177\uff1a\u5173\u952e\u8bcd\u641c\u7d22\u3001\u8bed\u4e49\u641c\u7d22\u548c\u5206\u5757\u8bfb\u53d6\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5728\u591a\u4e2a\u7c92\u5ea6\u4e0a\u641c\u7d22\u548c\u68c0\u7d22\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u653e\u57dfQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cA-RAG\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u68c0\u7d22token\u6570\u91cf\u76f8\u5f53\u6216\u66f4\u5c11\uff0c\u8868\u660eA-RAG\u80fd\u6709\u6548\u5229\u7528\u6a21\u578b\u80fd\u529b\u5e76\u52a8\u6001\u9002\u5e94\u4e0d\u540cRAG\u4efb\u52a1\u3002", "conclusion": "A-RAG\u901a\u8fc7\u8ba9\u6a21\u578b\u53c2\u4e0e\u68c0\u7d22\u51b3\u7b56\uff0c\u6210\u529f\u5229\u7528\u4e86\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765RAG\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\u548c\u8bc4\u4f30\u5957\u4ef6\u3002"}}
{"id": "2602.03230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03230", "abs": "https://arxiv.org/abs/2602.03230", "authors": ["Shaoyu Liu", "Jianing Li", "Guanghui Zhao", "Yunjian Zhang", "Wen Jiang", "Ming Li", "Xiangyang Ji"], "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision", "comment": null, "summary": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.", "AI": {"tldr": "EventFlash\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e8b\u4ef6\u9a71\u52a8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u4ee4\u724c\u7a00\u758f\u5316\u51cf\u5c11\u6570\u636e\u5197\u4f59\u5e76\u52a0\u901f\u63a8\u7406\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b012.4\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4e8b\u4ef6\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u91c7\u7528\u5bc6\u96c6\u7684\u56fe\u50cf\u5f0f\u5904\u7406\u8303\u5f0f\uff0c\u5ffd\u89c6\u4e86\u4e8b\u4ef6\u6d41\u7684\u65f6\u7a7a\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u4e8b\u4ef6\u6570\u636e\u3002", "method": "1) \u6784\u5efaEventMind\u5927\u89c4\u6a21\u591a\u6837\u5316\u573a\u666f\u6570\u636e\u96c6(\u8d85\u8fc750\u4e07\u6307\u4ee4\u96c6)\uff1b2) \u8bbe\u8ba1\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u53e3\u805a\u5408\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u65f6\u95f4\u91c7\u6837\uff1b3) \u5f00\u53d1\u7a00\u758f\u5bc6\u5ea6\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u9ad8\u7a7a\u95f4\u4ee4\u724c\u6548\u7387\u3002", "result": "EventFlash\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b(EventFlash-Zero)\u5b9e\u73b012.4\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u3002\u652f\u6301\u5904\u7406\u957f\u8fbe1000\u4e2a\u65f6\u95f4\u4ed3\u7684\u4e8b\u4ef6\u6d41\uff0c\u663e\u8457\u4f18\u4e8eEventGPT\u76845\u4ed3\u9650\u5236\u3002", "conclusion": "EventFlash\u901a\u8fc7\u63a2\u7d22\u65f6\u7a7a\u4ee4\u724c\u7a00\u758f\u5316\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u4e8b\u4ef6\u6570\u636e\u5197\u4f59\u5e76\u52a0\u901f\u4e86\u63a8\u7406\uff0c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u89c6\u89c9\u7684\u9ad8\u6548\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.02611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02611", "abs": "https://arxiv.org/abs/2602.02611", "authors": ["David Vigouroux", "Lucas Drumetz", "Ronan Fablet", "Fran\u00e7ois Rousseau"], "title": "Discovering Data Manifold Geometry via Non-Contracting Flows", "comment": null, "summary": "We introduce an unsupervised approach for constructing a global reference system by learning, in the ambient space, vector fields that span the tangent spaces of an unknown data manifold. In contrast to isometric objectives, which implicitly assume manifold flatness, our method learns tangent vector fields whose flows transport all samples to a common, learnable reference point. The resulting arc-lengths along these flows define interpretable intrinsic coordinates tied to a shared global frame. To prevent degenerate collapse, we enforce a non-shrinking constraint and derive a scalable, integration-free objective inspired by flow matching. Within our theoretical framework, we prove that minimizing the proposed objective recovers a global coordinate chart when one exists. Empirically, we obtain correct tangent alignment and coherent global coordinate structure on synthetic manifolds. We also demonstrate the scalability of our method on CIFAR-10, where the learned coordinates achieve competitive downstream classification performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5b66\u4e60\u73af\u5883\u7a7a\u95f4\u4e2d\u5b66\u4e60\u5411\u91cf\u573a\u6765\u6784\u5efa\u5168\u5c40\u53c2\u8003\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u5411\u91cf\u573a\u8de8\u8d8a\u672a\u77e5\u6570\u636e\u6d41\u5f62\u7684\u5207\u7a7a\u95f4\uff0c\u5c06\u6837\u672c\u4f20\u8f93\u5230\u5171\u540c\u53c2\u8003\u70b9\uff0c\u4ece\u800c\u5b9a\u4e49\u53ef\u89e3\u91ca\u7684\u5185\u5728\u5750\u6807\u3002", "motivation": "\u4f20\u7edf\u7b49\u8ddd\u76ee\u6807\u9690\u542b\u5047\u8bbe\u6d41\u5f62\u5e73\u5766\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u6d41\u5f62\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b66\u4e60\u5168\u5c40\u5750\u6807\u56fe\u7684\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u70b9\u6620\u5c04\u5230\u5171\u4eab\u53c2\u8003\u6846\u67b6\u4e2d\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5185\u5728\u5750\u6807\u8868\u793a\u3002", "method": "\u5b66\u4e60\u5207\u5411\u91cf\u573a\uff0c\u5176\u6d41\u5c06\u6240\u6709\u6837\u672c\u4f20\u8f93\u5230\u5171\u540c\u53ef\u5b66\u4e60\u53c2\u8003\u70b9\u3002\u4f7f\u7528\u975e\u6536\u7f29\u7ea6\u675f\u9632\u6b62\u9000\u5316\u5d29\u6e83\uff0c\u5e76\u63a8\u5bfc\u51fa\u53d7\u6d41\u5339\u914d\u542f\u53d1\u7684\u53ef\u6269\u5c55\u3001\u65e0\u9700\u79ef\u5206\u7684\u4f18\u5316\u76ee\u6807\u3002\u5728\u7406\u8bba\u6846\u67b6\u4e2d\u8bc1\u660e\u6700\u5c0f\u5316\u8be5\u76ee\u6807\u53ef\u6062\u590d\u5168\u5c40\u5750\u6807\u56fe\u3002", "result": "\u5728\u5408\u6210\u6d41\u5f62\u4e0a\u83b7\u5f97\u6b63\u786e\u7684\u5207\u5411\u5bf9\u9f50\u548c\u4e00\u81f4\u7684\u5168\u5c40\u5750\u6807\u7ed3\u6784\u3002\u5728CIFAR-10\u4e0a\u5c55\u793a\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\uff0c\u5b66\u4e60\u5230\u7684\u5750\u6807\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u65e0\u76d1\u7763\u5730\u5b66\u4e60\u6570\u636e\u6d41\u5f62\u7684\u5168\u5c40\u5750\u6807\u7cfb\u7edf\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5185\u5728\u8868\u793a\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u6d41\u5f62\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.02671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02671", "abs": "https://arxiv.org/abs/2602.02671", "authors": ["Francesco Leonardi", "Boris Bonev", "Kaspar Riesen"], "title": "MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields", "comment": null, "summary": "Machine learning force fields (MLFFs) have become essential for accurate and efficient atomistic modeling. Despite their high accuracy, most existing approaches rely on fixed angular expansions, limiting flexibility in weighting local geometric interactions. We introduce Modular Angular-Radial Attention (MARA), a module that extends spherical attention -- originally developed for SO(3) tasks -- to the molecular domain and SE(3), providing an efficient approximation of equivariant interactions. MARA operates directly on the angular and radial coordinates of neighboring atoms, enabling flexible, geometrically informed, and modular weighting of local environments. Unlike existing attention mechanisms in SE(3)-equivariant architectures, MARA can be integrated in a plug-and-play manner into models such as MACE without architectural modifications. Across molecular benchmarks, MARA improves energy and force predictions, reduces high-error events, and enhances robustness. These results demonstrate that continuous spherical attention is an effective and generalizable geometric operator that increases the expressiveness, stability, and reliability of atomistic models.", "AI": {"tldr": "MARA\u6a21\u5757\u5c06\u7403\u5f62\u6ce8\u610f\u529b\u6269\u5c55\u5230\u5206\u5b50\u9886\u57df\uff0c\u901a\u8fc7\u7075\u6d3b\u52a0\u6743\u5c40\u90e8\u51e0\u4f55\u76f8\u4e92\u4f5c\u7528\u63d0\u5347\u673a\u5668\u5b66\u4e60\u529b\u573a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u529b\u573a\u5927\u591a\u4f9d\u8d56\u56fa\u5b9a\u7684\u89d2\u5ea6\u5c55\u5f00\uff0c\u9650\u5236\u4e86\u5c40\u90e8\u51e0\u4f55\u76f8\u4e92\u4f5c\u7528\u52a0\u6743\u7684\u7075\u6d3b\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u51e0\u4f55\u611f\u77e5\u7684\u6a21\u5757\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u89d2\u5ea6-\u5f84\u5411\u6ce8\u610f\u529b(MARA)\uff0c\u5c06\u7403\u5f62\u6ce8\u610f\u529b\u6269\u5c55\u5230SE(3)\u7b49\u53d8\u67b6\u6784\uff0c\u76f4\u63a5\u64cd\u4f5c\u76f8\u90bb\u539f\u5b50\u7684\u89d2\u5ea6\u548c\u5f84\u5411\u5750\u6807\uff0c\u5b9e\u73b0\u7075\u6d3b\u3001\u51e0\u4f55\u611f\u77e5\u7684\u5c40\u90e8\u73af\u5883\u52a0\u6743\u3002", "result": "\u5728\u5206\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMARA\u6539\u5584\u4e86\u80fd\u91cf\u548c\u529b\u9884\u6d4b\uff0c\u51cf\u5c11\u4e86\u9ad8\u8bef\u5dee\u4e8b\u4ef6\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e14\u80fd\u5373\u63d2\u5373\u7528\u5730\u96c6\u6210\u5230MACE\u7b49\u6a21\u578b\u4e2d\u3002", "conclusion": "\u8fde\u7eed\u7403\u5f62\u6ce8\u610f\u529b\u662f\u6709\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u51e0\u4f55\u7b97\u5b50\uff0c\u80fd\u63d0\u9ad8\u539f\u5b50\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.03569", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03569", "abs": "https://arxiv.org/abs/2602.03569", "authors": ["Linjie Mu", "Zhongzhen Huang", "Yannian Gu", "Shengqian Qin", "Shaoting Zhang", "Xiaofan Zhang"], "title": "EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories", "comment": null, "summary": "World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.", "AI": {"tldr": "EHRWorld\uff1a\u57fa\u4e8e\u56e0\u679c\u5e8f\u5217\u8303\u5f0f\u8bad\u7ec3\u7684\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u4e16\u754c\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\uff0c\u5728\u957f\u671f\u4e34\u5e8a\u6a21\u62df\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a", "motivation": "\u4e16\u754c\u6a21\u578b\u4e3a\u5e72\u9884\u4e0b\u7684\u672a\u6765\u72b6\u6001\u6a21\u62df\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u4f46\u5728\u533b\u5b66\u7b49\u590d\u6742\u9ad8\u98ce\u9669\u9886\u57df\u5b9e\u73b0\u4ecd\u5177\u6311\u6218\u3002\u867d\u7136LLM\u5728\u9759\u6001\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u80fd\u5426\u4f5c\u4e3a\u52a8\u6001\u533b\u7597\u4e16\u754c\u6a21\u578b\u6765\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u548c\u6cbb\u7597\u7ed3\u679c\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u53d1\u73b0LLM\u5728\u8fde\u7eed\u5e72\u9884\u4e0b\u96be\u4ee5\u7ef4\u6301\u4e00\u81f4\u7684\u60a3\u8005\u72b6\u6001\uff0c\u5bfc\u81f4\u957f\u671f\u4e34\u5e8a\u6a21\u62df\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "method": "\u63d0\u51faEHRWorld\uff0c\u4e00\u4e2a\u5728\u56e0\u679c\u5e8f\u5217\u8303\u5f0f\u4e0b\u8bad\u7ec3\u7684\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u4e16\u754c\u6a21\u578b\uff0c\u540c\u65f6\u6784\u5efa\u4e86EHRWorld-110K\u2014\u2014\u4e00\u4e2a\u4ece\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7684\u5927\u89c4\u6a21\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u96c6\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cEHRWorld\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u957f\u671f\u6a21\u62df\u3001\u5bf9\u4e34\u5e8a\u654f\u611f\u4e8b\u4ef6\u7684\u6539\u8fdb\u5efa\u6a21\u4ee5\u53ca\u66f4\u4f18\u7684\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u53ef\u9760\u548c\u7a33\u5065\u7684\u533b\u7597\u4e16\u754c\u5efa\u6a21\u9700\u8981\u5728\u56e0\u679c\u57fa\u7840\u548c\u65f6\u5e8f\u6f14\u5316\u7684\u4e34\u5e8a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0cEHRWorld\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.02766", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02766", "abs": "https://arxiv.org/abs/2602.02766", "authors": ["Lucas Rosenblatt", "Peihan Liu", "Ryan McKenna", "Natalia Ponomareva"], "title": "Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data", "comment": null, "summary": "Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.", "AI": {"tldr": "PATH\uff1a\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u7eb5\u5411\u8868\u683c\u6570\u636e\uff0c\u76f8\u6bd4\u4f20\u7edf\u8fb9\u9645\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u5408\u6210\u8868\u683c\u6570\u636e\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u72ec\u7acb\u540c\u5206\u5e03\u7684\u884c\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u7eb5\u5411\u6570\u636e\u96c6\uff08\u5982\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff09\u7684\u65f6\u95f4\u590d\u6742\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u7528\u6237\u5386\u53f2\u6241\u5e73\u5316\u4e3a\u9ad8\u7ef4\u5411\u91cf\u4f1a\u7834\u574f\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faPATH\u6846\u67b6\uff0c\u5c06\u5b8c\u6574\u8868\u683c\u4f5c\u4e3a\u5408\u6210\u5355\u5143\uff0c\u5229\u7528\u5dee\u5206\u9690\u79c1\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u80fd\u529b\u6765\u751f\u6210\u6570\u636e\uff0c\u4fdd\u6301\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u8d2f\u6027\u3002", "result": "PATH\u76f8\u6bd4\u4e3b\u6d41\u8fb9\u9645\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u8f68\u8ff9\u7684\u5206\u5e03\u8ddd\u79bb\u51cf\u5c1160%\u4ee5\u4e0a\uff0c\u72b6\u6001\u8f6c\u79fb\u9519\u8bef\u51cf\u5c11\u8fd150%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u8fb9\u9645\u4fdd\u771f\u5ea6\u3002", "conclusion": "PATH\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u7eb5\u5411\u6570\u636e\u7684\u5dee\u5206\u9690\u79c1\u5408\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02767", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02767", "abs": "https://arxiv.org/abs/2602.02767", "authors": ["Meng Ding", "Jinhui Xu", "Kaiyi Ji"], "title": "Provable Effects of Data Replay in Continual Learning: A Feature Learning Perspective", "comment": "AISTATS 2026", "summary": "Continual learning (CL) aims to train models on a sequence of tasks while retaining performance on previously learned ones. A core challenge in this setting is catastrophic forgetting, where new learning interferes with past knowledge. Among various mitigation strategies, data-replay methods, where past samples are periodically revisited, are considered simple yet effective, especially when memory constraints are relaxed. However, the theoretical effectiveness of full data replay, where all past data is accessible during training, remains largely unexplored. In this paper, we present a comprehensive theoretical framework for analyzing full data-replay training in continual learning from a feature learning perspective. Adopting a multi-view data model, we identify the signal-to-noise ratio (SNR) as a critical factor affecting forgetting. Focusing on task-incremental binary classification across $M$ tasks, our analysis verifies two key conclusions: (1) forgetting can still occur under full replay when the cumulative noise from later tasks dominates the signal from earlier ones; and (2) with sufficient signal accumulation, data replay can recover earlier tasks-even if their initial learning was poor. Notably, we uncover a novel insight into task ordering: prioritizing higher-signal tasks not only facilitates learning of lower-signal tasks but also helps prevent catastrophic forgetting. We validate our theoretical findings through synthetic and real-world experiments that visualize the interplay between signal learning and noise memorization across varying SNRs and task correlation regimes.", "AI": {"tldr": "\u672c\u6587\u4ece\u7279\u5f81\u5b66\u4e60\u89d2\u5ea6\u5efa\u7acb\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u5168\u6570\u636e\u56de\u653e\u7684\u7406\u8bba\u6846\u67b6\uff0c\u53d1\u73b0\u4fe1\u566a\u6bd4\u662f\u5f71\u54cd\u9057\u5fd8\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5373\u4f7f\u5168\u6570\u636e\u56de\u653e\u4e0b\u9057\u5fd8\u4ecd\u53ef\u80fd\u53d1\u751f\uff0c\u4f46\u8db3\u591f\u4fe1\u53f7\u79ef\u7d2f\u53ef\u6062\u590d\u65e9\u671f\u4efb\u52a1\uff0c\u4e14\u4efb\u52a1\u6392\u5e8f\u7b56\u7565\uff08\u4f18\u5148\u9ad8\u4fe1\u53f7\u4efb\u52a1\uff09\u6709\u52a9\u4e8e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u7684\u6838\u5fc3\u6311\u6218\uff0c\u6570\u636e\u56de\u653e\u65b9\u6cd5\u867d\u7136\u7b80\u5355\u6709\u6548\uff0c\u4f46\u5176\u7406\u8bba\u6709\u6548\u6027\uff08\u7279\u522b\u662f\u5168\u6570\u636e\u56de\u653e\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u4ece\u7279\u5f81\u5b66\u4e60\u89d2\u5ea6\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u5168\u6570\u636e\u56de\u653e\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u591a\u89c6\u56fe\u6570\u636e\u6a21\u578b\uff0c\u4ee5\u4fe1\u566a\u6bd4\uff08SNR\uff09\u4e3a\u5173\u952e\u5206\u6790\u56e0\u7d20\uff0c\u805a\u7126\u4e8eM\u4e2a\u4efb\u52a1\u7684\u589e\u91cf\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e24\u4e2a\u5173\u952e\u7ed3\u8bba\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u53ef\u89c6\u5316\u4fe1\u53f7\u5b66\u4e60\u4e0e\u566a\u58f0\u8bb0\u5fc6\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "1) \u5373\u4f7f\u5728\u5168\u6570\u636e\u56de\u653e\u4e0b\uff0c\u5f53\u540e\u7eed\u4efb\u52a1\u7684\u7d2f\u79ef\u566a\u58f0\u4e3b\u5bfc\u65e9\u671f\u4efb\u52a1\u4fe1\u53f7\u65f6\uff0c\u9057\u5fd8\u4ecd\u4f1a\u53d1\u751f\uff1b2) \u901a\u8fc7\u8db3\u591f\u7684\u4fe1\u53f7\u79ef\u7d2f\uff0c\u6570\u636e\u56de\u653e\u53ef\u4ee5\u6062\u590d\u65e9\u671f\u4efb\u52a1\uff0c\u5373\u4f7f\u5176\u521d\u59cb\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\uff1b3) \u53d1\u73b0\u4efb\u52a1\u6392\u5e8f\u7684\u65b0\u89c1\u89e3\uff1a\u4f18\u5148\u5904\u7406\u9ad8\u4fe1\u53f7\u4efb\u52a1\u4e0d\u4ec5\u6709\u52a9\u4e8e\u5b66\u4e60\u4f4e\u4fe1\u53f7\u4efb\u52a1\uff0c\u8fd8\u80fd\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u5168\u6570\u636e\u56de\u653e\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4fe1\u566a\u6bd4\u5728\u9057\u5fd8\u673a\u5236\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u6392\u5e8f\u7b56\u7565\u7684\u65b0\u89c1\u89e3\uff0c\u4e3a\u7406\u89e3\u6570\u636e\u56de\u653e\u65b9\u6cd5\u7684\u7406\u8bba\u8fb9\u754c\u548c\u4f18\u5316\u6301\u7eed\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2602.02769", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02769", "abs": "https://arxiv.org/abs/2602.02769", "authors": ["Saurav Raj Pandey", "Harlin Lee"], "title": "BiTimeCrossNet: Time-Aware Self-Supervised Learning for Pediatric Sleep", "comment": null, "summary": "We present BiTimeCrossNet (BTCNet), a multimodal self-supervised learning framework for long physiological recordings such as overnight sleep studies. While many existing approaches train on short segments treated as independent samples, BTCNet incorporates information about when each segment occurs within its parent recording, for example within a sleep session. BTCNet further learns pairwise interactions between physiological signals via cross-attention, without requiring task labels or sequence-level supervision.\n  We evaluate BTCNet on pediatric sleep data across six downstream tasks, including sleep staging, arousal detection, and respiratory event detection. Under frozen-backbone linear probing, BTCNet consistently outperforms an otherwise identical non-time-aware variant, with gains that generalize to an independent pediatric dataset. Compared to existing multimodal self-supervised sleep models, BTCNet achieves strong performance, particularly on respiration-related tasks.", "AI": {"tldr": "BiTimeCrossNet (BTCNet) \u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u65f6\u95f4\u751f\u7406\u8bb0\u5f55\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff0c\u5728\u7761\u7720\u7814\u7a76\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u77ed\u7247\u6bb5\u89c6\u4e3a\u72ec\u7acb\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u5ffd\u7565\u4e86\u7247\u6bb5\u5728\u8bb0\u5f55\u4e2d\u7684\u65f6\u95f4\u4f4d\u7f6e\u4fe1\u606f\uff08\u5982\u5728\u7761\u7720\u4f1a\u8bdd\u4e2d\u7684\u4f4d\u7f6e\uff09\u3002\u540c\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u4e5f\u7f3a\u4e4f\u5bf9\u751f\u7406\u4fe1\u53f7\u95f4\u4ea4\u4e92\u5173\u7cfb\u7684\u6709\u6548\u5efa\u6a21\u3002", "method": "BTCNet \u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u65f6\u95f4\u611f\u77e5\u8bbe\u8ba1\uff0c\u5c06\u6bcf\u4e2a\u7247\u6bb5\u5728\u5176\u7236\u8bb0\u5f55\u4e2d\u7684\u65f6\u95f4\u4f4d\u7f6e\u4fe1\u606f\u7eb3\u5165\u5b66\u4e60\uff1b2) \u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b66\u4e60\u4e0d\u540c\u751f\u7406\u4fe1\u53f7\u4e4b\u95f4\u7684\u6210\u5bf9\u4ea4\u4e92\u5173\u7cfb\u3002\u6574\u4e2a\u6846\u67b6\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u6216\u5e8f\u5217\u7ea7\u76d1\u7763\u3002", "result": "\u5728\u513f\u79d1\u7761\u7720\u6570\u636e\u7684\u516d\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u7761\u7720\u5206\u671f\u3001\u89c9\u9192\u68c0\u6d4b\u3001\u547c\u5438\u4e8b\u4ef6\u68c0\u6d4b\u7b49\uff09\u8bc4\u4f30\u4e2d\uff0cBTCNet \u5728\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7edc\u7ebf\u6027\u63a2\u6d4b\u8bbe\u7f6e\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u975e\u65f6\u95f4\u611f\u77e5\u7684\u53d8\u4f53\uff0c\u4e14\u6027\u80fd\u63d0\u5347\u53ef\u63a8\u5e7f\u5230\u72ec\u7acb\u7684\u513f\u79d1\u6570\u636e\u96c6\u3002\u4e0e\u73b0\u6709\u591a\u6a21\u6001\u81ea\u76d1\u7763\u7761\u7720\u6a21\u578b\u76f8\u6bd4\uff0cBTCNet \u8868\u73b0\u5f3a\u52b2\uff0c\u5c24\u5176\u5728\u547c\u5438\u76f8\u5173\u4efb\u52a1\u4e0a\u3002", "conclusion": "BTCNet \u901a\u8fc7\u6574\u5408\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u4e3a\u957f\u65f6\u95f4\u751f\u7406\u8bb0\u5f55\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u513f\u79d1\u7761\u7720\u5206\u6790\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u547c\u5438\u76f8\u5173\u68c0\u6d4b\u65b9\u9762\u3002"}}
{"id": "2602.03604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03604", "abs": "https://arxiv.org/abs/2602.03604", "authors": ["Basile Terver", "Randall Balestriero", "Megi Dervishi", "David Fan", "Quentin Garrido", "Tushar Nagarajan", "Koustuv Sinha", "Wancong Zhang", "Mike Rabbat", "Yann LeCun", "Amir Bar"], "title": "A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures", "comment": null, "summary": "We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.", "AI": {"tldr": "EB-JEPA\u662f\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u5b66\u4e60\u8868\u793a\u548c\u4e16\u754c\u6a21\u578b\uff0c\u652f\u6301\u4ece\u56fe\u50cf\u5230\u89c6\u9891\u518d\u5230\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\u7684\u6269\u5c55\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u5b9e\u73b0\u548c\u5355GPU\u8bad\u7ec3\u3002", "motivation": "JEPA\u901a\u8fc7\u5728\u8868\u793a\u7a7a\u95f4\u800c\u975e\u50cf\u7d20\u7a7a\u95f4\u8fdb\u884c\u9884\u6d4b\uff0c\u907f\u514d\u4e86\u751f\u6210\u5efa\u6a21\u7684\u7f3a\u9677\uff0c\u540c\u65f6\u6355\u83b7\u9002\u5408\u4e0b\u6e38\u4efb\u52a1\u7684\u8bed\u4e49\u7279\u5f81\u3002\u9700\u8981\u4f7f\u57fa\u4e8e\u80fd\u91cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u66f4\u6613\u4e8e\u7814\u7a76\u548c\u6559\u80b2\u3002", "method": "\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u81ea\u5305\u542b\u7684JEPA\u5b9e\u73b0\uff0c\u5c06\u56fe\u50cf\u7ea7\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\u6269\u5c55\u5230\u89c6\u9891\uff08\u5904\u7406\u65f6\u95f4\u52a8\u6001\uff09\u548c\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\uff08\u9884\u6d4b\u63a7\u5236\u8f93\u5165\u6548\u679c\uff09\u3002\u652f\u6301\u5355GPU\u8bad\u7ec3\u3002", "result": "\u5728CIFAR-10\u4e0a\u83b7\u5f9791%\u7684\u8868\u793a\u63a2\u6d4b\u51c6\u786e\u7387\uff1b\u5728Moving MNIST\u4e0a\u5c55\u793a\u591a\u6b65\u9884\u6d4b\uff1b\u5728Two Rooms\u5bfc\u822a\u4efb\u52a1\u4e2d\u8fbe\u523097%\u7684\u89c4\u5212\u6210\u529f\u7387\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u5404\u6b63\u5219\u5316\u7ec4\u4ef6\u9632\u6b62\u8868\u793a\u5d29\u6e83\u7684\u5173\u952e\u91cd\u8981\u6027\u3002", "conclusion": "EB-JEPA\u6210\u529f\u5c55\u793a\u4e86JEPA\u6846\u67b6\u4ece\u9759\u6001\u56fe\u50cf\u5230\u65f6\u95f4\u89c6\u9891\u518d\u5230\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u7814\u7a76\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5de5\u5177\u3002"}}
{"id": "2602.03669", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.03669", "abs": "https://arxiv.org/abs/2602.03669", "authors": ["Sandeep Patil", "Yongqi Dong", "Haneen Farah", "Hans Hellendoorn"], "title": "Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images", "comment": "14 pages, 9 figures, under review by IEEE T-ITS", "summary": "Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u8f66\u9053\u7ebf\u68c0\u6d4b\uff0c\u5728\u906e\u6321\u548c\u5f3a\u5149\u7b49\u56f0\u96be\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u5f53\u524d\u8f66\u9053\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u591a\u529f\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u65b9\u6cd5\u5e38\u5ffd\u7565\u56fe\u50cf\u5173\u952e\u533a\u57df\u548c\u65f6\u7a7a\u663e\u8457\u6027\uff0c\u5bfc\u81f4\u5728\u4e25\u91cd\u906e\u6321\u548c\u5f3a\u5149\u7b49\u56f0\u96be\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6807\u51c6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u548c\u5e38\u89c1\u795e\u7ecf\u7f51\u7edc\u9aa8\u5e72\u7684\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5f15\u5165\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e13\u6ce8\u4e8e\u8f66\u9053\u7ebf\u5173\u952e\u7279\u5f81\u5e76\u5229\u7528\u8fde\u7eed\u56fe\u50cf\u5e27\u95f4\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5404\u79cd\u6d4b\u8bd5\u573a\u666f\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u4f7f\u6a21\u578b\u53c2\u6570\u66f4\u5c11\u3001MACs\u66f4\u4f4e\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e26\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u8f66\u9053\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u56f0\u96be\u573a\u666f\u4e0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.03747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03747", "abs": "https://arxiv.org/abs/2602.03747", "authors": ["Junchao Huang", "Ziyang Ye", "Xinting Hu", "Tianyu He", "Guiyu Zhang", "Shaoshuai Shi", "Jiang Bian", "Li Jiang"], "title": "LIVE: Long-horizon Interactive Video World Modeling", "comment": "18 pages, 22 figures", "summary": "Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.", "AI": {"tldr": "LIVE\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u9650\u5236\u8bef\u5dee\u7d2f\u79ef\u7684\u957f\u65f6\u57df\u4ea4\u4e92\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u65e0\u9700\u6559\u5e08\u84b8\u998f\uff0c\u5728\u957f\u65f6\u57df\u89c6\u9891\u751f\u6210\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5728\u77ed\u65f6\u57df\u9884\u6d4b\u6709\u6548\uff0c\u4f46\u5728\u957f\u65f6\u57df\u751f\u6210\u4e2d\uff0c\u5c0f\u7684\u9884\u6d4b\u8bef\u5dee\u4f1a\u968f\u65f6\u95f4\u7d2f\u79ef\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5f15\u5165\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u548c\u5e8f\u5217\u7ea7\u5206\u5e03\u5339\u914d\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u9632\u6b62\u8d85\u51fa\u8bad\u7ec3\u65f6\u57df\u7684\u8bef\u5dee\u4f20\u64ad\u3002", "method": "LIVE\u901a\u8fc7\u65b0\u9896\u7684\u5faa\u73af\u4e00\u81f4\u6027\u76ee\u6807\u5f3a\u5236\u6709\u754c\u8bef\u5dee\u7d2f\u79ef\uff1a1\uff09\u4ece\u771f\u5b9e\u5e27\u8fdb\u884c\u524d\u5411\u5c55\u5f00\uff1b2\uff09\u5e94\u7528\u53cd\u5411\u751f\u6210\u8fc7\u7a0b\u91cd\u5efa\u521d\u59cb\u72b6\u6001\uff1b3\uff09\u5728\u91cd\u5efa\u7684\u7ec8\u6b62\u72b6\u6001\u4e0a\u8ba1\u7b97\u6269\u6563\u635f\u5931\uff0c\u663e\u5f0f\u7ea6\u675f\u957f\u65f6\u57df\u8bef\u5dee\u4f20\u64ad\u3002\u8fd8\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\u548c\u6e10\u8fdb\u8bad\u7ec3\u8bfe\u7a0b\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLIVE\u5728\u957f\u65f6\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u8d85\u51fa\u8bad\u7ec3\u5c55\u5f00\u957f\u5ea6\u7684\u7a33\u5b9a\u3001\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "conclusion": "LIVE\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u57df\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u65e0\u9700\u6559\u5e08\u84b8\u998f\uff0c\u5728\u957f\u65f6\u57df\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.03143", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03143", "abs": "https://arxiv.org/abs/2602.03143", "authors": ["Baohao Liao", "Hanze Dong", "Xinxing Xu", "Christof Monz", "Jiang Bian"], "title": "Self-Hinting Language Models Enhance Reinforcement Learning", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $\u03c4$ conditioned on $(x,h)$. Crucially, the task reward $R(x,\u03c4)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.", "AI": {"tldr": "SAGE\u901a\u8fc7\u6ce8\u5165\u7279\u6743\u63d0\u793a\u6765\u589e\u52a0\u7ec4\u5185\u7ed3\u679c\u591a\u6837\u6027\uff0c\u89e3\u51b3GRPO\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u4f18\u52bf\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u65e0\u9700\u6d4b\u8bd5\u65f6\u63d0\u793a\u5373\u53ef\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "GRPO\u5728\u7a00\u758f\u7ec8\u7aef\u5956\u52b1\u4e0b\u7ecf\u5e38\u505c\u6ede\uff0c\u56e0\u4e3a\u7ec4\u5185rollout\u7ecf\u5e38\u83b7\u5f97\u76f8\u540c\u5956\u52b1\uff0c\u5bfc\u81f4\u76f8\u5bf9\u4f18\u52bf\u5d29\u6e83\u548c\u66f4\u65b0\u6d88\u5931\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u6765\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002", "method": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u65f6\u6ce8\u5165\u7279\u6743\u63d0\u793a\u6765\u91cd\u5851rollout\u5206\u5e03\uff0c\u6a21\u578b\u91c7\u6837\u7d27\u51d1\u63d0\u793a\uff08\u5982\u8ba1\u5212\u6216\u5206\u89e3\uff09\u540e\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u6301\u4efb\u52a1\u5956\u52b1\u4e0d\u53d8\uff0c\u4ec5\u589e\u52a0\u7ec4\u5185\u7ed3\u679c\u591a\u6837\u6027\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSAGE\u59cb\u7ec8\u4f18\u4e8eGRPO\uff0c\u5e73\u5747\u63d0\u5347\uff1aLlama-3.2-3B-Instruct +2.0\uff0cQwen2.5-7B-Instruct +1.2\uff0cQwen3-4B-Instruct +1.3\u3002", "conclusion": "SAGE\u901a\u8fc7\u81ea\u6211\u63d0\u793a\u4f5c\u4e3a\u81ea\u9002\u5e94\u8bfe\u7a0b\uff0c\u6709\u6548\u8ddf\u8e2a\u5b66\u4e60\u74f6\u9888\uff0c\u9632\u6b62GRPO\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u4f18\u52bf\u5d29\u6e83\uff0c\u65e0\u9700\u6d4b\u8bd5\u65f6\u7279\u6743\u4fe1\u606f\u5373\u53ef\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2602.03847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03847", "abs": "https://arxiv.org/abs/2602.03847", "authors": ["Shreyas Sachan", "Viktor Rudnev", "Mohamed Elgharib", "Christian Theobalt", "Vladislav Golyanik"], "title": "EventNeuS: 3D Mesh Reconstruction from a Single Event Camera", "comment": "13 pages, 10 figures, 3 tables; project page: https://4dqv.mpi-inf.mpg.de/EventNeuS/", "summary": "Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.", "AI": {"tldr": "EventNeuS\uff1a\u9996\u4e2a\u7ed3\u5408SDF\u548c\u5bc6\u5ea6\u573a\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u795e\u7ecf\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u5f69\u8272\u4e8b\u4ef6\u6d41\u8fdb\u884c3D\u91cd\u5efa\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8bb8\u591a\u573a\u666f\u4e2d\u6bd4RGB\u76f8\u673a\u66f4\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7684\u4e8b\u4ef6\u76f8\u673a3D\u91cd\u5efa\u65b9\u6cd5\u7cbe\u5ea6\u4e25\u91cd\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u51c6\u786e\u76843D\u8868\u793a\u5b66\u4e60\u65b9\u6cd5", "method": "\u7ed3\u54083D\u6709\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u548c\u5bc6\u5ea6\u573a\u5b66\u4e60\uff0c\u5f15\u5165\u4e8b\u4ef6\u76d1\u7763\u548c\u7403\u8c10\u7f16\u7801\u6765\u5904\u7406\u89c6\u89d2\u4f9d\u8d56\u6548\u5e94\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u5b66\u4e60", "result": "\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747Chamfer\u8ddd\u79bb\u964d\u4f4e34%\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e31%", "conclusion": "EventNeuS\u9996\u6b21\u6210\u529f\u5c06SDF\u548c\u5bc6\u5ea6\u573a\u5b66\u4e60\u4e0e\u4e8b\u4ef6\u76d1\u7763\u7ed3\u5408\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02917", "abs": "https://arxiv.org/abs/2602.02917", "authors": ["Yunsung Chung", "Keum San Chun", "Migyeong Gwak", "Han Feng", "Yingshuo Liu", "Chanho Lim", "Viswam Nathan", "Nassir Marrouche", "Sharanya Arcot Desai"], "title": "Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels", "comment": "ICASSP 2026", "summary": "Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u8870\u51cf\u7684\u6837\u672c\u6743\u91cd\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3PPG\u751f\u7269\u4fe1\u53f7\u4e0e\u4e34\u5e8a\u6807\u7b7e\u65f6\u95f4\u95f4\u9694\u5bfc\u81f4\u7684\u76d1\u7763\u4e0d\u53ef\u9760\u95ee\u9898\uff0c\u572810\u79cd\u751f\u7269\u6807\u5fd7\u7269\u4e0a\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u548cAI\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u57fa\u4e8ePPG\u7684\u5065\u5eb7\u76d1\u6d4b\uff0c\u4f46\u4e34\u5e8a\u6807\u7b7e\u7a00\u758f\u6027\u4f7f\u5f97\u8ddd\u79bb\u5b9e\u9a8c\u5ba4\u62bd\u8840\u65f6\u95f4\u8f83\u8fdc\u7684\u751f\u7269\u4fe1\u53f7\u76d1\u7763\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u89e3\u51b3\u65f6\u95f4\u95f4\u9694\u5bfc\u81f4\u7684\u6807\u7b7e\u53ef\u9760\u6027\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u5b66\u4e60\u751f\u7269\u6807\u5fd7\u7269\u7279\u5b9a\u7684\u6837\u672c\u6743\u91cd\u8870\u51cf\u51fd\u6570\uff0c\u6839\u636e\u751f\u7269\u4fe1\u53f7\u6bb5\u4e0e\u5176\u771f\u5b9e\u6807\u7b7e\u7684\u65f6\u95f4\u95f4\u9694\u8c03\u6574\u6837\u672c\u6743\u91cd\uff0c\u5e76\u5728\u635f\u5931\u51fd\u6570\u4e2d\u4f7f\u7528\u6b63\u5219\u5316\u9632\u6b62\u5e73\u51e1\u89e3\u3002", "result": "\u5728450\u540d\u53c2\u4e0e\u8005\u7684\u667a\u80fd\u624b\u8868PPG\u6570\u636e\u548c10\u79cd\u751f\u7269\u6807\u5fd7\u7269\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff1a\u5e73\u5747AUPRC\u4e3a0.715\uff0c\u4f18\u4e8e\u81ea\u76d1\u7763\u57fa\u7ebf\uff080.674\uff09\u548c\u968f\u673a\u68ee\u6797\uff080.626\uff09\u3002\u7ebf\u6027\u8870\u51cf\u51fd\u6570\u8868\u73b0\u6700\u7a33\u5065\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u8870\u51cf\u7387\u63d0\u4f9b\u4e86\u751f\u7269\u6807\u5fd7\u7269PPG\u8bc1\u636e\u968f\u65f6\u95f4\u5931\u6548\u901f\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\u89c6\u56fe\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u751f\u7269\u6807\u5fd7\u7269\u7684\u65f6\u95f4\u654f\u611f\u6027\u5dee\u5f02\u3002"}}
{"id": "2602.02930", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02930", "abs": "https://arxiv.org/abs/2602.02930", "authors": ["Yin Jin", "Tucker R. Stewart", "Deyi Zhou", "Chhavi Gupta", "Arjita Nema", "Scott C. Brakenridge", "Grant E. O'Keefe", "Juhua Hu"], "title": "Rare Event Early Detection: A Dataset of Sepsis Onset for Critically Ill Trauma Patients", "comment": null, "summary": "Sepsis is a major public health concern due to its high morbidity, mortality, and cost. Its clinical outcome can be substantially improved through early detection and timely intervention. By leveraging publicly available datasets, machine learning (ML) has driven advances in both research and clinical practice. However, existing public datasets consider ICU patients (Intensive Care Unit) as a uniform group and neglect the potential challenges presented by critically ill trauma patients in whom injury-related inflammation and organ dysfunction can overlap with the clinical features of sepsis. We propose that a targeted identification of post-traumatic sepsis is necessary in order to develop methods for early detection. Therefore, we introduce a publicly available standardized post-trauma sepsis onset dataset extracted, relabeled using standardized post-trauma clinical facts, and validated from MIMIC-III. Furthermore, we frame early detection of post-trauma sepsis onset according to clinical workflow in ICUs in a daily basis resulting in a new rare event detection problem. We then establish a general benchmark through comprehensive experiments, which shows the necessity of further advancements using this new dataset. The data code is available at https://github.com/ML4UWHealth/SepsisOnset_TraumaCohort.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9\u521b\u4f24\u60a3\u8005\u8113\u6bd2\u75c7\u53d1\u4f5c\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u65e9\u671f\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u521b\u4f24\u60a3\u8005\u8113\u6bd2\u75c7\u68c0\u6d4b\u7684\u7279\u6b8a\u6311\u6218\u3002", "motivation": "\u8113\u6bd2\u75c7\u5177\u6709\u9ad8\u53d1\u75c5\u7387\u3001\u6b7b\u4ea1\u7387\u548c\u533b\u7597\u6210\u672c\uff0c\u65e9\u671f\u68c0\u6d4b\u53ef\u663e\u8457\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u3002\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u5c06ICU\u60a3\u8005\u89c6\u4e3a\u540c\u8d28\u7fa4\u4f53\uff0c\u5ffd\u89c6\u4e86\u521b\u4f24\u5371\u91cd\u60a3\u8005\u7684\u7279\u6b8a\u6311\u6218\uff0c\u5176\u635f\u4f24\u76f8\u5173\u708e\u75c7\u548c\u5668\u5b98\u529f\u80fd\u969c\u788d\u53ef\u80fd\u4e0e\u8113\u6bd2\u75c7\u4e34\u5e8a\u7279\u5f81\u91cd\u53e0\u3002", "method": "\u4eceMIMIC-III\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u3001\u91cd\u65b0\u6807\u8bb0\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u521b\u4f24\u540e\u8113\u6bd2\u75c7\u53d1\u4f5c\u516c\u5f00\u6570\u636e\u96c6\u3002\u6839\u636eICU\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u521b\u4f24\u540e\u8113\u6bd2\u75c7\u65e9\u671f\u68c0\u6d4b\u6846\u67b6\u5316\u4e3a\u6bcf\u65e5\u57fa\u7840\u7684\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u5efa\u7acb\u901a\u7528\u57fa\u51c6\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9\u521b\u4f24\u60a3\u8005\u8113\u6bd2\u75c7\u53d1\u4f5c\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u65e9\u671f\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u6280\u672f\u8fdb\u6b65\u3002", "conclusion": "\u9488\u5bf9\u521b\u4f24\u60a3\u8005\u7684\u8113\u6bd2\u75c7\u68c0\u6d4b\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u8be5\u7814\u7a76\u63d0\u4f9b\u7684\u516c\u5f00\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5c06\u63a8\u52a8\u521b\u4f24\u540e\u8113\u6bd2\u75c7\u65e9\u671f\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.02958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02958", "abs": "https://arxiv.org/abs/2602.02958", "authors": ["Haocheng Xi", "Shuo Yang", "Yilong Zhao", "Muyang Li", "Han Cai", "Xingyang Li", "Yujun Lin", "Zhuoyang Zhang", "Jintao Zhang", "Xiuyu Li", "Zhiying Xu", "Jun Wu", "Chenfeng Xu", "Ion Stoica", "Song Han", "Kurt Keutzer"], "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization", "comment": "11 pages, 7 figures", "summary": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.", "AI": {"tldr": "QVG\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5e73\u6ed1\u548c\u6e10\u8fdb\u6b8b\u5dee\u91cf\u5316\u6280\u672f\uff0c\u5c06\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684KV\u7f13\u5b58\u5185\u5b58\u51cf\u5c11\u9ad8\u8fbe7\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2dKV\u7f13\u5b58\u5185\u5b58\u968f\u751f\u6210\u5386\u53f2\u589e\u957f\uff0c\u901a\u5e38\u8d85\u8fc730GB\uff0c\u9650\u5236\u4e86\u5728\u5e7f\u6cdb\u53ef\u7528\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u80fd\u529b\uff0c\u5e76\u635f\u5bb3\u4e86\u957f\u65f6\u7a0b\u751f\u6210\u7684\u4e00\u81f4\u6027\uff08\u8eab\u4efd\u3001\u5e03\u5c40\u3001\u8fd0\u52a8\uff09\u3002", "method": "\u63d0\u51faQuant VideoGen (QVG)\u6846\u67b6\uff1a1) \u8bed\u4e49\u611f\u77e5\u5e73\u6ed1\u5229\u7528\u89c6\u9891\u65f6\u7a7a\u5197\u4f59\u4ea7\u751f\u4f4e\u5e45\u503c\u3001\u91cf\u5316\u53cb\u597d\u7684\u6b8b\u5dee\uff1b2) \u6e10\u8fdb\u6b8b\u5dee\u91cf\u5316\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u591a\u9636\u6bb5\u65b9\u6848\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u5b9e\u73b0\u8d28\u91cf\u4e0e\u5185\u5b58\u7684\u5e73\u6ed1\u6743\u8861\u3002", "result": "\u5728LongCat Video\u3001HY WorldPlay\u548cSelf Forcing\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQVG\u5c06KV\u7f13\u5b58\u5185\u5b58\u51cf\u5c11\u9ad8\u8fbe7.0\u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u5f00\u9500\u5c0f\u4e8e4%\uff0c\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5efa\u7acb\u4e86\u8d28\u91cf\u4e0e\u5185\u5b58\u6548\u7387\u7684\u65b0\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "QVG\u901a\u8fc7\u9ad8\u6548\u7684KV\u7f13\u5b58\u91cf\u5316\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u4e0e\u5185\u5b58\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u5728\u5e7f\u6cdb\u786c\u4ef6\u4e0a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.03073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03073", "abs": "https://arxiv.org/abs/2602.03073", "authors": ["Rana Muhammad Shahroz Khan", "Zijie Liu", "Zhen Tan", "Charles Fleming", "Tianlong Chen"], "title": "TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT", "comment": null, "summary": "Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.", "AI": {"tldr": "TMS\u662f\u4e00\u79cd\u65e0\u9700\u5956\u52b1\u7684\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u5386\u53f2\u68c0\u67e5\u70b9\u521b\u5efa\u52a8\u6001\u8bfe\u7a0b\uff0c\u8fd1\u4f3cRL\u7684\u5728\u7ebf\u7b56\u7565\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u867d\u7136\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u9700\u8981\u590d\u6742\u7684\u5956\u52b1\u5de5\u7a0b\u3001\u4e0d\u7a33\u5b9a\u4e14\u91c7\u6837\u6210\u672c\u9ad8\uff1b\u800cSFT\u867d\u7136\u9ad8\u6548\u4f46\u5bb9\u6613\u56e0\u76d1\u7763\u4e0d\u5339\u914d\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002\u9700\u8981\u4e00\u79cd\u6298\u4e2d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u6df7\u5408\u76d1\u7763(TMS)\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u68c0\u67e5\u70b9\u521b\u5efa\u52a8\u6001\u8bfe\u7a0b\uff0c\u6700\u5c0f\u5316\u7b56\u7565-\u6807\u7b7e\u5206\u6b67(PLD)\uff0c\u907f\u514d\u6807\u51c6SFT\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "result": "\u5728\u63a8\u7406(MATH, GSM8K)\u548c\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTMS\u6709\u6548\u6539\u5584\u4e86\u51c6\u786e\u7387-\u4fdd\u6301\u80fd\u529b\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u548c\u8fed\u4ee3SFT\uff0c\u63a5\u8fd1RL\u6548\u679c\u4f46\u65e0\u9700\u5956\u52b1\u6a21\u578b\u6216\u9a8c\u8bc1\u5668\u3002", "conclusion": "TMS\u901a\u8fc7\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6709\u6548\u7f13\u89e3\u4e86SFT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edfSFT\u65b9\u6cd5\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.03293", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03293", "abs": "https://arxiv.org/abs/2602.03293", "authors": ["Pritam Kar", "Rahul Bordoloi", "Olaf Wolkenhauer", "Saptarshi Bej"], "title": "Anomaly Detection via Mean Shift Density Enhancement", "comment": null, "summary": "Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.", "AI": {"tldr": "\u63d0\u51faMSDE\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u5ea6\u589e\u5f3a\u8fc7\u7a0b\u4e2d\u7684\u51e0\u4f55\u4f4d\u79fb\u6765\u68c0\u6d4b\u5f02\u5e38\uff0c\u5728\u591a\u79cd\u5f02\u5e38\u7c7b\u578b\u548c\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u7a33\u5065", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u5728\u4e0d\u540c\u5f02\u5e38\u7c7b\u578b\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5", "method": "\u57fa\u4e8e\u52a0\u6743\u5747\u503c\u6f02\u79fb\u8fc7\u7a0b\uff0c\u4f7f\u7528UMAP\u6a21\u7cca\u90bb\u57df\u56fe\u8ba1\u7b97\u6837\u672c\u7279\u5b9a\u5bc6\u5ea6\u6743\u91cd\uff0c\u901a\u8fc7\u8fed\u4ee3\u5bc6\u5ea6\u589e\u5f3a\u8fc7\u7a0b\u4e2d\u7684\u7d2f\u79ef\u4f4d\u79fb\u5b9a\u4e49\u5f02\u5e38\u5206\u6570", "result": "\u5728ADBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd413\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0cMSDE\u5728AUC-ROC\u3001AUC-PR\u548cPrecision@n\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4e00\u81f4\u4e14\u7a33\u5065", "conclusion": "\u57fa\u4e8e\u4f4d\u79fb\u7684\u8bc4\u5206\u65b9\u6cd5\u4e3a\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u5f02\u5e38\u7c7b\u578b\u548c\u566a\u58f0\u73af\u5883"}}
{"id": "2602.03309", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03309", "abs": "https://arxiv.org/abs/2602.03309", "authors": ["Yuelin Hu", "Zhengxue Cheng", "Wei Liu", "Li Song"], "title": "Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models", "comment": "accepted by cscwd2026", "summary": "Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.\n  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.\n  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.", "AI": {"tldr": "EGSPO\u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684token\u7ea7\u68af\u5ea6\u8c03\u5236\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u5728\u6837\u672c\u7ea7\u522b\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u68af\u5ea6\u63a7\u5236\u3002EGSPO\u65e8\u5728\u901a\u8fc7token\u7ea7\u522b\u7684\u68af\u5ea6\u8c03\u5236\u6765\u66f4\u597d\u5730\u5e73\u8861\u63a2\u7d22\u4e0e\u77e5\u8bc6\u4fdd\u7559\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) SFT\u4e13\u5bb6\u5b66\u4e60\u5efa\u7acb\u57fa\u7840\u7b56\u7565\uff1b2) RL\u8f68\u8ff9\u751f\u6210\u5e76\u8ba1\u7b97\u6bcf\u4e2atoken\u7684\u9884\u6d4b\u71b5\uff1b3) EGSPO\u673a\u5236\u6839\u636e\u71b5\u503c\u8fdb\u884c\u68af\u5ea6\u5206\u914d\uff1a\u9ad8\u71b5token\u4f7f\u7528\u5b8c\u6574PPO\u66f4\u65b0\u9f13\u52b1\u63a2\u7d22\uff0c\u4f4e\u71b5token\u4f7f\u7528\u8870\u51cfPPO\u66f4\u65b0\u51cf\u5c11\u65b9\u5dee\u5e76\u4fdd\u7559\u77e5\u8bc6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff1aAIME\u63d0\u53473.8%\uff0cMATH\u63d0\u53472.9%\uff0c\u4ec5\u589e\u52a03.4%\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "EGSPO\u901a\u8fc7token\u7ea7\u522b\u7684\u71b5\u95e8\u63a7\u68af\u5ea6\u8c03\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u77e5\u8bc6\u4fdd\u7559\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002"}}
{"id": "2602.03452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03452", "abs": "https://arxiv.org/abs/2602.03452", "authors": ["Xin Sheng", "Jiaxin Li", "Yujuan Pang", "Ran Peng", "Yong Ma"], "title": "Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \\emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.", "AI": {"tldr": "\u63d0\u51fa\u6b63\u8d1f\u914d\u5bf9\u65b9\u6cd5\u6539\u8fdbRLVR\u4e2d\u7684\u63d0\u793a\u9009\u62e9\uff0c\u901a\u8fc7\u540c\u65f6\u91c7\u6837\u96be\u4f46\u53ef\u89e3\u7684\u6b63\u63d0\u793a\u548c\u6613\u4f46\u8106\u5f31\u7684\u8d1f\u63d0\u793a\uff0c\u914d\u5408\u52a0\u6743GRPO\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4e2d\u7684\u63d0\u793a\u9009\u62e9\u901a\u5e38\u4ec5\u57fa\u4e8e\u8bad\u7ec3\u51c6\u786e\u7387\u65b9\u5dee\uff0c\u5bfc\u81f4\u4f18\u5316\u65b9\u5411\u4e0d\u7a33\u5b9a\u4e14\u8fc1\u79fb\u80fd\u529b\u5f31\u3002\u9700\u8981\u4ece\u673a\u5236\u5c42\u9762\u91cd\u65b0\u5ba1\u89c6\u63d0\u793a\u9009\u62e9\uff0c\u786e\u4fdd\u5c0f\u6279\u91cf\u540c\u65f6\u63d0\u4f9b\u53ef\u9760\u7684\u6b63\u951a\u70b9\u548c\u660e\u786e\u7684\u8d1f\u5b66\u4e60\u4fe1\u53f7", "method": "\u63d0\u51fa\u6b63\u8d1f\u914d\u5bf9\u65b9\u6cd5\uff1a\u6bcf\u6b21\u66f4\u65b0\u91c7\u6837\u4e00\u4e2a\u96be\u4f46\u53ef\u89e3\u7684\u6b63\u63d0\u793aq+\uff08\u4f4e\u7ecf\u9a8c\u6210\u529f\u7387\uff09\u548c\u4e00\u4e2a\u6613\u4f46\u8106\u5f31\u7684\u8d1f\u63d0\u793aq-\uff08\u9ad8\u4f46\u4e0d\u5b8c\u7f8e\u7684\u6210\u529f\u7387\uff09\u3002\u5f15\u5165\u52a0\u6743GRPO\uff0c\u5728\u914d\u5bf9\u7ea7\u522b\u91cd\u65b0\u52a0\u6743\u4e8c\u5143\u7ed3\u679c\uff0c\u4f7f\u7528\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u6765\u653e\u5927q+\u4e0a\u7684\u7f55\u89c1\u6210\u529f\u4e3a\u5f3a\u6b63\u6307\u5bfc\uff0c\u540c\u65f6\u5c06q-\u4e0a\u7684\u7f55\u89c1\u5931\u8d25\u8f6c\u4e3a\u5f3a\u8d1f\u60e9\u7f5a", "result": "\u5728Qwen2.5-Math-7B\u4e0a\uff0c\u6bcf\u6b21\u66f4\u65b0\u4f7f\u7528\u5355\u4e2a\u914d\u5bf9\u5c0f\u6279\u91cf\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u65b9\u5dee\u9009\u62e9\u542f\u53d1\u5f0f\u7684GRPO\u57fa\u7ebf\uff1aAIME 2025 Pass@8\u4ece16.8\u63d0\u5347\u523022.2\uff0cAMC23 Pass@64\u4ece94.0\u63d0\u5347\u523097.0\uff0c\u540c\u65f6\u4e0e\u4ece1209\u4e2a\u8bad\u7ec3\u63d0\u793a\u6c60\u8bad\u7ec3\u7684\u5927\u89c4\u6a21RLVR\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u5728Qwen2.5-Math-7B-Instruct\u4e0a\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u589e\u76ca", "conclusion": "\u6b63\u8d1f\u914d\u5bf9\u65b9\u6cd5\u901a\u8fc7\u63d0\u4f9b\u53cc\u5411\u5b66\u4e60\u4fe1\u53f7\uff08\u6b63\u6307\u5bfc\u548c\u8d1f\u60e9\u7f5a\uff09\uff0c\u6539\u5584\u4e86\u6837\u672c\u6548\u7387\u800c\u4e0d\u6291\u5236\u63a2\u7d22\uff0c\u4e3aRLVR\u4e2d\u7684\u63d0\u793a\u9009\u62e9\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u673a\u5236\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.03461", "categories": ["cs.LG", "math.OC", "q-fin.CP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03461", "abs": "https://arxiv.org/abs/2602.03461", "authors": ["Philipp J. Schneider", "Daniel Kuhn"], "title": "Soft-Radial Projection for Constrained End-to-End Learning", "comment": null, "summary": "Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: gradient saturation. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the interior of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.", "AI": {"tldr": "\u63d0\u51faSoft-Radial Projection\u5c42\uff0c\u901a\u8fc7\u5f84\u5411\u6620\u5c04\u5c06\u70b9\u6620\u5c04\u5230\u53ef\u884c\u96c6\u5185\u90e8\uff0c\u89e3\u51b3\u4f20\u7edf\u6b63\u4ea4\u6295\u5f71\u5c42\u68af\u5ea6\u9971\u548c\u95ee\u9898\uff0c\u4fdd\u8bc1\u4e25\u683c\u53ef\u884c\u6027\u540c\u65f6\u4fdd\u6301\u96c5\u53ef\u6bd4\u77e9\u9635\u6ee1\u79e9\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\uff0c\u5c06\u786c\u7ea6\u675f\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5c06\u9884\u6d4b\u6295\u5f71\u5230\u7ea6\u675f\u8fb9\u754c\u7684\u65b9\u6cd5\u9762\u4e34\u68af\u5ea6\u9971\u548c\u74f6\u9888\uff1a\u6807\u51c6\u6b63\u4ea4\u6295\u5f71\u5c06\u5916\u90e8\u70b9\u574d\u7f29\u5230\u4f4e\u7ef4\u8868\u9762\uff0c\u5bfc\u81f4\u96c5\u53ef\u6bd4\u77e9\u9635\u79e9\u4e8f\uff0c\u4f7f\u5782\u76f4\u4e8e\u6d3b\u52a8\u7ea6\u675f\u7684\u68af\u5ea6\u4e3a\u96f6\uff0c\u963b\u788d\u4f18\u5316\u3002", "method": "\u5f15\u5165Soft-Radial Projection\u5c42\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u91cd\u53c2\u6570\u5316\u5c42\uff0c\u901a\u8fc7\u4ece\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5230\u53ef\u884c\u96c6\u5185\u90e8\u7684\u5f84\u5411\u6620\u5c04\u6765\u89c4\u907f\u68af\u5ea6\u9971\u548c\u95ee\u9898\u3002\u8be5\u6784\u9020\u4fdd\u8bc1\u4e25\u683c\u53ef\u884c\u6027\uff0c\u540c\u65f6\u51e0\u4e4e\u5904\u5904\u4fdd\u6301\u6ee1\u79e9\u96c5\u53ef\u6bd4\u77e9\u9635\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u8be5\u67b6\u6784\u4fdd\u7559\u4e86\u901a\u7528\u903c\u8fd1\u6027\u8d28\uff0c\u5b9e\u8bc1\u663e\u793a\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u4f18\u5316\u548c\u6295\u5f71\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u884c\u4e3a\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "Soft-Radial Projection\u5c42\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6295\u5f71\u5c42\u7684\u68af\u5ea6\u9971\u548c\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u4e25\u683c\u53ef\u884c\u6027\u7684\u540c\u65f6\u6539\u5584\u4e86\u4f18\u5316\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u7ea6\u675f\u96c6\u6210\u3002"}}
{"id": "2602.03515", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.03515", "abs": "https://arxiv.org/abs/2602.03515", "authors": ["Hyunji Jung", "Sungbin Shin", "Namhoon Lee"], "title": "Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation", "comment": "Preprint. Under review", "summary": "Asynchronous pipeline parallelism maximizes hardware utilization by eliminating the pipeline bubbles inherent in synchronous execution, offering a path toward efficient large-scale distributed training. However, this efficiency gain can be compromised by gradient staleness, where the immediate model updates with delayed gradients introduce noise into the optimization process. Crucially, we identify a critical, yet often overlooked, pathology: this delay scales linearly with pipeline depth, fundamentally undermining the very scalability that the method originally intends to provide. In this work, we investigate this inconsistency and bridge the gap by rectifying delayed gradients through basis rotation, restoring scalable asynchronous training while maintaining performance. Specifically, we observe that the deleterious effects of delayed gradients are exacerbated when the Hessian eigenbasis is misaligned with the standard coordinate basis. We demonstrate that this misalignment prevents coordinate-wise adaptive schemes, such as Adam, from effectively leveraging curvature-aware adaptivity. This failure leads to significant oscillations in the optimization trajectory and, consequently, slower convergence. We substantiate these findings through both rigorous theoretical analysis and empirical evaluation. To address this challenge, we propose the use of basis rotation, demonstrating that it effectively mitigates the alignment issue and significantly accelerates convergence in asynchronous settings. For example, our training of a 1B-parameter LLM with basis rotation achieves the same training loss in 76.8% fewer iterations compared to the best-performing asynchronous pipeline parallel training baseline.", "AI": {"tldr": "\u5f02\u6b65\u6d41\u6c34\u7ebf\u5e76\u884c\u901a\u8fc7\u57fa\u65cb\u8f6c\u4fee\u6b63\u5ef6\u8fdf\u68af\u5ea6\uff0c\u89e3\u51b3\u68af\u5ea6\u9648\u65e7\u6027\u95ee\u9898\uff0c\u5728\u8bad\u7ec310\u4ebf\u53c2\u6570LLM\u65f6\u6bd4\u6700\u4f73\u5f02\u6b65\u57fa\u7ebf\u51cf\u5c1176.8%\u8fed\u4ee3\u6b21\u6570", "motivation": "\u5f02\u6b65\u6d41\u6c34\u7ebf\u5e76\u884c\u867d\u7136\u80fd\u6d88\u9664\u540c\u6b65\u6267\u884c\u4e2d\u7684\u6d41\u6c34\u7ebf\u6c14\u6ce1\uff0c\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\uff0c\u4f46\u5b58\u5728\u68af\u5ea6\u9648\u65e7\u6027\u95ee\u9898\u3002\u5ef6\u8fdf\u68af\u5ea6\u4f1a\u5f15\u5165\u4f18\u5316\u566a\u58f0\uff0c\u4e14\u5ef6\u8fdf\u968f\u6d41\u6c34\u7ebf\u6df1\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u8fd9\u4ece\u6839\u672c\u4e0a\u7834\u574f\u4e86\u8be5\u65b9\u6cd5\u539f\u672c\u8981\u63d0\u4f9b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u65cb\u8f6c\u4fee\u6b63\u5ef6\u8fdf\u68af\u5ea6\u3002\u7814\u7a76\u53d1\u73b0\u5ef6\u8fdf\u68af\u5ea6\u7684\u6709\u5bb3\u6548\u5e94\u5728\u6d77\u68ee\u77e9\u9635\u7279\u5f81\u57fa\u4e0e\u6807\u51c6\u5750\u6807\u57fa\u4e0d\u5bf9\u9f50\u65f6\u52a0\u5267\uff0c\u8fd9\u963b\u788d\u4e86Adam\u7b49\u5750\u6807\u81ea\u9002\u5e94\u65b9\u6848\u6709\u6548\u5229\u7528\u66f2\u7387\u611f\u77e5\u80fd\u529b\u3002\u57fa\u65cb\u8f6c\u53ef\u4ee5\u7f13\u89e3\u8fd9\u79cd\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u57fa\u65cb\u8f6c\u663e\u8457\u52a0\u901f\u5f02\u6b65\u8bbe\u7f6e\u4e0b\u7684\u6536\u655b\u3002\u4f8b\u5982\uff0c\u5728\u8bad\u7ec310\u4ebf\u53c2\u6570LLM\u65f6\uff0c\u4f7f\u7528\u57fa\u65cb\u8f6c\u8fbe\u5230\u76f8\u540c\u8bad\u7ec3\u635f\u5931\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u6bd4\u6700\u4f73\u5f02\u6b65\u6d41\u6c34\u7ebf\u5e76\u884c\u57fa\u7ebf\u51cf\u5c1176.8%\u3002", "conclusion": "\u57fa\u65cb\u8f6c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5f02\u6b65\u6d41\u6c34\u7ebf\u5e76\u884c\u4e2d\u7684\u68af\u5ea6\u5ef6\u8fdf\u95ee\u9898\uff0c\u6062\u590d\u53ef\u6269\u5c55\u7684\u5f02\u6b65\u8bad\u7ec3\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5ef6\u8fdf\u68af\u5ea6\u968f\u6d41\u6c34\u7ebf\u6df1\u5ea6\u7ebf\u6027\u589e\u957f\u8fd9\u4e00\u6839\u672c\u6027\u53ef\u6269\u5c55\u6027\u969c\u788d\u3002"}}
{"id": "2602.03517", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03517", "abs": "https://arxiv.org/abs/2602.03517", "authors": ["Henri Arno", "Dennis Frauen", "Emil Javurek", "Thomas Demeester", "Stefan Feuerriegel"], "title": "Rank-Learner: Orthogonal Ranking of Treatment Effects", "comment": null, "summary": "Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.", "AI": {"tldr": "\u63d0\u51faRank-Learner\uff0c\u4e00\u79cd\u76f4\u63a5\u5b66\u4e60\u5904\u7406\u6548\u5e94\u6392\u540d\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u7cbe\u786e\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u6a21\u578b\u65e0\u5173\u6027\u3002", "motivation": "\u8bb8\u591a\u51b3\u7b56\u95ee\u9898\u9700\u8981\u6309\u5904\u7406\u6548\u5e94\u6392\u5e8f\u4e2a\u4f53\uff08\u5982\u4f18\u5148\u533b\u7597\u5e72\u9884\u6216\u5e7f\u544a\u6295\u653e\uff09\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7cbe\u786e\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\uff0c\u800c\u76f4\u63a5\u5b66\u4e60\u6392\u5e8f\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faRank-Learner\u4e24\u9636\u6bb5\u5b66\u4e60\u5668\uff1a1\uff09\u4f18\u5316\u6210\u5bf9\u5b66\u4e60\u76ee\u6807\u76f4\u63a5\u6062\u590d\u5904\u7406\u6548\u5e94\u6392\u5e8f\uff0c\u65e0\u9700\u663e\u5f0fCATE\u4f30\u8ba1\uff1b2\uff09\u5177\u6709Neyman\u6b63\u4ea4\u6027\uff0c\u5bf9\u5e72\u6270\u51fd\u6570\u4f30\u8ba1\u8bef\u5dee\u9c81\u68d2\uff1b3\uff09\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u4e0e\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cRank-Learner\u6301\u7eed\u4f18\u4e8e\u6807\u51c6CATE\u4f30\u8ba1\u5668\u548c\u975e\u6b63\u4ea4\u6392\u5e8f\u65b9\u6cd5\uff0c\u4e3a\u6309\u5904\u7406\u6548\u5e94\u6392\u5e8f\u4e2a\u4f53\u63d0\u4f9b\u4e86\u65b0\u7684\u6b63\u4ea4\u4e24\u9636\u6bb5\u5b66\u4e60\u5668\u3002", "conclusion": "Rank-Learner\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u76f4\u63a5\u5b66\u4e60\u5904\u7406\u6548\u5e94\u6392\u5e8f\u7684\u6709\u6548\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u7cbe\u786e\u4f30\u8ba1\u4e0d\u5fc5\u8981\u7684\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.03641", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03641", "abs": "https://arxiv.org/abs/2602.03641", "authors": ["Milosh Devic", "Jordan Gierschendorf", "David Garson"], "title": "CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets", "comment": null, "summary": "Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.", "AI": {"tldr": "CTTVAE+TBS\uff1a\u4e00\u79cd\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u8868\u683c\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7c7b\u611f\u77e5\u4e09\u5143\u7ec4\u8fb9\u754c\u635f\u5931\u548c\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u5c11\u6570\u7c7b\u6837\u672c\u7684\u751f\u6210\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5728\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u8868\u683c\u6570\u636e\u751f\u6210\u4e2d\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u8981\u4e48\u5ffd\u7565\u5c11\u6570\u7c7b\uff0c\u8981\u4e48\u751f\u6210\u7684\u6837\u672c\u5bf9\u4e0b\u6e38\u5b66\u4e60\u4efb\u52a1\u6ca1\u6709\u5e2e\u52a9\u3002\u5728\u533b\u7597\u3001\u6b3a\u8bc8\u68c0\u6d4b\u7b49\u5173\u952e\u9886\u57df\uff0c\u5c11\u6570\u7c7b\u4e8b\u4ef6\u867d\u7136\u7f55\u89c1\u4f46\u5f71\u54cd\u91cd\u5927\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u751f\u6210\u6709\u4ee3\u8868\u6027\u7684\u5c11\u6570\u7c7b\u6837\u672c\u3002", "method": "\u63d0\u51faCTTVAE+TBS\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u8868\u683c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CTTVAE\uff09\uff1b2\uff09\u7c7b\u611f\u77e5\u4e09\u5143\u7ec4\u8fb9\u754c\u635f\u5931\uff0c\u589e\u5f3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5206\u79bb\u6027\uff1b3\uff09\u8bad\u7ec3\u65f6\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff08TBS\uff09\uff0c\u589e\u52a0\u5bf9\u5c11\u6570\u7c7b\u6837\u672c\u7684\u66dd\u5149\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCTTVAE+TBS\u5728\u5c11\u6570\u7c7b\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u8d85\u8fc7\u4e86\u5728\u539f\u59cb\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u4e3a\u63d2\u503c\u91c7\u6837\u65b9\u6cd5\u548c\u6df1\u5ea6\u751f\u6210\u65b9\u6cd5\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\u3002", "conclusion": "CTTVAE+TBS\u901a\u8fc7\u663e\u5f0f\u4f18\u5148\u8003\u8651\u5c11\u6570\u7c7b\u7684\u4e0b\u6e38\u6027\u80fd\uff0c\u4e3a\u6761\u4ef6\u8868\u683c\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u7597\u3001\u6b3a\u8bc8\u68c0\u6d4b\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u7b49\u5173\u952e\u9886\u57df\uff0c\u8fd9\u4e9b\u9886\u57df\u4e2d\u5c11\u6570\u7c7b\u6027\u80fd\u7684\u5fae\u5c0f\u63d0\u5347\u90fd\u53ef\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.03678", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03678", "abs": "https://arxiv.org/abs/2602.03678", "authors": ["Simon Dietz", "Kai Klede", "An Nguyen", "Bjoern M Eskofier"], "title": "ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling", "comment": "26 pages with 16 figures", "summary": "Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.", "AI": {"tldr": "ContraLog\u662f\u4e00\u79cd\u65e0\u9700\u89e3\u6790\u5668\u7684\u81ea\u76d1\u7763\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8fde\u7eed\u6d88\u606f\u5d4c\u5165\u800c\u975e\u79bb\u6563\u6a21\u677fID\u6765\u68c0\u6d4b\u5f02\u5e38\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u65e5\u5fd7\u89e3\u6790\u5668\uff0c\u5c06\u6d88\u606f\u538b\u7f29\u4e3a\u79bb\u6563\u6a21\u677f\uff0c\u4e22\u5f03\u4e86\u53d8\u91cf\u503c\u548c\u8bed\u4e49\u5185\u5bb9\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u6d4b\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u7559\u5b8c\u6574\u8bed\u4e49\u4fe1\u606f\u7684\u89e3\u6790\u5668\u65e0\u5173\u65b9\u6cd5\u3002", "method": "ContraLog\u7ed3\u5408\u6d88\u606f\u7f16\u7801\u5668\u548c\u5e8f\u5217\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u9884\u6d4b\u88ab\u63a9\u7801\u7684\u6d88\u606f\u5d4c\u5165\u800c\u975e\u6a21\u677fID\uff0c\u4ece\u800c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u65e5\u5fd7\u6d88\u606f\u3002", "result": "\u5728HDFS\u3001BGL\u548cThunderbird\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cContraLog\u5728\u590d\u6742\u591a\u6837\u7684\u65e5\u5fd7\u6d88\u606f\u4e0a\u6709\u6548\u3002\u751f\u6210\u7684\u6d88\u606f\u5d4c\u5165\u5305\u542b\u6709\u610f\u4e49\u4fe1\u606f\uff0c\u5373\u4f7f\u6ca1\u6709\u5e8f\u5217\u4e0a\u4e0b\u6587\u4e5f\u80fd\u9884\u6d4b\u5f02\u5e38\u3002", "conclusion": "\u5d4c\u5165\u7ea7\u9884\u6d4b\u662f\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u6269\u5c55\u5230\u5176\u4ed6\u4e8b\u4ef6\u5e8f\u5217\u7684\u6f5c\u529b\uff0c\u4e3a\u65e0\u9700\u89e3\u6790\u5668\u7684\u81ea\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.03778", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03778", "abs": "https://arxiv.org/abs/2602.03778", "authors": ["Aneri Muni", "Vincent Taboga", "Esther Derman", "Pierre-Luc Bacon", "Erick Delage"], "title": "Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity", "comment": null, "summary": "Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9759\u6001CVaR\u76ee\u6807\u51fd\u6570\u516c\u5f0f\uff0c\u901a\u8fc7\u72b6\u6001\u589e\u5f3a\u5b9e\u73b0\u5177\u6709\u7a20\u5bc6\u5956\u52b1\u548c\u6536\u7f29\u6027\u8d28\u7684Bellman\u7b97\u5b50\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u98ce\u9669\u89c4\u907f\u7b97\u6cd5\u3002", "motivation": "\u9759\u6001CVaR\uff08\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff09\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7528\u4e8e\u9632\u6b62\u7f55\u89c1\u4f46\u707e\u96be\u6027\u4e8b\u4ef6\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u72b6\u6001\u589e\u5f3a\uff0c\u5bfc\u81f4\u7a00\u758f\u5956\u52b1\u548c\u9000\u5316\u56fa\u5b9a\u70b9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u589e\u5f3a\u7684\u65b0CVaR\u76ee\u6807\u51fd\u6570\u516c\u5f0f\uff0c\u8bbe\u8ba1\u5177\u6709\u7a20\u5bc6\u6bcf\u6b65\u5956\u52b1\u548c\u6536\u7f29\u6027\u8d28\u7684Bellman\u7b97\u5b50\uff0c\u5f00\u53d1\u98ce\u9669\u89c4\u907f\u503c\u8fed\u4ee3\u548c\u514d\u6a21\u578bQ\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u7528\u79bb\u6563\u5316\u589e\u5f3a\u72b6\u6001\u3002", "result": "\u65b0\u65b9\u6cd5\u6210\u529f\u5b66\u4e60CVaR\u654f\u611f\u7b56\u7565\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u6027\u80fd-\u5b89\u5168\u6743\u8861\uff0c\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u548c\u79bb\u6563\u5316\u5bfc\u81f4\u7684\u8fd1\u4f3c\u8bef\u5dee\u754c\u9650\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0CVaR\u516c\u5f0f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u98ce\u9669\u89c4\u907f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\u6846\u67b6\u3002"}}
