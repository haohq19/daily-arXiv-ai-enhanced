{"id": "2602.03883", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03883", "abs": "https://arxiv.org/abs/2602.03883", "authors": ["Akshansh Mishra", "Rakesh Morisetty"], "title": "Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing", "comment": "6 figures", "summary": "Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u6750\u5236\u9020\u4e2d\u5b54\u9699\u68c0\u6d4b\u4e0e\u4e34\u754c\u6027\u8bc4\u4f30\uff0c\u901a\u8fc7SHAP\u5206\u6790\u53d1\u73b0\u8868\u9762\u8ddd\u79bb\u662f\u9884\u6d4b\u5b54\u9699\u4e34\u754c\u6027\u7684\u6700\u4e3b\u8981\u56e0\u7d20", "motivation": "\u589e\u6750\u5236\u9020\u4e2d\u7684\u5185\u90e8\u5b54\u9699\u662f\u5f71\u54cd\u7ed3\u6784\u6027\u80fd\u7684\u5173\u952e\u7f3a\u9677\uff0c\u73b0\u6709\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5de5\u7a0b\u5e08\u65e0\u6cd5\u7406\u89e3\u4e34\u754c\u6027\u9884\u6d4b\u7684\u7269\u7406\u57fa\u7840", "method": "\u4f7f\u7528\u7070\u5ea6\u5207\u7247\u91cd\u5efa\u4e09\u7ef4\u4f53\u79ef\u6570\u636e\uff0c\u901a\u8fc7\u5f3a\u5ea6\u9608\u503c\u548c\u8fde\u901a\u5206\u91cf\u5206\u6790\u8bc6\u522b500\u4e2a\u5b54\u9699\uff1b\u63d0\u53d6\u51e0\u4f55\u7279\u5f81\uff08\u5c3a\u5bf8\u3001\u7eb5\u6a2a\u6bd4\u3001\u8303\u56f4\u3001\u7a7a\u95f4\u4f4d\u7f6e\uff09\uff1b\u6784\u5efa\u5b54\u9699\u4ea4\u4e92\u7f51\u7edc\uff1b\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e34\u754c\u6027\uff0c\u5e76\u901a\u8fc7SHAP\u5206\u6790\u91cf\u5316\u7279\u5f81\u8d21\u732e", "result": "\u5f52\u4e00\u5316\u8868\u9762\u8ddd\u79bb\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u8d21\u732e\u6bd4\u5176\u4ed6\u6240\u6709\u63cf\u8ff0\u7b26\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\uff1b\u5b54\u9699\u5c3a\u5bf8\u5f71\u54cd\u6700\u5c0f\uff0c\u51e0\u4f55\u53c2\u6570\u5f71\u54cd\u53ef\u5ffd\u7565\uff1b\u8868\u9762\u63a5\u8fd1\u5ea6\u4e0e\u4e34\u754c\u6027\u5448\u5f3a\u53cd\u6bd4\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u8fb9\u754c\u9a71\u52a8\u7684\u5931\u6548\u673a\u5236", "conclusion": "\u8be5\u53ef\u89e3\u91ca\u6846\u67b6\u5b9e\u73b0\u4e86\u900f\u660e\u7684\u7f3a\u9677\u8bc4\u4f30\uff0c\u4e3a\u589e\u6750\u5236\u9020\u7684\u5de5\u827a\u4f18\u5316\u548c\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3"}}
{"id": "2602.03924", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.03924", "abs": "https://arxiv.org/abs/2602.03924", "authors": ["Michael Aich", "Andreas F\u00fcrst", "Florian Sestak", "Carlos Ruiz-Gonzalez", "Niklas Boers", "Johannes Brandstetter"], "title": "WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling", "comment": null, "summary": "Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.", "AI": {"tldr": "WIND\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6c14\u8c61\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5904\u7406\u591a\u79cd\u5929\u6c14\u6c14\u5019\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u9006\u95ee\u9898\u6c42\u89e3\u5b9e\u73b0\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u5929\u6c14\u6c14\u5019\u5efa\u6a21\u4e2d\u9ad8\u5ea6\u788e\u7247\u5316\uff0c\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e13\u95e8\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u9700\u8981\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u6765\u66ff\u4ee3\u8fd9\u4e9b\u4e13\u95e8\u5316\u57fa\u7ebf\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u89c6\u9891\u91cd\u5efa\u76ee\u6807\u9884\u8bad\u7ec3\u65e0\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5b66\u4e60\u5927\u6c14\u7a33\u5065\u5148\u9a8c\u3002\u63a8\u7406\u65f6\u5c06\u9886\u57df\u7279\u5b9a\u95ee\u9898\u6846\u67b6\u5316\u4e3a\u9006\u95ee\u9898\uff0c\u901a\u8fc7\u540e\u9a8c\u91c7\u6837\u6c42\u89e3\u3002", "result": "WIND\u80fd\u591f\u5904\u7406\u6982\u7387\u9884\u6d4b\u3001\u65f6\u7a7a\u964d\u5c3a\u5ea6\u3001\u7a00\u758f\u91cd\u5efa\u3001\u5b88\u6052\u5b9a\u5f8b\u6267\u884c\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u80fd\u751f\u6210\u5168\u7403\u53d8\u6696\u60c5\u666f\u4e0b\u7684\u6781\u7aef\u5929\u6c14\u53cd\u4e8b\u5b9e\u60c5\u666f\u3002", "conclusion": "\u7ed3\u5408\u751f\u6210\u89c6\u9891\u5efa\u6a21\u548c\u9006\u95ee\u9898\u6c42\u89e3\uff0cWIND\u4e3aAI\u5927\u6c14\u5efa\u6a21\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u3002"}}
{"id": "2602.04206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04206", "abs": "https://arxiv.org/abs/2602.04206", "authors": ["Hsien-Jyh Liao"], "title": "Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry", "comment": "Submitted to ICAIL 2026. Under review", "summary": "Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.", "AI": {"tldr": "Soft-FSM\uff1a\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u901a\u8fc7\u5916\u90e8\u786e\u5b9a\u6027\u72b6\u6001\u63a7\u5236\u5668\u5f3a\u5236\u5b9e\u73b0\u5355\u8c03\u8fdb\u5c55\uff0c\u89e3\u51b3LLM\u5728\u7a0b\u5e8f\u6027\u7ea6\u675f\u4efb\u52a1\u4e2d\u7684\u505c\u6ede\u95ee\u9898\uff0c\u5728\u53f0\u6e7e\u5211\u4e8b\u6740\u4eba\u6848\u4ef6\u4ea4\u53c9\u8be2\u95ee\u4e2d\u8fbe\u523097%\u4ee5\u4e0a\u5b8c\u6574\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u6d41\u7545\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u660e\u786e\u7684\u7a0b\u5e8f\u6027\u7ea6\u675f\u4e0b\u53ef\u9760\u5b8c\u6210\u957f\u671f\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\u3002\u5728\u6cd5\u5f8b\u4ea4\u53c9\u8be2\u95ee\u4e2d\uff0c\u7eaf\u6982\u7387\u751f\u6210\u5f80\u5f80\u4fdd\u6301\u884c\u4e3a\u8fde\u8d2f\u6027\u4f46\u65e0\u6cd5\u786e\u4fdd\u7a0b\u5e8f\u63a8\u8fdb\uff0c\u8fd9\u79cd\u5931\u8d25\u88ab\u63cf\u8ff0\u4e3a\u7a0b\u5e8f\u6027\u505c\u6ede\u3002", "method": "\u63d0\u51faSoft-FSM\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u901a\u8fc7\u5916\u90e8\u786e\u5b9a\u6027\u72b6\u6001\u63a7\u5236\u5668\u5f3a\u5236\u5b9e\u73b0\u5bf9\u7d2f\u79ef\u5173\u952e\u4fe1\u606f\u5355\u5143(KIUs)\u7684\u5355\u8c03\u8fdb\u5c55\u63a7\u5236\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u53f0\u6e7e\u5211\u4e8b\u6740\u4eba\u6848\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u57fa\u7ebf\u65b9\u6cd5\u5b8c\u6574\u6027\u4f4e\u4e8e40%\uff0c\u800cSoft-FSM\u59cb\u7ec8\u8fbe\u523097%\u4ee5\u4e0a\u5b8c\u6574\u6027\u4e14\u5197\u4f59\u5ea6\u63a5\u8fd1\u96f6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u5728\u67d0\u4e9b\u9886\u57df\uff0c\u53ef\u9760\u7684\u5b8c\u6210\u4efb\u52a1\u4e0d\u80fd\u4ec5\u4f9d\u8d56LLM\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u800c\u9700\u8981\u901a\u8fc7\u660e\u786e\u4e14\u53ef\u9a8c\u8bc1\u7684\u5916\u90e8\u72b6\u6001\u63a7\u5236\u6765\u5f3a\u5236\u4fdd\u8bc1\uff0c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4e3a\u89e3\u51b3\u7a0b\u5e8f\u6027\u7ea6\u675f\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.04813", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04813", "abs": "https://arxiv.org/abs/2602.04813", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents", "comment": null, "summary": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e03\u7ef4\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u5206\u6790\u533b\u7597\u9886\u57dfLLM\u667a\u80fd\u4f53\u7684\u80fd\u529b\u73b0\u72b6\uff0c\u901a\u8fc7\u5bf949\u9879\u7814\u7a76\u8fdb\u884c\u91cf\u5316\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u533b\u7597AI\u667a\u80fd\u4f53\u5728\u77e5\u8bc6\u6574\u5408\u3001\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7b49\u65b9\u9762\u8f83\u4e3a\u6210\u719f\uff0c\u4f46\u5728\u4e8b\u4ef6\u89e6\u53d1\u6fc0\u6d3b\u3001\u6f02\u79fb\u68c0\u6d4b\u4e0e\u7f13\u89e3\u3001\u6cbb\u7597\u89c4\u5212\u7b49\u884c\u52a8\u5bfc\u5411\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u9886\u57dfLLM\u667a\u80fd\u4f53\u7684\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u73b0\u6709\u6587\u732e\u591a\u4e3a\u5bbd\u6cdb\u7efc\u8ff0\u6216\u9488\u5bf9\u5355\u4e00\u80fd\u529b\u7684\u6df1\u5165\u63a2\u8ba8\uff0c\u5bfc\u81f4\u533b\u7597\u5de5\u4f5c\u8005\u96be\u4ee5\u5f62\u6210\u5171\u8bc6\u6027\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u533b\u7597AI\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e03\u7ef4\u5206\u7c7b\u6cd5\uff08\u8ba4\u77e5\u80fd\u529b\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u4ea4\u4e92\u6a21\u5f0f\u3001\u9002\u5e94\u4e0e\u5b66\u4e60\u3001\u5b89\u5168\u4e0e\u4f26\u7406\u3001\u6846\u67b6\u7c7b\u578b\u3001\u6838\u5fc3\u4efb\u52a1\u4e0e\u5b50\u4efb\u52a1\uff09\uff0c\u5305\u542b29\u4e2a\u64cd\u4f5c\u6027\u5b50\u7ef4\u5ea6\u3002\u91c7\u7528\u660e\u786e\u7684\u7eb3\u5165\u6392\u9664\u6807\u51c6\u548c\u4e09\u7ea7\u6807\u7b7e\uff08\u5b8c\u5168\u5b9e\u73b0\u3001\u90e8\u5206\u5b9e\u73b0\u3001\u672a\u5b9e\u73b0\uff09\uff0c\u5bf949\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6620\u5c04\u548c\u91cf\u5316\u5206\u6790\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u660e\u663e\u7684\u4e0d\u5bf9\u79f0\u6027\uff1a\u77e5\u8bc6\u7ba1\u7406\u4e2d\u7684\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u666e\u904d\u5b9e\u73b0\uff08\u7ea676%\u5b8c\u5168\u5b9e\u73b0\uff09\uff0c\u800c\u4ea4\u4e92\u6a21\u5f0f\u4e2d\u7684\u4e8b\u4ef6\u89e6\u53d1\u6fc0\u6d3b\u57fa\u672c\u7f3a\u5931\uff08\u7ea692%\u672a\u5b9e\u73b0\uff09\uff0c\u9002\u5e94\u4e0e\u5b66\u4e60\u4e2d\u7684\u6f02\u79fb\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7f55\u89c1\uff08\u7ea698%\u672a\u5b9e\u73b0\uff09\u3002\u67b6\u6784\u4e0a\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u5360\u4e3b\u5bfc\uff08\u7ea682%\u5b8c\u5168\u5b9e\u73b0\uff09\u3002\u6838\u5fc3\u4efb\u52a1\u4e2d\u4fe1\u606f\u4e2d\u5fc3\u80fd\u529b\u9886\u5148\uff0c\u4f46\u6cbb\u7597\u89c4\u5212\u7b49\u884c\u52a8\u5bfc\u5411\u9886\u57df\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\uff08\u7ea659%\u672a\u5b9e\u73b0\uff09\u3002", "conclusion": "\u533b\u7597LLM\u667a\u80fd\u4f53\u5728\u77e5\u8bc6\u6574\u5408\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u65b9\u9762\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u4e8b\u4ef6\u9a71\u52a8\u4ea4\u4e92\u3001\u6301\u7eed\u9002\u5e94\u80fd\u529b\u548c\u884c\u52a8\u5bfc\u5411\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u533b\u7597AI\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.04289", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04289", "abs": "https://arxiv.org/abs/2602.04289", "authors": ["Lin Zheng", "Xinyu Li", "Qian Liu", "Xiachong Feng", "Lingpeng Kong"], "title": "Proxy Compression for Language Modeling", "comment": null, "summary": "Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.", "AI": {"tldr": "\u4ee3\u7406\u538b\u7f29\u8bad\u7ec3\u65b9\u6848\uff1a\u5728\u8bad\u7ec3\u65f6\u540c\u65f6\u4f7f\u7528\u539f\u59cb\u5b57\u8282\u548c\u538b\u7f29\u89c6\u56fe\uff0c\u63a8\u7406\u65f6\u4ec5\u7528\u539f\u59cb\u5b57\u8282\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u5e76\u8d85\u8d8a\u7eaf\u5b57\u8282\u7ea7\u57fa\u7ebf", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u56fa\u5b9a\u5206\u8bcd\u5668\uff0c\u5c06\u6a21\u578b\u4e0e\u5916\u90e8\u538b\u7f29\u5668\u8026\u5408\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u538b\u7f29\u8f93\u5165\u6548\u7387\u4f18\u52bf\uff0c\u53c8\u80fd\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u7aef\u5230\u7aef\u539f\u59cb\u5b57\u8282\u63a5\u53e3\u7684\u8bad\u7ec3\u65b9\u6848", "method": "\u63d0\u51fa\u4ee3\u7406\u538b\u7f29\u8bad\u7ec3\u65b9\u6848\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u540c\u65f6\u5728\u539f\u59cb\u5b57\u8282\u5e8f\u5217\u548c\u5916\u90e8\u538b\u7f29\u5668\u751f\u6210\u7684\u538b\u7f29\u89c6\u56fe\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u5185\u90e8\u5bf9\u9f50\u538b\u7f29\u5e8f\u5217\u548c\u539f\u59cb\u5b57\u8282", "result": "\u5728\u4ee3\u7801\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\u4e2d\uff0c\u4ee3\u7406\u538b\u7f29\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u4f18\u4e8e\u7eaf\u5b57\u8282\u7ea7\u57fa\u7ebf\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u6536\u76ca\u66f4\u660e\u663e\uff0c\u4ee3\u7406\u8bad\u7ec3\u6a21\u578b\u6700\u7ec8\u5339\u914d\u6216\u8d85\u8d8a\u5206\u8bcd\u5668\u65b9\u6cd5", "conclusion": "\u4ee3\u7406\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5728\u63a8\u7406\u65f6\u4ec5\u4f7f\u7528\u539f\u59cb\u5b57\u8282\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b57\u8282\u7ea7\u5efa\u6a21\u7684\u56fa\u6709\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6548\u7387\u4e0e\u63a8\u7406\u7075\u6d3b\u6027\u7684\u5e73\u8861"}}
{"id": "2602.04094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04094", "abs": "https://arxiv.org/abs/2602.04094", "authors": ["Junbo Zou", "Ziheng Huang", "Shengjie Zhang", "Liwen Zhang", "Weining Shen"], "title": "VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding", "comment": null, "summary": "Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.", "AI": {"tldr": "VideoBrain\uff1a\u4e00\u79cd\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u91c7\u6837\u7684\u957f\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u5728\u51cf\u5c1130-40%\u5e27\u6570\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd3.5-9.0%", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u9762\u4e34\u8ba1\u7b97\u7ea6\u675f\u4e0e\u4fe1\u606f\u5206\u5e03\u4e4b\u95f4\u7684\u56fa\u6709\u77db\u76fe\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5747\u5300\u91c7\u6837\uff08\u53ef\u80fd\u4e22\u5931\u4fe1\u606f\uff09\uff0c\u8981\u4e48\u5355\u6b21\u9009\u62e9\u5173\u952e\u5e27\uff08\u65e0\u6cd5\u7ea0\u6b63\u9519\u8bef\u9009\u62e9\uff09\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u3002", "method": "\u63d0\u51faVideoBrain\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u667a\u80fd\u4f53\uff1a\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u68c0\u7d22\u667a\u80fd\u4f53\uff08\u8de8\u89c6\u9891\u68c0\u7d22\u8bed\u4e49\u4fe1\u606f\uff09\u548c\u5747\u5300\u91c7\u6837\u667a\u80fd\u4f53\uff08\u5728\u533a\u95f4\u5185\u5bc6\u96c6\u91c7\u6837\uff09\u3002VLM\u76f4\u63a5\u611f\u77e5\u5e27\u5e76\u63a8\u7406\u4fe1\u606f\u5145\u5206\u6027\u3002\u5f15\u5165\u884c\u4e3a\u611f\u77e5\u5956\u52b1\u51fd\u6570\u548c\u6570\u636e\u5206\u7c7b\u6d41\u7a0b\uff0c\u9632\u6b62\u6a21\u578b\u6ee5\u7528\u667a\u80fd\u4f53\u3002", "result": "\u5728\u56db\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u53473.5%\u52309.0%\uff0c\u540c\u65f6\u51cf\u5c1130-40%\u7684\u5e27\u6570\u4f7f\u7528\u3002\u5728\u77ed\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VideoBrain\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.04182", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04182", "abs": "https://arxiv.org/abs/2602.04182", "authors": ["Weidong Hao"], "title": "HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating", "comment": null, "summary": "Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.", "AI": {"tldr": "HoloEv-Net\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u4e8b\u4ef6\u884c\u4e3a\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u51d1\u5168\u606f\u65f6\u7a7a\u8868\u793a\u548c\u5168\u5c40\u9891\u8c31\u95e8\u63a7\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u5197\u4f59\u3001\u7ed3\u6784\u5197\u4f59\u548c\u9891\u8c31\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u884c\u4e3a\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1) \u5bc6\u96c6\u4f53\u7d20\u8868\u793a\u7684\u8ba1\u7b97\u5197\u4f59\uff0c(2) \u591a\u5206\u652f\u67b6\u6784\u7684\u7ed3\u6784\u5197\u4f59\uff0c(3) \u9891\u8c31\u4fe1\u606f\u5728\u6355\u6349\u5168\u5c40\u8fd0\u52a8\u6a21\u5f0f\u4e2d\u7684\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51faHoloEv-Net\u6846\u67b6\uff1a1) \u7d27\u51d1\u5168\u606f\u65f6\u7a7a\u8868\u793a(CHSR)\uff0c\u5c06\u6c34\u5e73\u7a7a\u95f4\u7ebf\u7d22\u9690\u5f0f\u5d4c\u5165\u5230\u65f6\u95f4-\u9ad8\u5ea6\u89c6\u56fe\u4e2d\uff0c\u57282D\u8868\u793a\u4e2d\u4fdd\u75593D\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff1b2) \u5168\u5c40\u9891\u8c31\u95e8\u63a7(GSG)\u6a21\u5757\uff0c\u5229\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5728\u9891\u57df\u8fdb\u884c\u5168\u5c40\u4ee4\u724c\u6df7\u5408\u3002", "result": "\u5728THU-EACT-50-CHL\u3001HARDVS\u548cDailyDVS-200\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f9710.29%\u30011.71%\u548c6.25%\u7684\u6027\u80fd\u63d0\u5347\uff1b\u8f7b\u91cf\u7ea7\u53d8\u4f53HoloEv-Net-Small\u76f8\u6bd4\u91cd\u578b\u57fa\u7ebf\u51cf\u5c115.4\u500d\u53c2\u6570\u3001300\u500dFLOPs\u548c2.4\u500d\u5ef6\u8fdf\u3002", "conclusion": "HoloEv-Net\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5197\u4f59\u95ee\u9898\u5e76\u5145\u5206\u5229\u7528\u9891\u8c31\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u4e8b\u4ef6\u884c\u4e3a\u8bc6\u522b\uff0c\u7279\u522b\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u5e94\u7528\u3002"}}
{"id": "2602.04184", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04184", "abs": "https://arxiv.org/abs/2602.04184", "authors": ["Angel Martinez-Sanchez", "Parthib Roy", "Ross Greer"], "title": "Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models", "comment": null, "summary": "Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a \"good\" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning", "AI": {"tldr": "\u7814\u7a76\u5c06OpenEMMA\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\u9002\u914d\u5230doScenes\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e58\u5ba2\u5f0f\u8bed\u8a00\u6307\u4ee4\u6761\u4ef6\u5316\u8f68\u8ff9\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u884c\u4e3a\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u8ddf\u968f\u89c4\u5212\u5668\u5927\u591a\u4f9d\u8d56\u4eff\u771f\u6216\u56fa\u5b9a\u547d\u4ee4\u8bcd\u6c47\uff0c\u9650\u5236\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u6cdb\u5316\u80fd\u529b\u3002doScenes\u662f\u9996\u4e2a\u8fde\u63a5\u81ea\u7531\u5f62\u5f0f\u6307\u4ee4\u4e0e\u771f\u5b9e\u4e16\u754c\u8fd0\u52a8\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u590d\u73b0\u7684\u6307\u4ee4\u6761\u4ef6\u5316\u89c4\u5212\u57fa\u51c6\u3002", "method": "\u5c06doScenes\u6307\u4ee4\u4f5c\u4e3a\u4e58\u5ba2\u5f0f\u63d0\u793a\u96c6\u6210\u5230OpenEMMA\u89c6\u89c9\u8bed\u8a00\u754c\u9762\u4e2d\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8f93\u5165\u524d\u6444\u50cf\u5934\u89c6\u56fe\u548c\u81ea\u8f66\u72b6\u6001\uff0c\u8f93\u51fa10\u6b65\u901f\u5ea6-\u66f2\u7387\u8f68\u8ff9\u3002", "result": "\u5728849\u4e2a\u6807\u6ce8\u573a\u666f\u4e0a\u8bc4\u4f30\uff0c\u6307\u4ee4\u6761\u4ef6\u5316\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u9632\u6b62\u6781\u7aef\u57fa\u7ebf\u5931\u8d25\uff0c\u5e73\u5747ADE\u964d\u4f4e98.7%\u3002\u53bb\u9664\u5f02\u5e38\u503c\u540e\uff0c\u826f\u597d\u8868\u8ff0\u7684\u63d0\u793a\u4ecd\u80fd\u63d0\u5347ADE\u8fbe5.1%\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u6307\u4ee4\u611f\u77e5\u89c4\u5212\u7684\u53ef\u590d\u73b0\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u4ec0\u4e48\u6784\u6210OpenEMMA\u6846\u67b6\u7684\"\u597d\"\u6307\u4ee4\uff0c\u5e76\u53d1\u5e03\u4e86\u8bc4\u4f30\u63d0\u793a\u548c\u811a\u672c\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2602.04220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04220", "abs": "https://arxiv.org/abs/2602.04220", "authors": ["Yao Teng", "Minxuan Lin", "Xian Liu", "Shuai Wang", "Xiao Yang", "Xihui Liu"], "title": "Adaptive 1D Video Diffusion Autoencoder", "comment": null, "summary": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.", "AI": {"tldr": "\u63d0\u51faOne-DVA\uff1a\u57fa\u4e8eTransformer\u7684\u81ea\u9002\u5e941D\u7f16\u7801\u548c\u6269\u6563\u89e3\u7801\u7684\u89c6\u9891\u81ea\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u81ea\u7f16\u7801\u5668\u7684\u56fa\u5b9a\u538b\u7f29\u7387\u3001\u4e0d\u7075\u6d3b\u67b6\u6784\u548c\u786e\u5b9a\u6027\u89e3\u7801\u5668\u95ee\u9898", "motivation": "\u73b0\u6709\u89c6\u9891\u81ea\u7f16\u7801\u5668\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u56fa\u5b9a\u538b\u7f29\u7387\u5bfc\u81f4\u7b80\u5355\u89c6\u9891\u6d6a\u8d39token\uff1b2) \u4e0d\u7075\u6d3b\u7684CNN\u67b6\u6784\u65e0\u6cd5\u652f\u6301\u53d8\u957f\u6f5c\u5728\u5efa\u6a21\uff1b3) \u786e\u5b9a\u6027\u89e3\u7801\u5668\u96be\u4ee5\u4ece\u538b\u7f29\u6f5c\u5728\u8868\u793a\u4e2d\u6062\u590d\u7ec6\u8282", "method": "\u63d0\u51faOne-DVA\u6846\u67b6\uff1a\u7f16\u7801\u5668\u4f7f\u7528\u57fa\u4e8e\u67e5\u8be2\u7684\u89c6\u89c9Transformer\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u5e76\u751f\u6210\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u53d8\u957fdropout\u673a\u5236\u52a8\u6001\u8c03\u6574\u6f5c\u5728\u957f\u5ea6\uff1b\u89e3\u7801\u5668\u662f\u50cf\u7d20\u7a7a\u95f4\u6269\u6563Transformer\uff0c\u4ee5\u6f5c\u5728\u8868\u793a\u4e3a\u6761\u4ef6\u91cd\u5efa\u89c6\u9891\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565", "result": "\u5728\u76f8\u540c\u538b\u7f29\u6bd4\u4e0b\uff0c\u6027\u80fd\u4e0e3D-CNN VAE\u76f8\u5f53\uff1b\u652f\u6301\u81ea\u9002\u5e94\u538b\u7f29\uff0c\u53ef\u5b9e\u73b0\u66f4\u9ad8\u538b\u7f29\u6bd4\uff1b\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u5206\u5e03\u548c\u5fae\u8c03\u89e3\u7801\u5668\uff0c\u66f4\u597d\u5730\u652f\u6301\u4e0b\u6e38\u751f\u6210\u4efb\u52a1", "conclusion": "One-DVA\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u81ea\u7f16\u7801\u5668\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u81ea\u9002\u5e94\u538b\u7f29\u548c\u53d8\u957f\u6f5c\u5728\u5efa\u6a21\uff0c\u4e3a\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840"}}
{"id": "2602.04525", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04525", "abs": "https://arxiv.org/abs/2602.04525", "authors": ["Muhammad Taha Mukhtar", "Syed Musa Ali Kazmi", "Khola Naseem", "Muhammad Ali Chattha", "Andreas Dengel", "Sheraz Ahmed", "Muhammad Naseer Bajwa", "Muhammad Imran Malik"], "title": "SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking", "comment": "10 pages, 8 figures, 5 tables", "summary": "Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u975e\u6b63\u5f0f\u4f4f\u533a\u5927\u89c4\u6a21\u6d4b\u7ed8\u7684\u65b0\u534a\u76d1\u7763\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u611f\u77e5\u81ea\u9002\u5e94\u9608\u503c\u548c\u539f\u578b\u94f6\u884c\u7cfb\u7edf\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u5728\u516b\u4e2a\u57ce\u5e02\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u5feb\u901f\u57ce\u5e02\u5316\u5bfc\u81f4\u975e\u6b63\u5f0f\u4f4f\u533a\u589e\u957f\uff0c\u4f46\u5927\u89c4\u6a21\u6d4b\u7ed8\u9762\u4e34\u6807\u6ce8\u7a00\u7f3a\u3001\u5149\u8c31\u6a21\u7cca\u548c\u6807\u6ce8\u566a\u58f0\u7b49\u6570\u636e\u8d28\u91cf\u6311\u6218\u3002", "method": "1) \u6784\u5efa\u62c9\u5408\u5c14\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5361\u62c9\u5947\u3001\u5b5f\u4e70\u914d\u5957\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u65b0\u534a\u76d1\u7763\u5206\u5272\u6846\u67b6\uff0c\u5305\u542b\u7c7b\u611f\u77e5\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\u9632\u6b62\u5c11\u6570\u7c7b\u6291\u5236\uff0c\u539f\u578b\u94f6\u884c\u7cfb\u7edf\u901a\u8fc7\u5386\u53f2\u5b66\u4e60\u7684\u9ad8\u4fdd\u771f\u7279\u5f81\u8868\u793a\u5f3a\u5236\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u4e09\u5927\u6d32\u516b\u4e2a\u57ce\u5e02\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u57fa\u7ebf\uff0c\u4ec5\u752810%\u6e90\u6807\u7b7e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u5730\u7406\u533a\u57df\u8fbe\u52300.461 mIoU\uff0c\u4f18\u4e8e\u5b8c\u5168\u76d1\u7763\u6a21\u578b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u975e\u6b63\u5f0f\u4f4f\u533a\u6d4b\u7ed8\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u9886\u57df\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u57ce\u5e02\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04583", "abs": "https://arxiv.org/abs/2602.04583", "authors": ["Gabriele Magrini", "Federico Becattini", "Niccol\u00f2 Biondi", "Pietro Pala"], "title": "PEPR: Privileged Event-based Predictive Regularization for Domain Generalization", "comment": null, "summary": "Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.", "AI": {"tldr": "\u63d0\u51faPEPR\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\u8bad\u7ec3\u9c81\u68d2\u7684RGB\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u4e8b\u4ef6\u7279\u5f81\u800c\u975e\u76f4\u63a5\u5bf9\u9f50\u6765\u63d0\u5347\u57df\u6cdb\u5316\u80fd\u529b", "motivation": "\u89c6\u89c9\u611f\u77e5\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u57df\u504f\u79fb\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u9c81\u68d2\u6027\u3002RGB\u56fe\u50cf\u8bed\u4e49\u4e30\u5bcc\u4f46\u57df\u4f9d\u8d56\u6027\u5f3a\uff0c\u800c\u4e8b\u4ef6\u6d41\u7a00\u758f\u4f46\u66f4\u57df\u4e0d\u53d8\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "method": "\u63d0\u51fa\u7279\u6743\u4e8b\u4ef6\u9884\u6d4b\u6b63\u5219\u5316(PEPR)\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u4f7f\u7528\u7279\u6743\u4fe1\u606f(LUPI)\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9884\u6d4b\u95ee\u9898\u3002\u8bad\u7ec3RGB\u7f16\u7801\u5668\u9884\u6d4b\u4e8b\u4ef6\u6f5c\u5728\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u8bed\u4e49\u4e30\u5bcc\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u9c81\u68d2\u6027", "result": "\u8bad\u7ec3\u5f97\u5230\u7684\u72ec\u7acbRGB\u6a21\u578b\u5728\u65e5\u95f4\u5230\u591c\u95f4\u7b49\u57df\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e\u5bf9\u9f50\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "PEPR\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u6027\u6b63\u5219\u5316\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u7279\u6743\u4fe1\u606f\uff0c\u8bad\u7ec3\u51fa\u66f4\u9c81\u68d2\u7684RGB\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u57df\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u5355\u6a21\u6001\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2602.04388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04388", "abs": "https://arxiv.org/abs/2602.04388", "authors": ["Nadia Daoudi", "Jordi Cabot"], "title": "On the use of LLMs to generate a dataset of Neural Networks", "comment": null, "summary": "Neural networks are increasingly used to support decision-making. To verify their reliability and adaptability, researchers and practitioners have proposed a variety of tools and methods for tasks such as NN code verification, refactoring, and migration. These tools play a crucial role in guaranteeing both the correctness and maintainability of neural network architectures, helping to prevent implementation errors, simplify model updates, and ensure that complex networks can be reliably extended and reused. Yet, assessing their effectiveness remains challenging due to the lack of publicly diverse datasets of neural networks that would allow systematic evaluation. To address this gap, we leverage large language models (LLMs) to automatically generate a dataset of neural networks that can serve as a benchmark for validation. The dataset is designed to cover diverse architectural components and to handle multiple input data types and tasks. In total, 608 samples are generated, each conforming to a set of precise design choices. To further ensure their consistency, we validate the correctness of the generated networks using static analysis and symbolic tracing. We make the dataset publicly available to support the community in advancing research on neural network reliability and adaptability.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5305\u542b608\u4e2a\u6837\u672c\u7684\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u7684\u6709\u6548\u6027", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u516c\u5f00\u3001\u591a\u6837\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u3001\u91cd\u6784\u548c\u8fc1\u79fb\u5de5\u5177\u7684\u6709\u6548\u6027", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u67b6\u6784\u7ec4\u4ef6\u3001\u8f93\u5165\u6570\u636e\u7c7b\u578b\u548c\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u9759\u6001\u5206\u6790\u548c\u7b26\u53f7\u8ffd\u8e2a\u9a8c\u8bc1\u6b63\u786e\u6027", "result": "\u751f\u6210\u4e86\u5305\u542b608\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u7b26\u5408\u7cbe\u786e\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u7814\u7a76\u7684\u6807\u51c6\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u76f8\u5173\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u53d1\u5c55"}}
{"id": "2602.04448", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04448", "abs": "https://arxiv.org/abs/2602.04448", "authors": ["Jiacheng Liang", "Yuhui Wang", "Tanqiu Jiang", "Ting Wang"], "title": "RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models", "comment": "9 pages", "summary": "Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.", "AI": {"tldr": "RASA\uff1a\u9488\u5bf9MoE\u8bed\u8a00\u6a21\u578b\u7684\u8def\u7531\u611f\u77e5\u4e13\u5bb6\u7ea7\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u590d\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u800c\u975e\u5168\u5c40\u53c2\u6570\u66f4\u65b0\u6765\u63d0\u5347\u5b89\u5168\u5bf9\u9f50\u6548\u679c", "motivation": "MoE\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u7a00\u758f\u8def\u7531\u673a\u5236\uff0c\u5728\u6807\u51c6\u5168\u53c2\u6570\u5fae\u8c03\u4e0b\u53ef\u80fd\u51fa\u73b0\u9000\u5316\u4f18\u5316\u884c\u4e3a\uff0c\u5bfc\u81f4\u5b89\u5168\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002\u521d\u6b65\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5168\u53c2\u6570\u5b89\u5168\u5fae\u8c03\u53ef\u80fd\u901a\u8fc7\u8def\u7531\u6216\u4e13\u5bb6\u4e3b\u5bfc\u6548\u5e94\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u800c\u975e\u76f4\u63a5\u4fee\u590d\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u3002", "method": "\u63d0\u51faRASA\u6846\u67b6\uff1a1\uff09\u8bc6\u522b\u88ab\u6210\u529f\u8d8a\u72f1\u653b\u51fb\u8fc7\u5ea6\u6fc0\u6d3b\u7684\u4e13\u5bb6\uff1b2\uff09\u5728\u56fa\u5b9a\u8def\u7531\u4e0b\u4ec5\u5bf9\u8fd9\u4e9b\u5b89\u5168\u5173\u952e\u4e13\u5bb6\u8fdb\u884c\u9009\u62e9\u6027\u5fae\u8c03\uff1b3\uff09\u5f3a\u5236\u8def\u7531\u4e0e\u5b89\u5168\u5bf9\u9f50\u4e0a\u4e0b\u6587\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u79cd\u4ee3\u8868\u6027MoE\u67b6\u6784\u548c\u591a\u6837\u5316\u8d8a\u72f1\u653b\u51fb\u4e0a\uff0cRASA\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u9c81\u68d2\u6027\u3001\u5f3a\u5927\u7684\u8de8\u653b\u51fb\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\uff0c\u540c\u65f6\u5728MMLU\u3001GSM8K\u548cTruthfulQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "MoE\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u5e94\u4ece\u5168\u5c40\u53c2\u6570\u66f4\u65b0\u8f6c\u5411\u6709\u9488\u5bf9\u6027\u7684\u4e13\u5bb6\u4fee\u590d\uff0cRASA\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u4fdd\u6301\u67b6\u6784\u7279\u6027\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002"}}
{"id": "2602.04876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.04876", "abs": "https://arxiv.org/abs/2602.04876", "authors": ["Jiahao Zhan", "Zizhang Li", "Hong-Xing Yu", "Jiajun Wu"], "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation", "comment": "Project website: https://johnzhan2023.github.io/PerpetualWonder/", "summary": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.", "AI": {"tldr": "PerpetualWonder \u662f\u4e00\u4e2a\u6df7\u5408\u751f\u6210\u6a21\u62df\u5668\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u957f\u65f6\u7a0b\u3001\u52a8\u4f5c\u6761\u4ef6\u76844D\u573a\u666f\uff0c\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u548c\u95ed\u73af\u7cfb\u7edf\u89e3\u51b3\u7269\u7406\u72b6\u6001\u4e0e\u89c6\u89c9\u8868\u793a\u89e3\u8026\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u957f\u65f6\u7a0b4D\u573a\u666f\u65f6\u5931\u8d25\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u7269\u7406\u72b6\u6001\u4e0e\u89c6\u89c9\u8868\u793a\u662f\u89e3\u8026\u7684\uff0c\u8fd9\u5bfc\u81f4\u751f\u6210\u5f0f\u7ec6\u5316\u65e0\u6cd5\u66f4\u65b0\u5e95\u5c42\u7269\u7406\u72b6\u6001\u4ee5\u652f\u6301\u540e\u7eed\u4ea4\u4e92\u3002", "method": "1. \u5f15\u5165\u9996\u4e2a\u771f\u6b63\u7684\u95ed\u73af\u7cfb\u7edf\uff1b2. \u63d0\u51fa\u65b0\u9896\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5728\u7269\u7406\u72b6\u6001\u548c\u89c6\u89c9\u57fa\u5143\u4e4b\u95f4\u5efa\u7acb\u53cc\u5411\u94fe\u63a5\uff0c\u5141\u8bb8\u751f\u6210\u5f0f\u7ec6\u5316\u540c\u65f6\u4fee\u6b63\u52a8\u529b\u5b66\u548c\u5916\u89c2\uff1b3. \u5f15\u5165\u9c81\u68d2\u7684\u66f4\u65b0\u673a\u5236\uff0c\u4ece\u591a\u89c6\u89d2\u6536\u96c6\u76d1\u7763\u4ee5\u89e3\u51b3\u4f18\u5316\u6a21\u7cca\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u51fa\u53d1\uff0cPerpetualWonder \u80fd\u591f\u6210\u529f\u6a21\u62df\u590d\u6742\u3001\u591a\u6b65\u9aa4\u7684\u957f\u65f6\u7a0b\u4ea4\u4e92\uff0c\u4fdd\u6301\u7269\u7406\u5408\u7406\u6027\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "conclusion": "PerpetualWonder \u901a\u8fc7\u7edf\u4e00\u8868\u793a\u548c\u95ed\u73af\u7cfb\u7edf\u89e3\u51b3\u4e86\u7269\u7406\u72b6\u6001\u4e0e\u89c6\u89c9\u8868\u793a\u89e3\u8026\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u957f\u65f6\u7a0b\u3001\u52a8\u4f5c\u6761\u4ef6\u76844D\u573a\u666f\u6a21\u62df\u3002"}}
{"id": "2602.04609", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.04609", "abs": "https://arxiv.org/abs/2602.04609", "authors": ["Chenxi Hu", "Yue Ma", "Yifan Wu", "Yunhe Hou"], "title": "Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting", "comment": null, "summary": "Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.", "AI": {"tldr": "AdaCNP\u662f\u4e00\u4e2a\u7528\u4e8e\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u7535\u529b\u8d1f\u8377\u6982\u7387\u9884\u6d4b\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u76f8\u4f3c\u6027\uff0c\u5bf9\u5386\u53f2\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u91cd\u52a0\u6743\uff0c\u80fd\u591f\u5728\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5c11\u6837\u672c\u9002\u5e94\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u5206\u5e03\u3002", "motivation": "\u6781\u7aef\u5929\u6c14\u4f1a\u663e\u8457\u6539\u53d8\u7535\u529b\u6d88\u8d39\u884c\u4e3a\uff0c\u5bfc\u81f4\u8d1f\u8377\u66f2\u7ebf\u51fa\u73b0\u5c16\u5cf0\u548c\u5267\u70c8\u6ce2\u52a8\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u8fd9\u4e9b\u65f6\u671f\u5bb9\u6613\u4e0d\u51c6\u786e\uff0c\u53ef\u80fd\u5bfc\u81f4\u7535\u529b\u7cfb\u7edf\u4f9b\u5e94\u77ed\u7f3a\u6216\u5c40\u90e8\u8fc7\u8f7d\uff0c\u8feb\u4f7f\u91c7\u53d6\u7d27\u6025\u63aa\u65bd\u5982\u51cf\u8f7d\uff0c\u589e\u52a0\u670d\u52a1\u4e2d\u65ad\u548c\u516c\u5171\u5b89\u5168\u98ce\u9669\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u6781\u7aef\u4e8b\u4ef6\u4f1a\u5f15\u53d1\u8d1f\u8377\u6a21\u5f0f\u7684\u7a81\u7136\u8f6c\u53d8\uff0c\u800c\u76f8\u5173\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u4e14\u4e0d\u89c4\u5219\uff0c\u4f7f\u5f97\u53ef\u9760\u5b66\u4e60\u548c\u6821\u51c6\u53d8\u5f97\u56f0\u96be\u3002", "method": "AdaCNP\u662f\u4e00\u4e2a\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7684\u6982\u7387\u9884\u6d4b\u6a21\u578b\u3002\u5b83\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u76f8\u4f3c\u6027\uff0c\u5bf9\u6bcf\u4e2a\u76ee\u6807\u6570\u636e\u8bc4\u4f30\u5386\u53f2\u4e0a\u4e0b\u6587\u6bb5\u4e0e\u5f53\u524d\u6761\u4ef6\u7684\u76f8\u5173\u6027\uff0c\u5e76\u76f8\u5e94\u5730\u5bf9\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u91cd\u52a0\u6743\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5373\u4f7f\u5728\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u7a81\u51fa\u6700\u6709\u4fe1\u606f\u91cf\u7684\u5386\u53f2\u8bc1\u636e\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5bf9\u5148\u524d\u672a\u89c1\u8fc7\u7684\u6781\u7aef\u6a21\u5f0f\u8fdb\u884c\u5c11\u6837\u672c\u9002\u5e94\u3002\u6a21\u578b\u8fd8\u80fd\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u76ee\u6807\u57df\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u9884\u6d4b\u5206\u5e03\uff0c\u652f\u6301\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7535\u529b\u7cfb\u7edf\u8d1f\u8377\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAdaCNP\u5728\u6781\u7aef\u65f6\u671f\u66f4\u52a0\u9c81\u68d2\uff0c\u76f8\u5bf9\u4e8e\u6700\u5f3a\u57fa\u7ebf\u51cf\u5c11\u4e8622%\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\uff0c\u8868\u660e\u5176\u6982\u7387\u8f93\u51fa\u66f4\u52a0\u53ef\u9760\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eAdaCNP\u80fd\u6709\u6548\u7f13\u89e3\u7a81\u7136\u5206\u5e03\u8f6c\u53d8\u548c\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u7684\u7efc\u5408\u5f71\u54cd\u3002", "conclusion": "AdaCNP\u80fd\u591f\u6709\u6548\u7f13\u89e3\u7a81\u7136\u5206\u5e03\u8f6c\u53d8\u548c\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u7684\u7efc\u5408\u5f71\u54cd\uff0c\u4e3a\u6781\u7aef\u4e8b\u4ef6\u4e0b\u7684\u5f39\u6027\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u63d0\u4f9b\u66f4\u53ef\u4fe1\u8d56\u7684\u9884\u6d4b\uff0c\u652f\u6301\u66f4\u7a33\u5065\u7684\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u3002"}}
{"id": "2602.04643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04643", "abs": "https://arxiv.org/abs/2602.04643", "authors": ["Yanan He", "Yunshi Wen", "Xin Wang", "Tengfei Ma"], "title": "MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction", "comment": null, "summary": "Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.", "AI": {"tldr": "MTS-JEPA\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u9884\u6d4b\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u9884\u6d4b\u76ee\u6807\u548c\u8f6f\u7801\u672c\u74f6\u9888\u89e3\u51b3JEPA\u6846\u67b6\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u95ee\u9898\uff0c\u80fd\u6709\u6548\u5206\u79bb\u77ac\u6001\u51b2\u51fb\u4e0e\u957f\u671f\u8d8b\u52bf\uff0c\u5e76\u5728\u65e9\u671f\u9884\u8b66\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5bf9\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e3b\u52a8\u9884\u6d4b\u5f02\u5e38\u4ee5\u964d\u4f4e\u98ce\u9669\u3002\u867d\u7136\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\u4e3a\u5efa\u6a21\u7cfb\u7edf\u6f5c\u5728\u6f14\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u4f46\u5176\u5e94\u7528\u53d7\u5230\u8868\u793a\u5d29\u6e83\u548c\u65e0\u6cd5\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u524d\u5146\u4fe1\u53f7\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faMTS-JEPA\u67b6\u6784\uff0c\u96c6\u6210\u591a\u5206\u8fa8\u7387\u9884\u6d4b\u76ee\u6807\u548c\u8f6f\u7801\u672c\u74f6\u9888\u3002\u8be5\u8bbe\u8ba1\u660e\u786e\u5c06\u77ac\u6001\u51b2\u51fb\u4e0e\u957f\u671f\u8d8b\u52bf\u89e3\u8026\uff0c\u5e76\u5229\u7528\u7801\u672c\u6355\u6349\u79bb\u6563\u72b6\u6001\u8f6c\u6362\u3002\u8fd9\u79cd\u7ea6\u675f\u540c\u65f6\u4f5c\u4e3a\u5185\u5728\u6b63\u5219\u5316\u5668\u786e\u4fdd\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u5b9e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u9632\u6b62\u9000\u5316\u89e3\uff0c\u5e76\u5728\u65e9\u671f\u9884\u8b66\u534f\u8bae\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MTS-JEPA\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u9884\u6d4b\u548c\u8f6f\u7801\u672c\u74f6\u9888\u89e3\u51b3\u4e86JEPA\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u4e3b\u52a8\u98ce\u9669\u7f13\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.04757", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04757", "abs": "https://arxiv.org/abs/2602.04757", "authors": ["Yuchen Ye", "Zixuan Qi", "Shixuan Li", "Wei Qi", "Yanpeng Cai", "Chaoxia Yuan"], "title": "A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates", "comment": "75 pages,20 figures", "summary": "Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.", "AI": {"tldr": "\u63d0\u51faDDL-MSPMF\u53cc\u9636\u6bb5TransUNet\u6846\u67b6\uff0c\u878d\u5408\u591a\u6e90\u964d\u6c34\u4ea7\u54c1\u548cERA5\u7269\u7406\u9884\u6d4b\u56e0\u5b50\uff0c\u63d0\u5347\u4e2d\u56fd\u533a\u57df\u964d\u6c34\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u7279\u522b\u6539\u5584\u6781\u7aef\u964d\u6c34\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u591a\u6e90\u964d\u6c34\u4ea7\u54c1\u5b58\u5728\u7a7a\u95f4\u5f02\u8d28\u6027\u504f\u5dee\u548c\u6781\u7aef\u964d\u6c34\u68c0\u6d4b\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u6c34\u6587\u6c14\u5019\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u53cc\u9636\u6bb5TransUNet\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5206\u7c7b\u5668\u4f30\u8ba1\u65e5\u964d\u6c34\u53d1\u751f\u6982\u7387\uff0c\u7b2c\u4e8c\u9636\u6bb5\u56de\u5f52\u5668\u878d\u5408\u5206\u7c7b\u5668\u8f93\u51fa\u548c\u6240\u6709\u9884\u6d4b\u56e0\u5b50\uff0c\u4ee50.25\u5ea6\u5206\u8fa8\u7387\u4f30\u8ba1\u4e2d\u56fd2001-2020\u5e74\u65e5\u964d\u6c34\u91cf\u3002", "result": "DDL-MSPMF\u5728\u5b63\u8282\u5c3a\u5ea6\u8868\u73b0\u6700\u4f73\uff08R=0.75\uff1bRMSE=2.70 mm/day\uff09\uff0c\u76f8\u6bd4\u5355\u56de\u5f52\u5668\u8bbe\u7f6e\u66f4\u7a33\u5065\uff1b\u5bf9\u5f3a\u964d\u6c34\uff08>25 mm/day\uff09\u5728\u534e\u4e1c\u5927\u90e8\u5206\u5730\u533a\u63d0\u9ad8\u516c\u5e73\u5a01\u80c1\u8bc4\u5206\uff0c\u66f4\u597d\u5730\u518d\u73b02021\u5e747\u6708\u90d1\u5dde\u66b4\u96e8\u7a7a\u95f4\u683c\u5c40\uff1b\u5728\u9752\u85cf\u9ad8\u539f\u6570\u636e\u7a00\u7f3a\u533a\u57df\u4e5f\u8868\u73b0\u51fa\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u964d\u6c34\u878d\u5408\u548c\u6781\u7aef\u4e8b\u4ef6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7SHAP\u5206\u6790\u5f3a\u8c03\u4e86\u964d\u6c34\u53d1\u751f\u6982\u7387\u548c\u5730\u8868\u538b\u529b\u7684\u91cd\u8981\u6027\uff0c\u63d0\u4f9b\u7269\u7406\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u3002"}}
