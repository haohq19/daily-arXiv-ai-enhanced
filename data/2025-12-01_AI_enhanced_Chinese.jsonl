{"id": "2511.21784", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21784", "abs": "https://arxiv.org/abs/2511.21784", "authors": ["Chi Zhang", "Lin Wang"], "title": "Physics-Informed Spiking Neural Networks via Conservative Flux Quantization", "comment": null, "summary": "Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.", "AI": {"tldr": "\u63d0\u51faPISNN\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4e0e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7C-LIF\u795e\u7ecf\u5143\u548cCFQ\u7b56\u7565\u5b9e\u73b0\u4e25\u683c\u7269\u7406\u5b88\u6052\u548c\u957f\u671f\u6cdb\u5316\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5b9e\u65f6\u7269\u7406\u9884\u6d4b\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u5b9e\u65f6\u3001\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\uff0c\u4f46\u4f20\u7edfPINNs\u80fd\u8017\u9ad8\u4e14\u96be\u4ee5\u4e25\u683c\u4fdd\u8bc1\u7269\u7406\u5b88\u6052\u5b9a\u5f8b\u3002\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u867d\u9002\u5408\u8fb9\u7f18\u8ba1\u7b97\uff0c\u4f46\u7b80\u5355\u8f6c\u6362\u4f1a\u964d\u4f4e\u7269\u7406\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faPISNN\u6846\u67b6\uff1a1) \u8bbe\u8ba1C-LIF\u795e\u7ecf\u5143\uff0c\u5176\u52a8\u529b\u5b66\u7ed3\u6784\u4fdd\u8bc1\u5c40\u90e8\u8d28\u91cf\u5b88\u6052\uff1b2) \u63d0\u51faCFQ\u7b56\u7565\uff0c\u5c06\u795e\u7ecf\u8109\u51b2\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7269\u7406\u901a\u91cf\u7684\u79bb\u6563\u5305\uff0c\u5b66\u4e60\u65f6\u95f4\u4e0d\u53d8\u7684\u7269\u7406\u6f14\u5316\u7b97\u5b50\u3002", "result": "\u57281D\u70ed\u65b9\u7a0b\u548c2D\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPISNN\u80fd\u51c6\u786e\u6a21\u62df\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u540c\u65f6\u901a\u8fc7\u8bbe\u8ba1\u4fdd\u6301\u5b8c\u7f8e\u7684\u8d28\u91cf\u5b88\u6052\uff0c\u4f18\u4e8e\u4f20\u7edfPINNs\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u5c06\u79d1\u5b66\u8ba1\u7b97\u7684\u4e25\u8c28\u6027\u4e0e\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u6548\u7387\u878d\u5408\u7684\u7a33\u5065\u6846\u67b6\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u7684\u590d\u6742\u3001\u957f\u671f\u3001\u8282\u80fd\u7269\u7406\u9884\u6d4b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.22225", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22225", "abs": "https://arxiv.org/abs/2511.22225", "authors": ["Gabriel Aguirre", "Simay Atasoy Bing\u00f6l", "Heiko Hamann", "Jonas Kuckling"], "title": "Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates", "comment": "7 pages, 3 figures, submitted to IEEE MRS 2025", "summary": "Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u8ba9\u7b80\u5355\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u5371\u9669\u73af\u5883\u4e2d\u8bc6\u522b\u66f4\u5b89\u5168\u533a\u57df\uff0c\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u5efa\u6a21\u5371\u9669\u4e8b\u4ef6\uff0c\u4f7f\u7528\u5171\u8f6d\u5148\u9a8c\u9884\u6d4b\u4e8b\u4ef6\u95f4\u9694\uff0c\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u5b89\u5168\u51b3\u7b56\u3002", "motivation": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0c\u7fa4\u4f53\u673a\u5668\u4eba\u9700\u8981\u5e73\u8861\u63a2\u7d22\u3001\u901a\u4fe1\u548c\u4e2a\u4f53\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u76f4\u63a5\u6d4b\u91cf\u53d7\u9650\u6216\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u673a\u5668\u4eba\u4f7f\u7528\u5171\u8f6d\u5148\u9a8c\u9010\u6b65\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5e76\u63a8\u5bfc\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6765\u8c03\u6574\u884c\u4e3a\uff0c\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u5efa\u6a21\u4e24\u4e2a\u533a\u57df\u7684\u672a\u77e5\u5371\u9669\u4e8b\u4ef6\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u6301\u7eed\u9009\u62e9\u6b63\u786e\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u6027\u51cf\u5c11\u66b4\u9732\u4e8e\u5371\u9669\u4e8b\u4ef6\uff0c\u76f8\u6bd4\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u573a\u666f\u53ef\u6269\u5c55\u96c6\u4f53\u51b3\u7b56\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u65b9\u6cd5\u9002\u7528\u4e8e\u5371\u9669\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u9669\u611f\u77e5\u91c7\u6837\u548c\u63a2\u7d22\u5e94\u7528\u3002"}}
{"id": "2511.21982", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21982", "abs": "https://arxiv.org/abs/2511.21982", "authors": ["Futian Wang", "Chaoliu Weng", "Xiao Wang", "Zhen Chen", "Zhicheng Zhao", "Jin Tang"], "title": "DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models", "comment": null, "summary": "The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench", "AI": {"tldr": "\u63d0\u51faRPM-10K\u5927\u89c4\u6a21\u6307\u9488\u4eea\u8868\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u7269\u7406\u5173\u7cfb\u6ce8\u5165\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMRLM\uff0c\u7528\u4e8e\u7cbe\u786e\u8bfb\u53d6\u6307\u9488\u4eea\u8868\u8bfb\u6570", "motivation": "\u73b0\u6709\u6307\u9488\u4eea\u8868\u8bfb\u6570\u8bc6\u522b\u65b9\u6cd5\u5728\u53cd\u5c04\u3001\u906e\u6321\u3001\u52a8\u6001\u89c6\u89d2\u3001\u6307\u9488\u4e0e\u523b\u5ea6\u6807\u8bb0\u91cd\u53e0\u7b49\u6311\u6218\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u652f\u6301\u9c81\u68d2\u7b97\u6cd5\u5f00\u53d1", "method": "1) \u521b\u5efa\u5305\u542b10730\u5f20\u4eea\u8868\u56fe\u50cf\u7684RPM-10K\u6570\u636e\u96c6\uff1b2) \u63d0\u51faMRLM\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u5173\u7cfb\u6ce8\u5165\u663e\u5f0f\u7f16\u7801\u6307\u9488\u4e0e\u523b\u5ea6\u7684\u51e0\u4f55\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u91c7\u7528\u8de8\u6ce8\u610f\u529b\u878d\u5408\u548c\u81ea\u9002\u5e94\u4e13\u5bb6\u9009\u62e9\u673a\u5236", "result": "\u5728\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5f15\u5165\u7269\u7406\u5173\u7cfb\u6ce8\u5165\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u9488\u4eea\u8868\u8bfb\u6570\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027"}}
{"id": "2511.22354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22354", "abs": "https://arxiv.org/abs/2511.22354", "authors": ["Suraj Borate", "Bhavish Rai B", "Vipul Pardeshi", "Madhu Vadali"], "title": "LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning", "comment": "submitted to ICRA 2026", "summary": "This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.", "AI": {"tldr": "CoMuRoS\u662f\u4e00\u4e2a\u5206\u5c42\u67b6\u6784\uff0c\u5c06\u96c6\u4e2d\u5f0f\u89c4\u5212\u4e0e\u5206\u5e03\u5f0f\u6267\u884c\u7ed3\u5408\uff0c\u652f\u6301\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\uff0c\u5b9e\u73b0\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7f3a\u4e4f\u8fd0\u884c\u65f6\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\u80fd\u529b\uff0c\u96be\u4ee5\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u4efb\u52a1\u5931\u8d25\u3001\u7528\u6237\u610f\u56fe\u53d8\u5316\u548c\u610f\u5916\u4e8b\u4ef6\u7684\u901a\u7528\u67b6\u6784\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u4efb\u52a1\u7ba1\u7406\u5668LLM\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u3001\u5206\u7c7b\u4efb\u52a1\u3001\u5206\u914d\u5b50\u4efb\u52a1\uff1b\u6bcf\u4e2a\u673a\u5668\u4eba\u8fd0\u884c\u672c\u5730LLM\u5c06\u539f\u59cb\u6280\u80fd\u7ec4\u5408\u6210\u53ef\u6267\u884cPython\u4ee3\u7801\uff1b\u673a\u8f7d\u611f\u77e5\u6301\u7eed\u76d1\u63a7\u4e8b\u4ef6\u5e76\u5206\u7c7b\uff1b\u4e8b\u4ef6\u89e6\u53d1\u91cd\u89c4\u5212\u673a\u5236\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\uff1a\u80fd\u4ece\u5e72\u6270\u4e8b\u4ef6\u4e2d\u81ea\u4e3b\u6062\u590d\uff08\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7269\u4f53\u6062\u590d\u6210\u529f\u73879/10\uff09\u3001\u8fc7\u6ee4\u65e0\u5173\u5e72\u6270\u3001\u7d27\u5bc6\u534f\u8c03\u8fd0\u8f93\uff088/8\u6210\u529f\uff09\u3001\u4eba\u673a\u534f\u4f5c\u6062\u590d\uff085/5\u6210\u529f\uff09\u3002\u4eff\u771f\u9a8c\u8bc1\u610f\u56fe\u611f\u77e5\u91cd\u89c4\u5212\u3002\u57fa\u51c6\u6d4b\u8bd5\u572822\u4e2a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u6b63\u786e\u7387\u6700\u9ad80.91\uff09\u3002", "conclusion": "CoMuRoS\u9996\u6b21\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u8fd0\u884c\u65f6\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\uff0c\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u7075\u6d3b\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709LLM\u9a71\u52a8\u7684\u7cfb\u7edf\u3002"}}
{"id": "2511.22685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22685", "abs": "https://arxiv.org/abs/2511.22685", "authors": ["Haoyi Wang", "Licheng Luo", "Yiannis Kantaros", "Bruno Sinopoli", "Mingyu Cai"], "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation", "comment": null, "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\u6574\u5408RL\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700MAPF\uff0c\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bfc\u822a\u9762\u4e34\u53cd\u5e94\u5f0f\u907f\u78b0\u4e0e\u957f\u671f\u76ee\u6807\u8fbe\u6210\u7684\u5e73\u8861\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u72ed\u7a84\u901a\u9053\u4e2d\uff0cRL\u7b56\u7565\u9047\u5230\u8d85\u51fa\u5b66\u4e60\u5206\u5e03\u7684\u65b0\u914d\u7f6e\u65f6\u5bb9\u6613\u4ea7\u751f\u6b7b\u9501\uff0c\u73b0\u6709RL\u65b9\u6cd5\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u65e0\u7f1d\u6574\u5408\u57fa\u4e8eRL\u7684\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u3002\u5305\u542b\u5b89\u5168\u5c42\u76d1\u63a7\u667a\u80fd\u4f53\u8fdb\u5ea6\u68c0\u6d4b\u6b7b\u9501\uff0c\u89e6\u53d1\u65f6\u542f\u52a8\u53d7\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u534f\u8c03\u63a7\u5236\u5668\u3002\u6846\u67b6\u901a\u8fc7MAPF\u6784\u5efa\u5168\u5c40\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u8c03\u8282\u822a\u70b9\u8fdb\u5ea6\u4ee5\u51cf\u5c11\u5bfc\u822a\u4e2d\u7684\u667a\u80fd\u4f53\u95f4\u51b2\u7a81", "result": "\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u4ece\u8fb9\u7f18\u63d0\u5347\u5230\u63a5\u8fd1\u666e\u904d\u6210\u529f\uff0c\u663e\u8457\u51cf\u5c11\u6b7b\u9501\u548c\u78b0\u649e\u3002\u4e0e\u5206\u5c42\u4efb\u52a1\u89c4\u5212\u7ed3\u5408\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u5f02\u6784\u673a\u5668\u4eba\u7684\u534f\u8c03\u5bfc\u822a", "conclusion": "\u5c06\u53cd\u5e94\u5f0fRL\u5bfc\u822a\u4e0e\u9009\u62e9\u6027MAPF\u5e72\u9884\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2511.21729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21729", "abs": "https://arxiv.org/abs/2511.21729", "authors": ["Jithin Krishnan"], "title": "Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems", "comment": "10 pages, 4 figures", "summary": "Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \"abstained\" versus \"unsupported\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.", "AI": {"tldr": "RAG\u7cfb\u7edf\u7ec4\u4ef6\u534f\u540c\u96c6\u6210\u6bd4\u5355\u4e2a\u7ec4\u4ef6\u5f3a\u5ea6\u66f4\u91cd\u8981\uff0c\u901a\u8fc7\u7efc\u5408\u4f7f\u7528\u6df7\u5408\u68c0\u7d22\u3001\u96c6\u6210\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u9608\u503c\u7b49\u6280\u672f\uff0c\u53ef\u5c06\u5f03\u7b54\u7387\u4ece40%\u964d\u81f32%\u800c\u4e0d\u589e\u52a0\u5e7b\u89c9\uff0c\u540c\u65f6\u9700\u8981\u6807\u51c6\u5316\u6307\u6807\u548c\u6807\u7b7e\u6765\u51c6\u786e\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u6784\u5efa\u53ef\u9760\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u9700\u8981\u7406\u89e3\u5404\u7ec4\u4ef6\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6dfb\u52a0\u5f3a\u5927\u7ec4\u4ef6\u3002\u5f53\u524d\u5b58\u5728\u6d4b\u91cf\u6311\u6218\uff1a\u4e0d\u540c\u9a8c\u8bc1\u7b56\u7565\u53ef\u80fd\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u6807\u7b7e\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7387\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u4f7f\u7528\u6d88\u878d\u7814\u7a76\u5206\u679050\u4e2a\u67e5\u8be2\uff0815\u4e2a\u53ef\u56de\u7b54\u300110\u4e2a\u8fb9\u7f18\u6848\u4f8b\u300125\u4e2a\u5bf9\u6297\u6027\u67e5\u8be2\uff09\uff0c\u8bc4\u4f30\u6df7\u5408\u68c0\u7d22\u3001\u96c6\u6210\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u9608\u503c\u7b49\u6280\u672f\u5728\u5355\u72ec\u4f7f\u7528\u548c\u7ec4\u5408\u4f7f\u7528\u65f6\u7684\u6548\u679c\u3002", "result": "\u589e\u5f3a\u6280\u672f\u5355\u72ec\u4f7f\u7528\u65f6\u51e0\u4e4e\u65e0\u76ca\uff0c\u4f46\u534f\u540c\u4f7f\u7528\u65f6\u53ef\u5c06\u5f03\u7b54\u7387\u4ece40%\u964d\u81f32%\u800c\u4e0d\u589e\u52a0\u5e7b\u89c9\u3002\u540c\u65f6\u53d1\u73b0\u4e0d\u540c\u9a8c\u8bc1\u7b56\u7565\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u6807\u7b7e\uff08\u5982\"\u5f03\u7b54\"\u4e0e\"\u4e0d\u652f\u6301\"\uff09\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7387\u8bc4\u4f30\u51fa\u73b0\u4f2a\u5f71\u3002", "conclusion": "RAG\u7cfb\u7edf\u7684\u534f\u540c\u96c6\u6210\u6bd4\u4efb\u4f55\u5355\u4e2a\u7ec4\u4ef6\u7684\u5f3a\u5ea6\u66f4\u91cd\u8981\uff1b\u9700\u8981\u6807\u51c6\u5316\u6307\u6807\u548c\u6807\u7b7e\u6765\u6b63\u786e\u89e3\u91ca\u6027\u80fd\uff1b\u5373\u4f7f\u68c0\u7d22\u8d28\u91cf\u9ad8\uff0c\u4e5f\u9700\u8981\u81ea\u9002\u5e94\u6821\u51c6\u6765\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u7684\u8fc7\u5ea6\u56de\u7b54\u3002"}}
{"id": "2511.22729", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22729", "abs": "https://arxiv.org/abs/2511.22729", "authors": ["Anton Bulle Labate", "Valesca Moura de Sousa", "Sandro Rama Fiorini", "Leonardo Guerreiro Azevedo", "Raphael Melo Thiago", "Viviane Torres da Silva"], "title": "Solving Context Window Overflow in AI Agents", "comment": null, "summary": "Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8ba9LLM\u5904\u7406\u4efb\u610f\u957f\u5ea6\u5de5\u5177\u8f93\u51fa\u800c\u4e0d\u4e22\u5931\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u5b58\u6307\u9488\u66ff\u4ee3\u539f\u59cb\u6570\u636e\u4ea4\u4e92\uff0c\u51cf\u5c11token\u4f7f\u7528\u548c\u6267\u884c\u65f6\u95f4", "motivation": "LLM\u5728\u5904\u7406\u52a8\u6001\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\uff09\u65f6\uff0c\u5927\u578b\u5de5\u5177\u8f93\u51fa\u4f1a\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u73b0\u6709\u622a\u65ad\u6216\u6458\u8981\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u5b8c\u6574\u6570\u636e\uff0c\u4e0d\u9002\u5408\u9700\u8981\u5b8c\u6574\u8f93\u51fa\u7684\u5de5\u4f5c\u6d41\u7a0b", "method": "\u5c06LLM\u4e0e\u5de5\u5177\u7684\u4ea4\u4e92\u4ece\u539f\u59cb\u6570\u636e\u8f6c\u5411\u5185\u5b58\u6307\u9488\uff0c\u4fdd\u7559\u5de5\u5177\u529f\u80fd\uff0c\u5b9e\u73b0\u65e0\u7f1d\u96c6\u6210\u5230\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u4e2d", "result": "\u5728\u771f\u5b9e\u6750\u6599\u79d1\u5b66\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6267\u884c\u7684\u4efb\u52a1\u5f97\u4ee5\u5b8c\u6210\uff1b\u5728\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u6210\u529f\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u65b9\u6cd5\u6d88\u8017\u7684token\u6570\u91cf\u7ea6\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u4e03\u5206\u4e4b\u4e00", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7fLLM\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u5de5\u5177\u8f93\u51fa\u800c\u4e0d\u4e22\u5931\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5b8c\u6574\u6570\u636e\u7684\u5de5\u4f5c\u6d41\u7a0b"}}
{"id": "2511.22737", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22737", "abs": "https://arxiv.org/abs/2511.22737", "authors": ["Salman Jan", "Toqeer Ali Syed", "Gohar Ali", "Ali Akarma", "Mohammad Riyaz Belgaum", "Ahmad Ali"], "title": "Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being", "comment": "Presented at International Conference on Business and Digital Technology, Bahrain, Springer Nature, 27 November 2025", "summary": "The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u5c42\u4ee3\u7406AI\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u7528\u4ee3\u7406\uff08\u81b3\u98df\u89c4\u5212\u3001\u63d0\u9192\u3001\u98df\u7269\u6307\u5bfc\u3001\u76d1\u6d4b\uff09\u4e3a\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u63d0\u4f9b\u4e2a\u6027\u5316\u5065\u5eb7\u652f\u6301\uff0c\u7ed3\u5408\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u6e90\u548c\u53ef\u89e3\u91caAI\u6a21\u5757\u3002", "motivation": "\u4f20\u7edf\u8f85\u52a9\u7cfb\u7edf\u7f3a\u4e4f\u5305\u5bb9\u6027\u3001\u4e2a\u6027\u5316\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u9700\u8981\u4e3a\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u5f00\u53d1\u80fd\u591f\u4fc3\u8fdb\u5065\u5eb7\u3001\u89c4\u5f8b\u751f\u6d3b\u548c\u6570\u5b57\u516c\u5e73\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u5e94\u7528\u63a5\u53e3\u5c42\u3001\u4ee3\u7406\u5c42\u3001\u6570\u636e\u6e90\u5c42\u3002\u901a\u8fc7\u6df7\u5408\u63a8\u7406\u5f15\u64ce\u534f\u8c03\u56db\u4e2a\u4e13\u7528\u4ee3\u7406\uff08\u81b3\u98df\u89c4\u5212\u3001\u63d0\u9192\u3001\u98df\u7269\u6307\u5bfc\u3001\u76d1\u6d4b\uff09\uff0c\u4f7f\u7528\u9ed1\u677f/\u4e8b\u4ef6\u603b\u7ebf\u8fdb\u884c\u901a\u4fe1\uff0c\u6574\u5408EHR\u3001\u8425\u517b\u6570\u636e\u5e93\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u667a\u80fd\u53a8\u623fIoT\u7b49\u9690\u79c1\u654f\u611f\u6570\u636e\u6e90\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d85\u8d8a\u4f20\u7edf\u8f85\u52a9\u7cfb\u7edf\u7684\u4ee3\u7406AI\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5305\u5bb9\u6027\u3001\u4e2a\u6027\u5316\u548c\u53ef\u8bbf\u95ee\u6027\u7684\u6574\u5408\uff0c\u5c55\u793a\u4e86\u591a\u4ee3\u7406\u63a8\u7406\u3001\u591a\u6a21\u6001\u754c\u9762\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u4ea4\u53c9\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u589e\u5f3a\u6b8b\u969c\u548c\u795e\u7ecf\u591a\u6837\u6027\u4eba\u7fa4\u7684\u81ea\u4e3b\u6027\u3001\u5065\u5eb7\u548c\u6570\u5b57\u516c\u5e73\uff0c\u662f\u591a\u4ee3\u7406\u63a8\u7406\u3001\u591a\u6a21\u6001\u754c\u9762\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u521b\u65b0\u7ed3\u5408\u3002"}}
{"id": "2511.22767", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22767", "abs": "https://arxiv.org/abs/2511.22767", "authors": ["Toqeer Ali Syed", "Sohail Khan", "Salman Jan", "Gohar Ali", "Muhammad Nauman", "Ali Akarma", "Ahmad Ali"], "title": "Agentic AI Framework for Cloudburst Prediction and Coordinated Response", "comment": "Presented at International Conference on Business and Digital Technology, Bahrain, Springer Nature, 27 November 2025", "summary": "The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53AI\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u7528\u4e8e\u6781\u7aef\u77ed\u65f6\u964d\u96e8\u4e8b\u4ef6\uff08\u5982\u4e91\u7206\uff09\u7684\u9884\u6d4b\u548c\u54cd\u5e94\uff0c\u5c06\u4f20\u611f\u3001\u9884\u62a5\u3001\u964d\u5c3a\u5ea6\u3001\u6c34\u6587\u5efa\u6a21\u548c\u534f\u8c03\u54cd\u5e94\u6574\u5408\u4e3a\u5355\u4e00\u7cfb\u7edf\uff0c\u5728\u5df4\u57fa\u65af\u5766\u5317\u90e8\u5730\u533a\u9a8c\u8bc1\u4e2d\u63d0\u9ad8\u4e86\u9884\u62a5\u53ef\u9760\u6027\u548c\u9884\u8b66\u63d0\u524d\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u9884\u62a5\u7cfb\u7edf\u5c06\u9884\u6d4b\u548c\u54cd\u5e94\u89c6\u4e3a\u4e24\u4e2a\u72ec\u7acb\u8fc7\u7a0b\uff0c\u96be\u4ee5\u5e94\u5bf9\u6781\u7aef\u77ed\u65f6\u964d\u96e8\u4e8b\u4ef6\uff08\u5982\u4e91\u7206\uff09\u7684\u6311\u6218\u3002\u9700\u8981\u5c06\u4f20\u611f\u3001\u9884\u62a5\u3001\u5efa\u6a21\u548c\u54cd\u5e94\u6574\u5408\u4e3a\u95ed\u73af\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u51b3\u7b56\u667a\u80fd\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u5305\u542b\u81ea\u4e3b\u4f46\u534f\u4f5c\u7684\u667a\u80fd\u4f53\uff0c\u5728\u6574\u4e2a\u4e8b\u4ef6\u751f\u547d\u5468\u671f\u4e2d\u8fdb\u884c\u63a8\u7406\u3001\u611f\u77e5\u548c\u884c\u52a8\u3002\u7cfb\u7edf\u6574\u5408\u5927\u6c14\u6c34\u5faa\u73af\u667a\u80fd\uff0c\u5305\u62ec\u4f20\u611f\u3001\u9884\u62a5\u3001\u964d\u5c3a\u5ea6\u3001\u6c34\u6587\u5efa\u6a21\u548c\u534f\u8c03\u54cd\u5e94\uff0c\u5f62\u6210\u95ed\u73af\u7cfb\u7edf\u3002\u667a\u80fd\u4f53\u901a\u8fc7\u901a\u4fe1\u548c\u8def\u7531\u4f18\u5316\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u5f0f\u5b66\u4e60\u5c42\u5b9e\u73b0\u81ea\u9002\u5e94\u6821\u51c6\u548c\u900f\u660e\u5ba1\u8ba1\u3002", "result": "\u5728\u5df4\u57fa\u65af\u5766\u5317\u90e8\u5730\u533a\u7684\u591a\u5e74\u96f7\u8fbe\u3001\u536b\u661f\u548c\u5730\u9762\u6570\u636e\u8bc4\u4f30\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e86\u9884\u62a5\u53ef\u9760\u6027\u3001\u5173\u952e\u6210\u529f\u6307\u6570\u548c\u9884\u8b66\u63d0\u524d\u65f6\u95f4\u3002\u901a\u8fc7\u901a\u4fe1\u548c\u8def\u7531\u667a\u80fd\u4f53\u6700\u5927\u5316\u4eba\u53e3\u8986\u76d6\u8303\u56f4\uff0c\u6700\u5c0f\u5316\u758f\u6563\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\uff0c\u5d4c\u5165\u5f0f\u5b66\u4e60\u5c42\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u51c6\u548c\u900f\u660e\u5ba1\u8ba1\u80fd\u529b\u3002", "conclusion": "\u534f\u4f5cAI\u667a\u80fd\u4f53\u80fd\u591f\u5c06\u5927\u6c14\u6570\u636e\u6d41\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u9884\u89c1\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7684\u6c14\u5019\u97e7\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\uff0c\u80fd\u591f\u6539\u53d8\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u9884\u6d4b\u548c\u54cd\u5e94\u65b9\u5f0f\u3002"}}
{"id": "2511.22125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22125", "abs": "https://arxiv.org/abs/2511.22125", "authors": ["Bin Wang", "Ruotong Hu", "Wenqian Wang", "Wentong Li", "Mingliang Gao", "Runmin Cong", "Wei Zhang"], "title": "GA2-CLIP: Generic Attribute Anchor for Efficient Prompt Tuningin Video-Language Models", "comment": "Technical Report", "summary": "Visual and textual soft prompt tuning can effectively improve the adaptability of Vision-Language Models (VLMs) in downstream tasks. However, fine-tuning on video tasks impairs the model's generalization ability to unseen classes. Existing methods attempt to mitigate this forgetting effect by regularizing the gap between hand-crafted prompts and soft prompts, but this also weakens the learning ability of soft prompts. To address this challenge, we propose a plug-and-play coupling prompt learning framework to optimize the generalization performance of V-L models in video tasks, with the core motivation of mitigating semantic space narrowing during fine-tuning by introducing an externally supervised prompt. Specifically, for textual prompts, we introduce pre-trained prompts from other datasets as hard prompt tokens. These are concatenated with soft prompt tokens and coupled via a learnable mapping layer. This competitive prompting approach prevents the semantic space from overfitting to supervised categories. In addition, we introduce a set of well-designed irrelevant video sets and negative prompts as generic attribute anchors to maintain the generic relevance of the attributes in the pre-trained semantic space, thus preserving the generalization ability. Experiments on video tasks demonstrate that our method significantly outperforms state-of-the-art prompt tuning approaches across generalization benchmarks, particularly on base-to-new class prediction.", "AI": {"tldr": "\u63d0\u51fa\u8026\u5408\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u76d1\u7763\u63d0\u793a\u548c\u7ade\u4e89\u6027\u63d0\u793a\u673a\u5236\uff0c\u7f13\u89e3\u89c6\u9891\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u8bed\u4e49\u7a7a\u95f4\u7a84\u5316\u95ee\u9898\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u5fae\u8c03\u65f6\u4f1a\u51fa\u73b0\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6b63\u5219\u5316\u624b\u5de5\u63d0\u793a\u4e0e\u8f6f\u63d0\u793a\u4e4b\u95f4\u7684\u5dee\u8ddd\u6765\u7f13\u89e3\u9057\u5fd8\u6548\u5e94\uff0c\u4f46\u8fd9\u4f1a\u524a\u5f31\u8f6f\u63d0\u793a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u63d2\u62d4\u5f0f\u8026\u5408\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u6587\u672c\u63d0\u793a\u65b9\u9762\uff0c\u5f15\u5165\u5176\u4ed6\u6570\u636e\u96c6\u7684\u9884\u8bad\u7ec3\u63d0\u793a\u4f5c\u4e3a\u786c\u63d0\u793a\u6807\u8bb0\uff0c\u4e0e\u8f6f\u63d0\u793a\u6807\u8bb0\u62fc\u63a5\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u6620\u5c04\u5c42\u8026\u5408\uff1b2\uff09\u5f15\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65e0\u5173\u89c6\u9891\u96c6\u548c\u8d1f\u63d0\u793a\u4f5c\u4e3a\u901a\u7528\u5c5e\u6027\u951a\u70b9\uff0c\u4fdd\u6301\u9884\u8bad\u7ec3\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5c5e\u6027\u7684\u901a\u7528\u76f8\u5173\u6027\u3002", "result": "\u5728\u89c6\u9891\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u5728\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u7279\u522b\u662f\u5728\u57fa\u7c7b\u5230\u65b0\u7c7b\u7684\u9884\u6d4b\u4efb\u52a1\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u7ade\u4e89\u6027\u63d0\u793a\u673a\u5236\u548c\u901a\u7528\u5c5e\u6027\u951a\u70b9\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u7a7a\u95f4\u7a84\u5316\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.21740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21740", "abs": "https://arxiv.org/abs/2511.21740", "authors": ["Yizi Zhang", "Linyang He", "Chaofei Fan", "Tingkai Liu", "Han Yu", "Trung Le", "Jingyuan Li", "Scott Linderman", "Lea Duncker", "Francis R Willett", "Nima Mesgarani", "Liam Paninski"], "title": "Decoding inner speech with an end-to-end brain-to-text neural interface", "comment": null, "summary": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u8111\u5230\u6587\u672c\uff08BIT\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u53ef\u5fae\u5206\u795e\u7ecf\u7f51\u7edc\u5c06\u795e\u7ecf\u6d3b\u52a8\u76f4\u63a5\u7ffb\u8bd1\u4e3a\u8fde\u8d2f\u53e5\u5b50\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u548c\u8de8\u7269\u79cd\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u4f7f\u7528\u7ea7\u8054\u6846\u67b6\uff0c\u5148\u89e3\u7801\u97f3\u7d20\u518d\u7528\u8bed\u8a00\u6a21\u578b\u7ec4\u88c5\u53e5\u5b50\uff0c\u8fd9\u79cd\u5206\u79bb\u67b6\u6784\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u6240\u6709\u9636\u6bb5\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u6846\u67b6\u6765\u63d0\u5347\u89e3\u7801\u6548\u679c\u3002", "method": "\u63d0\u51faBIT\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u8de8\u4efb\u52a1\u3001\u8de8\u7269\u79cd\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f16\u7801\u5668\uff0c\u5176\u8868\u5f81\u53ef\u8fc1\u79fb\u5230\u5c1d\u8bd5\u6027\u548c\u60f3\u8c61\u6027\u8bed\u97f3\u3002\u5728\u7ea7\u8054\u8bbe\u7f6e\u4e2d\u4f7f\u7528n-gram\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7aef\u5230\u7aef\u8bbe\u7f6e\u4e2d\u4e0e\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728Brain-to-Text '24\u548c'25\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5728\u7ea7\u8054\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u65b0\u7684SOTA\u3002\u7aef\u5230\u7aef\u96c6\u6210\u5c06\u8bcd\u9519\u8bef\u7387\u4ece24.69%\u964d\u81f310.22%\u3002\u5c0f\u578b\u97f3\u9891LLM\u663e\u8457\u6539\u5584\u7aef\u5230\u7aef\u89e3\u7801\uff0cBIT\u6846\u67b6\u8fd8\u80fd\u5bf9\u9f50\u5c1d\u8bd5\u6027\u548c\u60f3\u8c61\u6027\u8bed\u97f3\u5d4c\u5165\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "BIT\u6846\u67b6\u63a8\u8fdb\u4e86\u5927\u89c4\u6a21\u591a\u6837\u5316\u795e\u7ecf\u6570\u636e\u96c6\u7684\u6574\u5408\uff0c\u4e3a\u652f\u6301\u65e0\u7f1d\u53ef\u5fae\u5206\u4f18\u5316\u7684\u7aef\u5230\u7aef\u89e3\u7801\u6846\u67b6\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u90fd\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2511.22134", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22134", "abs": "https://arxiv.org/abs/2511.22134", "authors": ["Zhen Fang", "Zhuoyang Liu", "Jiaming Liu", "Hao Chen", "Yu Zeng", "Shiting Huang", "Zehui Chen", "Lin Chen", "Shanghang Zhang", "Feng Zhao"], "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action", "comment": null, "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.", "AI": {"tldr": "DualVLA\u901a\u8fc7\u53cc\u9636\u6bb5\u540e\u8bad\u7ec3\u89e3\u51b3VLA\u6a21\u578b\u5728\u6062\u590d\u63a8\u7406\u80fd\u529b\u65f6\u51fa\u73b0\u7684\u52a8\u4f5c\u9000\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u52a8\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u4ece\u4e13\u5bb6\u6a21\u578b\u6269\u5c55\u5230\u901a\u7528\u63a8\u7406\u6a21\u578b\u65f6\uff0c\u7ecf\u5e38\u51fa\u73b0\u52a8\u4f5c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff08\u52a8\u4f5c\u9000\u5316\u73b0\u8c61\uff09\uff0c\u9700\u8981\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u52a8\u4f5c\u6267\u884c\u7cbe\u5ea6", "method": "1) \u53cc\u5c42\u6570\u636e\u526a\u679d\uff1a\u53bb\u9664\u5197\u4f59\u7684\u5177\u8eab\u63a8\u7406\u6570\u636e\uff0c\u9632\u6b62\u5e72\u6270\u52a8\u4f5c\u5b66\u4e60\uff1b2) \u53cc\u6559\u5e08\u81ea\u9002\u5e94\u84b8\u998f\uff1a\u5bf9\u4e0d\u540c\u6570\u636e\u57df\u5206\u914d\u4e0d\u540c\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u80fd\u529b\uff1b3) \u63d0\u51faVLA Score\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u63a8\u7406\u3001\u610f\u56fe\u3001\u52a8\u4f5c\u548c\u5bf9\u9f50\u56db\u4e2a\u7ef4\u5ea6\u89e3\u8026\u8bc4\u4f30", "result": "DualVLA\u5728SimplerEnv\u4e2d\u8fbe\u523061.0%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728\u516b\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u520665.4\uff0c\u5728\u7cbe\u786e\u52a8\u4f5c\u6267\u884c\u548c\u591a\u6a21\u6001\u7406\u89e3\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861", "conclusion": "DualVLA\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u52a8\u4f5c\u9000\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u6027\u80fd\uff0c\u4e3a\u901a\u7528VLA\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2511.21743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21743", "abs": "https://arxiv.org/abs/2511.21743", "authors": ["Mukul Singh", "Ananya Singha", "Arjun Radhakrishna", "Sumit Gulwani"], "title": "Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning", "comment": null, "summary": "We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u5de5\u4f5c\u8bb0\u5fc6\uff0c\u7ecf\u5386\u56db\u4e2a\u80fd\u529b\u9636\u6bb5\uff1a\u4ece\u65e0\u63a8\u7406\u9519\u8bef\u8f93\u51fa\uff0c\u5230\u6709\u63a8\u7406\u4f46\u9519\u8bef\uff0c\u518d\u5230\u6709\u6548\u63a8\u7406\uff0c\u6700\u540e\u65e0\u9700\u663e\u5f0f\u63a8\u7406\u5373\u53ef\u89e3\u51b3\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u5c06\u63a8\u7406\u6807\u8bb0\uff08\u89e3\u51b3\u95ee\u9898\u7684\u4e2d\u95f4\u6b65\u9aa4\uff09\u4e0e\u4eba\u7c7b\u5de5\u4f5c\u8bb0\u5fc6\u8fdb\u884c\u7c7b\u6bd4\uff0c\u4ece\u8ba4\u77e5\u79d1\u5b66\u89d2\u5ea6\u7406\u89e3\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u4ece\u8ba4\u77e5\u79d1\u5b66\u7684\"\u80fd\u529b\u56db\u9636\u6bb5\"\u7406\u8bba\u51fa\u53d1\uff0c\u5c06\u8bad\u7ec3\u52a8\u6001\u4e0e\u56db\u4e2a\u9636\u6bb5\u5bf9\u9f50\uff1a\u65e0\u610f\u8bc6\u65e0\u80fd\u3001\u6709\u610f\u8bc6\u65e0\u80fd\u3001\u6709\u610f\u8bc6\u80fd\u529b\u3001\u65e0\u610f\u8bc6\u80fd\u529b\u3002\u901a\u8fc7\u5206\u6790\u63a8\u7406\u6807\u8bb0\u957f\u5ea6\u53d8\u5316\u6765\u8ddf\u8e2a\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u63a8\u7406\u6807\u8bb0\u957f\u5ea6\u968f\u6027\u80fd\u63d0\u5347\u800c\u6269\u5c55\uff0c\u5728\"\u6709\u610f\u8bc6\u80fd\u529b\"\u9636\u6bb5\u8fbe\u5230\u5cf0\u503c\uff0c\u7136\u540e\u968f\u7740\u6a21\u578b\u5c06\u4efb\u52a1\u5185\u5316\u800c\u4e0b\u964d\u3002\u8bad\u7ec3\u540e\uff0c\u5373\u4f7f\u79fb\u9664\u63a8\u7406\u6807\u8bb0\uff0c\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u8868\u660e\u63a8\u7406\u8d77\u5230\u4e86\u5b66\u4e60\u652f\u67b6\u4f5c\u7528\u3002", "conclusion": "\u63a8\u7406\u6807\u8bb0\u52a8\u6001\u53ef\u4f5c\u4e3a\u8bca\u65ad\u8bad\u7ec3\u9636\u6bb5\u3001\u8bc6\u522b\u6536\u655b\u548c\u6307\u5bfc\u65e9\u505c\u7684\u4fe1\u53f7\u3002\u63d0\u51fa\u7684\u6307\u6807\u53ef\u8ddf\u8e2a\u8fd9\u4e00\u8f68\u8ff9\uff0c\u63a8\u7406\u884c\u4e3a\u5bf9\u4e8e\u7406\u89e3\u548c\u4f18\u5316\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.22169", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22169", "abs": "https://arxiv.org/abs/2511.22169", "authors": ["Inha Kang", "Eunki Kim", "Wonjeong Ryu", "Jaeyo Shin", "Seungjun Yu", "Yoon-Hee Kang", "Seongeun Jeong", "Eunhye Kim", "Soontae Kim", "Hyunjung Shim"], "title": "Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization", "comment": "10 pages", "summary": "Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4e1c\u4e9a\u5730\u533aPM\u6d53\u5ea6\u957f\u671f\u9884\u6d4b\u7684\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u96c6\u548c\u5f15\u5165\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff08\u5982Aurora\uff09\u5728\u590d\u6742\u5730\u5f62\u548c\u5f3a\u5927\u6c14\u52a8\u529b\u5b66\u7684\u4e1c\u4e9a\u5730\u533a\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u533a\u57df\u7279\u5b9a\u52a8\u6001\u3001\u4f9d\u8d56\u975e\u5b9e\u65f6\u8f93\u5165\uff0c\u4e14\u6807\u51c6\u70b9\u9884\u6d4b\u76ee\u6807\u65e0\u6cd5\u53cd\u6620\u64cd\u4f5c\u6210\u672c\u7684\u4e0d\u5bf9\u79f0\u6027\uff08\u8bef\u62a5\u635f\u5bb3\u516c\u4f17\u4fe1\u4efb\uff0c\u6f0f\u62a5\u5371\u53ca\u4eba\u7fa4\u5065\u5eb7\uff09\u3002", "method": "1) \u6784\u5efa\u5e76\u53d1\u5e03\u4e1c\u4e9a\u5730\u533a\u7684\u771f\u5b9e\u89c2\u6d4b\u548c\u9ad8\u5206\u8fa8\u7387CMAQ-OBS\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u7ed3\u5408\u7c7b\u522b\u7279\u5b9a\u5956\u52b1\u548c\u8bfe\u7a0b\u5f0f\u5c55\u5f00\uff0c\u4f7f\u9884\u6d4b\u4e0e\u64cd\u4f5c\u4f18\u5148\u7ea7\u5bf9\u9f50\u3002", "result": "\u533a\u57df\u8bef\u5dee\u964d\u4f4e59.5%\uff0c\u652f\u6301\u5b9e\u65f648-120\u5c0f\u65f6\u9884\u6d4b\uff1b\u4e0e\u4ec5\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8bef\u62a5\u7387\u964d\u4f4e47.3%\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684F1\u5206\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u6d4b\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u636e\u96c6\u6784\u5efa\u548c\u64cd\u4f5c\u6210\u672c\u5bf9\u9f50\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e1c\u4e9a\u5730\u533aPM\u6d53\u5ea6\u957f\u671f\u9884\u6d4b\u7684\u5b9e\u8df5\u6311\u6218\uff0c\u4e3a\u516c\u5171\u536b\u751f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u5b9e\u7528\u7684\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23366", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.23366", "abs": "https://arxiv.org/abs/2511.23366", "authors": ["Toqeer Ali Syed", "Salman Jan", "Gohar Ali", "Ali Akarma", "Ahmad Ali", "Qurat-ul-Ain Mastoi"], "title": "Agentic AI Framework for Smart Inventory Replenishment", "comment": "Presented at International Conference on Business and Digital Technology, Bahrain, Springer Nature, 27 November 2025", "summary": "In contemporary retail, the variety of products available (e.g. clothing, groceries, cosmetics, frozen goods) make it difficult to predict the demand, prevent stockouts, and find high-potential products. We suggest an agentic AI model that will be used to monitor the inventory, initiate purchase attempts to the appropriate suppliers, and scan for trending or high-margin products to incorporate. The system applies demand forecasting, supplier selection optimization, multi-agent negotiation and continuous learning. We apply a prototype to a setting in the store of a middle scale mart, test its performance on three conventional and artificial data tables, and compare the results to the base heuristics. Our findings indicate that there is a decrease in stockouts, a reduction of inventory holding costs, and an improvement in product mix turnover. We address constraints, scalability as well as improvement prospect.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u96f6\u552e\u5e93\u5b58\u7ba1\u7406\u7684\u667a\u80fd\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u9700\u6c42\u9884\u6d4b\u3001\u4f9b\u5e94\u5546\u4f18\u5316\u3001\u591a\u4ee3\u7406\u534f\u5546\u548c\u6301\u7eed\u5b66\u4e60\u6765\u51cf\u5c11\u7f3a\u8d27\u3001\u964d\u4f4e\u5e93\u5b58\u6210\u672c\u5e76\u4f18\u5316\u4ea7\u54c1\u7ec4\u5408\u3002", "motivation": "\u73b0\u4ee3\u96f6\u552e\u4e2d\u4ea7\u54c1\u79cd\u7c7b\u7e41\u591a\uff08\u670d\u88c5\u3001\u6742\u8d27\u3001\u5316\u5986\u54c1\u3001\u51b7\u51bb\u98df\u54c1\u7b49\uff09\uff0c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u9700\u6c42\u3001\u9632\u6b62\u7f3a\u8d27\u5e76\u53d1\u73b0\u9ad8\u6f5c\u529b\u4ea7\u54c1\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u5e93\u5b58\u7ba1\u7406\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fd\u4ee3\u7406AI\u6a21\u578b\uff0c\u5305\u542b\u5e93\u5b58\u76d1\u63a7\u3001\u4f9b\u5e94\u5546\u91c7\u8d2d\u542f\u52a8\u3001\u8d8b\u52bf\u4ea7\u54c1\u626b\u63cf\u529f\u80fd\uff0c\u5e94\u7528\u9700\u6c42\u9884\u6d4b\u3001\u4f9b\u5e94\u5546\u9009\u62e9\u4f18\u5316\u3001\u591a\u4ee3\u7406\u534f\u5546\u548c\u6301\u7eed\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u5728\u4e2d\u578b\u8d85\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u539f\u578b\u6d4b\u8bd5\u3002", "result": "\u5728\u4e09\u79cd\u4f20\u7edf\u548c\u4eba\u5de5\u6570\u636e\u8868\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u7cfb\u7edf\u76f8\u6bd4\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u51cf\u5c11\u7f3a\u8d27\u3001\u964d\u4f4e\u5e93\u5b58\u6301\u6709\u6210\u672c\uff0c\u5e76\u6539\u5584\u4ea7\u54c1\u7ec4\u5408\u5468\u8f6c\u7387\u3002", "conclusion": "\u8be5\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u80fd\u6709\u6548\u6539\u5584\u96f6\u552e\u5e93\u5b58\u7ba1\u7406\uff0c\u4f46\u5b58\u5728\u7ea6\u675f\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u672a\u6765\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2511.23387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23387", "abs": "https://arxiv.org/abs/2511.23387", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting", "comment": "9 pages, 4 figures", "summary": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42AI\u6c14\u8c61\u5b66\u5bb6\u7cfb\u7edf\uff0c\u4f7f\u7528LLM\u4ee3\u7406\u901a\u8fc7\u5206\u5c42\u9884\u62a5\u63a8\u7406\u548c\u5929\u6c14\u5173\u952e\u8bcd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5929\u6c14\u62a5\u544a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349\u77ed\u671f\u52a8\u6001\u548c\u957f\u671f\u8d8b\u52bf\u3002", "motivation": "\u4f20\u7edf\u5929\u6c14\u9884\u6d4b\u65b9\u6cd5\u5c06\u9884\u62a5\u89c6\u4e3a\u6241\u5e73\u65f6\u95f4\u5e8f\u5217\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5c3a\u5ea6\u5929\u6c14\u6a21\u5f0f\u7684\u7406\u89e3\u548c\u89e3\u91ca\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u5929\u6c14\u62a5\u544a\u5e76\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u5206\u5c42LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728\u5c0f\u65f6\u30016\u5c0f\u65f6\u548c\u65e5\u5c3a\u5ea6\u8fdb\u884c\u591a\u5c3a\u5ea6\u63a8\u7406\u3002\u6838\u5fc3\u63a8\u7406\u4ee3\u7406\u5c06\u7ed3\u6784\u5316\u6c14\u8c61\u8f93\u5165\u8f6c\u6362\u4e3a\u8fde\u8d2f\u53d9\u8ff0\uff0c\u540c\u65f6\u63d0\u53d6\u603b\u7ed3\u4e3b\u8981\u6c14\u8c61\u4e8b\u4ef6\u7684\u5173\u952e\u8bcd\u3002\u8fd9\u4e9b\u5173\u952e\u8bcd\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u9a8c\u8bc1\u62a5\u544a\u7684\u4e00\u81f4\u6027\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u5bf9\u9f50\u3002", "result": "\u4f7f\u7528OpenWeather\u548cMeteostat\u6570\u636e\u9a8c\u8bc1\uff0c\u5206\u5c42\u4e0a\u4e0b\u6587\u548c\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u9a8c\u8bc1\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210\u5929\u6c14\u53d9\u8ff0\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u6c14\u8c61\u62a5\u544a\u7684\u8bed\u4e49\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6846\u67b6\u3002", "conclusion": "\u5206\u5c42AI\u6c14\u8c61\u5b66\u5bb6\u7cfb\u7edf\u901a\u8fc7\u591a\u5c3a\u5ea6\u63a8\u7406\u548c\u5173\u952e\u8bcd\u9a8c\u8bc1\uff0c\u63d0\u5347\u4e86LLM\u751f\u6210\u5929\u6c14\u62a5\u544a\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u57fa\u4e8e\u4ee3\u7406\u7684\u79d1\u5b66\u63a8\u7406\u548c\u81ea\u52a8\u5316\u6c14\u8c61\u62a5\u544a\u8bed\u4e49\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2511.21752", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21752", "abs": "https://arxiv.org/abs/2511.21752", "authors": ["Yanxi Li", "Ruocheng Shan"], "title": "Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification", "comment": null, "summary": "Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.", "AI": {"tldr": "\u63d0\u51faLabel Disguise Defense(LDD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u771f\u5b9e\u6807\u7b7e\u66ff\u6362\u4e3a\u8bed\u4e49\u8f6c\u6362\u6216\u65e0\u5173\u7684\u522b\u540d\u6807\u7b7e\u6765\u9632\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u5bb9\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u7279\u522b\u662f\u5229\u7528\u6a21\u578b\u6807\u7b7e\u96c6\u77e5\u8bc6\u7684\u7c7b\u522b\u5bfc\u5411\u6ce8\u5165\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u8981\u4e48\u5bb9\u6613\u53d7\u5230\u6df7\u6dc6\u653b\u51fb\u3002", "method": "\u63d0\u51faLabel Disguise Defense(LDD)\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6a21\u578b\u65e0\u5173\u7684\u7b56\u7565\u3002\u901a\u8fc7\u5c06\u771f\u5b9e\u6807\u7b7e\u66ff\u6362\u4e3a\u8bed\u4e49\u8f6c\u6362\u6216\u65e0\u5173\u7684\u522b\u540d\u6807\u7b7e\uff08\u5982\"\u84dd\u8272\"vs\"\u9ec4\u8272\"\uff09\uff0c\u6a21\u578b\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u6f14\u793a\u9690\u5f0f\u5b66\u4e60\u8fd9\u4e9b\u65b0\u6807\u7b7e\u6620\u5c04\uff0c\u9632\u6b62\u6ce8\u5165\u6307\u4ee4\u4e0e\u51b3\u7b56\u8f93\u51fa\u4e4b\u95f4\u7684\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u57289\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5305\u62ecGPT-5\u3001GPT-4o\u3001LLaMA3.2\u3001Gemma3\u548cMistral\u53d8\u4f53\uff09\u4e0a\u8bc4\u4f30LDD\u3002\u7ed3\u679c\u663e\u793a\uff0cLDD\u6062\u590d\u56e0\u5bf9\u6297\u653b\u51fb\u800c\u635f\u5931\u7684\u6027\u80fd\u80fd\u529b\u56e0\u6a21\u578b\u548c\u522b\u540d\u9009\u62e9\u800c\u5f02\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u8bc4\u4f30\u7684\u6a21\u578b\uff0cLDD\u90fd\u80fd\u6062\u590d\u90e8\u5206\u56e0\u653b\u51fb\u800c\u964d\u4f4e\u7684\u51c6\u786e\u7387\u3002\u5bf9\u4e8e\u7edd\u5927\u591a\u6570\u6a21\u578b\uff0c\u53ef\u4ee5\u627e\u5230\u591a\u4e2a\u522b\u540d\u5bf9\uff0c\u5176\u51c6\u786e\u7387\u9ad8\u4e8e\u4ec5\u4f9d\u8d56\u5c11\u91cf\u5b66\u4e60\u800c\u65e0\u9632\u5fa1\u673a\u5236\u7684\u53d7\u653b\u51fb\u57fa\u7ebf\u3002\u8bed\u4e49\u5bf9\u9f50\u7684\u522b\u540d\u6807\u7b7e\u6bd4\u672a\u5bf9\u9f50\u7684\u7b26\u53f7\u6807\u7b7e\u63d0\u4f9b\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6807\u7b7e\u8bed\u4e49\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u9632\u5fa1\u5c42\uff0c\u5c06\u610f\u4e49\u672c\u8eab\u8f6c\u5316\u4e3a\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u7684\u76fe\u724c\u3002Label Disguise Defense\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u9690\u85cf\u771f\u5b9e\u6807\u7b7e\u6765\u62b5\u5fa1\u7c7b\u522b\u5bfc\u5411\u6ce8\u5165\u653b\u51fb\u3002"}}
{"id": "2511.22199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22199", "abs": "https://arxiv.org/abs/2511.22199", "authors": ["Sejeong Jang", "Joo Heung Yoon", "Hyo Kyung Lee"], "title": "PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units", "comment": null, "summary": "Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.", "AI": {"tldr": "PULSE-ICU\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u5d4c\u5165\u6a21\u5757\u548cLongformer\u7f16\u7801\u5668\u5904\u7406\u4e0d\u89c4\u5219ICU\u6570\u636e\uff0c\u572818\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u9886\u57df\u9002\u5e94\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "ICU\u6570\u636e\u5177\u6709\u9ad8\u5ea6\u4e0d\u89c4\u5219\u6027\u3001\u5f02\u8d28\u6027\u548c\u65f6\u95f4\u788e\u7247\u5316\u7684\u7279\u70b9\uff0c\u8fd9\u7ed9\u4e34\u5e8a\u9884\u6d4b\u7684\u6cdb\u5316\u5e26\u6765\u4e86\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u91cd\u91c7\u6837\u6216\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u4e34\u5e8a\u73af\u5883\u3002", "method": "\u63d0\u51faPULSE-ICU\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff1a1\uff09\u7edf\u4e00\u5d4c\u5165\u6a21\u5757\u7f16\u7801\u4e8b\u4ef6\u8eab\u4efd\u3001\u8fde\u7eed\u503c\u3001\u5355\u4f4d\u548c\u65f6\u95f4\u5c5e\u6027\uff1b2\uff09\u57fa\u4e8eLongformer\u7684\u7f16\u7801\u5668\u9ad8\u6548\u5efa\u6a21\u957f\u8f68\u8ff9\uff1b3\uff09\u65e0\u9700\u91cd\u91c7\u6837\u6216\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u76f4\u63a5\u4ece\u5927\u89c4\u6a21EHR\u5e8f\u5217\u5b66\u4e60\u4e8b\u4ef6\u7ea7ICU\u8868\u793a\u3002", "result": "\u572818\u4e2a\u9884\u6d4b\u4efb\u52a1\uff08\u5305\u62ec\u6b7b\u4ea1\u7387\u3001\u5e72\u9884\u9884\u6d4b\u548c\u8868\u578b\u8bc6\u522b\uff09\u4e0a\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u5f02\u3002\u5728eICU\u3001HiRID\u548cP12\u6570\u636e\u96c6\u7684\u5916\u90e8\u9a8c\u8bc1\u4e2d\uff0c\u4ec5\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u663e\u793a\u51fa\u5bf9\u9886\u57df\u504f\u79fb\u548c\u53d8\u91cf\u7ea6\u675f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u4e2d\u7684ICU\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2511.22237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22237", "abs": "https://arxiv.org/abs/2511.22237", "authors": ["Qi Song", "Ziyuan Luo", "Renjie Wan"], "title": "Creating Blank Canvas Against AI-enabled Image Forgery", "comment": "Accepted by AAAI 2026", "summary": "AIGC-based image editing technology has greatly simplified the realistic-level image modification, causing serious potential risks of image forgery. This paper introduces a new approach to tampering detection using the Segment Anything Model (SAM). Instead of training SAM to identify tampered areas, we propose a novel strategy. The entire image is transformed into a blank canvas from the perspective of neural models. Any modifications to this blank canvas would be noticeable to the models. To achieve this idea, we introduce adversarial perturbations to prevent SAM from ``seeing anything'', allowing it to identify forged regions when the image is tampered with. Due to SAM's powerful perceiving capabilities, naive adversarial attacks cannot completely tame SAM. To thoroughly deceive SAM and make it blind to the image, we introduce a frequency-aware optimization strategy, which further enhances the capability of tamper localization. Extensive experimental results demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSAM\u7684\u7be1\u6539\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6270\u52a8\u4f7fSAM\"\u770b\u4e0d\u89c1\"\u539f\u59cb\u56fe\u50cf\uff0c\u4ece\u800c\u5728\u56fe\u50cf\u88ab\u7be1\u6539\u65f6\u8bc6\u522b\u4f2a\u9020\u533a\u57df", "motivation": "AIGC\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u7b80\u5316\u4e86\u903c\u771f\u56fe\u50cf\u4fee\u6539\uff0c\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u56fe\u50cf\u4f2a\u9020\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u7be1\u6539\u68c0\u6d4b\u65b9\u6cd5", "method": "1) \u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u795e\u7ecf\u6a21\u578b\u7684\"\u7a7a\u767d\u753b\u5e03\"\uff1b2) \u5f15\u5165\u5bf9\u6297\u6270\u52a8\u4f7fSAM\u65e0\u6cd5\"\u770b\u89c1\"\u539f\u59cb\u5185\u5bb9\uff1b3) \u91c7\u7528\u9891\u7387\u611f\u77e5\u4f18\u5316\u7b56\u7565\u5f7b\u5e95\u6b3a\u9a97SAM\uff1b4) \u5f53\u56fe\u50cf\u88ab\u7be1\u6539\u65f6\uff0cSAM\u80fd\u8bc6\u522b\u4f2a\u9020\u533a\u57df", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7be1\u6539\u5b9a\u4f4d\u65b9\u9762\u5177\u6709\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u4f7fSAM\u5bf9\u539f\u59cb\u56fe\u50cf\"\u5931\u660e\"\u7684\u65b0\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u7be1\u6539\u68c0\u6d4b\uff0c\u4e3a\u89e3\u51b3AIGC\u5e26\u6765\u7684\u56fe\u50cf\u4f2a\u9020\u98ce\u9669\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2511.22036", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22036", "abs": "https://arxiv.org/abs/2511.22036", "authors": ["Jingjun Xu", "Chongshan Lin", "Haofei Yu", "Tao Feng", "Jiaxuan You"], "title": "ResearchArcade: Graph Interface for Academic Tasks", "comment": null, "summary": "Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.", "AI": {"tldr": "ResearchArcade\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u7edf\u4e00\u6570\u636e\u63a5\u53e3\uff0c\u8fde\u63a5\u591a\u4e2a\u5b66\u672f\u6570\u636e\u6e90\uff0c\u652f\u6301\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u7528\u4e8e\u5f00\u53d1\u5404\u79cd\u5b66\u672f\u4efb\u52a1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5b66\u672f\u7814\u7a76\u4ea7\u751f\u591a\u6837\u5316\u7684\u6570\u636e\u6e90\uff0c\u7814\u7a76\u4eba\u5458\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u7814\u7a76\u4efb\u52a1\uff0c\u9700\u8981\u6784\u5efa\u7edf\u4e00\u7684\u6570\u636e\u63a5\u53e3\u6765\u652f\u6301\u5f00\u53d1\u5404\u79cd\u5b66\u672f\u4efb\u52a1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u52a0\u901f\u77e5\u8bc6\u53d1\u73b0\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u63a5\u53e3\uff0c\u4f7f\u7528\u4e00\u81f4\u7684\u591a\u8868\u683c\u5f0f\u548c\u56fe\u7ed3\u6784\u7ec4\u7ec7\u4e0d\u540c\u6765\u6e90\u7684\u6570\u636e\uff08\u5982ArXiv\u548cOpenReview\uff09\uff0c\u5305\u542b\u6587\u672c\u3001\u56fe\u8868\u7b49\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4fdd\u7559\u624b\u7a3f\u548c\u793e\u533a\u5c42\u9762\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u7edf\u4e00\u5b66\u672f\u4efb\u52a1\u5b9a\u4e49\u5e76\u652f\u6301\u591a\u79cd\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u516d\u4e2a\u5b66\u672f\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u8de8\u6e90\u548c\u591a\u6a21\u6001\u4fe1\u606f\u80fd\u591f\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u8303\u56f4\uff0c\u800c\u56fe\u7ed3\u6784\u7684\u52a0\u5165\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "ResearchArcade\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u5177\u6709\u63a8\u52a8\u7814\u7a76\u8fdb\u5c55\u7684\u6f5c\u529b\uff0c\u4e3a\u5b66\u672f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u636e\u63a5\u53e3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22245", "abs": "https://arxiv.org/abs/2511.22245", "authors": ["Seoyun Yang", "Gihoon Kim", "Taesup Kim"], "title": "Semantic Anchoring for Robust Personalization in Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image diffusion models have achieved remarkable progress in generating diverse and realistic images from textual descriptions. However, they still struggle with personalization, which requires adapting a pretrained model to depict user-specific subjects from only a few reference images. The key challenge lies in learning a new visual concept from a limited number of reference images while preserving the pretrained semantic prior that maintains text-image alignment. When the model focuses on subject fidelity, it tends to overfit the limited reference images and fails to leverage the pretrained distribution. Conversely, emphasizing prior preservation maintains semantic consistency but prevents the model from learning new personalized attributes. Building on these observations, we propose the personalization process through a semantic anchoring that guides adaptation by grounding new concepts in their corresponding distributions. We therefore reformulate personalization as the process of learning a rare concept guided by its frequent counterpart through semantic anchoring. This anchoring encourages the model to adapt new concepts in a stable and controlled manner, expanding the pretrained distribution toward personalized regions while preserving its semantic structure. As a result, the proposed method achieves stable adaptation and consistent improvements in both subject fidelity and text-image alignment compared to baseline methods. Extensive experiments and ablation studies further demonstrate the robustness and effectiveness of the proposed anchoring strategy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u951a\u5b9a\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65b0\u6982\u5ff5\u951a\u5b9a\u5230\u9884\u8bad\u7ec3\u5206\u5e03\u4e2d\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b66\u4e60\u7528\u6237\u7279\u5b9a\u7684\u89c6\u89c9\u6982\u5ff5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u4e2a\u6027\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff1a\u4ece\u5c11\u91cf\u53c2\u8003\u56fe\u50cf\u5b66\u4e60\u65b0\u89c6\u89c9\u6982\u5ff5\u65f6\uff0c\u8981\u4e48\u8fc7\u5ea6\u62df\u5408\u53c2\u8003\u56fe\u50cf\u800c\u5931\u53bb\u9884\u8bad\u7ec3\u8bed\u4e49\u5148\u9a8c\uff0c\u8981\u4e48\u5f3a\u8c03\u5148\u9a8c\u4fdd\u6301\u800c\u65e0\u6cd5\u5b66\u4e60\u65b0\u7684\u4e2a\u6027\u5316\u5c5e\u6027\u3002\u9700\u8981\u5728\u4fdd\u6301\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u7684\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u7684\u4e2a\u6027\u5316\u9002\u5e94\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u951a\u5b9a\u7b56\u7565\uff0c\u5c06\u4e2a\u6027\u5316\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u8bed\u4e49\u951a\u5b9a\u5728\u9891\u7e41\u5bf9\u5e94\u6982\u5ff5\u7684\u6307\u5bfc\u4e0b\u5b66\u4e60\u7f55\u89c1\u6982\u5ff5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u65b0\u6982\u5ff5\u951a\u5b9a\u5230\u5176\u5bf9\u5e94\u5206\u5e03\u4e2d\u6765\u5f15\u5bfc\u6a21\u578b\u9002\u5e94\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ee5\u7a33\u5b9a\u53ef\u63a7\u7684\u65b9\u5f0f\u9002\u5e94\u65b0\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u9002\u5e94\uff0c\u5e76\u5728\u4e3b\u4f53\u4fdd\u771f\u5ea6\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6240\u63d0\u951a\u5b9a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8bed\u4e49\u951a\u5b9a\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u6307\u5bfc\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u8fc7\u7a0b\uff0c\u5728\u6269\u5c55\u9884\u8bad\u7ec3\u5206\u5e03\u5230\u4e2a\u6027\u5316\u533a\u57df\u7684\u540c\u65f6\u4fdd\u6301\u5176\u8bed\u4e49\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e3b\u4f53\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2511.22038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22038", "abs": "https://arxiv.org/abs/2511.22038", "authors": ["Rochana Chaturvedi", "Yue Zhou", "Andrew Boyd", "Brian T. Layden", "Mudassir Rashid", "Lu Cheng", "Ali Cinar", "Barbara Di Eugenio"], "title": "Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing", "comment": null, "summary": "Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.", "AI": {"tldr": "\u63d0\u51fa\u4e86HiTGNN\u548cReVeAL\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u4e34\u5e8a\u7b14\u8bb0\u8fdb\u884c\u65f6\u95f4\u4e0a\u4e0b\u6587\u98ce\u9669\u9884\u6d4b\uff0c\u7279\u522b\u9488\u5bf92\u578b\u7cd6\u5c3f\u75c5\u7684\u65e9\u671f\u7b5b\u67e5\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u65f6\u5e8f\u4fe1\u606f\u548c\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7f3a\u5931\u7684\u4e8b\u4ef6\u7ec6\u8282\uff0c\u53ef\u7528\u4e8e\u6162\u6027\u75be\u75c5\u7684\u53ca\u65f6\u8bc6\u522b\uff0c\u4f46\u9762\u4e34\u6587\u672c\u957f\u3001\u4e8b\u4ef6\u5206\u5e03\u4e0d\u89c4\u5219\u3001\u65f6\u95f4\u4f9d\u8d56\u590d\u6742\u3001\u9690\u79c1\u9650\u5236\u548c\u8d44\u6e90\u6709\u9650\u7b49NLP\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1) HiTGNN\uff1a\u5206\u5c42\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u6574\u5408\u7b14\u8bb0\u5185\u4e8b\u4ef6\u7ed3\u6784\u3001\u5c31\u8bca\u95f4\u52a8\u6001\u548c\u533b\u5b66\u77e5\u8bc6\uff0c\u4ee5\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7c92\u5ea6\u5efa\u6a21\u60a3\u8005\u8f68\u8ff9\uff1b2) ReVeAL\uff1a\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u8f83\u5c0f\u7684\u9a8c\u8bc1\u5668\u6a21\u578b\u4e2d\u3002", "result": "\u57282\u578b\u7cd6\u5c3f\u75c5\u673a\u4f1a\u6027\u7b5b\u67e5\u4e2d\uff0cHiTGNN\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5bf9\u8fd1\u671f\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u5e76\u51cf\u5c11\u5bf9\u5927\u6a21\u578b\u7684\u4f9d\u8d56\uff1bReVeAL\u63d0\u9ad8\u4e86\u5bf9\u771f\u5b9e\u75c5\u4f8b\u7684\u654f\u611f\u6027\u5e76\u4fdd\u7559\u89e3\u91ca\u6027\u63a8\u7406\uff1b\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u4e86\u65f6\u95f4\u7ed3\u6784\u548c\u77e5\u8bc6\u589e\u5f3a\u7684\u4ef7\u503c\uff0c\u516c\u5e73\u6027\u5206\u6790\u663e\u793aHiTGNN\u5728\u5404\u4e9a\u7ec4\u4e2d\u8868\u73b0\u66f4\u516c\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u4e34\u5e8a\u7b14\u8bb0\u8fdb\u884c\u65f6\u5e8f\u98ce\u9669\u9884\u6d4b\u7684\u6311\u6218\uff0cHiTGNN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cReVeAL\u589e\u5f3a\u4e86\u654f\u611f\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6162\u6027\u75be\u75c5\u7684\u65e9\u671f\u7b5b\u67e5\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9690\u79c1\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22249", "abs": "https://arxiv.org/abs/2511.22249", "authors": ["Bolin Lai", "Xudong Wang", "Saketh Rambhatla", "James M. Rehg", "Zsolt Kira", "Rohit Girdhar", "Ishan Misra"], "title": "Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective", "comment": "11 pages, 7 figures, 4 tables", "summary": "Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.", "AI": {"tldr": "FreqWarm\uff1a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9891\u7387\u9884\u70ed\u8bfe\u7a0b\uff0c\u901a\u8fc7\u589e\u52a0\u6269\u6563\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5\u5bf9\u9ad8\u9891\u6f5c\u5728\u4fe1\u53f7\u7684\u66b4\u9732\uff0c\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u4e2d\u91cd\u5efa\u4e0e\u751f\u6210\u7684\u6743\u8861\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u6216\u91cd\u65b0\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u3002", "motivation": "\u6f5c\u5728\u6269\u6563\u5df2\u6210\u4e3a\u89c6\u89c9\u751f\u6210\u7684\u9ed8\u8ba4\u8303\u5f0f\uff0c\u4f46\u89c2\u5bdf\u5230\u968f\u7740\u6f5c\u5728\u7ef4\u5ea6\u589e\u52a0\uff0c\u91cd\u5efa\u4e0e\u751f\u6210\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u6743\u8861\uff1a\u66f4\u9ad8\u5bb9\u91cf\u7684\u81ea\u7f16\u7801\u5668\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u4f46\u751f\u6210\u8d28\u91cf\u6700\u7ec8\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u9ad8\u9891\u7f16\u7801\u548c\u89e3\u7801\u884c\u4e3a\u5dee\u5f02\u5bfc\u81f4\u7684\u3002", "method": "\u901a\u8fc7RGB\u548c\u6f5c\u5728\u57df\u4e2d\u7684\u53d7\u63a7\u6270\u52a8\u5206\u6790\u7f16\u7801\u5668/\u89e3\u7801\u5668\u884c\u4e3a\uff0c\u53d1\u73b0\u89e3\u7801\u5668\u5f3a\u70c8\u4f9d\u8d56\u9ad8\u9891\u6f5c\u5728\u5206\u91cf\u6765\u6062\u590d\u7ec6\u8282\uff0c\u800c\u7f16\u7801\u5668\u5bf9\u9ad8\u9891\u5185\u5bb9\u8868\u793a\u4e0d\u8db3\u3002\u4e3a\u6b64\u63d0\u51faFreqWarm\uff0c\u4e00\u79cd\u9891\u7387\u9884\u70ed\u8bfe\u7a0b\uff0c\u5728\u6269\u6563\u6216\u6d41\u5339\u914d\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5\u589e\u52a0\u5bf9\u9ad8\u9891\u6f5c\u5728\u4fe1\u53f7\u7684\u66b4\u9732\uff0c\u65e0\u9700\u4fee\u6539\u81ea\u7f16\u7801\u5668\u3002", "result": "\u5728\u591a\u4e2a\u9ad8\u7ef4\u81ea\u7f16\u7801\u5668\u4e0a\u5e94\u7528FreqWarm\uff0c\u4e00\u81f4\u6539\u5584\u751f\u6210\u8d28\u91cf\uff1a\u5728Wan2.2-VAE\u4e0agFID\u964d\u4f4e14.11\uff0c\u5728LTX-VAE\u4e0a\u964d\u4f4e6.13\uff0c\u5728DC-AE-f32\u4e0a\u964d\u4f4e4.42\u3002\u8be5\u65b9\u6cd5\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u517c\u5bb9\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u663e\u5f0f\u7ba1\u7406\u9891\u7387\u66b4\u9732\u53ef\u4ee5\u6210\u529f\u5730\u5c06\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8f6c\u5316\u4e3a\u66f4\u53ef\u6269\u6563\u7684\u76ee\u6807\uff0cFreqWarm\u4e3a\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u4e2d\u7684\u91cd\u5efa-\u751f\u6210\u6743\u8861\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22176", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22176", "abs": "https://arxiv.org/abs/2511.22176", "authors": ["Lukas Struppek", "Dominik Hintersdorf", "Hannah Struppek", "Daniel Neider", "Kristian Kersting"], "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information", "comment": null, "summary": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.", "AI": {"tldr": "Focused Chain-of-Thought (F-CoT) \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u8f93\u5165\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u4fe1\u606f\u63d0\u53d6\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u67e5\u8be2\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u7ec4\u7ec7\u6210\u7b80\u6d01\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u51cf\u5c11\u63a8\u7406\u8def\u5f84\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5c06\u751f\u6210token\u51cf\u5c112-3\u500d\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u8be6\u7ec6\u7684\u601d\u7ef4\u94fe\u5b9e\u73b0\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u5bfc\u81f4\u8fc7\u591a\u7684token\u4f7f\u7528\u548c\u8f83\u9ad8\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u73b0\u6709\u6548\u7387\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u6a21\u578b\u4e2d\u5fc3\u7684\u5e72\u9884\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u6216\u76d1\u7763\u5fae\u8c03\uff09\u6765\u51cf\u5c11\u5197\u4f59\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u8f93\u5165\u4e2d\u5fc3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFocused Chain-of-Thought (F-CoT)\uff0c\u53d7\u8ba4\u77e5\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u5c06\u4fe1\u606f\u63d0\u53d6\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\u3002\u9996\u5148\u4ece\u67e5\u8be2\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u7ec4\u7ec7\u6210\u7b80\u6d01\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u5f15\u5bfc\u6a21\u578b\u4ec5\u5728\u6b64\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u907f\u514d\u5173\u6ce8\u65e0\u5173\u7ec6\u8282\uff0c\u81ea\u7136\u4ea7\u751f\u66f4\u77ed\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u7b97\u672f\u6587\u5b57\u95ee\u9898\u4e0a\uff0cF-CoT\u5c06\u751f\u6210\u7684token\u51cf\u5c112-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002\u7ed3\u6784\u5316\u8f93\u5165\u88ab\u8bc1\u660e\u662f\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u6760\u6746\u3002", "conclusion": "F-CoT\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u8f93\u5165\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11token\u4f7f\u7528\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.22404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22404", "abs": "https://arxiv.org/abs/2511.22404", "authors": ["Longkun Zou", "Jiale Wang", "Rongqin Liang", "Hai Wu", "Ke Chen", "Yaowei Wang"], "title": "UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data", "comment": null, "summary": "Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.", "AI": {"tldr": "UAV-MM3D\u662f\u4e00\u4e2a\u7528\u4e8e\u4f4e\u7a7a\u65e0\u4eba\u673a\u611f\u77e5\u7684\u9ad8\u4fdd\u771f\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b40\u4e07\u5e27\u540c\u6b65\u6570\u636e\uff0c\u6db5\u76d6\u591a\u79cd\u573a\u666f\u3001\u5929\u6c14\u6761\u4ef6\u548c\u65e0\u4eba\u673a\u6a21\u578b\uff0c\u63d0\u4f9b\u4e94\u79cd\u4f20\u611f\u5668\u6a21\u6001\u548c\u4e30\u5bcc\u6807\u6ce8\uff0c\u7528\u4e8e3D\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u3001\u8ddf\u8e2a\u548c\u8f68\u8ff9\u9884\u6d4b\u7b49\u4efb\u52a1\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u6570\u636e\u6536\u96c6\u9762\u4e34\u7a7a\u57df\u7ba1\u5236\u3001\u9690\u79c1\u95ee\u9898\u548c\u73af\u5883\u53d8\u5316\u7b49\u9650\u5236\uff0c\u800c\u624b\u52a8\u6807\u6ce83D\u59ff\u6001\u548c\u8de8\u6a21\u6001\u5bf9\u5e94\u5173\u7cfb\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u56e0\u6b64\u9700\u8981\u5927\u89c4\u6a21\u3001\u51c6\u786e\u6807\u6ce8\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6765\u63a8\u52a8\u65e0\u4eba\u673a\u611f\u77e5\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u53ef\u63a7\u4eff\u771f\u73af\u5883\u521b\u5efaUAV-MM3D\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b40\u4e07\u5e27\u540c\u6b65\u6570\u636e\uff0c\u6db5\u76d6\u57ce\u5e02\u3001\u90ca\u533a\u3001\u68ee\u6797\u3001\u6d77\u5cb8\u7b49\u591a\u79cd\u573a\u666f\u548c\u5929\u6c14\u6761\u4ef6\uff0c\u63d0\u4f9bRGB\u3001\u7ea2\u5916\u3001LiDAR\u3001\u96f7\u8fbe\u548cDVS\u4e94\u79cd\u4f20\u611f\u5668\u6a21\u6001\uff0c\u5e76\u5305\u542b2D/3D\u8fb9\u754c\u6846\u30016\u81ea\u7531\u5ea6\u59ff\u6001\u548c\u5b9e\u4f8b\u7ea7\u6807\u6ce8\u3002", "result": "\u5f00\u53d1\u4e86UAV-MM3D\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faLGFusionNet\uff08LiDAR\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u57fa\u7ebf\uff09\u548c\u4e13\u7528\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u9884\u6d4b\u57fa\u7ebf\uff0c\u4e3a\u65e0\u4eba\u673a3D\u611f\u77e5\u548c\u8fd0\u52a8\u7406\u89e3\u63d0\u4f9b\u4e86\u516c\u5f00\u57fa\u51c6\u3002", "conclusion": "UAV-MM3D\u901a\u8fc7\u5176\u53ef\u63a7\u4eff\u771f\u73af\u5883\u3001\u5168\u9762\u7684\u573a\u666f\u8986\u76d6\u548c\u4e30\u5bcc\u6807\u6ce8\uff0c\u4e3a\u63a8\u8fdb\u65e0\u4eba\u673a3D\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u516c\u5171\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u514b\u670d\u771f\u5b9e\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u7684\u6311\u6218\u3002"}}
{"id": "2511.22425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22425", "abs": "https://arxiv.org/abs/2511.22425", "authors": ["Minghao Yin", "Yukang Cao", "Kai Han"], "title": "Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models", "comment": null, "summary": "We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.", "AI": {"tldr": "WUKONG\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u4fdd\u771f\u7eb9\u74063D\u53d8\u5f62\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u6d41\u7684transformer\u751f\u6210\u5148\u9a8c\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u91cd\u5fc3\u95ee\u9898\u5b9e\u73b0\u5e73\u6ed1\u5f62\u72b6\u8fc7\u6e21\uff0c\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u673a\u5236\u4fdd\u6301\u7eb9\u7406\u7ec6\u8282\u3002", "motivation": "\u4f20\u7edf3D\u53d8\u5f62\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u5bf9\u5e94\u5339\u914d\u548c\u53d8\u5f62\u8f68\u8ff9\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u4e14\u9700\u8981\u6602\u8d35\u7684\u9884\u5904\u7406\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf3D\u53d8\u5f62\u5e76\u4fdd\u6301\u7eb9\u7406\u7ec6\u8282\u7684\u65b9\u6cd5\u3002", "method": "1) \u5229\u7528\u57fa\u4e8e\u6d41\u7684transformer\u751f\u6210\u5148\u9a8c\uff1b2) \u5c06\u53d8\u5f62\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u91cd\u5fc3\u95ee\u9898\u786e\u4fdd\u5e73\u6ed1\u5f62\u72b6\u8fc7\u6e21\uff1b3) \u5f15\u5165\u5e8f\u5217\u521d\u59cb\u5316\u7b56\u7565\u9632\u6b62\u51e0\u4f55\u7578\u53d8\uff1b4) \u63d0\u51fa\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u673a\u5236\u9009\u62e9\u6027\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u548c\u63a7\u5236\u6df7\u5408\u52a8\u6001\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0cWUKONG\u5728\u591a\u6837\u5316\u7684\u51e0\u4f55\u548c\u7eb9\u7406\u53d8\u5316\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u53d8\u5f62\u7ed3\u679c\u3002", "conclusion": "WUKONG\u6846\u67b6\u901a\u8fc7\u5229\u7528\u751f\u6210\u5148\u9a8c\u548c\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u4fdd\u771f3D\u53d8\u5f62\uff0c\u5728\u4fdd\u6301\u7eb9\u7406\u7ec6\u8282\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.22456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22456", "abs": "https://arxiv.org/abs/2511.22456", "authors": ["Zhenglin Zhou", "Fan Ma", "Xiaobo Xia", "Hehe Fan", "Yi Yang", "Tat-Seng Chua"], "title": "ITS3D: Inference-Time Scaling for Text-Guided 3D Diffusion Models", "comment": "25 pages, 11 figures", "summary": "We explore inference-time scaling in text-guided 3D diffusion models to enhance generative quality without additional training. To this end, we introduce ITS3D, a framework that formulates the task as an optimization problem to identify the most effective Gaussian noise input. The framework is driven by a verifier-guided search algorithm, where the search algorithm iteratively refines noise candidates based on verifier feedback. To address the inherent challenges of 3D generation, we introduce three techniques for improved stability, efficiency, and exploration capability. 1) Gaussian normalization is applied to stabilize the search process. It corrects distribution shifts when noise candidates deviate from a standard Gaussian distribution during iterative updates. 2) The high-dimensional nature of the 3D search space increases computational complexity. To mitigate this, a singular value decomposition-based compression technique is employed to reduce dimensionality while preserving effective search directions. 3) To further prevent convergence to suboptimal local minima, a singular space reset mechanism dynamically updates the search space based on diversity measures. Extensive experiments demonstrate that ITS3D enhances text-to-3D generation quality, which shows the potential of computationally efficient search methods in generative processes. The source code is available at https://github.com/ZhenglinZhou/ITS3D.", "AI": {"tldr": "ITS3D\u662f\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u7f29\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u9ad8\u65af\u566a\u58f0\u8f93\u5165\u6765\u63d0\u5347\u6587\u672c\u5f15\u5bfc3D\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u91c7\u7528\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u641c\u7d22\u7b97\u6cd5\u548c\u4e09\u79cd\u6280\u672f\u6539\u8fdb\u7a33\u5b9a\u6027\u3001\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u7f29\u653e\u6765\u63d0\u5347\u6587\u672c\u5f15\u5bfc3D\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b33D\u751f\u6210\u4e2d\u7684\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002", "method": "\u63d0\u51faITS3D\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u4f18\u5316\u95ee\u9898\u4ee5\u5bfb\u627e\u6700\u4f18\u9ad8\u65af\u566a\u58f0\u8f93\u5165\u3002\u91c7\u7528\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u8fed\u4ee3\u4f18\u5316\u566a\u58f0\u5019\u9009\u3002\u5f15\u5165\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u9ad8\u65af\u5f52\u4e00\u5316\u7a33\u5b9a\u641c\u7d22\u8fc7\u7a0b\uff1b2) \u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u7684\u538b\u7f29\u6280\u672f\u964d\u4f4e\u9ad8\u7ef4\u641c\u7d22\u7a7a\u95f4\u590d\u6742\u5ea6\uff1b3) \u5947\u5f02\u7a7a\u95f4\u91cd\u7f6e\u673a\u5236\u9632\u6b62\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eITS3D\u80fd\u591f\u663e\u8457\u63d0\u5347\u6587\u672c\u52303D\u751f\u6210\u7684\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u8ba1\u7b97\u9ad8\u6548\u641c\u7d22\u65b9\u6cd5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "ITS3D\u901a\u8fc7\u63a8\u7406\u65f6\u4f18\u5316\u566a\u58f0\u8f93\u5165\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u751f\u6210\u8d28\u91cf\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.23225", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23225", "abs": "https://arxiv.org/abs/2511.23225", "authors": ["Guang Liang", "Jie Shao", "Ningyuan Tang", "Xinyao Liu", "Jianxin Wu"], "title": "TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies", "comment": null, "summary": "Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.", "AI": {"tldr": "TWEO\u901a\u8fc7\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\u9879\u6d88\u9664Transformer\u8bad\u7ec3\u4e2d\u7684\u6781\u7aef\u5f02\u5e38\u503c\uff0c\u5b9e\u73b0\u65e0\u9700\u5de5\u7a0b\u6280\u5de7\u7684\u5168\u6a21\u578bFP8\u8bad\u7ec3\uff0c\u5e76\u9996\u6b21\u4f7fW8A8\u9759\u6001\u91cf\u5316\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u786c\u4ef6\u539f\u751f\u652f\u6301FP8\u5bf9\u4e8e\u8bad\u7ec3\u5927\u578bTransformer\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6781\u7aef\u6fc0\u6d3b\u5f02\u5e38\u503c\u4e25\u91cd\u963b\u788d\u4e86\u5176\u5e94\u7528\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f9d\u8d56\u590d\u6742\u7684\u6df7\u5408\u7cbe\u5ea6\u5de5\u7a0b\uff0c\u8981\u4e48\u9700\u8981\u4fb5\u5165\u5f0f\u67b6\u6784\u4fee\u6539\u3002", "method": "TWEO\uff08Transformers Without Extreme Outliers\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u975e\u4fb5\u5165\u5f0f\u635f\u5931\u51fd\u6570\u3002\u5b83\u57fa\u4e8e\u4e00\u4e2a\u5173\u952e\u53d1\u73b0\uff1a\u6781\u7aef\u5f02\u5e38\u503c\u4e0d\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u800c\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7531\u6743\u91cd\u77e9\u9635\u7684\u7279\u5b9a\u7ed3\u6784\u7279\u6027\uff08\u5373\u5171\u7ebf\u6027\uff09\u4ea7\u751f\u7684\u673a\u68b0\u6027\u4f2a\u5f71\u3002TWEO\u901a\u8fc7\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u635f\u5931\u9879\u6709\u6548\u9632\u6b62\u6781\u7aef\u5f02\u5e38\u503c\u3002", "result": "TWEO\u5c06\u5f02\u5e38\u503c\u4ece10000+\u51cf\u5c11\u5230\u5c0f\u4e8e20\uff0c\u5b9e\u73b0\u5168\u6a21\u578bFP8\u9884\u8bad\u7ec3\uff08LLM\u548cViT\uff09\uff0c\u6027\u80fd\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534736%\u3002\u540c\u65f6\u4f7f\u786c\u4ef6\u53cb\u597d\u7684W8A8\u6bcf\u5f20\u91cf\u9759\u6001\u91cf\u5316\u9996\u6b21\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "TWEO\u901a\u8fc7\u6311\u6218\u4f20\u7edf\u8ba4\u77e5\uff0c\u63ed\u793a\u4e86\u5f02\u5e38\u503c\u7684\u673a\u68b0\u6027\u672c\u8d28\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3aFP8\u8bad\u7ec3\u548c\u65b0\u578b\u91cf\u5316\u8303\u5f0f\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u65e0\u9700\u5de5\u7a0b\u6280\u5de7\u6216\u67b6\u6784\u4fee\u6539\u3002"}}
{"id": "2511.23162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23162", "abs": "https://arxiv.org/abs/2511.23162", "authors": ["Anders Vestergaard N\u00f8rskov", "Kasper J\u00f8rgensen", "Alexander Neergaard Zahid", "Morten M\u00f8rup"], "title": "Estimating the Event-Related Potential from Few EEG Trials", "comment": "Accepted by Transactions on Machine Learning Research (TMLR). 15 pages main manuscript, 30 pages total including supplementary material", "summary": "Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP", "AI": {"tldr": "EEG2ERP\u662f\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u7f16\u7801\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u4efb\u610f\u6570\u91cf\u7684EEG\u8bd5\u6b21\u6620\u5c04\u5230\u76f8\u5e94\u7684ERP\uff0c\u663e\u8457\u51cf\u5c11\u4f20\u7edf\u5e73\u5747\u65b9\u6cd5\u6240\u9700\u7684\u8bd5\u6b21\u6570\u3002", "motivation": "\u4f20\u7edfERP\u4f30\u8ba1\u9700\u8981\u5927\u91cfEEG\u8bd5\u6b21\u7684\u5e73\u5747\u6765\u964d\u4f4e\u566a\u58f0\u548c\u4fe1\u53f7\u53d8\u5f02\u6027\uff0c\u8fd9\u9650\u5236\u4e86ERP\u7814\u7a76\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51cf\u5c11\u6240\u9700\u8bd5\u6b21\u6570\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEEG2ERP\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f15\u5bfc\u8bad\u7ec3\u76ee\u6807\u548c\u5355\u72ec\u7684\u65b9\u5dee\u89e3\u7801\u5668\u6765\u5efa\u6a21ERP\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u88ab\u8bd5\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08ERP CORE\u3001P300\u62fc\u5199\u5668BCI\u3001\u9762\u90e8\u611f\u77e5\u795e\u7ecf\u5f71\u50cf\uff09\u4e0a\u8bc4\u4f30\uff0c\u5728\u5c11\u91cf\u8bd5\u6b21\u60c5\u51b5\u4e0b\uff0cEEG2ERP\u6bd4\u4f20\u7edf\u5e73\u5747\u65b9\u6cd5\u63d0\u4f9b\u663e\u8457\u66f4\u597d\u7684ERP\u4f30\u8ba1\u3002", "conclusion": "EEG2ERP\u662f\u9996\u4e2a\u5c06EEG\u4fe1\u53f7\u6620\u5c04\u5230\u76f8\u5173ERP\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11ERP\u7814\u7a76\u6240\u9700\u7684\u8bd5\u6b21\u6570\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.23311", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23311", "abs": "https://arxiv.org/abs/2511.23311", "authors": ["Haruki Sakajo", "Hiroshi Takato", "Hiroshi Tsutsui", "Komei Soda", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach", "comment": "Accepted to MMLoSo 2025", "summary": "Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u540c\u6b65\u5904\u7406\u9a7e\u9a76\u5458\u89c6\u89d2\u548c\u9053\u8def\u89c6\u89d2\u89c6\u9891\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7528\u4e8e\u751f\u6210\u5b89\u5168\u9a7e\u9a76\u6307\u4ee4\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u6548\u679c\u6709\u9650\u4f46\u5fae\u8c03\u540e\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7b49\u5de5\u4e1a\u5e94\u7528\u9700\u8981\u540c\u65f6\u76d1\u63a7\u9a7e\u9a76\u5458\u884c\u4e3a\u548c\u9053\u8def\u72b6\u51b5\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u4f46\u73b0\u6709LVLMs\u5728\u5904\u7406\u540c\u6b65\u591a\u89c6\u89d2\u89c6\u9891\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u5176\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u9a7e\u9a76\u5458\u89c6\u89d2\u548c\u9053\u8def\u89c6\u89d2\u540c\u6b65\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u5bf9LVLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u751f\u6210\u5b89\u5168\u9a7e\u9a76\u6307\u4ee4\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u9884\u8bad\u7ec3LVLMs\u6548\u679c\u6709\u9650\uff0c\u4f46\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u80fd\u591f\u751f\u6210\u51c6\u786e\u4e14\u5b89\u5168\u610f\u8bc6\u7684\u9a7e\u9a76\u6307\u4ee4\uff0c\u4e0d\u8fc7\u5728\u68c0\u6d4b\u89c6\u9891\u4e2d\u7ec6\u5fae\u6216\u590d\u6742\u4e8b\u4ef6\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "LVLMs\u5728\u540c\u6b65\u591a\u89c6\u89d2\u89c6\u9891\u5904\u7406\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u68c0\u6d4b\u590d\u6742\u4e8b\u4ef6\u4ecd\u662f\u6311\u6218\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u8be5\u9886\u57df\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.23276", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.23276", "abs": "https://arxiv.org/abs/2511.23276", "authors": ["Joongwon Chae", "Runming Wang", "Chen Xiong", "Gong Yunhan", "Lian Zhang", "Ji Jiansong", "Dongmei Yu", "Peiwu Qin"], "title": "Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting", "comment": null, "summary": "Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM \"event interpreter\" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u89e3\u91ca\u4e0e\u6982\u7387\u9884\u6d4b\u89e3\u8026\uff0c\u4f7f\u7528LLM\u5904\u7406\u5f02\u6784\u4fe1\u53f7\u751f\u6210\u4f20\u64ad\u5f71\u54cd\u4fe1\u53f7\uff0c\u518d\u7ed3\u5408\u5386\u53f2\u75c5\u4f8b\u6570\u636e\u8fdb\u884c\u6982\u7387\u9884\u6d4b\uff0c\u5728\u624b\u8db3\u53e3\u75c5\u76d1\u6d4b\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u867d\u7136\u80fd\u7eb3\u5165\u534f\u53d8\u91cf\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u6765\u89e3\u91ca\u51b2\u7a81\u9a71\u52a8\u56e0\u7d20\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002\u624b\u8db3\u53e3\u75c5\u7684\u6709\u6548\u76d1\u6d4b\u9700\u8981\u8003\u8651\u6d41\u884c\u75c5\u5b66\u6a21\u5f0f\u548c\u5b66\u6821\u65e5\u5386\u3001\u5929\u6c14\u7b49\u4e0a\u4e0b\u6587\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e24\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) LLM\"\u4e8b\u4ef6\u89e3\u91ca\u5668\"\u5904\u7406\u5f02\u6784\u4fe1\u53f7\uff08\u5b66\u6821\u65e5\u7a0b\u3001\u6c14\u8c61\u6458\u8981\u3001\u62a5\u544a\u7b49\uff09\u751f\u6210\u6807\u91cf\u4f20\u64ad\u5f71\u54cd\u4fe1\u53f7\uff1b2) \u795e\u7ecf\u7b26\u53f7\u6838\u5fc3\u5c06\u6b64\u4fe1\u53f7\u4e0e\u5386\u53f2\u75c5\u4f8b\u6570\u7ed3\u5408\uff0c\u4ea7\u751f\u6821\u51c6\u7684\u6982\u7387\u9884\u6d4b\u3002", "result": "\u5728\u9999\u6e2f\uff082023-2024\uff09\u548c\u4e3d\u6c34\uff082024\uff09\u7684\u771f\u5b9e\u624b\u8db3\u53e3\u75c5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5728\u70b9\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u7a33\u5065\u768490%\u9884\u6d4b\u533a\u95f4\uff08\u8986\u76d6\u73870.85-1.00\uff09\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "conclusion": "\u901a\u8fc7LLM\u7ed3\u6784\u5316\u6574\u5408\u9886\u57df\u77e5\u8bc6\u53ef\u4ee5\u5339\u914d\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4ea7\u751f\u7b26\u5408\u516c\u5171\u536b\u751f\u5de5\u4f5c\u6d41\u7a0b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u6d4b\uff0c\u4e3a\u75be\u75c5\u76d1\u6d4b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u6846\u67b6\u3002"}}
{"id": "2511.22968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22968", "abs": "https://arxiv.org/abs/2511.22968", "authors": ["Shouhe Zhang", "Dayong Ren", "Sensen Song", "Yurong Qian", "Zhenhong Jia"], "title": "Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM", "comment": null, "summary": "Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems. To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs. First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting. By learning a standardized, illumination-invariant appearance model, it assigns a stable and consistent color representation to each Gaussian primitive. Second, the Dynamic Radiance Balancing Loss (DRB-Loss) reactively handles frames with extreme exposure. It activates only when an image's exposure is poor, operating directly on the radiance field to guide targeted optimization. This prevents error accumulation from extreme lighting without compromising performance under normal conditions. The synergy between IAN's proactive invariance and DRB-Loss's reactive correction endows our system with unprecedented robustness. Evaluations on public datasets demonstrate state-of-the-art performance in camera tracking, map quality, and semantic and geometric accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u7684\u56fa\u6709\u5916\u89c2\u5f52\u4e00\u5316\u6a21\u5757\u548c\u53cd\u5e94\u5f0f\u7684\u52a8\u6001\u8f90\u5c04\u5e73\u8861\u635f\u5931\uff0c\u5b9e\u73b0\u5149\u7167\u4e0d\u53d8\u6027\uff0c\u63d0\u5347\u6781\u7aef\u66dd\u5149\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u6781\u7aef\u66dd\u5149\u4f1a\u540c\u65f6\u964d\u4f4e3D\u5730\u56fe\u91cd\u5efa\u548c\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u5bf9\u7d27\u8026\u5408\u7cfb\u7edf\u7279\u522b\u6709\u5bb3\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5149\u7167\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6781\u7aef\u5149\u7167\u6761\u4ef6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u56fa\u6709\u5916\u89c2\u5f52\u4e00\u5316\u6a21\u5757\uff1a\u4e3b\u52a8\u89e3\u8026\u573a\u666f\u7684\u56fa\u6709\u5c5e\u6027\uff08\u5982\u53cd\u7167\u7387\uff09\u548c\u77ac\u65f6\u5149\u7167\uff0c\u5b66\u4e60\u6807\u51c6\u5316\u7684\u5149\u7167\u4e0d\u53d8\u5916\u89c2\u6a21\u578b\uff0c\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u57fa\u5143\u5206\u914d\u7a33\u5b9a\u4e00\u81f4\u7684\u989c\u8272\u8868\u793a\u3002\n2. \u52a8\u6001\u8f90\u5c04\u5e73\u8861\u635f\u5931\uff1a\u4ec5\u5728\u56fe\u50cf\u66dd\u5149\u4e0d\u4f73\u65f6\u6fc0\u6d3b\uff0c\u76f4\u63a5\u5728\u8f90\u5c04\u573a\u4e0a\u64cd\u4f5c\u4ee5\u6307\u5bfc\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u9632\u6b62\u6781\u7aef\u5149\u7167\u4e0b\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6b63\u5e38\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u76f8\u673a\u8ddf\u8e2a\u3001\u5730\u56fe\u8d28\u91cf\u3001\u8bed\u4e49\u548c\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "IAN\u7684\u4e3b\u52a8\u4e0d\u53d8\u6027\u548cDRB-Loss\u7684\u53cd\u5e94\u5f0f\u6821\u6b63\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u8d4b\u4e88\u4e86\u7cfb\u7edf\u524d\u6240\u672a\u6709\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5728\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u8bed\u4e49SLAM\u6027\u80fd\u3002"}}
{"id": "2511.22989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22989", "abs": "https://arxiv.org/abs/2511.22989", "authors": ["Yuta Oshima", "Daiki Miyake", "Kohsei Matsutani", "Yusuke Iwasawa", "Masahiro Suzuki", "Yutaka Matsuo", "Hiroki Furuta"], "title": "MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation", "comment": "Code: https://github.com/matsuolab/multibanana", "summary": "Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as \"what to edit\" or \"how many references are given\", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .", "AI": {"tldr": "MultiBanana\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u591a\u53c2\u8003\u6761\u4ef6\u4e0b\u7684\u80fd\u529b\uff0c\u5305\u62ec\u53c2\u8003\u6570\u91cf\u53d8\u5316\u3001\u9886\u57df\u4e0d\u5339\u914d\u3001\u5c3a\u5ea6\u4e0d\u5339\u914d\u3001\u7f55\u89c1\u6982\u5ff5\u548c\u591a\u8bed\u8a00\u6587\u672c\u53c2\u8003\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u53c2\u8003\u6216\u5c11\u91cf\u53c2\u8003\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u53c2\u8003\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4e14\u4efb\u52a1\u5b9a\u4e49\u6a21\u7cca\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u53c2\u8003\u8bbe\u7f6e\u7684\u5185\u5728\u96be\u5ea6\u3002", "method": "\u8bbe\u8ba1MultiBanana\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e7f\u6cdb\u8986\u76d6\u591a\u53c2\u8003\u7279\u5b9a\u95ee\u9898\uff1a1) \u53c2\u8003\u6570\u91cf\u53d8\u5316\uff1b2) \u53c2\u8003\u56fe\u50cf\u95f4\u9886\u57df\u4e0d\u5339\u914d\uff08\u5982\u7167\u7247vs\u52a8\u6f2b\uff09\uff1b3) \u53c2\u8003\u4e0e\u76ee\u6807\u573a\u666f\u5c3a\u5ea6\u4e0d\u5339\u914d\uff1b4) \u5305\u542b\u7f55\u89c1\u6982\u5ff5\u7684\u53c2\u8003\uff1b5) \u591a\u8bed\u8a00\u6587\u672c\u53c2\u8003\u3002", "result": "\u5bf9\u591a\u79cd\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5206\u6790\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u4f18\u8d8a\u6027\u80fd\u3001\u5178\u578b\u5931\u8d25\u6a21\u5f0f\u548c\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\u3002MultiBanana\u5c06\u4f5c\u4e3a\u5f00\u653e\u57fa\u51c6\u53d1\u5e03\uff0c\u4e3a\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u5efa\u7acb\u6807\u51c6\u5316\u6bd4\u8f83\u57fa\u7840\u3002", "conclusion": "MultiBanana\u586b\u8865\u4e86\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\uff0c\u4e3a\u516c\u5e73\u6bd4\u8f83\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u7840\uff0c\u5e76\u8bc6\u522b\u6a21\u578b\u5728\u591a\u53c2\u8003\u6761\u4ef6\u4e0b\u7684\u80fd\u529b\u8fb9\u754c\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2511.23002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23002", "abs": "https://arxiv.org/abs/2511.23002", "authors": ["Yunlong Lin", "Linqing Wang", "Kunjie Lin", "Zixu Lin", "Kaixiong Gong", "Wenbo Li", "Bin Lin", "Zhenxi Li", "Shiyi Zhang", "Yuyang Peng", "Wenxun Dai", "Xinghao Ding", "Chunyu Wang", "Qinglin Lu"], "title": "JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization", "comment": "31 pages, 18 figures", "summary": "Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity.", "AI": {"tldr": "JarvisEvo\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u7f16\u8f91\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u534f\u540c\u7f16\u8f91-\u8bc4\u4f30\u7b56\u7565\u4f18\u5316\uff0c\u89e3\u51b3\u6307\u4ee4\u5e7b\u89c9\u548c\u5956\u52b1\u653b\u51fb\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7f16\u8f91\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u6307\u4ee4\u5e7b\u89c9 - \u7eaf\u6587\u672c\u601d\u7ef4\u94fe\u63a8\u7406\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u4e8b\u5b9e\u9519\u8bef\uff1b2) \u5956\u52b1\u653b\u51fb - \u52a8\u6001\u7b56\u7565\u4f18\u5316\u4f1a\u5229\u7528\u9759\u6001\u5956\u52b1\u51fd\u6570\u7684\u7f3a\u9677\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u6765\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u63d0\u51faJarvisEvo\u7edf\u4e00\u56fe\u50cf\u7f16\u8f91\u667a\u80fd\u4f53\uff0c\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u5e08\u7684\u8fed\u4ee3\u7f16\u8f91\u8fc7\u7a0b\u3002\u6838\u5fc3\u5305\u62ec\uff1a1) \u4ea4\u9519\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236(iMCoT)\uff1b2) \u534f\u540c\u7f16\u8f91-\u8bc4\u4f30\u7b56\u7565\u4f18\u5316\u6846\u67b6(SEPO)\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff1b3) \u96c6\u6210Adobe Lightroom\u652f\u6301\u5168\u5c40\u548c\u5c40\u90e8\u7cbe\u7ec6\u7f16\u8f91\u3002", "result": "\u5728ArtEdit-Bench\u4e0a\uff0cJarvisEvo\u6bd4Nano-Banana\u5e73\u5747\u63d0\u534718.95%\u7684\u4fdd\u62a4\u6027\u7f16\u8f91\u6307\u6807\uff0c\u5176\u4e2d\u50cf\u7d20\u7ea7\u5185\u5bb9\u4fdd\u771f\u5ea6\u5927\u5e45\u63d0\u534744.96%\u3002", "conclusion": "JarvisEvo\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u5e7b\u89c9\u548c\u5956\u52b1\u653b\u51fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.23355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23355", "abs": "https://arxiv.org/abs/2511.23355", "authors": ["Vinh Chau", "Khoa Le Dinh Van", "Hon Huynh Ngoc", "Binh Nguyen Thien", "Hao Nguyen Thien", "Vy Nguyen Quang", "Phuc Vo Hong", "Yen Lam Minh", "Kieu Pham Tieu", "Trinh Nguyen Thi Diem", "Louise Thwaites", "Hai Ho Bich"], "title": "A Hierarchical Computer Vision Pipeline for Physiological Data Extraction from Bedside Monitors", "comment": "11 pages, 3 figures", "summary": "In many low-resource healthcare settings, bedside monitors remain standalone legacy devices without network connectivity, creating a persistent interoperability gap that prevents seamless integration of physiological data into electronic health record (EHR) systems. To address this challenge without requiring costly hardware replacement, we present a computer vision-based pipeline for the automated capture and digitisation of vital sign data directly from bedside monitor screens. Our method employs a hierarchical detection framework combining YOLOv11 for accurate monitor and region of interest (ROI) localisation with PaddleOCR for robust text extraction. To enhance reliability across variable camera angles and lighting conditions, a geometric rectification module standardizes the screen perspective before character recognition. We evaluated the system on a dataset of 6,498 images collected from open-source corpora and real-world intensive care units in Vietnam. The model achieved a mean Average Precision (mAP@50-95) of 99.5% for monitor detection and 91.5% for vital sign ROI localisation. The end-to-end extraction accuracy exceeded 98.9% for core physiological parameters, including heart rate, oxygen saturation SpO2, and arterial blood pressure. These results demonstrate that a lightweight, camera-based approach can reliably transform unstructured information from screen captures into structured digital data, providing a practical and scalable pathway to improve information accessibility and clinical documentation in low-resource settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7ba1\u9053\uff0c\u4ece\u5e8a\u65c1\u76d1\u62a4\u4eea\u5c4f\u5e55\u81ea\u52a8\u6355\u83b7\u548c\u6570\u5b57\u5316\u751f\u547d\u4f53\u5f81\u6570\u636e\uff0c\u89e3\u51b3\u4f4e\u8d44\u6e90\u533b\u7597\u73af\u5883\u4e2d\u8bbe\u5907\u7f3a\u4e4f\u7f51\u7edc\u8fde\u63a5\u7684\u95ee\u9898\u3002", "motivation": "\u4f4e\u8d44\u6e90\u533b\u7597\u73af\u5883\u4e2d\uff0c\u5e8a\u65c1\u76d1\u62a4\u4eea\u591a\u4e3a\u72ec\u7acb\u9057\u7559\u8bbe\u5907\uff0c\u7f3a\u4e4f\u7f51\u7edc\u8fde\u63a5\uff0c\u5bfc\u81f4\u751f\u7406\u6570\u636e\u65e0\u6cd5\u65e0\u7f1d\u96c6\u6210\u5230\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\uff0c\u5f62\u6210\u4e92\u64cd\u4f5c\u6027\u969c\u788d\u3002", "method": "\u91c7\u7528\u5206\u5c42\u68c0\u6d4b\u6846\u67b6\uff1aYOLOv11\u7528\u4e8e\u7cbe\u786e\u5b9a\u4f4d\u76d1\u62a4\u4eea\u548c\u611f\u5174\u8da3\u533a\u57df\uff0cPaddleOCR\u7528\u4e8e\u7a33\u5065\u7684\u6587\u672c\u63d0\u53d6\u3002\u5305\u542b\u51e0\u4f55\u6821\u6b63\u6a21\u5757\uff0c\u5728\u4e0d\u540c\u6444\u50cf\u5934\u89d2\u5ea6\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u6807\u51c6\u5316\u5c4f\u5e55\u900f\u89c6\u3002", "result": "\u57286,498\u5f20\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1a\u76d1\u62a4\u4eea\u68c0\u6d4bmAP@50-95\u8fbe99.5%\uff0c\u751f\u547d\u4f53\u5f81ROI\u5b9a\u4f4d\u8fbe91.5%\u3002\u7aef\u5230\u7aef\u63d0\u53d6\u6838\u5fc3\u751f\u7406\u53c2\u6570\uff08\u5fc3\u7387\u3001\u8840\u6c27\u9971\u548c\u5ea6\u3001\u52a8\u8109\u8840\u538b\uff09\u51c6\u786e\u7387\u8d85\u8fc798.9%\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u65b9\u6cd5\u80fd\u53ef\u9760\u5730\u5c06\u5c4f\u5e55\u6355\u83b7\u7684\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u5b57\u6570\u636e\uff0c\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6539\u5584\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u548c\u4e34\u5e8a\u6587\u6863\u8bb0\u5f55\u3002"}}
