<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/abs/2511.16166)
*Zeting Liu,Zida Yang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EvoVLA是一个自监督的视觉语言动作模型框架，通过阶段对齐奖励、基于姿态的对象探索和长时程记忆三个组件，解决了长时程机器人操作中的阶段幻觉问题，在模拟和真实环境中都显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在阶段幻觉问题，即智能体利用粗糙的评估信号来走捷径完成多步任务，报告高进度但并未真正完成任务。

Method: 1. 阶段对齐奖励(SAR)：使用三元组对比学习和Gemini生成的困难负样本来防止视觉捷径；2. 基于姿态的对象探索(POE)：基于相对对象-夹爪姿态而非原始像素来建立好奇心；3. 长时程记忆：使用选择性上下文保留和门控融合来稳定扩展执行过程中的内在塑造。

Result: 在Discoverse-L长时程操作基准测试中，EvoVLA比最强基线(OpenVLA-OFT)平均任务成功率提高10.2个百分点，达到69.2%。样本效率提高1.5倍，阶段幻觉从38.5%降至14.8%。真实世界部署在物理机器人上平均成功率达到54.6%，比OpenVLA-OFT高11个百分点。

Conclusion: EvoVLA有效解决了VLA模型中的阶段幻觉问题，实现了有效的模拟到真实世界迁移和强大的泛化能力。

Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

</details>


### [2] [Graph Neural Networks for Surgical Scene Segmentation](https://arxiv.org/abs/2511.16430)
*Yihan Li,Nikhil Churamani,Maria Robu,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 提出了两种结合Vision Transformer和Graph Neural Networks的图基分割模型，用于提高腹腔镜胆囊切除术中肝胆囊解剖结构的识别精度，在遮挡、长距离依赖和精细几何结构分割方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术中准确识别肝胆囊解剖结构对防止手术并发症至关重要。深度学习模型在处理遮挡、长距离依赖和罕见结构精细几何特征方面存在困难，需要增强空间和语义理解能力。

Method: 提出两种分割模型：(1) 静态k近邻图结合GCNII实现稳定长距离信息传播；(2) 动态可微分图生成器结合GAT支持自适应拓扑学习。两种模型均集成ViT特征编码器和GNN，在Endoscapes-Seg50和CholecSeg8k基准上评估。

Result: 相比最先进基线方法，在mIoU和mDice指标上分别提升7-8%和6%，在薄、罕见和安全关键结构上产生解剖学一致的预测结果。

Conclusion: 图基分割方法提高了手术场景分割的性能和解剖一致性，通过结合ViT的全局上下文和图基关系推理，增强了模型的可解释性和可靠性，为更安全的腹腔镜和机器人辅助手术铺平道路。

Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.

</details>


### [3] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 本文提出了视频下一事件预测（VNEP）任务，将传统的文本回答扩展为视频回答，并开发了VANS模型，通过强化学习将视觉语言模型与视频扩散模型对齐，实现准确且视觉友好的视频生成。


<details>
  <summary>Details</summary>
Motivation: 视频具有展示物理世界信息的独特能力，而语言模型在视频生成方面主要局限于娱乐应用。作者发现将视频作为下一事件预测的新回答模态的未充分利用机会，能够提供更直观和定制化的程序性学习和创意探索答案。

Method: 提出VANS模型，使用强化学习将视觉语言模型（VLM）与视频扩散模型（VDM）对齐。核心是Joint-GRPO方法，通过共享奖励机制优化VLM生成准确且易于可视化的描述，同时指导VDM生成忠实于描述和输入视觉上下文的视频。

Result: 在程序性和预测性基准测试中，VANS在视频事件预测和可视化方面均达到了最先进的性能。

Conclusion: VNEP任务将视频作为回答模态，为程序性学习和创意探索提供了更直观的解决方案。VANS模型通过强化学习有效对齐VLM和VDM，在视频生成质量和语义一致性方面表现出色。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty](https://arxiv.org/abs/2511.16225)
*Victor Croisfelt,João Henrique Inacio de Souza,Shashi Raj Pandey,Beatriz Soret,Petar Popovski*

Main category: cs.LG

TL;DR: 提出了一种基于自适应时间窗口整合的神经启发式非阻塞推理范式，能够动态适应异构流中的随机延迟模式，无需依赖参考模态要求


<details>
  <summary>Details</summary>
Motivation: 现有非阻塞推理方法依赖参考模态范式，需要一个模态输入完全接收后才能处理，且需要昂贵的离线分析，无法有效应对跨数据流的随机通信延迟挑战

Method: 采用自适应时间窗口整合(TWIs)技术，动态调整以适应异构流中的随机延迟模式，同时放宽参考模态要求，构建通信延迟感知框架

Result: 在音频-视觉事件定位任务上的实验表明，相比现有方法具有更好的网络动态适应性

Conclusion: 该框架实现了具有更精细精度-延迟权衡控制的鲁棒实时推理

Abstract: Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.

</details>


### [5] [Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning](https://arxiv.org/abs/2511.16333)
*Mohammad Areeb Qazi,Maryam Nadeem,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 本文综述了医疗领域的世界模型，这些模型学习预测动态以实现多步推演、反事实评估和规划。文章调查了医学影像、电子健康记录和机器人手术三个领域的应用，并提出了L1-L4能力评估框架，指出了当前研究的局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要预测性、可靠性和数据效率，但现有生成模型缺乏物理基础和时序推理能力。世界模型能够学习多模态、时序一致且动作条件化的表示，反映医疗的物理和因果结构。

Method: 通过文献综述方法，分析医疗世界模型在三个领域的应用：医学影像与诊断、电子健康记录的疾病进展建模、机器人手术与手术规划。建立了L1-L4能力评估框架。

Result: 大多数系统达到L1-L2能力（时序预测和动作条件预测），较少达到L3（反事实推演），极少达到L4（规划/控制）。识别出限制临床可靠性的关键问题。

Conclusion: 需要开发临床稳健的预测优先世界模型，将生成主干网络（Transformer、扩散模型、VAE）与因果/机械基础相结合，为医疗安全决策支持提供保障。

Abstract: Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.

</details>


### [6] [Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin](https://arxiv.org/abs/2511.16523)
*Ming-Lun Lee,Fu-Shiang Yang,Cheng-Kuan Lin,Yan-Ann Chen,Chih-Yu Lin,Yu-Chee Tseng*

Main category: cs.LG

TL;DR: 提出了首个专门用于动态客户端参与场景的联邦学习基准框架，揭示了动态参与导致的性能显著下降问题，并提出KPFL解决方案来维持共享知识池以提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究大多假设客户端持续参与，忽略了实际中客户端动态加入/退出的场景，且缺乏专门针对动态参与联邦学习(DPFL)的基准框架。

Method: 开发了可配置数据分布、参与模式和评估指标的DPFL基准框架；提出KPFL方法，通过双年龄和数据偏差加权结合生成式知识蒸馏来维护共享知识池。

Result: 基准测试显示动态参与导致联邦学习模型性能显著下降；KPFL能有效提升模型在动态参与场景下的鲁棒性和泛化能力。

Conclusion: 动态参与是联邦学习中的重要实际问题，KPFL通过知识池机制成功缓解了由此带来的不稳定性和知识损失问题。

Abstract: Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.

</details>


### [7] [Toward Valid Generative Clinical Trial Data with Survival Endpoints](https://arxiv.org/abs/2511.16551)
*Perrine Chassat,Van Tuan Nguyen,Lucas Ducrot,Emilie Lanoy,Agathe Guilloux*

Main category: cs.LG

TL;DR: 提出一种基于变分自编码器(VAE)的生成模型，用于联合生成混合类型协变量和生存结局，解决临床试验中合成对照组生成的时间-事件结果建模难题。


<details>
  <summary>Details</summary>
Motivation: 临床试验面临患者群体分散、入组缓慢和成本高昂等挑战，特别是在肿瘤学和罕见病领域。现有基于GAN的方法数据需求大、不稳定且依赖独立删失等强假设。

Method: 使用变分自编码器(VAE)在统一潜变量框架下联合生成混合类型协变量和生存结局，不假设独立删失。

Result: 在合成和真实试验数据集上评估，在保真度、实用性和隐私指标方面优于GAN基线方法，但发现I类错误和功效的系统性误校准问题。

Conclusion: 提出后生成选择程序改善校准，展示了生成生存建模的进展和开放挑战。

Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation](https://arxiv.org/abs/2511.15825)
*Tuan-Anh Le,Anh Mai Vu,David Yang,Akash Awasthi,Hien Van Nguyen*

Main category: cs.AI

TL;DR: IMACT-CXR是一个基于AutoGen的多智能体交互式胸片解读教学系统，整合了空间标注、视线分析、知识检索和图像推理功能，通过专业智能体评估学习者表现并提供个性化辅导。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够统一处理学习者边界框、视线样本和自由文本观察的智能教学系统，帮助医学实习生提高胸片解读能力，解决传统教学方法中缺乏个性化反馈和实时指导的问题。

Method: 使用AutoGen框架构建多智能体系统，包含定位质量评估、苏格拉底式辅导、PubMed证据检索、相似病例推荐等专业智能体，结合贝叶斯知识追踪维护技能掌握度，并集成肺叶分割模块进行解剖学感知的视线反馈。

Result: 系统展示了响应式教学流程，具有有限延迟、精确控制答案泄露和可扩展性，初步评估显示相比基线方法在定位和诊断推理方面有显著改进。

Conclusion: IMACT-CXR成功实现了胸片解读的智能教学，为实时住院医师培训部署提供了可行方案，证明了多智能体系统在医学教育中的有效性。

Abstract: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

</details>


### [9] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: 提出了FOOTPASS数据集，这是首个用于足球比赛全程多模态、多智能体战术背景下逐场比赛动作识别的基准，旨在结合计算机视觉输出和足球战术知识来自动生成可靠的逐场比赛数据流。


<details>
  <summary>Details</summary>
Motivation: 当前足球视频理解方法在构建可靠的逐场比赛数据方面仍不足，通常只能辅助而非完全自动化标注。同时战术建模、轨迹预测等研究需要基于比赛状态和逐场比赛数据，这促使利用战术知识作为先验来支持基于计算机视觉的预测。

Method: 引入FOOTPASS数据集，支持开发利用计算机视觉任务输出（如跟踪、识别）和足球战术知识（包括长期战术规律）的球员中心动作识别方法。

Result: 创建了首个用于足球比赛全程逐场比赛动作识别的基准数据集，支持多模态、多智能体战术上下文的分析。

Conclusion: FOOTPASS数据集为开发能够生成可靠逐场比赛数据流的方法提供了基础，这些数据流是数据驱动体育分析的重要输入。

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [10] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite是一个轻量级的贝叶斯增强框架，用于transformer临床语言模型，无需重新训练即可提供不确定性感知预测，减少过度自信32-48%，防止41%的诊断错误。


<details>
  <summary>Details</summary>
Motivation: transformer模型在临床决策支持中容易过度自信，特别是在模糊的医疗案例中，需要校准的不确定性评估。

Method: 集成三个组件：贝叶斯嵌入校准（使用蒙特卡洛dropout）、不确定性加权注意力（对token可靠性进行边缘化）、置信度引导决策塑造（基于临床风险最小化）。

Result: 在MedQA、PubMedQA、MIMIC-III等基准测试中，持续改善校准和可信度，减少过度自信32-48%，在模拟临床环境中可防止41%的诊断错误。

Conclusion: 该框架能有效实现可靠的不确定性传播，提高医疗AI系统的可解释性，参数开销低于3%且无需重新训练。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [11] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: SEG框架通过将支持结构优化集成到3D模型生成过程中，显著减少3D打印所需支撑材料的用量，提高打印效率。


<details>
  <summary>Details</summary>
Motivation: 当前3D切片技术主要关注后处理优化，而非在设计阶段解决支撑结构效率问题，导致材料浪费和打印时间增加。

Method: 提出SEG框架，将带偏移的直接偏好优化(ODPO)集成到3D生成流程中，在训练过程中融入支撑结构模拟，直接优化模型以减少支撑需求。

Result: 在Thingi10k-Val和GPT-3DP-Val数据集上的实验表明，SEG在支撑体积减少和可打印性方面显著优于TRELLIS、DPO和DRO等基线模型。

Conclusion: SEG通过在生成过程中直接优化模型，为更可持续和高效的数字制造实践铺平了道路。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>
