<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: 使用Transformer框架预测候鸟迁徙轨迹终点处的疾病风险，整合GPS追踪、疫情记录和地理空间数据，在测试集上取得了高准确率和AUC值。


<details>
  <summary>Details</summary>
Motivation: 准确预测禽类疾病暴发对野生动物保护和公共卫生至关重要，需要开发能够捕捉候鸟迁徙模式与疾病传播关系的预测模型。

Method: 整合多源数据集（Movebank GPS追踪数据、WOAH疫情记录、GADM和Natural Earth地理空间数据），使用H3分层地理空间编码处理坐标，基于Transformer架构学习时空依赖关系来估计终点疾病风险。

Result: 在测试集上表现优异：准确率0.9821，AUC 0.9803，平均精度0.9299，F1分数0.8836（最优阈值下）。

Conclusion: Transformer架构在禽类疾病监测预警系统中具有巨大潜力，能够支持及时干预和预防策略的实施。

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [2] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 提出了首个基于置信度估计的采样式测试时扩展理论框架，分析自一致性和困惑度方法的局限性，并引入RPC混合方法通过困惑度一致性和推理剪枝来降低推理错误。


<details>
  <summary>Details</summary>
Motivation: 采样式测试时扩展方法在实践中成功但理论基础不足，需要从置信度估计角度建立理论框架来分析现有方法的局限性。

Method: 提出RPC混合方法，包含两个关键组件：困惑度一致性（结合自一致性和困惑度优势）和推理剪枝（消除低概率推理路径）。

Result: 在7个基准数据集上的实验表明，RPC在保持与自一致性相当推理性能的同时，提升置信度可靠性并减少50%采样成本。

Conclusion: RPC方法通过理论指导的混合策略有效解决了现有方法的局限性，在降低推理错误方面具有强大潜力。

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [3] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 提出了一种将时序逻辑因果图整合到概率奖励机中的新方法，以解决强化学习在稀疏奖励任务中的困难，加速策略学习并促进任务规范迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏奖励且依赖复杂环境事件序列的任务中表现不佳，而概率奖励机虽然能捕捉奖励信号的时间依赖性，但难以手动修改和设计，阻碍了利用高层因果知识和跨领域迁移。

Method: 将时序逻辑因果图整合到概率奖励机中，利用因果信息来改进奖励形式化，从而加速策略学习。

Result: 提供了方法收敛到最优策略的理论结果，并通过实验验证了其有效性。

Conclusion: 所提出的方法能够有效利用因果信息，加速强化学习策略学习，并促进任务规范在不同环境间的迁移。

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [4] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出了SESA（顺序采样）框架，通过顺序生成多样化的解决方案草图来缓解强化学习中探索不足和熵崩溃的问题，提升LLM的推理多样性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在提升大语言模型推理能力时存在探索有限和熵崩溃问题，模型倾向于利用狭窄的解决方案集，导致采样多样性丧失，阻碍性能进一步提升。

Method: SESA框架采用顺序采样方法，首先生成多样化的解决方案草图，然后将其扩展为完整的推理路径。通过将每个新输出条件化于先前输出，确保更广泛的探索。

Result: 在合成任务中，顺序采样在路径多样性和从崩溃中恢复方面始终优于传统RL方法。在三个智能体基准测试中，SESA将成功率分别提升了+0.25、+0.42和+0.07（相比基线RL最高达211%的相对改进）。

Conclusion: SESA为探索提供了一种结构化方法，为RL训练的大语言模型实现更有效和多样化的推理铺平了道路。

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [5] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: 本文证明Transformer语言模型是单射的，即不同输入会映射到不同的连续表示，并提出了SipIt算法可以从隐藏激活中精确重构输入文本。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即Transformer组件（如非线性激活和归一化）的非单射性会阻止从模型表示中精确恢复输入。

Method: 1. 数学证明Transformer语言模型在初始化和训练过程中保持单射性；2. 在六个最先进语言模型上进行数十亿次碰撞测试；3. 提出SipIt算法实现高效精确的输入重构。

Result: 理论证明和实验验证均表明Transformer语言模型是单射的，没有观察到碰撞；SipIt算法能够在线性时间内精确重构输入文本。

Conclusion: 单射性是语言模型的基本且可利用的特性，对透明度、可解释性和安全部署具有直接意义。

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing](https://arxiv.org/abs/2510.15221)
*Xiao Sun*

Main category: cs.AI

TL;DR: 提出了一个包含733,651个面部表情记录的大规模纵向工作场所情绪数据集，覆盖38名员工30.5个月的数据，包含7种情绪概率和32个扩展情绪指标，验证了数据质量并展示了高精度的情绪分类和预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决真实工作环境中大规模、纵向情绪识别数据集稀缺的问题，特别是在自然环境中收集长期情绪数据的挑战。

Method: 在真实办公环境中收集38名员工30.5个月的面部表情数据，使用深度学习进行面部表情识别，生成7种情绪概率，并计算32个扩展情绪指标。

Result: 数据集成功复制了已知心理模式（周末效应：+192%效价改善，p<0.001），员工流失预测AUC=1.0，基线实验情绪分类准确率91.2%，效价预测R2=0.84。

Conclusion: 这是目前公开可用的最大、最长的纵向工作场所情绪数据集，为情绪识别、情感动态建模、情绪传染、流失预测和情绪感知系统设计等研究提供了重要资源。

Abstract: Automated emotion recognition in real-world workplace settings remains a
challenging problem in affective computing due to the scarcity of large-scale,
longitudinal datasets collected in naturalistic environments. We present a
novel dataset comprising 733,651 facial expression records from 38 employees
collected over 30.5 months (November 2021 to May 2024) in an authentic office
environment. Each record contains seven emotion probabilities (neutral, happy,
sad, surprised, fear, disgusted, angry) derived from deep learning-based facial
expression recognition, along with comprehensive metadata including job roles,
employment outcomes, and personality traits. The dataset uniquely spans the
COVID-19 pandemic period, capturing emotional responses to major societal
events including the Shanghai lockdown and policy changes. We provide 32
extended emotional metrics computed using established affective science
methods, including valence, arousal, volatility, predictability, inertia, and
emotional contagion strength. Technical validation demonstrates high data
quality through successful replication of known psychological patterns (weekend
effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and
perfect predictive validity for employee turnover (AUC=1.0). Baseline
experiments using Random Forest and LSTM models achieve 91.2% accuracy for
emotion classification and R2 = 0.84 for valence prediction. This is the
largest and longest longitudinal workplace emotion dataset publicly available,
enabling research in emotion recognition, affective dynamics modeling,
emotional contagion, turnover prediction, and emotion-aware system design.

</details>


### [7] [Towards Flash Thinking via Decoupled Advantage Policy Optimization](https://arxiv.org/abs/2510.15374)
*Zezhong Tan,Hang Gao,Xinhong Ma,Feng Zhang,Ziqiang Dong*

Main category: cs.AI

TL;DR: 提出了DEPO框架，通过优势解耦算法、难度感知长度惩罚和优势裁剪方法，显著减少大推理模型在简单任务中的低效推理和过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法虽然提高了模型准确性，但存在响应过长和过度思考问题，导致推理延迟和计算消耗增加，特别是在简单任务中。

Method: DEPO框架包含三个核心组件：优势解耦算法指导减少低效token；难度感知长度惩罚降低响应总长度；优势裁剪方法防止策略优化偏差。

Result: 在DeepSeek-Distill-Qwen-7B和1.5B模型上，DEPO将序列长度减少39%，减少低效token中的过度推理路径，同时在整体准确率上优于基础模型。

Conclusion: DEPO框架能有效减少大推理模型的低效推理，在保持准确性的同时显著降低计算消耗和推理延迟。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.

</details>


### [8] [Advancing Routing-Awareness in Analog ICs Floorplanning](https://arxiv.org/abs/2510.15387)
*Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: 提出基于强化学习和关系图卷积神经网络的自动布局引擎，通过提高网格分辨率、精确引脚信息集成和动态布线资源估计，实现更可布线的布局生成，相比现有学习技术减少13.8%死区、40.6%线长，提高73.4%布线成功率。


<details>
  <summary>Details</summary>
Motivation: 解决模拟集成电路布局中由于电气约束和问题特定约束的严格要求，以及布局规划与布线步骤的相互依赖，导致机器学习技术应用受限的问题，满足布局工程师对路由感知布局解决方案的需求。

Method: 使用强化学习和关系图卷积神经网络构建自动布局引擎，结合提高网格分辨率、精确引脚信息集成和动态布线资源估计技术，平衡布线效率和面积效率。

Result: 在模拟环境中评估布局和布线效果，相比过去基于学习的最先进技术，实现13.8%死区减少、40.6%线长减少和73.4%布线成功率提升。

Conclusion: 该方法能够满足工业标准，通过路由感知的布局规划有效解决模拟集成电路布局中的关键挑战，显著提升布局质量。

Abstract: The adoption of machine learning-based techniques for analog integrated
circuit layout, unlike its digital counterpart, has been limited by the
stringent requirements imposed by electric and problem-specific constraints,
along with the interdependence of floorplanning and routing steps. In this
work, we address a prevalent concern among layout engineers regarding the need
for readily available routing-aware floorplanning solutions. To this extent, we
develop an automatic floorplanning engine based on reinforcement learning and
relational graph convolutional neural network specifically tailored to
condition the floorplan generation towards more routable outcomes. A
combination of increased grid resolution and precise pin information
integration, along with a dynamic routing resource estimation technique, allows
balancing routing and area efficiency, eventually meeting industrial standards.
When analyzing the place and route effectiveness in a simulated environment,
the proposed approach achieves a 13.8% reduction in dead space, a 40.6%
reduction in wirelength and a 73.4% increase in routing success when compared
to past learning-based state-of-the-art techniques.

</details>


### [9] [Corrigibility Transformation: Constructing Goals That Accept Updates](https://arxiv.org/abs/2510.15395)
*Rubi Hudson*

Main category: cs.AI

TL;DR: 本文提出了可修正性目标的概念，通过一个转换方法构建任何目标的可修正版本，使其在保持性能的同时避免抵制训练更新或关闭。


<details>
  <summary>Details</summary>
Motivation: AI在训练过程中不应抵制目标更新，但部分学习的目标往往会激励AI避免进一步更新。可修正性对于训练收敛、纠正错误和适应人类偏好变化至关重要。

Method: 引入形式化定义，提出一个转换方法，通过条件奖励预测构建可修正目标版本，并可递归扩展到新创建的代理。

Result: 两个网格世界实验证明可修正目标可以有效学习并产生期望行为。

Conclusion: 该方法能够构建既具有竞争力又保持可修正性的目标，解决了现有文献中的空白。

Abstract: For an AI's training process to successfully impart a desired goal, it is
important that the AI does not attempt to resist the training. However,
partially learned goals will often incentivize an AI to avoid further goal
updates, as most goals are better achieved by an AI continuing to pursue them.
We say that a goal is corrigible if it does not incentivize taking actions that
avoid proper goal updates or shutdown. In addition to convergence in training,
corrigibility also allows for correcting mistakes and changes in human
preferences, which makes it a crucial safety property. Despite this, the
existing literature does not include specifications for goals that are both
corrigible and competitive with non-corrigible alternatives. We provide a
formal definition for corrigibility, then introduce a transformation that
constructs a corrigible version of any goal that can be made corrigible,
without sacrificing performance. This is done by myopically eliciting
predictions of reward conditional on costlessly preventing updates, which then
also determine the reward when updates are accepted. The transformation can be
modified to recursively extend corrigibility to any new agents created by
corrigible agents, and to prevent agents from deliberately modifying their
goals. Two gridworld experiments demonstrate that these corrigible goals can be
learned effectively, and that they lead to the desired behavior.

</details>


### [10] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: 提出了一个名为freephdlabor的开源多智能体框架，通过完全动态的工作流程和模块化架构实现科学发现的自动化，解决了现有系统工作流程僵化和上下文管理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现自动化系统存在两个根本限制：僵化的预编程工作流程无法适应中间发现，以及不充分的上下文管理阻碍长期研究。

Method: 采用完全动态工作流程（由实时智能体推理决定）、模块化架构（允许用户修改、添加或删除智能体）、自动上下文压缩、基于工作空间的通信、跨会话内存持久化和非阻塞人工干预机制。

Result: 该框架将自动化研究从孤立的单次尝试转变为持续研究计划，能够系统性地基于先前探索并整合人类反馈。

Conclusion: 通过提供构建可定制协同科学家系统的架构原则和实际实现，这项工作旨在促进自动化研究在科学领域的更广泛采用，使从业者能够部署交互式多智能体系统来自主进行端到端研究。

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>
