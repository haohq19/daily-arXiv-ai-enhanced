<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 提出GFP框架，用高级特征预测替代传统低级坐标重建，通过轻量级目标生成网络动态产生多样化监督信号，实现高效骨架动作识别


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法局限于原始关节坐标重建，导致计算冗余和语义表示有限，需要更高效的掩码骨架建模方法

Method: 采用高级特征预测框架，包含轻量级目标生成网络动态产生空间-时间层次化监督信号，结合约束优化确保特征多样性

Result: 在NTU RGB+D 60/120和PKU-MMD数据集上取得SOTA性能，训练速度比标准方法快6.2倍

Conclusion: GFP框架通过高级特征预测和动态监督信号生成，显著提升了骨架动作识别的计算效率和表示质量

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于事件相机的激光融合氛流缓解方法(EGTM)，通过事件流的异步特性提取像素级的无氛流引导信息，在保持高恢复质量的同时大幅提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习氛流缓解方法需要高容量网络从有限帧率的同步帧中学习粗粒度的氛流动力学，计算和存储效率低。事件相机具有微秒级别的时间分辨率，可以根本解决这个瓶颈。

Method: 首先提出"事件-幸运见解"来揭示氛流形变与事件流逆时空分布的相关性，然后构建EGTM框架，从噪声的氛流事件中提取像素级的可靠无氛流引导信息进行时间幸运融合。

Result: 在真实数据集上，该方法模型大小、推理延迟和模型复杂度分别超过现有SOTA方法710倍、214倍和224倍，恢复质量也达到最佳水平(+0.94 PSNR和+0.08 SSIM)。

Conclusion: 这个工作证明了在氛流缓解任务中引入事件模态的巨大效率优势，为高效氛流缓解提供了新的解决方案。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [3] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: FocusMamba是一个RGB-事件检测方法，通过事件引导的多模态稀疏化和跨模态聚焦融合，自适应地丢弃低信息区域，在保持精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-事件检测方法在处理两种模态的低信息区域（图像背景和事件数据中的非事件区域）时采用统一处理方式，导致计算成本高且性能不佳。需要一种能够自适应处理不同复杂度样本的方法。

Method: 提出事件引导多模态稀疏化(EGMS)策略，利用事件相机感知的场景变化来自适应识别和丢弃每个模态中的低信息区域；设计跨模态聚焦融合(CMFF)模块来有效捕获和整合两种模态的互补特征。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，该方法在准确性和效率方面均优于现有方法。

Conclusion: FocusMamba通过自适应协作稀疏化和高效融合机制，成功实现了RGB-事件检测任务中精度与效率的更好平衡，为多模态视觉处理提供了有效解决方案。

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [4] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出BiT和CATS模块结合的方法，解决音频-视觉视频解析中伪标签噪声和注意力扩散问题，在多个指标上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：一是将噪声段级伪标签视为可靠监督，二是让无差别注意力将伪标签扩散到所有帧，导致训练过程中初始错误被反复放大

Method: 结合双向文本融合(BiT)模块和类别感知时序图(CATS)模块。BiT模块进行语义注入和动态校准来定位和净化语义线索，CATS模块进行语义传播和连接以实现精确的语义信息跨时间传播

Result: 在两个基准数据集LLP和UnAV-100上的多个关键指标上实现了最先进的性能

Conclusion: 该方法有效整合了先前两个研究方向的优势，通过语义净化和精确传播解决了伪标签噪声问题，取得了优异的性能表现

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [5] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: DVS-PedX是一个用于行人检测和过街意图分析的神经形态数据集，包含合成和真实世界的事件流数据，支持多种天气条件，旨在推动基于事件的行人安全研究。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、高动态范围和运动鲁棒性等优势，但缺乏专门用于行人检测和意图分析的数据集。DVS-PedX旨在填补这一空白，特别是在恶劣天气条件下的研究需求。

Method: 数据集包含两个互补来源：(1) CARLA模拟器生成的合成事件流，控制不同的天气和光照条件；(2) 真实世界JAAD行车记录仪视频通过v2e工具转换的事件流。提供RGB帧、事件帧和标注信息。

Result: 提供了包含合成和真实事件流的数据集，支持多种格式（AEDAT 2.0/4.0、AVI DVS视频）。基线SNN实验显示了模拟到真实的差距，需要领域适应和多模态融合。

Conclusion: DVS-PedX数据集将加速基于事件的行人安全、意图预测和神经形态感知的研究，为解决模拟到真实差距和多模态融合提供了重要资源。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [6] [Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage](https://arxiv.org/abs/2509.04370)
*Dor Cohen,Inga Efrosman,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 提出了一种将执法记录仪视频转换为全景图像摘要的计算机视觉方法，通过SLAM估计相机轨迹和空间布局，聚类关键视角并选择代表性帧，最终拼接成空间一致的全景图像来快速理解复杂环境。


<details>
  <summary>Details</summary>
Motivation: 应急响应人员广泛使用执法记录仪记录现场，但在时间紧迫的情况下无法有效审阅冗长视频。需要能够快速解读的简洁视觉摘要来提升态势感知能力。

Method: 使用单目SLAM估计相机轨迹和重建环境空间布局，通过聚类相机位姿识别关键视点，从每个聚类中选择代表性帧，采用多帧拼接技术将这些帧融合成空间一致的全景图像。

Result: 该方法能够生成信息丰富的全景图像摘要，有效总结事故现场，支持快速环境理解和决策制定。

Conclusion: 提出的计算机视觉流水线成功将执法记录仪视频转换为全景图像摘要，为应急响应提供了高效的视觉分析工具，有助于快速决策和事故回顾。

Abstract: First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.

</details>


### [7] [Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.04446)
*Kiymet Akdemir,Jing Shi,Kushal Kafle,Brian Price,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出了Plot'n Polish零样本框架，用于实现一致的故事可视化生成，并提供细粒度的多层级控制


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在故事可视化应用中缺乏灵活性，无法在保持视觉和叙事一致性的同时进行精细或粗粒度编辑，限制了创作者对视觉故事的完善能力

Method: Plot'n Polish零样本框架，能够实现一致的故事生成，并在不同细节层次上提供细粒度控制

Result: 该框架解决了现有方法在保持多帧视觉和叙事一致性方面的不足，为创作者提供了更灵活的故事可视化编辑能力

Conclusion: Plot'n Polish框架为文本到图像扩散模型在故事可视化应用中提供了更好的控制和一致性保持能力，有助于提升创作体验

Abstract: Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 本文提出了一个超参数无关的梯度归一化方法，通过分析梯度方差和标准差在训练中的演化规律，使梯度缩放与其自然演化保持一致，从而稳定优化并保持收敛保证。


<details>
  <summary>Details</summary>
Motivation: 梯度动态在深度神经网络的稳定性和泛化性中起核心作用。研究发现梯度方差和标准差在训练过程中存在一致的演化模式，但现有方法可能导致意外的梯度放大问题。

Method: 提出超参数无关的梯度归一化方法，通过跟踪梯度动态并使其缩放与自然演化对齐，防止意外的梯度放大，稳定优化过程。

Result: 在CIFAR-100基准测试中使用ResNet-20、ResNet-56和VGG-16-BN进行实验，表明该方法在强泛化条件下保持或提高了测试准确率。

Conclusion: 研究强调了直接跟踪梯度动态的重要性，旨在弥合理论期望与实证行为之间的差距，为未来的优化研究提供见解。

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [9] [Predicting Traffic Accident Severity with Deep Neural Networks](https://arxiv.org/abs/2509.03819)
*Meghan Bibb,Pablo Rivas,Mahee Tayba*

Main category: cs.LG

TL;DR: 使用自编码器进行无监督维度降低和密集网络，对交通事故数据进行严重程度分类，达到92%的准确率


<details>
  <summary>Details</summary>
Motivation: 利用机器学习技术研究交通事故数据，降低事故风险，新模型能够在不平衡数据上实现良好的模性汇聚和预测能力

Method: 首先分析特征共线性，使用自编码器进行无监督维度降低，然后通过密集网络进行严重程度分类

Result: 实验结果显示，使用深度神经网络进行交叉验证时，事故严重程度分类的准确率达到92%

Conclusion: 神经网络基础模型在交通事故数据分析中表现出艰的预测性能，为事故风险管理提供了有效的技术手段

Abstract: Traffic accidents can be studied to mitigate the risk of further events.
Recent advances in machine learning have provided an alternative way to study
data associated with traffic accidents. New models achieve good generalization
and high predictive power over imbalanced data. In this research, we study
neural network-based models on data related to traffic accidents. We begin
analyzing relative feature colinearity and unsupervised dimensionality
reduction through autoencoders, followed by a dense network. The features are
related to traffic accident data and the target is to classify accident
severity. Our experiments show cross-validated results of up to 92% accuracy
when classifying accident severity using the proposed deep neural network.

</details>


### [10] [Characteristic Energy Behavior Profiling of Non-Residential Buildings](https://arxiv.org/abs/2509.04322)
*Haley Dozier,Althea Henslee*

Main category: cs.LG

TL;DR: 美军基地基础设施面临气候变化风险，需要数据驱动的能源使用行为模型来评估弹性和创建基准测量


<details>
  <summary>Details</summary>
Motivation: 美军基地依赖商业能源和水源，面临气候变化和极端天气威胁，需要提高基础设施的弹性和耐受性

Method: 提出数据驱动的行为模型，使用多模态数据分析、预测和聚类非住宅建筑能源使用行为，使用结构相似的公开数据进行方法论证

Result: 建立能源使用行为模型中心，能够分析、预测和聚类多模态能源数据

Conclusion: 该模型方法可为美军基地提供能源系统受干扰影响的基准评估，并为未来弹性措施提供对比基准，提高基地的气候变化应对能力

Abstract: Due to the threat of changing climate and extreme weather events, the
infrastructure of the United States Army installations is at risk. More than
ever, climate resilience measures are needed to protect facility assets that
support critical missions and help generate readiness. As most of the Army
installations within the continental United States rely on commercial energy
and water sources, resilience to the vulnerabilities within independent energy
resources (electricity grids, natural gas pipelines, etc) along with a baseline
understanding of energy usage within installations must be determined. This
paper will propose a data-driven behavioral model to determine behavior
profiles of energy usage on installations. These profiles will be used 1) to
create a baseline assessment of the impact of unexpected disruptions on energy
systems and 2) to benchmark future resiliency measures. In this methodology,
individual building behavior will be represented with models that can
accurately analyze, predict, and cluster multimodal data collected from energy
usage of non-residential buildings. Due to the nature of Army installation
energy usage data, similarly structured open access data will be used to
illustrate this methodology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 基于Model Context Protocol的零训练幻觉防止系统，通过实时词汇查找和结构化推理，高效准确地将源医学术语映射到OMOP标准概念


<details>
  <summary>Details</summary>
Motivation: OMOP CDM数据标准化过程中，源医学术语映射到标准概念的过程非常资源浪费且容易出错，而大语言模型存在幻觉问题不适合直接临床部署

Method: 基于Model Context Protocol(MCP)标准化框架开发零训练系统，允许LLM与外部资源和工具交互，实现实时词汇查找和结构化推理

Result: 系统显著提高了映射效率和准确性，支持可解释的映射过程，适合在探索性和生产环境中即时使用

Conclusion: 该方法在不需训练的情况下有效防止幻觉现象，为OMOP CDM数据标准化提供了高效、准确且可靠的自动化映射解决方案

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [12] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: 通过构建确定性知识图和LLM生成知识图的对比监控，提出了一种可扩展的生成式AI可靠性评估方法


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI模型的可靠性问题（如幻觉、语义偏移、偏见），充当黑盒模型的透明化评估方法

Method: 构建两个并行知识图：确定性KG（规则基础）和LLM生成KG（实时数据流），使用KG指标（ICR、IPR、CI）量化结构和语义偏差

Result: 开发了自动化实时监控框架，基于历史统计分布设置动态异常阈值，可及旹检测语义异常和幻觉

Conclusion: 该结构化、指标驱动的对比方法提供了稳健且可扩展的生成式AI可靠性评估框架

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [13] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 论文认为AI虽然在能力上可能超越人类，但由于缺乏中枢神经系统(CNS)带来的情感体验和道德理解，永远无法真正替代人类成为宇宙的领导者。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否能够超越人类成为宇宙的领导者，分析人类与AI的根本差异。

Method: 通过比较人类中枢神经系统(CNS)与AI系统的本质区别，论证情感体验和道德理解的重要性。

Result: AI可能在各种能力指标上超越人类，但缺乏CNS带来的情感体验使其无法真正理解行为后果，因此不适合担任宇宙领导角色。

Conclusion: 人类DNA而非硅基技术才是宇宙领导权的最佳基础，CNS的生物学特性是AI无法复制的关键优势。

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

TL;DR: ProLiFIC是一个从1987年到2022年意大利立法过程的综合事件日志，通过LLM从非结构化数据创建，旨在为法律流程挖掘提供基准数据集。


<details>
  <summary>Details</summary>
Motivation: 流程挖掘在工业领域应用成熟，但在法律领域受限于数据可获取性和质量，需要高质量的法律流程数据集来推动该领域发展。

Method: 从Normattiva门户网站提取非结构化数据，使用大语言模型(LLMs)进行结构化处理，构建意大利立法过程的事件日志。

Result: 创建了ProLiFIC数据集，涵盖35年的意大利立法流程，为法律流程挖掘提供了首个综合性基准数据集。

Conclusion: ProLiFIC数据集填补了法律流程挖掘领域的数据空白，为后续研究提供了重要基础，展示了LLM与流程挖掘结合的应用潜力。

Abstract: Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [15] [Explicit and Implicit Data Augmentation for Social Event Detection](https://arxiv.org/abs/2509.04202)
*Congbo Ma,Yuxia Wang,Jia Wu,Jian Yang,Jing Du,Zitai Qiu,Qing Li,Hu Wang,Preslav Nakov*

Main category: cs.CL

TL;DR: SED-Aug是一个用于社交媒体事件检测的双重增强框架，通过显式文本增强和隐式特征空间增强来提高数据多样性和模型鲁棒性，在两个Twitter数据集上相比最佳基线模型F1分数提升约17.67%和15.57%。


<details>
  <summary>Details</summary>
Motivation: 社交媒体事件检测依赖标注数据，但标注成本高且劳动密集，需要解决数据稀缺问题。

Method: 提出SED-Aug双重增强框架：1）显式文本增强使用大语言模型通过5种生成策略增强文本信息；2）隐式特征空间增强设计5种新颖的扰动技术，在结构融合嵌入上进行操作，保持嵌入的语义和关系属性。

Result: 在Twitter2012数据集上平均F1分数比最佳基线模型提升约17.67%，在Twitter2018数据集上提升约15.57%。

Conclusion: SED-Aug框架通过双重增强有效提高了社交媒体事件检测的性能，解决了数据稀缺问题，代码已开源。

Abstract: Social event detection involves identifying and categorizing important events
from social media, which relies on labeled data, but annotation is costly and
labor-intensive. To address this problem, we propose Augmentation framework for
Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework,
which combines explicit text-based and implicit feature-space augmentation to
enhance data diversity and model robustness. The explicit augmentation utilizes
large language models to enhance textual information through five diverse
generation strategies. For implicit augmentation, we design five novel
perturbation techniques that operate in the feature space on structural fused
embeddings. These perturbations are crafted to keep the semantic and relational
properties of the embeddings and make them more diverse. Specifically, SED-Aug
outperforms the best baseline model by approximately 17.67% on the Twitter2012
dataset and by about 15.57% on the Twitter2018 dataset in terms of the average
F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach](https://arxiv.org/abs/2509.03563)
*Quan Quan,Jiwen Xu,Runxiao Liu,Yi Ding,Jiaxing Che,Kai-Yuan Cai*

Main category: cs.RO

TL;DR: 提出了一种基于物理启发的无人机群协同运输方法，模仿桌腿负载分配的耗散力学原理，实现无显式通信的自适应负载分配和编队稳定。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、通信依赖性和动态故障鲁棒性方面存在不足，而无人机群协同运输在物流和灾难响应中具有变革潜力。

Method: 开发去中心化耗散力模型，每个机器人根据邻居机器人和悬挂负载动态调整位置，类似于能量耗散的桌腿反应机制。

Result: 仿真显示跟踪误差比现有方法降低20%-68.5%，真实实验中6架无人机在单机故障、断开事件、25%负载变化和40%线长不确定下达到94%成功率，在4级风力下保持强鲁棒性。

Conclusion: 该方法将群体智能与机械稳定性原理相结合，为异构空中系统在通信受限环境中处理复杂运输任务提供了可扩展框架。

Abstract: In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.

</details>


### [17] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 提出DRLR框架，通过改进IBRL算法的动作选择模块来减少bootstrapping误差，使用SAC替代TD3防止策略收敛到次优解，在机器人任务中验证了有效性并实现了sim2real部署


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在机器人任务中探索效率低和bootstrapping误差导致的学习效率问题，通过结合示范数据提高学习性能

Method: 基于IBRL算法改进动作选择模块，提供校准的Q值来减少bootstrapping误差；使用SAC作为RL策略防止收敛到次优解；在装桶和开抽屉两个机器人任务上进行验证

Result: 实验验证了方法在减少bootstrapping误差和防止过拟合方面的有效性，框架在不同状态-动作维度和示范质量的任务中表现出鲁棒性，成功实现了从仿真到真实世界的部署

Conclusion: DRLR框架通过改进的动作选择模块和SAC策略，有效提高了机器人任务的探索效率和学习性能，证明了在真实工业机器人任务中的实用性

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>
