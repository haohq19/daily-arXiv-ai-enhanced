<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning](https://arxiv.org/abs/2512.04219)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur\\*

Main category: cs.CV

TL;DR: PARSE是一个无监督学习框架，通过多尺度递归预测器从流式视频中学习层次化事件结构，利用预测误差峰值检测事件边界，在多个基准测试中达到最先进的流式方法性能。


<details>
  <summary>Details</summary>
Motivation: 人类自然地将连续经验感知为时间嵌套事件的层次结构，但计算机视觉需要能够预测性和层次性地分割视频的模型。现有方法通常需要监督或只能进行回顾性分析，而无法像人类那样进行预测性、层次化的事件理解。

Method: PARSE采用统一框架，通过多尺度递归预测器学习事件结构：底层建模短期动态，高层通过基于注意力的反馈整合长期上下文。事件边界自然地从预测误差的瞬态峰值中涌现，形成时间连贯的嵌套部分关系。

Result: 在Breakfast Actions、50 Salads和Assembly 101三个基准测试中，PARSE在流式方法中达到最先进性能，在时间对齐（H-GEBD）和结构一致性（TED, hF1）方面与离线基线方法相当。

Conclusion: 研究表明，在不确定性下的预测学习为类似人类的时间抽象和组合事件理解提供了可扩展的路径，证明了无监督层次化事件分割的有效性。

Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.

</details>


### [2] [DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision](https://arxiv.org/abs/2512.04314)
*Jiashu Liao,Pietro Liò,Marc de Kamps,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出DisentangleFormer，通过空间-通道解耦的Transformer架构解决视觉Transformer中空间和通道维度纠缠的问题，在超光谱图像处理中实现更好的结构和语义依赖建模。


<details>
  <summary>Details</summary>
Motivation: 标准视觉Transformer的自注意力机制将空间和通道维度联合处理，导致表示纠缠，无法独立建模结构和语义依赖。这在超光谱成像中尤为严重，因为不同通道捕获不同的生物物理或生化线索。

Method: 提出DisentangleFormer架构，基于信息论的去相关表示学习原理，采用并行设计实现空间-通道解耦。包含三个核心组件：1) 并行解耦：独立处理空间token和通道token流；2) 压缩token增强器：动态融合空间和通道流的自适应校准模块；3) 多尺度FFN：补充全局注意力，捕获细粒度结构和语义依赖。

Result: 在超光谱基准测试中达到最先进性能，在Indian Pine、Pavia University、Houston、BigEarthNet遥感数据集和红外病理数据集上均优于现有模型。在ImageNet上保持竞争性准确率的同时，计算成本降低17.8% FLOPs。

Conclusion: DisentangleFormer通过空间-通道解耦实现了鲁棒的多通道视觉表示，解决了视觉Transformer中的表示纠缠问题，在超光谱图像处理中表现出色，同时计算效率更高。

Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.

</details>


### [3] [StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios](https://arxiv.org/abs/2512.04451)
*Yifei Wang,Zhenkai Li,Tianwen Qian,Huanran Zheng,Zheng Wang,Yuqian Fu,Xiaoling Wang*

Main category: cs.CV

TL;DR: StreamEQA：首个面向具身场景的流式视频问答基准，评估MLLM在具身和流式两个维度的能力，包含3个具身层级和3种时序推理模式，基于156个长视频构建了42个任务和约21K问答对。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能向真实世界部署发展，需要能够持续感知和理解流式视觉输入的能力。现有基准无法充分评估模型在具身场景下对流式视频的理解能力，因此需要专门的基准来推动这一方向的研究。

Method: 构建StreamEQA基准：1) 从具身和流式两个维度设计评估框架；2) 具身维度分为感知、交互和规划三个层级；3) 流式维度分为后向、实时和前向推理三种模式；4) 基于156个独立长视频，通过自动生成和人工精炼的混合流程创建约21K个带精确时间戳的问答对。

Result: 评估了13个最先进的视频-LLM，发现尽管在传统基准上表现良好，这些模型在具身场景下的流式视频理解方面仍然存在困难，表明现有模型在这一新兴任务上仍有很大提升空间。

Conclusion: StreamEQA是首个面向具身场景的流式视频问答基准，填补了现有评估体系的空白，有望推动具身应用中的流式视频理解研究，为开发更智能的具身代理提供重要工具。

Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.

</details>


### [4] [SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding](https://arxiv.org/abs/2512.04643)
*Chang-Hsun Wu,Kai-Po Chang,Yu-Yang Sheng,Hung-Kai Chung,Kuei-Chun Wang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: SEASON是一种无需训练的方法，通过自适应对比解码来增强视频大语言模型的时空忠实性，减少幻觉问题


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在处理视频时难以有效感知和利用丰富的时间信息，导致生成的事件描述存在时间不一致或因果不合理的问题，产生严重幻觉。现有研究主要关注空间幻觉（如物体不匹配），而时间推理相对未被充分探索。

Method: 提出自诊断对比解码（SEASON），这是一种无需训练的方法，通过动态诊断每个输出token的幻觉倾向，并对其对应的时空负样本应用自适应对比解码，从而自适应地增强每个输出token的时间和空间忠实性。

Result: 在三个幻觉检测基准测试中，SEASON优于所有现有的无需训练幻觉缓解方法，同时在四个通用视频理解基准测试中进一步提升了视频大语言模型的性能。

Conclusion: SEASON通过自适应对比解码有效解决了视频大语言模型中的时空幻觉问题，无需额外训练即可显著提升模型的时空忠实性和视频理解能力。

Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.

</details>


### [5] [Controllable Long-term Motion Generation with Extended Joint Targets](https://arxiv.org/abs/2512.04487)
*Eunjong Lee,Eunhee Kim,Sanghoon Hong,Eunho Jung,Jihoon Kim*

Main category: cs.CV

TL;DR: COMET：实时角色动画生成框架，通过Transformer条件VAE实现精细关节控制，引入参考引导反馈机制保证长期稳定性，支持实时风格迁移


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时角色动画生成中存在两个主要问题：1）无法提供精细粒度的控制；2）长序列中运动质量会逐渐退化。这限制了它们在交互式应用中的使用。

Method: 提出COMET自回归框架，采用高效的Transformer条件VAE，允许对任意用户指定关节进行精确交互控制。引入新颖的参考引导反馈机制防止误差累积，该机制也可作为即插即用的风格化模块。

Result: COMET能够以实时速度稳健生成高质量运动，在复杂运动控制任务中显著优于现有方法，证明其已准备好用于要求苛刻的交互应用。

Conclusion: COMET框架成功解决了实时角色动画生成的挑战，实现了精细控制、长期稳定性和实时风格迁移，为交互式应用提供了实用的解决方案。

Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.

</details>


### [6] [Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation](https://arxiv.org/abs/2512.04678)
*Yunhong Lu,Yanhong Zeng,Haobo Li,Hao Ouyang,Qiuyu Wang,Ka Leong Cheng,Jiapeng Zhu,Hengyuan Cao,Zhipeng Zhang,Xing Zhu,Yujun Shen,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出Reward Forcing框架，通过EMA-Sink和Re-DMD两个创新设计解决视频生成中初始帧复制和运动动态不足的问题，实现高质量流式视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频生成方法使用滑动窗口注意力，以初始帧作为sink tokens来维持注意力性能和减少误差累积，但这导致视频帧过度依赖静态token，造成初始帧复制和运动动态减弱的问题。

Method: 提出Reward Forcing框架，包含两个核心设计：1) EMA-Sink：维护固定大小的token，从初始帧初始化并通过指数移动平均融合被淘汰的token，在不增加计算成本的情况下捕获长期上下文和近期动态；2) Re-DMD：奖励分布匹配蒸馏，通过视觉语言模型评估动态性，将模型输出分布偏向高奖励区域，优先处理动态性更强的样本。

Result: Reward Forcing在标准基准测试中达到最先进性能，在单个H100 GPU上实现23.1 FPS的高质量流式视频生成。定量和定性实验均显示该方法显著提升运动质量同时保持数据保真度。

Conclusion: Reward Forcing框架通过EMA-Sink和Re-DMD有效解决了现有流式视频生成方法中的初始帧复制和运动动态不足问题，实现了高效、高质量的流式视频生成，为交互式和动态世界模拟提供了重要技术基础。

Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2是基于Gemini基础模型构建的通用具身智能体，能够在多种3D虚拟世界中理解和行动，支持语言和图像指令，接近人类表现，并能通过自我改进学习新技能。


<details>
  <summary>Details</summary>
Motivation: 开发能够主动、目标导向地在具身环境中交互的通用智能体，超越之前只能处理简单语言指令的限制，实现与用户的交互式合作。

Method: 基于Gemini基础模型构建，支持语言和图像输入，能够推理高级目标、与用户对话，并通过Gemini生成任务和奖励实现自主学习和技能改进。

Result: 在多种游戏中大幅缩小与人类表现的差距，在未见环境中展现强大泛化能力，同时保持基础模型的核心推理能力，并能从零开始在新环境中自主学习新技能。

Conclusion: 这项工作验证了创建适用于虚拟和物理世界的多功能、持续学习智能体的可行路径，代表了具身智能领域的重要进展。

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [8] [Sequential Enumeration in Large Language Models](https://arxiv.org/abs/2512.04727)
*Kuinan Hou,Marco Zorzi,Alberto Testolin*

Main category: cs.AI

TL;DR: LLMs在序列计数任务中表现有限：虽然能在明确提示下执行计数程序，但不会自发采用计数策略，显示出神经方法与符号方法在组合泛化上的差距。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够像基于规则的符号系统那样，可靠地对离散符号序列进行系统化计数。虽然循环架构只能近似跟踪和枚举事件序列，但现代深度学习系统（包括LLMs）是否能够部署系统化的计数程序仍不清楚。

Method: 研究了五种最先进的LLM（包括专有、开源和推理模型），在涉及字母和单词列表的顺序命名和生产任务中进行探测。采用多种提示指令探索思维链在计数策略自发出现中的作用。评估相同架构但规模递增的开源模型，分析序列枚举过程中的嵌入动态以研究数量编码的涌现。

Result: 一些LLMs在明确提示下确实能够部署计数程序，但没有一个模型在简单要求枚举序列中项目数量时会自发进行计数。模型规模增加并未带来计数能力的系统提升。

Conclusion: 尽管LLMs具有令人印象深刻的涌现能力，但它们还不能稳健且系统地部署计数程序，这突显了神经方法和符号方法在组合泛化方面的持续差距。

Abstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.

</details>


### [9] [The AI Consumer Index (ACE)](https://arxiv.org/abs/2512.04921)
*Julien Benchek,Rohit Shetty,Benjamin Hunsberger,Ajay Arun,Zach Richards,Brendan Foody,Osvald Nitski,Bertie Vidgen*

Main category: cs.AI

TL;DR: AI Consumer Index (ACE) 是首个评估前沿AI模型执行高价值消费者任务能力的基准，包含400个测试案例，涵盖购物、饮食、游戏、DIY四大领域。GPT 5 (Thinking = High) 以56.1%得分位居榜首，但整体表现显示当前模型与消费者AI需求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估AI模型在实际消费者任务中表现的标准基准。消费者日常活动（如购物、饮食、游戏、DIY）对AI助手有重要需求，但现有评估方法未能充分测试模型在这些高价值任务中的真实能力。

Method: 创建包含400个隐藏测试案例的ACE基准，分为购物、饮食、游戏、DIY四个领域。开源80个案例作为开发集。采用新颖的评分方法，动态检查模型回答是否基于检索到的网页来源。评估10个前沿模型（开启网络搜索功能）。

Result: GPT 5 (Thinking = High) 以56.1%得分排名第一，o3 Pro (Thinking = On) 55.2%第二，GPT 5.1 (Thinking = High) 55.1%第三。不同领域表现差异明显，购物领域最佳模型得分低于50%。模型在价格信息和链接提供等任务上容易产生幻觉。

Conclusion: ACE基准揭示了即使最佳AI模型与消费者实际需求之间仍存在显著差距。模型在不同消费者活动领域表现不均，且在关键信息准确性方面存在严重幻觉问题，表明AI助手技术仍需大幅改进才能满足消费者日常需求。

Abstract: We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.

</details>


### [10] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: 提出TDKPS框架，首次实现黑盒多智能体系统中行为动态的监测，通过时间联合嵌入和假设检验检测智能体及群体层面的行为变化。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体应用的普及，动态多智能体系统自然涌现，但现有研究仅关注单时间点的低维表示，缺乏对黑盒多智能体系统行为动态监测的规范框架。

Method: 提出时间数据核视角空间(TDKPS)，将智能体在时间维度上联合嵌入，并设计多个新颖的假设检验方法，用于检测智能体层面和群体层面的行为变化。

Result: 在模拟实验和自然实验中验证了所提检验方法的有效性，包括对关键超参数的敏感性分析，并证明能够灵敏、特异且显著地检测到与真实外部事件相关的行为变化。

Conclusion: TDKPS是首个用于监测黑盒多智能体系统行为动态的规范框架，随着生成式智能体部署规模的扩大，这一能力至关重要。

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)
*Wenlong Deng,Yushu Li,Boying Gong,Yi Ren,Christos Thrampoulidis,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 本文识别了工具集成强化学习中GRPO方法存在的训练崩溃问题，提出了LLD死亡螺旋机制，并设计了轻量级正则化方法LLDS来稳定训练，在多个基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 工具集成强化学习（TI-RL）让大语言模型能够通过外部工具进行多步推理，但GRPO方法（如Search-R1）虽然收敛快且无需价值函数，却持续遭受训练崩溃问题。本文旨在探究这一失败的根本原因并提供解决方案。

Method: 首先识别了"懒惰似然位移"（LLD）作为GRPO失败的核心机制，并揭示了LLD死亡螺旋的动态过程。然后提出了轻量级似然保持正则化方法LLDS，该方法仅在轨迹似然下降时激活，并只对相关token进行正则化，以最小化对优化的干扰。

Result: 在Search-R1风格的搜索集成问答任务上，实证揭示了LLD的三阶段轨迹：早期停滞、稳定衰减和加速崩溃。提出的LLDS方法在7个开放域和多跳QA基准上稳定了训练，防止了梯度爆炸，并带来了显著性能提升，包括Qwen2.5-3B上+37.8%和Qwen2.5-7B上+32.0%的增益。

Conclusion: LLD是GRPO基TI-RL的根本瓶颈，提出的LLDS方法为稳定、可扩展的工具集成LLM训练提供了实用路径，能够有效缓解LLD问题，同时最小化对优化的干扰。

Abstract: Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

</details>


### [12] [MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection](https://arxiv.org/abs/2512.04492)
*Yuanshuo Zhang,Aohua Li,Bo Chen,Jingbo Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: MSME：一个多阶段、多专家的零样本立场检测框架，通过知识准备、专家推理和决策聚合三阶段解决复杂现实场景中的立场理解挑战


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在复杂现实场景中仍存在困难：需要动态背景知识、目标定义涉及复合实体/事件需要与立场标签明确关联、修辞手法（如反讽）常掩盖作者真实意图

Method: 提出MSME框架，包含三个阶段：1) 知识准备（检索背景知识、澄清立场标签）；2) 专家推理（知识专家从知识角度提炼关键事实和推理，标签专家细化立场标签及相应推理，语用专家检测修辞线索从语用角度推断意图）；3) 决策聚合（元法官整合所有专家分析生成最终立场预测）

Result: 在三个公共数据集上的实验表明，MSME在所有数据集上都达到了最先进的性能

Conclusion: MSME框架通过多阶段、多专家的协同工作，有效解决了复杂现实场景中的零样本立场检测挑战，显著提升了性能

Abstract: LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.

</details>


### [13] [UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction](https://arxiv.org/abs/2512.04518)
*Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 该论文介绍了ChemoTimelines共享任务中从临床记录构建癌症患者化疗时间线的方法，重点评估了思维链、监督微调、直接偏好优化和字典查找等策略，最佳模型Qwen3-14B获得了0.678的官方分数。


<details>
  <summary>Details</summary>
Motivation: 从电子健康记录中准确提取癌症患者的化疗时间线对于临床决策和研究至关重要，但这是一个具有挑战性的自然语言处理任务，需要开发有效的方法来处理原始临床文本。

Method: 采用两步工作流程：首先使用LLM从单个临床记录中提取化疗事件，然后通过算法将事件规范化和聚合成患者级别的时间线。评估了多种LLM使用和训练策略，包括思维链推理、监督微调、直接偏好优化和字典查找方法。

Result: 多种方法在测试集排行榜上表现出竞争力，其中经过微调的Qwen3-14B模型获得了最佳官方分数0.678。不同策略在事件提取和聚合方面各有优劣。

Conclusion: 该研究为从临床文本构建化疗时间线任务提供了有效的方法框架和实用见解，其工作流程和评估策略可推广到类似的医疗信息提取任务中。

Abstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions](https://arxiv.org/abs/2512.04773)
*Giorgos Polychronis,Foivos Pournaropoulos,Christos D. Antonopoulos,Spyros Lalis*

Main category: cs.RO

TL;DR: 无人机在数据驱动任务中需要实时处理数据以决定是否在原地采取额外行动，否则会浪费时间或需要折返。论文提出基于分支预测和强化学习的机器学习方法，显著改善了任务时间。


<details>
  <summary>Details</summary>
Motivation: 无人机在数据采集任务中面临决策困境：如果原地等待处理数据但不需要后续行动，会浪费时间；如果直接飞向下一个点但需要返回处理，又会增加飞行时间。需要智能决策方法来优化任务执行效率。

Method: 提出了基于分支预测和强化学习的机器学习方法，用于决定无人机在采集点是否应该等待处理数据还是直接飞向下一个点。这些方法能够适应事件发生概率随时间变化的各种场景。

Result: 提出的方法显著优于文献中基于回归的方法，最坏情况任务时间最多改善4.1倍。中位任务时间非常接近具有完美事件概率知识的方法，仅高出最多2.7%。

Conclusion: 基于分支预测和强化学习的方法能够有效解决无人机在数据驱动任务中的决策问题，显著提高任务执行效率，接近理想情况下的性能表现。

Abstract: Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.

</details>
