{"id": "2602.15156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15156", "abs": "https://arxiv.org/abs/2602.15156", "authors": ["Shreyas Rajesh", "Pavan Holur", "Mehmet Yigit Turali", "Chenda Duan", "Vwani Roychowdhury"], "title": "Panini: Continual Learning in Token Space via Structured Memory", "comment": "35 pages, code available at: https://github.com/roychowdhuryresearch/gsw-memory", "summary": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.", "AI": {"tldr": "Panini\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u6587\u6863\u8868\u793a\u4e3a\u751f\u6210\u8bed\u4e49\u5de5\u4f5c\u7a7a\u95f4\uff08GSW\uff09\uff0c\u901a\u8fc7\u5b9e\u4f53\u548c\u4e8b\u4ef6\u611f\u77e5\u7684\u95ee\u7b54\u5bf9\u7f51\u7edc\u6765\u79ef\u7d2f\u548c\u6574\u5408\u77e5\u8bc6\uff0c\u76f8\u6bd4\u4f20\u7edfRAG\u65b9\u6cd5\u66f4\u9ad8\u6548\u51c6\u786e\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\uff08LLM\u91cd\u590d\u5904\u7406\u76f8\u540c\u6587\u6863\uff09\u548c\u68c0\u7d22\u4e0d\u76f8\u5173\u5185\u5bb9\u5bfc\u81f4\u751f\u6210\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u77e5\u8bc6\u6574\u5408\u65b9\u5f0f\u3002", "method": "\u63d0\u51faPanini\u6846\u67b6\uff0c\u5c06\u6587\u6863\u8868\u793a\u4e3a\u751f\u6210\u8bed\u4e49\u5de5\u4f5c\u7a7a\u95f4\uff08GSW\uff09\u2014\u2014\u4e00\u4e2a\u5b9e\u4f53\u548c\u4e8b\u4ef6\u611f\u77e5\u7684\u95ee\u7b54\u5bf9\u7f51\u7edc\uff0c\u80fd\u591f\u91cd\u6784\u7ecf\u9a8c\u60c5\u5883\u5e76\u901a\u8fc7\u63a8\u7406\u94fe\u6316\u6398\u6f5c\u5728\u77e5\u8bc6\u3002\u67e5\u8be2\u65f6\u53ea\u904d\u5386\u6301\u7eed\u66f4\u65b0\u7684GSW\u800c\u975e\u539f\u59cb\u6587\u6863\u3002", "result": "\u5728\u516d\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPanini\u5e73\u5747\u6027\u80fd\u6bd4\u5176\u4ed6\u7ade\u4e89\u57fa\u7ebf\u9ad85%-7%\uff0c\u540c\u65f6\u4f7f\u75282-30\u500d\u66f4\u5c11\u7684\u7b54\u6848\u4e0a\u4e0b\u6587token\uff0c\u652f\u6301\u5b8c\u5168\u5f00\u6e90\u6d41\u7a0b\uff0c\u5e76\u5728\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u51cf\u5c11\u4e86\u4e0d\u53ef\u652f\u6301\u7684\u7b54\u6848\u3002", "conclusion": "\u5728\u5199\u5165\u65f6\u9ad8\u6548\u51c6\u786e\u5730\u7ed3\u6784\u5316\u7ecf\u9a8c\uff08\u5982GSW\u6846\u67b6\u6240\u5b9e\u73b0\u7684\uff09\u80fd\u5728\u8bfb\u53d6\u65f6\u540c\u65f6\u83b7\u5f97\u6548\u7387\u548c\u53ef\u9760\u6027\u6536\u76ca\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.15072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15072", "abs": "https://arxiv.org/abs/2602.15072", "authors": ["Abdul Joseph Fofanah", "Lian Wen", "Alpha Alimamy Kamara", "Zhongyi Zhang", "David Chen", "Albert Patrick Sankoh"], "title": "GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation", "comment": null, "summary": "Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.", "AI": {"tldr": "GRAFNet\u662f\u4e00\u79cd\u53d7\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u542f\u53d1\u7684\u606f\u8089\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u5c42\u6b21\u7ed3\u6784\uff0c\u7ed3\u5408\u5b9a\u5411\u6ce8\u610f\u529b\u3001\u591a\u5c3a\u5ea6\u7279\u5f81\u5206\u6790\u548c\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c\u606f\u8089\u5206\u5272\u5bf9\u764c\u75c7\u9884\u9632\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u606f\u8089\u5f62\u6001\u9ad8\u5ea6\u53ef\u53d8\uff08\u4ece\u6241\u5e73\u5230\u7a81\u51fa\u75c5\u53d8\uff09\u3001\u4e0e\u6b63\u5e38\u7ed3\u6784\uff08\u5982\u8936\u76b1\u548c\u8840\u7ba1\uff09\u89c6\u89c9\u76f8\u4f3c\u6027\u5f3a\u3001\u9700\u8981\u9c81\u68d2\u7684\u591a\u5c3a\u5ea6\u68c0\u6d4b\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5355\u5411\u5904\u7406\u3001\u591a\u5c3a\u5ea6\u878d\u5408\u5f31\u3001\u7f3a\u4e4f\u89e3\u5256\u7ea6\u675f\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5047\u9633\u6027\uff08\u6b63\u5e38\u7ed3\u6784\u8fc7\u5206\u5272\uff09\u548c\u5047\u9634\u6027\uff08\u9057\u6f0f\u7ec6\u5fae\u6241\u5e73\u75c5\u53d8\uff09\u3002", "method": "\u63d0\u51faGRAFNet\uff0c\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u5f15\u5bfc\u975e\u5bf9\u79f0\u6ce8\u610f\u529b\u6a21\u5757\uff08GAAM\uff09\uff0c\u6a21\u62df\u65b9\u5411\u8c03\u8c10\u76ae\u5c42\u795e\u7ecf\u5143\u4ee5\u589e\u5f3a\u606f\u8089\u8fb9\u754c\uff1b2\uff09\u591a\u5c3a\u5ea6\u89c6\u7f51\u819c\u6a21\u5757\uff08MSRM\uff09\uff0c\u6a21\u62df\u89c6\u7f51\u819c\u795e\u7ecf\u8282\u7ec6\u80de\u901a\u8def\u8fdb\u884c\u5e76\u884c\u591a\u7279\u5f81\u5206\u6790\uff1b3\uff09\u5f15\u5bfc\u76ae\u5c42\u6ce8\u610f\u529b\u53cd\u9988\u6a21\u5757\uff08GCAFM\uff09\uff0c\u5e94\u7528\u9884\u6d4b\u7f16\u7801\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u8fd9\u4e9b\u6a21\u5757\u5728\u606f\u8089\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u5757\uff08PEDM\uff09\u4e2d\u7edf\u4e00\uff0c\u901a\u8fc7\u5206\u8fa8\u7387\u81ea\u9002\u5e94\u53cd\u9988\u5f3a\u5236\u7a7a\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff08Kvasir-SEG\u3001CVC-300\u3001CVC-ColonDB\u3001CVC-Clinic\u548cPolypGen\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGRAFNet\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u9886\u5148\u65b9\u6cd5Dice\u7cfb\u6570\u63d0\u53473-8%\uff0c\u6cdb\u5316\u80fd\u529b\u63d0\u9ad810-20%\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8def\u5f84\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u8303\u5f0f\uff0c\u5176\u4e2d\u795e\u7ecf\u8ba1\u7b97\u539f\u7406\u5f25\u5408\u4e86AI\u51c6\u786e\u6027\u4e0e\u4e34\u5e8a\u53ef\u4fe1\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u751f\u7269\u5b66\u542f\u53d1\u7684\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15154", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15154", "abs": "https://arxiv.org/abs/2602.15154", "authors": ["Praditha Alwis", "Soumyadeep Chandra", "Deepak Ravikumar", "Kaushik Roy"], "title": "Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories", "comment": "8 pages, 5 figures, 6 tables", "summary": "High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7d2f\u79ef\u6837\u672c\u635f\u5931(CSL)\u7684\u89c6\u9891\u6807\u6ce8\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5e27\u7ea7\u635f\u5931\u8f68\u8ff9\u8bc6\u522b\u9519\u8bef\u6807\u6ce8\u548c\u65f6\u5e8f\u9519\u4e71", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u6570\u636e\u96c6\u5e38\u5b58\u5728\u6807\u6ce8\u9519\u8bef\uff08\u9519\u8bef\u6807\u7b7e\u548c\u65f6\u5e8f\u9519\u4e71\uff09\uff0c\u8fd9\u4e9b\u9519\u8bef\u5728\u76f8\u4f4d\u6807\u6ce8\u4efb\u52a1\u4e2d\u5c24\u4e3a\u6709\u5bb3\uff0c\u56e0\u4e3a\u65f6\u5e8f\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u9519\u8bef\u7684\u5730\u9762\u771f\u503c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684\u6807\u6ce8\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\uff1a\u8bad\u7ec3\u89c6\u9891\u5206\u5272\u6a21\u578b\u5e76\u4fdd\u5b58\u6bcf\u4e2aepoch\u7684\u6743\u91cd\uff0c\u8ba1\u7b97\u7d2f\u79ef\u6837\u672c\u635f\u5931(CSL)\u2014\u2014\u5e27\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u6240\u6709\u68c0\u67e5\u70b9\u7684\u5e73\u5747\u635f\u5931\u3002\u9519\u8bef\u6807\u6ce8\u6216\u65f6\u5e8f\u9519\u4e71\u7684\u5e27\u901a\u5e38\u8868\u73b0\u51fa\u6301\u7eed\u9ad8\u635f\u5931\u6216\u4e0d\u89c4\u5219\u6a21\u5f0f\uff0c\u800c\u6b63\u786e\u6807\u6ce8\u7684\u5e27\u5219\u80fd\u5feb\u901f\u6536\u655b\u5230\u4f4e\u635f\u5931\u3002", "result": "\u5728EgoPER\u548cCholec80\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u9519\u8bef\u6807\u6ce8\u548c\u5e27\u7ea7\u65f6\u5e8f\u9519\u4e71\uff0c\u65e0\u9700\u6807\u6ce8\u9519\u8bef\u7684\u5730\u9762\u771f\u503c\uff0c\u5177\u6709\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684CSL\u65b9\u6cd5\u4e3a\u89c6\u9891\u6570\u636e\u96c6\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u80fd\u8bc6\u522b\u6807\u6ce8\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u89c6\u9891\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u7684\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u6a21\u578b\u65e0\u5173\u3001\u65e0\u9700\u9519\u8bef\u6807\u6ce8\u771f\u503c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.15181", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15181", "abs": "https://arxiv.org/abs/2602.15181", "authors": ["Yunxiao Zhang", "William Stone", "Suryansh Kumar"], "title": "Time-Archival Camera Virtualization for Sports and Visual Performances", "comment": "Project Page: https://yunxiaozhangjack.com/tacv/; Under minor revision in Journal of Computer Vision and Image Understanding (CVIU); Special Issue: Computer Vision for Sports and Winter Sports. Outcome of a master and bachelor student project completed in Visual and Spatial AI Lab at TAMU", "summary": "Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u4f53\u79ef\u6e32\u67d3\u7684\u76f8\u673a\u865a\u62df\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u52a8\u6001\u573a\u666f\u7684\u65f6\u95f4\u5f52\u6863\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u4f53\u80b2\u76f4\u64ad\u7b49\u5e94\u7528", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u52a8\u6001\u573a\u666f\u6e32\u67d3\u65b9\u6cd5\u4f9d\u8d56\u51c6\u786e3D\u70b9\u4e91\uff0c\u96be\u4ee5\u5904\u7406\u5feb\u901f\u975e\u521a\u6027\u8fd0\u52a8\u548c\u591a\u4e3b\u4f53\u72ec\u7acb\u8fd0\u52a8\uff0c\u4e14\u7f3a\u4e4f\u65f6\u95f4\u5f52\u6863\u529f\u80fd", "method": "\u901a\u8fc7\u591a\u89c6\u89d2\u540c\u6b65\u76f8\u673a\u95f4\u7684\u521a\u6027\u53d8\u6362\u5efa\u6a21\u52a8\u6001\u573a\u666f\uff0c\u8fdb\u884c\u795e\u7ecf\u8868\u793a\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u65f6\u95f4\u5f52\u6863", "result": "\u65b9\u6cd5\u63d0\u4f9b\u589e\u5f3a\u7684\u89c6\u89c9\u6e32\u67d3\u8d28\u91cf\uff0c\u652f\u6301\u56de\u6eaf\u4efb\u4f55\u8fc7\u53bb\u65f6\u95f4\u70b9\u7684\u52a8\u6001\u573a\u666f\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210", "conclusion": "\u795e\u7ecf\u4f53\u79ef\u6e32\u67d3\u6846\u67b6\u4e3a\u76f8\u673a\u865a\u62df\u5316\u548c\u65f6\u95f4\u5f52\u6863\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f53\u80b2\u5e7f\u64ad\u7b49\u9700\u8981\u56de\u653e\u548c\u5206\u6790\u7684\u5e94\u7528"}}
{"id": "2602.15391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15391", "abs": "https://arxiv.org/abs/2602.15391", "authors": ["Ankit Sharma", "Nachiket Tapas", "Jyotiprakash Patra"], "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection", "comment": null, "summary": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5f03\u6743\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b89\u5168\u9608\u503c\u548c\u7ea7\u8054\u68c0\u6d4b\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5e73\u8861LLM\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709LLM\u90e8\u7f72\u9762\u4e34\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u6839\u672c\u6743\u8861\uff1a\u4e25\u683c\u8fc7\u6ee4\u673a\u5236\u4f1a\u963b\u6b62\u826f\u6027\u67e5\u8be2\uff0c\u800c\u5bbd\u677e\u63a7\u5236\u5219\u53ef\u80fd\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u3002\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u89c4\u5219\u6216\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u9608\u503c\u7684\u62a4\u680f\u901a\u5e38\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u7528\u6237\u4f53\u9a8c\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u5f03\u6743\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5b9e\u65f6\u4e0a\u4e0b\u6587\u4fe1\u53f7\uff08\u5982\u9886\u57df\u548c\u7528\u6237\u5386\u53f2\uff09\u52a8\u6001\u8c03\u6574\u5b89\u5168\u9608\u503c\u3002\u6846\u67b6\u96c6\u6210\u4e86\u7531\u4e94\u4e2a\u5e76\u884c\u68c0\u6d4b\u5668\u7ec4\u6210\u7684\u591a\u7ef4\u68c0\u6d4b\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u7ea7\u8054\u673a\u5236\u4f18\u5316\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002\u7ea7\u8054\u8bbe\u8ba1\u901a\u8fc7\u9010\u6b65\u8fc7\u6ee4\u67e5\u8be2\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u3002", "result": "\u5728\u6df7\u5408\u548c\u7279\u5b9a\u9886\u57df\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u8bef\u62a5\u663e\u8457\u51cf\u5c11\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u5efa\u8bae\u548c\u521b\u610f\u5199\u4f5c\u7b49\u654f\u611f\u9886\u57df\u3002\u7cfb\u7edf\u5728\u4e25\u683c\u64cd\u4f5c\u6a21\u5f0f\u4e0b\u4fdd\u6301\u9ad8\u5b89\u5168\u7cbe\u5ea6\u548c\u63a5\u8fd1\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\u3002\u4e0e\u975e\u7ea7\u8054\u6a21\u578b\u548c\u5916\u90e8\u62a4\u680f\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u6539\u8fdb\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5f03\u6743\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u4e3a\u53ef\u9760\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15329", "abs": "https://arxiv.org/abs/2602.15329", "authors": ["Siwei Wen", "Zhangcheng Wang", "Xingjian Zhang", "Lei Huang", "Wenjun Wu"], "title": "EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use", "comment": null, "summary": "Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.", "AI": {"tldr": "EventMemAgent\uff1a\u57fa\u4e8e\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\u7684\u4e3b\u52a8\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u77ed\u671f\u8bb0\u5fc6\u68c0\u6d4b\u4e8b\u4ef6\u8fb9\u754c\u3001\u957f\u671f\u8bb0\u5fc6\u7ed3\u6784\u5316\u5f52\u6863\uff0c\u7ed3\u5408\u591a\u7c92\u5ea6\u611f\u77e5\u5de5\u5177\u5305\u548cAgentic RL\u5b9e\u73b0\u7aef\u5230\u7aef\u63a8\u7406", "motivation": "\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u9762\u4e34\u65e0\u9650\u6d41\u5a92\u4f53\u8f93\u5165\u4e0eMLLMs\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u77db\u76fe\uff0c\u73b0\u6709\u88ab\u52a8\u5904\u7406\u65b9\u6cd5\u5728\u4fdd\u6301\u957f\u7a0b\u4e0a\u4e0b\u6587\u4e0e\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e4b\u95f4\u5b58\u5728\u6743\u8861", "method": "\u63d0\u51fa\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\uff1a\u77ed\u671f\u8bb0\u5fc6\u52a8\u6001\u5904\u7406\u6d41\u89c6\u9891\u5e27\uff08\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b+\u4e8b\u4ef6\u7c92\u5ea6\u6c34\u5e93\u91c7\u6837\uff09\uff0c\u957f\u671f\u8bb0\u5fc6\u6309\u4e8b\u4ef6\u7ed3\u6784\u5316\u5f52\u6863\uff1b\u7ed3\u5408\u591a\u7c92\u5ea6\u611f\u77e5\u5de5\u5177\u5305\u8fdb\u884c\u4e3b\u52a8\u8fed\u4ee3\u8bc1\u636e\u6355\u83b7\uff0c\u4f7f\u7528Agentic RL\u7aef\u5230\u7aef\u5185\u5316\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565", "result": "\u5728\u5728\u7ebf\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c", "conclusion": "EventMemAgent\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u548c\u5206\u5c42\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u4e2d\u65e0\u9650\u8f93\u5165\u4e0e\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u77db\u76fe\uff0c\u4e3a\u6d41\u5a92\u4f53\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6"}}
{"id": "2602.15236", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15236", "abs": "https://arxiv.org/abs/2602.15236", "authors": ["Anjie Qiao", "Zhen Wang", "Yaliang Li", "Jiahua Rao", "Yuedong Yang"], "title": "BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening", "comment": null, "summary": "Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.", "AI": {"tldr": "BindCLIP\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u751f\u6210\u5f0f\u59ff\u6001\u76d1\u7763\uff0c\u6539\u8fdb\u865a\u62df\u7b5b\u9009\u4e2d\u7684\u53e3\u888b-\u914d\u4f53\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u5347\u5bf9\u771f\u5b9e\u7ed3\u5408\u76f8\u4e92\u4f5c\u7528\u7684\u654f\u611f\u6027", "motivation": "\u73b0\u6709CLIP\u98ce\u683c\u6a21\u578b\uff08\u5982DrugCLIP\uff09\u7684\u8868\u793a\u5bf9\u7ec6\u7c92\u5ea6\u7ed3\u5408\u76f8\u4e92\u4f5c\u7528\u4e0d\u654f\u611f\uff0c\u53ef\u80fd\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6377\u5f84\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u6309\u771f\u5b9e\u7ed3\u5408\u517c\u5bb9\u6027\u5bf9\u914d\u4f53\u8fdb\u884c\u6392\u5e8f\u7684\u80fd\u529b", "method": "\u63d0\u51faBindCLIP\u7edf\u4e00\u5bf9\u6bd4-\u751f\u6210\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff1a\u8054\u5408\u8bad\u7ec3\u53e3\u888b\u548c\u914d\u4f53\u7f16\u7801\u5668\uff0c\u4f7f\u7528CLIP\u98ce\u683c\u5bf9\u6bd4\u5b66\u4e60\u7ed3\u5408\u53e3\u888b\u6761\u4ef6\u6269\u6563\u76ee\u6807\u8fdb\u884c\u7ed3\u5408\u59ff\u6001\u751f\u6210\uff0c\u59ff\u6001\u7ea7\u76d1\u7763\u76f4\u63a5\u5851\u9020\u68c0\u7d22\u5d4c\u5165\u7a7a\u95f4\u671d\u5411\u76f8\u4e92\u4f5c\u7528\u76f8\u5173\u7279\u5f81\uff1b\u5f15\u5165\u786c\u8d1f\u6837\u672c\u589e\u5f3a\u548c\u914d\u4f53-\u914d\u4f53\u951a\u5b9a\u6b63\u5219\u5316\u5668\u9632\u6b62\u8868\u793a\u5d29\u6e83", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff1b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u5916\u865a\u62df\u7b5b\u9009\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1b\u5728FEP+\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6539\u8fdb\u914d\u4f53\u7c7b\u4f3c\u7269\u6392\u5e8f", "conclusion": "\u5c06\u751f\u6210\u5f0f\u59ff\u6001\u7ea7\u76d1\u7763\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4ea7\u751f\u66f4\u5177\u76f8\u4e92\u4f5c\u7528\u611f\u77e5\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5728\u73b0\u5b9e\u7b5b\u9009\u8bbe\u7f6e\u4e2d\u6539\u5584\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u865a\u62df\u7b5b\u9009\u66f4\u63a5\u8fd1\u5b9e\u9645\u5e94\u7528"}}
{"id": "2602.15304", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15304", "abs": "https://arxiv.org/abs/2602.15304", "authors": ["Farzana Akter", "Rakib Hossain", "Deb Kanna Roy Toushi", "Mahmood Menon Khan", "Sultana Amin", "Lisan Al Amin"], "title": "Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization", "comment": null, "summary": "Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u4e0e\u5206\u5272\u5b66\u4e60\u7684\u6df7\u5408\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u51b3\u7b56\u652f\u6301\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u3001\u9690\u79c1\u6cc4\u9732\u3001\u901a\u4fe1\u5f00\u9500\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5e38\u53d7\u6cbb\u7406\u548c\u9690\u79c1\u89c4\u5219\u9650\u5236\uff0c\u65e0\u6cd5\u8de8\u673a\u6784\u6c47\u96c6\u60a3\u8005\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u652f\u6301\u51b3\u7b56\u5bfc\u5411\u533b\u7597\u5efa\u6a21\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u8054\u90a6\u5b66\u4e60(FL)\u548c\u5206\u5272\u5b66\u4e60(SL)\u7684\u6df7\u5408\u6846\u67b6\uff1a\u5ba2\u6237\u7aef\u4fdd\u7559\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\uff0c\u670d\u52a1\u5668\u6258\u7ba1\u9884\u6d4b\u5934\u90e8\uff0c\u901a\u8fc7\u6210\u5458\u63a8\u7406\u5ba1\u8ba1\u6cc4\u9732\uff0c\u91c7\u7528\u6fc0\u6d3b\u88c1\u526a\u548c\u9ad8\u65af\u566a\u58f0\u7b49\u8f7b\u91cf\u9632\u5fa1\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6df7\u5408FL-SL\u53d8\u4f53\u5728\u9884\u6d4b\u6027\u80fd\u548c\u51b3\u7b56\u4f18\u5148\u7ea7\u65b9\u9762\u4e0e\u72ec\u7acbFL\u6216SL\u76f8\u5f53\uff0c\u63d0\u4f9b\u53ef\u8c03\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u80fd\u51cf\u5c11\u5ba1\u8ba1\u6cc4\u9732\u3002", "conclusion": "\u6df7\u5408FL-SL\u4e3a\u9690\u79c1\u4fdd\u62a4\u533b\u7597\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u53ef\u5728\u6548\u7528\u3001\u6cc4\u9732\u98ce\u9669\u548c\u90e8\u7f72\u6210\u672c\u95f4\u8fdb\u884c\u660e\u786e\u5e73\u8861\u3002"}}
{"id": "2602.15656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15656", "abs": "https://arxiv.org/abs/2602.15656", "authors": ["Mustafa Yurdakul", "Zeynep Sena Bastug", "Ali Emre Gok", "Sakir Ta\u015fdemir"], "title": "A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models", "comment": null, "summary": "The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5f00\u8349\u8393\u6210\u719f\u5ea6\u6570\u636e\u96c6\uff0c\u5305\u542b566\u5f20\u56fe\u50cf\u548c1201\u4e2a\u6807\u6ce8\u5bf9\u8c61\uff0c\u5e76\u5728YOLO\u7cfb\u5217\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u6d4b\u8bd5\uff0c\u4e3a\u667a\u80fd\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\u3002", "motivation": "\u8349\u8393\u4f5c\u4e3a\u7ecf\u6d4e\u4ef7\u503c\u9ad8\u3001\u8425\u517b\u4e30\u5bcc\u7684\u6c34\u679c\uff0c\u4f20\u7edf\u6210\u719f\u5ea6\u5224\u65ad\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\u3001\u8bef\u5dee\u5927\uff0c\u9700\u8981\u8ba1\u7b97\u673a\u8f85\u52a9\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u516c\u5f00\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u8fdb\u884c\u6709\u6548\u6bd4\u8f83\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5f00\u8349\u8393\u6210\u719f\u5ea6\u6570\u636e\u96c6\uff0c\u5305\u542b\u5728\u571f\u8033\u5176\u4e24\u4e2a\u4e0d\u540c\u6e29\u5ba4\u4e2d\u91c7\u96c6\u7684566\u5f20\u56fe\u50cf\u548c1201\u4e2a\u6807\u6ce8\u5bf9\u8c61\uff0c\u4f7f\u7528YOLOv8\u3001YOLOv9\u548cYOLO11\u7b49\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "YOLOv9c\u6a21\u578b\u83b7\u5f97\u6700\u9ad8\u7cbe\u786e\u738790.94%\uff0cYOLO11s\u6a21\u578b\u83b7\u5f97\u6700\u9ad8\u53ec\u56de\u738783.74%\uff0cYOLOv8s\u6a21\u578b\u5728mAP@50\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523086.09%\u3002\u4e2d\u5c0f\u578b\u6a21\u578b\u5728\u6b64\u7c7b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u5e73\u8861\u9ad8\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e3a\u8349\u8393\u6210\u719f\u5ea6\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u4e2d\u5c0f\u578bYOLO\u6a21\u578b\u5728\u6b64\u7c7b\u519c\u4e1a\u5e94\u7528\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u667a\u80fd\u519c\u4e1a\u5e94\u7528\u5efa\u7acb\u4e86\u57fa\u7840\u53c2\u8003\u70b9\u3002"}}
{"id": "2602.15393", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15393", "abs": "https://arxiv.org/abs/2602.15393", "authors": ["Tom Trigano", "Yann Sepulcre", "Itshak Lapidot"], "title": "Doubly Stochastic Mean-Shift Clustering", "comment": "30 pages. arXiv admin note: text overlap with arXiv:2511.09202", "summary": "Standard Mean-Shift algorithms are notoriously sensitive to the bandwidth hyperparameter, particularly in data-scarce regimes where fixed-scale density estimation leads to fragmentation and spurious modes. In this paper, we propose Doubly Stochastic Mean-Shift (DSMS), a novel extension that introduces randomness not only in the trajectory updates but also in the kernel bandwidth itself. By drawing both the data samples and the radius from a continuous uniform distribution at each iteration, DSMS effectively performs a better exploration of the density landscape. We show that this randomized bandwidth policy acts as an implicit regularization mechanism, and provide convergence theoretical results. Comparative experiments on synthetic Gaussian mixtures reveal that DSMS significantly outperforms standard and stochastic Mean-Shift baselines, exhibiting remarkable stability and preventing over-segmentation in sparse clustering scenarios without other performance degradation.", "AI": {"tldr": "DSMS\u901a\u8fc7\u968f\u673a\u5316\u5e26\u5bbd\u548c\u6570\u636e\u91c7\u6837\u6539\u8fdbMean-Shift\u7b97\u6cd5\uff0c\u89e3\u51b3\u56fa\u5b9a\u5e26\u5bbd\u5728\u6570\u636e\u7a00\u758f\u65f6\u7684\u8fc7\u5206\u5272\u95ee\u9898", "motivation": "\u4f20\u7edfMean-Shift\u7b97\u6cd5\u5bf9\u5e26\u5bbd\u8d85\u53c2\u6570\u654f\u611f\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u56fa\u5b9a\u5c3a\u5ea6\u7684\u5bc6\u5ea6\u4f30\u8ba1\u4f1a\u5bfc\u81f4\u788e\u7247\u5316\u548c\u865a\u5047\u6a21\u5f0f", "method": "\u63d0\u51fa\u53cc\u91cd\u968f\u673aMean-Shift\uff08DSMS\uff09\uff0c\u5728\u8f68\u8ff9\u66f4\u65b0\u548c\u6838\u5e26\u5bbd\u4e2d\u90fd\u5f15\u5165\u968f\u673a\u6027\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ece\u8fde\u7eed\u5747\u5300\u5206\u5e03\u4e2d\u91c7\u6837\u6570\u636e\u548c\u534a\u5f84", "result": "\u5728\u5408\u6210\u9ad8\u65af\u6df7\u5408\u6570\u636e\u4e0a\u7684\u6bd4\u8f83\u5b9e\u9a8c\u663e\u793a\uff0cDSMS\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u548c\u968f\u673aMean-Shift\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u9632\u6b62\u7a00\u758f\u805a\u7c7b\u573a\u666f\u4e2d\u7684\u8fc7\u5206\u5272", "conclusion": "\u968f\u673a\u5e26\u5bbd\u7b56\u7565\u4f5c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u673a\u5236\uff0c\u80fd\u66f4\u597d\u5730\u63a2\u7d22\u5bc6\u5ea6\u666f\u89c2\uff0c\u63d0\u4f9b\u6536\u655b\u7406\u8bba\u7ed3\u679c\uff0c\u4e14\u4e0d\u4f1a\u5bfc\u81f4\u5176\u4ed6\u6027\u80fd\u4e0b\u964d"}}
{"id": "2602.15457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15457", "abs": "https://arxiv.org/abs/2602.15457", "authors": ["Dmitry Zhevnenko", "Ilya Makarov", "Aleksandr Kovalenko", "Fedor Meshchaninov", "Anton Kozhukhov", "Vladislav Travnikov", "Makar Ippolitov", "Kirill Yashunin", "Iurii Katser"], "title": "Benchmarking IoT Time-Series AD with Event-Level Augmentations", "comment": "https://underline.io/events/521/sessions/21822/lecture/143905-benchmarking-iot-time-series-ad-with-event-level-augmentations?tab=poster", "summary": "Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u7269\u8054\u7f51\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u4e8b\u4ef6\u7ea7\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u542b\u7edf\u4e00\u7684\u73b0\u5b9e\u6270\u52a8\u6a21\u62df\u548c\u4f20\u611f\u5668\u7ea7\u6839\u56e0\u5206\u6790\uff0c\u8bc4\u4f3014\u4e2a\u6a21\u578b\u540e\u53d1\u73b0\u6ca1\u6709\u901a\u7528\u6700\u4f18\u6a21\u578b\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5404\u5f02\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u591a\u5173\u6ce8\u70b9\u7ea7\u7ed3\u679c\u548c\u7cbe\u5fc3\u6574\u7406\u7684\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u9760\u6027\u3001\u53ca\u65f6\u6027\u548c\u73b0\u5b9e\u6270\u52a8\u7684\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9009\u62e9\u4ef7\u503c\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u7684\u4e8b\u4ef6\u7ea7\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u542b\u6821\u51c6\u4f20\u611f\u5668\u4e22\u5931\u3001\u7ebf\u6027\u548c\u5bf9\u6570\u6f02\u79fb\u3001\u52a0\u6027\u566a\u58f0\u3001\u7a97\u53e3\u504f\u79fb\u7b49\u73b0\u5b9e\u6270\u52a8\u6a21\u62df\uff1b\u901a\u8fc7\u63a9\u7801\u7f3a\u5931\u5f52\u96f6\u548c\u6bcf\u901a\u9053\u5f71\u54cd\u4f30\u8ba1\u8fdb\u884c\u4f20\u611f\u5668\u7ea7\u63a2\u6d4b\uff1b\u57285\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c2\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f3014\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u3002", "result": "\u6ca1\u6709\u901a\u7528\u6700\u4f18\u6a21\u578b\uff1a\u56fe\u7ed3\u6784\u6a21\u578b\u5728\u4f20\u611f\u5668\u4e22\u5931\u548c\u957f\u4e8b\u4ef6\u4e0b\u8f6c\u79fb\u6027\u6700\u597d\uff1b\u5bc6\u5ea6/\u6d41\u6a21\u578b\u5728\u6e05\u6d01\u7a33\u5b9a\u5de5\u5382\u8868\u73b0\u597d\u4f46\u5bf9\u5355\u8c03\u6f02\u79fb\u8106\u5f31\uff1b\u8c31CNN\u5728\u5468\u671f\u6027\u5f3a\u7684\u573a\u666f\u9886\u5148\uff1b\u91cd\u5efa\u81ea\u7f16\u7801\u5668\u5728\u57fa\u672c\u4f20\u611f\u5668\u5ba1\u67e5\u540e\u5177\u6709\u7ade\u4e89\u529b\uff1b\u9884\u6d4b/\u6df7\u5408\u52a8\u6001\u6a21\u578b\u5728\u6545\u969c\u7834\u574f\u65f6\u95f4\u4f9d\u8d56\u65f6\u6709\u6548\u4f46\u5bf9\u7a97\u53e3\u654f\u611f\u3002", "conclusion": "\u8bc4\u4f30\u534f\u8bae\u4e3a\u5b9e\u9645\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u73b0\u5b9e\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u80fd\u4e3a\u8bbe\u8ba1\u9009\u62e9\u63d0\u4f9b\u4fe1\u606f\uff08\u5982\u7528\u9ad8\u65af\u5bc6\u5ea6\u66ff\u6362\u5f52\u4e00\u5316\u6d41\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u56fa\u5b9a\u5b66\u4e60DAG\u4f1a\u8f7b\u5fae\u63d0\u5347\u6e05\u6d01\u96c6\u8868\u73b0\u4f46\u5927\u5e45\u589e\u52a0\u6f02\u79fb\u654f\u611f\u6027\uff09\u3002"}}
{"id": "2602.15782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15782", "abs": "https://arxiv.org/abs/2602.15782", "authors": ["Ines Montoya-Espinagosa", "Antonio Agudo"], "title": "Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting", "comment": "CAI 2026", "summary": "Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5929\u7a7a\u56fe\u50cf\u3001\u5149\u4f0f\u5386\u53f2\u6570\u636e\u548c\u6c14\u8c61\u6570\u636e\u7684\u591a\u6a21\u6001\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u77ed\u671f\u548c\u957f\u671f\u5149\u4f0f\u9884\u6d4b\uff0c\u7279\u522b\u5173\u6ce8\u63d0\u9ad8\u659c\u5761\u4e8b\u4ef6\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e91\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u7279\u522b\u662f\u592a\u9633\u80fd\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5149\u4f0f\u53d1\u7535\u7684\u6ce2\u52a8\u6027\u7ed9\u7535\u7f51\u7ba1\u7406\u5e26\u6765\u6311\u6218\u3002\u9700\u8981\u63d0\u9ad8\u5149\u4f0f\u9884\u6d4b\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4e91\u5929\u6c14\u6761\u4ef6\u4e0b\uff0c\u4ee5\u652f\u6301\u7535\u7f51\u9ad8\u6548\u8fd0\u884c\u548c\u592a\u9633\u80fd\u53d8\u5f02\u6027\u7ba1\u7406\u3002", "method": "\u91c7\u7528\u6df7\u5408\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u5929\u7a7a\u56fe\u50cf\u3001\u5149\u4f0f\u5386\u53f2\u6570\u636e\u548c\u6c14\u8c61\u6570\u636e\u3002\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6574\u5408\u5355\u4e2a\u548c\u591a\u4e2a\u6c14\u8c61\u53d8\u91cf\u4ee5\u53ca\u592a\u9633\u4f4d\u7f6e\u5206\u6790\uff0c\u7528\u4e8e\u5373\u65f6\u9884\u6d4b\u548c\u77ed\u671f/\u957f\u671f\u9884\u6d4b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u52a0\u5165\u6c14\u8c61\u6570\u636e\uff08\u7279\u522b\u662f\u5730\u8868\u957f\u6ce2\u8f90\u5c04\u3001\u5411\u4e0b\u8f90\u5c04\u4ee5\u53ca\u98ce\u4e0e\u592a\u9633\u4f4d\u7f6e\u7684\u7ec4\u5408\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u5373\u65f6\u9884\u6d4b\u548c\u77ed\u671f/\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u4e91\u5929\u6c14\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6574\u5408\u591a\u6837\u5316\u6570\u636e\u6e90\u5bf9\u4e8e\u63d0\u9ad8\u592a\u9633\u80fd\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u7535\u7f51\u8fd0\u8425\u548c\u592a\u9633\u80fd\u53d8\u5f02\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2602.15546", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15546", "abs": "https://arxiv.org/abs/2602.15546", "authors": ["Tom\u00e0s Garriga", "Gerard Sanz", "Eduard Serrahima de Cambra", "Axel Brando"], "title": "CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals", "comment": null, "summary": "The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b0\u65b9\u6cd5CEPAE\uff0c\u901a\u8fc7\u71b5\u60e9\u7f5a\u635f\u5931\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9f13\u52b1\u89e3\u8026\u8868\u793a\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u91d1\u878d\u3001\u533b\u7597\u548c\u8425\u9500\u7b49\u9886\u57df\uff0c\u51c6\u786e\u7684\u65f6\u95f4\u5e8f\u5217\u53cd\u4e8b\u5b9e\u63a8\u7406\u5bf9\u4e8e\u7406\u89e3\u4e8b\u4ef6\u6216\u5904\u7406\u5bf9\u65f6\u95f4\u5e8f\u5217\u7ed3\u679c\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u6846\u67b6\u548c\u6eaf\u56e0-\u884c\u52a8-\u9884\u6d4b\u6d41\u7a0b\uff0c\u9996\u5148\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\u9002\u914d\u5230\u65f6\u95f4\u5e8f\u5217\u8bbe\u7f6e\uff0c\u7136\u540e\u63d0\u51fa\u6761\u4ef6\u71b5\u60e9\u7f5a\u81ea\u7f16\u7801\u5668\uff08CEPAE\uff09\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u71b5\u60e9\u7f5a\u635f\u5931\u6765\u9f13\u52b1\u89e3\u8026\u7684\u6570\u636e\u8868\u793a\u3002", "result": "\u5728\u5408\u6210\u3001\u534a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cCEPAE\u5728\u8bc4\u4f30\u6307\u6807\u4e0a\u901a\u5e38\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "CEPAE\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u7f16\u7801\u5668\u57fa\u7840\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53d7\u5e02\u573a\u4e8b\u4ef6\u5f71\u54cd\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u53cd\u4e8b\u5b9e\u9884\u6d4b\u3002"}}
{"id": "2602.15637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15637", "abs": "https://arxiv.org/abs/2602.15637", "authors": ["Amirreza Dolatpour Fathkouhi", "Alireza Namazi", "Heman Shakeri"], "title": "The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems", "comment": null, "summary": "Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \\emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \\emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \\emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.", "AI": {"tldr": "\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u57fa\u51c6\u5b58\u5728\u5e73\u7a33\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u7b80\u5355\u65b9\u6cd5\u5728\u4e3b\u5bfc\u7a33\u6001\u7cfb\u7edf\u4e2d\u8868\u73b0\u865a\u5047\u4f18\u8d8a\u3002\u8bba\u6587\u63d0\u51fa\u5206\u5c42\u538b\u529b\u6d4b\u8bd5\uff0c\u533a\u5206\u5e73\u7a33\u548c\u77ac\u6001\u4e24\u79cd\u72b6\u6001\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u5173\u952e\u77ac\u6001\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u4fdd\u6301\u66f4\u597d\u7684\u5f62\u6001\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u57fa\u51c6\u4f7f\u7528\u5747\u5300\u968f\u673a\u63a9\u7801\u548c\u5f62\u72b6\u65e0\u5173\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982MSE\u3001RMSE\uff09\uff0c\u5728\u5177\u6709\u4e3b\u5bfc\u5438\u5f15\u5b50\u7684\u7cfb\u7edf\u4e2d\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u5e73\u7a33\u6027\u504f\u5dee\u3002\u8fd9\u79cd\u504f\u5dee\u4f7f\u5f97\u7b80\u5355\u65b9\u6cd5\u5728\u4e3b\u8981\u7531\u4f4e\u71b5\u7a33\u6001\u7ec4\u6210\u7684\u57fa\u51c6\u4e2d\u8868\u73b0\u865a\u5047\u4f18\u8d8a\uff0c\u800c\u5ffd\u7565\u4e86\u5728\u5173\u952e\u77ac\u6001\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u538b\u529b\u6d4b\u8bd5\u6846\u67b6\uff0c\u5c06\u8bc4\u4f30\u5212\u5206\u4e3a\u5e73\u7a33\u548c\u77ac\u6001\u4e24\u79cd\u72b6\u6001\u3002\u4f7f\u7528\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\uff08CGM\uff09\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5229\u7528\u5176\u7cbe\u786e\u7684\u5730\u9762\u771f\u5b9e\u5f3a\u8feb\u51fd\u6570\uff08\u8fdb\u98df\u3001\u80f0\u5c9b\u7d20\uff09\u5b9e\u73b0\u51c6\u786e\u7684\u72b6\u6001\u8bc6\u522b\u3002\u4ece\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u63a8\u5bfc\u7ecf\u9a8c\u7f3a\u5931\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5b8c\u6574\u8bad\u7ec3\u6570\u636e\uff0c\u9632\u6b62\u6a21\u578b\u5229\u7528\u4e0d\u73b0\u5b9e\u7684\u5e72\u51c0\u89c2\u6d4b\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1a1\uff09\u5728\u7a33\u5b9a\u533a\u95f4\uff0c\u7ebf\u6027\u63d2\u503c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u660e\u590d\u6742\u67b6\u6784\u5728\u4f4e\u71b5\u72b6\u6001\u4e0b\u8ba1\u7b97\u6d6a\u8d39\uff1b2\uff09\u5728\u5173\u952e\u77ac\u6001\uff08\u9910\u540e\u5cf0\u503c\u3001\u4f4e\u8840\u7cd6\u4e8b\u4ef6\uff09\u4e2d\uff0c\u7ebf\u6027\u65b9\u6cd5\u5f62\u6001\u4fdd\u771f\u5ea6\uff08DTW\uff09\u663e\u8457\u4e0b\u964d\uff0c\u4e0eRMSE\u4e0d\u6210\u6bd4\u4f8b\uff0c\u79f0\u4e3aRMSE\u5e7b\u8c61\uff1b3\uff09\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u77ac\u6001\u4e2d\u80fd\u540c\u65f6\u4fdd\u6301\u70b9\u51c6\u786e\u6027\u548c\u5f62\u6001\u5b8c\u6574\u6027\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u63d2\u503c\u8bc4\u4f30\u9700\u8981\u8003\u8651\u72b6\u6001\u6761\u4ef6\u6027\uff0c\u907f\u514d\u5e73\u7a33\u6027\u504f\u5dee\u3002\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u77ac\u6001\u4e2d\u4fdd\u6301\u5f62\u6001\u4fdd\u771f\u5ea6\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u8be5\u6846\u67b6\u53ef\u63a8\u5e7f\u5230\u4efb\u4f55\u5e38\u89c4\u5e73\u7a33\u6027\u4e3b\u5bfc\u5173\u952e\u77ac\u6001\u7684\u53d7\u8c03\u8282\u7cfb\u7edf\u3002"}}
{"id": "2602.15677", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.15677", "abs": "https://arxiv.org/abs/2602.15677", "authors": ["Neelay Velingker", "Alaia Solko-Breslin", "Mayank Keoliya", "Seewon Choi", "Jiayi Xin", "Anika Marathe", "Alireza Oraii", "Rajat Deo", "Sameed Khatana", "Rajeev Alur", "Mayur Naik", "Eric Wong"], "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events", "comment": "24 pages, 6 figures", "summary": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).", "AI": {"tldr": "CAMEL\u662f\u9996\u4e2a\u80fd\u591f\u8fdb\u884c\u5fc3\u7535\u56fe\u4fe1\u53f7\u957f\u671f\u63a8\u7406\u548c\u9884\u6d4b\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u5f53\u524d\u7684\u5fc3\u7535\u56fe\u8bed\u8a00\u6a21\u578b\uff08ELMs\uff09\u867d\u7136\u80fd\u591f\u8fdb\u884c\u5206\u7c7b\u548c\u62a5\u544a\u751f\u6210\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u7684\u5fc3\u810f\u4e8b\u4ef6\uff0c\u800c\u8fd9\u5bf9\u4e34\u5e8a\u65e9\u671f\u5e72\u9884\u5177\u6709\u91cd\u8981\u4ef7\u503c", "method": "\u5f00\u53d1\u4e13\u95e8\u7684ECG\u7f16\u7801\u5668\u5b9e\u73b0\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e0e\u6587\u672c\u7684\u8de8\u6a21\u6001\u7406\u89e3\uff0c\u91c7\u7528LoRA\u9002\u914d\u548c\u8bfe\u7a0b\u5b66\u4e60\u7ba1\u9053\u8bad\u7ec3\uff0c\u5305\u62ecECG\u5206\u7c7b\u3001\u6307\u6807\u8ba1\u7b97\u548c\u591a\u8f6e\u5bf9\u8bdd\u63a8\u7406", "result": "\u57286\u4e2a\u4efb\u52a1\u548c9\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728ECGBench\u4e0a\u83b7\u5f97+7.0%\u7edd\u5bf9\u5e73\u5747\u589e\u76ca\uff0c\u5728ECGForecastBench\u4e0a\u6bd4\u5168\u76d1\u7763\u6a21\u578b\u9ad8+12.4%\uff0c\u6bd4\u96f6\u6837\u672cELMs\u9ad8+21.1%", "conclusion": "CAMEL\u662f\u9996\u4e2a\u5177\u5907\u5fc3\u7535\u56fe\u4fe1\u53f7\u957f\u671f\u63a8\u7406\u80fd\u529b\u7684ELM\uff0c\u80fd\u591f\u9884\u6d4b\u672a\u6765\u5fc3\u810f\u4e8b\u4ef6\uff0c\u4e3a\u4e34\u5e8a\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u91cd\u8981\u5de5\u5177"}}
{"id": "2602.15752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15752", "abs": "https://arxiv.org/abs/2602.15752", "authors": ["Ren Kishimoto", "Rikiya Takehi", "Koichi Tanaka", "Masahiro Nomura", "Riku Togashi", "Yoji Tomita", "Yuta Saito"], "title": "Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching", "comment": "Published as a conference paper at ICLR 2026", "summary": "On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.\n  In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.", "AI": {"tldr": "\u63d0\u51faMRet\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u7559\u5b58\u66f2\u7ebf\u52a8\u6001\u8c03\u6574\u63a8\u8350\uff0c\u6700\u5927\u5316\u53cc\u8fb9\u5339\u914d\u5e73\u53f0\u7684\u7528\u6237\u7559\u5b58\u7387\uff0c\u800c\u975e\u4f20\u7edf\u65b9\u6cd5\u53ea\u5173\u6ce8\u5339\u914d\u6570\u91cf\u6216\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u8fb9\u5339\u914d\u5e73\u53f0\uff08\u5982\u5728\u7ebf\u7ea6\u4f1a\u3001\u62db\u8058\uff09\u901a\u5e38\u4ee5\u6700\u5927\u5316\u5339\u914d\u6570\u91cf\u4e3a\u76ee\u6807\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u7528\u6237\u5339\u914d\u5206\u5e03\u4e0d\u5747\uff1a\u90e8\u5206\u7528\u6237\u83b7\u5f97\u8fc7\u591a\u5339\u914d\uff0c\u800c\u8bb8\u591a\u7528\u6237\u83b7\u5f97\u5f88\u5c11\u5339\u914d\u5e76\u6700\u7ec8\u6d41\u5931\u3002\u867d\u7136\u516c\u5e73\u6027\u76ee\u6807\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u516c\u5e73\u6027\u672c\u8eab\u5e76\u975e\u5e73\u53f0\u7684\u6700\u7ec8\u76ee\u6807\uff0c\u7528\u6237\u4e0d\u4f1a\u4ec5\u4ec5\u56e0\u4e3a\u66dd\u5149\u5747\u7b49\u800c\u5956\u52b1\u5e73\u53f0\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u7528\u6237\u7559\u5b58\u901a\u5e38\u662f\u5e73\u53f0\u7684\u7ec8\u6781\u76ee\u6807\uff0c\u5355\u7eaf\u4f9d\u8d56\u516c\u5e73\u6027\u65e0\u6cd5\u6709\u6548\u4f18\u5316\u7559\u5b58\u3002", "method": "\u63d0\u51faMRet\uff08Matching for Retention\uff09\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u52a8\u6001\u5b66\u4e60\u6392\u5e8f\uff08LTR\uff09\u65b9\u6cd5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u4ece\u6bcf\u4e2a\u7528\u6237\u7684\u4e2a\u4eba\u8d44\u6599\u548c\u4ea4\u4e92\u5386\u53f2\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u7559\u5b58\u66f2\u7ebf\u6765\u5efa\u6a21\u7528\u6237\u7559\u5b58\u3002\u57fa\u4e8e\u8fd9\u4e9b\u66f2\u7ebf\uff0cMRet\u52a8\u6001\u8c03\u6574\u63a8\u8350\u7b56\u7565\uff0c\u8054\u5408\u8003\u8651\u63a8\u8350\u63a5\u6536\u65b9\u548c\u88ab\u63a8\u8350\u65b9\u7684\u7559\u5b58\u6536\u76ca\uff0c\u4ece\u800c\u5c06\u6709\u9650\u7684\u5339\u914d\u673a\u4f1a\u5206\u914d\u5230\u6700\u80fd\u63d0\u5347\u6574\u4f53\u7559\u5b58\u7684\u5730\u65b9\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u6765\u81ea\u4e3b\u8981\u5728\u7ebf\u7ea6\u4f1a\u5e73\u53f0\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cMRet\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7528\u6237\u7559\u5b58\u7387\u3002\u7531\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u5339\u914d\u6570\u91cf\u6216\u516c\u5e73\u6027\u800c\u975e\u7559\u5b58\uff0cMRet\u5728\u63d0\u5347\u7528\u6237\u7559\u5b58\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MRet\u7b97\u6cd5\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u7528\u6237\u7559\u5b58\u800c\u975e\u5339\u914d\u6570\u91cf\u6216\u516c\u5e73\u6027\uff0c\u4e3a\u53cc\u8fb9\u5339\u914d\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u7528\u6237\u4fdd\u7559\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u7528\u6237\u4e2a\u6027\u5316\u7559\u5b58\u884c\u4e3a\uff0c\u52a8\u6001\u5206\u914d\u5339\u914d\u673a\u4f1a\u4ee5\u6700\u5927\u5316\u6574\u4f53\u7559\u5b58\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
