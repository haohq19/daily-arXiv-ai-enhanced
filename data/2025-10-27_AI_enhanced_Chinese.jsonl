{"id": "2510.20887", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20887", "abs": "https://arxiv.org/abs/2510.20887", "authors": ["Anujraaj Argo Goyal", "Guocheng Gordon Qian", "Huseyin Coskun", "Aarush Gupta", "Himmy Tam", "Daniil Ostashev", "Ju Hu", "Dhritiman Sagar", "Sergey Tulyakov", "Kfir Aberman", "Kuan-Chieh Jackson Wang"], "title": "Preventing Shortcuts in Adapter Training via Providing the Shortcuts", "comment": "Accepted to NeurIPS 2025, webpage:\n  https://snap-research.github.io/shortcut-rerouting/", "summary": "Adapter-based training has emerged as a key mechanism for extending the\ncapabilities of powerful foundation image generators, enabling personalized and\nstylized text-to-image synthesis. These adapters are typically trained to\ncapture a specific target attribute, such as subject identity, using\nsingle-image reconstruction objectives. However, because the input image\ninevitably contains a mixture of visual factors, adapters are prone to entangle\nthe target attribute with incidental ones, such as pose, expression, and\nlighting. This spurious correlation problem limits generalization and obstructs\nthe model's ability to adhere to the input text prompt. In this work, we\nuncover a simple yet effective solution: provide the very shortcuts we wish to\neliminate during adapter training. In Shortcut-Rerouted Adapter Training,\nconfounding factors are routed through auxiliary modules, such as ControlNet or\nLoRA, eliminating the incentive for the adapter to internalize them. The\nauxiliary modules are then removed during inference. When applied to tasks like\nfacial and full-body identity injection, our approach improves generation\nquality, diversity, and prompt adherence. These results point to a general\ndesign principle in the era of large models: when seeking disentangled\nrepresentations, the most effective path may be to establish shortcuts for what\nshould NOT be learned.", "AI": {"tldr": "\u63d0\u51faShortcut-Rerouted Adapter Training\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9002\u914d\u5668\u8bad\u7ec3\u4e2d\u4e3a\u6df7\u6dc6\u56e0\u7d20\u5efa\u7acb\u8f85\u52a9\u6a21\u5757\uff0c\u907f\u514d\u76ee\u6807\u5c5e\u6027\u4e0e\u65e0\u5173\u56e0\u7d20\u7684\u7ea0\u7f20\uff0c\u4ece\u800c\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u6587\u672c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9002\u914d\u5668\u7684\u8bad\u7ec3\u65b9\u6cd5\u5bb9\u6613\u5c06\u76ee\u6807\u5c5e\u6027\u4e0e\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u7b49\u65e0\u5173\u56e0\u7d20\u7ea0\u7f20\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6587\u672c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "method": "\u5728\u9002\u914d\u5668\u8bad\u7ec3\u671f\u95f4\uff0c\u4e3a\u6df7\u6dc6\u56e0\u7d20\u5efa\u7acb\u8f85\u52a9\u6a21\u5757\uff08\u5982ControlNet\u6216LoRA\uff09\uff0c\u8ba9\u8fd9\u4e9b\u56e0\u7d20\u901a\u8fc7\u6377\u5f84\u4f20\u9012\uff0c\u6d88\u9664\u9002\u914d\u5668\u5b66\u4e60\u5b83\u4eec\u7684\u52a8\u673a\uff0c\u63a8\u7406\u65f6\u79fb\u9664\u8f85\u52a9\u6a21\u5757\u3002", "result": "\u5728\u4eba\u8138\u548c\u5168\u8eab\u8eab\u4efd\u6ce8\u5165\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "conclusion": "\u5728\u5927\u578b\u6a21\u578b\u65f6\u4ee3\uff0c\u5bfb\u6c42\u89e3\u8026\u8868\u793a\u7684\u6700\u6709\u6548\u8def\u5f84\u53ef\u80fd\u662f\u4e3a\u4e0d\u5e94\u5b66\u4e60\u7684\u5185\u5bb9\u5efa\u7acb\u6377\u5f84\u3002"}}
{"id": "2510.20888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20888", "abs": "https://arxiv.org/abs/2510.20888", "authors": ["Yuxuan Bian", "Xin Chen", "Zenan Li", "Tiancheng Zhi", "Shen Sang", "Linjie Luo", "Qiang Xu"], "title": "Video-As-Prompt: Unified Semantic Control for Video Generation", "comment": "Website: https://bytedance.github.io/Video-As-Prompt", "summary": "Unified, generalizable semantic control in video generation remains a\ncritical open challenge. Existing methods either introduce artifacts by\nenforcing inappropriate pixel-wise priors from structure-based controls, or\nrely on non-generalizable, condition-specific finetuning or task-specific\narchitectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes\nthis problem as in-context generation. VAP leverages a reference video as a\ndirect semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via\na plug-and-play Mixture-of-Transformers (MoT) expert. This architecture\nprevents catastrophic forgetting and is guided by a temporally biased position\nembedding that eliminates spurious mapping priors for robust context retrieval.\nTo power this approach and catalyze future research, we built VAP-Data, the\nlargest dataset for semantic-controlled video generation with over 100K paired\nvideos across 100 semantic conditions. As a single unified model, VAP sets a\nnew state-of-the-art for open-source methods, achieving a 38.7% user preference\nrate that rivals leading condition-specific commercial models. VAP's strong\nzero-shot generalization and support for various downstream applications mark a\nsignificant advance toward general-purpose, controllable video generation.", "AI": {"tldr": "Video-As-Prompt (VAP) \u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u53c2\u8003\u89c6\u9891\u4f5c\u4e3a\u8bed\u4e49\u63d0\u793a\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684Video Diffusion Transformer\u548c\u63d2\u4ef6\u5f0fMixture-of-Transformers\u4e13\u5bb6\u5b9e\u73b0\u7edf\u4e00\u7684\u8bed\u4e49\u63a7\u5236\uff0c\u65e0\u9700\u7279\u5b9a\u6761\u4ef6\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4e2d\u7edf\u4e00\u3001\u53ef\u6cdb\u5316\u7684\u8bed\u4e49\u63a7\u5236\u96be\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5f15\u5165\u4f2a\u5f71\uff0c\u8981\u4e48\u4f9d\u8d56\u975e\u6cdb\u5316\u7684\u7279\u5b9a\u6761\u4ef6\u5fae\u8c03\u6216\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u4e0a\u4e0b\u6587\u751f\u6210\uff0c\u4f7f\u7528\u53c2\u8003\u89c6\u9891\u4f5c\u4e3a\u76f4\u63a5\u8bed\u4e49\u63d0\u793a\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684Video Diffusion Transformer\u548c\u63d2\u4ef6\u5f0fMixture-of-Transformers\u4e13\u5bb6\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u504f\u7f6e\u4f4d\u7f6e\u5d4c\u5165\u6765\u6d88\u9664\u865a\u5047\u6620\u5c04\u5148\u9a8c\u3002", "result": "\u4f5c\u4e3a\u5355\u4e00\u7edf\u4e00\u6a21\u578b\uff0cVAP\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u83b7\u5f9738.7%\u7684\u7528\u6237\u504f\u597d\u7387\uff0c\u4e0e\u9886\u5148\u7684\u7279\u5b9a\u6761\u4ef6\u5546\u4e1a\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u5bf9\u89c6\u9891\u7684VAP-Data\u6570\u636e\u96c6\u3002", "conclusion": "VAP\u7684\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u7684\u652f\u6301\uff0c\u6807\u5fd7\u7740\u5411\u901a\u7528\u53ef\u63a7\u89c6\u9891\u751f\u6210\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.21027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21027", "abs": "https://arxiv.org/abs/2510.21027", "authors": ["Zhe Fei", "Mehmet Yigit Turali", "Shreyas Rajesh", "Xinyang Dai", "Huyen Pham", "Pavan Holur", "Yuhui Zhu", "Larissa Mooney", "Yih-Ing Hser", "Vwani Roychowdhury"], "title": "Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems", "comment": "NeurIPS 2025: The Second Workshop on GenAI for Health: Potential,\n  Trust, and Policy Compliance", "summary": "Harmonizing medication data across Electronic Health Record (EHR) systems is\na persistent barrier to monitoring medications for opioid use disorder (MOUD).\nIn heterogeneous EHR systems, key prescription attributes are scattered across\ndifferently formatted fields and freetext notes. We present a practical\nframework that customizes open source large language models (LLMs), including\nLlama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription\nattributes (prescription date, drug name, duration, total quantity, daily\nquantity, and refills) from heterogeneous, site specific data and compute a\nstandardized metric of medication coverage, \\emph{MOUD days}, per patient. Our\npipeline processes records directly in a fixed JSON schema, followed by\nlightweight normalization and cross-field consistency checks. We evaluate the\nsystem on prescription level EHR data from five clinics in a national OUD study\n(25{,}605 records from 1{,}257 patients), using a previously annotated\nbenchmark of 10{,}369 records (776 patients) as the ground truth. Performance\nis reported as coverage (share of records with a valid, matchable output) and\nrecord-level exact-match accuracy. Larger models perform best overall:\nQwen2.5-32B achieves \\textbf{93.4\\%} coverage with \\textbf{93.0\\%} exact-match\naccuracy across clinics, and MedGemma-27B attains\n\\textbf{93.1\\%}/\\textbf{92.2\\%}. A brief error review highlights three common\nissues and fixes: imputing missing dosage fields using within-drug norms,\nhandling monthly/weekly injectables (e.g., Vivitrol) by setting duration from\nthe documented schedule, and adding unit checks to prevent mass units (e.g.,\n``250 g'') from being misread as daily counts. By removing brittle,\nsite-specific ETL and supporting local, privacy-preserving deployment, this\napproach enables consistent cross-site analyses of MOUD exposure, adherence,\nand retention in real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4ece\u5f02\u6784\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u6cbb\u7597\u5904\u65b9\u4fe1\u606f\uff0c\u5e76\u8ba1\u7b97\u6807\u51c6\u5316\u7684\u836f\u7269\u8986\u76d6\u5929\u6570\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\u4e2d\u836f\u7269\u6570\u636e\u683c\u5f0f\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u6cbb\u7597\u5904\u65b9\u7684\u6807\u51c6\u5316\u76d1\u6d4b\u9700\u6c42\u3002", "method": "\u5b9a\u5236\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08Llama\u3001Qwen\u3001Gemma\u3001MedGemma\uff09\u6765\u63d0\u53d6\u5904\u65b9\u5c5e\u6027\uff0c\u91c7\u7528\u56fa\u5b9aJSON\u6a21\u5f0f\u5904\u7406\u8bb0\u5f55\uff0c\u5e76\u8fdb\u884c\u8f7b\u91cf\u7ea7\u6807\u51c6\u5316\u548c\u8de8\u5b57\u6bb5\u4e00\u81f4\u6027\u68c0\u67e5\u3002", "result": "\u572825,605\u6761\u8bb0\u5f55\u4e0a\u8bc4\u4f30\uff0cQwen2.5-32B\u8fbe\u523093.4%\u8986\u76d6\u7387\u548c93.0%\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\uff0cMedGemma-27B\u8fbe\u523093.1%/92.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u8106\u5f31\u7684\u7ad9\u70b9\u7279\u5b9aETL\u6d41\u7a0b\uff0c\u652f\u6301\u672c\u5730\u9690\u79c1\u4fdd\u62a4\u90e8\u7f72\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2dMOUD\u66b4\u9732\u3001\u4f9d\u4ece\u6027\u548c\u4fdd\u7559\u7684\u8de8\u7ad9\u70b9\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.21063", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21063", "abs": "https://arxiv.org/abs/2510.21063", "authors": ["Abdullah Turer", "Yongsheng Bai", "Halil Sezen", "Alper Yilmaz"], "title": "Deep learning-based automated damage detection in concrete structures using images from earthquake events", "comment": "6 pages, 1 figure", "summary": "Timely assessment of integrity of structures after seismic events is crucial\nfor public safety and emergency response. This study focuses on assessing the\nstructural damage conditions using deep learning methods to detect exposed\nsteel reinforcement in concrete buildings and bridges after large earthquakes.\nSteel bars are typically exposed after concrete spalling or large flexural or\nshear cracks. The amount and distribution of exposed steel reinforcement is an\nindication of structural damage and degradation. To automatically detect\nexposed steel bars, new datasets of images collected after the 2023 Turkey\nEarthquakes were labeled to represent a wide variety of damaged concrete\nstructures. The proposed method builds upon a deep learning framework, enhanced\nwith fine-tuning, data augmentation, and testing on public datasets. An\nautomated classification framework is developed that can be used to identify\ninside/outside buildings and structural components. Then, a YOLOv11 (You Only\nLook Once) model is trained to detect cracking and spalling damage and exposed\nbars. Another YOLO model is finetuned to distinguish different categories of\nstructural damage levels. All these trained models are used to create a hybrid\nframework to automatically and reliably determine the damage levels from input\nimages. This research demonstrates that rapid and automated damage detection\nfollowing disasters is achievable across diverse damage contexts by utilizing\nimage data collection, annotation, and deep learning approaches.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u68c0\u6d4b\u5730\u9707\u540e\u6df7\u51dd\u571f\u7ed3\u6784\u4e2d\u66b4\u9732\u7684\u94a2\u7b4b\uff0c\u901a\u8fc7YOLO\u6a21\u578b\u8bc6\u522b\u5f00\u88c2\u3001\u5265\u843d\u548c\u94a2\u7b4b\u66b4\u9732\u7b49\u635f\u4f24\uff0c\u6784\u5efa\u6df7\u5408\u6846\u67b6\u8bc4\u4f30\u7ed3\u6784\u635f\u4f24\u7b49\u7ea7\u3002", "motivation": "\u5730\u9707\u540e\u53ca\u65f6\u8bc4\u4f30\u7ed3\u6784\u5b8c\u6574\u6027\u5bf9\u516c\u5171\u5b89\u5168\u548c\u5e94\u6025\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u68c0\u6d4b\u6df7\u51dd\u571f\u7ed3\u6784\u4e2d\u66b4\u9732\u7684\u94a2\u7b4b\u6765\u8bc4\u4f30\u635f\u4f24\u7a0b\u5ea6\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528YOLOv11\u6a21\u578b\u68c0\u6d4b\u5f00\u88c2\u3001\u5265\u843d\u548c\u66b4\u9732\u94a2\u7b4b\uff0c\u901a\u8fc7\u5fae\u8c03\u3001\u6570\u636e\u589e\u5f3a\u548c\u516c\u5171\u6570\u636e\u96c6\u6d4b\u8bd5\uff0c\u6784\u5efa\u81ea\u52a8\u5206\u7c7b\u6846\u67b6\u8bc6\u522b\u5efa\u7b51\u5185\u5916\u548c\u7ed3\u6784\u6784\u4ef6\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u81ea\u52a8\u53ef\u9760\u5730\u4ece\u8f93\u5165\u56fe\u50cf\u786e\u5b9a\u635f\u4f24\u7b49\u7ea7\u7684\u6df7\u5408\u6846\u67b6\uff0c\u57282023\u5e74\u571f\u8033\u5176\u5730\u9707\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u56fe\u50cf\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u635f\u4f24\u80cc\u666f\u4e0b\u5b9e\u73b0\u5feb\u901f\u81ea\u52a8\u5316\u7684\u707e\u540e\u635f\u4f24\u68c0\u6d4b\u3002"}}
{"id": "2510.21438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21438", "abs": "https://arxiv.org/abs/2510.21438", "authors": ["Satheeshkumar Veeramani", "Zhengxue Zhou", "Francisco Munguia-Galeano", "Hatem Fakhruldeen", "Thomas Roddelkopf", "Mohammed Faeik Ruzaij Al-Okby", "Kerstin Thurow", "Andrew Ian Cooper"], "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees", "comment": "25 pages, 8 figures, paper submitted to Robotics and Autonomous\n  Systems Journal", "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86PREVENT\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u884c\u4e3a\u6811\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5316\u5b66\u5bb6\u7cfb\u7edf\uff0c\u80fd\u591f\u68c0\u6d4b\u548c\u907f\u514d\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5f02\u5e38\u60c5\u51b5\uff0c\u5b8c\u5168\u907f\u514d\u4e86\u8bef\u62a5\u548c\u6f0f\u62a5\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u673a\u5668\u4eba\u5316\u5b66\u5bb6\u7f3a\u4e4f\u5de5\u4f5c\u6d41\u7a0b\u610f\u8bc6\u80fd\u529b\uff0c\u5c0f\u7684\u5f02\u5e38\uff08\u5982\u6837\u54c1\u74f6\u672a\u6b63\u786e\u76d6\u597d\uff09\u53ef\u80fd\u4e2d\u65ad\u6574\u4e2a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6d6a\u8d39\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u5e76\u5bf9\u7814\u7a76\u4eba\u5458\u6784\u6210\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u611f\u77e5\u673a\u5236\u4f1a\u4ea7\u751f\u8fc7\u591a\u8bef\u62a5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u591a\u6a21\u6001\u884c\u4e3a\u6811\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u5206\u5c42\u611f\u77e5\u673a\u5236\uff0c\u5229\u7528AI\u6280\u672f\u548c\u901a\u8fc7\u7075\u5de7\u89c6\u89c9\u3001\u5bfc\u822a\u89c6\u89c9\u6444\u50cf\u5934\u53ca\u7269\u8054\u7f51\u6c14\u4f53\u4f20\u611f\u5668\u7684\u611f\u5b98\u53cd\u9988\u8fdb\u884c\u6267\u884c\u76f8\u5173\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u76f8\u5bf9\u9ad8\u6548\uff0c\u5728\u6a21\u62df\u98ce\u9669\u573a\u666f\u4e2d\u5b8c\u5168\u907f\u514d\u4e86\u8bef\u62a5\u548c\u6f0f\u62a5\u3002\u591a\u6a21\u6001\u611f\u77e5\u6280\u80fd\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u65b9\u9762\u7684\u90e8\u7f72\u51c6\u786e\u7387\u5747\u9ad8\u4e8e\u76f8\u5e94\u5355\u6a21\u6001\u6280\u80fd\u7684\u5e73\u5747\u6c34\u5e73\u3002", "conclusion": "PREVENT\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u96c6\u6210\u5230\u73b0\u6709\u8f6f\u4ef6\u67b6\u6784\u4e2d\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u7a0b\u5f02\u5e38\u68c0\u6d4b\u548c\u907f\u514d\u80fd\u529b\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5316\u5b66\u5bb6\u7684\u81ea\u4e3b\u64cd\u4f5c\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.20955", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20955", "abs": "https://arxiv.org/abs/2510.20955", "authors": ["Jeff Pflueger", "Michael Everett"], "title": "Safety Assessment in Reinforcement Learning via Model Predictive Control", "comment": "7 pages, 4 figures", "summary": "Model-free reinforcement learning approaches are promising for control but\ntypically lack formal safety guarantees. Existing methods to shield or\notherwise provide these guarantees often rely on detailed knowledge of the\nsafety specifications. Instead, this work's insight is that many\ndifficult-to-specify safety issues are best characterized by invariance.\nAccordingly, we propose to leverage reversibility as a method for preventing\nthese safety issues throughout the training process. Our method uses\nmodel-predictive path integral control to check the safety of an action\nproposed by a learned policy throughout training. A key advantage of this\napproach is that it only requires the ability to query the black-box dynamics,\nnot explicit knowledge of the dynamics or safety constraints. Experimental\nresults demonstrate that the proposed algorithm successfully aborts before all\nunsafe actions, while still achieving comparable training progress to a\nbaseline PPO approach that is allowed to violate safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9006\u6027\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u6765\u68c0\u67e5\u5b66\u4e60\u7b56\u7565\u63d0\u51fa\u7684\u52a8\u4f5c\u5b89\u5168\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u52a8\u6001\u6a21\u578b\u6216\u5b89\u5168\u7ea6\u675f\u77e5\u8bc6\u3002", "motivation": "\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8be6\u7ec6\u7684\u5b89\u5168\u89c4\u8303\u77e5\u8bc6\uff0c\u800c\u8bb8\u591a\u96be\u4ee5\u660e\u786e\u89c4\u8303\u7684\u5b89\u5168\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u4e0d\u53d8\u6027\u6765\u8868\u5f81\u3002", "method": "\u5229\u7528\u53ef\u9006\u6027\u9632\u6b62\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u68c0\u67e5\u5b66\u4e60\u7b56\u7565\u63d0\u51fa\u7684\u52a8\u4f5c\u5b89\u5168\u6027\uff0c\u4ec5\u9700\u67e5\u8be2\u9ed1\u76d2\u52a8\u6001\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u4e0d\u5b89\u5168\u52a8\u4f5c\u53d1\u751f\u524d\u6210\u529f\u4e2d\u6b62\uff0c\u540c\u65f6\u8bad\u7ec3\u8fdb\u5ea6\u4e0e\u5141\u8bb8\u8fdd\u53cd\u5b89\u5168\u6027\u7684\u57fa\u7ebfPPO\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u57fa\u4e8e\u53ef\u9006\u6027\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\uff0c\u4ec5\u9700\u9ed1\u76d2\u52a8\u6001\u67e5\u8be2\u80fd\u529b\uff0c\u65e0\u9700\u663e\u5f0f\u52a8\u6001\u6a21\u578b\u6216\u5b89\u5168\u7ea6\u675f\u77e5\u8bc6\u3002"}}
{"id": "2510.20997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20997", "abs": "https://arxiv.org/abs/2510.20997", "authors": ["James Ghawaly", "Andrew Nicholson", "Catherine Schuman", "Dalton Diez", "Aaron Young", "Brett Witherspoon"], "title": "Exploring Spiking Neural Networks for Binary Classification in Multivariate Time Series at the Edge", "comment": "Accepted in 2025 International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "We present a general framework for training spiking neural networks (SNNs) to\nperform binary classification on multivariate time series, with a focus on\nstep-wise prediction and high precision at low false alarm rates. The approach\nuses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm to\nevolve sparse, stateful SNNs by jointly optimizing their architectures and\nparameters. Inputs are encoded into spike trains, and predictions are made by\nthresholding a single output neuron's spike counts. We also incorporate simple\nvoting ensemble methods to improve performance and robustness.\n  To evaluate the framework, we apply it with application-specific\noptimizations to the task of detecting low signal-to-noise ratio radioactive\nsources in gamma-ray spectral data. The resulting SNNs, with as few as 49\nneurons and 66 synapses, achieve a 51.8% true positive rate (TPR) at a false\nalarm rate of 1/hr, outperforming PCA (42.7%) and deep learning (49.8%)\nbaselines. A three-model any-vote ensemble increases TPR to 67.1% at the same\nfalse alarm rate. Hardware deployment on the microCaspian neuromorphic platform\ndemonstrates 2mW power consumption and 20.2ms inference latency.\n  We also demonstrate generalizability by applying the same framework, without\ndomain-specific modification, to seizure detection in EEG recordings. An\nensemble achieves 95% TPR with a 16% false positive rate, comparable to recent\ndeep learning approaches with significant reduction in parameter count.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u8fdb\u884c\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e8c\u5206\u7c7b\u7684\u901a\u7528\u6846\u67b6\uff0c\u7279\u522b\u5173\u6ce8\u9010\u6b65\u9884\u6d4b\u548c\u4f4e\u8bef\u62a5\u7387\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u3002\u901a\u8fc7EONS\u7b97\u6cd5\u8054\u5408\u4f18\u5316SNN\u67b6\u6784\u548c\u53c2\u6570\uff0c\u5728\u653e\u5c04\u6027\u6e90\u68c0\u6d4b\u548c\u766b\u75eb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u901a\u7528\u7684SNN\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u4e8c\u8fdb\u5236\u5206\u7c7b\uff0c\u7279\u522b\u5f3a\u8c03\u5728\u4f4e\u8bef\u62a5\u7387\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528EONS\u7b97\u6cd5\u8fdb\u5316\u7a00\u758f\u3001\u6709\u72b6\u6001\u7684SNN\uff0c\u8054\u5408\u4f18\u5316\u67b6\u6784\u548c\u53c2\u6570\uff1b\u5c06\u8f93\u5165\u7f16\u7801\u4e3a\u8109\u51b2\u5e8f\u5217\uff0c\u901a\u8fc7\u9608\u503c\u5316\u5355\u4e2a\u8f93\u51fa\u795e\u7ecf\u5143\u7684\u8109\u51b2\u8ba1\u6570\u8fdb\u884c\u9884\u6d4b\uff1b\u91c7\u7528\u7b80\u5355\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u63d0\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u653e\u5c04\u6027\u6e90\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4ec5\u752849\u4e2a\u795e\u7ecf\u5143\u548c66\u4e2a\u7a81\u89e6\u7684SNN\u57281/\u5c0f\u65f6\u8bef\u62a5\u7387\u4e0b\u8fbe\u523051.8%\u771f\u9633\u6027\u7387\uff0c\u4f18\u4e8ePCA(42.7%)\u548c\u6df1\u5ea6\u5b66\u4e60(49.8%)\u57fa\u7ebf\uff1b\u4e09\u6a21\u578b\u96c6\u6210\u5c06\u771f\u9633\u6027\u7387\u63d0\u5347\u81f367.1%\uff1b\u786c\u4ef6\u90e8\u7f72\u663e\u793a2mW\u529f\u8017\u548c20.2ms\u63a8\u7406\u5ef6\u8fdf\u3002\u5728\u766b\u75eb\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u96c6\u6210\u65b9\u6cd5\u8fbe\u523095%\u771f\u9633\u6027\u7387\u548c16%\u5047\u9633\u6027\u7387\uff0c\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u5f53\u4f46\u53c2\u6570\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u8bad\u7ec3\u51fa\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684SNN\uff0c\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\uff0c\u5c55\u793a\u4e86SNN\u5728\u4f4e\u529f\u8017\u5b9e\u65f6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.21017", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21017", "abs": "https://arxiv.org/abs/2510.21017", "authors": ["Yuhong Luo", "Austin Hoag", "Xintong Wang", "Philip S. Thomas", "Przemyslaw A. Grabowicz"], "title": "Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference", "comment": "Accepted by NeurIPS 2025", "summary": "Representation learning is increasingly applied to generate representations\nthat generalize well across multiple downstream tasks. Ensuring fairness\nguarantees in representation learning is crucial to prevent unfairness toward\nspecific demographic groups in downstream tasks. In this work, we formally\nintroduce the task of learning representations that achieve high-confidence\nfairness. We aim to guarantee that demographic disparity in every downstream\nprediction remains bounded by a *user-defined* error threshold $\\epsilon$, with\n*controllable* high probability. To this end, we propose the ***F**air\n**R**epresentation learning with high-confidence **G**uarantees (FRG)*\nframework, which provides these high-confidence fairness guarantees by\nleveraging an optimized adversarial model. We empirically evaluate FRG on three\nreal-world datasets, comparing its performance to six state-of-the-art fair\nrepresentation learning methods. Our results demonstrate that FRG consistently\nbounds unfairness across a range of downstream models and tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86FRG\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6a21\u578b\u5b66\u4e60\u5177\u6709\u9ad8\u7f6e\u4fe1\u5ea6\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u8868\u793a\uff0c\u786e\u4fdd\u4e0b\u6e38\u9884\u6d4b\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\u4e0d\u8d85\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u8bef\u5dee\u9608\u503c\u3002", "motivation": "\u8868\u793a\u5b66\u4e60\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u516c\u5e73\u6027\uff0c\u9632\u6b62\u5bf9\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u516c\u5e73\u6027\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faFRG\u6846\u67b6\uff0c\u5229\u7528\u4f18\u5316\u7684\u5bf9\u6297\u6a21\u578b\u6765\u63d0\u4f9b\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u516c\u5e73\u6027\u4fdd\u8bc1\uff0c\u786e\u4fdd\u4e0b\u6e38\u9884\u6d4b\u7684\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\u6709\u754c\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFRG\u5728\u591a\u79cd\u4e0b\u6e38\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u80fd\u7ea6\u675f\u4e0d\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u516d\u79cd\u6700\u5148\u8fdb\u7684\u516c\u5e73\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FRG\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5177\u6709\u9ad8\u7f6e\u4fe1\u5ea6\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u8868\u793a\uff0c\u4e3a\u516c\u5e73\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2510.21329", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21329", "abs": "https://arxiv.org/abs/2510.21329", "authors": ["Priyanshu Karmakar", "Soumyabrata Chaudhuri", "Shubhojit Mallick", "Manish Gupta", "Abhik Jana", "Shreya Ghosh"], "title": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions", "comment": "12 pages, 12 tables and 7 figures", "summary": "Recent efforts like TripCraft and TravelPlanner have advanced the use of\nLarge Language Models ( LLMs) for personalized, constraint aware travel\nitinerary generation. Yet, real travel often faces disruptions. To address\nthis, we present TripTide, the first benchmark evaluating LLM's ability to\nrevise itineraries under realistic disruptions. TripTide models key dimensions\nsuch as disruption severity and traveler tolerance, enabling nuanced assessment\nof LLM adaptability to events like flight cancellations, weather closures, or\noverbooked attractions. We conduct a threefold evaluation. First, we introduce\nautomatic metrics including Preservation of Intent (how well the revised plan\nmaintains feasibility and goals), Responsiveness (promptness and\nappropriateness of disruption handling), and Adaptability (semantic, spatial,\nand sequential divergence between original and revised plans). Second, we apply\nan LLM-as-a-judge approach to automatically assess revision quality. Third, we\nperform manual expert evaluation to verify whether revisions preserve semantic,\nspatial, sequential, and responsive aspects. Our experiments show that LLMs\nmaintain strong sequential consistency and semantic stability, while spatial\ndeviations are larger for shorter trips but decrease with longer ones,\nindicating that extended plans encourage better geographic coherence. However,\ndisruption-handling ability declines as plan length increases, highlighting\nlimits in LLM robustness. TripTide establishes a benchmark for evaluating\nadaptability, personalization, and resilience in LLM-based travel planning\nunder real-world uncertainty.", "AI": {"tldr": "TripTide\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u65c5\u884c\u4e2d\u65ad\u60c5\u51b5\u4e0b\u4fee\u8ba2\u884c\u7a0b\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5efa\u6a21\u4e2d\u65ad\u4e25\u91cd\u6027\u548c\u65c5\u884c\u8005\u5bb9\u5fcd\u5ea6\u7b49\u7ef4\u5ea6\uff0c\u7efc\u5408\u8bc4\u4f30LLM\u5bf9\u822a\u73ed\u53d6\u6d88\u3001\u5929\u6c14\u5173\u95ed\u7b49\u4e8b\u4ef6\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u65c5\u884c\u89c4\u5212\u7cfb\u7edf\u5982TripCraft\u548cTravelPlanner\u867d\u7136\u80fd\u751f\u6210\u4e2a\u6027\u5316\u884c\u7a0b\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u73b0\u5b9e\u65c5\u884c\u4e2d\u7684\u5404\u79cd\u4e2d\u65ad\u60c5\u51b5\uff0c\u9700\u8981\u8bc4\u4f30LLM\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e09\u91cd\u8bc4\u4f30\u65b9\u6cd5\uff1a1) \u5f15\u5165\u81ea\u52a8\u6307\u6807\uff08\u610f\u56fe\u4fdd\u6301\u3001\u54cd\u5e94\u6027\u3001\u9002\u5e94\u6027\uff09\uff1b2) \u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u81ea\u52a8\u8bc4\u4f30\u4fee\u8ba2\u8d28\u91cf\uff1b3) \u8fdb\u884c\u4eba\u5de5\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u8bed\u4e49\u3001\u7a7a\u95f4\u3001\u987a\u5e8f\u548c\u54cd\u5e94\u65b9\u9762\u7684\u4fdd\u6301\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u5728\u987a\u5e8f\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7a7a\u95f4\u504f\u5dee\u5728\u77ed\u9014\u65c5\u884c\u4e2d\u8f83\u5927\u4f46\u968f\u884c\u7a0b\u5ef6\u957f\u800c\u51cf\u5c0f\u3002\u7136\u800c\uff0c\u968f\u7740\u8ba1\u5212\u957f\u5ea6\u589e\u52a0\uff0c\u4e2d\u65ad\u5904\u7406\u80fd\u529b\u4e0b\u964d\uff0c\u8868\u660eLLM\u9c81\u68d2\u6027\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "TripTide\u4e3a\u8bc4\u4f30LLM\u5728\u73b0\u5b9e\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u65c5\u884c\u89c4\u5212\u9002\u5e94\u6027\u3001\u4e2a\u6027\u5316\u548c\u97e7\u6027\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u957f\u884c\u7a0b\u4e2d\u65ad\u5904\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.21360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21360", "abs": "https://arxiv.org/abs/2510.21360", "authors": ["Jenny Kunz"], "title": "A Diagnostic Benchmark for Sweden-Related Factual Knowledge", "comment": null, "summary": "Many Swedish benchmarks are translated US-centric benchmarks, and therefore\nnot suitable for testing knowledge that is particularly relevant, or even\nspecific, to Sweden. We therefore introduce a manually written\nquestion-answering benchmark specifically targeted to Sweden-related\npersonalities and events, many of which receive very limited coverage in\ninternational media. Our annotators drew inspiration from a popular radio\nprogram featuring public figures from culture and media, as well as major\nsports events in Sweden. The dataset can be used to measure factual recall\nacross models of varying sizes and degrees of Swedish coverage, and allows to\nprobe cross-lingual factual consistency as to contains English translations.\nUsing the dataset, we find that smaller models with stronger Swedish coverage\nperform comparably to a three times larger multilingual model in recalling\nSweden-related facts. We also observe that continued pre-training on Swedish\ngenerally improves factual knowledge but also leads to forgetting of a part of\nthe previously known information. These results demonstrate the dataset's\npotential as a diagnostic tool for studying language adaptation and knowledge\nretention in multilingual models and during language adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u745e\u5178\u76f8\u5173\u4eba\u7269\u548c\u4e8b\u4ef6\u7684\u624b\u52a8\u7f16\u5199\u95ee\u7b54\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u5728\u745e\u5178\u7279\u5b9a\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8bb8\u591a\u745e\u5178\u57fa\u51c6\u6d4b\u8bd5\u662f\u7ffb\u8bd1\u81ea\u7f8e\u56fd\u4e2d\u5fc3\u7684\u57fa\u51c6\uff0c\u4e0d\u9002\u5408\u6d4b\u8bd5\u745e\u5178\u7279\u6709\u7684\u76f8\u5173\u77e5\u8bc6\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u745e\u5178\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u672c\u5730\u77e5\u8bc6\u7684\u638c\u63e1\u7a0b\u5ea6\u3002", "method": "\u624b\u52a8\u7f16\u5199\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7075\u611f\u6765\u81ea\u745e\u5178\u6587\u5316\u5a92\u4f53\u540d\u4eba\u53c2\u4e0e\u7684\u6d41\u884c\u5e7f\u64ad\u8282\u76ee\u548c\u4e3b\u8981\u4f53\u80b2\u8d5b\u4e8b\uff0c\u5305\u542b\u82f1\u8bed\u7ffb\u8bd1\u4ee5\u6d4b\u8bd5\u8de8\u8bed\u8a00\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "result": "\u53d1\u73b0\u5177\u6709\u66f4\u5f3a\u745e\u5178\u8986\u76d6\u8303\u56f4\u7684\u5c0f\u578b\u6a21\u578b\u5728\u56de\u5fc6\u745e\u5178\u76f8\u5173\u4e8b\u5b9e\u65b9\u9762\u4e0e\u4e09\u500d\u5927\u7684\u591a\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002\u7ee7\u7eed\u5728\u745e\u5178\u8bed\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u901a\u5e38\u80fd\u63d0\u9ad8\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u4f46\u4e5f\u4f1a\u5bfc\u81f4\u90e8\u5206\u5148\u524d\u5df2\u77e5\u4fe1\u606f\u7684\u9057\u5fd8\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53ef\u4f5c\u4e3a\u7814\u7a76\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u8a00\u9002\u5e94\u548c\u77e5\u8bc6\u4fdd\u7559\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bc4\u4f30\u8bed\u8a00\u7279\u5b9a\u77e5\u8bc6\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.21524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21524", "abs": "https://arxiv.org/abs/2510.21524", "authors": ["Ilija Lichkovski", "Alexander M\u00fcller", "Mariam Ibrahim", "Tiwai Mhundwa"], "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.", "AI": {"tldr": "EU-Agent-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u6b27\u76df\u6cd5\u5f8b\u6846\u67b6\u4e0b\u5408\u89c4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6a21\u62df\u53ef\u80fd\u5f15\u53d1\u975e\u6cd5\u884c\u4e3a\u7684\u826f\u6027\u7528\u6237\u8bf7\u6c42\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u6cd5\u5f8b\u9075\u4ece\u6027\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u5b83\u4eec\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\uff0c\u5305\u62ec\u91c7\u53d6\u4e0d\u826f\u548c/\u6216\u4e0d\u5b89\u5168\u7684\u884c\u52a8\u3002\u9700\u8981\u6d4b\u91cfLLM\u4ee3\u7406\u5728\u6b27\u76df\u7acb\u6cd5\u80cc\u666f\u4e0b\u91c7\u53d6\u975e\u6cd5\u884c\u4e3a\u7684\u6f5c\u5728\u503e\u5411\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u4eba\u5de5\u7b56\u5212\u57fa\u51c6\uff0c\u6db5\u76d6\u6570\u636e\u4fdd\u62a4\u3001\u504f\u89c1/\u6b67\u89c6\u548c\u79d1\u5b66\u8bda\u4fe1\u7b49\u591a\u4e2a\u7c7b\u522b\u3002\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u51fd\u6570\u8c03\u7528\u4e0e\u8be6\u5c3d\u5f15\u7528\u76f8\u5173\u7acb\u6cd5\u7684\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc4\u4f30\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u5e76\u7814\u7a76\u5728\u7cfb\u7edf\u63d0\u793a\u4e2d\u63d0\u4f9b\u76f8\u5173\u7acb\u6cd5\u6458\u5f55\u5bf9\u5408\u89c4\u6027\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u4e86\u524d\u6cbfLLM\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u63d0\u4f9b\u7acb\u6cd5\u6458\u5f55\u5bf9\u5408\u89c4\u6027\u7684\u5f71\u54cd\u3002\u53d1\u5e03\u4e86\u516c\u5f00\u9884\u89c8\u96c6\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u7559\u79c1\u6709\u6d4b\u8bd5\u96c6\u4ee5\u9632\u6b62\u6570\u636e\u6c61\u67d3\u3002", "conclusion": "\u9f13\u52b1\u672a\u6765\u5de5\u4f5c\u5c06\u4ee3\u7406\u5b89\u5168\u57fa\u51c6\u6269\u5c55\u5230\u4e0d\u540c\u7684\u6cd5\u5f8b\u7ba1\u8f96\u533a\uff0c\u4ee5\u53ca\u591a\u8f6e\u548c\u591a\u8bed\u8a00\u4ea4\u4e92\u3002\u53d1\u5e03\u4e86\u4ee3\u7801\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2510.21356", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21356", "abs": "https://arxiv.org/abs/2510.21356", "authors": ["Anupam Pani", "Yanchao Yang"], "title": "Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding", "comment": null, "summary": "Eye gaze offers valuable cues about attention, short-term intent, and future\nactions, making it a powerful signal for modeling egocentric behavior. In this\nwork, we propose a gaze-regularized framework that enhances VLMs for two key\negocentric understanding tasks: fine-grained future event prediction and\ncurrent activity understanding. Unlike prior approaches that rely solely on\nvisual inputs or use gaze as an auxiliary input signal , our method uses gaze\nonly during training. We introduce a gaze-regularized attention mechanism that\naligns model focus with human visual gaze. This design is flexible and modular,\nallowing it to generalize across multiple VLM architectures that utilize\nattention. Experimental results show that our approach improves semantic\nprediction scores by up to 11 for future event prediction and around 7 for\ncurrent activity understanding, compared to the corresponding baseline models\ntrained without gaze regularization. These results highlight the value of\ngaze-guided training in improving the accuracy and robustness of egocentric\nVLMs. Overall, this work establishes a foundation for using human gaze to\nenhance the predictive capabilities of VLMs in real-world scenarios like\nassistive robots and human-machine collaboration. Code and additional\ninformation is available at: https://github.com/anupampani/Gaze-VLM", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ce8\u89c6\u6b63\u5219\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u6ce8\u610f\u529b\u4e0e\u4eba\u7c7b\u89c6\u89c9\u6ce8\u89c6\u5bf9\u9f50\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ec5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u6ce8\u89c6\u6570\u636e\u3002", "motivation": "\u4eba\u7c7b\u6ce8\u89c6\u63d0\u4f9b\u4e86\u5173\u4e8e\u6ce8\u610f\u529b\u3001\u77ed\u671f\u610f\u56fe\u548c\u672a\u6765\u884c\u52a8\u7684\u6709\u4ef7\u503c\u7ebf\u7d22\uff0c\u662f\u5efa\u6a21\u81ea\u6211\u4e2d\u5fc3\u884c\u4e3a\u7684\u5f3a\u5927\u4fe1\u53f7\u3002", "method": "\u5f15\u5165\u6ce8\u89c6\u6b63\u5219\u5316\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u5c06\u6a21\u578b\u7126\u70b9\u4e0e\u4eba\u7c7b\u89c6\u89c9\u6ce8\u89c6\u5bf9\u9f50\uff0c\u8be5\u8bbe\u8ba1\u7075\u6d3b\u6a21\u5757\u5316\uff0c\u53ef\u6cdb\u5316\u5230\u591a\u79cd\u4f7f\u7528\u6ce8\u610f\u529b\u7684VLM\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u65e0\u6ce8\u89c6\u6b63\u5219\u5316\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8bed\u4e49\u9884\u6d4b\u5206\u6570\u63d0\u5347\u9ad8\u8fbe11\u5206\uff0c\u5728\u5f53\u524d\u6d3b\u52a8\u7406\u89e3\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea67\u5206\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f7f\u7528\u4eba\u7c7b\u6ce8\u89c6\u589e\u5f3aVLM\u5728\u73b0\u5b9e\u573a\u666f\uff08\u5982\u8f85\u52a9\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\uff09\u4e2d\u7684\u9884\u6d4b\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.21406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21406", "abs": "https://arxiv.org/abs/2510.21406", "authors": ["Yue Feng", "Jinwei Hu", "Qijia Lu", "Jiawei Niu", "Li Tan", "Shuo Yuan", "Ziyi Yan", "Yizhen Jia", "Qingzhi He", "Shiping Ge", "Ethan Q. Chen", "Wentong Li", "Limin Wang", "Jie Qin"], "title": "MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence", "comment": "Accepted to NeurIPS 2025 D&B Track", "summary": "We propose the Multi-modal Untrimmed Video Retrieval task, along with a new\nbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims\nto retrieve untrimmed videos containing relevant segments using multi-modal\nqueries. It has the following features: 1) Practical retrieval paradigm: MUVR\nsupports video-centric multi-modal queries, expressing fine-grained retrieval\nneeds through long text descriptions, video tag prompts, and mask prompts. It\nadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,\ntailored for long-video platform applications. 2) Multi-level visual\ncorrespondence: To cover common video categories (e.g., news, travel, dance)\nand precisely define retrieval matching criteria, we construct multi-level\nvisual correspondence based on core video content (e.g., news events, travel\nlocations, dance moves) which users are interested in and want to retrieve. It\ncovers six levels: copy, event, scene, instance, action, and others. 3)\nComprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,\nFilter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA\nassesses MLLMs in a question-answering format. We also propose a Reranking\nScore to evaluate the reranking ability of MLLMs. MUVR consists of 53K\nuntrimmed videos from the video platform Bilibili, with 1,050 multi-modal\nqueries and 84K matches. Extensive evaluations of 3 state-of-the-art video\nretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals\nthe limitations of retrieval methods in processing untrimmed videos and\nmulti-modal queries, as well as MLLMs in multi-video understanding and\nreranking. Our code and benchmark is available at\nhttps://github.com/debby-0527/MUVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u672a\u4fee\u526a\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u548cMUVR\u57fa\u51c6\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u5e73\u53f0\u7684\u89c6\u9891\u68c0\u7d22\uff0c\u652f\u6301\u591a\u6a21\u6001\u67e5\u8be2\u5e76\u8bc4\u4f30\u68c0\u7d22\u6a21\u578b\u548cMLLMs\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u77ed\u89c6\u9891\u548c\u5355\u6a21\u6001\u67e5\u8be2\uff0c\u65e0\u6cd5\u6ee1\u8db3\u957f\u89c6\u9891\u5e73\u53f0\u5bf9\u672a\u4fee\u526a\u89c6\u9891\u548c\u591a\u6a21\u6001\u67e5\u8be2\u7684\u68c0\u7d22\u9700\u6c42\u3002", "method": "\u6784\u5efaMUVR\u57fa\u51c6\uff0c\u5305\u542b53K\u672a\u4fee\u526a\u89c6\u9891\u30011,050\u4e2a\u591a\u6a21\u6001\u67e5\u8be2\u548c84K\u5339\u914d\u9879\uff0c\u652f\u6301\u89c6\u9891\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u67e5\u8be2\uff08\u957f\u6587\u672c\u3001\u89c6\u9891\u6807\u7b7e\u3001\u63a9\u7801\u63d0\u793a\uff09\u548c\u591a\u7ea7\u89c6\u89c9\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u8bc4\u4f30\u4e863\u4e2aSOTA\u89c6\u9891\u68c0\u7d22\u6a21\u578b\u30016\u4e2a\u57fa\u4e8e\u56fe\u50cf\u7684VLM\u548c10\u4e2aMLLM\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u672a\u4fee\u526a\u89c6\u9891\u548c\u591a\u6a21\u6001\u67e5\u8be2\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MUVR\u57fa\u51c6\u4e3a\u957f\u89c6\u9891\u5e73\u53f0\u7684\u591a\u6a21\u6001\u89c6\u9891\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\u548c\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.21704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21704", "abs": "https://arxiv.org/abs/2510.21704", "authors": ["Christy Li", "Josep Lopez Camu\u00f1as", "Jake Thomas Touchet", "Jacob Andreas", "Agata Lapedriza", "Antonio Torralba", "Tamar Rott Shaham"], "title": "Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent", "comment": "32 pages, 10 figures, Neurips 2025", "summary": "When a vision model performs image recognition, which visual attributes drive\nits predictions? Detecting unintended reliance on specific visual features is\ncritical for ensuring model robustness, preventing overfitting, and avoiding\nspurious correlations. We introduce an automated framework for detecting such\ndependencies in trained vision models. At the core of our method is a\nself-reflective agent that systematically generates and tests hypotheses about\nvisual attributes that a model may rely on. This process is iterative: the\nagent refines its hypotheses based on experimental outcomes and uses a\nself-evaluation protocol to assess whether its findings accurately explain\nmodel behavior. When inconsistencies arise, the agent self-reflects over its\nfindings and triggers a new cycle of experimentation. We evaluate our approach\non a novel benchmark of 130 models designed to exhibit diverse visual attribute\ndependencies across 18 categories. Our results show that the agent's\nperformance consistently improves with self-reflection, with a significant\nperformance increase over non-reflective baselines. We further demonstrate that\nthe agent identifies real-world visual attribute dependencies in\nstate-of-the-art models, including CLIP's vision encoder and the YOLOv8 object\ndetector.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u53cd\u601d\u4ee3\u7406\u7cfb\u7edf\u6027\u5730\u751f\u6210\u548c\u6d4b\u8bd5\u89c6\u89c9\u5c5e\u6027\u4f9d\u8d56\u5047\u8bbe\uff0c\u4ee5\u68c0\u6d4b\u8bad\u7ec3\u597d\u7684\u89c6\u89c9\u6a21\u578b\u5bf9\u7279\u5b9a\u89c6\u89c9\u7279\u5f81\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u68c0\u6d4b\u89c6\u89c9\u6a21\u578b\u5bf9\u7279\u5b9a\u89c6\u89c9\u7279\u5f81\u7684\u610f\u5916\u4f9d\u8d56\u5bf9\u4e8e\u786e\u4fdd\u6a21\u578b\u9c81\u68d2\u6027\u3001\u9632\u6b62\u8fc7\u62df\u5408\u548c\u907f\u514d\u865a\u5047\u76f8\u5173\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u81ea\u53cd\u601d\u4ee3\u7406\u8fed\u4ee3\u751f\u6210\u548c\u6d4b\u8bd5\u5173\u4e8e\u6a21\u578b\u53ef\u80fd\u4f9d\u8d56\u7684\u89c6\u89c9\u5c5e\u6027\u7684\u5047\u8bbe\uff0c\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\u7cbe\u70bc\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u81ea\u8bc4\u4f30\u534f\u8bae\u8bc4\u4f30\u53d1\u73b0\u662f\u5426\u51c6\u786e\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5728\u5305\u542b130\u4e2a\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee3\u7406\u6027\u80fd\u968f\u81ea\u53cd\u601d\u6301\u7eed\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u53cd\u601d\u57fa\u7ebf\uff0c\u5e76\u80fd\u8bc6\u522bCLIP\u89c6\u89c9\u7f16\u7801\u5668\u548cYOLOv8\u7b49\u5148\u8fdb\u6a21\u578b\u4e2d\u7684\u771f\u5b9e\u89c6\u89c9\u5c5e\u6027\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u81ea\u53cd\u601d\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u89c6\u89c9\u6a21\u578b\u5bf9\u7279\u5b9a\u89c6\u89c9\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u4e3a\u6a21\u578b\u5206\u6790\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.21389", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21389", "abs": "https://arxiv.org/abs/2510.21389", "authors": ["Stefan Kraft", "Andreas Theissler", "Vera Wienhausen-Wilke", "Gjergji Kasneci", "Hendrik Lensch"], "title": "Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study", "comment": null, "summary": "Artificial intelligence (AI) systems increasingly match or surpass human\nexperts in biomedical signal interpretation. However, their effective\nintegration into clinical practice requires more than high predictive accuracy.\nClinicians must discern \\textit{when} and \\textit{why} to trust algorithmic\nrecommendations. This work presents an application-grounded user study with\neight professional sleep medicine practitioners, who score nocturnal arousal\nevents in polysomnographic data under three conditions: (i) manual scoring,\n(ii) black-box (BB) AI assistance, and (iii) transparent white-box (WB) AI\nassistance. Assistance is provided either from the \\textit{start} of scoring or\nas a post-hoc quality-control (\\textit{QC}) review. We systematically evaluate\nhow the type and timing of assistance influence event-level and clinically most\nrelevant count-based performance, time requirements, and user experience. When\nevaluated against the clinical standard used to train the AI, both AI and\nhuman-AI teams significantly outperform unaided experts, with collaboration\nalso reducing inter-rater variability. Notably, transparent AI assistance\napplied as a targeted QC step yields median event-level performance\nimprovements of approximately 30\\% over black-box assistance, and QC timing\nfurther enhances count-based outcomes. While WB and QC approaches increase the\ntime required for scoring, start-time assistance is faster and preferred by\nmost participants. Participants overwhelmingly favor transparency, with seven\nout of eight expressing willingness to adopt the system with minor or no\nmodifications. In summary, strategically timed transparent AI assistance\neffectively balances accuracy and clinical efficiency, providing a promising\npathway toward trustworthy AI integration and user acceptance in clinical\nworkflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u7761\u7720\u533b\u5b66\u4e2d\uff0c\u900f\u660eAI\u8f85\u52a9\u4f5c\u4e3a\u8d28\u91cf\u63a7\u5236\u6b65\u9aa4\u53ef\u663e\u8457\u63d0\u5347\u4e13\u5bb6\u8bc4\u5206\u6027\u80fd30%\uff0c\u6bd4\u9ed1\u76d2AI\u66f4\u6709\u6548\uff0c\u4e14\u8d28\u91cf\u63a7\u5236\u65f6\u673a\u80fd\u8fdb\u4e00\u6b65\u6539\u5584\u8ba1\u6570\u7ed3\u679c\u3002", "motivation": "AI\u7cfb\u7edf\u5728\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u89e3\u91ca\u65b9\u9762\u5df2\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5339\u654c\uff0c\u4f46\u4e34\u5e8a\u5b9e\u8df5\u9700\u8981\u533b\u751f\u80fd\u5224\u65ad\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u4fe1\u4efb\u7b97\u6cd5\u5efa\u8bae\uff0c\u56e0\u6b64\u7814\u7a76AI\u8f85\u52a9\u7684\u7c7b\u578b\u548c\u65f6\u673a\u5bf9\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf98\u540d\u4e13\u4e1a\u7761\u7720\u533b\u5b66\u4ece\u4e1a\u8005\u8fdb\u884c\u5e94\u7528\u5bfc\u5411\u7684\u7528\u6237\u7814\u7a76\uff0c\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u8bc4\u5206\u591a\u5bfc\u7761\u7720\u56fe\u6570\u636e\uff1a\u624b\u52a8\u8bc4\u5206\u3001\u9ed1\u76d2AI\u8f85\u52a9\u548c\u900f\u660e\u767d\u76d2AI\u8f85\u52a9\uff0c\u8f85\u52a9\u65f6\u673a\u5206\u4e3a\u8d77\u59cb\u8f85\u52a9\u548c\u8d28\u91cf\u63a7\u5236\u540e\u8f85\u52a9\u3002", "result": "AI\u548c\u4eba\u7c7b-AI\u56e2\u961f\u5728\u4e34\u5e8a\u6807\u51c6\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u672a\u8f85\u52a9\u4e13\u5bb6\uff0c\u534f\u4f5c\u8fd8\u51cf\u5c11\u4e86\u8bc4\u5206\u8005\u95f4\u53d8\u5f02\u6027\u3002\u900f\u660eAI\u8f85\u52a9\u4f5c\u4e3a\u8d28\u91cf\u63a7\u5236\u6b65\u9aa4\u6bd4\u9ed1\u76d2\u8f85\u52a9\u63d0\u5347\u7ea630%\u7684\u4e8b\u4ef6\u7ea7\u6027\u80fd\uff0c\u8d28\u91cf\u63a7\u5236\u65f6\u673a\u8fdb\u4e00\u6b65\u6539\u5584\u8ba1\u6570\u7ed3\u679c\u3002", "conclusion": "\u7b56\u7565\u6027\u5b9a\u65f6\u7684\u900f\u660eAI\u8f85\u52a9\u80fd\u6709\u6548\u5e73\u8861\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u6548\u7387\uff0c\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u53ef\u4fe1AI\u96c6\u6210\u548c\u7528\u6237\u63a5\u53d7\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
