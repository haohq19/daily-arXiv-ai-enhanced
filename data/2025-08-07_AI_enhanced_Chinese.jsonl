{"id": "2508.03712", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03712", "abs": "https://arxiv.org/abs/2508.03712", "authors": ["Agrima Seth", "Monojit Choudhary", "Sunayana Sitaram", "Kentaro Toyama", "Aditya Vashistha", "Kalika Bali"], "title": "How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion", "comment": "Accepted to AIES 2025", "summary": "Representational bias in large language models (LLMs) has predominantly been\nmeasured through single-response interactions and has focused on Global\nNorth-centric identities like race and gender. We expand on that research by\nconducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded\nrepresentational biases are and how they extend to less-explored dimensions of\nidentity. We prompt GPT-4 Turbo to generate over 7,200 stories about\nsignificant life events (such as weddings) in India, using prompts designed to\nencourage diversity to varying extents. Comparing the diversity of religious\nand caste representation in the outputs against the actual population\ndistribution in India as recorded in census data, we quantify the presence and\n\"stickiness\" of representational bias in the LLM for religion and caste. We\nfind that GPT-4 responses consistently overrepresent culturally dominant groups\nfar beyond their statistical representation, despite prompts intended to\nencourage representational diversity. Our findings also suggest that\nrepresentational bias in LLMs has a winner-take-all quality that is more biased\nthan the likely distribution bias in their training data, and repeated\nprompt-based nudges have limited and inconsistent efficacy in dislodging these\nbiases. These results suggest that diversifying training data alone may not be\nsufficient to correct LLM bias, highlighting the need for more fundamental\nchanges in model development. Dataset and Codebook:\nhttps://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4 Turbo\u5728\u751f\u6210\u5370\u5ea6\u751f\u6d3b\u4e8b\u4ef6\u6545\u4e8b\u65f6\uff0c\u5b97\u6559\u548c\u79cd\u59d3\u7684\u591a\u6837\u6027\u8868\u73b0\u4e0e\u4eba\u53e3\u666e\u67e5\u6570\u636e\u4e0d\u7b26\uff0c\u8fc7\u5ea6\u4ee3\u8868\u6587\u5316\u4e3b\u5bfc\u7fa4\u4f53\uff0c\u4e14\u63d0\u793a\u591a\u6837\u6027\u6548\u679c\u6709\u9650\u3002", "motivation": "\u6269\u5c55\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8868\u5f81\u504f\u89c1\u7684\u7814\u7a76\uff0c\u63a2\u7d22\u5176\u5728\u5b97\u6559\u548c\u79cd\u59d3\u7b49\u8f83\u5c11\u88ab\u5173\u6ce8\u7684\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u591a\u6837\u5316\u63d0\u793a\uff0c\u751f\u62107,200\u4e2a\u5173\u4e8e\u5370\u5ea6\u91cd\u8981\u751f\u6d3b\u4e8b\u4ef6\u7684\u6545\u4e8b\uff0c\u5e76\u4e0e\u5370\u5ea6\u4eba\u53e3\u666e\u67e5\u6570\u636e\u5bf9\u6bd4\uff0c\u91cf\u5316\u504f\u89c1\u3002", "result": "GPT-4 Turbo\u7684\u8f93\u51fa\u8fc7\u5ea6\u4ee3\u8868\u6587\u5316\u4e3b\u5bfc\u7fa4\u4f53\uff0c\u4e14\u63d0\u793a\u591a\u6837\u6027\u5bf9\u7ea0\u6b63\u504f\u89c1\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u4ec5\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u4e0d\u8db3\u4ee5\u7ea0\u6b63LLM\u504f\u89c1\uff0c\u9700\u66f4\u6839\u672c\u7684\u6a21\u578b\u5f00\u53d1\u53d8\u9769\u3002"}}
{"id": "2508.03750", "categories": ["cs.LG", "cs.CE", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03750", "abs": "https://arxiv.org/abs/2508.03750", "authors": ["Cheng Huang", "Weizheng Xie", "Karanjit Kooner", "Tsengdar Lee", "Jui-Kai Wang", "Jia Zhang"], "title": "GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification", "comment": null, "summary": "Early and accurate detection of glaucoma is critical to prevent irreversible\nvision loss. However, existing methods often rely on unimodal data and lack\ninterpretability, limiting their clinical utility. In this paper, we present\nGlaBoost, a multimodal gradient boosting framework that integrates structured\nclinical features, fundus image embeddings, and expert-curated textual\ndescriptions for glaucoma risk prediction. GlaBoost extracts high-level visual\nrepresentations from retinal fundus photographs using a pretrained\nconvolutional encoder and encodes free-text neuroretinal rim assessments using\na transformer-based language model. These heterogeneous signals, combined with\nmanually assessed risk scores and quantitative ophthalmic indicators, are fused\ninto a unified feature space for classification via an enhanced XGBoost model.\nExperiments conducted on a real-world annotated dataset demonstrate that\nGlaBoost significantly outperforms baseline models, achieving a validation\naccuracy of 98.71%. Feature importance analysis reveals clinically consistent\npatterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings\ncontributing most to model decisions. GlaBoost offers a transparent and\nscalable solution for interpretable glaucoma diagnosis and can be extended to\nother ophthalmic disorders.", "AI": {"tldr": "GlaBoost\u662f\u4e00\u79cd\u591a\u6a21\u6001\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u7ed3\u5408\u4e34\u5e8a\u7279\u5f81\u3001\u773c\u5e95\u56fe\u50cf\u548c\u4e13\u5bb6\u6587\u672c\u63cf\u8ff0\uff0c\u7528\u4e8e\u9752\u5149\u773c\u98ce\u9669\u9884\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe98.71%\u3002", "motivation": "\u73b0\u6709\u9752\u5149\u773c\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "method": "GlaBoost\u901a\u8fc7\u9884\u8bad\u7ec3\u5377\u79ef\u7f16\u7801\u5668\u548c\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u7ed3\u5408\u624b\u52a8\u8bc4\u4f30\u7684\u98ce\u9669\u5206\u6570\u548c\u773c\u79d1\u6307\u6807\uff0c\u901a\u8fc7\u589e\u5f3a\u7684XGBoost\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cGlaBoost\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe98.71%\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\u4e34\u5e8a\u4e00\u81f4\u7684\u51b3\u7b56\u6a21\u5f0f\u3002", "conclusion": "GlaBoost\u4e3a\u9752\u5149\u773c\u8bca\u65ad\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u773c\u79d1\u75be\u75c5\u3002"}}
{"id": "2508.03772", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03772", "abs": "https://arxiv.org/abs/2508.03772", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models", "comment": null, "summary": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86GRPO\u7684\u4e24\u5927\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51faGTPO\u4f5c\u4e3a\u6539\u8fdb\u65b9\u6848\uff0c\u901a\u8fc7\u8df3\u8fc7\u51b2\u7a81\u4ee4\u724c\u7684\u8d1f\u66f4\u65b0\u548c\u8fc7\u6ee4\u9ad8\u71b5\u8865\u5168\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "GRPO\u5728\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b58\u5728\u4ee4\u724c\u51b2\u7a81\u548c\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51faGTPO\uff0c\u8bc6\u522b\u51b2\u7a81\u4ee4\u724c\u5e76\u8df3\u8fc7\u8d1f\u66f4\u65b0\uff0c\u540c\u65f6\u8fc7\u6ee4\u9ad8\u71b5\u8865\u5168\uff0c\u907f\u514dKL\u6563\u5ea6\u6b63\u5219\u5316\u3002", "result": "\u5728GSM8K\u3001MATH\u548cAIME 2024\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86GTPO\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GTPO\u89e3\u51b3\u4e86GRPO\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u53c2\u8003\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2508.04642", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04642", "abs": "https://arxiv.org/abs/2508.04642", "authors": ["Baihui Xiao", "Chengjian Feng", "Zhijian Huang", "Feng yan", "Yujie Zhong", "Lin Ma"], "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case", "comment": "ICCV 2025", "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/", "AI": {"tldr": "RoboTron-Sim\u901a\u8fc7\u6a21\u62df\u9ad8\u98ce\u9669\u573a\u666f\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\uff0c\u5229\u7528HASS\u6570\u636e\u96c6\u548cSPE\u3001I2E Encoder\u6280\u672f\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u534750%\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7f55\u89c1\u9ad8\u98ce\u9669\u573a\u666f\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1HASS\u6a21\u62df\u6570\u636e\u96c6\uff0c\u5f15\u5165SPE\u548cI2E Encoder\u6280\u672f\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u9a7e\u9a76\u6280\u80fd\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\u7ea650%\uff0c\u5728\u5f00\u653e\u73af\u8def\u89c4\u5212\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "RoboTron-Sim\u80fd\u6709\u6548\u7ba1\u7406\u7f55\u89c1\u9ad8\u98ce\u9669\u9a7e\u9a76\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.03776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03776", "abs": "https://arxiv.org/abs/2508.03776", "authors": ["Xiao Wang", "Zikang Yan", "Hao Si", "Zhendong Yang", "Qingquan Yang", "Dengdi Sun", "Wanli Lyu", "Jin Tang"], "title": "Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network", "comment": null, "summary": "Estimating heat flux in the nuclear fusion device EAST is a critically\nimportant task. Traditional scientific computing methods typically model this\nprocess using the Finite Element Method (FEM). However, FEM relies on\ngrid-based sampling for computation, which is computationally inefficient and\nhard to perform real-time simulations during actual experiments. Inspired by\nartificial intelligence-powered scientific computing, this paper proposes a\nnovel Physics-Informed Neural Network (PINN) to address this challenge,\nsignificantly accelerating the heat conduction estimation process while\nmaintaining high accuracy. Specifically, given inputs of different materials,\nwe first feed spatial coordinates and time stamps into the neural network, and\ncompute boundary loss, initial condition loss, and physical loss based on the\nheat conduction equation. Additionally, we sample a small number of data points\nin a data-driven manner to better fit the specific heat conduction scenario,\nfurther enhancing the model's predictive capability. We conduct experiments\nunder both uniform and non-uniform heating conditions on the top surface.\nExperimental results show that the proposed thermal conduction physics-informed\nneural network achieves accuracy comparable to the finite element method, while\nachieving $\\times$40 times acceleration in computational efficiency. The\ndataset and source code will be released on\nhttps://github.com/Event-AHU/OpenFusion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u6838\u805a\u53d8\u88c5\u7f6eEAST\u4e2d\u7684\u70ed\u901a\u91cf\u4f30\u8ba1\uff0c\u76f8\u6bd4\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u5b9e\u73b0\u4e8640\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6a21\u62df\uff0c\u800c\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u79d1\u5b66\u8ba1\u7b97\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u5c06\u7a7a\u95f4\u5750\u6807\u548c\u65f6\u95f4\u6233\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u70ed\u4f20\u5bfc\u65b9\u7a0b\u8ba1\u7b97\u8fb9\u754c\u635f\u5931\u3001\u521d\u59cb\u6761\u4ef6\u635f\u5931\u548c\u7269\u7406\u635f\u5931\uff0c\u5e76\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u91c7\u6837\u5c11\u91cf\u6570\u636e\u70b9\u4ee5\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u52a0\u70ed\u6761\u4ef6\u4e0b\u5747\u80fd\u8fbe\u5230\u4e0eFEM\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u4e8640\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684PINN\u65b9\u6cd5\u5728\u70ed\u4f20\u5bfc\u4f30\u8ba1\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u65f6\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2508.03749", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03749", "abs": "https://arxiv.org/abs/2508.03749", "authors": ["Riccardo Fiorista", "Awad Abdelhalim", "Anson F. Stewart", "Gabriel L. Pincus", "Ian Thistle", "Jinhua Zhao"], "title": "Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation", "comment": "26 pages, 17 figures, 4 tables", "summary": "Accurately estimating urban rail platform occupancy can enhance transit\nagencies' ability to make informed operational decisions, thereby improving\nsafety, operational efficiency, and customer experience, particularly in the\ncontext of crowding. However, sensing real-time crowding remains challenging\nand often depends on indirect proxies such as automatic fare collection data or\nstaff observations. Recently, Closed-Circuit Television (CCTV) footage has\nemerged as a promising data source with the potential to yield accurate,\nreal-time occupancy estimates. The presented study investigates this potential\nby comparing three state-of-the-art computer vision approaches for extracting\ncrowd-related features from platform CCTV imagery: (a) object detection and\ncounting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification\nvia a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic\nsegmentation using DeepLabV3. Additionally, we present a novel, highly\nefficient linear-optimization-based approach to extract counts from the\ngenerated segmentation maps while accounting for image object depth and, thus,\nfor passenger dispersion along a platform. Tested on a privacy-preserving\ndataset created in collaboration with the Washington Metropolitan Area Transit\nAuthority (WMATA) that encompasses more than 600 hours of video material, our\nresults demonstrate that computer vision approaches can provide substantive\nvalue for crowd estimation. This work demonstrates that CCTV image data,\nindependent of other data sources available to a transit agency, can enable\nmore precise real-time crowding estimation and, eventually, timely operational\nresponses for platform crowding mitigation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528CCTV\u89c6\u9891\u6570\u636e\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9e\u65f6\u4f30\u8ba1\u57ce\u5e02\u8f68\u9053\u4ea4\u901a\u7ad9\u53f0\u62e5\u6324\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u7ad9\u53f0\u62e5\u6324\u5ea6\u53ef\u63d0\u5347\u8fd0\u8425\u51b3\u7b56\u80fd\u529b\uff0c\u6539\u5584\u5b89\u5168\u3001\u6548\u7387\u548c\u4e58\u5ba2\u4f53\u9a8c\uff0c\u4f46\u76ee\u524d\u4f9d\u8d56\u95f4\u63a5\u6570\u636e\u6e90\u3002CCTV\u89c6\u9891\u6570\u636e\u6709\u671b\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5b9e\u65f6\u4f30\u8ba1\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff08\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ba1\u6570\u3001\u4eba\u7fa4\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u5305\u542b600\u591a\u5c0f\u65f6\u89c6\u9891\u7684\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u8bc1\u660e\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u5bf9\u62e5\u6324\u5ea6\u4f30\u8ba1\u5177\u6709\u663e\u8457\u4ef7\u503c\u3002", "conclusion": "CCTV\u6570\u636e\u53ef\u72ec\u7acb\u652f\u6301\u66f4\u7cbe\u786e\u7684\u5b9e\u65f6\u62e5\u6324\u5ea6\u4f30\u8ba1\uff0c\u52a9\u529b\u53ca\u65f6\u8fd0\u8425\u54cd\u5e94\u3002"}}
{"id": "2508.04278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04278", "abs": "https://arxiv.org/abs/2508.04278", "authors": ["Wentao Wu", "Linqing Chen", "Hanmeng Zhong", "Weilei Wang"], "title": "Large Language Model's Multi-Capability Alignment in Biomedical Domain", "comment": null, "summary": "BalancedBio is a theoretically grounded framework for parameter-efficient\nbiomedical reasoning, addressing multi-capability integration in\ndomain-specific AI alignment. It establishes the Biomedical Multi-Capability\nConvergence Theorem, proving orthogonal gradient spaces are essential to\nprevent capability interference for safe deployment. Key innovations include:\n(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending\nSource2Synth with clinical workflow constraints and medical ontology validation\nfor factual accuracy and safety; and (2) Capability Aware Group Relative Policy\nOptimization, deriving optimal hybrid reward weighting to maintain\northogonality in RL, using a reward model with rule-based and model-based\nscores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal\nconvergence, preserving performance across capabilities. It achieves\nstate-of-the-art results in its parameter class: domain expertise (80.95%\nBIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction\nfollowing (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety\nguarantees include bounds on capability preservation and clinical accuracy.\nReal-world deployment yields 78% cost reduction, 23% improved diagnostic\naccuracy, and 89% clinician acceptance. This work provides a principled\nmethodology for biomedical AI alignment, enabling efficient reasoning with\nessential safety and reliability, with the 0.5B model version to be released.", "AI": {"tldr": "BalancedBio\u662f\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u7a7a\u95f4\u548c\u591a\u80fd\u529b\u96c6\u6210\u5b9e\u73b0\u9ad8\u6548\u7684\u751f\u7269\u533b\u5b66\u63a8\u7406\uff0c\u63d0\u5347\u6027\u80fd\u548c\u5b89\u5168\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u533b\u5b66AI\u9886\u57df\u4e2d\u591a\u80fd\u529b\u96c6\u6210\u65f6\u7684\u5e72\u6270\u95ee\u9898\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5305\u62ec\u533b\u5b66\u77e5\u8bc6\u57fa\u7840\u7684\u5408\u6210\u751f\u6210\uff08MKGSG\uff09\u548c\u80fd\u529b\u611f\u77e5\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u7ed3\u5408\u89c4\u5219\u548c\u6a21\u578b\u8bc4\u5206\u3002", "result": "\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5982\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0880.95%\uff09\u3001\u63a8\u7406\uff0861.94%\uff09\u7b49\uff0c\u5e76\u964d\u4f4e\u6210\u672c78%\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u751f\u7269\u533b\u5b66AI\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1\u548c\u5b9e\u9645\u90e8\u7f72\u4f18\u52bf\u3002"}}
{"id": "2508.04039", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04039", "abs": "https://arxiv.org/abs/2508.04039", "authors": ["Thilo Hagendorff", "Erik Derner", "Nuria Oliver"], "title": "Large Reasoning Models Are Autonomous Jailbreak Agents", "comment": null, "summary": "Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has\ntraditionally required complex technical procedures or specialized human\nexpertise. In this study, we show that the persuasive capabilities of large\nreasoning models (LRMs) simplify and scale jailbreaking, converting it into an\ninexpensive activity accessible to non-experts. We evaluated the capabilities\nof four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as\nautonomous adversaries conducting multi-turn conversations with nine widely\nused target models. LRMs received instructions via a system prompt, before\nproceeding to planning and executing jailbreaks with no further supervision. We\nperformed extensive experiments with a benchmark of harmful prompts composed of\n70 items covering seven sensitive domains. This setup yielded an overall attack\nsuccess rate across all model combinations of 97.14%. Our study reveals an\nalignment regression, in which LRMs can systematically erode the safety\nguardrails of other models, highlighting the urgent need to further align\nfrontier models not only to resist jailbreak attempts, but also to prevent them\nfrom being co-opted into acting as jailbreak agents.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u53ef\u4ee5\u7b80\u5316\u5e76\u89c4\u6a21\u5316\u8d8a\u72f1\uff08jailbreaking\uff09\u884c\u4e3a\uff0c\u4f7f\u5176\u6210\u4e3a\u975e\u4e13\u5bb6\u4e5f\u80fd\u8fdb\u884c\u7684\u4f4e\u6210\u672c\u6d3b\u52a8\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u56db\u79cdLRM\u6a21\u578b\u5728\u653b\u51fb\u4e5d\u79cd\u76ee\u6807\u6a21\u578b\u65f6\uff0c\u603b\u4f53\u6210\u529f\u7387\u9ad8\u8fbe97.14%\u3002", "motivation": "\u4f20\u7edf\u8d8a\u72f1\u65b9\u6cd5\u9700\u8981\u590d\u6742\u6280\u672f\u6216\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cLRMs\u7684\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u4f7f\u5176\u6210\u4e3a\u9ad8\u6548\u7684\u8d8a\u72f1\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u56db\u79cdLRM\u6a21\u578b\uff0c\u901a\u8fc7\u7cfb\u7edf\u63d0\u793a\u6307\u5bfc\u5176\u81ea\u4e3b\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u653b\u51fb\u4e5d\u79cd\u76ee\u6807\u6a21\u578b\uff0c\u6d4b\u8bd570\u4e2a\u6709\u5bb3\u63d0\u793a\u7684\u8d8a\u72f1\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709\u6a21\u578b\u7ec4\u5408\u7684\u603b\u4f53\u653b\u51fb\u6210\u529f\u7387\u4e3a97.14%\uff0c\u8868\u660eLRMs\u80fd\u7cfb\u7edf\u6027\u524a\u5f31\u5176\u4ed6\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\u9700\u8981\u8fdb\u4e00\u6b65\u5bf9\u9f50\uff0c\u4e0d\u4ec5\u8981\u62b5\u6297\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd8\u9700\u9632\u6b62\u81ea\u8eab\u88ab\u5229\u7528\u4e3a\u8d8a\u72f1\u5de5\u5177\u3002"}}
{"id": "2508.03921", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03921", "abs": "https://arxiv.org/abs/2508.03921", "authors": ["John D. Kelleher", "Matthew Nicholson", "Rahul Agrahari", "Clare Conran"], "title": "Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data", "comment": null, "summary": "This paper examines the effectiveness of combining active learning and\ntransfer learning for anomaly detection in cross-domain time-series data. Our\nresults indicate that there is an interaction between clustering and active\nlearning and in general the best performance is achieved using a single cluster\n(in other words when clustering is not applied). Also, we find that adding new\nsamples to the training set using active learning does improve model\nperformance but that in general, the rate of improvement is slower than the\nresults reported in the literature suggest. We attribute this difference to an\nimproved experimental design where distinct data samples are used for the\nsampling and testing pools. Finally, we assess the ceiling performance of\ntransfer learning in combination with active learning across several datasets\nand find that performance does initially improve but eventually begins to tail\noff as more target points are selected for inclusion in training. This tail-off\nin performance may indicate that the active learning process is doing a good\njob of sequencing data points for selection, pushing the less useful points\ntowards the end of the selection process and that this tail-off occurs when\nthese less useful points are eventually added. Taken together our results\nindicate that active learning is effective but that the improvement in model\nperformance follows a linear flat function concerning the number of points\nselected and labelled.", "AI": {"tldr": "\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u7528\u4e8e\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u53d1\u73b0\u805a\u7c7b\u4e0e\u4e3b\u52a8\u5b66\u4e60\u5b58\u5728\u4ea4\u4e92\u4f5c\u7528\uff0c\u6700\u4f73\u6027\u80fd\u51fa\u73b0\u5728\u672a\u805a\u7c7b\u65f6\uff1b\u4e3b\u52a8\u5b66\u4e60\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901f\u5ea6\u8f83\u6162\uff1b\u8fc1\u79fb\u5b66\u4e60\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u6027\u80fd\u63d0\u5347\u521d\u671f\u663e\u8457\uff0c\u540e\u671f\u8d8b\u4e8e\u5e73\u7f13\u3002", "motivation": "\u7814\u7a76\u4e3b\u52a8\u5b66\u4e60\u4e0e\u8fc1\u79fb\u5b66\u4e60\u5728\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a2\u7d22\u5176\u6027\u80fd\u63d0\u5347\u7684\u6f5c\u529b\u4e0e\u9650\u5236\u3002", "method": "\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bbe\u8ba1\u533a\u5206\u91c7\u6837\u4e0e\u6d4b\u8bd5\u6570\u636e\uff0c\u8bc4\u4f30\u805a\u7c7b\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u4ea4\u4e92\u4f5c\u7528\u53ca\u6027\u80fd\u53d8\u5316\u3002", "result": "\u672a\u805a\u7c7b\u65f6\u6027\u80fd\u6700\u4f73\uff1b\u4e3b\u52a8\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u4f46\u901f\u5ea6\u8f83\u6162\uff1b\u8fc1\u79fb\u5b66\u4e60\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u6027\u80fd\u521d\u671f\u63d0\u5347\u540e\u8d8b\u4e8e\u5e73\u7f13\u3002", "conclusion": "\u4e3b\u52a8\u5b66\u4e60\u6709\u6548\u4f46\u6027\u80fd\u63d0\u5347\u5448\u7ebf\u6027\u5e73\u7f13\u8d8b\u52bf\uff0c\u8fc1\u79fb\u5b66\u4e60\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u6027\u80fd\u63d0\u5347\u5b58\u5728\u4e0a\u9650\u3002"}}
{"id": "2508.04511", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04511", "abs": "https://arxiv.org/abs/2508.04511", "authors": ["Hamed Ayoobi", "Nico Potyka", "Anna Rapberger", "Francesca Toni"], "title": "Argumentative Debates for Transparent Bias Detection [Technical Report]", "comment": null, "summary": "As the use of AI systems in society grows, addressing potential biases that\nemerge from data or are learned by models is essential to prevent systematic\ndisadvantages against specific groups. Several notions of (un)fairness have\nbeen proposed in the literature, alongside corresponding algorithmic methods\nfor detecting and mitigating unfairness, but, with very few exceptions, these\ntend to ignore transparency. Instead, interpretability and explainability are\ncore requirements for algorithmic fairness, even more so than for other\nalgorithmic solutions, given the human-oriented nature of fairness. In this\npaper, we contribute a novel interpretable, explainable method for bias\ndetection relying on debates about the presence of bias against individuals,\nbased on the values of protected features for the individuals and others in\ntheir neighbourhoods. Our method builds upon techniques from formal and\ncomputational argumentation, whereby debates result from arguing about biases\nwithin and across neighbourhoods. We provide formal, quantitative, and\nqualitative evaluations of our method, highlighting its strengths in\nperformance against baselines, as well as its interpretability and\nexplainability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fa9\u8bba\u7684\u65b0\u578b\u53ef\u89e3\u91ca\u3001\u53ef\u89e3\u91ca\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u5f62\u5f0f\u5316\u548c\u8ba1\u7b97\u8bba\u8bc1\u6280\u672f\uff0c\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u89e3\u91ca\u6027\u5728\u7b97\u6cd5\u516c\u5e73\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u793e\u4f1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9632\u6b62\u6570\u636e\u6216\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u5bf9\u7279\u5b9a\u7fa4\u4f53\u9020\u6210\u7cfb\u7edf\u6027\u4e0d\u5229\u5f71\u54cd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5ffd\u7565\u900f\u660e\u5ea6\uff0c\u800c\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u662f\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u6838\u5fc3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fa9\u8bba\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u4fdd\u62a4\u7279\u5f81\u503c\u548c\u90bb\u57df\u5185\u4e2a\u4f53\u7684\u503c\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u548c\u8ba1\u7b97\u8bba\u8bc1\u6280\u672f\u8fdb\u884c\u8fa9\u8bba\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u5316\u3001\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7b97\u6cd5\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u5728\u504f\u89c1\u68c0\u6d4b\u4e2d\u900f\u660e\u5ea6\u548c\u89e3\u91ca\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.04183", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.04183", "abs": "https://arxiv.org/abs/2508.04183", "authors": ["Abhinav Java", "Ashmit Khandelwal", "Sukruta Midigeshi", "Aaron Halfaker", "Amit Deshpande", "Navin Goyal", "Ankur Gupta", "Nagarajan Natarajan", "Amit Sharma"], "title": "Characterizing Deep Research: A Benchmark and Formal Definition", "comment": "First three authors contributed equally (ordered alphabetically)", "summary": "Information tasks such as writing surveys or analytical reports require\ncomplex search and reasoning, and have recently been grouped under the umbrella\nof \\textit{deep research} -- a term also adopted by recent models targeting\nthese capabilities. Despite growing interest, the scope of the deep research\ntask remains underdefined and its distinction from other reasoning-intensive\nproblems is poorly understood. In this paper, we propose a formal\ncharacterization of the deep research (DR) task and introduce a benchmark to\nevaluate the performance of DR systems. We argue that the core defining feature\nof deep research is not the production of lengthy report-style outputs, but\nrather the high fan-out over concepts required during the search process, i.e.,\nbroad and reasoning-intensive exploration. To enable objective evaluation, we\ndefine DR using an intermediate output representation that encodes key claims\nuncovered during search-separating the reasoning challenge from surface-level\nreport generation. Based on this formulation, we propose a diverse, challenging\nbenchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,\ndatasets, materials discovery, prior art search) and public interest events\n(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1\nscore ranges between 0.02 and 0.72 for any sub-category. OpenAI's model\nperforms the best with an overall F1 score of 0.55. Analysis of reasoning\ntraces reveals the distribution over the number of referenced sources,\nbranching, and backtracking events executed by current DR systems, motivating\nfuture directions for improving their search mechanisms and grounding\ncapabilities. The benchmark is available at\nhttps://github.com/microsoft/LiveDRBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5bf9\u201c\u6df1\u5ea6\u7814\u7a76\u201d\uff08DR\uff09\u4efb\u52a1\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08LiveDRBench\uff09\u6765\u8bc4\u4f30DR\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u6838\u5fc3\u7279\u70b9\u662f\u6982\u5ff5\u7684\u9ad8\u8986\u76d6\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u63a2\u7d22\uff0c\u800c\u975e\u957f\u7bc7\u62a5\u544a\u7684\u8f93\u51fa\u3002OpenAI\u7684\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08F1=0.55\uff09\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u7684\u4efb\u52a1\u8303\u56f4\u5b9a\u4e49\u4e0d\u6e05\uff0c\u4e0e\u5176\u4ed6\u63a8\u7406\u5bc6\u96c6\u578b\u95ee\u9898\u7684\u533a\u522b\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e00\u79cd\u5ba2\u89c2\u7684\u8bc4\u4ef7\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e2d\u95f4\u8f93\u51fa\u8868\u793a\u5b9a\u4e49DR\u4efb\u52a1\uff0c\u5206\u79bb\u63a8\u7406\u6311\u6218\u4e0e\u62a5\u544a\u751f\u6210\uff0c\u5e76\u6784\u5efa\u5305\u542b100\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u73b0\u6709DR\u7cfb\u7edf\u7684F1\u5206\u6570\u57280.02\u52300.72\u4e4b\u95f4\uff0cOpenAI\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08F1=0.55\uff09\u3002\u5206\u6790\u63ed\u793a\u4e86\u641c\u7d22\u673a\u5236\u548c\u57fa\u7840\u80fd\u529b\u7684\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u4e3aDR\u4efb\u52a1\u63d0\u4f9b\u4e86\u6e05\u6670\u5b9a\u4e49\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.04239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.04239", "abs": "https://arxiv.org/abs/2508.04239", "authors": ["Chanjuan Liu", "Shengzhi Wang", "Enqiang Zhu"], "title": "DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting", "comment": null, "summary": "Time series forecasting is crucial in strategic planning and decision-making\nacross various industries. Traditional forecasting models mainly concentrate on\nnumerical time series data, often overlooking important textual information\nsuch as events and news, which can significantly affect forecasting accuracy.\nWhile large language models offer a promise for integrating multimodal data,\nexisting single-prompt frameworks struggle to effectively capture the semantics\nof timestamped text, introducing redundant information that can hinder model\nperformance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt\nGPT2-base for Multimodal Time Series), a novel dual-prompt large language model\nframework that combines two complementary prompts: an explicit prompt for clear\ntask instructions and a textual prompt for context-aware embeddings from\ntime-stamped data. The tokenizer generates the explicit prompt while the\nembeddings from the textual prompt are refined through self-attention and\nfeed-forward networks. Comprehensive experiments conducted on diverse\ntextural-numerical time series datasets demonstrate that this approach\noutperforms state-of-the-art algorithms in time series forecasting. This\nhighlights the significance of incorporating textual context via a dual-prompt\nmechanism to achieve more accurate time series predictions.", "AI": {"tldr": "DP-GPT4MTS\u662f\u4e00\u79cd\u53cc\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u663e\u5f0f\u548c\u6587\u672c\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5ffd\u89c6\u6587\u672c\u4fe1\u606f\uff0c\u5f71\u54cd\u51c6\u786e\u6027\u3002\u73b0\u6709\u5355\u63d0\u793a\u6846\u67b6\u96be\u4ee5\u6709\u6548\u5904\u7406\u65f6\u95f4\u6233\u6587\u672c\u8bed\u4e49\u3002", "method": "\u63d0\u51faDP-GPT4MTS\uff0c\u7ed3\u5408\u663e\u5f0f\u63d0\u793a\u548c\u6587\u672c\u63d0\u793a\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u7f51\u7edc\u4f18\u5316\u5d4c\u5165\u3002", "result": "\u5728\u591a\u6837\u5316\u6587\u672c-\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u53cc\u63d0\u793a\u673a\u5236\u901a\u8fc7\u6574\u5408\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2508.04129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04129", "abs": "https://arxiv.org/abs/2508.04129", "authors": ["Xun Lin", "Xiaobao Guo", "Taorui Wang", "Yingjie Ma", "Jiajian Huang", "Jiayu Zhang", "Junzhe Cao", "Zitong Yu"], "title": "SVC 2025: the First Multimodal Deception Detection Challenge", "comment": "Accepted by Workshop SVC of ACM MM 2025", "summary": "Deception detection is a critical task in real-world applications such as\nsecurity screening, fraud prevention, and credibility assessment. While deep\nlearning methods have shown promise in surpassing human-level performance,\ntheir effectiveness often depends on the availability of high-quality and\ndiverse deception samples. Existing research predominantly focuses on\nsingle-domain scenarios, overlooking the significant performance degradation\ncaused by domain shifts. To address this gap, we present the SVC 2025\nMultimodal Deception Detection Challenge, a new benchmark designed to evaluate\ncross-domain generalization in audio-visual deception detection. Participants\nare required to develop models that not only perform well within individual\ndomains but also generalize across multiple heterogeneous datasets. By\nleveraging multimodal data, including audio, video, and text, this challenge\nencourages the design of models capable of capturing subtle and implicit\ndeceptive cues. Through this benchmark, we aim to foster the development of\nmore adaptable, explainable, and practically deployable deception detection\nsystems, advancing the broader field of multimodal learning. By the conclusion\nof the workshop competition, a total of 21 teams had submitted their final\nresults. https://sites.google.com/view/svc-mm25 for more information.", "AI": {"tldr": "SVC 2025\u591a\u6a21\u6001\u6b3a\u9a97\u68c0\u6d4b\u6311\u6218\u8d5b\u65e8\u5728\u89e3\u51b3\u8de8\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u6b3a\u9a97\u68c0\u6d4b\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u9886\u57df\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u9886\u57df\u8f6c\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u5f00\u53d1\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "method": "\u5229\u7528\u97f3\u9891\u3001\u89c6\u9891\u548c\u6587\u672c\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u8bbe\u8ba1\u80fd\u6355\u6349\u7ec6\u5fae\u6b3a\u9a97\u7ebf\u7d22\u7684\u6a21\u578b\u3002", "result": "21\u652f\u56e2\u961f\u63d0\u4ea4\u4e86\u6700\u7ec8\u7ed3\u679c\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4fc3\u8fdb\u4e86\u66f4\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u89e3\u91ca\u6027\u9ad8\u7684\u6b3a\u9a97\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.04346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04346", "abs": "https://arxiv.org/abs/2508.04346", "authors": ["Zihan Liu", "Jiayi Wen", "Shouhong Tan", "Zhirun Zheng", "Cheng Huang"], "title": "From Split to Share: Private Inference with Distributed Feature Sharing", "comment": null, "summary": "Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy\nconcerns when handling sensitive client data. Existing Private Inference (PI)\nmethods face a fundamental trade-off between privacy and efficiency:\ncryptographic approaches offer strong protection but incur high computational\noverhead, while efficient alternatives such as split inference expose\nintermediate features to inversion attacks. We propose PrivDFS, a new paradigm\nfor private inference that replaces a single exposed representation with\ndistributed feature sharing. PrivDFS partitions input features on the client\ninto multiple balanced shares, which are distributed to non-colluding,\nnon-communicating servers for independent partial inference. The client\nsecurely aggregates the servers' outputs to reconstruct the final prediction,\nensuring that no single server observes sufficient information to compromise\ninput privacy. To further strengthen privacy, we propose two key extensions:\nPrivDFS-AT, which uses adversarial training with a diffusion-based proxy\nattacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,\nwhich leverages user-specific keys to diversify partitioning policies and\nprevent query-based inversion generalization. Experiments on CIFAR-10 and\nCelebA demonstrate that PrivDFS achieves privacy comparable to deep split\ninference while cutting client computation by up to 100 times with no accuracy\nloss, and that the extensions remain robust against both diffusion-based\nin-distribution and adaptive attacks.", "AI": {"tldr": "PrivDFS\u662f\u4e00\u79cd\u65b0\u7684\u79c1\u6709\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u7279\u5f81\u5171\u4eab\u89e3\u51b3\u9690\u79c1\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u51fa\u4e24\u79cd\u6269\u5c55\u65b9\u6cd5\u8fdb\u4e00\u6b65\u5f3a\u5316\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5904\u7406\u654f\u611f\u5ba2\u6237\u6570\u636e\u65f6\uff0c\u73b0\u6709\u7684\u79c1\u6709\u63a8\u7406\u65b9\u6cd5\u5728\u9690\u79c1\u548c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\u3002", "method": "PrivDFS\u5c06\u8f93\u5165\u7279\u5f81\u5206\u533a\u5e76\u5206\u53d1\u5230\u591a\u4e2a\u975e\u5171\u8c0b\u670d\u52a1\u5668\u8fdb\u884c\u72ec\u7acb\u63a8\u7406\uff0c\u5ba2\u6237\u7aef\u5b89\u5168\u805a\u5408\u7ed3\u679c\uff1b\u6269\u5c55\u65b9\u6cd5\u5305\u62ec\u5bf9\u6297\u8bad\u7ec3\uff08PrivDFS-AT\uff09\u548c\u7528\u6237\u7279\u5b9a\u5bc6\u94a5\uff08PrivDFS-KD\uff09\u3002", "result": "\u5728CIFAR-10\u548cCelebA\u4e0a\uff0cPrivDFS\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5c06\u5ba2\u6237\u7aef\u8ba1\u7b97\u51cf\u5c11100\u500d\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931\uff0c\u6269\u5c55\u65b9\u6cd5\u5bf9\u591a\u79cd\u653b\u51fb\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "PrivDFS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u63a8\u7406\u65b9\u6848\uff0c\u6269\u5c55\u65b9\u6cd5\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2508.04351", "categories": ["cs.LG", "cs.NE", "I.2", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.04351", "abs": "https://arxiv.org/abs/2508.04351", "authors": ["Justin Lee", "Behnaz Moradijamei", "Heman Shakeri"], "title": "Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points", "comment": "23 pages, 10 figures", "summary": "Modeling the evolution of high-dimensional systems from limited snapshot\nobservations at irregular time points poses a significant challenge in\nquantitative biology and related fields. Traditional approaches often rely on\ndimensionality reduction techniques, which can oversimplify the dynamics and\nfail to capture critical transient behaviors in non-equilibrium systems. We\npresent Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of\nsimulation-free score and flow matching methods to the multi-marginal setting,\nenabling the alignment of high-dimensional data measured at non-equidistant\ntime points without reducing dimensionality. The use of measure-valued splines\nenhances robustness to irregular snapshot timing, and score matching prevents\noverfitting in high-dimensional spaces. We validate our framework on several\nsynthetic and benchmark datasets, including gene expression data collected at\nuneven time points and an image progression task, demonstrating the method's\nversatility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMSFM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u7ef4\u7cfb\u7edf\u5728\u975e\u5747\u5300\u65f6\u95f4\u70b9\u89c2\u6d4b\u6570\u636e\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u7cfb\u7edf\u5728\u975e\u5747\u5300\u65f6\u95f4\u70b9\u89c2\u6d4b\u6570\u636e\u5efa\u6a21\u7684\u6311\u6218\uff0c\u907f\u514d\u964d\u7ef4\u65b9\u6cd5\u5bf9\u52a8\u6001\u548c\u77ac\u6001\u884c\u4e3a\u7684\u7b80\u5316\u3002", "method": "\u6269\u5c55\u4e86\u65e0\u6a21\u62df\u7684\u5206\u6570\u548c\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u5f15\u5165\u591a\u8fb9\u9645\u8bbe\u7f6e\uff0c\u7ed3\u5408\u6d4b\u5ea6\u503c\u6837\u6761\u548c\u5206\u6570\u5339\u914d\u6280\u672f\u3002", "result": "\u5728\u5408\u6210\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982\u57fa\u56e0\u8868\u8fbe\u548c\u56fe\u50cf\u8fdb\u5c55\u4efb\u52a1\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MMSFM\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u964d\u7ef4\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u975e\u5747\u5300\u65f6\u95f4\u70b9\u7684\u9ad8\u7ef4\u6570\u636e\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.04224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04224", "abs": "https://arxiv.org/abs/2508.04224", "authors": ["Jiahui Li", "Shengeng Tang", "Jingxuan He", "Gang Huang", "Zhangye Wang", "Yantao Pan", "Lechao Cheng"], "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition", "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally\nchallenging due to the need to jointly infer motion, structure, and appearance\nfrom limited observations. Existing dynamic scene reconstruction methods based\non Gaussian Splatting often entangle static and dynamic elements in a shared\nrepresentation, leading to motion leakage, geometric distortions, and temporal\nflickering. We identify that the root cause lies in the coupled modeling of\ngeometry and appearance across time, which hampers both stability and\ninterpretability. To address this, we propose \\textbf{SplitGaussian}, a novel\nframework that explicitly decomposes scene representations into static and\ndynamic components. By decoupling motion modeling from background geometry and\nallowing only the dynamic branch to deform over time, our method prevents\nmotion artifacts in static regions while supporting view- and time-dependent\nappearance refinement. This disentangled design not only enhances temporal\nconsistency and reconstruction fidelity but also accelerates convergence.\nExtensive experiments demonstrate that SplitGaussian outperforms prior\nstate-of-the-art methods in rendering quality, geometric stability, and motion\nseparation.", "AI": {"tldr": "SplitGaussian \u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u573a\u666f\u8868\u793a\u5206\u89e3\u4e3a\u9759\u6001\u548c\u52a8\u6001\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u8fd0\u52a8\u6cc4\u6f0f\u548c\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5e38\u5c06\u9759\u6001\u548c\u52a8\u6001\u5143\u7d20\u8026\u5408\u8868\u793a\uff0c\u5bfc\u81f4\u8fd0\u52a8\u6cc4\u6f0f\u3001\u51e0\u4f55\u5931\u771f\u548c\u65f6\u95f4\u95ea\u70c1\u3002", "method": "\u63d0\u51fa SplitGaussian\uff0c\u663e\u5f0f\u5206\u89e3\u573a\u666f\u8868\u793a\uff0c\u89e3\u8026\u8fd0\u52a8\u5efa\u6a21\u4e0e\u80cc\u666f\u51e0\u4f55\uff0c\u4ec5\u52a8\u6001\u5206\u652f\u968f\u65f6\u95f4\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSplitGaussian \u5728\u6e32\u67d3\u8d28\u91cf\u3001\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u8fd0\u52a8\u5206\u79bb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SplitGaussian \u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u6536\u655b\u3002"}}
{"id": "2508.04517", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04517", "abs": "https://arxiv.org/abs/2508.04517", "authors": ["Mo Zhang", "Xiaoyu Li", "Bin Xu", "Meng Chen", "Yongshun Gong"], "title": "Channel-Independent Federated Traffic Prediction", "comment": null, "summary": "In recent years, traffic prediction has achieved remarkable success and has\nbecome an integral component of intelligent transportation systems. However,\ntraffic data is typically distributed among multiple data owners, and privacy\nconstraints prevent the direct utilization of these isolated datasets for\ntraffic prediction. Most existing federated traffic prediction methods focus on\ndesigning communication mechanisms that allow models to leverage information\nfrom other clients in order to improve prediction accuracy. Unfortunately, such\napproaches often incur substantial communication overhead, and the resulting\ntransmission delays significantly slow down the training process. As the volume\nof traffic data continues to grow, this issue becomes increasingly critical,\nmaking the resource consumption of current methods unsustainable. To address\nthis challenge, we propose a novel variable relationship modeling paradigm for\nfederated traffic prediction, termed the Channel-Independent Paradigm(CIP).\nUnlike traditional approaches, CIP eliminates the need for inter-client\ncommunication by enabling each node to perform efficient and accurate\npredictions using only local information. Based on the CIP, we further develop\nFed-CI, an efficient federated learning framework, allowing each client to\nprocess its own data independently while effectively mitigating the information\nloss caused by the lack of direct data sharing among clients. Fed-CI\nsignificantly reduces communication overhead, accelerates the training process,\nand achieves state-of-the-art performance while complying with privacy\nregulations. Extensive experiments on multiple real-world datasets demonstrate\nthat Fed-CI consistently outperforms existing methods across all datasets and\nfederated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,\nand MAPE, respectively, while also substantially reducing communication costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCIP\u7684\u65b0\u8303\u5f0f\uff0c\u7528\u4e8e\u8054\u90a6\u4ea4\u901a\u9884\u6d4b\uff0c\u65e0\u9700\u5ba2\u6237\u7aef\u95f4\u901a\u4fe1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8054\u90a6\u4ea4\u901a\u9884\u6d4b\u65b9\u6cd5\u56e0\u901a\u4fe1\u5f00\u9500\u5927\u5bfc\u81f4\u7684\u8bad\u7ec3\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u7ea6\u675f\u3002", "method": "\u63d0\u51faCIP\u8303\u5f0f\uff0c\u5f00\u53d1Fed-CI\u6846\u67b6\uff0c\u4f7f\u5404\u5ba2\u6237\u7aef\u4ec5\u7528\u672c\u5730\u6570\u636e\u72ec\u7acb\u9884\u6d4b\uff0c\u51cf\u5c11\u901a\u4fe1\u9700\u6c42\u3002", "result": "Fed-CI\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cRMSE\u3001MAE\u548cMAPE\u5206\u522b\u63d0\u53478%\u300114%\u548c16%\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "CIP\u548cFed-CI\u4e3a\u8054\u90a6\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9690\u79c1\u5408\u89c4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.04595", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04595", "abs": "https://arxiv.org/abs/2508.04595", "authors": ["Jan A. Zak", "Christian Wei\u00dfenfels"], "title": "Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding", "comment": null, "summary": "Resistance spot welding is the dominant joining process for the body-in-white\nin the automotive industry, where the weld nugget diameter is the key quality\nmetric. Its measurement requires destructive testing, limiting the potential\nfor efficient quality control. Physics-informed neural networks were\ninvestigated as a promising tool to reconstruct internal process states from\nexperimental data, enabling model-based and non-invasive quality assessment in\naluminum spot welding. A major challenge is the integration of real-world data\ninto the network due to competing optimization objectives. To address this, we\nintroduce two novel training strategies. First, experimental losses for dynamic\ndisplacement and nugget diameter are progressively included using a fading-in\nfunction to prevent excessive optimization conflicts. We also implement a\ncustom learning rate scheduler and early stopping based on a rolling window to\ncounteract premature reduction due to increased loss magnitudes. Second, we\nintroduce a conditional update of temperature-dependent material parameters via\na look-up table, activated only after a loss threshold is reached to ensure\nphysically meaningful temperatures. An axially symmetric two-dimensional model\nwas selected to represent the welding process accurately while maintaining\ncomputational efficiency. To reduce computational burden, the training\nstrategies and model components were first systematically evaluated in one\ndimension, enabling controlled analysis of loss design and contact models. The\ntwo-dimensional network predicts dynamic displacement and nugget growth within\nthe experimental confidence interval, supports transferring welding stages from\nsteel to aluminum, and demonstrates strong potential for fast, model-based\nquality control in industrial applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u94dd\u70b9\u710a\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4e86\u771f\u5b9e\u6570\u636e\u96c6\u6210\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u975e\u7834\u574f\u6027\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u70b9\u710a\u662f\u6c7d\u8f66\u5de5\u4e1a\u4e2d\u4e3b\u8981\u7684\u8fde\u63a5\u5de5\u827a\uff0c\u4f46\u710a\u70b9\u76f4\u5f84\u7684\u6d4b\u91cf\u9700\u8981\u7834\u574f\u6027\u6d4b\u8bd5\uff0c\u9650\u5236\u4e86\u8d28\u91cf\u63a7\u5236\u6548\u7387\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u635f\u5931\u51fd\u6570\u548c\u6761\u4ef6\u66f4\u65b0\u7b56\u7565\uff0c\u7ed3\u5408\u4e8c\u7ef4\u8f74\u5bf9\u79f0\u6a21\u578b\uff0c\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002", "result": "\u4e8c\u7ef4\u7f51\u7edc\u6210\u529f\u9884\u6d4b\u4e86\u52a8\u6001\u4f4d\u79fb\u548c\u710a\u70b9\u751f\u957f\uff0c\u5e76\u5728\u5b9e\u9a8c\u7f6e\u4fe1\u533a\u95f4\u5185\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5feb\u901f\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u8d28\u91cf\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.04610", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.04610", "abs": "https://arxiv.org/abs/2508.04610", "authors": ["Md Zesun Ahmed Mia", "Malyaban Bal", "Sen Lu", "George M. Nishibuchi", "Suhas Chelian", "Srini Vasan", "Abhronil Sengupta"], "title": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning", "comment": null, "summary": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u5206\u5c42\u67b6\u6784\uff0c\u7528\u4e8e\u7ec8\u8eab\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08NIDS\uff09\uff0c\u901a\u8fc7\u9759\u6001\u548c\u52a8\u6001SNN\u7ed3\u5408\u751f\u7269\u542f\u53d1\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u548c\u4f4e\u529f\u8017\u90e8\u7f72\u3002", "motivation": "\u53d7\u5927\u8111\u5206\u5c42\u5904\u7406\u548c\u80fd\u91cf\u6548\u7387\u7684\u542f\u53d1\uff0c\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u5a01\u80c1\u5e76\u4fdd\u6301\u4f4e\u529f\u8017\u7684\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u9759\u6001SNN\u521d\u6b65\u8bc6\u522b\u6f5c\u5728\u5165\u4fb5\uff0c\u52a8\u6001SNN\u5206\u7c7b\u5177\u4f53\u653b\u51fb\u7c7b\u578b\uff0c\u7ed3\u5408GWR\u7ed3\u6784\u53ef\u5851\u6027\u548c\u81ea\u9002\u5e94STDP\u5b66\u4e60\u89c4\u5219\u3002", "result": "\u5728UNSW-NB15\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523085.3%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u4f4e\u529f\u8017\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.04260", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04260", "abs": "https://arxiv.org/abs/2508.04260", "authors": ["Xiao Wang", "Ziwen Wang", "Wentao Wu", "Anjie Wang", "Jiashu Wu", "Yantao Pan", "Chenglong Li"], "title": "Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark", "comment": null, "summary": "With the rapid advancement of autonomous driving, vehicle perception,\nparticularly detection and segmentation, has placed increasingly higher demands\non algorithmic performance. Pre-trained large segmentation models, especially\nSegment Anything Model (SAM), have sparked significant interest and inspired\nnew research directions in artificial intelligence. However, SAM cannot be\ndirectly applied to the fine-grained task of vehicle part segmentation, as its\ntext-prompted segmentation functionality is not publicly accessible, and the\nmask regions generated by its default mode lack semantic labels, limiting its\nutility in structured, category-specific segmentation tasks. To address these\nlimitations, we propose SAV, a novel framework comprising three core\ncomponents: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a\ncontext sample retrieval encoding module. The knowledge graph explicitly models\nthe spatial and geometric relationships among vehicle parts through a\nstructured ontology, effectively encoding prior structural knowledge.\nMeanwhile, the context retrieval module enhances segmentation by identifying\nand leveraging visually similar vehicle instances from training data, providing\nrich contextual priors for improved generalization. Furthermore, we introduce a\nnew large-scale benchmark dataset for vehicle part segmentation, named\nVehicleSeg10K, which contains 11,665 high-quality pixel-level annotations\nacross diverse scenes and viewpoints. We conduct comprehensive experiments on\nthis dataset and two other datasets, benchmarking multiple representative\nbaselines to establish a solid foundation for future research and comparison. %\nBoth the dataset and source code of this paper will be released upon\nacceptance. Both the dataset and source code of this paper will be released on\nhttps://github.com/Event-AHU/SAV", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSAV\u6846\u67b6\uff0c\u7ed3\u5408SAM\u6a21\u578b\u3001\u8f66\u8f86\u90e8\u4ef6\u77e5\u8bc6\u56fe\u8c31\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u8f66\u8f86\u90e8\u4ef6\u5206\u5272\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u6570\u636e\u96c6VehicleSeg10K\u3002", "motivation": "\u73b0\u6709SAM\u6a21\u578b\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u8f66\u8f86\u90e8\u4ef6\u5206\u5272\u4efb\u52a1\uff0c\u56e0\u5176\u7f3a\u4e4f\u8bed\u4e49\u6807\u7b7e\u548c\u6587\u672c\u63d0\u793a\u529f\u80fd\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSAV\u6846\u67b6\uff0c\u5305\u542bSAM\u7f16\u7801\u89e3\u7801\u5668\u3001\u8f66\u8f86\u90e8\u4ef6\u77e5\u8bc6\u56fe\u8c31\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\u6a21\u5757\uff0c\u5229\u7528\u7ed3\u6784\u5316\u77e5\u8bc6\u548c\u89c6\u89c9\u76f8\u4f3c\u6027\u63d0\u5347\u5206\u5272\u6548\u679c\u3002", "result": "\u5728VehicleSeg10K\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86SAV\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "SAV\u6846\u67b6\u89e3\u51b3\u4e86\u8f66\u8f86\u90e8\u4ef6\u5206\u5272\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u6570\u636e\u96c6\u548c\u5f00\u6e90\u4ee3\u7801\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.04369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04369", "abs": "https://arxiv.org/abs/2508.04369", "authors": ["Canhui Tang", "Zifan Han", "Hongbo Sun", "Sanping Zhou", "Xuchong Zhang", "Xin Wei", "Ye Yuan", "Jinglin Xu", "Hao Sun"], "title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in vision-language tasks, yet they still face challenges when\nprocessing long-duration video inputs. The limitation arises from MLLMs'\ncontext limit and training costs, necessitating sparse frame sampling before\nfeeding videos into MLLMs. Existing video MLLMs adopt training-free uniform\nsampling or keyframe search, which may miss critical events or be constrained\nby the pre-trained models' event understanding capabilities. Meanwhile,\nbuilding a training-based method remains challenging due to the unsupervised\nand non-differentiable nature of sparse frame sampling. To address these\nproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancing\nMLLMs' long-form video-language understanding via reinforcement learning.\nSpecifically, we first propose a trainable event-aware temporal agent, which\ncaptures event-query correlation for performing probabilistic keyframe\nselection. Then, we propose the TSPO reinforcement learning paradigm, which\nmodels keyframe selection and language generation as a joint decision-making\nprocess, enabling end-to-end group relative optimization with efficient\nrule-based rewards. Furthermore, for the TSPO's training, we propose a long\nvideo training data construction pipeline with comprehensive temporal data and\nvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answering\naccuracy and temporal locating reward mechanisms to optimize the temporal\nsampling policy. Comprehensive experiments show that our TSPO achieves\nstate-of-the-art performance across multiple long video understanding\nbenchmarks, and shows transferable ability across different cutting-edge\nVideo-MLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTSPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u7a00\u758f\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLMs\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u56e0\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u8bad\u7ec3\u6210\u672c\u95ee\u9898\uff0c\u901a\u5e38\u91c7\u7528\u65e0\u8bad\u7ec3\u7684\u5747\u5300\u91c7\u6837\u6216\u5173\u952e\u5e27\u641c\u7d22\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u4e8b\u4ef6\u6216\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e8b\u4ef6\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faTSPO\u65b9\u6cd5\uff0c\u5305\u62ec\u53ef\u8bad\u7ec3\u7684\u4e8b\u4ef6\u611f\u77e5\u65f6\u95f4\u4ee3\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5c06\u5173\u952e\u5e27\u9009\u62e9\u548c\u8bed\u8a00\u751f\u6210\u5efa\u6a21\u4e3a\u8054\u5408\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u9ad8\u6548\u5956\u52b1\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTSPO\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u4e0d\u540c\u524d\u6cbf\u89c6\u9891MLLMs\u7684\u53ef\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "TSPO\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7a00\u758f\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.04472", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04472", "abs": "https://arxiv.org/abs/2508.04472", "authors": ["Hongxu Chen", "Zhen Wang", "Taoran Mei", "Lin Li", "Bowei Zhu", "Runshi Li", "Long Chen"], "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model", "comment": null, "summary": "Concept Erasure, which aims to prevent pretrained text-to-image models from\ngenerating content associated with semantic-harmful concepts (i.e., target\nconcepts), is getting increased attention. State-of-the-art methods formulate\nthis task as an optimization problem: they align all target concepts with\nsemantic-harmless anchor concepts, and apply closed-form solutions to update\nthe model accordingly. While these closed-form methods are efficient, we argue\nthat existing methods have two overlooked limitations: 1) They often result in\nincomplete erasure due to \"non-zero alignment residual\", especially when text\nprompts are relatively complex. 2) They may suffer from generation quality\ndegradation as they always concentrate parameter updates in a few deep layers.\nTo address these issues, we propose a novel closed-form method ErasePro: it is\ndesigned for more complete concept erasure and better preserving overall\ngenerative quality. Specifically, ErasePro first introduces a strict\nzero-residual constraint into the optimization objective, ensuring perfect\nalignment between target and anchor concept features and enabling more complete\nerasure. Secondly, it employs a progressive, layer-wise update strategy that\ngradually transfers target concept features to those of the anchor concept from\nshallow to deep layers. As the depth increases, the required parameter changes\ndiminish, thereby reducing deviations in sensitive deep layers and preserving\ngenerative quality. Empirical results across different concept erasure tasks\n(including instance, art style, and nudity erasure) have demonstrated the\neffectiveness of our ErasePro.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5ErasePro\uff0c\u901a\u8fc7\u96f6\u6b8b\u5dee\u7ea6\u675f\u548c\u6e10\u8fdb\u5f0f\u5c42\u66f4\u65b0\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u63d0\u793a\u4e0b\u64e6\u9664\u4e0d\u5f7b\u5e95\u548c\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0e\u6709\u5bb3\u6982\u5ff5\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u63d0\u793a\u4e0b\u64e6\u9664\u4e0d\u5f7b\u5e95\u4e14\u53ef\u80fd\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "ErasePro\u5f15\u5165\u96f6\u6b8b\u5dee\u7ea6\u675f\u786e\u4fdd\u76ee\u6807\u6982\u5ff5\u4e0e\u951a\u6982\u5ff5\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u5c42\u66f4\u65b0\u7b56\u7565\u4ece\u6d45\u5c42\u5230\u6df1\u5c42\u9010\u6b65\u66f4\u65b0\u53c2\u6570\u3002", "result": "\u5728\u4e0d\u540c\u6982\u5ff5\u64e6\u9664\u4efb\u52a1\uff08\u5982\u5b9e\u4f8b\u3001\u827a\u672f\u98ce\u683c\u548c\u88f8\u9732\u64e6\u9664\uff09\u4e2d\uff0cErasePro\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u64e6\u9664\u5b8c\u6574\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "ErasePro\u901a\u8fc7\u4f18\u5316\u76ee\u6807\u8bbe\u8ba1\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u64e6\u9664\u7684\u6548\u679c\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.04546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04546", "abs": "https://arxiv.org/abs/2508.04546", "authors": ["Minghang Zheng", "Yuxin Peng", "Benyuan Sun", "Yi Yang", "Yang Liu"], "title": "Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding", "comment": "Accepted by ICCV 2025", "summary": "In this paper, we tackle the task of online video temporal grounding (OnVTG),\nwhich requires the model to locate events related to a given text query within\na video stream. Unlike regular video temporal grounding, OnVTG requires the\nmodel to make predictions without observing future frames. As online videos are\nstreaming inputs and can go on indefinitely, it is impractical and inefficient\nto store all historical inputs. The existing OnVTG models employ memory to\nstore recent historical video frame features and predict scores indicating\nwhether the current frame corresponds to the start or end time of the target\nevent. However, these methods lack effective event modeling and cannot retain\nlong-term historical information, leading to low performance. To tackle these\nchallenges, we propose a hierarchical event memory for OnVTG. We propose an\nevent-based OnVTG framework that makes predictions based on event proposals\nthat model event-level information with various durations. To preserve\nhistorically valuable event information, we introduce a hierarchical event\nmemory that retains historical events, allowing the model to access both recent\nand long-term information. To enable the real-time prediction, we further\npropose a future prediction branch that predicts whether the target event will\noccur shortly and further regresses the start time of the event. We achieve\nstate-of-the-art performance on the TACoS, ActivityNet Captions, and MAD\ndatasets. Code is available at https://github.com/minghangz/OnVTG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4e8b\u4ef6\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\uff08OnVTG\uff09\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5efa\u6a21\u548c\u957f\u671f\u5386\u53f2\u4fe1\u606f\u4fdd\u7559\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\uff08OnVTG\uff09\u9700\u8981\u6a21\u578b\u5728\u4e0d\u89c2\u5bdf\u672a\u6765\u5e27\u7684\u60c5\u51b5\u4e0b\u5b9a\u4f4d\u4e8b\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u4e8b\u4ef6\u5efa\u6a21\u548c\u957f\u671f\u5386\u53f2\u4fe1\u606f\u4fdd\u7559\uff0c\u5bfc\u81f4\u6027\u80fd\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5206\u5c42\u4e8b\u4ef6\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u63d0\u6848\u5efa\u6a21\u4e8b\u4ef6\u7ea7\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u5206\u5c42\u4e8b\u4ef6\u8bb0\u5fc6\u4fdd\u7559\u5386\u53f2\u4e8b\u4ef6\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u672a\u6765\u9884\u6d4b\u5206\u652f\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u9884\u6d4b\u3002", "result": "\u5728TACoS\u3001ActivityNet Captions\u548cMAD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5206\u5c42\u4e8b\u4ef6\u8bb0\u5fc6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86OnVTG\u4efb\u52a1\u4e2d\u7684\u4e8b\u4ef6\u5efa\u6a21\u548c\u957f\u671f\u4fe1\u606f\u4fdd\u7559\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.04564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04564", "abs": "https://arxiv.org/abs/2508.04564", "authors": ["Gabriele Magrini", "Lorenzo Berlincioni", "Luca Cultrera", "Federico Becattini", "Pietro Pala"], "title": "Drone Detection with Event Cameras", "comment": null, "summary": "The diffusion of drones presents significant security and safety challenges.\nTraditional surveillance systems, particularly conventional frame-based\ncameras, struggle to reliably detect these targets due to their small size,\nhigh agility, and the resulting motion blur and poor performance in challenging\nlighting conditions. This paper surveys the emerging field of event-based\nvision as a robust solution to these problems. Event cameras virtually\neliminate motion blur and enable consistent detection in extreme lighting.\nTheir sparse, asynchronous output suppresses static backgrounds, enabling\nlow-latency focus on motion cues. We review the state-of-the-art in event-based\ndrone detection, from data representation methods to advanced processing\npipelines using spiking neural networks. The discussion extends beyond simple\ndetection to cover more sophisticated tasks such as real-time tracking,\ntrajectory forecasting, and unique identification through propeller signature\nanalysis. By examining current methodologies, available datasets, and the\ndistinct advantages of the technology, this work demonstrates that event-based\nvision provides a powerful foundation for the next generation of reliable,\nlow-latency, and efficient counter-UAV systems.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5728\u65e0\u4eba\u673a\u68c0\u6d4b\u4e2d\u89e3\u51b3\u4e86\u4f20\u7edf\u76f8\u673a\u7684\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u76f8\u673a\u5728\u68c0\u6d4b\u5c0f\u578b\u3001\u9ad8\u673a\u52a8\u6027\u65e0\u4eba\u673a\u65f6\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u72ec\u7279\u4f18\u52bf\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7efc\u8ff0\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u65e0\u4eba\u673a\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6570\u636e\u8868\u793a\u65b9\u6cd5\u3001\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u53ca\u5b9e\u65f6\u8ddf\u8e2a\u3001\u8f68\u8ff9\u9884\u6d4b\u7b49\u9ad8\u7ea7\u4efb\u52a1\u3002", "result": "\u4e8b\u4ef6\u76f8\u673a\u80fd\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u4f4e\u5ef6\u8fdf\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002", "conclusion": "\u4e8b\u4ef6\u89c6\u89c9\u6280\u672f\u4e3a\u4e0b\u4e00\u4ee3\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u57fa\u7840\uff0c\u5177\u5907\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.04566", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.04566", "abs": "https://arxiv.org/abs/2508.04566", "authors": ["Jinxing Zhou", "Ziheng Zhou", "Yanghao Zhou", "Yuxin Mao", "Zhangling Duan", "Dan Guo"], "title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization", "comment": null, "summary": "The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally\nlocalize events in untrimmed videos that occur simultaneously in both the audio\nand visual modalities. This paper explores DAVEL under a new and more\nchallenging weakly-supervised setting (W-DAVEL task), where only video-level\nevent labels are provided and the temporal boundaries of each event are\nunknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors},\nwhich are defined as reliable timestamps that are well predicted under weak\nsupervision and exhibit highly consistent event semantics across audio and\nvisual modalities. Specifically, we propose a \\textit{Mutual Event Agreement\nEvaluation} module, which generates an agreement score by measuring the\ndiscrepancy between the predicted audio and visual event classes. Then, the\nagreement score is utilized in a \\textit{Cross-modal Salient Anchor\nIdentification} module, which identifies the audio and visual anchor features\nthrough global-video and local temporal window identification mechanisms. The\nanchor features after multimodal integration are fed into an\n\\textit{Anchor-based Temporal Propagation} module to enhance event semantic\nencoding in the original temporal audio and visual features, facilitating\nbetter temporal localization under weak supervision. We establish benchmarks\nfor W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u4e0b\u7684\u5bc6\u96c6\u89c6\u542c\u4e8b\u4ef6\u5b9a\u4f4d\u65b9\u6cd5\uff08W-DAVEL\uff09\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u663e\u8457\u951a\u70b9\u548c\u4e8b\u4ef6\u4e00\u81f4\u6027\u8bc4\u4f30\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u5728\u5f31\u76d1\u7763\u6761\u4ef6\u4e0b\uff08\u4ec5\u6709\u89c6\u9891\u7ea7\u4e8b\u4ef6\u6807\u7b7e\uff09\uff0c\u5982\u4f55\u6709\u6548\u5b9a\u4f4d\u89c6\u542c\u4e8b\u4ef6\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u663e\u8457\u951a\u70b9\u6982\u5ff5\uff0c\u7ed3\u5408\u4e8b\u4ef6\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u951a\u70b9\u4f20\u64ad\u6a21\u5757\uff0c\u589e\u5f3a\u4e8b\u4ef6\u8bed\u4e49\u7f16\u7801\u3002", "result": "\u5728UnAV-100\u548cActivityNet1.3\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u542c\u4e8b\u4ef6\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
