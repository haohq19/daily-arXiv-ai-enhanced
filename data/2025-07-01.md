<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 14]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: 通过低成本负载传感器预测患者离床意图，提出ViFusionTST模型，融合多模态图像数据，在真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医院和长期护理机构中床旁跌倒问题，现有警报系统反应滞后。

Method: 使用四个负载传感器生成信号，转换为多模态图像，提出双流Swin Transformer模型ViFusionTST进行融合分类。

Result: 在真实数据集上达到0.885准确率和0.794 F1分数，优于现有基线。

Conclusion: 基于图像融合的负载信号分类是实时、隐私保护的跌倒预防有效方案。

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [2] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种解决多模态大语言模型（MLLMs）在手术室风险检测中视觉-语义知识冲突（VS-KC）的方法，通过生成合成图像数据集OR-VSKC，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 手术风险识别对患者安全至关重要，但现有MLLMs在视觉安全违规检测中存在不足，需要解决视觉与语义知识冲突问题。

Method: 利用扩散模型生成34,000多张合成图像，模拟手术室违规场景，并辅以214张人工标注图像作为验证基准。通过微调MLLMs，研究其检测能力。

Result: 微调后的MLLMs在训练过的冲突实体检测上表现显著提升，但对未训练实体类型效果不佳，显示学习特异性。

Conclusion: OR-VSKC数据集和方法为研究VS-KC提供了资源，但需更全面的训练以提升模型泛化能力。

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [3] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: VSRM是一种基于Mamba的视频超分辨率框架，通过空间-时间和时间-空间Mamba块提取长范围时空特征，并引入可变形交叉Mamba对齐模块和频率损失函数，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer方法在视频超分辨率中存在局部感受野限制和二次复杂度问题，而Mamba因其长序列建模能力和线性复杂度成为潜在解决方案。

Method: 提出VSRM框架，结合空间-时间和时间-空间Mamba块提取特征，使用可变形交叉Mamba对齐模块动态对齐帧，并引入频率损失函数优化高频内容。

Result: VSRM在多个基准测试中达到最先进水平。

Conclusion: VSRM为视频超分辨率提供了高效且性能优越的解决方案，为未来研究奠定基础。

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [4] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出了一种名为Concept Pinpoint Eraser (CPE)的新框架，通过非线性模块选择性擦除目标概念，同时保护其他概念，并增强对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于线性模块的概念擦除方法可能无法有效保护多样化的剩余概念，因此需要一种更高效的方法。

Method: CPE框架通过引入非线性Residual Attention Gates (ResAGs)和注意力锚定损失，选择性擦除目标概念，并通过对抗训练增强鲁棒性。

Result: 实验表明，CPE在擦除名人、艺术风格和不当内容方面优于现有方法，同时保持剩余概念的多样性。

Conclusion: CPE提供了一种高效且鲁棒的概念擦除方法，适用于多种应用场景。

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [5] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/abs/2506.23157)
*Hanyu Zhou,Haonan Wang,Haoyue Liu,Yuxing Duan,Luxin Yan,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种时空解耦的高斯溅射框架，结合事件相机和帧相机，用于高动态场景重建，解决了背景与动态对象时空特征不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用统一表示模型（如高斯）直接匹配动态场景的时空特征，但无法处理对象的潜在不连续时间特征和背景与对象的异质空间特征。

Method: 通过事件相机补偿帧相机，提出时空解耦的高斯溅射框架，利用聚类区分背景与对象的时空特征，并利用高斯表示与事件数据的一致性指导对象的时空解耦。

Result: 实验验证了该方法的优越性，能够提高背景与对象的时空区分度，实现时间连续的动态场景渲染。

Conclusion: 该方法通过时空解耦和事件相机辅助，有效解决了高动态场景重建中的时空特征匹配问题。

Abstract: High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between background and objects.
To address this issue, we disentangle the spatiotemporal features into various
latent representations to alleviate the spatiotemporal mismatching between
background and objects. In this work, we introduce event camera to compensate
for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting
framework for high-dynamic scene reconstruction. As for dynamic scene, we
figure out that background and objects have appearance discrepancy in
frame-based spatial features and motion discrepancy in event-based temporal
features, which motivates us to distinguish the spatiotemporal features between
background and objects via clustering. As for dynamic object, we discover that
Gaussian representations and event data share the consistent spatiotemporal
characteristic, which could serve as a prior to guide the spatiotemporal
disentanglement of object Gaussians. Within Gaussian splatting framework, the
cumulative scene-object disentanglement can improve the spatiotemporal
discrimination between background and objects to render the time-continuous
dynamic scene. Extensive experiments have been performed to verify the
superiority of the proposed method.

</details>


### [6] [Trident: Detecting Face Forgeries with Adversarial Triplet Learning](https://arxiv.org/abs/2506.23189)
*Mustafa Hakan Kara,Aysegul Dundar,Uğur Güdükbay*

Main category: cs.CV

TL;DR: 论文提出了一种名为Trident的人脸伪造检测框架，通过三元组学习和Siamese网络架构提高对不同伪造方法的适应性，并结合对抗训练增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络生成的人脸伪造技术日益复杂，检测数字媒体中的人脸伪造变得更具挑战性，维护数字媒体完整性和打击视觉虚假信息的重要性凸显。

Method: Trident框架采用三元组学习和Siamese网络架构，通过对抗训练和梯度控制避免过拟合，以捕捉伪造样本的细微差异。

Result: 在多个基准测试和消融研究中，Trident表现出色，证明了其有效性。

Conclusion: Trident框架通过创新的训练策略和对抗训练，显著提升了人脸伪造检测的泛化能力和鲁棒性。

Abstract: As face forgeries generated by deep neural networks become increasingly
sophisticated, detecting face manipulations in digital media has posed a
significant challenge, underscoring the importance of maintaining digital media
integrity and combating visual disinformation. Current detection models,
predominantly based on supervised training with domain-specific data, often
falter against forgeries generated by unencountered techniques. In response to
this challenge, we introduce \textit{Trident}, a face forgery detection
framework that employs triplet learning with a Siamese network architecture for
enhanced adaptability across diverse forgery methods. \textit{Trident} is
trained on curated triplets to isolate nuanced differences of forgeries,
capturing fine-grained features that distinguish pristine samples from
manipulated ones while controlling for other variables. To further enhance
generalizability, we incorporate domain-adversarial training with a forgery
discriminator. This adversarial component guides our embedding model towards
forgery-agnostic representations, improving its robustness to unseen
manipulations. In addition, we prevent gradient flow from the classifier head
to the embedding model, avoiding overfitting induced by artifacts peculiar to
certain forgeries. Comprehensive evaluations across multiple benchmarks and
ablation studies demonstrate the effectiveness of our framework. We will
release our code in a GitHub repository.

</details>


### [7] [DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding](https://arxiv.org/abs/2506.23196)
*Mona Ahmadian,Amir Shirian,Frank Guerin,Andrew Gilbert*

Main category: cs.CV

TL;DR: DEL框架通过音频和视觉特征对齐及多模态交互细化模块，实现了在长未剪辑视频中高精度检测和分类多个动作，并在多个TAL数据集上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频中的重叠事件和复杂时间依赖关系使得多模态交互建模具有挑战性。

Method: DEL框架包含两个关键模块：利用掩码自注意力增强模态内一致性的音频和视觉特征对齐模块，以及建模跨模态依赖关系的多尺度多模态交互细化模块。

Result: 在UnAV-100、THUMOS14、ActivityNet 1.3和EPIC-Kitchens-100数据集上，DEL的平均mAP分别提升了+3.3%、+2.6%、+1.2%、+1.7%（动词）和+1.4%（名词）。

Conclusion: DEL框架通过多模态特征对齐和交互细化，显著提升了密集语义动作定位的性能。

Abstract: Real-world videos often contain overlapping events and complex temporal
dependencies, making multimodal interaction modeling particularly challenging.
We introduce DEL, a framework for dense semantic action localization, aiming to
accurately detect and classify multiple actions at fine-grained temporal
resolutions in long untrimmed videos. DEL consists of two key modules: the
alignment of audio and visual features that leverage masked self-attention to
enhance intra-mode consistency and a multimodal interaction refinement module
that models cross-modal dependencies across multiple scales, enabling
high-level semantics and fine-grained details. Our method achieves
state-of-the-art performance on multiple real-world Temporal Action
Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and
EPIC-Kitchens-100, surpassing previous approaches with notable average mAP
gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

</details>


### [8] [A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans](https://arxiv.org/abs/2506.23209)
*Chia-Wen Huang,Haw Hwai,Chien-Chang Lee,Pei-Yuan Wu*

Main category: cs.CV

TL;DR: 提出了一种基于3D CT扫描的深度学习模型，结合切片注意力机制和预训练的2D模型，用于阑尾炎的准确分类和复杂阑尾炎的区分，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 阑尾炎的及时准确诊断至关重要，但CT扫描数量增加可能导致放射科医生超负荷，引发延误。

Method: 利用3D CT扫描和切片注意力机制，结合预训练的2D模型进行分层分类。

Result: 阑尾炎分类的AUC提升了3%，复杂阑尾炎分类的AUC提升了5.9%。

Conclusion: 该方法比现有方法更高效可靠，为阑尾炎诊断提供了更好的解决方案。

Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical
settings to prevent serious complications. While CT imaging remains the
standard diagnostic tool, the growing number of cases can overwhelm
radiologists, potentially causing delays. In this paper, we propose a deep
learning model that leverages 3D CT scans for appendicitis classification,
incorporating Slice Attention mechanisms guided by external 2D datasets to
enhance small lesion detection. Additionally, we introduce a hierarchical
classification framework using pre-trained 2D models to differentiate between
simple and complicated appendicitis. Our approach improves AUC by 3% for
appendicitis and 5.9% for complicated appendicitis, offering a more efficient
and reliable diagnostic solution compared to previous work.

</details>


### [9] [PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation](https://arxiv.org/abs/2506.23257)
*Chongke Bi,Xin Gao,Baofeng Fu,Yuheng Zhao,Siming Chen,Ying Zhao,Yunhai Wang*

Main category: cs.CV

TL;DR: PCLVis框架帮助用户分析超级计算机模拟中的进程通信延迟（PCL）事件，无需物理链路层信息，通过MPI数据和交互式可视化优化模拟效率。


<details>
  <summary>Details</summary>
Motivation: 大规模超级计算机模拟的通信延迟问题严重，现有方法依赖管理员才能获取的物理链路层信息，普通用户难以分析。

Method: PCLVis利用MPI通信数据，通过进程相关性树聚类、通信依赖DAG分析和CS-Glyph可视化，提供交互式PCL事件分析。

Result: 在TH-1A超级计算机上验证，PCLVis能有效定位和分析PCL事件，显著提升模拟效率。

Conclusion: PCLVis为普通用户提供了一种无需物理链路层信息的PCL分析工具，优化了大规模模拟的通信效率。

Abstract: Large-scale simulations on supercomputers have become important tools for
users. However, their scalability remains a problem due to the huge
communication cost among parallel processes. Most of the existing communication
latency analysis methods rely on the physical link layer information, which is
only available to administrators. In this paper, a framework called PCLVis is
proposed to help general users analyze process communication latency (PCL)
events. Instead of the physical link layer information, the PCLVis uses the MPI
process communication data for the analysis. First, a spatial PCL event
locating method is developed. All processes with high correlation are
classified into a single cluster by constructing a process-correlation tree.
Second, the propagation path of PCL events is analyzed by constructing a
communication-dependency-based directed acyclic graph (DAG), which can help
users interactively explore a PCL event from the temporal evolution of a
located PCL event cluster. In this graph, a sliding window algorithm is
designed to generate the PCL events abstraction. Meanwhile, a new glyph called
the communication state glyph (CS-Glyph) is designed for each process to show
its communication states, including its in/out messages and load balance. Each
leaf node can be further unfolded to view additional information. Third, a PCL
event attribution strategy is formulated to help users optimize their
simulations. The effectiveness of the PCLVis framework is demonstrated by
analyzing the PCL events of several simulations running on the TH-1A
supercomputer. By using the proposed framework, users can greatly improve the
efficiency of their simulations.

</details>


### [10] [Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation](https://arxiv.org/abs/2506.23271)
*Jinxing Zhou,Zhihui Li,Yongqiang Yu,Yanghao Zhou,Ruohao Guo,Guangyao Li,Yuxin Mao,Mingfei Han,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: Mettle是一种高效的内存方法，通过并行蒸馏音频或视觉特征为元标记，适应大规模预训练Transformer模型到下游视听任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在适应大规模预训练模型时内存占用高、训练时间长的问题。

Method: 使用轻量级Layer-Centric Distillation模块并行蒸馏特征为元标记，并引入Meta-Token Injection模块支持细粒度分割任务。

Result: 在多个视听基准测试中显著减少内存使用和训练时间，同时保持参数效率和竞争力准确性。

Conclusion: Mettle是一种高效且内存友好的方法，适用于多种视听任务。

Abstract: We present \textbf{Met}a-\textbf{T}oken \textbf{Le}arning (Mettle), a simple
and memory-efficient method for adapting large-scale pretrained transformer
models to downstream audio-visual tasks. Instead of sequentially modifying the
output feature distribution of the transformer backbone, Mettle utilizes a
lightweight \textit{Layer-Centric Distillation (LCD)} module to distill in
parallel the intact audio or visual features embedded by each transformer layer
into compact meta-tokens. This distillation process considers both pretrained
knowledge preservation and task-specific adaptation. The obtained meta-tokens
can be directly applied to classification tasks, such as audio-visual event
localization and audio-visual video parsing. To further support fine-grained
segmentation tasks, such as audio-visual segmentation, we introduce a
\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual
meta-tokens distilled from the top transformer layer to guide feature
adaptation in earlier layers. Extensive experiments on multiple audiovisual
benchmarks demonstrate that our method significantly reduces memory usage and
training time while maintaining parameter efficiency and competitive accuracy.

</details>


### [11] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于线性复杂度Vision Mamba网络的高效RGB-Event目标跟踪框架Mamba-FETrack V2，通过轻量级Prompt Generator和FEMamba主干实现跨模态特征提取与融合。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态跟踪算法依赖高复杂度Vision Transformer架构，导致计算开销大且跨模态交互效果受限。

Method: 设计轻量级Prompt Generator生成模态特定提示向量，结合Vision Mamba的FEMamba主干实现特征提取与融合。

Result: 在多个RGB-Event跟踪基准测试中表现优异，验证了框架的性能与效率。

Conclusion: Mamba-FETrack V2在减少计算开销的同时提升了跨模态跟踪效果，具有实际应用潜力。

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [12] [Event-based Tiny Object Detection: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2506.23575)
*Nuo Chen,Chao Xiao,Yimian Dai,Shiman He,Miao Li,Wei An*

Main category: cs.CV

TL;DR: 论文提出了首个大规模、多样化的基于事件的小目标检测数据集EV-UAV，并提出了EV-SpSegNet方法和STC损失函数，用于解决反无人机任务中的小目标检测问题。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机在复杂环境中检测小目标（如无人机）存在困难，而事件相机因其高动态范围和微秒级分辨率更适合此类任务。然而，现有事件数据集规模小、目标大且背景单一，无法满足需求。

Method: 提出了EV-UAV数据集，包含147个序列和230万事件级标注。同时，提出了EV-SpSegNet方法，利用时空事件点云中的运动连续性，结合STC损失函数进行目标检测。

Result: 在EV-UAV数据集上的实验证明了方法的优越性，为未来研究提供了基准。

Conclusion: EV-UAV数据集和EV-SpSegNet方法为事件相机在反无人机任务中的小目标检测提供了有效解决方案。

Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to
the small size of UAVs and complex backgrounds. Traditional frame-based cameras
struggle to detect small objects in complex environments due to their low frame
rates, limited dynamic range, and data redundancy. Event cameras, with
microsecond temporal resolution and high dynamic range, provide a more
effective solution for SOD. However, existing event-based object detection
datasets are limited in scale, feature large targets size, and lack diverse
backgrounds, making them unsuitable for SOD benchmarks. In this paper, we
introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),
the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes
147 sequences with over 2.3 million event-level annotations, featuring
extremely small targets (averaging 6.8 $\times$ 5.4 pixels) and diverse
scenarios such as urban clutter and extreme lighting conditions. Furthermore,
based on the observation that small moving targets form continuous curves in
spatiotemporal event point clouds, we propose Event based Sparse Segmentation
Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud
space, along with a Spatiotemporal Correlation (STC) loss that leverages motion
continuity to guide the network in retaining target events. Extensive
experiments on the EV-UAV dataset demonstrate the superiority of our method and
provide a benchmark for future research in EVSOD. The dataset and code are at
https://github.com/ChenYichen9527/Ev-UAV.

</details>


### [13] [Visual and Memory Dual Adapter for Multi-Modal Object Tracking](https://arxiv.org/abs/2506.23972)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一种新颖的视觉和记忆双重适配器（VMDA），通过联合建模频率、空间和通道特征，以及利用记忆机制存储全局时间线索，提升了多模态跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在频率和时间域中未能充分利用关键线索，导致提示学习不可靠。

Method: 设计了视觉适配器和记忆适配器，前者自适应地转移辅助模态的判别线索，后者存储全局时间线索并动态更新。

Result: 在RGB-Thermal、RGB-Depth和RGB-Event等多模态跟踪任务中实现了最先进的性能。

Conclusion: VMDA通过结合视觉和记忆适配器，显著提升了多模态跟踪的鲁棒性和判别性。

Abstract: Prompt-learning-based multi-modal trackers have achieved promising progress
by employing lightweight visual adapters to incorporate auxiliary modality
features into frozen foundation models. However, existing approaches often
struggle to learn reliable prompts due to limited exploitation of critical cues
across frequency and temporal domains. In this paper, we propose a novel visual
and memory dual adapter (VMDA) to construct more robust and discriminative
representations for multi-modal tracking. Specifically, we develop a simple but
effective visual adapter that adaptively transfers discriminative cues from
auxiliary modality to dominant modality by jointly modeling the frequency,
spatial, and channel-wise features. Additionally, we design the memory adapter
inspired by the human memory mechanism, which stores global temporal cues and
performs dynamic update and retrieval operations to ensure the consistent
propagation of reliable temporal information across video sequences. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,
and RGB-Event tracking. Code and models are available at
https://github.com/xuboyue1999/mmtrack.git.

</details>


### [14] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/abs/2506.24063)
*Deng Li,Aming Wu,Yang Li,Yaowei Wang,Yahong Han*

Main category: cs.CV

TL;DR: 论文提出了一种新的持续测试时间适应机制，通过将微调过程转换为特定参数生成，解决了传统方法因少量测试图像导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实环境中数据的分布会随时间空间变化，传统基于封闭集假设的目标检测器难以适应这种变化，导致性能下降。

Method: 设计了双路径LoRA域感知适配器分离特征，提出条件扩散参数生成机制，并引入类中心最优传输对齐方法防止灾难性遗忘。

Result: 在多种连续域自适应目标检测任务中验证了方法的有效性，生成参数提取的表征能捕获更多目标相关信息并增强泛化能力。

Conclusion: 该方法通过参数生成和特征分离，显著提升了目标检测器在动态环境中的适应性和泛化性能。

Abstract: In practice, environments constantly change over time and space, posing
significant challenges for object detectors trained based on a closed-set
assumption, i.e., training and test data share the same distribution. To this
end, continual test-time adaptation has attracted much attention, aiming to
improve detectors' generalization by fine-tuning a few specific parameters,
e.g., BatchNorm layers. However, based on a small number of test images,
fine-tuning certain parameters may affect the representation ability of other
fixed parameters, leading to performance degradation. Instead, we explore a new
mechanism, i.e., converting the fine-tuning process to a specific-parameter
generation. Particularly, we first design a dual-path LoRA-based domain-aware
adapter that disentangles features into domain-invariant and domain-specific
components, enabling efficient adaptation. Additionally, a conditional
diffusion-based parameter generation mechanism is presented to synthesize the
adapter's parameters based on the current environment, preventing the
optimization from getting stuck in local optima. Finally, we propose a
class-centered optimal transport alignment method to mitigate catastrophic
forgetting. Extensive experiments conducted on various continuous domain
adaptive object detection tasks demonstrate the effectiveness. Meanwhile,
visualization results show that the representation extracted by the generated
parameters can capture more object-related information and strengthen the
generalization ability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Main category: cs.LG

TL;DR: 研究提出了一种基于大语言模型和主动注意力网络的PASC临床风险预测方法，旨在提高预测准确性并减少标注需求。


<details>
  <summary>Details</summary>
Motivation: PASC的长期影响对全球医疗系统构成挑战，传统模型难以捕捉其复杂进展，需更精准的风险预测和事件识别方法。

Method: 使用Llama-3.1-70B-Instruct生成文本时间序列特征，结合临床专家标注，提出主动注意力网络预测风险和进展事件。

Result: 方法整合人类专业知识和主动学习，提升风险预测准确性并减少标注量。

Conclusion: 研究目标是通过改进风险预测和事件识别，优化SARS-CoV-2患者的护理和决策。

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [16] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Main category: cs.LG

TL;DR: 提出了一种新型框架，通过引入可训练的量子卷积层和解决相关挑战，提升了量子卷积神经网络（QuNNs）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统量子卷积层虽有益于特征提取，但静态性限制了其适应性。研究旨在通过可训练层提升灵活性和潜力。

Method: 提出残差量子卷积神经网络（ResQuNNs），利用残差学习概念，通过跳跃连接增强梯度流动。

Result: 实验表明，残差块的策略性放置显著提升了训练性能，梯度流动更高效。

Conclusion: 研究为量子深度学习的发展迈出重要一步，为理论和实际应用开辟了新途径。

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [17] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer,Adewale Akinfaderin*

Main category: cs.LG

TL;DR: 提出了一种基于用户行为序列的Transformer方法（UBS-Transformer），用于检测内部威胁，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测因恶意行为者的授权身份和异常行为的隐蔽性而具有挑战性，现有方法未能充分利用用户行为的时序依赖性。

Method: 将CERT数据集转化为时序序列，使用Transformer Encoder建模正常行为，并通过重建误差作为异常分数，结合三种无监督异常检测算法进行评估。

Result: 在四个测试集上表现优异：准确率96.61%，召回率99.43%，F1分数96.38%，AUROC 95.00%，假阴性和假阳性率极低。

Conclusion: UBS-Transformer证明了时序建模和高级异常检测在内部威胁领域的有效性，显著优于传统方法。

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [18] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao,Jiahui Lu,Xovee Xu,Fan Zhou,Tianshu Xie,Wei Lu,Lifeng Xu*

Main category: cs.LG

TL;DR: TripleAD是一个基于互蒸馏的三通道图异常检测框架，通过三个模块分别检测属性、结构和混合异常，解决了现有方法在两类异常检测中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在同时检测属性和结构异常时存在性能问题，TripleAD旨在通过多通道协作解决这一问题。

Method: TripleAD包含三个模块：多尺度属性估计模块、链接增强结构估计模块和属性混合曲率模块，并通过互蒸馏策略促进模块间协作。

Result: 实验表明TripleAD在检测各类异常时优于现有基线方法。

Conclusion: TripleAD通过多通道协作和互蒸馏策略，显著提升了图异常检测的性能。

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [19] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park,Won-Jeong Lee,Seong Tae Kim,Gyeong-Moon Park*

Main category: cs.LG

TL;DR: 论文提出了一种名为A2P的新框架，用于预测未来异常事件，包括异常感知预测（AAF）和合成异常提示（SAP），并在多个真实数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预测未来异常时间点（Anomaly Prediction, AP）方面表现不足，无法提供精确预测或仅关注即时异常。

Method: A2P框架结合AAF和SAP，通过异常关系学习和可学习的异常提示池（APP）模拟多样化异常模式。

Result: 在多个真实数据集上，A2P优于现有方法，能够有效预测未来异常。

Conclusion: A2P为解决未来异常预测问题提供了一种有效且创新的解决方案。

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [20] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 论文分析了扩散模型和图像自回归模型中水印的放射性问题，提出了一种针对图像自回归模型的水印方法，确保水印在训练后仍可识别。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在扩散模型中无法保持放射性，且图像自回归模型缺乏相关水印技术，需要解决这一问题以防止生成图像的未经授权使用。

Method: 提出了一种针对图像自回归模型的水印方法，借鉴了大型语言模型的技术，确保水印在训练后仍具有放射性。

Result: 实验证明该方法在图像自回归模型中能有效保留水印的放射性，支持强大的来源追踪。

Conclusion: 该方法填补了图像自回归模型水印技术的空白，为生成图像的版权保护提供了有效工具。

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [21] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 提出了一种基于随机过程模型的轨迹聚类方法，利用熵相关性度量优化聚类，提高模型可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 高变异性导致过程模型复杂难懂，现有聚类方法忽略活动频率和转移概率，无法捕捉真实执行动态。

Method: 使用基于直接跟随概率的熵相关性度量，优化随机过程模型，指导轨迹分配。

Result: 在公开数据集上表现优于现有方法，揭示了考虑随机性时聚类性能排名的变化。

Conclusion: 该方法计算高效，提升模型可解释性，并能更准确地表示过程行为。

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [22] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

Main category: cs.LG

TL;DR: 提出了一种基于静态阵型模板和成本最小化的足球阵型识别方法EFPI，通过线性分配优化球员位置匹配，并支持时间段的扩展分析。


<details>
  <summary>Details</summary>
Motivation: 足球战术分析需要准确识别球队阵型和球员位置，传统方法缺乏灵活性。

Method: 使用预定义静态阵型模板，通过线性总和分配优化球员位置匹配，最小化实际位置与模板位置的距离，并引入稳定性参数减少不必要的阵型变化。

Result: EFPI方法能有效识别阵型，适用于单帧或更长时间段的分析，并已开源。

Conclusion: EFPI为足球战术分析提供了一种灵活且准确的阵型识别工具。

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [23] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez,Fernando García-Torres,Jesper John Lisegaard,Rocío del Amor,Sankhya Mohanty,Valery Naranjo*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习（RL）的策略，用于控制树脂注入过程中的流动动态，以提高复合材料制造的均匀性和质量。


<details>
  <summary>Details</summary>
Motivation: 树脂注入（RI）和树脂传递模塑（RTM）是制造高性能纤维增强复合材料的关键工艺，但树脂流动动态的控制对确保纤维均匀浸渍至关重要，以避免孔隙和干斑影响结构完整性。

Method: 采用近端策略优化（PPO）的强化学习方法，通过过程模拟建立策略，以同步两个树脂入口和单个出口的流动前沿。

Result: 结果表明，RL方法能有效实现流动收敛，提高工艺控制和产品质量。

Conclusion: 强化学习方法在复合材料制造中具有改善工艺控制和产品质量的潜力。

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: 论文提出了一种基于统计决策理论的框架，强调解释性机器学习应针对具体用途设计和评估，并通过实际用例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 现有解释性机器学习方法未充分考虑解释的实际用途，导致解释可能被误用或效果不佳。

Method: 采用统计决策理论框架，将解释设计与具体任务目标结合，并通过理想化决策者模型量化解释的潜在性能提升。

Result: 展示了该框架在临床决策支持、提供补救措施和调试等多样化用例中的应用，并量化了解释的最大性能提升。

Conclusion: 解释性机器学习的评估应结合理论和实际用途，明确具体用例以避免歧义，并提出跨视角的定义。

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [25] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: 本文提出了针对通用人工智能（GPAI）和基础模型的风险管理实践，旨在帮助开发者识别、分析和减轻相关风险。


<details>
  <summary>Details</summary>
Motivation: 随着多用途AI模型的普及，其带来的潜在风险日益显著，需要专门的风险管理措施。

Method: 文档基于NIST AI风险管理框架和ISO/IEC 23894标准，提供了针对GPAI/基础模型的独特风险管理实践。

Result: 为开发者提供了具体的风险控制指南，有助于降低GPAI/基础模型可能带来的负面影响。

Conclusion: 本文为GPAI/基础模型的开发者提供了实用的风险管理工具，有助于推动AI技术的安全发展。

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [26] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: 论文提出PromptAug，一种基于LLM的数据增强方法，用于解决冲突检测任务中高质量标注数据稀缺的问题，并在准确率和F1分数上提升了2%。


<details>
  <summary>Details</summary>
Motivation: 社交媒体冲突增多，但高质量标注数据稀缺且难以获取，同时平台限制研究数据访问，数据增强成为替代方案。

Method: 提出PromptAug方法，利用LLM生成增强数据，并通过极端数据稀缺场景、多样性分析和主题分析进行评估。

Result: PromptAug在冲突和情感数据集上显著提升了2%的准确率和F1分数。

Conclusion: PromptAug是一种有效的数据增强方法，适用于敏感任务如冲突检测，结合了自然语言处理和社会科学方法。

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [27] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: 论文提出了一种名为MDGCL的多域预训练和跨域迁移框架，用于解决图数据中不同域间语义和属性差异的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 图数据在不同域间存在巨大语义和属性差异，传统对比预训练策略无法有效吸收多域知识，因此需要一种新方法。

Method: 设计了能识别和捕获域差异的对比学习策略，引入域令牌编码全局信息，并在下游任务中使用域注意力机制实现细粒度知识迁移。

Result: 在五个基准数据集上，MDGCL方法显著优于现有技术，最高提升19.33%的准确率和19.13%的Macro-F1分数。

Conclusion: MDGCL框架通过多域预训练和跨域迁移，有效解决了图数据中的域差异问题，显著提升了模型性能。

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [28] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: 提出了一种基于局部事件驱动涌现学习的神经符号生成语言建模框架，核心是分层Hopfield记忆链，用作组合短期记忆和动态分词器。模型从零构建结构，学习符号序列作为多尺度表示，并能从噪声中过滤自然语言模式。


<details>
  <summary>Details</summary>
Motivation: 探索如何从局部神经学习中涌现出符号结构，为构建可扩展、可解释的神经符号系统提供新途径。

Method: 使用分层Hopfield记忆链作为动态分词器，通过投影张量绑定共现特征为分层标记，引入冗余并压缩局部激活为长程依赖。

Result: 模型能生成具有内部形态一致性的合成语言，与人类语言量化相同，并支持长时记忆和组合推理。

Conclusion: 该框架为研究符号结构如何从局部神经学习中涌现提供了方法论基础，推动了生成语言模型的神经形态架构发展。

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [29] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 该研究通过构建基于BERT的集成模型，从临床笔记中检测和分类药物事件，显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 识别健康记录和临床笔记中的关键变量（如药物、疾病、关系）在临床领域有广泛应用。n2c2 2022提供了共享任务，旨在解决电子健康记录（EHR）中的自然语言处理挑战。

Method: 研究采用预训练BERT模型（基于Wikipedia和MIMIC数据），并在CMED训练数据上微调，通过投票策略集成多个预测结果。

Result: 实验结果表明，BERT集成模型将严格Micro-F分数和Macro-F分数分别提高了约5%和6%。

Conclusion: 基于BERT的集成模型能有效提升药物事件分类的性能。

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example](https://arxiv.org/abs/2506.22894)
*Bei Zhou,Baha Zarrouki,Mattia Piccinini,Cheng Hu,Lei Xie,Johannes Betz*

Main category: cs.RO

TL;DR: 提出了一种基于安全强化学习（RL）的运动规划方法，用于自动驾驶漂移，结合模型漂移动力学和预测安全过滤器（PSF），确保安全高效的学习和稳定漂移操作。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在高速漂移时难以应对不稳定性和不可预测性，现有学习方法的探索能力有限且未有效解决安全问题。

Method: 结合RL代理和模型漂移动力学，通过PSF在线调整动作以避免不安全状态。

Result: 在Matlab-Carsim平台上验证，漂移性能显著提升，跟踪误差减少，计算效率优于传统方法。

Conclusion: 该方法有望提升自动驾驶车辆在安全关键场景中的能力。

Abstract: Autonomous drifting is a complex and crucial maneuver for safety-critical
scenarios like slippery roads and emergency collision avoidance, requiring
precise motion planning and control. Traditional motion planning methods often
struggle with the high instability and unpredictability of drifting,
particularly when operating at high speeds. Recent learning-based approaches
have attempted to tackle this issue but often rely on expert knowledge or have
limited exploration capabilities. Additionally, they do not effectively address
safety concerns during learning and deployment. To overcome these limitations,
we propose a novel Safe Reinforcement Learning (RL)-based motion planner for
autonomous drifting. Our approach integrates an RL agent with model-based drift
dynamics to determine desired drift motion states, while incorporating a
Predictive Safety Filter (PSF) that adjusts the agent's actions online to
prevent unsafe states. This ensures safe and efficient learning, and stable
drift operation. We validate the effectiveness of our method through
simulations on a Matlab-Carsim platform, demonstrating significant improvements
in drift performance, reduced tracking errors, and computational efficiency
compared to traditional methods. This strategy promises to extend the
capabilities of autonomous vehicles in safety-critical maneuvers.

</details>


### [31] [Event-based Stereo Visual-Inertial Odometry with Voxel Map](https://arxiv.org/abs/2506.23078)
*Zhaoxing Zhang,Xiaoxiang Wang,Chengliang Zhang,Yangyang Guo,Zikang Yuan,Xin Yang*

Main category: cs.RO

TL;DR: Voxel-ESVIO是一种基于事件相机的立体视觉-惯性里程计系统，通过体素地图管理高效筛选高质量3D点，提升状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高动态范围和高时间分辨率使其成为视觉里程计的重要传感器，但事件流中的噪声影响了高质量地图点的选择，从而影响状态估计精度。

Method: 采用基于体素的点选择和体素感知的点管理策略，优化地图点的选择和更新，高效提取抗噪声的地图点。

Result: 在三个公开基准测试中，Voxel-ESVIO在精度和计算效率上均优于现有方法。

Conclusion: Voxel-ESVIO通过体素地图管理有效解决了事件相机噪声问题，显著提升了视觉-惯性里程计的性能。

Abstract: The event camera, renowned for its high dynamic range and exceptional
temporal resolution, is recognized as an important sensor for visual odometry.
However, the inherent noise in event streams complicates the selection of
high-quality map points, which critically determine the precision of state
estimation. To address this challenge, we propose Voxel-ESVIO, an event-based
stereo visual-inertial odometry system that utilizes voxel map management,
which efficiently filter out high-quality 3D points. Specifically, our
methodology utilizes voxel-based point selection and voxel-aware point
management to collectively optimize the selection and updating of map points on
a per-voxel basis. These synergistic strategies enable the efficient retrieval
of noise-resilient map points with the highest observation likelihood in
current frames, thereby ensureing the state estimation accuracy. Extensive
evaluations on three public benchmarks demonstrate that our Voxel-ESVIO
outperforms state-of-the-art methods in both accuracy and computational
efficiency.

</details>


### [32] [Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models](https://arxiv.org/abs/2506.23164)
*Maarten Hugenholtz,Anna Meszaros,Jens Kober,Zlatan Ajanovic*

Main category: cs.RO

TL;DR: 提出了一种评估多模态预测模型模式崩溃的新框架，重点关注安全关键交互，并引入新指标。


<details>
  <summary>Details</summary>
Motivation: 现有模型可能因模式崩溃仅预测最可能模式，忽视交互多样性，且传统评估指标未定量评估交互或模式崩溃。

Method: 提出评估框架，引入模式崩溃、模式正确性和覆盖率的指标，测试四种多智能体轨迹预测模型。

Result: 模式崩溃确实存在，尽管预测准确性在接近交互事件时提高，但模型仍可能无法预测正确的交互模式。

Conclusion: 该框架有助于开发更一致、准确的预测模型，提升自动驾驶安全性。

Abstract: Autonomous Vehicle decisions rely on multimodal prediction models that
account for multiple route options and the inherent uncertainty in human
behavior. However, models can suffer from mode collapse, where only the most
likely mode is predicted, posing significant safety risks. While existing
methods employ various strategies to generate diverse predictions, they often
overlook the diversity in interaction modes among agents. Additionally,
traditional metrics for evaluating prediction models are dataset-dependent and
do not evaluate inter-agent interactions quantitatively. To our knowledge, none
of the existing metrics explicitly evaluates mode collapse. In this paper, we
propose a novel evaluation framework that assesses mode collapse in joint
trajectory predictions, focusing on safety-critical interactions. We introduce
metrics for mode collapse, mode correctness, and coverage, emphasizing the
sequential dimension of predictions. By testing four multi-agent trajectory
prediction models, we demonstrate that mode collapse indeed happens. When
looking at the sequential dimension, although prediction accuracy improves
closer to interaction events, there are still cases where the models are unable
to predict the correct interaction mode, even just before the interaction mode
becomes inevitable. We hope that our framework can help researchers gain new
insights and advance the development of more consistent and accurate prediction
models, thus enhancing the safety of autonomous driving systems.

</details>
