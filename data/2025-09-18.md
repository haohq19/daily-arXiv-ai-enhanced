<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 事件相机处理新方法，直接处理原始事件流，采用Mamba模型实现线性复杂度的时间建模，通过变速控制器动态调整处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机方法需要将事件流转换为中间表示（如帧、立方体网格等），引入了时间窗口延迟；点级检测方法计算成本高无法实时。

Method: 提出Variable-Rate Spatial Event Mamba架构：1）轻量级因果空间邻域编码器捕捉局部几何关系；2）Mamba基于状态空间模型进行可扩展的时间建模；3）推理时通过控制器根据事件率动态调整处理速度。

Result: 方法能够直接处理原始事件流，避免了中间表示转换带来的窗口延迟，同时保持了线性计算复杂度。

Conclusion: 该方法充分利用事件相机的微秒级时间分辨率优势，通过变速处理机制实现了延迟与计算效率的最佳平衡，适用于高速视觉任务。

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [2] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss是一种基于高斯泼溅辐射光栅化的CL重建算法，通过专用探测器到世界变换模型和初始化策略，在稀疏视图条件下实现高质量重建


<details>
  <summary>Details</summary>
Motivation: 传统CT在平板结构检测中存在几何限制，而稀疏视图条件下的CL重建质量仍然具有挑战性

Method: 结合高斯泼溅辐射光栅化和包含层析倾斜角的专用探测器到世界变换模型，采用初始化策略过滤常见伪影

Result: 在合成和真实数据集上表现优异，仅需3%的全视图即可达到优于在全数据集上优化的迭代方法的性能

Conclusion: LamiGauss能够直接从稀疏投影中有效优化，在有限数据条件下实现准确高效的重建

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [3] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: EvHand-FPV是一个轻量级单事件相机第一人称视角3D手部追踪框架，通过手腕ROI定位、端到端映射和多任务学习策略，在保持高精度的同时大幅降低计算量和参数数量，适合XR设备应用。


<details>
  <summary>Details</summary>
Motivation: 传统帧式手部追踪方法在精度、延迟和能耗方面难以满足XR设备的需求，而事件相机具有微秒级时间分辨率和毫瓦级功耗的优势，但缺乏第一人称视角的基准数据集。

Method: 构建合成训练数据与真实评估数据相结合的事件数据集；引入基于手腕的ROI定位和端到端映射策略减少计算；采用多任务学习辅助几何特征头提升表征能力。

Result: 在真实测试集上2D-AUCp从0.77提升到0.85，参数减少89%（11.2M→1.2M），计算量减少89%（1.648G→0.185G FLOPs），合成数据上保持0.84的3D-AUCp。

Conclusion: EvHand-FPV实现了准确高效的自我中心事件手部追踪，适合设备端XR应用，解决了资源受限环境下的性能需求。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [4] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出EMA引导的伪监督框架和类感知跨模态一致性损失，在弱监督音频-视觉视频解析任务上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决现有方法忽视稳定片段级监督和类感知跨模态对齐的问题

Method: 1) EMA引导的伪监督框架通过自适应阈值或top-k选择生成可靠片段级掩码；2) 类感知跨模态一致性损失在可靠片段-类别对上对齐音频和视觉嵌入

Result: 在LLP和UnAV-100数据集上实现了最先进的性能

Conclusion: 所提出的方法通过提供稳定片段级监督和跨模态对齐，显著提升了弱监督音频-视觉视频解析的性能

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [5] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025挑战赛综述，聚焦多模态推理，发布两个定制数据集Lens和AdsQA，评估40+基线模型，设立三个竞赛赛道，吸引76个团队参与。


<details>
  <summary>Details</summary>
Motivation: 整合多模态机器学习和LLM的不同方法，通过大型基准测试让研究者跟上这一快速发展领域的最新进展，并专注于现实世界和专业化场景以拓宽MLLM的多模态推理应用。

Method: 发布两个定制数据集（Lens支持12个日常场景的通用推理，AdsQA支持广告视频的领域特定推理），评估40+基线模型（包括通用MLLM和任务特定模型），设立三个竞赛赛道（VG-RS、VQA-SA、VR-Ads）。

Result: 76个来自知名学术和工业机构的团队注册，1200+提交中40+有效提交被纳入排名列表。数据集、代码集和排名已公开。

Conclusion: MARS2 2025挑战赛成功推动了多模态推理研究，提供了有价值的基准测试资源，并为该领域的未来发展奠定了基础。

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 基于语义嵌入和序列感知神经网络的ALS故障自动分析框架，通过实时事件日志处理实现异常检测


<details>
  <summary>Details</summary>
Motivation: 为先进光源(ALS)控制系统开发自动化故障分析方法，帮助操作员快速识别导致复杂系统故障的关键事件序列

Method: 将EPICS控制系统的事件日志作为自然语言处理，使用语义嵌入技术转换为上下文向量表示，用序列感知神经网络对正常操作数据训练并实时计算异常分数

Result: 能够标记与基线行为的偏差，实现对系统故障前关键事件序列的快速识别

Conclusion: 该方法为大型科学设施的控制系统提供了有效的实时故障分析和异常检测解决方案

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 该位置文章主张采用反脏脉视角来实现AI安全性，让系统在应对稀罕和分布外事件时能力持续增长，而非依赖静态测试。


<details>
  <summary>Details</summary>
Motivation: 现有的静态测试和单次粗糖性测试忽视了环境演化和模型漏洞的实际问题，如奖励窃取、过度优化等。需要一种更适应长期可靠性的方法。

Method: 推荐反脏脉方法，利用不确定性来预防未来更大的不可预测风险，重新调整AI安全性的测量、标准化和持续改进方法。

Result: 提出了建立反脏脉AI安全社区的伦理和实践指南，补充现有粗糖性方法的不足。

Conclusion: 反脏脉方法对于开放式机器学习系统的长期可靠性至关重要，是实现AI安全性持续改进的关键。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 提出了第一个中文文献语法纠错持续学习基准CL²GEC，包含10个学科的10,000句标注数据，评估LLM在跨学科语法纠错中的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏多学科学术写作的专门基准，忽视了持续学习在处理领域特定语言变异和防止灾难性遗忘方面的潜力。

Method: 构建包含10个学科10,000句标注数据的基准，评估大语言模型在顺序调优、参数高效适应和四种代表性持续学习算法下的表现。

Result: 实验表明基于正则化的方法比基于重放或简单顺序方法更能有效缓解遗忘问题。

Conclusion: 该基准为跨学科学术领域的自适应语法纠错研究提供了严谨的基础。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [9] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: VEGA是一个基于强化学习的电动汽车充电感知导航系统，使用物理信息神经网络和PPO算法优化路径规划和充电策略，无需额外传感器即可实现个性化能效优化。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆需求增长和电动汽车计算能力提升，需要开发能够根据车辆当前状态和环境进行充电感知路径优化的车载AI系统。

Method: 采用两模块设计：1)物理信息神经网络算子(PINO)从车速日志学习车辆定制化动力学参数；2)强化学习代理使用PPO算法在充电站标注的道路图上优化路径和充电策略。

Result: 在长距离路线(如旧金山到纽约)上，VEGA的停车点、停留时间、电量管理和总旅行时间与特斯拉行程规划器接近但更保守，且在法国和日本也表现出良好的泛化能力。

Conclusion: VEGA成功实现了物理信息学习与强化学习的实用集成，为电动汽车生态路由提供了有效的解决方案，可作为虚拟传感器降低EV成本。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [10] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: DREAM是一个基于视觉语言模型的自主框架，用于长期水下探索和栖息地监测，在寻找和探索目标物体（如牡蛎、沉船）方面表现出高效性，比基线方法节省时间且覆盖更广。


<details>
  <summary>Details</summary>
Motivation: 海洋变暖和酸化增加了温度敏感贝类（如牡蛎）大规模死亡事件的风险，需要开发长期监测系统。人类水下作业成本高且危险，因此需要机器人解决方案来实现实时、环境感知的自主决策。

Method: 提出了DREAM框架，这是一个基于视觉语言模型（VLM）的自主系统，用于长期水下探索和栖息地监测，不需要先验位置信息。

Result: 在牡蛎监测任务中，比基线方法节省31.5%时间；相比原始VLM，步骤减少23%且牡蛎覆盖增加8.88%。在沉船场景中，实现100%覆盖且无碰撞，步骤减少27.5%，而原始VLM平均覆盖率为60.23%。

Conclusion: DREAM框架为水下长期监测提供了一种高效、自主的解决方案，在目标探索和覆盖方面显著优于现有方法，具有重要的实际应用价值。

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [11] [Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models](https://arxiv.org/abs/2509.13839)
*Motonari Kambara,Komei Sugiura*

Main category: cs.RO

TL;DR: 提出了一种预测开放词汇对象操作任务未来成功率的模型，通过多级轨迹融合模块分析预操作图像、规划轨迹和语言指令的对齐关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在操作完成后判断成功与否，难以预防潜在危险且依赖失败触发重规划，降低了操作序列的效率。

Method: 使用多级轨迹融合模块，结合先进深度状态空间模型和transformer编码器并行捕获末端执行器轨迹中的多级时间序列自相关性。

Result: 实验结果表明，所提方法优于包括基础模型在内的现有方法。

Conclusion: 该方法能够有效预测操作任务的成功率，提高操作安全性和效率。

Abstract: In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.

</details>
