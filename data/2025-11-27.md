<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2511.20795)
*Souradeep Dutta,Keshav Bulia,Neena S Nair*

Main category: cs.CV

TL;DR: 本文对KRISP模型进行了轻量级复现，参数大幅减少但性能达到原版的75%，揭示了原模型的设计缺陷和现实问题，并在资源受限条件下评估了知识增强VQA架构的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 原版KRISP模型虽然有效，但需要工业级训练规模、计算需求大且与大型骨干网络紧密耦合。本文旨在重新审视KRISP，提供参数更少的轻量级复现版本。

Method: 通过系统消融研究进行轻量级复现，包括在合成VQA数据上的概念验证和在DAQUAR数据集上的评估。模型采用低参数配置，受外部知识图谱领域约束。

Result: 复现模型性能达到原版的75%，揭示了原模型未充分覆盖的设计缺陷和实际问题。模型能防止AI幻觉，仅在知识图谱领域内生成输出。

Conclusion: 轻量级复现模型参数少，可在智能手机和AR-VR等边缘设备上运行，进一步改善了离线视觉推理能力。

Abstract: Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.

</details>


### [2] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: Δ-NeRF是一个用于增量式NeRF优化的模块化残差框架，通过残差控制器、不确定性感知门控机制和视图选择策略，实现在不访问过去数据的情况下优化NeRF模型，同时通过知识蒸馏压缩模型大小。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF框架在引入新视图时需要完全重新训练，限制了在数据顺序到达场景（如卫星地形分析）中的应用。增量式NeRF优化研究不足，简单方法存在灾难性遗忘问题。

Method: 提出Δ-NeRF框架：1）残差控制器向冻结的基础NeRF注入逐层修正；2）不确定性感知门控机制自适应结合基础和优化预测；3）视图选择策略减少训练数据；4）知识蒸馏压缩模型。

Result: 在卫星图像上的实验表明，Δ-NeRF性能与联合训练相当，训练时间减少30-42%，PSNR比简单微调提升43.5%，在某些指标上超过联合训练。

Conclusion: Δ-NeRF有效解决了NeRF增量优化问题，在不访问历史数据的情况下实现高性能，显著减少训练时间和数据需求，同时保持模型紧凑性。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [3] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出Null-Text Test-Time Alignment (Null-TTA)方法，通过优化分类器自由引导中的无条件嵌入来对齐扩散模型，避免奖励破解问题，在语义空间实现有效的测试时对齐。


<details>
  <summary>Details</summary>
Motivation: 现有测试时对齐方法容易导致奖励欠优化或过优化（奖励破解），需要一种能有效对齐目标奖励同时保持跨奖励泛化能力的方法。

Method: 优化分类器自由引导中的无条件嵌入而非潜在变量或噪声变量，利用文本嵌入空间的结构化语义特性，在语义相干流形上进行对齐。

Result: Null-TTA实现了最先进的目标测试时对齐性能，同时保持强大的跨奖励泛化能力。

Conclusion: 语义空间优化是测试时对齐的有效且原则性的新范式。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [4] [Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI](https://arxiv.org/abs/2511.20983)
*Al Amin,Kamrul Hasan,Liang Hong,Sharif Ullah*

Main category: cs.CV

TL;DR: 提出结合Vision Transformers和同态加密的隐私保护联邦学习框架，用于医疗影像分类，通过加密CLS token实现安全聚合，相比梯度加密减少30倍通信量，防止模型反转攻击。


<details>
  <summary>Details</summary>
Motivation: 医疗机构的协作机器学习能提高诊断准确性，但隐私法规禁止直接共享患者数据。传统联邦学习的模型梯度仍易受重建攻击，可能泄露敏感医疗信息。

Method: 使用Vision Transformers的CLS token作为768维特征表示，采用CKKS同态加密在传输前加密这些token，实现安全聚合。

Result: 梯度易受模型反转攻击（PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741），而CLS保护的HE方法能防止此类攻击，每轮聚合仅需326 KB加密数据传输，在未加密域达到96.12%分类准确率，加密域达到90.02%。

Conclusion: 提出的框架在保护隐私的同时实现了高效的联邦学习，显著减少了通信开销并防止了敏感信息泄露。

Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.

</details>


### [5] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra,Kshitiz Bansal,Dinesh Bharadia*

Main category: cs.CV

TL;DR: RadarFM是一个雷达基础模型，通过结构化空间语言监督学习统一的场景级表示，解决了现有雷达方法碎片化和任务特定化的问题。


<details>
  <summary>Details</summary>
Motivation: 雷达传感器在恶劣天气、光照和远距离条件下提供可靠的感知，但现有雷达方法碎片化且任务特定，阻碍了跨任务迁移。基础模型在视觉和语言理解方面取得进展，但与雷达感知的整合仍待探索。

Method: 提出结构化标题框架在原生雷达坐标中编码车辆分布，以及哈希感知对比学习目标量化连续场景相似性而非二元匹配，实现细粒度空间推理。利用CARLA模拟器生成大规模、标注良好的雷达数据集。

Result: 开发了RadarFM雷达基础模型，能够学习统一的场景级表示，并提出定位感知指标评估空间准确性。

Conclusion: RadarFM通过结构化空间语言监督和对比学习，为雷达感知提供了统一的基础模型框架，支持跨任务迁移和细粒度空间推理。

Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.

</details>


### [6] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen,Chao Xu,Yanjun Cao*

Main category: cs.CV

TL;DR: MatchGS框架通过几何校正的3DGS数据生成和2D-3D表示对齐，实现了零样本图像匹配的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 基于学习的图像匹配需要大规模、多样且几何准确的训练数据，但3DGS存在几何不准确性和深度渲染偏差问题，限制了其在对应关系标注中的应用。

Method: 1) 几何忠实的数据生成管道，精炼3DGS几何以产生精确对应标签；2) 2D-3D表示对齐策略，将3DGS的显式3D知识注入2D匹配器。

Result: 生成的对应关系将极线误差降低达40倍，在公共基准测试中零样本性能提升达17.7%。

Conclusion: 通过适当的几何精炼，3DGS可以作为可扩展、高保真且结构丰富的数据源，为新一代鲁棒零样本图像匹配器铺平道路。

Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.

</details>


### [7] [Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure](https://arxiv.org/abs/2511.21337)
*Munish Rathee,Boris Bačić,Maryam Doborjeh*

Main category: cs.CV

TL;DR: SIFT-SNN框架：一种低延迟神经形态信号处理管道，用于实时检测交通基础设施中的结构异常，结合SIFT空间特征编码和SNN分类，在奥克兰海港大桥数据集上达到92.3%准确率和9.5ms推理时间。


<details>
  <summary>Details</summary>
Motivation: 开发实时、低功耗的边缘部署系统，用于交通基础设施的结构安全监测，克服传统CNN方法在延迟和可解释性方面的限制。

Method: 集成尺度不变特征变换(SIFT)进行空间特征编码，结合延迟驱动的脉冲转换层和泄漏积分点火(LIF)脉冲神经网络(SNN)进行分类。

Result: 在奥克兰海港大桥数据集(6000标记帧)上达到92.3%分类准确率，每帧推理时间9.5ms，脉冲稀疏度8.1%，支持实时低功耗边缘部署。

Conclusion: SIFT-SNN框架在保持空间特征基础的同时，实现了低延迟、高能效的实时结构安全监测，为移动混凝土护栏等交通基础设施提供可推广的解决方案。

Abstract: This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.

</details>


### [8] [SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning](https://arxiv.org/abs/2511.21420)
*Futian Wang,Mengqi Wang,Xiao Wang,Haowen Wang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一种利用SAM基础模型进行遥感变化描述的新方法，通过提取区域级表示和注入感兴趣区域知识，解决了现有方法区域感知弱和时间对齐有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化描述方法通常使用CNNs/Transformers提取视觉表示或结合辅助任务增强结果，但存在区域感知弱和时间对齐有限的问题。

Method: 使用CNN/Transformer提取全局视觉特征，利用SAM基础模型描绘语义和运动级变化区域，构建知识图谱提供感兴趣对象信息，通过交叉注意力融合异构信息，使用Transformer解码器生成自然语言描述。

Result: 在多个广泛使用的基准数据集上实现了最先进的性能。

Conclusion: 该方法通过结合SAM基础模型和知识图谱，有效提升了遥感变化描述的性能，解决了区域感知和时间对齐的挑战。

Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

</details>


### [9] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam,Stefano Fiorini,Manuel Lecha,Theodore Tsesmelis,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: E-M3RF是一个等变多模态3D重组框架，通过结合几何特征和颜色特征，使用SE(3)流匹配来预测碎片重组所需的变换，在几何特征不足或模糊的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的3D重组方法主要依赖几何特征，在几何信息不足（如小碎片、侵蚀碎片或对称碎片）时表现不佳，且缺乏防止重叠组装的物理约束。

Method: 使用旋转等变编码器提取几何特征，结合Transformer提取颜色特征，形成多模态表示，通过SE(3)流匹配预测碎片重组变换。

Result: 在四个数据集上的实验表明，E-M3RF在RePAIR数据集上相比竞争方法，旋转误差降低23.1%，平移误差降低13.2%，Chamfer距离减少18.4%。

Conclusion: E-M3RF通过结合几何和颜色多模态特征，有效解决了传统方法在几何信息不足时的重组问题，显著提升了重组精度。

Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

</details>


### [10] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang,Fan Zhang,Xiao Wang,Mengqi Wang,Dexing Huang,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种基于超图的时空事件流补全机制，通过超图连接不同时间和空间位置的事件标记，利用上下文信息传递来补全稀疏事件，并能灵活融合RGB标记实现多模态信息补全。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生的事件流具有空间稀疏但时间密集的特点，现有方法使用事件帧、体素或张量作为输入，但难以解决空间稀疏导致的欠采样问题。

Method: 使用超图引导的时空事件流补全机制，通过超图连接事件标记并利用上下文信息传递进行补全，可融合RGB标记实现多模态信息补全，然后通过自注意力聚合不同时间步的超图节点信息。

Result: 在单标签和多标签事件分类任务上的大量实验充分验证了所提框架的有效性。

Conclusion: 提出的超图引导事件流补全机制能有效解决事件数据空间稀疏问题，并支持多模态特征融合，在事件分类任务中表现出色。

Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding](https://arxiv.org/abs/2511.20696)
*Dan Li,Hye-Bin Shin,Yeon-Woo Choi*

Main category: cs.LG

TL;DR: 提出ProNECL框架，通过原型引导的非示例持续学习方法解决跨个体EEG解码中的灾难性遗忘问题，无需存储历史EEG数据。


<details>
  <summary>Details</summary>
Motivation: 由于EEG信号的个体间差异显著，在持续EEG解码任务中，引入新个体时会覆盖先前获得的知识。现有方法依赖存储历史数据作为回放缓冲区，但存在隐私和内存限制问题。

Method: 构建类级别原型来总结每个个体的判别性表示，通过跨个体特征对齐和知识蒸馏逐步将新特征空间与全局原型记忆对齐。

Result: 在BCI Competition IV 2a和2b数据集上验证，框架有效平衡知识保留和适应性，在跨个体持续EEG解码任务中取得优越性能。

Conclusion: ProNECL框架能够在无需访问历史EEG样本的情况下保留先验知识，为持续EEG解码提供了一种实用的解决方案。

Abstract: Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.

</details>


### [12] [ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training](https://arxiv.org/abs/2511.20718)
*Chenliang Li,Adel Elmahdy,Alex Boyd,Zhongruo Wang,Alfredo Garcia,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了两种稳定多轮对话中PPO训练的技术：轮级重要性采样和裁剪偏差校正，解决了token级重要性采样与多轮环境结构不匹配以及离策略样本优势估计不准确的问题。


<details>
  <summary>Details</summary>
Motivation: PPO在多轮对话和推理任务中训练大型语言模型时性能不稳定且容易崩溃，主要问题包括token级重要性采样与多轮环境结构不匹配，以及离策略样本导致的高方差梯度。

Method: 引入两种互补的稳定技术：1) 轮级重要性采样，使优化与多轮推理的自然结构对齐；2) 裁剪偏差校正，通过降低不可靠的离策略样本权重来归一化梯度。组合得到三种变体：Turn-PPO、S-PPO和ST-PPO。

Result: 在多轮搜索任务上的实验表明，ST-PPO和S-PPO能防止大模型训练中的性能崩溃，保持较低的裁剪比率，并获得比标准token级PPO更高的任务性能。

Conclusion: 结合轮级重要性采样和裁剪偏差校正为稳定多轮LLM智能体训练提供了实用且可扩展的解决方案。

Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

</details>


### [13] [Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge](https://arxiv.org/abs/2511.20726)
*Yuhang Wang,Heye Huang,Zhenhua Xu,Kailai Sun,Baoshen Guo,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出了一种结合条件变分自编码器和大型语言模型的高保真场景生成框架，用于生成罕见长尾事件和复杂多智能体交互的自动驾驶测试场景。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在罕见长尾事件和复杂多智能体交互方面的关键挑战，这些事件在真实世界数据中稀缺但对安全验证至关重要。

Method: 使用CVAE编码历史轨迹和地图信息学习潜在交通结构，生成物理一致的基础场景；利用LLM作为对抗推理引擎，将非结构化场景描述解析为领域特定损失函数，动态指导不同风险级别的场景生成。

Result: 在CARLA和SMARTS中的广泛实验表明，该框架显著增加了高风险和长尾事件的覆盖范围，改善了模拟与现实世界交通分布的一致性，并暴露了比现有方法更具挑战性的交互场景。

Conclusion: 为安全验证建立了新途径，能够在罕见但重要的事件下对自动驾驶系统进行原则性压力测试。

Abstract: Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.

</details>


### [14] [A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems](https://arxiv.org/abs/2511.21032)
*Yuxuan Zhu,Cong Fu,Yabo Ni,Anxiang Zeng,Yuan Fang*

Main category: cs.LG

TL;DR: ELBOₜᴅs是一个概率框架，通过数据增强和因果图建模解决推荐系统中的时间分布偏移问题，已在Shopee产品搜索中成功部署。


<details>
  <summary>Details</summary>
Motivation: 时间分布偏移会侵蚀推荐系统的长期准确性，现有方法如增量训练、不变性学习和自监督学习在时间泛化稳定性、表示崩溃或数据利用效率方面存在不足。

Method: 首先通过统计分析识别关键偏移因子并设计数据增强策略扩展训练支持；其次使用因果图建模时间推荐场景，推导出自监督变分目标ELBOₜᴅs。

Result: 实验证明该方法实现了优越的时间泛化能力，用户GMV提升2.33%，并已在Shopee产品搜索中成功部署。

Conclusion: ELBOₜᴅs框架有效解决了时间分布偏移问题，在工业规模增量学习管道中表现出色。

Abstract: Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.

</details>


### [15] [Efficient Diffusion Planning with Temporal Diffusion](https://arxiv.org/abs/2511.21054)
*Jiaming Guo,Rui Zhang,Zerun Li,Yunkai Gao,Shaohui Peng,Siming Lan,Xing Hu,Zidong Du,Xishan Zhang,Ling Li*

Main category: cs.LG

TL;DR: 提出Temporal Diffusion Planner (TDP)方法，通过将去噪步骤分布在时间维度上，提高扩散规划的决策效率，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划方法在每个时间步生成新计划，计算开销大且决策频率低。受人类规划方式启发，希望创建详细短期计划和模糊长期计划，并随时间调整。

Method: TDP首先生成随时间逐渐模糊的初始计划，随后在每个时间步仅用少量去噪步骤更新前一个计划，而非生成全新计划，并引入自动重新规划机制防止计划与现实偏差过大。

Result: 在D4RL基准测试中，相比每步生成新计划的方法，TDP将决策频率提高11-24.8倍，同时达到更高或相当的性能。

Conclusion: TDP通过时间维度上的去噪步骤分布，显著提高了扩散规划的决策效率，同时保持了良好的性能表现。

Abstract: Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.

</details>


### [16] [Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning](https://arxiv.org/abs/2511.21075)
*Zhenchao Tang,Fang Wang,Haohuai He,Jiale Zhou,Tianxu Lv,Jun Zhu,Shouzhi Chen,Minghao Yang,Yu Wang,Jiayang Wu,Yidong Song,Jianhua Yao*

Main category: cs.LG

TL;DR: 提出了平衡微调(BFT)方法，通过两层加权机制从稀疏数据中学习复杂推理，无需外部奖励信号，在生物医学任务中显著优于标准监督微调。


<details>
  <summary>Details</summary>
Motivation: 当前方法在将大语言模型与专业生物医学知识对齐时面临挑战：标准监督微调容易过拟合表面指令模式，而强化学习因需要实验验证奖励而不实用。

Method: BFT采用两层加权机制：1) 在token级别通过预测概率缩放损失以稳定梯度；2) 在样本级别使用"最小组置信度"自适应增强困难样本的学习。

Result: BFT在医学任务中让LLMs学到了SFT遗漏的知识，在生物任务中超越了GeneAgent，其生成的文本嵌入可直接用于下游任务如基因相互作用预测。

Conclusion: BFT促进了LLMs在生物医学研究中的广泛应用，能够有效学习稀疏数据中的复杂推理机制。

Abstract: Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.

</details>


### [17] [Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination](https://arxiv.org/abs/2511.21118)
*Pius Onobhayedo,Paul Osemudiame Oamen*

Main category: cs.LG

TL;DR: 本文提出了一种解决联邦学习中聚合器缺乏问责、激励机制易被操纵、协调可扩展性差和治理存在追溯性操纵等问题的框架，通过加密收据、几何新颖性测量、并行对象所有权和时间锁定策略来实现可扩展的民主化AI。


<details>
  <summary>Details</summary>
Motivation: 人工智能正从集中式提供向分布式创建转变，但当前的联邦学习存在聚合器缺乏问责、经济机制易被操纵、协调可扩展性差和治理存在追溯性操纵等问题，阻碍了民主化AI愿景的实现。

Method: 使用加密收据证明聚合正确性，几何新颖性测量防止激励操纵，并行对象所有权实现线性可扩展性，时间锁定策略检查追溯性操纵。

Result: 提出的方法解决了联邦学习中的关键问题，为大规模分布式AI提供了可行的技术路径。

Conclusion: 通过密码学和经济机制的结合，可以实现可扩展、安全、民主的联邦学习系统，推动AI从集中式向分布式的转变。

Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.

</details>


### [18] [Privacy in Federated Learning with Spiking Neural Networks](https://arxiv.org/abs/2511.21181)
*Dogukan Aksu,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.LG

TL;DR: 脉冲神经网络（SNNs）在联邦学习中具有天然隐私保护优势，梯度反转攻击难以从SNN梯度中重建有意义的训练数据。


<details>
  <summary>Details</summary>
Motivation: 研究SNNs在联邦学习中的隐私保护特性，探索梯度反转攻击对SNNs的有效性，因为SNNs的非可微性和替代梯度训练可能减少梯度信息泄露风险。

Method: 将不同梯度泄漏攻击方法适配到脉冲域，在多种数据域上对SNNs进行梯度反转攻击的实证研究，并与传统人工神经网络（ANNs）进行对比。

Result: 与传统ANNs不同，SNN梯度产生的重建结果噪声大、时间不一致，无法恢复有意义的空间或时间结构，表明事件驱动动态和替代梯度训练显著降低了梯度信息量。

Conclusion: SNNs具有固有的隐私保护潜力，在联邦学习场景中比传统ANNs更能抵御梯度反转攻击，突显了神经形态计算在隐私保护方面的优势。

Abstract: Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.

</details>


### [19] [An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids](https://arxiv.org/abs/2511.21590)
*Muhammad Siddique,Sohaib Zafar*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的智能电网数字取证框架，结合传感器数据采集、认证通信、云存储和自动化取证分析，用于实时异常检测和入侵分析。


<details>
  <summary>Details</summary>
Motivation: 智能电网融合了传统电力基础设施和先进通信网络，这种集成带来了可能破坏电网稳定性和可靠性的漏洞，需要有效的数字取证方法来识别、检测和缓解安全事件。

Method: 开发了一个部署在云端的机器学习数字取证框架，使用监督和无监督学习算法（如随机森林、支持向量机、梯度提升树和深度神经网络）进行异常检测、事件重建和实时入侵分析。

Result: 通过对实时智能电表数据流的模拟和实验研究，该框架在准确性、可扩展性和对网络攻击（包括数据篡改、虚假数据注入和协调控制回路操纵）的韧性方面表现出色。

Conclusion: 云服务是大数据驱动取证工作流程的最佳骨干，使能源公用事业能够实现快速态势感知和智能事件响应。

Abstract: Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.

</details>


### [20] [DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving](https://arxiv.org/abs/2511.21669)
*Fengze Yu,Leshu Li,Brad McDanel,Saiqian Zhang*

Main category: cs.LG

TL;DR: DSD是一个分布式推测解码框架，通过协调草稿-目标执行在多设备部署中扩展推测解码，解决了LLM推理在异构边缘-云环境中的高延迟和有限可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理在异构边缘-云环境中面临高解码延迟和有限可扩展性挑战，现有的推测解码技术仅限于单节点执行。

Method: 提出DSD分布式推测解码框架，引入DSD-Sim离散事件模拟器捕捉网络、批处理和调度动态，并设计自适应窗口控制策略动态调整推测窗口大小。

Result: 实验显示DSD相比现有推测解码基线实现了最高1.1倍加速和9.7%吞吐量提升，在边缘和云环境中实现敏捷可扩展的LLM服务。

Conclusion: DSD框架成功将推测解码扩展到分布式环境，通过自适应窗口控制优化了异构边缘-云部署中的LLM推理性能。

Abstract: Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术从NASA航天器模拟发展而来，现正推动医疗健康转型，通过创建患者特异性虚拟模型来支持诊断、治疗规划和药物开发，但仍面临互操作性、数据隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 将数字孪生技术从工业领域引入医疗健康，旨在实现从被动治疗向预测性、预防性和个性化医疗的转变，通过创建患者特异性虚拟模型来优化医疗决策。

Method: 整合医学影像、生物传感器和计算模型，构建动态数据驱动的患者虚拟对应体，通过实时数据流持续更新，支持双向交互和个性化模拟。

Result: 代表性应用包括：心脏数字孪生预测心律失常治疗结果、肿瘤学数字孪生跟踪肿瘤进展优化放疗、药理学数字孪生加速药物发现。

Conclusion: 虽然面临互操作性、数据隐私和模型保真度等挑战，但可解释AI、联邦学习和统一监管框架等新兴解决方案为未来发展提供了有希望的途径，多器官数字孪生、基因组学整合和伦理治理将是关键发展方向。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [22] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 提出ICPO方法，通过利用LLM生成不同响应的概率来反映其推理过程的自我评估，结合可验证奖励解决现有RLVR方法中的粗粒度奖励、奖励噪声和低效探索问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在粗粒度奖励、奖励噪声和低效探索等问题，导致训练不稳定和熵崩溃，需要改进以增强LLM的推理能力。

Method: ICPO方法计算每个响应的偏好优势分数，通过比较同一输入提示下多个响应的相对生成概率，并将该分数与可验证奖励结合来指导探索过程。

Result: 在四个通用领域基准和三个数学基准上的综合实验表明，ICPO相比GRPO能稳定提升推理能力。

Conclusion: ICPO方法有效缓解了粗粒度奖励和奖励噪声问题，抑制过度自信错误，增强被低估高质量响应的相对优势，防止模型过拟合特定策略，促进更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [23] [EWE: An Agentic Framework for Extreme Weather Analysis](https://arxiv.org/abs/2511.21444)
*Zhe Jiang,Jiong Wang,Xiaoyu Yue,Zijie Guo,Wenlong Zhang,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.AI

TL;DR: EWE是首个专门用于极端天气自动诊断推理的智能代理框架，通过模拟专家工作流程，能够从原始气象数据自主生成和解释多模态可视化，实现全面诊断分析。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件对全球社会构成日益严重的风险，但当前专家驱动、劳动密集的诊断范式造成了关键分析瓶颈，阻碍了科学进展。虽然AI在地球科学预测方面取得了进展，但自动诊断推理这一同等重要的挑战仍未被充分探索。

Method: EWE通过知识引导规划、闭环推理和领域定制的气象工具包来模拟专家工作流程。它能够从原始气象数据自主生成和解释多模态可视化，实现全面诊断分析。

Result: 为促进该新兴领域的发展，作者引入了首个基准测试，包含103个高影响事件的精选数据集和新的逐步评估指标。

Conclusion: EWE标志着向自动化科学发现迈出的一步，并有望民主化专业知识和智力资源，特别是对易受极端天气影响的发展中国家。

Abstract: Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.

</details>
