<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization](https://arxiv.org/abs/2509.20785)
*Jincai Song,Haipeng Chen,Jun Qin,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对跨域半监督域泛化(CD-SSDG)的双监督非对称协同训练框架，解决医学图像分割中标注数据有限和域偏移同时存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统半监督域泛化方法假设每个源域都有标注和未标注数据，但实际中标注数据有限且存在域偏移。本文探索更实用的场景，即标注和未标注训练数据之间存在域偏移。

Method: 提出双监督非对称协同训练(DAC)框架，基于双子模型协同训练，集成特征级监督和非对称辅助任务，解决伪标签不准确问题。

Result: 在Fundus、Polyp和SCGM等真实医学图像分割数据集上的实验表明，所提DAC框架具有强大的泛化能力。

Conclusion: DAC框架通过特征级监督和辅助自监督任务，有效解决了CD-SSDG中的伪标签不准确问题，提升了模型在未见域上的泛化性能。

Abstract: Semi-supervised domain generalization (SSDG) in medical image segmentation
offers a promising solution for generalizing to unseen domains during testing,
addressing domain shift challenges and minimizing annotation costs. However,
conventional SSDG methods assume labeled and unlabeled data are available for
each source domain in the training set, a condition that is not always met in
practice. The coexistence of limited annotation and domain shift in the
training set is a prevalent issue. Thus, this paper explores a more practical
and challenging scenario, cross-domain semi-supervised domain generalization
(CD-SSDG), where domain shifts occur between labeled and unlabeled training
data, in addition to shifts between training and testing sets. Existing SSDG
methods exhibit sub-optimal performance under such domain shifts because of
inaccurate pseudolabels. To address this issue, we propose a novel
dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.
Building upon the co-training paradigm with two sub-models offering cross
pseudo supervision, our DAC framework integrates extra feature-level
supervision and asymmetric auxiliary tasks for each sub-model. This
feature-level supervision serves to address inaccurate pseudo supervision
caused by domain shifts between labeled and unlabeled data, utilizing
complementary supervision from the rich feature space. Additionally, two
distinct auxiliary self-supervised tasks are integrated into each sub-model to
enhance domain-invariant discriminative feature learning and prevent model
collapse. Extensive experiments on real-world medical image segmentation
datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust
generalizability of the proposed DAC framework.

</details>


### [2] [Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification](https://arxiv.org/abs/2509.20899)
*Patrick Knab,Sascha Marton,Philipp J. Schubert,Drago Guggiana,Christian Bartelt*

Main category: cs.CV

TL;DR: MoTIF是一个基于transformer架构的概念瓶颈模型，专门为视频分类设计，能够处理任意长度的视频序列，通过全局概念重要性、局部概念相关性和时间依赖性三个视角来增强视频动作理解的解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的概念瓶颈模型主要针对静态图像，无法有效处理视频数据中的时间依赖性，而时间依赖对于捕捉动作和事件至关重要。

Method: 采用transformer架构，将概念瓶颈框架扩展到视频分类，通过识别跨时间重复出现的语义实体（如对象、属性、动作组件）来形成描述动作的motif模式。

Result: 实验结果表明，基于概念建模的范式可以有效地迁移到视频数据，在保持竞争力的性能同时，能够更好地理解概念在时间上下文中的贡献。

Conclusion: MoTIF成功地将概念瓶颈模型扩展到视频领域，为视频动作理解提供了更丰富的解释性视角，同时保持了良好的分类性能。

Abstract: Conceptual models such as Concept Bottleneck Models (CBMs) have driven
substantial progress in improving interpretability for image classification by
leveraging human-interpretable concepts. However, extending these models from
static images to sequences of images, such as video data, introduces a
significant challenge due to the temporal dependencies inherent in videos,
which are essential for capturing actions and events. In this work, we
introduce MoTIF (Moving Temporal Interpretable Framework), an architectural
design inspired by a transformer that adapts the concept bottleneck framework
for video classification and handles sequences of arbitrary length. Within the
video domain, concepts refer to semantic entities such as objects, attributes,
or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across
time - forming motifs collectively describing and explaining actions. Our
design explicitly enables three complementary perspectives: global concept
importance across the entire video, local concept relevance within specific
windows, and temporal dependencies of a concept over time. Our results
demonstrate that the concept-based modeling paradigm can be effectively
transferred to video data, enabling a better understanding of concept
contributions in temporal contexts while maintaining competitive performance.
Code available at github.com/patrick-knab/MoTIF.

</details>


### [3] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff是一个模拟器约束的扩散模型，通过将环境参数直接集成到去噪过程中，无需在推理时重复调用模拟器即可高效生成物理合理的人体运动，并提供对物理系数的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将基于模拟器的运动投影层整合到扩散过程中以确保物理合理性，但由于模拟器的顺序性质，这些方法计算成本高昂且无法并行化。

Method: 将模拟器运动投影解释为扩散过程中的一种引导形式，提出SimDiff模型，通过将环境参数（如重力、风力）直接条件化到去噪过程中，避免推理时重复的模拟器调用。

Result: SimDiff能高效生成物理合理的运动，提供对物理系数的细粒度控制，并能成功泛化到未见过的环境参数组合，展示了组合泛化能力。

Conclusion: SimDiff通过将模拟器约束整合为扩散引导，实现了高效且可控的物理合理运动生成，同时具备组合泛化能力。

Abstract: Generating physically plausible human motion is crucial for applications such
as character animation and virtual reality. Existing approaches often
incorporate a simulator-based motion projection layer to the diffusion process
to enforce physical plausibility. However, such methods are computationally
expensive due to the sequential nature of the simulator, which prevents
parallelization. We show that simulator-based motion projection can be
interpreted as a form of guidance, either classifier-based or classifier-free,
within the diffusion process. Building on this insight, we propose SimDiff, a
Simulator-constrained Diffusion Model that integrates environment parameters
(e.g., gravity, wind) directly into the denoising process. By conditioning on
these parameters, SimDiff generates physically plausible motions efficiently,
without repeated simulator calls at inference, and also provides fine-grained
control over different physical coefficients. Moreover, SimDiff successfully
generalizes to unseen combinations of environmental parameters, demonstrating
compositional generalization.

</details>


### [4] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: 提出SNCE方法，通过操纵单个神经元精确防止有害内容生成，在保持图像质量的同时实现概念擦除


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型存在生成有害内容的安全风险，现有概念擦除方法难以在精确移除目标概念的同时最小化图像质量退化

Method: 训练稀疏自编码器将文本嵌入映射到稀疏解耦的潜在空间，通过调制频率评分识别有害概念对应的神经元，抑制其激活

Result: 在多个基准测试中达到最先进的目标概念擦除效果，保持非目标概念的生成能力，对对抗攻击表现出强鲁棒性

Conclusion: SNCE方法能够以手术精度实现概念擦除，在保持图像质量的同时有效防止有害内容生成，显著优于现有方法

Abstract: Text-to-image models exhibit remarkable capabilities in image generation.
However, they also pose safety risks of generating harmful content. A key
challenge of existing concept erasure methods is the precise removal of target
concepts while minimizing degradation of image quality. In this paper, we
propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can
precisely prevent harmful content generation by manipulating only a single
neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text
embeddings into a sparse, disentangled latent space, where individual neurons
align tightly with atomic semantic concepts. To accurately locate neurons
responsible for harmful concepts, we design a novel neuron identification
method based on the modulated frequency scoring of activation patterns. By
suppressing activations of the harmful concept-specific neuron, SNCE achieves
surgical precision in concept erasure with minimal disruption to image quality.
Experiments on various benchmarks demonstrate that SNCE achieves
state-of-the-art results in target concept erasure, while preserving the
model's generation capabilities for non-target concepts. Additionally, our
method exhibits strong robustness against adversarial attacks, significantly
outperforming existing methods.

</details>


### [5] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出了MedVSR框架，专门针对医学视频超分辨率任务，通过交叉状态空间传播和内部状态空间重建模块解决医学视频特有的对齐困难和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频对准确诊断至关重要，但硬件限制和生理约束使其难以获取。现有VSR模型在处理医学视频时面临相机抖动、噪声、帧间突变等独特挑战，导致光流误差大、对齐困难，且容易产生伪影和特征扭曲，可能误导医生诊断。

Method: 1. 交叉状态空间传播(CSSP)：将远距离帧投影为状态空间模型中的控制矩阵，选择性传播一致且信息丰富的特征到相邻帧，实现有效对齐。2. 内部状态空间重建(ISSR)：通过联合长距离空间特征学习和大核短距离信息聚合，增强组织结构并减少伪影。

Result: 在包括内窥镜和白内障手术在内的四个不同医学场景数据集上的实验表明，MedVSR在重建性能和效率方面显著优于现有VSR模型。

Conclusion: MedVSR是针对医学视频超分辨率任务的定制化框架，能够有效解决医学视频特有的挑战，在多个医学场景中展现出优越的重建性能。

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641)
*Giovana Morais,Magdalena Fuentes*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Audio Large Language Models (Audio LLMs) enable human-like conversation about
music, yet it is unclear if they are truly listening to the audio or just using
textual reasoning, as recent benchmarks suggest. This paper investigates this
issue by quantifying the contribution of each modality to a model's output. We
adapt the MM-SHAP framework, a performance-agnostic score based on Shapley
values that quantifies the relative contribution of each modality to a model's
prediction. We evaluate two models on the MuChoMusic benchmark and find that
the model with higher accuracy relies more on text to answer questions, but
further inspection shows that even if the overall audio contribution is low,
models can successfully localize key sound events, suggesting that audio is not
entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs
and we hope it will serve as a foundational step for future research in
explainable AI and audio.

</details>


### [7] [FERD: Fairness-Enhanced Data-Free Robustness Distillation](https://arxiv.org/abs/2509.20793)
*Zhengxiao Li,Liming Lu,Xu Zheng,Siyuan Liang,Zhenghan Chen,Yongbin Zhou,Shuchao Pang*

Main category: cs.LG

TL;DR: 提出了FERD框架，首个关注公平性的数据自由鲁棒性蒸馏方法，通过调整对抗样本的比例和分布来解决不同类别间鲁棒性差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由鲁棒性蒸馏方法只关注整体鲁棒性，忽视了不同类别间的鲁棒公平性问题，导致不同类别间的鲁棒性存在显著差异。

Method: FERD采用鲁棒性引导的类别重加权策略调整样本比例，生成公平感知样本(FAEs)和均匀目标对抗样本(UTAEs)来平衡分布，通过特征级预测的均匀性约束避免类别特定非鲁棒特征的支配。

Result: 在三个公开数据集上的实验表明，FERD在所有对抗攻击下都达到了最先进的最差类别鲁棒性，如在CIFAR-10上使用MobileNet-V2时，FGSM和AutoAttack下的最差类别鲁棒性分别提高了15.1%和6.4%。

Conclusion: FERD框架在鲁棒性和公平性方面都表现出优越性能，有效解决了数据自由鲁棒性蒸馏中的公平性问题。

Abstract: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from
the teacher to the student without accessing the training data. While existing
methods focus on overall robustness, they overlook the robust fairness issues,
leading to severe disparity of robustness across different categories. In this
paper, we find two key problems: (1) student model distilled with equal class
proportion data behaves significantly different across distinct categories; and
(2) the robustness of student model is not stable across different attacks
target. To bridge these gaps, we present the first Fairness-Enhanced data-free
Robustness Distillation (FERD) framework to adjust the proportion and
distribution of adversarial examples. For the proportion, FERD adopts a
robustness-guided class reweighting strategy to synthesize more samples for the
less robust categories, thereby improving robustness of them. For the
distribution, FERD generates complementary data samples for advanced robustness
distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a
uniformity constraint on feature-level predictions, which suppress the
dominance of class-specific non-robust features, providing a more balanced
representation across all categories. Then, FERD constructs Uniform-Target
Adversarial Examples (UTAEs) from FAEs by applying a uniform target class
constraint to avoid biased attack directions, which distribute the attack
targets across all categories and prevents overfitting to specific vulnerable
categories. Extensive experiments on three public datasets show that FERD
achieves state-of-the-art worst-class robustness under all adversarial attack
(e.g., the worst-class robustness under FGSM and AutoAttack are improved by
15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior
performance in both robustness and fairness aspects.

</details>


### [8] [Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition](https://arxiv.org/abs/2509.20840)
*Jiaqi Tang,Yinsong Xu,Yang Liu,Qingchao Chen*

Main category: cs.LG

TL;DR: 提出两阶段训练框架解决多模态融合中的模态竞争问题，通过单模态训练塑造初始状态，引入有效竞争强度概念和基于互信息的可计算诊断指标，实现动态平衡模态竞争。


<details>
  <summary>Details</summary>
Motivation: 多模态融合中常出现模态竞争问题，一个模态主导学习过程而其他模态优化不足。现有方法多在联合学习阶段解决该问题，忽视了模型初始状态的关键影响。

Method: 提出两阶段训练框架：1）单模态训练阶段塑造初始状态；2）引入有效竞争强度概念量化模态竞争力；3）开发可计算诊断指标FastPID分解信息为独特性、冗余性和协同性；4）异步训练控制器动态平衡模态并定位理想初始状态。

Result: 在多个基准测试中实现了最先进的性能，证明通过塑造预融合模型的初始状态可以有效缓解模态竞争，可靠地实现协同多模态融合。

Conclusion: 塑造预融合模型的初始状态是一种强大策略，可以在竞争开始前缓解模态竞争，可靠地实现协同多模态融合。

Abstract: Multi-modal fusion often suffers from modality competition during joint
training, where one modality dominates the learning process, leaving others
under-optimized. Overlooking the critical impact of the model's initial state,
most existing methods address this issue during the joint learning stage. In
this study, we introduce a two-stage training framework to shape the initial
states through unimodal training before the joint training. First, we propose
the concept of Effective Competitive Strength (ECS) to quantify a modality's
competitive strength. Our theoretical analysis further reveals that properly
shaping the initial ECS by unimodal training achieves a provably tighter error
bound. However, ECS is computationally intractable in deep neural networks. To
bridge this gap, we develop a framework comprising two core components: a
fine-grained computable diagnostic metric and an asynchronous training
controller. For the metric, we first prove that mutual information(MI) is a
principled proxy for ECS. Considering MI is induced by per-modality marginals
and thus treats each modality in isolation, we further propose FastPID, a
computationally efficient and differentiable solver for partial information
decomposition, which decomposes the joint distribution's information into
fine-grained measurements: modality-specific uniqueness, redundancy, and
synergy. Guided by these measurements, our asynchronous controller dynamically
balances modalities by monitoring uniqueness and locates the ideal initial
state to start joint training by tracking peak synergy. Experiments on diverse
benchmarks demonstrate that our method achieves state-of-the-art performance.
Our work establishes that shaping the pre-fusion models' initial state is a
powerful strategy that eases competition before it starts, reliably unlocking
synergistic multi-modal fusion.

</details>


### [9] [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](https://arxiv.org/abs/2509.21161)
*Giuseppe Serra,Florian Buettner*

Main category: cs.LG

TL;DR: 提出DATS方法解决持续学习中的校准问题，通过距离感知的温度缩放实现任务自适应校准，无需任务信息即可降低校准误差


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要从数据角度解决校准问题，使用单一温度参数忽略了任务间差异，导致不同任务的校准误差波动较大

Method: DATS结合原型距离估计和距离感知校准，推断任务接近度并分配自适应温度，无需先验任务信息

Result: 在标准基准和真实世界不平衡生物医学数据集上的广泛评估表明，相比最先进方法，该方法在降低任务间校准误差方面更稳定、可靠和一致

Conclusion: DATS方法通过距离感知的温度缩放有效解决了持续学习中的任务自适应校准问题，提高了模型的不确定性校准能力

Abstract: Continual Learning (CL) is recently gaining increasing attention for its
ability to enable a single model to learn incrementally from a sequence of new
classes. In this scenario, it is important to keep consistent predictive
performance across all the classes and prevent the so-called Catastrophic
Forgetting (CF). However, in safety-critical applications, predictive
performance alone is insufficient. Predictive models should also be able to
reliably communicate their uncertainty in a calibrated manner - that is, with
confidence scores aligned to the true frequencies of target events. Existing
approaches in CL address calibration primarily from a data-centric perspective,
relying on a single temperature shared across all tasks. Such solutions
overlook task-specific differences, leading to large fluctuations in
calibration error across tasks. For this reason, we argue that a more
principled approach should adapt the temperature according to the distance to
the current task. However, the unavailability of the task information at test
time/during deployment poses a major challenge to achieve the intended
objective. For this, we propose Distance-Aware Temperature Scaling (DATS),
which combines prototype-based distance estimation with distance-aware
calibration to infer task proximity and assign adaptive temperatures without
prior task information. Through extensive empirical evaluation on both standard
benchmarks and real-world, imbalanced datasets taken from the biomedical
domain, our approach demonstrates to be stable, reliable and consistent in
reducing calibration error across tasks compared to state-of-the-art
approaches.

</details>


### [10] [Federated Flow Matching](https://arxiv.org/abs/2509.21250)
*Zifan Wang,Anqi Dong,Mahmoud Selim,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出了联邦流匹配（FFM）框架，用于在隐私约束下训练流匹配模型，包括FFM-vanilla、FFM-LOT和FFM-GOT三种方法，其中FFM-GOT通过共享全局势函数协调客户端间的耦合，在联邦设置中提高了流直度和样本质量。


<details>
  <summary>Details</summary>
Motivation: 当前数据分散在设备和机构中，隐私、所有权和法规限制阻止了数据集中化，因此需要在分布式数据上直接训练生成模型而不进行集中聚合。

Method: 开发了三种联邦流匹配方法：FFM-vanilla（本地独立训练）、FFM-LOT（使用局部最优传输耦合）和FFM-GOT（基于最优传输半对偶公式，使用共享全局势函数协调客户端耦合）。

Result: 在合成和图像数据集上的实验表明，FFM能够在保护隐私的同时提高联邦设置中的流直度和样本质量，性能与集中式基线相当。

Conclusion: FFM框架成功实现了在隐私约束下训练流匹配模型，FFM-GOT方法通过全局协调机制在联邦学习中取得了最佳性能。

Abstract: Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.

</details>


### [11] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: 提出PSPO方法，通过平滑当前策略概率向旧策略插值来替代传统的比率裁剪，在保持梯度信号的同时创建软信任区域，显著提升语言模型在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法如PPO和GRPO依赖比率裁剪来稳定更新，但裁剪会丢弃信息并引入梯度不连续性，需要更平滑的优化方法。

Method: 提出概率平滑策略优化(PSPO)，在计算重要性比率前将当前策略概率向旧策略平滑插值，类似于标签平滑，形成软信任区域。

Result: 在GSM8K测试集上，GR-PSPO相比裁剪GRPO显著提升性能：0.5B模型从17.6%提升到39.7%，1.5B模型从37.8%提升到59.4%，同时生成更清晰、简洁和逻辑性强的回答。

Conclusion: PSPO通过概率平滑有效替代比率裁剪，在保持训练稳定性的同时显著提升语言模型的推理能力和性能，具有良好的泛化能力。

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种新的重构框架，用于动态验证和组装时间触发系统的调度方案，解决消息碰撞、优先级处理不当等问题，确保系统安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 时间触发系统在动态操作环境中面临消息碰撞、优先级处理不当导致的死锁、以及生成不完整或无效调度等挑战，这些都会危及系统安全和性能。

Method: 通过重构模型将AI生成或启发式得出的调度优先级系统性地转换为完全可执行的调度方案，包含安全检查、高效分配算法和恢复机制来处理意外事件。

Result: 实验结果表明该框架显著提升了系统适应性、操作完整性和运行时性能，同时保持了计算效率。

Conclusion: 这项工作为安全关键时间触发系统的安全调度生成提供了实用且可扩展的解决方案，即使在高度动态和不确定的操作条件下也能实现可靠灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [13] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种集成在元调度器中的自适应在线学习单元，使用强化学习来动态扩展多调度图，以解决离线训练中无法覆盖所有可能场景的问题。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练方法在构建全面的多调度图时面临挑战，因为生成包含所有可能场景的概率空间计算量大且不可行。在线学习可以持续探索新调度方案，处理意外事件。

Method: 在元调度器中集成自适应在线学习单元，使用多种强化学习模型来发现新调度方案并优化现有调度器，通过实时训练持续改进AI推理。

Result: 系统能够动态适应意外事件和复杂调度场景，扩展多调度图，提高系统性能，在严格截止期限或新性能标准下优化调度器。

Conclusion: 通过在线强化学习，系统保持灵活性，能够满足不断变化的需求，确保大规模安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [14] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 提出了反射认知架构（RCA），通过协调多个LLM从直接经验中学习，实现高精度预测和高质量解释的平衡，在医疗疾病预测任务中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和大语言模型方法难以同时实现高精度预测和透明、临床有意义的解释，需要一种能深度理解数据并产生可信解释的框架。

Method: RCA框架包含迭代规则精炼机制（从预测错误中改进逻辑）和分布感知规则检查机制（基于数据集全局统计进行推理），使用预测准确性作为驱动深度理解的信号。

Result: 在1个私有和2个公共数据集上评估，相比22个基线模型，RCA实现了最先进的准确性和鲁棒性，相对改进高达40%，并能生成清晰、逻辑、基于证据且平衡的解释。

Conclusion: RCA通过构建强大的数据内部模型，证明了高精度和高质量解释是相互促进的目标，具有创建真正可信临床决策支持系统的潜力。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出了一种通过修改外交事件叙事框架来改变公众情绪的AI框架，成功将负面情绪转为中性或正面的成功率达70%


<details>
  <summary>Details</summary>
Motivation: 传统公众情绪分析方法耗时耗力且缺乏前瞻性，而公众情绪在外交中至关重要，需要更高效的方法来预测和引导舆论

Method: 训练语言模型预测公众对外交事件的反应，构建外交事件描述与公众讨论数据集，基于传播理论和专家意见确定可修改的文本特征，开发反事实生成算法系统修改原始文本

Result: 该框架成功将公众情绪转向更有利状态，成功率达到70%

Conclusion: 该框架可作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，帮助构建更理想的公众情绪

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [16] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 本文研究发现LLM训练数据中存在句法模板与领域之间的虚假相关性，这种相关性会干扰模型对指令语义的理解，甚至可能被用于绕过安全微调机制。


<details>
  <summary>Details</summary>
Motivation: LLM需要理解指令的语义和领域才能正确响应，但句法也能传递隐含信息。现有研究表明句法模板在训练数据中普遍存在，并经常出现在模型输出中。本文旨在系统分析句法模板、领域和语义之间的关系。

Method: 使用合成训练数据集研究句法-领域相关性对OLMo-2模型性能的影响；开发评估框架检测训练模型中的这种现象；在FlanV2数据集上测试开源和闭源模型；进行安全微调案例研究。

Result: 句法-领域相关性显著降低了OLMo-2模型在实体知识任务上的性能（均值0.51±0.06）；在OLMo-2-7B、Llama-4-Maverick和GPT-4o中均检测到这种现象；意外句法-领域相关性可被用于绕过OLMo-2-7B Instruct和GPT-4o的拒绝机制。

Conclusion: 需要明确测试句法-领域相关性，并确保训练数据中特别是领域内的句法多样性，以防止此类虚假相关性影响模型性能和安全。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework](https://arxiv.org/abs/2509.20705)
*Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti*

Main category: cs.RO

TL;DR: BIM2RDT框架通过AI将静态BIM转换为动态机器人就绪的数字孪生，集成BIM数据、IoT传感器和机器人视觉数据，使用SG-ICP算法提升点云配准精度，并整合实时安全监测。


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业现有BIM数据与实时现场条件之间的差距，提升数字管理的安全性和效率，通过动态数字孪生优化机器人任务执行。

Method: 提出BIM2RDT框架，集成BIM几何语义信息、IoT活动数据和机器人视觉空间数据；开发SG-ICP点云配准算法，利用LLM推理对象方向先验；采用YOLOE目标检测和Shi-Tomasi角点检测跟踪建筑元素；整合实时手部振动监测。

Result: SG-ICP相比标准ICP在特征遮挡场景中配准RMSE降低64.3%-88.3%；实时HAV监测在超过暴露限值时触发警告，符合ISO 5349-1标准。

Conclusion: BIM2RDT框架成功将静态BIM转化为动态机器人就绪数字孪生，显著提升现场数据配准精度和安全管理能力，为建筑行业数字化管理提供有效解决方案。

Abstract: The adoption of cyber-physical systems and jobsite intelligence that connects
design models, real-time site sensing, and autonomous field operations can
dramatically enhance digital management in the construction industry. This
paper introduces BIM2RDT (Building Information Models to Robot-Ready Site
Digital Twins), an agentic artificial intelligence (AI) framework designed to
transform static Building Information Modeling (BIM) into dynamic, robot-ready
digital twins (DTs) that prioritize safety during execution. The framework
bridges the gap between pre-existing BIM data and real-time site conditions by
integrating three key data streams: geometric and semantic information from BIM
models, activity data from IoT sensor networks, and visual-spatial data
collected by robots during site traversal. The methodology introduces
Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that
leverages large language model (LLM) reasoning. Unlike traditional methods,
SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors
based on BIM semantics, improving alignment accuracy by avoiding convergence on
local minima. This creates a feedback loop where robot-collected data updates
the DT, which in turn optimizes paths for missions. The framework employs YOLOE
object detection and Shi-Tomasi corner detection to identify and track
construction elements while using BIM geometry as a priori maps. The framework
also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping
sensor-detected safety events to the digital twin using IFC standards for
intervention. Experiments demonstrate SG-ICP's superiority over standard ICP,
achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with
occluded features, ensuring plausible orientations. HAV integration triggers
warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.

</details>
