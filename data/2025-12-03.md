<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale](https://arxiv.org/abs/2512.02055)
*Mirela G. Tulbure,Julio Caineta,Mark Broich,Mollie D. Gaines,Philippe Rufin,Leon-Friedrich Thomas,Hamed Alemohammad,Jan Hemmerling,Patrick Hostert*

Main category: cs.CV

TL;DR: 研究通过微调TerraMind地理空间基础模型，结合多模态光学和SAR数据，提升了全球洪水范围制图的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 洪水是最具破坏性的天气相关灾害之一，2024年作为有记录以来最暖年份，极端洪水事件影响了五大洲的社区。虽然地球观测卫星提供了关键的洪水监测覆盖，但操作准确性严重依赖标注数据集和模型泛化能力。最近的地理空间基础模型（如ESA-IBM的TerraMind）通过大规模自监督预训练提高了泛化性，但其在全球多样化洪水事件中的性能仍不清楚。

Method: 使用FloodsNet数据集（包含85个全球洪水事件的协调多模态数据，包括Sentinel-1 SAR和Sentinel-2光学图像）对TerraMind进行微调。测试了四种配置（基础模型vs大型模型；冻结vs解冻骨干网络），并与TerraMind Sen1Floods11示例以及同时在FloodsNet和Sen1Floods11上训练的U-Net进行比较。

Result: 基础-解冻配置在准确性、精确度和召回率方面提供了最佳平衡，且计算成本显著低于大型模型。大型解冻模型实现了最高召回率。在FloodsNet上训练的模型在召回率上优于Sen1Floods11训练的示例，同时保持相似的总体准确性。U-Net比所有GFM配置实现了更高的召回率，但准确性和精确度略低。

Conclusion: 研究表明，整合多模态光学和SAR数据并微调地理空间基础模型可以增强近实时洪水制图能力。这项研究提供了首批全球规模的地理空间基础模型洪水分割评估之一，突出了其在气候适应和灾害恢复方面的潜力和当前局限性。

Abstract: Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.

</details>


### [2] [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425)
*Woongyeong Yeo,Kangsan Kim,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: WorldMM是一个多模态记忆代理，通过构建文本和视觉的互补记忆来解决长视频理解问题，在五个长视频问答基准上平均提升8.4%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理小时或天级别的长视频时面临挑战：上下文容量有限，抽象过程中关键视觉细节丢失。现有的记忆增强方法主要依赖文本摘要，无法在复杂场景推理中利用视觉证据，且固定时间尺度的检索限制了捕捉可变时长事件的能力。

Method: WorldMM构建三种互补记忆：情景记忆（跨多时间尺度索引事实事件）、语义记忆（持续更新高层概念知识）、视觉记忆（保留场景详细信息）。推理时，自适应检索代理迭代选择最相关的记忆源，利用多时间粒度，直到收集足够信息。

Result: 在五个长视频问答基准上显著优于现有基线，平均性能比之前最先进方法提升8.4%，证明了其在长视频推理中的有效性。

Conclusion: WorldMM通过多模态记忆架构解决了长视频理解的关键挑战，结合文本和视觉表示，自适应多时间尺度检索，为长视频推理提供了有效解决方案。

Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.

</details>


### [3] [Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors](https://arxiv.org/abs/2512.02447)
*Fan Luo,Zeyu Gao,Xinhao Luo,Kai Zhao,Yanfeng Lu*

Main category: cs.CV

TL;DR: TDE通过Spiking Encoder生成多样化时间刺激和Attention Gating Module建模时域依赖，结合Spike-Driven Attention降低能耗，显著提升SNN检测性能


<details>
  <summary>Details</summary>
Motivation: 现有SNN通常直接复制输入或固定间隔聚合，导致神经元接收几乎相同的时间刺激，限制了模型表达能力，尤其在复杂任务如目标检测中表现不足

Method: 提出TDE框架：1) Spiking Encoder生成多样化时间步输入刺激；2) Attention Gating Module基于时域依赖指导SE生成；3) Spike-Driven Attention消除高能耗乘法操作

Result: 在PASCAL VOC静态数据集达到57.7% mAP50-95，在神经形态EvDET200K数据集达到47.6%；SDA能耗仅为传统注意力模块的0.240倍

Conclusion: TDE能无缝集成现有SNN检测器，显著提升时域信息建模能力，同时通过SDA大幅降低能耗，为高效SNN目标检测提供有效解决方案

Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.

</details>


### [4] [Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation](https://arxiv.org/abs/2512.02457)
*Jianzong Wu,Hao Lian,Dachao Hao,Ye Tian,Qingyu Shi,Biaolong Chen,Hao Jiang*

Main category: cs.CV

TL;DR: 音频-视频联合去噪训练能提升视频生成质量，即使只关注视频质量本身


<details>
  <summary>Details</summary>
Motivation: 探索音频-视频联合去噪训练是否能够改善视频生成质量，即使最终只关心视频质量。现有研究表明多模态耦合不仅有利于音频-视频同步，也可能提升视频本身质量。

Method: 提出参数高效的AVFullDiT架构，利用预训练的文本到视频(T2V)和文本到音频(T2A)模块进行联合去噪。训练T2AV模型和T2V-only对照模型进行对比实验。

Result: 首次系统证明音频-视频联合去噪能带来超越同步的改进。在具有大运动和物体接触运动的挑战性子集上观察到一致改进。预测音频作为特权信号，鼓励模型内化视觉事件与其声学后果之间的因果关系。

Conclusion: 跨模态协同训练是开发更强、更具物理基础的世界模型的有前景方法。音频预测通过建立视觉事件与声学后果的因果关系来正则化视频动态。

Abstract: Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.

</details>


### [5] [ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning](https://arxiv.org/abs/2512.02835)
*Yifan Li,Yingda Yin,Lingting Zhu,Weikai Chen,Shengju Qian,Xin Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: ReVSeg提出了一种显式分解的推理方法，将视频目标分割任务分解为语义解释、时序证据选择和空间定位三个步骤，通过强化学习优化多步推理链，实现可解释的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有视频目标分割方法通常将复杂的动态、因果关系和时序交互推理简化为潜在嵌入表示，导致推理链不透明且难以追踪。需要一种能够显式分解推理过程的方法。

Method: 采用显式分解视角，将推理分解为三个顺序操作：语义解释、时序证据选择、空间定位。利用预训练视觉语言模型的原生接口执行推理，并通过强化学习优化多步推理链，使模型能够从结果驱动的信号中自我优化决策质量。

Result: ReVSeg在标准视频目标分割基准测试中达到了最先进的性能，并产生了可解释的推理轨迹。

Conclusion: 通过显式分解推理过程并利用强化学习优化多步决策链，ReVSeg不仅提升了视频目标分割的性能，还提供了透明可解释的推理轨迹，解决了现有方法推理不透明的问题。

Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting](https://arxiv.org/abs/2512.02061)
*Zhenliang Ni,Xiaowen Ma,Zhenkai Wu,Shuai Xiao,Han Shu,Xinghao Chen*

Main category: cs.LG

TL;DR: 提出Ada-MoGE模型，通过自适应高斯混合专家解决传统MoE在多元时间序列预测中频率覆盖不平衡问题，在6个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，数据频谱分布会随时间演变，导致主导频率偏移。传统固定专家数量的MoE模型难以适应这种变化，出现频率覆盖不平衡问题：专家太少会忽略关键信息，太多会引入噪声。

Method: 提出Ada-MoGE自适应高斯混合专家模型：1) 集成频谱强度和频率响应来自适应确定专家数量，确保与输入数据频率分布对齐；2) 使用高斯带通滤波器平滑分解频域特征，避免直接频带截断引入噪声。

Result: 在6个公开基准测试中达到最先进性能，仅使用20万参数。

Conclusion: Ada-MoGE通过自适应确定专家数量和高斯带通滤波，有效解决了传统MoE的频率覆盖不平衡问题，在多元时间序列预测中实现了更好的性能。

Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.

</details>


### [7] [TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links](https://arxiv.org/abs/2512.02465)
*Xingwang Li,Mengyun Chen,Jiamou Liu,Sijie Wang,Shuanggen Jin,Jafet C. M. Andersson,Jonas Olsson,Remco,van de Beek,Hai Victor Habi,Congzheng Han*

Main category: cs.LG

TL;DR: 提出名为TabGRU的混合深度学习架构，结合Transformer和双向门控循环单元，用于商业微波链路降雨监测，在瑞典哥德堡数据集上优于传统物理模型和深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 面对全球城市化加速和极端天气事件增多，高分辨率城市降雨监测对建设韧性智慧城市至关重要。传统基于物理模型的CML降雨反演方法难以处理现实世界中的信号噪声和非线性衰减等复杂问题。

Method: 提出TabGRU混合深度学习架构，结合Transformer和双向门控循环单元，捕捉CML信号数据中的长期依赖和局部序列特征。采用可学习位置嵌入和注意力池化机制增强动态特征提取和泛化能力。

Result: 在瑞典哥德堡数据集（2015年6-9月）上验证，使用12个子链路和两个雨量计（Torp和Barl），测试期覆盖约10次降雨事件。TabGRU在Torp站点的R²为0.91，Barl站点为0.96，优于深度学习基线，相比物理模型能有效缓解峰值降雨时的过估计问题。

Conclusion: TabGRU模型能有效克服传统方法的局限性，在测试条件下为基于CML的城市降雨监测提供稳健准确的解决方案。

Abstract: In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.

</details>


### [8] [FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization](https://arxiv.org/abs/2512.02901)
*Feiyu Wang,Xinyu Tan,Bokai Huang,Yihao Zhang,Guoan Wang,Peizhuang Cong,Tong Yang*

Main category: cs.LG

TL;DR: Fairy2i是一个将预训练实值LLM转换为等效复值表示的通用框架，支持极低比特量化（如2比特），无需重新训练即可重用现有模型检查点，显著提升量化效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM量化已逼近单比特理论极限，而复值模型（如iFairy）虽在低比特表示上更有优势，但需要从头训练，无法利用现有庞大的预训练实值基础模型生态系统。

Method: 1) 证明实值与广泛线性复值映射之间的无损数学等价性，将标准Transformer转换到复域；2) 采用相位感知量化方案，使用四次单位根的高效码本；3) 引入递归残差量化机制，迭代最小化量化误差，实现无乘法累加的高效推理。

Result: Fairy2i将LLaMA-2 7B模型在有效2比特精度下的性能恢复到接近全精度基线水平，显著优于最先进的实值二值化和三值化量化方法。

Conclusion: 该工作弥合了复值算术表示效率与预训练模型实际实用性之间的差距，为在商用硬件上进行高效推理开辟了新途径。

Abstract: Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 论文分析了当前AI系统的七大核心缺陷，提出需要向基于认知原理的AI（认知自主性）进行范式转变，以实现真正的自主适应和动态行为管理。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在感知、语言、推理和多模态领域取得了快速进展，但现代AI系统在自我监控、自我纠正和自主行为调节方面仍然存在根本性限制。这些限制阻碍了AI实现稳健泛化、终身适应性和真实世界自主性。

Method: 通过识别和分析当代AI模型的七大核心缺陷：缺乏内在自我监控、元认知意识缺失、固定非自适应学习机制、无法重构目标、缺乏表征维护、不足的具身反馈、内在能动性缺失。结合人工智能研究、认知科学和神经科学的见解，进行人工系统与生物认知的比较分析。

Result: 论文指出当前架构（包括深度学习和基于Transformer的系统）存在结构性限制，仅靠规模扩展无法解决这些问题。这些缺陷导致AI系统无法实现真正的自主适应、动态表征管理和目标导向行为。

Conclusion: 主张向基于认知原理的AI（认知自主性）进行范式转变，这种AI能够实现自我导向的适应、动态表征管理和有意图的目标导向行为，同时配备改革性监督机制，确保自主系统保持可解释性、可治理性并与人类价值观保持一致。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [10] [Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games](https://arxiv.org/abs/2512.02358)
*Ran Zhang,Kun Ouyang,Tiancheng Ma,Yida Yang,Dong Fang*

Main category: cs.AI

TL;DR: 使用LLM驱动的生成式智能体模拟MMO游戏玩家行为，为数值系统优化提供低成本、高保真度的离线仿真框架


<details>
  <summary>Details</summary>
Motivation: 传统MMO游戏数值系统优化依赖大规模在线实验或基于统计模型的参数调优，成本高、耗时长且可能破坏玩家体验。现有离线仿真系统保真度有限，无法准确模拟真实玩家的推理和干预反应

Method: 提出基于大语言模型的生成式智能体MMO仿真系统：1) 使用监督微调和强化学习在真实玩家行为数据上训练LLM，使其具备游戏特定领域的决策能力；2) 基于真实游戏日志训练数据驱动的环境模型，重建动态游戏系统

Result: 实验表明系统与真实世界玩家行为高度一致，在干预下能产生合理的因果响应，为数据驱动的数值设计优化提供了可靠、可解释且成本效益高的框架

Conclusion: LLM驱动的生成式智能体仿真系统能够准确模拟MMO游戏玩家行为，为游戏数值系统优化提供了一种高效、保真且可解释的离线解决方案，克服了传统方法的局限性

Abstract: Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.

</details>


### [11] [Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets](https://arxiv.org/abs/2512.02436)
*Agostino Capponi,Alfio Gliozzo,Brian Zhu*

Main category: cs.AI

TL;DR: 论文提出一个AI代理管道，自动聚类预测市场并识别市场间的依赖关系，通过交易策略验证发现的关系具有可操作性。


<details>
  <summary>Details</summary>
Motivation: 预测市场存在碎片化问题，包括重叠问题、隐含等价性和隐藏矛盾，需要系统方法来发现市场间的语义结构关系。

Method: 开发AI代理管道：1) 使用自然语言理解对市场合同文本和元数据进行聚类分析；2) 识别聚类内市场对之间的依赖关系（正相关和负相关）；3) 基于历史Polymarket数据评估关系预测准确性；4) 将发现的关系转化为简单交易策略。

Result: AI识别的市场关系准确率达到60-70%，基于这些关系构建的交易策略在一周时间范围内获得约20%的平均回报。

Conclusion: 代理AI和大语言模型能够有效发现预测市场中潜在的语义结构，这些发现的关系具有实际交易价值，为市场分析提供了新方法。

Abstract: Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.

</details>


### [12] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou,Andre Lin Huikai,Nuo Chen,Yiwei Gong,Bingsheng He*

Main category: cs.AI

TL;DR: PaperDebugger：一个基于多代理的Overleaf插件式学术写作助手，将LLM推理直接集成到LaTeX编辑器中，支持上下文感知的文档操作和修订管理。


<details>
  <summary>Details</summary>
Motivation: 现有AI写作助手与编辑器分离，无法深度交互文档状态、结构和修订历史，无法在LaTeX编辑器（如Overleaf）中实现上下文感知的智能操作。

Method: 开发Chrome扩展插件，采用Kubernetes原生编排层和MCP工具链，实现可靠的双向编辑器同步、细粒度版本控制、安全状态管理、多代理调度和外部工具集成。

Result: 实现了完全集成的工作流，包括本地化编辑、结构化评审、并行代理执行和基于差异的更新，用户界面干扰最小，早期分析显示积极的用户参与度。

Conclusion: PaperDebugger验证了编辑器原生、代理式写作助手的实用性，为学术写作提供了深度集成的AI辅助工具。

Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [13] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2512.02633)
*Mattia Giuri,Mathias Jackermeier,Alessandro Abate*

Main category: cs.AI

TL;DR: 提出一种基于图神经网络编码布尔公式序列的新方法，用于学习遵循任意LTL指令的多任务策略，解决了现有方法在多个高层事件同时发生且复杂交互环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有将LTL指令解释为有限自动机的方法在多个原子命题同时为真且可能复杂交互的环境中表现不足，需要新的方法来处理这种复杂情况。

Method: 提出基于图神经网络（GNN）编码简单布尔公式序列的方法，这些公式直接对应自动机中的转移，从而生成结构化任务表示，用于条件化策略。

Result: 在复杂的基于象棋的环境中进行的实验证明了该方法的优势。

Conclusion: 该方法能够有效处理多个高层事件同时发生且复杂交互的环境，为遵循任意LTL指令的多任务策略学习提供了新的解决方案。

Abstract: Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.

</details>


### [14] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: MIGR框架通过引入模态重要性机制，从情感主导模态开始推理，解决多模态情感理解中的推理漂移问题，显著提升推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的多模态情感理解方法存在推理漂移问题：模型逐渐依赖自身生成的文本而非多模态证据，且解释过度受视觉推理路径影响，导致情感不一致的解释。

Method: 提出模态重要性机制识别情感主导模态，基于此重新组织推理序列，使解释从最关键的情感模态开始。采用两阶段框架：模态对齐监督微调和模态感知奖励优化。

Result: 在DFEW基准测试中，MIGR显著提升推理可靠性，将正确预测但情感不一致解释的比例从18.10%降低到7.37%。

Conclusion: 从情感主导模态开始推理能有效防止早期推理被误导，生成情感基础扎实、因果相关且保持连贯性的解释，证实了模态重要性引导推理的价值。

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [15] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang,Wenyilin Xiao,Songqiao Han,Hailiang Huang*

Main category: cs.AI

TL;DR: StockMem：一个事件-反思双层记忆框架，通过结构化新闻为事件并进行横向整合与纵向追踪，构建事件知识库和因果经验知识库，用于股票价格预测，提供可解释的推理。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测面临市场波动性和对实时事件敏感性的挑战。虽然大语言模型为基于文本的预测提供了新途径，但在金融领域的应用受到噪声新闻数据和文本中缺乏明确答案的限制。通用记忆架构难以识别价格变动的关键驱动因素。

Method: 提出StockMem事件-反思双层记忆框架：1）将新闻结构化处理为事件；2）横向整合整合每日事件；3）纵向追踪捕捉事件演化以提取反映市场预期差异的增量信息；4）构建时序事件知识库；5）通过分析事件-价格动态形成因果经验的反思知识库；6）预测时检索类似历史场景，结合当前事件、增量数据和过去经验进行推理。

Result: 实验表明StockMem优于现有记忆架构，通过追踪影响价格的信息链提供更优且可解释的推理，增强了金融预测中的决策透明度。

Conclusion: StockMem通过事件-反思双层记忆框架有效解决了股票预测中的关键挑战，不仅提升了预测性能，还提供了可追溯的推理过程，增强了金融预测的透明度和可解释性。

Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [16] [Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning](https://arxiv.org/abs/2512.02914)
*Zhonghao He,Tianyi Qiu,Hirokazu Shirado,Maarten Sap*

Main category: cs.AI

TL;DR: 本文提出了一种基于鞅性质的无监督评估框架，用于检测LLM推理中的信念固化现象，发现当前信念能正向预测未来信念更新，表明存在确认偏误而非理性贝叶斯更新。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推理能力有所提升，但迭代推理可能加剧信念固化和确认偏误，而非促进真相寻求。需要系统评估LLM是否真正遵循理性贝叶斯更新原则。

Method: 提出基于鞅性质的无监督回归评分方法（Martingale Score），该性质要求理性信念更新中未来信念的期望值等于当前信念。通过测量违反此性质的程度来评估信念固化。

Result: 在事件预测、价值负载问题和学术论文评审等开放领域，发现信念固化现象普遍存在，当前信念能正向预测未来信念更新。该评分能预测有标签领域的真实准确性。

Conclusion: Martingale Score作为无监督指标，能有效评估LLM推理过程中的真相寻求能力，揭示了当前LLM推理中普遍存在的信念固化问题，而非真正的理性贝叶斯更新。

Abstract: Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.

</details>


### [17] [Invasive Context Engineering to Control Large Language Models](https://arxiv.org/abs/2512.03001)
*Thomas Rivasseau*

Main category: cs.AI

TL;DR: 提出侵入式上下文工程，通过向LLM上下文插入控制语句来增强长上下文下的安全防护，无需模型训练即可防止越狱和恶意行为


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法（偏好训练、提示工程、输入输出过滤）在长上下文场景下效果有限，越狱概率随上下文长度增加而上升，需要更鲁棒的长上下文安全保证

Method: 提出侵入式上下文工程，在LLM上下文中插入控制语句来约束模型行为，并可将该技术推广到思维链过程中防止恶意推理

Result: 该方法不依赖LLM训练，避免了长上下文场景下训练数据不足的问题，为长上下文安全提供了新的解决方案

Conclusion: 侵入式上下文工程是解决LLM长上下文安全问题的有效方法，能部分解决现有安全机制的不足，具有实际应用价值

Abstract: Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [18] [TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models](https://arxiv.org/abs/2512.02402)
*Yunchao Wang,Guodao Sun,Zihang Fu,Zhehao Liu,Kaixing Du,Haidong Gao,Ronghua Liang*

Main category: cs.CL

TL;DR: TaleFrame是一个结合LLM和HCI的故事生成系统，通过将故事分解为实体、事件、关系和故事大纲四个基本单元，实现结构化控制，并提供了直观的交互界面。


<details>
  <summary>Details</summary>
Motivation: 当前故事生成系统缺乏细粒度控制和清晰的输入规范，难以准确将用户意图转化为满意的故事输出，限制了其应用性。

Method: 1. 将故事结构分解为实体、事件、关系和故事大纲四个基本单元；2. 利用Tinystories数据集构建包含9,851个JSON条目的偏好数据集；3. 微调本地Llama模型实现JSON2Story转换；4. 提供直观界面支持拖放、附加和连接等交互操作；5. 在七个维度评估故事并提供改进建议。

Result: 通过定量评估和用户研究证明了TaleFrame的有效性，数据集已在Hugging Face上公开。

Conclusion: TaleFrame通过结构化信息生成故事，实现了对生成过程的精确控制，解决了现有系统在意图翻译和控制粒度方面的不足。

Abstract: With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [19] [Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach](https://arxiv.org/abs/2512.02834)
*Siyuan Yang,Yang Zhang,Haoran He,Ling Pan,Xiu Li,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 提出TACO框架，通过轻量级伪计数估计器在推理时验证动作块，解决VLA模型微调后的推理不稳定性问题


<details>
  <summary>Details</summary>
Motivation: VLA模型在预训练阶段融合了多样化的数据模式，而微调数据集往往包含运动学上不理想或次优的演示数据，导致存在与下游任务成功动作模式无关的冗余动作模式。这造成了预训练VLA模型在监督微调后，在不同采样噪声下表现出推理时脆弱性。

Method: 提出TACO（测试时缩放）框架，使用轻量级伪计数估计器作为动作块的高保真验证器。VLA模型集成TACO后，可以从所有采样的动作块中执行具有最大伪计数的动作，从而防止分布偏移，同时保持VLA的泛化能力（约束仅在推理时应用）。该方法类似于离线强化学习中的经典反探索原则，且无需梯度计算，相比RL更新具有显著计算优势。

Result: 在四个仿真基准（RoboTwin2.0、Robotwin、LIBERO、SimplerEnv）和双臂平台上进行了广泛实验，结果表明该方法显著提高了下游任务适应中的推理稳定性和成功率。

Conclusion: TACO框架有效解决了VLA模型微调后的推理不稳定性问题，通过测试时动作验证防止分布偏移，同时保持模型的泛化能力，为基于流匹配或扩散目标的VLA模型提供了一种高效的下游任务适应方案。

Abstract: Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.

</details>
