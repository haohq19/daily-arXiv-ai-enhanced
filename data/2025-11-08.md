<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出了一种无监督、无需专家标注的医学影像异常检测框架，通过增量扩展正常样本集，结合轻量级适配器和不确定性门控机制，在多个医学影像数据集上显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的未知异常检测面临标注数据稀缺和专家监督成本高的问题，需要开发无需异常标签的无监督方法。

Method: 使用冻结的预训练视觉骨干网络，添加微小卷积适配器进行快速领域适应。通过k近邻异常评分和双重概率门控机制（距离z-score阈值和SWAG认知不确定性边界）安全地增量扩展正常样本集。

Result: 在COVID-CXR数据集上ROC-AUC从0.9489提升至0.9982，F1从0.8048提升至0.9746；在肺炎CXR数据集上ROC-AUC从0.6834提升至0.8968；在脑部MRI ND-5数据集上ROC-AUC从0.6041提升至0.7269，PR-AUC从0.7539提升至0.8211。

Conclusion: 该框架在标签稀缺的医学影像应用中表现出高效性和有效性，能够稳定地改进正常性概念的定义。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [2] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 提出一种两阶段方法，结合体素稀疏化和子流形稀疏卷积网络，用于高分辨率3D医学图像肿瘤分割，在保持高精度的同时显著降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 放射图像中肿瘤的精确分割是专业且耗时的任务，目前是临床环境中定量分析的瓶颈。传统卷积神经网络处理3D扫描时面临计算资源不足的问题，需要降采样或使用图像块。

Method: 两阶段方法：第一阶段进行体素稀疏化，第二阶段使用子流形稀疏卷积网络。该方法允许使用高分辨率输入和原生3D模型架构进行分割。

Result: 在KiTS23挑战赛的肾癌CT图像上，获得了与挑战赛优胜者竞争的结果：肾脏+肿块Dice相似系数95.8%，肿瘤+囊肿85.7%，单独肿瘤80.3%。计算效率显著提升，推理时间减少60%，VRAM使用减少75%。

Conclusion: 该方法在保持最先进精度的同时，显著减少了GPU内存和时间的计算资源需求，为临床环境中的自动化肿瘤分割提供了可行的解决方案。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [3] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出从反应式任务驱动系统转向空间超感知的新范式，包含语义感知、流式事件认知、隐式3D空间认知和预测性世界建模四个阶段，并开发了VSI-SUPER基准测试来推动这一领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能系统主要关注反应式任务驱动方法，缺乏真正的空间认知和世界建模能力，需要转向更全面的超感知范式。

Method: 提出空间超感知的四阶段框架，开发VSI-SUPER基准测试（包含VSR和VSC任务），训练Cambrian-S模型，并引入基于预测误差的自监督下一潜在帧预测器方法。

Result: 通过VSI-590K数据训练，在VSI-Bench上获得+30%绝对提升，但在VSI-SUPER上表现仍有限；预测性感知方法显著优于领先的专有基线模型。

Conclusion: 仅靠规模扩展不足以实现空间超感知，需要能够预测、选择和组织经验的模型，预测性感知是前进的方向。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 使用连续血糖监测和可穿戴技术结合机器学习，可以从高分辨率血糖数据中预测胰岛素抵抗和β细胞功能，实现个性化代谢分型，为精准糖尿病预防提供新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的糖尿病和糖尿病前期分类使用静态血糖阈值，掩盖了由胰岛素抵抗、β细胞功能障碍和肠促胰岛素缺乏引起的病理生理异质性，需要更精确的代谢分型方法。

Method: 利用连续血糖监测和可穿戴技术收集高分辨率血糖数据，结合机器学习模型预测肌肉胰岛素抵抗和β细胞功能，分析个体对标准化餐食的餐后血糖反应，并整合饮食、睡眠和体力活动模式数据。

Result: 研究表明机器学习模型能准确预测金标准胰岛素抵抗和β细胞功能指标，个体餐后血糖反应可作为代谢亚型生物标志物，生活方式模式与特定代谢功能障碍相关，饮食干预效果具有表型依赖性。

Conclusion: 连续血糖监测可将早期血糖异常解构为可操作的亚表型，超越简单血糖控制，为针对个体核心代谢缺陷的精准营养、行为和药物干预策略铺平道路。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [5] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 提出了首个面向边缘设备LLM部署的自回归感知分割计算框架，通过混合精度量化、中间压缩和联合优化，在资源受限的IoT设备上实现高效推理。


<details>
  <summary>Details</summary>
Motivation: LLMs在资源受限的IoT设备上部署不切实际，现有分割计算方法无法解决自回归推理的独特挑战，特别是迭代令牌生成和扩展的KV缓存需求。

Method: 开发了三点关键技术：1) 一点分割压缩(OPSC)混合精度量化方案；2) 两阶段中间压缩管道(TS+TAB-Q)；3) 统一优化框架联合选择分割点、量化设置和序列长度。

Result: 在多种LLM和硬件平台上评估，相比SmoothQuant、OmniQuant和Atom等最先进方法，实现了1.49倍推理加速和显著通信开销减少，同时保持或提高模型精度。

Conclusion: 该框架成功解决了边缘设备上LLM部署的关键挑战，为资源受限环境下的高效自回归推理提供了可行解决方案。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [6] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出交换策略优化(EPO)框架，解决半无限安全强化学习问题，通过迭代求解有限约束集的安全RL子问题，自适应调整活动约束集，确保策略性能最优且安全约束严格有界。


<details>
  <summary>Details</summary>
Motivation: 实际应用中安全强化学习常涉及无限约束条件（半无限安全RL），如需要在连续参数空间上强制执行安全条件。现有方法难以处理此类问题。

Method: EPO通过迭代求解有限约束集的安全RL子问题，采用约束扩展和删除机制自适应调整活动集：违反预定义容差的约束被添加，拉格朗日乘子为零的约束被移除。

Result: 理论分析表明，在温和假设下，通过EPO训练的策略性能接近最优解，且全局约束违反严格保持在预定界限内。

Conclusion: EPO为半无限安全RL问题提供了有效的算法框架，能够平衡策略性能与安全约束，防止工作集无控制增长，支持有效的策略训练。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [7] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出基于Transformer的纵向建模方法，用于处理异构电子健康记录数据中的临床风险分类问题，解决不规则时间模式、大模态差异和复杂语义结构等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决临床风险分类中异构EHR数据面临的挑战：不规则时间模式、大模态差异和复杂语义结构，以实现更准确的风险识别。

Method: 采用特征嵌入层统一表示结构化和非结构化数据，引入可学习的时间编码机制处理不均匀采样间隔，使用多头自注意力结构进行全局依赖建模，设计语义加权池化模块自适应分配医疗事件重要性。

Result: 实验结果表明，该模型在准确率、召回率、精确率和F1分数上优于传统机器学习和时序深度学习模型，在多源异构EHR环境中实现稳定精确的风险识别。

Conclusion: 该方法为临床智能决策提供了一个高效可靠的框架，能够有效处理异构EHR数据并实现准确的临床风险分类。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [8] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出了一种用于关系深度学习的Relational Graph Perceiver (RGP)模型，通过时间子图采样器和交叉注意力潜在瓶颈来整合时空依赖关系，支持多任务预测。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，将时间信息仅作为过滤约束而非建模信号，且通常只支持单任务预测。需要能够整合长距离时空依赖并支持多任务的关系深度学习模型。

Method: 引入时间子图采样器捕获时间相关关系，提出RGP图变换器架构，使用交叉注意力潜在瓶颈整合结构和时间上下文信息，并采用灵活的解码器支持多任务联合学习。

Result: 在RelBench、SALT和CTU数据集上的实验表明，RGP实现了最先进的性能。

Conclusion: RGP为关系深度学习提供了一个通用且可扩展的解决方案，能够支持多样化的预测任务。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [9] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文提出在无线时间触发联邦学习系统中引入自适应模型剪枝，通过联合优化剪枝率和带宽分配来最小化训练损失并保证学习延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习网络包含大量用户设备，但无线带宽有限，导致学习延迟和通信开销问题。传统时间触发联邦学习按固定时间间隔分层用户，但面临通信瓶颈。

Method: 基于模型剪枝进行收敛分析，推导梯度l2范数的收敛上界，建立剪枝率和带宽分配的联合优化问题，利用KKT条件推导闭式解。

Result: 仿真结果表明，模型剪枝可减少40%的通信成本，同时保持模型性能不变。

Conclusion: 自适应模型剪枝能有效解决无线联邦学习中的通信瓶颈问题，在保证性能的同时显著降低通信开销。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [10] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种灰盒三维临近预报框架，结合物理约束神经网络与数据驱动学习，直接处理三维雷达反射率数据，实现更准确的极端降水预报。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：数值天气预报太慢太粗糙，外推法和纯数据驱动模型存在误差累积和过度平滑问题，二维雷达方法丢弃了关键的垂直信息。

Method: 使用物理约束神经网络学习垂直变化的3D平流场，参数化空间变化的扩散，引入布朗运动启发的随机项表示未解析运动，残差分支捕捉小尺度对流启动和微物理变化。

Result: 在长达三小时的预报时间内实现更准确的降水预报，在160名气象学家的盲评中57%的情况下排名第一。

Conclusion: 通过恢复完整的三维动力学并保持物理一致性，为极端降水的熟练可靠临近预报提供了可扩展且稳健的途径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: SSPO提出句子级重要性比率，在GRPO和GSPO之间取得平衡，避免训练崩溃和高方差，同时防止整个响应被裁剪机制丢弃，在五个数据集上平均得分46.57，优于GRPO和GSPO。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO存在不稳定策略更新问题，GSPO存在采样数据利用率低的问题，需要一种平衡的方法来解决这些问题。

Method: SSPO应用句子级重要性比率，并在PPO-CLIP中使用句子熵来稳定调整裁剪边界，鼓励高熵token探索，缩小低熵token的裁剪范围。

Result: SSPO在五个数据集上平均得分46.57，超过GRPO(43.01)和GSPO(44.42)，在三个数据集上达到最先进性能。

Conclusion: SSPO通过采用GSPO的优点但拒绝其缺点，有效利用了生成数据，在强化学习验证奖励中表现出色。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [12] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: LLM代码助手生成的漏洞在网络安全中日益重要，但最新开源模型仍存在早期报告的漏洞场景，安全-功能权衡阻碍了有效修复。作者提出新的严重性度量Prompt Exposure(PE)和Model Exposure(ME)来评估漏洞风险。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的代码助手在软件开发中的关键作用增强，其生成的漏洞对网络安全的影响也日益重要。现有安全基准和改进方法对广泛使用的编码LLM的实际影响尚不明确。

Method: 引入新的严重性度量Prompt Exposure(PE)，综合考虑漏洞严重性、生成概率和诱导漏洞代码生成的提示表述。基于PE定义Model Exposure(ME)评分，指示模型生成漏洞的严重性和普遍性。

Result: 研究表明即使最新的开源模型在现实使用环境中仍对早期报告的漏洞场景存在脆弱性，安全-功能权衡阻碍了漏洞的有效修补。

Conclusion: 需要新的严重性度量来鼓励缓解最严重和普遍的漏洞，PE和ME评分有助于评估和改善LLM生成代码的安全性。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>
