{"id": "2508.02947", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02947", "abs": "https://arxiv.org/abs/2508.02947", "authors": ["M Tanjid Hasan Tonmoy", "Rahath Malladi", "Kaustubh Singh", "Forsad Al Hossain", "Rajesh Gupta", "Andr\u00e9s E. Tejada-Mart\u00ednez", "Tauhidur Rahman"], "title": "AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed", "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025. Author Accepted Manuscript", "summary": "Indoor air quality plays an essential role in the safety and well-being of\noccupants, especially in the context of airborne diseases. This paper\nintroduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor\nair purification systems through a robotic cough emulator testbed and a\ndigital-twins-based aerosol residence time analysis. Current portable air\nfilters often overlook the concentrations of respiratory aerosols generated by\ncoughs, posing a risk, particularly in high-exposure environments like\nhealthcare facilities and public spaces. To address this gap, we present a\nrobotic dual-agent physical emulator comprising a maneuverable mannequin\nsimulating cough events and a portable air purifier autonomously responding to\naerosols. The generated data from this emulator trains a digital twins model,\ncombining a physics-based compartment model with a machine learning approach,\nusing Long Short-Term Memory (LSTM) networks and graph convolution layers.\nExperimental results demonstrate the model's ability to predict aerosol\nconcentration dynamics with a mean residence time prediction error within 35\nseconds. The proposed system's real-time intervention strategies outperform\nstatic air filter placement, showcasing its potential in mitigating airborne\npathogen risks.", "AI": {"tldr": "AeroSafe\u901a\u8fc7\u673a\u5668\u4eba\u54b3\u55fd\u6a21\u62df\u5668\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u63d0\u5347\u5ba4\u5185\u7a7a\u6c14\u51c0\u5316\u6548\u679c\uff0c\u9884\u6d4b\u6c14\u6eb6\u80f6\u6d53\u5ea6\u52a8\u6001\uff0c\u5b9e\u65f6\u5e72\u9884\u7b56\u7565\u4f18\u4e8e\u9759\u6001\u8fc7\u6ee4\u5668\u3002", "motivation": "\u5f53\u524d\u4fbf\u643a\u5f0f\u7a7a\u6c14\u8fc7\u6ee4\u5668\u5ffd\u89c6\u54b3\u55fd\u4ea7\u751f\u7684\u6c14\u6eb6\u80f6\u6d53\u5ea6\uff0c\u5c24\u5176\u5728\u533b\u7597\u548c\u516c\u5171\u573a\u6240\u5b58\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u53cc\u4ee3\u7406\u6a21\u62df\u5668\uff08\u6a21\u62df\u54b3\u55fd\u548c\u7a7a\u6c14\u51c0\u5316\u5668\u54cd\u5e94\uff09\u8bad\u7ec3\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548cLSTM\u7f51\u7edc\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u6c14\u6eb6\u80f6\u6d53\u5ea6\u52a8\u6001\uff0c\u5e73\u5747\u505c\u7559\u65f6\u95f4\u8bef\u5dee\u572835\u79d2\u5185\uff0c\u5b9e\u65f6\u5e72\u9884\u7b56\u7565\u4f18\u4e8e\u9759\u6001\u8fc7\u6ee4\u5668\u3002", "conclusion": "AeroSafe\u7cfb\u7edf\u80fd\u6709\u6548\u964d\u4f4e\u7a7a\u6c14\u4f20\u64ad\u75c5\u539f\u4f53\u98ce\u9669\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u73af\u5883\u3002"}}
{"id": "2508.02872", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02872", "abs": "https://arxiv.org/abs/2508.02872", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "title": "Highlight & Summarize: RAG without the jailbreaks", "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHighlight & Summarize (H&S)\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u901a\u8fc7\u5206\u79bb\u7528\u6237\u95ee\u9898\u548c\u751f\u6210LLM\u7684\u4ea4\u4e92\uff0c\u9632\u6b62LLM\u88ab\u6076\u610f\u653b\u51fb\u3002", "motivation": "\u9632\u6b62\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u8d8a\u72f1\u548c\u52ab\u6301\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u88ab\u7ed5\u8fc7\u3002", "method": "H&S\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6d41\u7a0b\u5206\u4e3a\u9ad8\u4eae\u5668\u548c\u603b\u7ed3\u5668\u4e24\u90e8\u5206\uff0c\u907f\u514d\u7528\u6237\u95ee\u9898\u76f4\u63a5\u66b4\u9732\u7ed9\u751f\u6210LLM\u3002", "result": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u9ad8\u4eae\u5668\u65f6\uff0cH&S\u751f\u6210\u7684\u56de\u7b54\u5728\u6b63\u786e\u6027\u3001\u76f8\u5173\u6027\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6807\u51c6RAG\u6d41\u7a0b\u3002", "conclusion": "H&S\u901a\u8fc7\u8bbe\u8ba1\u6709\u6548\u9632\u6b62\u4e86LLM\u7684\u6076\u610f\u653b\u51fb\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u56de\u7b54\u8d28\u91cf\u3002"}}
{"id": "2508.02829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02829", "abs": "https://arxiv.org/abs/2508.02829", "authors": ["Adam Colton"], "title": "Elucidating the Role of Feature Normalization in IJEPA", "comment": null, "summary": "In the standard image joint embedding predictive architecture (IJEPA),\nfeatures at the output of the teacher encoder are layer normalized (LN) before\nserving as a distillation target for the student encoder and predictor. We\npropose that this feature normalization disrupts the natural energy hierarchy\nof visual tokens, where high-energy tokens (those with larger L2 norms) encode\nsemantically important image regions. LN forces all features to have identical\nL2 norms, effectively equalizing their energies and preventing the model from\nprioritizing semantically rich regions. We find that IJEPA models trained with\nfeature LN exhibit loss maps with significant checkerboard-like artifacts. We\npropose that feature LN be replaced with a DynTanh activation as the latter\nbetter preserves token energies and allows high-energy tokens to greater\ncontribute to the prediction loss. We show that IJEPA trained with feature\nDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard\nartifacts in the loss map. Our empirical results show that our simple\nmodification improves ImageNet linear probe accuracy from 38% to 42.7% for\nViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.\nThese results suggest that preserving natural token energies is crucial for\neffective self-supervised visual representation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5728IJEPA\u67b6\u6784\u4e2d\u7528DynTanh\u6fc0\u6d3b\u66ff\u4ee3\u5c42\u5f52\u4e00\u5316\uff08LN\uff09\uff0c\u4ee5\u4fdd\u7559\u89c6\u89c9\u6807\u8bb0\u7684\u81ea\u7136\u80fd\u91cf\u5c42\u7ea7\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5c42\u5f52\u4e00\u5316\uff08LN\uff09\u7834\u574f\u4e86\u89c6\u89c9\u6807\u8bb0\u7684\u81ea\u7136\u80fd\u91cf\u5c42\u7ea7\uff0c\u5bfc\u81f4\u8bed\u4e49\u91cd\u8981\u533a\u57df\u65e0\u6cd5\u88ab\u4f18\u5148\u5904\u7406\uff0c\u5e76\u5f15\u5165\u68cb\u76d8\u72b6\u4f2a\u5f71\u3002", "method": "\u7528DynTanh\u6fc0\u6d3b\u66ff\u6362LN\uff0c\u4ee5\u4fdd\u7559\u6807\u8bb0\u80fd\u91cf\u5e76\u8ba9\u9ad8\u80fd\u91cf\u6807\u8bb0\u5bf9\u9884\u6d4b\u635f\u5931\u8d21\u732e\u66f4\u5927\u3002", "result": "\u6539\u8fdb\u540e\u6a21\u578b\u5728ImageNet\u7ebf\u6027\u63a2\u9488\u51c6\u786e\u7387\u4ece38%\u63d0\u5347\u81f342.7%\uff0c\u5e76\u5728NYU Depth V2\u4e0a\u964d\u4f4eRMSE 0.08\u3002", "conclusion": "\u4fdd\u7559\u81ea\u7136\u6807\u8bb0\u80fd\u91cf\u5bf9\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.02749", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02749", "abs": "https://arxiv.org/abs/2508.02749", "authors": ["Lu Gao", "Ke Yu", "Pan Lu"], "title": "Considering Spatial Structure of the Road Network in Pavement Deterioration Modeling", "comment": null, "summary": "Pavement deterioration modeling is important in providing information\nregarding the future state of the road network and in determining the needs of\npreventive maintenance or rehabilitation treatments. This research incorporated\nspatial dependence of road network into pavement deterioration modeling through\na graph neural network (GNN). The key motivation of using a GNN for pavement\nperformance modeling is the ability to easily and directly exploit the rich\nstructural information in the network. This paper explored if considering\nspatial structure of the road network will improve the prediction performance\nof the deterioration models. The data used in this research comprises a large\npavement condition data set with more than a half million observations taken\nfrom the Pavement Management Information System (PMIS) maintained by the Texas\nDepartment of Transportation. The promising comparison results indicates that\npavement deterioration prediction models perform better when spatial\nrelationship is considered.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5c06\u9053\u8def\u7f51\u7edc\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u7eb3\u5165\u8def\u9762\u9000\u5316\u5efa\u6a21\uff0c\u53d1\u73b0\u8003\u8651\u7a7a\u95f4\u7ed3\u6784\u80fd\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5229\u7528GNN\u80fd\u591f\u76f4\u63a5\u5229\u7528\u9053\u8def\u7f51\u7edc\u7684\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\uff0c\u4ece\u800c\u6539\u8fdb\u8def\u9762\u6027\u80fd\u5efa\u6a21\u3002", "method": "\u4f7f\u7528GNN\u5efa\u6a21\u9053\u8def\u7f51\u7edc\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u57fa\u4e8e\u5fb7\u514b\u8428\u65af\u5dde\u4ea4\u901a\u90e8\u7684\u8def\u9762\u7ba1\u7406\u4fe1\u606f\u7cfb\u7edf\uff08PMIS\uff09\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8003\u8651\u7a7a\u95f4\u5173\u7cfb\u7684\u8def\u9762\u9000\u5316\u9884\u6d4b\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7a7a\u95f4\u7ed3\u6784\u5bf9\u8def\u9762\u9000\u5316\u5efa\u6a21\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0cGNN\u662f\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2508.03110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03110", "abs": "https://arxiv.org/abs/2508.03110", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5TPARAG\uff0c\u901a\u8fc7\u4f18\u5316\u6076\u610f\u6587\u672c\u5728\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u7684\u653b\u51fb\u6548\u679c\uff0c\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1RAG\u6846\u67b6\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u4f46\u5176\u96c6\u6210\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728\u6548\u679c\u548c\u9002\u7528\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTPARAG\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u767d\u76d2\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u8fed\u4ee3\u4f18\u5316\u6076\u610f\u6587\u672c\uff0c\u9488\u5bf9\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u8fdb\u884c\u653b\u51fb\u3002", "result": "\u5728\u5f00\u653e\u57dfQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTPARAG\u5728\u68c0\u7d22\u548c\u7aef\u5230\u7aef\u653b\u51fb\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u4e3a\u63d0\u5347\u5176\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.03137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03137", "abs": "https://arxiv.org/abs/2508.03137", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "title": "Long Story Generation via Knowledge Graph and Literary Theory", "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6545\u4e8b\u751f\u6210\u7ed3\u6784\uff0c\u901a\u8fc7\u957f\u77ed\u671f\u8bb0\u5fc6\u5b58\u50a8\u548c\u6545\u4e8b\u4e3b\u9898\u969c\u788d\u6846\u67b6\u89e3\u51b3\u4e3b\u9898\u6f02\u79fb\u548c\u903b\u8f91\u4e0d\u8fde\u8d2f\u95ee\u9898\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u957f\u6545\u4e8b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u5927\u7eb2\u751f\u6210\u65b9\u6cd5\u4e2d\u4e3b\u9898\u6f02\u79fb\u548c\u903b\u8f91\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u957f\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7ed3\u6784\uff0c\u7ed3\u5408\u957f\u77ed\u671f\u8bb0\u5fc6\u5b58\u50a8\u9632\u6b62\u4e3b\u9898\u6f02\u79fb\uff0c\u8bbe\u8ba1\u6545\u4e8b\u4e3b\u9898\u969c\u788d\u6846\u67b6\u589e\u5f3a\u6545\u4e8b\u5438\u5f15\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6a21\u62df\u4f5c\u5bb6-\u8bfb\u8005\u4e92\u52a8\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u957f\u6545\u4e8b\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u548c\u8bb0\u5fc6\u5b58\u50a8\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6545\u4e8b\u7684\u8fde\u8d2f\u6027\u548c\u5438\u5f15\u529b\u3002"}}
{"id": "2508.03009", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03009", "abs": "https://arxiv.org/abs/2508.03009", "authors": ["Xuyi Yang", "Wenhao Zhang", "Hongbo Jin", "Lin Liu", "Hongbo Xu", "Yongwei Nie", "Fei Yu", "Fei Ma"], "title": "Enhancing Long Video Question Answering with Scene-Localized Frame Grouping", "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) often perform poorly in long\nvideo understanding, primarily due to resource limitations that prevent them\nfrom processing all video frames and their associated information. Efficiently\nextracting relevant information becomes a challenging task. Existing frameworks\nand evaluation tasks focus on identifying specific frames containing core\nobjects from a large number of irrelevant frames, which does not align with the\npractical needs of real-world applications. To address this issue, we propose a\nnew scenario under the video question-answering task, SceneQA, which emphasizes\nscene-based detail perception and reasoning abilities. And we develop the LVSQA\ndataset to support the SceneQA task, which is built upon carefully selected\nvideos from LVBench and contains a new collection of question-answer pairs to\npromote a more fair evaluation of MLLMs' scene perception abilities in long\nvideos. Inspired by human cognition, we introduce a novel method called SLFG.\nThe core idea of SLFG is to combine individual frames into semantically\ncoherent scene frames. By leveraging scene localization methods and dynamic\nframe reassembly mechanisms, SLFG significantly enhances the understanding\ncapabilities of existing MLLMs in long videos. SLFG requires no modification to\nthe original model architecture and boasts excellent plug-and-play usability.\nExperimental results show that this method performs exceptionally well in\nseveral long video benchmark tests. Code and dataset will be released at\nhttp://www.slfg.pkuzwh.cn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSLFG\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u5e27\u7ec4\u5408\u6210\u8bed\u4e49\u8fde\u8d2f\u7684\u573a\u666f\u5e27\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86LVSQA\u6570\u636e\u96c6\u652f\u6301SceneQA\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8d44\u6e90\u9650\u5236\u65e0\u6cd5\u5904\u7406\u6240\u6709\u5e27\u4fe1\u606f\uff0c\u4e14\u73b0\u6709\u6846\u67b6\u4e0d\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u63d0\u51faSLFG\u65b9\u6cd5\uff0c\u7ed3\u5408\u573a\u666f\u5b9a\u4f4d\u548c\u52a8\u6001\u5e27\u91cd\u7ec4\u673a\u5236\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSLFG\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SLFG\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.02887", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02887", "abs": "https://arxiv.org/abs/2508.02887", "authors": ["Jialin Zheng", "Haoyu Wang", "Yangbin Zeng", "Di Mou", "Xin Zhang", "Hong Li", "Sergio Vazquez", "Leopoldo G. Franquelo"], "title": "Physics-Embedded Neural ODEs for Sim2Real Edge Digital Twins of Hybrid Power Electronics Systems", "comment": null, "summary": "Edge Digital Twins (EDTs) are crucial for monitoring and control of Power\nElectronics Systems (PES). However, existing modeling approaches struggle to\nconsistently capture continuously evolving hybrid dynamics that are inherent in\nPES, degrading Sim-to-Real generalization on resource-constrained edge devices.\nTo address these challenges, this paper proposes a Physics-Embedded Neural ODEs\n(PENODE) that (i) embeds the hybrid operating mechanism as an event automaton\nto explicitly govern discrete switching and (ii) injects known governing ODE\ncomponents directly into the neural parameterization of unmodeled dynamics.\nThis unified design yields a differentiable end-to-end trainable architecture\nthat preserves physical interpretability while reducing redundancy, and it\nsupports a cloud-to-edge toolchain for efficient FPGA deployment. Experimental\nresults demonstrate that PENODE achieves significantly higher accuracy in\nbenchmarks in white-box, gray-box, and black-box scenarios, with a 75%\nreduction in neuron count, validating that the proposed PENODE maintains\nphysical interpretability, efficient edge deployment, and real-time control\nenhancement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5d4c\u5165\u795e\u7ecfODE\uff08PENODE\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u529b\u7535\u5b50\u7cfb\u7edf\uff08PES\uff09\u4e2d\u6df7\u5408\u52a8\u6001\u5efa\u6a21\u7684\u6311\u6218\uff0c\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7535\u529b\u7535\u5b50\u7cfb\u7edf\u4e2d\u6301\u7eed\u6f14\u53d8\u7684\u6df7\u5408\u52a8\u6001\uff0c\u5bfc\u81f4\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002", "method": "PENODE\u901a\u8fc7\u5d4c\u5165\u6df7\u5408\u64cd\u4f5c\u673a\u5236\u4f5c\u4e3a\u4e8b\u4ef6\u81ea\u52a8\u673a\u6765\u663e\u5f0f\u63a7\u5236\u79bb\u6563\u5207\u6362\uff0c\u5e76\u5c06\u5df2\u77e5\u7684ODE\u7ec4\u4ef6\u76f4\u63a5\u6ce8\u5165\u672a\u5efa\u6a21\u52a8\u6001\u7684\u795e\u7ecf\u53c2\u6570\u5316\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPENODE\u5728\u767d\u76d2\u3001\u7070\u76d2\u548c\u9ed1\u76d2\u573a\u666f\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u795e\u7ecf\u5143\u6570\u91cf\u51cf\u5c11\u4e8675%\u3002", "conclusion": "PENODE\u5728\u4fdd\u6301\u7269\u7406\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fb9\u7f18\u90e8\u7f72\u548c\u5b9e\u65f6\u63a7\u5236\u589e\u5f3a\u3002"}}
{"id": "2508.03341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03341", "abs": "https://arxiv.org/abs/2508.03341", "authors": ["Jiayan Nan", "Wenquan Ma", "Wenlong Wu", "Yize Chen"], "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts.", "AI": {"tldr": "Nemori\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\u548c\u9884\u6d4b\u6821\u51c6\u539f\u5219\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u8bb0\u5fc6\u6301\u4e45\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u6301\u4e45\u8bb0\u5fc6\u80fd\u529b\uff0c\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u88ab\u52a8\u89c4\u5219\uff0c\u9650\u5236\u4e86\u5176\u5b66\u4e60\u548c\u8fdb\u5316\u80fd\u529b\u3002", "method": "Nemori\u91c7\u7528\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\uff08\u57fa\u4e8e\u4e8b\u4ef6\u5206\u5272\u7406\u8bba\uff09\u548c\u9884\u6d4b\u6821\u51c6\u539f\u5219\uff08\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\uff09\uff0c\u5b9e\u73b0\u8bed\u4e49\u8fde\u8d2f\u7684\u8bb0\u5fc6\u7ec4\u7ec7\u548c\u81ea\u9002\u5e94\u77e5\u8bc6\u8fdb\u5316\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNemori\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Nemori\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u957f\u671f\u52a8\u6001\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u539f\u5219\u63d0\u5347\u4e86\u8bb0\u5fc6\u548c\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2508.03094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03094", "abs": "https://arxiv.org/abs/2508.03094", "authors": ["Jiantao Tan", "Peixian Ma", "Kanghao Chen", "Zhiming Dai", "Ruixuan Wang"], "title": "Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts", "comment": null, "summary": "Continual learning is essential for medical image classification systems to\nadapt to dynamically evolving clinical environments. The integration of\nmultimodal information can significantly enhance continual learning of image\nclasses. However, while existing approaches do utilize textual modality\ninformation, they solely rely on simplistic templates with a class name,\nthereby neglecting richer semantic information. To address these limitations,\nwe propose a novel framework that harnesses visual concepts generated by large\nlanguage models (LLMs) as discriminative semantic guidance. Our method\ndynamically constructs a visual concept pool with a similarity-based filtering\nmechanism to prevent redundancy. Then, to integrate the concepts into the\ncontinual learning process, we employ a cross-modal image-concept attention\nmodule, coupled with an attention loss. Through attention, the module can\nleverage the semantic knowledge from relevant visual concepts and produce\nclass-representative fused features for classification. Experiments on medical\nand natural image datasets show our method achieves state-of-the-art\nperformance, demonstrating the effectiveness and superiority of our method. We\nwill release the code publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89c6\u89c9\u6982\u5ff5\u4f5c\u4e3a\u8bed\u4e49\u6307\u5bfc\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6a21\u5757\u6574\u5408\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7b80\u5355\u7684\u6587\u672c\u6a21\u677f\uff0c\u5ffd\u7565\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u52a8\u6001\u6784\u5efa\u89c6\u89c9\u6982\u5ff5\u6c60\uff0c\u91c7\u7528\u76f8\u4f3c\u6027\u8fc7\u6ee4\u673a\u5236\u907f\u514d\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u56fe\u50cf-\u6982\u5ff5\u6ce8\u610f\u529b\u6a21\u5757\u6574\u5408\u8bed\u4e49\u77e5\u8bc6\u3002", "result": "\u5728\u533b\u5b66\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u4f18\u8d8a\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.03244", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03244", "abs": "https://arxiv.org/abs/2508.03244", "authors": ["Chuanzhi Xu", "Haoxian Zhou", "Langyi Chen", "Yuk Ying Chung", "Qiang Qu"], "title": "Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution", "comment": null, "summary": "Event cameras offer unparalleled advantages such as high temporal resolution,\nlow latency, and high dynamic range. However, their limited spatial resolution\nposes challenges for fine-grained perception tasks. In this work, we propose an\nultra-lightweight, stream-based event-to-event super-resolution method based on\nSpiking Neural Networks (SNNs), designed for real-time deployment on\nresource-constrained devices. To further reduce model size, we introduce a\nnovel Dual-Forward Polarity-Split Event Encoding strategy that decouples\npositive and negative events into separate forward paths through a shared SNN.\nFurthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss\n(LearnSTPLoss) that adaptively balances temporal, spatial, and polarity\nconsistency using learnable uncertainty-based weights. Experimental results\ndemonstrate that our method achieves competitive super-resolution performance\non multiple datasets while significantly reducing model size and inference\ntime. The lightweight design enables embedding the module into event cameras or\nusing it as an efficient front-end preprocessing for downstream vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u8d85\u8f7b\u91cf\u7ea7\u4e8b\u4ef6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u4f18\u52bf\u660e\u663e\uff0c\u4f46\u5176\u6709\u9650\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u611f\u77e5\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u53cc\u5411\u524d\u6781\u6027\u5206\u5272\u4e8b\u4ef6\u7f16\u7801\u7b56\u7565\uff0c\u5c06\u6b63\u8d1f\u4e8b\u4ef6\u5206\u79bb\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u5171\u4eabSNN\u5b9e\u73b0\uff1b\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u65f6\u7a7a\u6781\u6027\u611f\u77e5\u635f\u5931\uff08LearnSTPLoss\uff09\u81ea\u9002\u5e94\u5e73\u8861\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u4f7f\u5176\u53ef\u5d4c\u5165\u4e8b\u4ef6\u76f8\u673a\u6216\u4f5c\u4e3a\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u9ad8\u6548\u524d\u7aef\u9884\u5904\u7406\u6a21\u5757\u3002"}}
{"id": "2508.03277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03277", "abs": "https://arxiv.org/abs/2508.03277", "authors": ["Hang Guo", "Qing Zhang", "Zixuan Gao", "Siyuan Yang", "Shulin Peng", "Xiang Tao", "Ting Yu", "Yan Wang", "Qingli Li"], "title": "Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification", "comment": "Accepted by ACMMM'25", "summary": "Accurate prediction of placental diseases via whole slide images (WSIs) is\ncritical for preventing severe maternal and fetal complications. However, WSI\nanalysis presents significant computational challenges due to the massive data\nvolume. Existing WSI classification methods encounter critical limitations: (1)\ninadequate patch selection strategies that either compromise performance or\nfail to sufficiently reduce computational demands, and (2) the loss of global\nhistological context resulting from patch-level processing approaches. To\naddress these challenges, we propose an Efficient multimodal framework for\nPatient-level placental disease Diagnosis, named EmmPD. Our approach introduces\na two-stage patch selection module that combines parameter-free and learnable\ncompression strategies, optimally balancing computational efficiency with\ncritical feature preservation. Additionally, we develop a hybrid multimodal\nfusion module that leverages adaptive graph learning to enhance pathological\nfeature representation and incorporates textual medical reports to enrich\nglobal contextual understanding. Extensive experiments conducted on both a\nself-constructed patient-level Placental dataset and two public datasets\ndemonstrating that our method achieves state-of-the-art diagnostic performance.\nThe code is available at https://github.com/ECNU-MultiDimLab/EmmPD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEmmPD\u7684\u9ad8\u6548\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u9884\u6d4b\u80ce\u76d8\u75be\u75c5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fdd\u7559\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u80ce\u76d8\u75be\u75c5\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u9884\u9632\u6bcd\u5a74\u5e76\u53d1\u75c7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709WSI\u5206\u6790\u65b9\u6cd5\u56e0\u6570\u636e\u91cf\u5927\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4e22\u5931\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8865\u4e01\u9009\u62e9\u6a21\u5757\u548c\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u548c\u6587\u672c\u62a5\u544a\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u81ea\u5efa\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "EmmPD\u6846\u67b6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2508.03324", "categories": ["cs.CV", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03324", "abs": "https://arxiv.org/abs/2508.03324", "authors": ["Satyapreet Singh Yadav", "Chandra Sekhar Seelamantula", "Chetan Singh Thakur"], "title": "Live Demonstration: Neuromorphic Radar for Gesture Recognition", "comment": "Neuromorphic Radar, Hand Gesture Recognition, Event-Driven,\n  Sigma-Delta Encoding, Sparse Representation. Presented in ICASSP 2025 at\n  Hyderabad, India", "summary": "We present a neuromorphic radar framework for real-time, low-power hand\ngesture recognition (HGR) using an event-driven architecture inspired by\nbiological sensing. Our system comprises a 24 GHz Doppler radar front-end and a\ncustom neuromorphic sampler that converts intermediate-frequency (IF) signals\ninto sparse spike-based representations via asynchronous sigma-delta encoding.\nThese events are directly processed by a lightweight neural network deployed on\na Cortex-M0 microcontroller, enabling low-latency inference without requiring\nspectrogram reconstruction. Unlike conventional radar HGR pipelines that\ncontinuously sample and process data, our architecture activates only when\nmeaningful motion is detected, significantly reducing memory, power, and\ncomputation overhead. Evaluated on a dataset of five gestures collected from\nseven users, our system achieves > 85% real-time accuracy. To the best of our\nknowledge, this is the first work that employs bio-inspired asynchronous\nsigma-delta encoding and an event-driven processing framework for radar-based\nHGR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u7684\u795e\u7ecf\u5f62\u6001\u96f7\u8fbe\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7684\u624b\u52bf\u8bc6\u522b\uff08HGR\uff09\u3002", "motivation": "\u4f20\u7edf\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u9700\u8981\u8fde\u7eed\u91c7\u6837\u548c\u5904\u7406\u6570\u636e\uff0c\u5bfc\u81f4\u9ad8\u529f\u8017\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u5f02\u6b65\u7f16\u7801\u548c\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5305\u62ec24 GHz\u591a\u666e\u52d2\u96f7\u8fbe\u524d\u7aef\u548c\u81ea\u5b9a\u4e49\u795e\u7ecf\u5f62\u6001\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u5f02\u6b65sigma-delta\u7f16\u7801\u5c06\u4e2d\u9891\u4fe1\u53f7\u8f6c\u6362\u4e3a\u7a00\u758f\u7684\u57fa\u4e8e\u5c16\u5cf0\u7684\u8868\u793a\uff0c\u5e76\u7531\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5728Cortex-M0\u5fae\u63a7\u5236\u5668\u4e0a\u8fdb\u884c\u5904\u7406\u3002", "result": "\u5728\u4e03\u540d\u7528\u6237\u7684\u4e94\u79cd\u624b\u52bf\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86>85%\u7684\u5b9e\u65f6\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u751f\u7269\u542f\u53d1\u7684\u5f02\u6b65sigma-delta\u7f16\u7801\u548c\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u6846\u67b6\u5e94\u7528\u4e8e\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\u7684\u5de5\u4f5c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u3001\u529f\u8017\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2508.03426", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03426", "abs": "https://arxiv.org/abs/2508.03426", "authors": ["Futian Wang", "Yuhan Qiao", "Xiao Wang", "Fuling Wang", "Yuxiang Zhang", "Dengdi Sun"], "title": "R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation", "comment": null, "summary": "X-ray medical report generation is one of the important applications of\nartificial intelligence in healthcare. With the support of large foundation\nmodels, the quality of medical report generation has significantly improved.\nHowever, challenges such as hallucination and weak disease diagnostic\ncapability still persist. In this paper, we first construct a large-scale\nmulti-modal medical knowledge graph (termed M3KG) based on the ground truth\nmedical report using the GPT-4o. It contains 2477 entities, 3 kinds of\nrelations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert\nPlus dataset. Then, we sample it to obtain multi-granularity semantic graphs\nand use an R-GCN encoder for feature extraction. For the input X-ray image, we\nadopt the Swin-Transformer to extract the vision features and interact with the\nknowledge using cross-attention. The vision tokens are fed into a Q-former and\nretrieved the disease-aware vision tokens using another cross-attention.\nFinally, we adopt the large language model to map the semantic knowledge graph,\ninput X-ray image, and disease-aware vision tokens into language descriptions.\nExtensive experiments on multiple datasets fully validated the effectiveness of\nour proposed knowledge graph and X-ray report generation framework. The source\ncode of this paper will be released on\nhttps://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff08M3KG\uff09\u7684X\u5149\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408GPT-4o\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u901a\u8fc7R-GCN\u548cSwin-Transformer\u63d0\u53d6\u7279\u5f81\uff0c\u6700\u7ec8\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u62a5\u544a\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3X\u5149\u533b\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u75be\u75c5\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u6784\u5efaM3KG\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408R-GCN\u548cSwin-Transformer\u63d0\u53d6\u7279\u5f81\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u62a5\u544a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u77e5\u8bc6\u56fe\u8c31\u548c\u62a5\u544a\u751f\u6210\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86X\u5149\u533b\u5b66\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.03516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03516", "abs": "https://arxiv.org/abs/2508.03516", "authors": ["Shiben Liu", "Mingyue Xu", "Huijie Fan", "Qiang Wang", "Yandong Tang", "Zhi Han"], "title": "Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification", "comment": "9 papges, 6 figures", "summary": "Lifelong person re-identification (LReID) encounters a key challenge:\nbalancing the preservation of old knowledge with adaptation to new information.\nExisting LReID methods typically employ knowledge distillation to enforce\nrepresentation alignment. However, these approaches ignore two crucial aspects:\nspecific distribution awareness and cross-domain unified knowledge learning,\nboth of which are essential for addressing this challenge. To overcome these\nlimitations, we propose a novel distribution-aware knowledge unification and\nassociation (DKUA) framework where domain-style modeling is performed for each\ninstance to propagate domain-specific representations, enhancing\nanti-forgetting and generalization capacity. Specifically, we design a\ndistribution-aware model to transfer instance-level representations of the\ncurrent domain into the domain-specific representations with the different\ndomain styles, preserving learned knowledge without storing old samples. Next,\nwe propose adaptive knowledge consolidation (AKC) to dynamically generate the\nunified representation as a cross-domain representation center. To further\nmitigate forgetting, we develop a unified knowledge association (UKA)\nmechanism, which explores the unified representation as a bridge to explicitly\nmodel inter-domain associations, reducing inter-domain gaps. Finally,\ndistribution-based knowledge transfer (DKT) is proposed to prevent the current\ndomain distribution from deviating from the cross-domain distribution center,\nimproving adaptation capacity. Experimental results show our DKUA outperforms\nthe existing methods by 7.6%/5.3% average mAP/R@1 improvement on\nanti-forgetting and generalization capacity, respectively. Our code will be\npublicly released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u611f\u77e5\u77e5\u8bc6\u7edf\u4e00\u4e0e\u5173\u8054\uff08DKUA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u57df\u98ce\u683c\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u77e5\u8bc6\u6574\u5408\uff0c\u89e3\u51b3\u4e86\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\uff08LReID\uff09\u4e2d\u7684\u65e7\u77e5\u8bc6\u4fdd\u7559\u4e0e\u65b0\u4fe1\u606f\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\uff08LReID\uff09\u9700\u5e73\u8861\u65e7\u77e5\u8bc6\u4fdd\u7559\u4e0e\u65b0\u4fe1\u606f\u9002\u5e94\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u7279\u5b9a\u5206\u5e03\u611f\u77e5\u548c\u8de8\u57df\u7edf\u4e00\u77e5\u8bc6\u5b66\u4e60\u3002", "method": "\u63d0\u51faDKUA\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5e03\u611f\u77e5\u6a21\u578b\u3001\u81ea\u9002\u5e94\u77e5\u8bc6\u6574\u5408\uff08AKC\uff09\u3001\u7edf\u4e00\u77e5\u8bc6\u5173\u8054\uff08UKA\uff09\u548c\u57fa\u4e8e\u5206\u5e03\u7684\u77e5\u8bc6\u8f6c\u79fb\uff08DKT\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDKUA\u5728\u6297\u9057\u5fd8\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u53477.6%\u548c5.3%\u3002", "conclusion": "DKUA\u6709\u6548\u89e3\u51b3\u4e86LReID\u4e2d\u7684\u77e5\u8bc6\u4fdd\u7559\u4e0e\u9002\u5e94\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03609", "abs": "https://arxiv.org/abs/2508.03609", "authors": ["Rodrigo Verschae", "Ignacio Bugueno-Cordova"], "title": "evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition", "comment": null, "summary": "Event-based cameras are bio-inspired vision sensors that asynchronously\ncapture per-pixel intensity changes with microsecond latency, high temporal\nresolution, and high dynamic range, providing valuable information about the\nspatio-temporal dynamics of the scene. In the present work, we propose\nevTransFER, a transfer learning-based framework and architecture for face\nexpression recognition using event-based cameras. The main contribution is a\nfeature extractor designed to encode the spatio-temporal dynamics of faces,\nbuilt by training an adversarial generative method on a different problem\n(facial reconstruction) and then transferring the trained encoder weights to\nthe face expression recognition system. We show that this proposed transfer\nlearning method greatly improves the ability to recognize facial expressions\ncompared to training a network from scratch. In addition, we propose an\narchitecture that incorporates an LSTM to capture longer-term facial expression\ndynamics, and we introduce a new event-based representation, referred to as\nTIE, both of which further improve the results. We evaluate the proposed\nframework on the event-based facial expression database e-CK+ and compare it to\nstate-of-the-art methods. The results show that the proposed framework\nevTransFER achieves a 93.6\\% recognition rate on the e-CK+ database,\nsignificantly improving the accuracy (25.9\\% points or more) when compared to\nstate-of-the-art performance for similar problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86evTransFER\uff0c\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u4eba\u8138\u8868\u60c5\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u5fae\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5176\u7279\u6027\u7684\u9ad8\u6548\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u751f\u6210\u65b9\u6cd5\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u8fc1\u79fb\u81f3\u8868\u60c5\u8bc6\u522b\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408LSTM\u548c\u65b0\u7684TIE\u8868\u793a\u3002", "result": "\u5728e-CK+\u6570\u636e\u5e93\u4e0a\u8fbe\u523093.6%\u7684\u8bc6\u522b\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534725.9%\u4ee5\u4e0a\u3002", "conclusion": "evTransFER\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u4eba\u8138\u8868\u60c5\u8bc6\u522b\u6027\u80fd\u3002"}}
