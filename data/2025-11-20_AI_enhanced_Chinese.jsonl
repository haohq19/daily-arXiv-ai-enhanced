{"id": "2511.14773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14773", "abs": "https://arxiv.org/abs/2511.14773", "authors": ["Joey David"], "title": "Temporal Predictors of Outcome in Reasoning Language Models", "comment": "4 pages, 4 figures", "summary": "The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u5c31\u80fd\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u5373\u4f7f\u9700\u8981\u66f4\u591a\u6b65\u9aa4\u624d\u80fd\u5f97\u51fa\u660e\u786e\u7ed3\u8bba\u3002", "motivation": "\u63a2\u7d22\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e2d\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u4f55\u65f6\u5185\u90e8\u786e\u5b9a\u6700\u7ec8\u7b54\u6848\uff0c\u4ee5\u53ca\u8fd9\u79cd\u65e9\u671f\u9884\u6d4b\u80fd\u529b\u5bf9\u6a21\u578b\u89e3\u91ca\u6027\u548c\u63a8\u7406\u63a7\u5236\u7684\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u524dt\u4e2atoken\u5904\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u6765\u9884\u6d4b\u9690\u85cf\u72b6\u6001\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u6700\u7ec8\u6b63\u786e\u6027\u7684\u65e9\u671f\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u4ec5\u7ecf\u8fc7\u51e0\u4e2a\u63a8\u7406token\u540e\u5c31\u80fd\u9ad8\u5ea6\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u4e14\u96be\u5ea6\u8f83\u9ad8\u7684\u95ee\u9898\u5728\u957f\u94fe\u5f0f\u601d\u7ef4\u4e2d\u5360\u6bd4\u8fc7\u9ad8\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u5728\u65e9\u671f\u9636\u6bb5\u5c31\u5177\u5907\u5185\u90e8\u81ea\u6211\u8bc4\u4f30\u6210\u529f\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u6a21\u578b\u89e3\u91ca\u6027\u548c\u63a8\u7406\u65f6\u63a7\u5236\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.14780", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14780", "abs": "https://arxiv.org/abs/2511.14780", "authors": ["Keith Moore", "Jun W. Kim", "David Lyu", "Jeffrey Heo", "Ehsan Adeli"], "title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents", "comment": "Preprint. Accepted for publication at AIAS 2025", "summary": "We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.", "AI": {"tldr": "Ask WhAI\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u67e5\u548c\u6270\u52a8\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u4fe1\u5ff5\u72b6\u6001\u7684\u7cfb\u7edf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u8bb0\u5f55\u56de\u653e\u4ea4\u4e92\u3001\u67e5\u8be2\u4fe1\u5ff5\u548c\u6ce8\u5165\u53cd\u4e8b\u5b9e\u8bc1\u636e\u6765\u6d4b\u8bd5\u4fe1\u5ff5\u7ed3\u6784\u5bf9\u65b0\u4fe1\u606f\u7684\u54cd\u5e94\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u5f62\u6210\u548c\u8ba4\u77e5\u5b64\u5c9b\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u91cd\u73b0\u7684\u65b9\u6cd5\u6765\u89c2\u5bdf\u548c\u6d4b\u8bd5\u8fd9\u4e9b\u52a8\u6001\u8fc7\u7a0b\uff0c\u8fd9\u5728\u4eba\u7c7b\u4e13\u5bb6\u4e2d\u662f\u4e0d\u53ef\u80fd\u7684\u3002", "method": "\u4f7f\u7528\u5177\u6709\u89d2\u8272\u7279\u5b9a\u5148\u9a8c\u7684LLM\u667a\u80fd\u4f53\u5728\u533b\u7597\u6848\u4f8b\u6a21\u62df\u5668\u4e2d\u4ea4\u4e92\uff0c\u901a\u8fc7\u5171\u4eab\u533b\u7597\u8bb0\u5f55\u548c\u4e0e\u8c03\u89e3\u5458\u4e92\u52a8\uff0c\u5728\u5173\u952e\u8bca\u65ad\u65f6\u523b\u8bbe\u7f6e\u65ad\u70b9\u8fdb\u884c\u4fe1\u5ff5\u67e5\u8be2\u548c\u53cd\u4e8b\u5b9e\u8bc1\u636e\u6ce8\u5165\u3002", "result": "\u667a\u80fd\u4f53\u4fe1\u5ff5\u5f80\u5f80\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u5b66\u79d1\u7acb\u573a\uff0c\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u7ecf\u5178\u7814\u7a76\u548c\u62b5\u5236\u53cd\u8bc1\u636e\uff0c\u8fd9\u4e9b\u4fe1\u5ff5\u53ef\u4ee5\u88ab\u8ffd\u8e2a\u548c\u8d28\u8be2\u3002", "conclusion": "Ask WhAI\u901a\u8fc7\u4f7f\u4fe1\u5ff5\u52a8\u6001\u53ef\u89c1\u548c\u53ef\u6d4b\u8bd5\uff0c\u4e3a\u7814\u7a76\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u5f62\u6210\u548c\u8ba4\u77e5\u5b64\u5c9b\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.14788", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.14788", "abs": "https://arxiv.org/abs/2511.14788", "authors": ["Michele Ronco", "Damien Delforge", "Wiebke S. J\u00e4ger", "Christina Corbane"], "title": "Subnational Geocoding of Global Disasters Using Large Language Models", "comment": null, "summary": "Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5904\u7406\u707e\u5bb3\u6570\u636e\u5e93\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u4f4d\u7f6e\u4fe1\u606f\uff0c\u901a\u8fc7GPT-4o\u6e05\u7406\u6587\u672c\u5e76\u4f7f\u7528\u4e09\u4e2a\u5730\u7406\u4fe1\u606f\u5e93\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u4e3aEM-DAT\u6570\u636e\u96c6\u4e2d\u768414,215\u4e2a\u4e8b\u4ef6\u751f\u6210\u5b50\u56fd\u5bb6\u7ea7\u51e0\u4f55\u6570\u636e\u3002", "motivation": "\u707e\u5bb3\u6570\u636e\u5e93\u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\u901a\u5e38\u4ee5\u975e\u7ed3\u6784\u5316\u6587\u672c\u5f62\u5f0f\u5b58\u5728\uff0c\u5177\u6709\u4e0d\u4e00\u81f4\u7684\u7c92\u5ea6\u548c\u62fc\u5199\uff0c\u96be\u4ee5\u4e0e\u7a7a\u95f4\u6570\u636e\u96c6\u96c6\u6210\uff0c\u8fd9\u963b\u788d\u4e86\u98ce\u9669\u8bc4\u4f30\u548c\u707e\u5bb3\u98ce\u9669\u51cf\u5c11\u5de5\u4f5c\u3002", "method": "\u4f7f\u7528GPT-4o\u5904\u7406\u6e05\u7406\u6587\u672c\u4f4d\u7f6e\u4fe1\u606f\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1GADM\u3001OpenStreetMap\u548cWikidata\u4e09\u4e2a\u72ec\u7acb\u5730\u7406\u4fe1\u606f\u5e93\u6765\u5206\u914d\u51e0\u4f55\u6570\u636e\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u5206\u914d\u53ef\u9760\u6027\u8bc4\u5206\u3002", "result": "\u5e94\u7528\u4e8e2000-2024\u5e74EM-DAT\u6570\u636e\u96c6\uff0c\u6210\u529f\u5730\u7406\u7f16\u780114,215\u4e2a\u4e8b\u4ef6\uff0c\u8986\u76d617,948\u4e2a\u72ec\u7279\u4f4d\u7f6e\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u6db5\u76d6\u6240\u6709\u707e\u5bb3\u7c7b\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86LLMs\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u548c\u7ed3\u6784\u5316\u5730\u7406\u4fe1\u606f\u7684\u6f5c\u529b\uff0c\u4e3a\u76f8\u5173\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.14899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14899", "abs": "https://arxiv.org/abs/2511.14899", "authors": ["Daniel Gilo", "Or Litany"], "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization", "comment": null, "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.", "AI": {"tldr": "\u63d0\u51faInstructMix2Mix\u6846\u67b6\uff0c\u5c062D\u6269\u6563\u6a21\u578b\u7684\u7f16\u8f91\u80fd\u529b\u84b8\u998f\u5230\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u8fdb\u884c\u591a\u89c6\u56fe\u56fe\u50cf\u7f16\u8f91\uff0c\u4fdd\u6301\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9010\u573a\u666f\u795e\u7ecf\u573a\u6216\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u7f16\u8f91\u4e2d\u5bb9\u6613\u4ea7\u751f\u4f2a\u5f71\u548c\u4e0d\u4e00\u81f4\u7f16\u8f91\uff0c\u9700\u8981\u6539\u8fdb\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "method": "\u7528\u591a\u89c6\u56fe\u6269\u6563\u5b66\u751f\u66ff\u6362SDS\u4e2d\u7684\u4f20\u7edf\u795e\u7ecf\u573a\u6574\u5408\u5668\uff0c\u91c7\u7528\u589e\u91cf\u5b66\u751f\u66f4\u65b0\u3001\u4e13\u7528\u6559\u5e08\u566a\u58f0\u8c03\u5ea6\u5668\u548c\u6ce8\u610f\u529b\u4fee\u6539\u6765\u589e\u5f3a\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eI-Mix2Mix\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5355\u5e27\u7f16\u8f91\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c062D\u7f16\u8f91\u80fd\u529b\u84b8\u998f\u5230\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u7f16\u8f91\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2511.15163", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15163", "abs": "https://arxiv.org/abs/2511.15163", "authors": ["Yang Wu", "Rujing Yao", "Tong Zhang", "Yufei Shi", "Zhuoren Jiang", "Zhushan Li", "Xiaozhong Liu"], "title": "Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs", "comment": "AAAI 2026 Workshop", "summary": "Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.", "AI": {"tldr": "TASA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u6570\u5b66\u8f85\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5b66\u751f\u753b\u50cf\u3001\u8bb0\u5fc6\u548c\u9057\u5fd8\u52a8\u6001\u6765\u5b9e\u73b0\u4e2a\u6027\u5316\u6559\u5b66\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u5bfc\u7cfb\u7edf\u672a\u80fd\u6709\u6548\u6355\u6349\u5b66\u751f\u77e5\u8bc6\u968f\u65f6\u95f4\u7684\u52a8\u6001\u6f14\u53d8\uff0c\u5305\u62ec\u719f\u7ec3\u5ea6\u3001\u6982\u5ff5\u5dee\u8ddd\u548c\u9057\u5fd8\u6a21\u5f0f\uff0c\u8fd9\u5728\u9700\u8981\u7cbe\u7ec6\u6821\u51c6\u7684\u6570\u5b66\u8f85\u5bfc\u4e2d\u5c24\u4e3a\u5173\u952e\u3002", "method": "TASA\u7ef4\u62a4\u7ed3\u6784\u5316\u7684\u5b66\u751f\u753b\u50cf\uff08\u8bb0\u5f55\u719f\u7ec3\u5ea6\u6863\u6848\uff09\u548c\u4e8b\u4ef6\u8bb0\u5fc6\uff08\u8bb0\u5f55\u5148\u524d\u5b66\u4e60\u4e92\u52a8\uff09\uff0c\u7ed3\u5408\u8fde\u7eed\u9057\u5fd8\u66f2\u7ebf\u548c\u77e5\u8bc6\u8ffd\u8e2a\u6765\u52a8\u6001\u66f4\u65b0\u5b66\u751f\u638c\u63e1\u72b6\u6001\uff0c\u5e76\u751f\u6210\u60c5\u5883\u9002\u5f53\u3001\u96be\u5ea6\u6821\u51c6\u7684\u95ee\u9898\u548c\u89e3\u91ca\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cTASA\u76f8\u6bd4\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u5b66\u4e60\u6548\u679c\u548c\u66f4\u81ea\u9002\u5e94\u7684\u8f85\u5bfc\u884c\u4e3a\u3002", "conclusion": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u5efa\u6a21\u65f6\u95f4\u9057\u5fd8\u548c\u5b66\u4e60\u8005\u753b\u50cf\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0cTASA\u6846\u67b6\u5c55\u793a\u4e86\u8fd9\u79cd\u6574\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.15274", "categories": ["cs.RO", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.15274", "abs": "https://arxiv.org/abs/2511.15274", "authors": ["Alexander Boldachev"], "title": "Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms", "comment": "22 pages, 8 figures", "summary": "This paper compares two distinct approaches to modeling robotic behavior: imperative Behavior Trees (BTs) and declarative Executable Ontologies (EO), implemented through the boldsea framework. BTs structure behavior hierarchically using control-flow, whereas EO represents the domain as a temporal, event-based semantic graph driven by dataflow rules. We demonstrate that EO achieves comparable reactivity and modularity to BTs through a fundamentally different architecture: replacing polling-based tick execution with event-driven state propagation. We propose that EO offers an alternative framework, moving from procedural programming to semantic domain modeling, to address the semantic-process gap in traditional robotic control. EO supports runtime model modification, full temporal traceability, and a unified representation of data, logic, and interface - features that are difficult or sometimes impossible to achieve with BTs, although BTs excel in established, predictable scenarios. The comparison is grounded in a practical mobile manipulation task. This comparison highlights the respective operational strengths of each approach in dynamic, evolving robotic systems.", "AI": {"tldr": "\u5bf9\u6bd4\u4e86\u6307\u4ee4\u5f0f\u884c\u4e3a\u6811(BT)\u548c\u58f0\u660e\u5f0f\u53ef\u6267\u884c\u672c\u4f53(EO)\u4e24\u79cd\u673a\u5668\u4eba\u884c\u4e3a\u5efa\u6a21\u65b9\u6cd5\uff0c\u53d1\u73b0EO\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u72b6\u6001\u4f20\u64ad\u5b9e\u73b0\u4e86\u4e0eBT\u76f8\u5f53\u7684\u54cd\u5e94\u6027\u548c\u6a21\u5757\u6027\uff0c\u540c\u65f6\u652f\u6301\u8fd0\u884c\u65f6\u6a21\u578b\u4fee\u6539\u548c\u5b8c\u6574\u65f6\u95f4\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u8bed\u4e49-\u8fc7\u7a0b\u9e3f\u6c9f\u95ee\u9898\uff0c\u63a2\u7d22\u4ece\u8fc7\u7a0b\u5f0f\u7f16\u7a0b\u5411\u8bed\u4e49\u9886\u57df\u5efa\u6a21\u7684\u8f6c\u53d8\uff0c\u4e3a\u52a8\u6001\u6f14\u8fdb\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u66ff\u4ee3\u6846\u67b6\u3002", "method": "\u901a\u8fc7boldsea\u6846\u67b6\u5b9e\u73b0\u4e24\u79cd\u65b9\u6cd5\uff1aBT\u4f7f\u7528\u5c42\u6b21\u5316\u63a7\u5236\u6d41\u7ed3\u6784\uff0cEO\u4f7f\u7528\u57fa\u4e8e\u6570\u636e\u6d41\u89c4\u5219\u7684\u65f6\u6001\u4e8b\u4ef6\u8bed\u4e49\u56fe\u3002\u5728\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u9645\u5bf9\u6bd4\u3002", "result": "EO\u5b9e\u73b0\u4e86\u4e0eBT\u76f8\u5f53\u7684\u54cd\u5e94\u6027\u548c\u6a21\u5757\u6027\uff0c\u4f46\u901a\u8fc7\u5b8c\u5168\u4e0d\u540c\u7684\u67b6\u6784\uff08\u4e8b\u4ef6\u9a71\u52a8vs\u8f6e\u8be2\u6267\u884c\uff09\u3002EO\u652f\u6301\u8fd0\u884c\u65f6\u6a21\u578b\u4fee\u6539\u3001\u5b8c\u6574\u65f6\u95f4\u53ef\u8ffd\u6eaf\u6027\u4ee5\u53ca\u6570\u636e\u3001\u903b\u8f91\u548c\u63a5\u53e3\u7684\u7edf\u4e00\u8868\u793a\u3002", "conclusion": "EO\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4ece\u8fc7\u7a0b\u5f0f\u7f16\u7a0b\u5411\u8bed\u4e49\u9886\u57df\u5efa\u6a21\u7684\u66ff\u4ee3\u6846\u67b6\uff0c\u7279\u522b\u9002\u5408\u52a8\u6001\u6f14\u8fdb\u7cfb\u7edf\uff0c\u800cBT\u5728\u65e2\u5b9a\u53ef\u9884\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002"}}
{"id": "2511.15112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15112", "abs": "https://arxiv.org/abs/2511.15112", "authors": ["Wei-hsiang Yen", "Lyn Chao-ling Chen"], "title": "Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data", "comment": "Accepted in Taiwan Academic Network Conference (TANET 2025)", "summary": "The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e0e\u60c5\u611f\u5206\u6790\u6574\u5408\u5230\u4f20\u7edf\u5546\u4e1a\u6a21\u5f0f\u5206\u6790\u4e2d\uff0c\u4ee5\u53f0\u79ef\u7535\u4e3a\u7814\u7a76\u5bf9\u8c61\u9884\u6d4b\u53f0\u6e7e\u534a\u5bfc\u4f53\u884c\u4e1a\u8d8b\u52bf\u3002\u901a\u8fc7\u5206\u6790\u53f0\u79ef\u7535\u5b63\u5ea6\u62a5\u544a\u4e2d\u7684\u6587\u672c\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408\u5185\u5916\u90e8\u4e8b\u4ef6\u5e72\u9884\u7684\u60c5\u611f\u5206\u6790\uff0c\u4f7f\u7528LSTM\u6a21\u578b\u8fdb\u884c\u884c\u4e1a\u8d8b\u52bf\u9884\u6d4b\u3002", "motivation": "\u534a\u5bfc\u4f53\u884c\u4e1a\u5e02\u573a\u53d8\u5316\u5feb\u901f\uff0c\u4f20\u7edf\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u53d8\u5316\u6027\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u6536\u96c6\u53f0\u79ef\u7535\u5b63\u5ea6\u62a5\u544a\u4e2d\u7684\u6587\u672c\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u6790\u8003\u8651\u516c\u53f8\u5185\u90e8\u4e8b\u4ef6\u548c\u5916\u90e8\u5168\u7403\u4e8b\u4ef6\u7684\u5e72\u9884\uff0c\u4f7f\u7528\u60c5\u611f\u589e\u5f3a\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8bad\u7ec3LSTM\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u9884\u6d4b\u7ed3\u679c\u63ed\u793a\u4e86\u53f0\u79ef\u7535\u6676\u5706\u6280\u672f\u7684\u663e\u8457\u53d1\u5c55\u548c\u5168\u7403\u5e02\u573a\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u4e0e\u53f0\u79ef\u7535\u4ea7\u54c1\u53d1\u5e03\u65b0\u95fb\u548c\u56fd\u9645\u65b0\u95fb\u76f8\u7b26\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8003\u8651\u5185\u5916\u90e8\u4e8b\u4ef6\u5e72\u9884\uff0c\u5728\u534a\u5bfc\u4f53\u884c\u4e1a\u8d8b\u52bf\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51c6\u786e\uff0c\u4e3a\u7814\u7a76\u548c\u5546\u4e1a\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u534a\u5bfc\u4f53\u884c\u4e1a\u4fe1\u606f\u3002"}}
{"id": "2511.15174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15174", "abs": "https://arxiv.org/abs/2511.15174", "authors": ["Yi Xu", "Zhigang Chen", "Rui Wang", "Yangfan Li", "Fengxiao Tang", "Ming Zhao", "Jiaqi Liu"], "title": "FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model", "comment": "4 figures, 5 tables ,8 pages", "summary": "In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5c11\u6837\u672c\u6545\u969c\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u8d1f\u5dee\u5f02\u9002\u914d\u5668\u548c\u591a\u6837\u6027\u635f\u5931\u89e3\u51b3\u6545\u969c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u8bbe\u5907\u76d1\u6d4b\u4e2d\u6545\u969c\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u96be\u4ee5\u6355\u6349\u6545\u969c\u5206\u5e03\uff0c\u751f\u6210\u7684\u6837\u672c\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u6b63\u8d1f\u5dee\u5f02\u9002\u914d\u5668\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6b63\u5e38\u6570\u636e\u5206\u5e03\u5efa\u6a21\u6b63\u5e38\u4e0e\u6545\u969c\u57df\u7684\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u591a\u6837\u6027\u635f\u5931\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u6545\u969c\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u5de5\u4e1a\u8bbe\u5907\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2511.15529", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15529", "abs": "https://arxiv.org/abs/2511.15529", "authors": ["Yifei Gao", "Hans J. He", "Daniel J. Stilwell", "James McMahon"], "title": "Decentralized Gaussian Process Classification and an Application in Subsea Robotics", "comment": "8 pages, 8 figures, IROS 2025 conference", "summary": "Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668(AUV)\u56e2\u961f\u5b9e\u65f6\u6784\u5efa\u901a\u4fe1\u6210\u529f\u6982\u7387\u5730\u56fe\u7684\u6570\u636e\u5171\u4eab\u7b56\u7565\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5171\u4eab\u901a\u4fe1\u6d4b\u91cf\u6570\u636e\u6765\u5e94\u5bf9\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u5b58\u5728\u8ddd\u79bb\u6709\u9650\u3001\u591a\u5f84\u6548\u5e94\u548c\u4f4e\u5e26\u5bbd\u7b49\u9650\u5236\uff0c\u5bfc\u81f4\u901a\u4fe1\u4e0d\u786e\u5b9a\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u5b9e\u65f6\u5b66\u4e60\u901a\u4fe1\u73af\u5883\uff0c\u6784\u5efa\u901a\u4fe1\u6210\u529f\u6982\u7387\u5730\u56fe\u3002", "method": "\u91c7\u7528\u5206\u6563\u5f0f\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06\u901a\u4fe1\u4e8b\u4ef6\u5206\u4e3a\u6210\u529f\u6216\u5931\u8d25\uff0cAUVs\u9009\u62e9\u6027\u5730\u5171\u4eab\u90e8\u5206\u901a\u4fe1\u6d4b\u91cf\u6570\u636e\u6765\u6784\u5efa\u5730\u56fe\u3002\u4e3b\u8981\u8d21\u732e\u662f\u4e25\u683c\u63a8\u5bfc\u7684\u6570\u636e\u5171\u4eab\u7b56\u7565\u6765\u9009\u62e9\u8981\u5171\u4eab\u7684\u6d4b\u91cf\u6570\u636e\u3002", "result": "\u4f7f\u7528\u5f17\u5409\u5c3c\u4e9a\u7406\u5de5\u5927\u5b66690 AUVs\u56e2\u961f\u6536\u96c6\u7684\u771f\u5b9e\u58f0\u5b66\u901a\u4fe1\u6570\u636e\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u7b56\u7565\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u5171\u4eab\u7b56\u7565\u80fd\u591f\u6709\u6548\u5e2e\u52a9AUV\u56e2\u961f\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u5b9e\u65f6\u6784\u5efa\u901a\u4fe1\u6210\u529f\u6982\u7387\u5730\u56fe\uff0c\u63d0\u9ad8\u56e2\u961f\u534f\u4f5c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.15552", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15552", "abs": "https://arxiv.org/abs/2511.15552", "authors": ["Artem Chervyakov", "Ulyana Isaeva", "Anton Emelyanov", "Artem Safin", "Maria Tikhonova", "Alexander Kharitonov", "Yulia Lyakh", "Petr Surovtsev", "Denis Shevelev Vildan Saburov", "Vasily Konovalov", "Elisei Rykov", "Ivan Sviridov", "Amina Miftakhova", "Ilseyar Alimova", "Alexander Panchenko", "Alexander Kapitanov", "Alena Fenogenova"], "title": "Multimodal Evaluation of Russian-language Architectures", "comment": null, "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.", "AI": {"tldr": "Mera Multi\u662f\u4e00\u4e2a\u9488\u5bf9\u4fc4\u8bed\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b18\u4e2a\u65b0\u6784\u5efa\u7684\u8bc4\u4f30\u4efb\u52a1\uff0c\u6db5\u76d6\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\uff0c\u4e3a\u4fc4\u8bed\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u4fc4\u8bed\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u5145\u5206\u7406\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fc4\u8bed\u73af\u5883\u4e0b\u7684\u667a\u80fd\u3001\u5c40\u9650\u6027\u548c\u98ce\u9669\u3002", "method": "\u6784\u5efa\u4e8618\u4e2a\u5168\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5173\u6ce8\u4fc4\u8bed\u6587\u5316\u548c\u8bed\u8a00\u7279\u6027\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u63d0\u793a\u548c\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9632\u6b62\u57fa\u51c6\u6cc4\u6f0f\u7684\u65b9\u6cd5\uff08\u6c34\u5370\u548c\u79c1\u6709\u96c6\u8bb8\u53ef\uff09\u3002", "result": "\u4e3a\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\uff0c\u5efa\u7acb\u4e86\u901a\u7528\u7684\u591a\u6a21\u6001\u80fd\u529b\u5206\u7c7b\u4f53\u7cfb\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u6784\u5efa\u65af\u62c9\u592b\u8bed\u7cfb\u7b49\u7c7b\u578b\u591a\u6837\u8bed\u8a00\u7684\u591a\u6a21\u6001\u57fa\u51c6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u8bba\uff0c\u586b\u8865\u4e86\u4fc4\u8bed\u591a\u6a21\u6001\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.15248", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15248", "abs": "https://arxiv.org/abs/2511.15248", "authors": ["Kai Yang", "Xin Xu", "Yangkun Chen", "Weijie Liu", "Jiafei Lyu", "Zichuan Lin", "Deheng Ye", "Saiyong Yang"], "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control", "comment": null, "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.", "AI": {"tldr": "\u63d0\u51faEntroPIC\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u4f8b-\u79ef\u5206\u63a7\u5236\u52a8\u6001\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u7684\u635f\u5931\u7cfb\u6570\uff0c\u7a33\u5b9a\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u71b5\u503c\uff0c\u9632\u6b62\u8fc7\u65e9\u6536\u655b\u5230\u6b21\u4f18\u89e3\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u9002\u5f53\u7684\u71b5\u6c34\u5e73\uff0c\u56e0\u4e3a\u6b63\u8d1f\u6837\u672c\u5728\u4e0d\u540c\u6b65\u9aa4\u4e2d\u5bf9\u71b5\u7684\u5f71\u54cd\u65b9\u5f0f\u4e0d\u540c\uff0c\u5bfc\u81f4\u63a2\u7d22\u4e0d\u7a33\u5b9a\u3002", "method": "EntroPIC\u65b9\u6cd5\u4f7f\u7528\u6bd4\u4f8b-\u79ef\u5206\u63a7\u5236\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u7684\u635f\u5931\u7cfb\u6570\uff0c\u4ece\u800c\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u503c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u7ef4\u6301\u671f\u671b\u7684\u71b5\u6c34\u5e73\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u6700\u4f18\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "conclusion": "EntroPIC\u65b9\u6cd5\u901a\u8fc7\u71b5\u7a33\u5b9a\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u957f\u671f\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u9a8c\u8bc1\u3002"}}
{"id": "2511.15117", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.15117", "abs": "https://arxiv.org/abs/2511.15117", "authors": ["Jun-Yi Liu", "Chung-Hao Chen", "Ya-Chi Tsao", "Ssu-Yao Wu", "Yu-Ting Tsao", "Lyn Chao-ling Chen"], "title": "An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring", "comment": "Accepted in the 35th IPPR Conference on Computer Vision, Graphics, and Image Processing (CVGIP2022)", "summary": "In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u89e6\u53d1\u7cfb\u7edf\u6765\u76d1\u6d4b\u8001\u5e74\u4eba\u7684\u8eab\u5fc3\u5065\u5eb7\u72b6\u6001\uff0c\u901a\u8fc7GMM\u80cc\u666f\u5efa\u6a21\u68c0\u6d4b\u8bbf\u5ba2\u548c\u8001\u4eba\u7684\u8fd0\u52a8\u884c\u4e3a\uff0c\u4f7f\u7528SVM\u5206\u6790\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f4\u89c2\u7684\u64cd\u4f5c\u65b9\u5f0f\u8ba9\u8001\u4eba\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u4e0e\u4eb2\u5c5e\u6c9f\u901a\u3002", "motivation": "\u540c\u65f6\u8003\u8651\u8001\u5e74\u4eba\u7684\u8eab\u4f53\u548c\u5fc3\u7406\u72b6\u6001\uff0c\u4e3a\u7f3a\u4e4f\u6280\u672f\u7ecf\u9a8c\u7684\u8001\u5e74\u4eba\u8bbe\u8ba1\u76f4\u89c2\u7684\u6c9f\u901a\u65b9\u5f0f\u3002", "method": "\u91c7\u7528GMM\u80cc\u666f\u5efa\u6a21\u68c0\u6d4b\u8fd0\u52a8\u884c\u4e3a\uff0c\u4f7f\u7528SVM\u673a\u5668\u5b66\u4e60\u5206\u6790\u56fe\u50cf\uff0c\u8bbe\u8ba1\u4e8b\u4ef6\u89e6\u53d1\u7cfb\u7edf\uff08\u770b\u95e8\u72d7\u3001\u5371\u9669\u901a\u77e5\u548c\u7167\u7247\u94fe\u63a5\u4e8b\u4ef6\uff09\u3002", "result": "\u5728\u5bb6\u5ead\u573a\u666f\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c5\u4e2a\u5bb6\u5ead\u53c2\u4e0e\uff0c\u6210\u529f\u68c0\u6d4b\u548c\u8bb0\u5f55\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u76d1\u6d4b\u8001\u5e74\u4eba\u7684\u8eab\u5fc3\u5065\u5eb7\uff0c\u5e76\u901a\u8fc7\u76f4\u89c2\u7684\u64cd\u4f5c\u65b9\u5f0f\u4fc3\u8fdb\u8001\u4eba\u4e0e\u4eb2\u5c5e\u7684\u6c9f\u901a\u3002"}}
{"id": "2511.15197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15197", "abs": "https://arxiv.org/abs/2511.15197", "authors": ["Raghu Vamsi Chittersu", "Yuvraj Singh Rathore", "Pranav Adlinge", "Kunal Swami"], "title": "Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition", "comment": null, "summary": "Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical \"blenders\" that lack generative fidelity and \"generators\" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.", "AI": {"tldr": "Insert In Style\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u5728\u98ce\u683c\u5316\u57df\u4e2d\u63d2\u5165\u771f\u5b9e\u4e16\u754c\u5bf9\u8c61\uff0c\u65e0\u9700\u5728\u7ebf\u5fae\u8c03\uff0c\u5728\u8eab\u4efd\u548c\u98ce\u683c\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u53c2\u8003\u7684\u5bf9\u8c61\u7ec4\u5408\u65b9\u6cd5\u5728\u5c06\u771f\u5b9e\u4e16\u754c\u5bf9\u8c61\u63d2\u5165\u98ce\u683c\u5316\u57df\u65f6\u5931\u8d25\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u751f\u6210\u4fdd\u771f\u5ea6\uff0c\u8981\u4e48\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u5728\u7ebf\u5fae\u8c03\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u591a\u9636\u6bb5\u8bad\u7ec3\u534f\u8bae\u6765\u89e3\u8026\u8eab\u4efd\u3001\u98ce\u683c\u548c\u7ec4\u5408\u8868\u793a\uff0c\u4ee5\u53ca\u4e13\u95e8\u7684\u63a9\u7801\u6ce8\u610f\u529b\u67b6\u6784\u6765\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u5b9e\u65bd\u8fd9\u79cd\u89e3\u8026\u3002", "result": "\u5728\u65b0\u5efa\u7684\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8eab\u4efd\u548c\u98ce\u683c\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u4e5f\u5f3a\u70c8\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u7ed3\u679c\u3002", "conclusion": "Insert In Style\u662f\u7b2c\u4e00\u4e2a\u65e2\u5b9e\u7528\u53c8\u9ad8\u4fdd\u771f\u7684\u96f6\u6837\u672c\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u5316\u57df\u4e2d\u5bf9\u8c61\u63d2\u5165\u7684\u6311\u6218\u3002"}}
{"id": "2511.15258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15258", "abs": "https://arxiv.org/abs/2511.15258", "authors": ["Yitong Yang", "Yinglin Wang", "Changshuo Wang", "Yongjun Zhang", "Ziyang Chen", "Shuting He"], "title": "SplitFlux: Learning to Decouple Content and Style from a Single Image", "comment": null, "summary": "Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.", "AI": {"tldr": "\u63d0\u51faSplitFlux\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790Flux\u6a21\u578b\u7684\u7279\u6027\uff0c\u53d1\u73b0\u65e9\u671f\u5355\u6d41\u5757\u63a7\u5236\u5185\u5bb9\u3001\u540e\u671f\u5757\u63a7\u5236\u98ce\u683c\uff0c\u5229\u7528LoRA\u5fae\u8c03\u5b9e\u73b0\u5185\u5bb9\u548c\u98ce\u683c\u7684\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u79e9\u7ea6\u675f\u9002\u5e94\u548c\u89c6\u89c9\u95e8\u63a7LoRA\u63d0\u5347\u5185\u5bb9\u4fdd\u6301\u548c\u98ce\u683c\u5316\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709SDXL\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\uff0c\u800cFlux\u6a21\u578b\u7531\u4e8e\u7279\u6027\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u65e0\u6cd5\u6709\u6548\u5206\u79bb\u5185\u5bb9\u548c\u98ce\u683c\u3002", "method": "\u57fa\u4e8e\u5bf9Flux\u6a21\u578b\u7684\u5206\u6790\uff0c\u63d0\u51faSplitFlux\u65b9\u6cd5\uff1a1\uff09\u79e9\u7ea6\u675f\u9002\u5e94\uff1a\u538b\u7f29\u7279\u5b9a\u5757\u7684\u79e9\u5e76\u653e\u5927\u66f4\u65b0\u5e45\u5ea6\uff0c\u9632\u6b62\u5185\u5bb9\u6cc4\u6f0f\u5230\u98ce\u683c\u5757\uff1b2\uff09\u89c6\u89c9\u95e8\u63a7LoRA\uff1a\u5c06\u5185\u5bb9LoRA\u5206\u4e3a\u9ad8\u79e9\u548c\u4f4e\u79e9\u5206\u652f\uff0c\u5206\u522b\u4fdd\u7559\u4e3b\u4f53\u4fe1\u606f\u548c\u6b8b\u5dee\u7ec6\u8282\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSplitFlux\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5185\u5bb9\u4fdd\u6301\u548c\u98ce\u683c\u5316\u8d28\u91cf\u3002", "conclusion": "SplitFlux\u901a\u8fc7\u7cfb\u7edf\u5206\u6790Flux\u6a21\u578b\u7279\u6027\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5185\u5bb9\u548c\u98ce\u683c\u7684\u6709\u6548\u89e3\u8026\uff0c\u4e3a\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.15633", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15633", "abs": "https://arxiv.org/abs/2511.15633", "authors": ["Tao Hu", "Lan Li", "Zhen-Hao Xie", "Da-Wei Zhou"], "title": "Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning", "comment": null, "summary": "Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like \"dog\" subsumes fine-grained categories such as \"Labrador\" and \"Golden Retriever,\" and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.", "AI": {"tldr": "HASTEN\u662f\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\u6765\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728\u4fdd\u6301\u5c42\u6b21\u7ed3\u6784\u7684\u540c\u65f6\u9632\u6b62\u7279\u5f81\u6f02\u79fb\u3002", "motivation": "\u73b0\u6709\u7684CLIP\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u6355\u6349\u89c6\u89c9\u548c\u8bed\u8a00\u6982\u5ff5\u56fa\u6709\u7684\u5c42\u6b21\u7ed3\u6784\uff08\u5982'\u72d7'\u5305\u542b'\u62c9\u5e03\u62c9\u591a'\u548c'\u91d1\u6bdb'\u7b49\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff09\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u7c7b\u522b\u7279\u5f81\u5728\u589e\u91cf\u66f4\u65b0\u65f6\u6f02\u79fb\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "1. \u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u76d1\u7763\uff0c\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5d4c\u5165\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u4ee5\u4fdd\u6301\u5c42\u6b21\u7ed3\u6784\uff1b2. \u5c06\u68af\u5ea6\u6295\u5f71\u5230\u5171\u4eab\u53cc\u66f2\u6620\u5c04\u5668\u7684\u96f6\u7a7a\u95f4\uff0c\u9632\u6b62\u5bf9\u5148\u524d\u4efb\u52a1\u7684\u5e72\u6270\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eHASTEN\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u8868\u793a\u3002", "conclusion": "HASTEN\u901a\u8fc7\u5c42\u6b21\u8bed\u4e49\u6811\u951a\u5b9a\u6280\u672f\uff0c\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u6709\u6548\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u6301\u5c42\u6b21\u5173\u7cfb\u3002"}}
