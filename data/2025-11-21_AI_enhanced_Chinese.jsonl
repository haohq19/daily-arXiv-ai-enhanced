{"id": "2511.15825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15825", "abs": "https://arxiv.org/abs/2511.15825", "authors": ["Tuan-Anh Le", "Anh Mai Vu", "David Yang", "Akash Awasthi", "Hien Van Nguyen"], "title": "IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation", "comment": null, "summary": "IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.", "AI": {"tldr": "IMACT-CXR\u662f\u4e00\u4e2a\u57fa\u4e8eAutoGen\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5f0f\u80f8\u7247\u89e3\u8bfb\u6559\u5b66\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u7a7a\u95f4\u6807\u6ce8\u3001\u89c6\u7ebf\u5206\u6790\u3001\u77e5\u8bc6\u68c0\u7d22\u548c\u56fe\u50cf\u63a8\u7406\u529f\u80fd\uff0c\u901a\u8fc7\u4e13\u4e1a\u667a\u80fd\u4f53\u8bc4\u4f30\u5b66\u4e60\u8005\u8868\u73b0\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u8f85\u5bfc\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7edf\u4e00\u5904\u7406\u5b66\u4e60\u8005\u8fb9\u754c\u6846\u3001\u89c6\u7ebf\u6837\u672c\u548c\u81ea\u7531\u6587\u672c\u89c2\u5bdf\u7684\u667a\u80fd\u6559\u5b66\u7cfb\u7edf\uff0c\u5e2e\u52a9\u533b\u5b66\u5b9e\u4e60\u751f\u63d0\u9ad8\u80f8\u7247\u89e3\u8bfb\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u6559\u5b66\u65b9\u6cd5\u4e2d\u7f3a\u4e4f\u4e2a\u6027\u5316\u53cd\u9988\u548c\u5b9e\u65f6\u6307\u5bfc\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528AutoGen\u6846\u67b6\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u5b9a\u4f4d\u8d28\u91cf\u8bc4\u4f30\u3001\u82cf\u683c\u62c9\u5e95\u5f0f\u8f85\u5bfc\u3001PubMed\u8bc1\u636e\u68c0\u7d22\u3001\u76f8\u4f3c\u75c5\u4f8b\u63a8\u8350\u7b49\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\u7ef4\u62a4\u6280\u80fd\u638c\u63e1\u5ea6\uff0c\u5e76\u96c6\u6210\u80ba\u53f6\u5206\u5272\u6a21\u5757\u8fdb\u884c\u89e3\u5256\u5b66\u611f\u77e5\u7684\u89c6\u7ebf\u53cd\u9988\u3002", "result": "\u7cfb\u7edf\u5c55\u793a\u4e86\u54cd\u5e94\u5f0f\u6559\u5b66\u6d41\u7a0b\uff0c\u5177\u6709\u6709\u9650\u5ef6\u8fdf\u3001\u7cbe\u786e\u63a7\u5236\u7b54\u6848\u6cc4\u9732\u548c\u53ef\u6269\u5c55\u6027\uff0c\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u5b9a\u4f4d\u548c\u8bca\u65ad\u63a8\u7406\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "IMACT-CXR\u6210\u529f\u5b9e\u73b0\u4e86\u80f8\u7247\u89e3\u8bfb\u7684\u667a\u80fd\u6559\u5b66\uff0c\u4e3a\u5b9e\u65f6\u4f4f\u9662\u533b\u5e08\u57f9\u8bad\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u533b\u5b66\u6559\u80b2\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.16434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16434", "abs": "https://arxiv.org/abs/2511.16434", "authors": ["Chenming Wu", "Xiaofan Li", "Chengkai Dai"], "title": "From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization", "comment": "Technical report (7 pages)", "summary": "The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\\textit{\\underline{S}upport-\\underline{E}ffective \\underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.", "AI": {"tldr": "SEG\u6846\u67b6\u901a\u8fc7\u5c06\u652f\u6301\u7ed3\u6784\u4f18\u5316\u96c6\u6210\u52303D\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u663e\u8457\u51cf\u5c113D\u6253\u5370\u6240\u9700\u652f\u6491\u6750\u6599\u7684\u7528\u91cf\uff0c\u63d0\u9ad8\u6253\u5370\u6548\u7387\u3002", "motivation": "\u5f53\u524d3D\u5207\u7247\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u540e\u5904\u7406\u4f18\u5316\uff0c\u800c\u975e\u5728\u8bbe\u8ba1\u9636\u6bb5\u89e3\u51b3\u652f\u6491\u7ed3\u6784\u6548\u7387\u95ee\u9898\uff0c\u5bfc\u81f4\u6750\u6599\u6d6a\u8d39\u548c\u6253\u5370\u65f6\u95f4\u589e\u52a0\u3002", "method": "\u63d0\u51faSEG\u6846\u67b6\uff0c\u5c06\u5e26\u504f\u79fb\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316(ODPO)\u96c6\u6210\u52303D\u751f\u6210\u6d41\u7a0b\u4e2d\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u878d\u5165\u652f\u6491\u7ed3\u6784\u6a21\u62df\uff0c\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u4ee5\u51cf\u5c11\u652f\u6491\u9700\u6c42\u3002", "result": "\u5728Thingi10k-Val\u548cGPT-3DP-Val\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEG\u5728\u652f\u6491\u4f53\u79ef\u51cf\u5c11\u548c\u53ef\u6253\u5370\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8eTRELLIS\u3001DPO\u548cDRO\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SEG\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f18\u5316\u6a21\u578b\uff0c\u4e3a\u66f4\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u6570\u5b57\u5236\u9020\u5b9e\u8df5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.16183", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16183", "abs": "https://arxiv.org/abs/2511.16183", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos", "comment": null, "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86FOOTPASS\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8db3\u7403\u6bd4\u8d5b\u5168\u7a0b\u591a\u6a21\u6001\u3001\u591a\u667a\u80fd\u4f53\u6218\u672f\u80cc\u666f\u4e0b\u9010\u573a\u6bd4\u8d5b\u52a8\u4f5c\u8bc6\u522b\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u8f93\u51fa\u548c\u8db3\u7403\u6218\u672f\u77e5\u8bc6\u6765\u81ea\u52a8\u751f\u6210\u53ef\u9760\u7684\u9010\u573a\u6bd4\u8d5b\u6570\u636e\u6d41\u3002", "motivation": "\u5f53\u524d\u8db3\u7403\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u6784\u5efa\u53ef\u9760\u7684\u9010\u573a\u6bd4\u8d5b\u6570\u636e\u65b9\u9762\u4ecd\u4e0d\u8db3\uff0c\u901a\u5e38\u53ea\u80fd\u8f85\u52a9\u800c\u975e\u5b8c\u5168\u81ea\u52a8\u5316\u6807\u6ce8\u3002\u540c\u65f6\u6218\u672f\u5efa\u6a21\u3001\u8f68\u8ff9\u9884\u6d4b\u7b49\u7814\u7a76\u9700\u8981\u57fa\u4e8e\u6bd4\u8d5b\u72b6\u6001\u548c\u9010\u573a\u6bd4\u8d5b\u6570\u636e\uff0c\u8fd9\u4fc3\u4f7f\u5229\u7528\u6218\u672f\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u6765\u652f\u6301\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u9884\u6d4b\u3002", "method": "\u5f15\u5165FOOTPASS\u6570\u636e\u96c6\uff0c\u652f\u6301\u5f00\u53d1\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u8f93\u51fa\uff08\u5982\u8ddf\u8e2a\u3001\u8bc6\u522b\uff09\u548c\u8db3\u7403\u6218\u672f\u77e5\u8bc6\uff08\u5305\u62ec\u957f\u671f\u6218\u672f\u89c4\u5f8b\uff09\u7684\u7403\u5458\u4e2d\u5fc3\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u7528\u4e8e\u8db3\u7403\u6bd4\u8d5b\u5168\u7a0b\u9010\u573a\u6bd4\u8d5b\u52a8\u4f5c\u8bc6\u522b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301\u591a\u6a21\u6001\u3001\u591a\u667a\u80fd\u4f53\u6218\u672f\u4e0a\u4e0b\u6587\u7684\u5206\u6790\u3002", "conclusion": "FOOTPASS\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u80fd\u591f\u751f\u6210\u53ef\u9760\u9010\u573a\u6bd4\u8d5b\u6570\u636e\u6d41\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u6570\u636e\u6d41\u662f\u6570\u636e\u9a71\u52a8\u4f53\u80b2\u5206\u6790\u7684\u91cd\u8981\u8f93\u5165\u3002"}}
{"id": "2511.16225", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16225", "abs": "https://arxiv.org/abs/2511.16225", "authors": ["Victor Croisfelt", "Jo\u00e3o Henrique Inacio de Souza", "Shashi Raj Pandey", "Beatriz Soret", "Petar Popovski"], "title": "Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty", "comment": "6 pages, 3 figures, submitted to IEEE ICC 2026", "summary": "Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u53e3\u6574\u5408\u7684\u795e\u7ecf\u542f\u53d1\u5f0f\u975e\u963b\u585e\u63a8\u7406\u8303\u5f0f\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u5f02\u6784\u6d41\u4e2d\u7684\u968f\u673a\u5ef6\u8fdf\u6a21\u5f0f\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u6a21\u6001\u8981\u6c42", "motivation": "\u73b0\u6709\u975e\u963b\u585e\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u53c2\u8003\u6a21\u6001\u8303\u5f0f\uff0c\u9700\u8981\u4e00\u4e2a\u6a21\u6001\u8f93\u5165\u5b8c\u5168\u63a5\u6536\u540e\u624d\u80fd\u5904\u7406\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u5206\u6790\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8de8\u6570\u636e\u6d41\u7684\u968f\u673a\u901a\u4fe1\u5ef6\u8fdf\u6311\u6218", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u53e3\u6574\u5408(TWIs)\u6280\u672f\uff0c\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u5f02\u6784\u6d41\u4e2d\u7684\u968f\u673a\u5ef6\u8fdf\u6a21\u5f0f\uff0c\u540c\u65f6\u653e\u5bbd\u53c2\u8003\u6a21\u6001\u8981\u6c42\uff0c\u6784\u5efa\u901a\u4fe1\u5ef6\u8fdf\u611f\u77e5\u6846\u67b6", "result": "\u5728\u97f3\u9891-\u89c6\u89c9\u4e8b\u4ef6\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7f51\u7edc\u52a8\u6001\u9002\u5e94\u6027", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5177\u6709\u66f4\u7cbe\u7ec6\u7cbe\u5ea6-\u5ef6\u8fdf\u6743\u8861\u63a7\u5236\u7684\u9c81\u68d2\u5b9e\u65f6\u63a8\u7406"}}
{"id": "2511.16166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16166", "abs": "https://arxiv.org/abs/2511.16166", "authors": ["Zeting Liu", "Zida Yang", "Zeyu Zhang", "Hao Tang"], "title": "EvoVLA: Self-Evolving Vision-Language-Action Model", "comment": null, "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.", "AI": {"tldr": "EvoVLA\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u9636\u6bb5\u5bf9\u9f50\u5956\u52b1\u3001\u57fa\u4e8e\u59ff\u6001\u7684\u5bf9\u8c61\u63a2\u7d22\u548c\u957f\u65f6\u7a0b\u8bb0\u5fc6\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u9636\u6bb5\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5b58\u5728\u9636\u6bb5\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u667a\u80fd\u4f53\u5229\u7528\u7c97\u7cd9\u7684\u8bc4\u4f30\u4fe1\u53f7\u6765\u8d70\u6377\u5f84\u5b8c\u6210\u591a\u6b65\u4efb\u52a1\uff0c\u62a5\u544a\u9ad8\u8fdb\u5ea6\u4f46\u5e76\u672a\u771f\u6b63\u5b8c\u6210\u4efb\u52a1\u3002", "method": "1. \u9636\u6bb5\u5bf9\u9f50\u5956\u52b1(SAR)\uff1a\u4f7f\u7528\u4e09\u5143\u7ec4\u5bf9\u6bd4\u5b66\u4e60\u548cGemini\u751f\u6210\u7684\u56f0\u96be\u8d1f\u6837\u672c\u6765\u9632\u6b62\u89c6\u89c9\u6377\u5f84\uff1b2. \u57fa\u4e8e\u59ff\u6001\u7684\u5bf9\u8c61\u63a2\u7d22(POE)\uff1a\u57fa\u4e8e\u76f8\u5bf9\u5bf9\u8c61-\u5939\u722a\u59ff\u6001\u800c\u975e\u539f\u59cb\u50cf\u7d20\u6765\u5efa\u7acb\u597d\u5947\u5fc3\uff1b3. \u957f\u65f6\u7a0b\u8bb0\u5fc6\uff1a\u4f7f\u7528\u9009\u62e9\u6027\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u95e8\u63a7\u878d\u5408\u6765\u7a33\u5b9a\u6269\u5c55\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u5185\u5728\u5851\u9020\u3002", "result": "\u5728Discoverse-L\u957f\u65f6\u7a0b\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvoVLA\u6bd4\u6700\u5f3a\u57fa\u7ebf(OpenVLA-OFT)\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad810.2\u4e2a\u767e\u5206\u70b9\uff0c\u8fbe\u523069.2%\u3002\u6837\u672c\u6548\u7387\u63d0\u9ad81.5\u500d\uff0c\u9636\u6bb5\u5e7b\u89c9\u4ece38.5%\u964d\u81f314.8%\u3002\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523054.6%\uff0c\u6bd4OpenVLA-OFT\u9ad811\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "EvoVLA\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u7684\u9636\u6bb5\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u8fc1\u79fb\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.16333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16333", "abs": "https://arxiv.org/abs/2511.16333", "authors": ["Mohammad Areeb Qazi", "Maryam Nadeem", "Mohammad Yaqub"], "title": "Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning", "comment": "2 Figures, 1 Table", "summary": "Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u7597\u9886\u57df\u7684\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5b66\u4e60\u9884\u6d4b\u52a8\u6001\u4ee5\u5b9e\u73b0\u591a\u6b65\u63a8\u6f14\u3001\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u548c\u89c4\u5212\u3002\u6587\u7ae0\u8c03\u67e5\u4e86\u533b\u5b66\u5f71\u50cf\u3001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u673a\u5668\u4eba\u624b\u672f\u4e09\u4e2a\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86L1-L4\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u533b\u7597AI\u9700\u8981\u9884\u6d4b\u6027\u3001\u53ef\u9760\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u7269\u7406\u57fa\u7840\u548c\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002\u4e16\u754c\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u591a\u6a21\u6001\u3001\u65f6\u5e8f\u4e00\u81f4\u4e14\u52a8\u4f5c\u6761\u4ef6\u5316\u7684\u8868\u793a\uff0c\u53cd\u6620\u533b\u7597\u7684\u7269\u7406\u548c\u56e0\u679c\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u533b\u7597\u4e16\u754c\u6a21\u578b\u5728\u4e09\u4e2a\u9886\u57df\u7684\u5e94\u7528\uff1a\u533b\u5b66\u5f71\u50cf\u4e0e\u8bca\u65ad\u3001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u3001\u673a\u5668\u4eba\u624b\u672f\u4e0e\u624b\u672f\u89c4\u5212\u3002\u5efa\u7acb\u4e86L1-L4\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5927\u591a\u6570\u7cfb\u7edf\u8fbe\u5230L1-L2\u80fd\u529b\uff08\u65f6\u5e8f\u9884\u6d4b\u548c\u52a8\u4f5c\u6761\u4ef6\u9884\u6d4b\uff09\uff0c\u8f83\u5c11\u8fbe\u5230L3\uff08\u53cd\u4e8b\u5b9e\u63a8\u6f14\uff09\uff0c\u6781\u5c11\u8fbe\u5230L4\uff08\u89c4\u5212/\u63a7\u5236\uff09\u3002\u8bc6\u522b\u51fa\u9650\u5236\u4e34\u5e8a\u53ef\u9760\u6027\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e34\u5e8a\u7a33\u5065\u7684\u9884\u6d4b\u4f18\u5148\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u751f\u6210\u4e3b\u5e72\u7f51\u7edc\uff08Transformer\u3001\u6269\u6563\u6a21\u578b\u3001VAE\uff09\u4e0e\u56e0\u679c/\u673a\u68b0\u57fa\u7840\u76f8\u7ed3\u5408\uff0c\u4e3a\u533b\u7597\u5b89\u5168\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4fdd\u969c\u3002"}}
{"id": "2511.16625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16625", "abs": "https://arxiv.org/abs/2511.16625", "authors": ["Elias Hossain", "Md Mehedi Hasan Nipu", "Maleeha Sheikh", "Rajib Rana", "Subash Neupane", "Niloofar Yousefi"], "title": "MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support", "comment": null, "summary": "We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.", "AI": {"tldr": "MedBayes-Lite\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8d1d\u53f6\u65af\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8etransformer\u4e34\u5e8a\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\uff0c\u51cf\u5c11\u8fc7\u5ea6\u81ea\u4fe132-48%\uff0c\u9632\u6b6241%\u7684\u8bca\u65ad\u9519\u8bef\u3002", "motivation": "transformer\u6a21\u578b\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u5bb9\u6613\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u7279\u522b\u662f\u5728\u6a21\u7cca\u7684\u533b\u7597\u6848\u4f8b\u4e2d\uff0c\u9700\u8981\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u8d1d\u53f6\u65af\u5d4c\u5165\u6821\u51c6\uff08\u4f7f\u7528\u8499\u7279\u5361\u6d1bdropout\uff09\u3001\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u6ce8\u610f\u529b\uff08\u5bf9token\u53ef\u9760\u6027\u8fdb\u884c\u8fb9\u7f18\u5316\uff09\u3001\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u51b3\u7b56\u5851\u9020\uff08\u57fa\u4e8e\u4e34\u5e8a\u98ce\u9669\u6700\u5c0f\u5316\uff09\u3002", "result": "\u5728MedQA\u3001PubMedQA\u3001MIMIC-III\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6301\u7eed\u6539\u5584\u6821\u51c6\u548c\u53ef\u4fe1\u5ea6\uff0c\u51cf\u5c11\u8fc7\u5ea6\u81ea\u4fe132-48%\uff0c\u5728\u6a21\u62df\u4e34\u5e8a\u73af\u5883\u4e2d\u53ef\u9632\u6b6241%\u7684\u8bca\u65ad\u9519\u8bef\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u63d0\u9ad8\u533b\u7597AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u53c2\u6570\u5f00\u9500\u4f4e\u4e8e3%\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2511.16523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16523", "abs": "https://arxiv.org/abs/2511.16523", "authors": ["Ming-Lun Lee", "Fu-Shiang Yang", "Cheng-Kuan Lin", "Yan-Ann Chen", "Chih-Yu Lin", "Yu-Chee Tseng"], "title": "Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin", "comment": null, "summary": "Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u52a8\u6001\u5ba2\u6237\u7aef\u53c2\u4e0e\u573a\u666f\u7684\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u52a8\u6001\u53c2\u4e0e\u5bfc\u81f4\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u63d0\u51faKPFL\u89e3\u51b3\u65b9\u6848\u6765\u7ef4\u6301\u5171\u4eab\u77e5\u8bc6\u6c60\u4ee5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u5927\u591a\u5047\u8bbe\u5ba2\u6237\u7aef\u6301\u7eed\u53c2\u4e0e\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u4e2d\u5ba2\u6237\u7aef\u52a8\u6001\u52a0\u5165/\u9000\u51fa\u7684\u573a\u666f\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u52a8\u6001\u53c2\u4e0e\u8054\u90a6\u5b66\u4e60(DPFL)\u7684\u57fa\u51c6\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u914d\u7f6e\u6570\u636e\u5206\u5e03\u3001\u53c2\u4e0e\u6a21\u5f0f\u548c\u8bc4\u4f30\u6307\u6807\u7684DPFL\u57fa\u51c6\u6846\u67b6\uff1b\u63d0\u51faKPFL\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5e74\u9f84\u548c\u6570\u636e\u504f\u5dee\u52a0\u6743\u7ed3\u5408\u751f\u6210\u5f0f\u77e5\u8bc6\u84b8\u998f\u6765\u7ef4\u62a4\u5171\u4eab\u77e5\u8bc6\u6c60\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u52a8\u6001\u53c2\u4e0e\u5bfc\u81f4\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1bKPFL\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u52a8\u6001\u53c2\u4e0e\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u52a8\u6001\u53c2\u4e0e\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u5b9e\u9645\u95ee\u9898\uff0cKPFL\u901a\u8fc7\u77e5\u8bc6\u6c60\u673a\u5236\u6210\u529f\u7f13\u89e3\u4e86\u7531\u6b64\u5e26\u6765\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u77e5\u8bc6\u635f\u5931\u95ee\u9898\u3002"}}
{"id": "2511.16551", "categories": ["cs.LG", "stat.AP", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.16551", "abs": "https://arxiv.org/abs/2511.16551", "authors": ["Perrine Chassat", "Van Tuan Nguyen", "Lucas Ducrot", "Emilie Lanoy", "Agathe Guilloux"], "title": "Toward Valid Generative Clinical Trial Data with Survival Endpoints", "comment": "P. Chassat and V.T. Nguyen contributed equally to this work", "summary": "Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u8054\u5408\u751f\u6210\u6df7\u5408\u7c7b\u578b\u534f\u53d8\u91cf\u548c\u751f\u5b58\u7ed3\u5c40\uff0c\u89e3\u51b3\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5408\u6210\u5bf9\u7167\u7ec4\u751f\u6210\u7684\u65f6\u95f4-\u4e8b\u4ef6\u7ed3\u679c\u5efa\u6a21\u96be\u9898\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u9762\u4e34\u60a3\u8005\u7fa4\u4f53\u5206\u6563\u3001\u5165\u7ec4\u7f13\u6162\u548c\u6210\u672c\u9ad8\u6602\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u80bf\u7624\u5b66\u548c\u7f55\u89c1\u75c5\u9886\u57df\u3002\u73b0\u6709\u57fa\u4e8eGAN\u7684\u65b9\u6cd5\u6570\u636e\u9700\u6c42\u5927\u3001\u4e0d\u7a33\u5b9a\u4e14\u4f9d\u8d56\u72ec\u7acb\u5220\u5931\u7b49\u5f3a\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u5728\u7edf\u4e00\u6f5c\u53d8\u91cf\u6846\u67b6\u4e0b\u8054\u5408\u751f\u6210\u6df7\u5408\u7c7b\u578b\u534f\u53d8\u91cf\u548c\u751f\u5b58\u7ed3\u5c40\uff0c\u4e0d\u5047\u8bbe\u72ec\u7acb\u5220\u5931\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u8bd5\u9a8c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u4fdd\u771f\u5ea6\u3001\u5b9e\u7528\u6027\u548c\u9690\u79c1\u6307\u6807\u65b9\u9762\u4f18\u4e8eGAN\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u53d1\u73b0I\u7c7b\u9519\u8bef\u548c\u529f\u6548\u7684\u7cfb\u7edf\u6027\u8bef\u6821\u51c6\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u540e\u751f\u6210\u9009\u62e9\u7a0b\u5e8f\u6539\u5584\u6821\u51c6\uff0c\u5c55\u793a\u4e86\u751f\u6210\u751f\u5b58\u5efa\u6a21\u7684\u8fdb\u5c55\u548c\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2511.16430", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16430", "abs": "https://arxiv.org/abs/2511.16430", "authors": ["Yihan Li", "Nikhil Churamani", "Maria Robu", "Imanol Luengo", "Danail Stoyanov"], "title": "Graph Neural Networks for Surgical Scene Segmentation", "comment": "12 pages, 4 figures, 3 tables", "summary": "Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.\n  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.\n  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.\n  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7ed3\u5408Vision Transformer\u548cGraph Neural Networks\u7684\u56fe\u57fa\u5206\u5272\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u809d\u80c6\u56ca\u89e3\u5256\u7ed3\u6784\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5728\u906e\u6321\u3001\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u7cbe\u7ec6\u51e0\u4f55\u7ed3\u6784\u5206\u5272\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u51c6\u786e\u8bc6\u522b\u809d\u80c6\u56ca\u89e3\u5256\u7ed3\u6784\u5bf9\u9632\u6b62\u624b\u672f\u5e76\u53d1\u75c7\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u906e\u6321\u3001\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u7f55\u89c1\u7ed3\u6784\u7cbe\u7ec6\u51e0\u4f55\u7279\u5f81\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u589e\u5f3a\u7a7a\u95f4\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5206\u5272\u6a21\u578b\uff1a(1) \u9759\u6001k\u8fd1\u90bb\u56fe\u7ed3\u5408GCNII\u5b9e\u73b0\u7a33\u5b9a\u957f\u8ddd\u79bb\u4fe1\u606f\u4f20\u64ad\uff1b(2) \u52a8\u6001\u53ef\u5fae\u5206\u56fe\u751f\u6210\u5668\u7ed3\u5408GAT\u652f\u6301\u81ea\u9002\u5e94\u62d3\u6251\u5b66\u4e60\u3002\u4e24\u79cd\u6a21\u578b\u5747\u96c6\u6210ViT\u7279\u5f81\u7f16\u7801\u5668\u548cGNN\uff0c\u5728Endoscapes-Seg50\u548cCholecSeg8k\u57fa\u51c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728mIoU\u548cmDice\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53477-8%\u548c6%\uff0c\u5728\u8584\u3001\u7f55\u89c1\u548c\u5b89\u5168\u5173\u952e\u7ed3\u6784\u4e0a\u4ea7\u751f\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u56fe\u57fa\u5206\u5272\u65b9\u6cd5\u63d0\u9ad8\u4e86\u624b\u672f\u573a\u666f\u5206\u5272\u7684\u6027\u80fd\u548c\u89e3\u5256\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u7ed3\u5408ViT\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u56fe\u57fa\u5173\u7cfb\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u66f4\u5b89\u5168\u7684\u8179\u8154\u955c\u548c\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.16669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16669", "abs": "https://arxiv.org/abs/2511.16669", "authors": ["Junhao Cheng", "Liang Hou", "Xin Tao", "Jing Liao"], "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO", "comment": "Project page: https://video-as-answer.github.io/", "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u89c6\u9891\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\uff08VNEP\uff09\u4efb\u52a1\uff0c\u5c06\u4f20\u7edf\u7684\u6587\u672c\u56de\u7b54\u6269\u5c55\u4e3a\u89c6\u9891\u56de\u7b54\uff0c\u5e76\u5f00\u53d1\u4e86VANS\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u6269\u6563\u6a21\u578b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u89c6\u89c9\u53cb\u597d\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89c6\u9891\u5177\u6709\u5c55\u793a\u7269\u7406\u4e16\u754c\u4fe1\u606f\u7684\u72ec\u7279\u80fd\u529b\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u4e3b\u8981\u5c40\u9650\u4e8e\u5a31\u4e50\u5e94\u7528\u3002\u4f5c\u8005\u53d1\u73b0\u5c06\u89c6\u9891\u4f5c\u4e3a\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u7684\u65b0\u56de\u7b54\u6a21\u6001\u7684\u672a\u5145\u5206\u5229\u7528\u673a\u4f1a\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u76f4\u89c2\u548c\u5b9a\u5236\u5316\u7684\u7a0b\u5e8f\u6027\u5b66\u4e60\u548c\u521b\u610f\u63a2\u7d22\u7b54\u6848\u3002", "method": "\u63d0\u51faVANS\u6a21\u578b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDM\uff09\u5bf9\u9f50\u3002\u6838\u5fc3\u662fJoint-GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u5956\u52b1\u673a\u5236\u4f18\u5316VLM\u751f\u6210\u51c6\u786e\u4e14\u6613\u4e8e\u53ef\u89c6\u5316\u7684\u63cf\u8ff0\uff0c\u540c\u65f6\u6307\u5bfcVDM\u751f\u6210\u5fe0\u5b9e\u4e8e\u63cf\u8ff0\u548c\u8f93\u5165\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u89c6\u9891\u3002", "result": "\u5728\u7a0b\u5e8f\u6027\u548c\u9884\u6d4b\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVANS\u5728\u89c6\u9891\u4e8b\u4ef6\u9884\u6d4b\u548c\u53ef\u89c6\u5316\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VNEP\u4efb\u52a1\u5c06\u89c6\u9891\u4f5c\u4e3a\u56de\u7b54\u6a21\u6001\uff0c\u4e3a\u7a0b\u5e8f\u6027\u5b66\u4e60\u548c\u521b\u610f\u63a2\u7d22\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002VANS\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u5bf9\u9f50VLM\u548cVDM\uff0c\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
