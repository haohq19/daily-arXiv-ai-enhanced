<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 提出GFP框架，用高级特征预测替代传统低级重建，实现高效骨架动作识别


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法局限于原始关节坐标重建，导致计算冗余和语义表示有限

Method: 使用轻量级目标生成网络动态产生多样化监督信号，采用约束优化确保特征多样性

Result: 训练速度提升6.2倍，在NTU RGB+D 60/120和PKU-MMD数据集上达到SOTA性能

Conclusion: GFP框架通过高级特征预测实现了计算效率和表示质量的双重提升

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于事件相机的激光融合氛流减缓方法(EGTM)，通过事件流的异步特性提取像素级可靠无氛流指导，在保持预算效率的同时显著提升了图像还原质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习氛流减缓方法需要高容量网络学习同步帧之间粗粒度的氛流动力学，计算和存储效率低。事件相机具有微秒级时间分辨率，有潜力根本解决这个瓶颈。

Method: 首先提出"事件幸运见解"揭示氛流扭曲与事件流逆时空分布的相关性，然后构建EGTM框架从噪声氛流事件中提取像素级可靠无氛流指导进行时间幸运融合，并构建了第一个真实世界事件驱动氛流数据集。

Result: 在真实世界EGTM数据集上，方法在模型大小、推理延迟和模型复杂度上分别超过现有SOTA方法710倍、214倍和224倍，还原质量也达到最佳水平(+0.94 PSNR和+0.08 SSIM)。

Conclusion: 这一研究证明了将事件手段引入氛流减缓任务的巨大效率优势，为高效氛流减缓提供了新的解决方案。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [3] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: FocusMamba通过事件引导的多模态稀疏化和跨模态聚焦融合，自适应地丢弃RGB和事件数据中的低信息区域，在保持精度的同时显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有RGB-事件检测方法对所有区域进行统一处理，导致计算成本高且性能不佳。固定阈值的分词稀疏化方法无法适应不同复杂度样本的需求

Method: 提出事件引导多模态稀疏化(EGMS)策略，利用事件相机感知的场景变化自适应识别和丢弃低信息区域；设计跨模态聚焦融合(CMFF)模块有效整合互补特征

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上，相比现有方法在精度和效率方面都取得了更优的性能

Conclusion: FocusMamba通过自适应协作稀疏化和高效融合机制，成功实现了RGB-事件检测中精度与效率的更好平衡

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [4] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出结合双向文本融合和类别感知时序图的新方法，解决音频-视觉视频解析中伪标签噪声传播问题，在多个基准数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将噪声段级伪标签视为可靠监督，要么让无差别注意力将伪标签传播到所有帧，导致训练过程中初始错误被反复放大

Method: 结合双向文本融合模块(BiT)进行语义注入和动态校准来定位和净化语义线索，使用类别感知时序图模块(CATS)进行语义传播和连接

Result: 在两个基准数据集LLP和UnAV-100的多个关键指标上实现了最先进的性能

Conclusion: 通过整合两种研究方向的优势，有效解决了弱监督音频-视觉视频解析中的噪声传播问题，取得了优异性能

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [5] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: DVS-PedX是一个基于事件相机的行人检测和过街意图分析数据集，包含合成和真实世界的事件流数据，支持SNN基准测试，揭示了仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、高动态范围和运动鲁棒性等优势，但缺乏专门针对行人安全和意图预测的数据集。DVS-PedX旨在填补这一空白，推动基于事件相机的行人安全研究。

Method: 数据集包含两个互补来源：(1) CARLA仿真器生成的合成事件流，控制天气和光照条件；(2) JAAD行车记录仪视频通过v2e工具转换的真实事件流。每个序列包含配对的RGB帧、事件帧和帧级标签。

Result: 提供了基线脉冲神经网络(SNN)实验结果，展示了数据集的可使用性，并揭示了仿真到现实之间的性能差距，为领域自适应和多模态融合提供了动机。

Conclusion: DVS-PedX数据集将加速基于事件相机的行人安全、意图预测和神经形态感知研究，为领域自适应和多模态方法的发展提供重要资源。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [6] [Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage](https://arxiv.org/abs/2509.04370)
*Dor Cohen,Inga Efrosman,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 通过单目SLAM和聚类技术将身体摄像头拍摄的长时间视频转换为简洁的全景图像摘要，提高应急情况下的情况意识和决策效率


<details>
  <summary>Details</summary>
Motivation: 解决应急員在时间压力下对长时间视频资料进行快速理解的需求，提供能够快速解释的简洁视觉摘要

Method: 使用单目SLAM估计摄像头轨迹和环境空间布局，通过聚类识别关键视点，选择代表性帧，采用多帧缩绣技术合成空间一致的全景图像

Result: 得到能够快速理解复杂环境的视觉摘要，支持高效决策和事件回顾

Conclusion: 该方法为应急員提供了一种高效的视觉摘要工具，显著提升了情况意识能力和事故分析效率

Abstract: First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.

</details>


### [7] [Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.04446)
*Kiymet Akdemir,Jing Shi,Kushal Kafle,Brian Price,Pinar Yanardag*

Main category: cs.CV

TL;DR: Plot'n Polish是一个零样本框架，用于实现一致的故事可视化生成，提供细粒度的控制能力，支持生成后修改并保持视觉和叙事一致性


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在故事可视化应用中缺乏灵活性，无法在保持多帧视觉和叙事一致性的同时进行精细或粗略的编辑，限制了创作者对视觉故事的流畅创作和精炼

Method: 提出了Plot'n Polish零样本框架，能够在不同细节层次上对故事可视化提供细粒度控制，支持生成后的一致修改

Result: 该框架解决了现有方法在保持多帧一致性和编辑灵活性方面的不足

Conclusion: Plot'n Polish为故事可视化提供了增强的控制和精炼能力，使创作者能够无缝地制作和完善视觉故事

Abstract: Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度动态分析的无超参数梯度归一化方法，通过匹配梯度的自然演化来稳定优化过程，在CIFAR-100基准测试中保持或提高了测试精度。


<details>
  <summary>Details</summary>
Motivation: 梯度动态对深度神经网络的稳定性和泛化能力至关重要。研究发现梯度方差和标准差在训练过程中呈现一致的变化模式，这启发了对梯度缩放进行自然对齐的需求。

Method: 提出超参数自由的梯度归一化方法，使梯度缩放与其自然演化保持一致，防止意外放大，稳定优化过程，同时保持收敛保证。

Result: 在CIFAR-100基准测试中使用ResNet-20、ResNet-56和VGG-16-BN进行实验，结果表明该方法即使在强泛化条件下也能保持或提高测试精度。

Conclusion: 研究强调了直接跟踪梯度动态的重要性，旨在弥合理论预期与实证行为之间的差距，为未来优化研究提供见解。

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [9] [Predicting Traffic Accident Severity with Deep Neural Networks](https://arxiv.org/abs/2509.03819)
*Meghan Bibb,Pablo Rivas,Mahee Tayba*

Main category: cs.LG

TL;DR: 通过深度神经网络模型分析交通事故数据，实现了出色的事故严重程度分类准确率，达到92%


<details>
  <summary>Details</summary>
Motivation: 利用机器学习技术研究交通事故数据，降低未来事故风险，并处理数据不平衡问题

Method: 首先分析特征共线性，使用自编码器进行无监督维度降缩，然后构建密集神经网络进行事故严重程庤分类

Result: 实验结果显示，交叉验证准确率达到92%，在事故严重程度分类任务中表现优异

Conclusion: 神经网络模型在交通事故数据分析中具有高准确率和良好的模性能，为事故预防提供了有效技术支撑

Abstract: Traffic accidents can be studied to mitigate the risk of further events.
Recent advances in machine learning have provided an alternative way to study
data associated with traffic accidents. New models achieve good generalization
and high predictive power over imbalanced data. In this research, we study
neural network-based models on data related to traffic accidents. We begin
analyzing relative feature colinearity and unsupervised dimensionality
reduction through autoencoders, followed by a dense network. The features are
related to traffic accident data and the target is to classify accident
severity. Our experiments show cross-validated results of up to 92% accuracy
when classifying accident severity using the proposed deep neural network.

</details>


### [10] [Characteristic Energy Behavior Profiling of Non-Residential Buildings](https://arxiv.org/abs/2509.04322)
*Haley Dozier,Althea Henslee*

Main category: cs.LG

TL;DR: 美军基地基础设施面临气候变化风险，需要通过数据驱动的能源使用行为模型来评估弹性和建立基准


<details>
  <summary>Details</summary>
Motivation: 应对气候变化和极端天气威胁，保护美军基地的关键设施资产，确保战备生产力

Method: 使用多模态数据建立能源使用行为模型，进行分析、预测和聚类，并利用结构相似的公开数据进行方法语例

Result: 提出了一种能够分析、预测和聚类非住宅建筑能源使用数据的行为模型方法

Conclusion: 该方法可为美军基地能源系统的弹性评估提供基准，并为未来的弹性措施提供标准化的评估基准

Abstract: Due to the threat of changing climate and extreme weather events, the
infrastructure of the United States Army installations is at risk. More than
ever, climate resilience measures are needed to protect facility assets that
support critical missions and help generate readiness. As most of the Army
installations within the continental United States rely on commercial energy
and water sources, resilience to the vulnerabilities within independent energy
resources (electricity grids, natural gas pipelines, etc) along with a baseline
understanding of energy usage within installations must be determined. This
paper will propose a data-driven behavioral model to determine behavior
profiles of energy usage on installations. These profiles will be used 1) to
create a baseline assessment of the impact of unexpected disruptions on energy
systems and 2) to benchmark future resiliency measures. In this methodology,
individual building behavior will be represented with models that can
accurately analyze, predict, and cluster multimodal data collected from energy
usage of non-residential buildings. Due to the nature of Army installation
energy usage data, similarly structured open access data will be used to
illustrate this methodology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 基于Model Context Protocol的无训练、防幻觉的OMOP CDM医学术语映射系统，通过外部资源查询提高映射效率和准确性


<details>
  <summary>Details</summary>
Motivation: OMOP CDM标准化过程中的源医学术语映射工作耗费资源且容易出错，而大语言模型存在幻觉问题不适合直接临床部署

Method: 采用Model Context Protocol(MCP)标准化框架，允许LLM与外部资源和工具交互，实现零训练、可解释的映射系统

Result: 系统能够提供实时词汇查找和结构化推理输出，显著提高了映射效率和准确性

Conclusion: 该方法在不需训练的情况下实现了高效、准确的医学术语映射，适用于探索和生产环境

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [12] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [13] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 论文认为即使AI在能力上超越人类，但由于缺乏中枢神经系统（CNS）带来的情感体验和道德理解，AI永远无法真正取代人类成为宇宙的领导者。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否会成为超越人类的"数字物种"并接管宇宙领导权的问题，指出当前讨论中忽略了人类与AI的根本区别。

Method: 通过哲学分析和生物学视角，比较人类中枢神经系统与AI系统的本质差异，论证情感体验和道德理解的重要性。

Result: AI可能在几乎所有能力指标上超越人类，但由于无法拥有真正的中枢神经系统和情感体验，无法发展出可持续的伦理体系，因此不具备领导宇宙的资格。

Conclusion: 宇宙领导权的最佳基础永远是DNA而非硅基技术，人类的中枢神经系统和情感体验是AI无法复制的独特优势。

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

TL;DR: ProLiFIC是一个从1987年到2022年意大利立法过程的综合事件日志，使用LLM从非结构化数据创建，旨在为法律流程挖掘提供基准数据集。


<details>
  <summary>Details</summary>
Motivation: 流程挖掘在工业领域应用成熟，但在法律领域的应用受限于数据可访问性和质量。意大利立法过程缺乏结构化的事件日志数据，限制了流程挖掘在法律领域的有效性。

Method: 从Normattiva门户获取非结构化数据，使用大型语言模型(LLMs)进行结构化处理，创建了ProLiFIC事件日志数据集。

Result: 成功构建了涵盖35年意大利立法过程的综合事件日志，展示了初步分析示例，证明了数据集的有效性。

Conclusion: ProLiFIC为法律流程挖掘提供了有价值的基准数据集，促进了流程挖掘与大型语言模型的结合，推动了法律领域流程挖掘的新发展。

Abstract: Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [15] [Explicit and Implicit Data Augmentation for Social Event Detection](https://arxiv.org/abs/2509.04202)
*Congbo Ma,Yuxia Wang,Jia Wu,Jian Yang,Jing Du,Zitai Qiu,Qing Li,Hu Wang,Preslav Nakov*

Main category: cs.CL

TL;DR: SED-Aug是一个用于社交媒体事件检测的双重增强框架，通过显式文本增强和隐式特征空间增强来解决标注数据稀缺问题，在两个Twitter数据集上显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 社交媒体事件检测依赖标注数据，但人工标注成本高昂且费时，需要解决数据稀缺问题来提升模型性能。

Method: 提出双重增强框架：1）显式文本增强使用大语言模型通过5种生成策略增强文本信息；2）隐式特征空间增强设计5种新颖的扰动技术，在结构融合嵌入上操作以保持语义和关系属性。

Result: 在Twitter2012数据集上平均F1分数比最佳基线模型提升约17.67%，在Twitter2018数据集上提升约15.57%。

Conclusion: SED-Aug框架通过双重增强有效提升了社交媒体事件检测的性能，解决了数据稀缺问题，代码已开源。

Abstract: Social event detection involves identifying and categorizing important events
from social media, which relies on labeled data, but annotation is costly and
labor-intensive. To address this problem, we propose Augmentation framework for
Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework,
which combines explicit text-based and implicit feature-space augmentation to
enhance data diversity and model robustness. The explicit augmentation utilizes
large language models to enhance textual information through five diverse
generation strategies. For implicit augmentation, we design five novel
perturbation techniques that operate in the feature space on structural fused
embeddings. These perturbations are crafted to keep the semantic and relational
properties of the embeddings and make them more diverse. Specifically, SED-Aug
outperforms the best baseline model by approximately 17.67% on the Twitter2012
dataset and by about 15.57% on the Twitter2018 dataset in terms of the average
F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach](https://arxiv.org/abs/2509.03563)
*Quan Quan,Jiwen Xu,Runxiao Liu,Yi Ding,Jiaxing Che,Kai-Yuan Cai*

Main category: cs.RO

TL;DR: 提出了一种基于物理启发的无人机群协同运输方法，模仿桌腿负载分配的耗散力学原理，实现了无需显式通信的自主编队稳定和自适应负载分配。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、通信依赖性和动态故障鲁棒性方面存在不足，而无人机群协同运输在物流和灾难响应中具有变革潜力。

Method: 开发去中心化耗散力模型，每个机器人根据邻居机器人和悬挂负载动态调整位置，类似于能量耗散的桌腿反应机制。

Result: 仿真显示跟踪误差比现有方法降低20%-68.5%，真实实验在单机故障、断连事件、25%负载变化和40%缆长不确定性下达到94%成功率，在4级风力下仍保持强鲁棒性。

Conclusion: 该方法将群体智能与机械稳定性原理相结合，为异构空中系统在通信受限环境中处理复杂运输任务提供了可扩展框架。

Abstract: In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.

</details>


### [17] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 提出DRLR框架，通过改进IBRL算法的动作选择模块来减少bootstrapping误差，使用SAC替代TD3防止策略收敛到次优解，在机器人任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在机器人任务中探索效率低和bootstrapping误差导致学习效率低的问题，通过结合示范数据提高学习效率。

Method: 基于IBRL算法改进动作选择模块，提供校准的Q值来减少bootstrapping误差；使用SAC作为RL策略防止收敛到次优解；在bucket loading和open drawer两个机器人任务上进行验证。

Result: 实验证明该方法能有效减少bootstrapping误差和防止过拟合，在低维和高维状态-动作空间任务中都具有鲁棒性，且对不同质量的示范数据都有效；在真实轮式装载机上成功部署。

Conclusion: DRLR框架通过改进动作选择和策略优化，显著提高了强化学习在机器人任务中的探索效率和性能，实现了从仿真到真实环境的成功迁移。

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>
