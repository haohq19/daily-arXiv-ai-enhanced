<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images](https://arxiv.org/abs/2511.16717)
*Asya Y. Akkus,Bradley T. Wolfe,Pinghan Chu,Chengkun Huang,Chris S. Campbell,Mariana Alvarado Alvarez,Petr Volegov,David Fittinghoff,Robert Reinovsky,Zhehui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于无监督自动编码器和CDF 97小波变换的混合高斯-泊松去噪方法，用于中子成像数据的噪声去除，相比传统方法具有更低的重建误差和更好的边缘保持能力。


<details>
  <summary>Details</summary>
Motivation: 中子成像在惯性约束聚变(ICF)分析中很重要，但图像常被高斯和泊松噪声共同污染，传统滤波和阈值方法难以有效去除这些重叠噪声，需要开发能保持图像保真度的去噪技术。

Method: 使用无监督自动编码器，在潜在空间中结合Cohen-Daubechies-Feauveau (CDF 97)小波变换，专门针对混合高斯-泊松噪声进行去噪处理。

Result: 该方法成功去除了中子成像数据中的噪声，相比非机器学习方法如BM3D，具有更低的重建误差和更优的边缘保持指标。

Conclusion: 这种基于机器学习的去噪方法为中子图像噪声减少和ICF实验的三维重建分析提供了有前景的进展。

Abstract: Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.

</details>


### [2] [R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios](https://arxiv.org/abs/2511.16901)
*Lu Zhu,Tiantian Geng,Yangye Chen,Teng Wang,Ping Lu,Feng Zheng*

Main category: cs.CV

TL;DR: 提出了R-AVST数据集和AVST-Zero模型，用于解决真实世界音频-视频时空推理任务。R-AVST包含5K个未剪辑视频和27K个对象，覆盖100种音频-视觉事件；AVST-Zero采用强化学习方法直接优化行为。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频理解任务上进展迅速，但主要关注简单视频场景，无法反映真实世界中复杂多样的音频-视觉事件。需要构建更贴近现实的数据集和模型。

Method: 1) 构建R-AVST数据集：使用LLM提取关键对象、自动空间标注和人工质量检查；2) 提出AVST-Zero模型：基于强化学习，避免中间监督，通过多维奖励直接优化行为。

Result: 实验验证了R-AVST数据集在推进音频-视频时空推理方面的有效性，AVST-Zero相比现有模型表现出竞争力。

Conclusion: R-AVST是首个专为真实世界音频-视频时空推理设计的数据集，AVST-Zero为该领域未来挑战提供了新的解决视角。

Abstract: Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.

</details>


### [3] [Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models](https://arxiv.org/abs/2511.17094)
*He Huang,Zixuan Hu,Dongxiao Li,Yao Xiao,Ling-Yu Duan*

Main category: cs.CV

TL;DR: ReCoVAD是一个训练自由的视频异常检测框架，受人类神经系统启发，通过选择性帧处理减少冗余计算，在保持SOTA性能的同时仅处理少量帧。


<details>
  <summary>Details</summary>
Motivation: 现有基于大预训练模型的VAD方法依赖密集帧级推理，计算成本高。本文探讨在使用强大预训练模型时，密集推理是否真正必要。

Method: 采用双通路架构：Reflex通路使用轻量CLIP模块融合视觉特征和原型提示，查询动态记忆进行快速响应；Conscious通路使用中规模VLM生成事件描述和精炼异常分数，通过LLM定期审查识别未见异常。

Result: 在UCF-Crime和XD-Violence数据集上达到SOTA训练自由性能，分别仅处理28.55%和16.04%的帧数。

Conclusion: 稀疏推理对于基于大模型的有效VAD是足够的，ReCoVAD证明了选择性帧处理的可行性。

Abstract: Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\% and 16.04\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.

</details>


### [4] [ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better](https://arxiv.org/abs/2511.17106)
*Yuan Zhang,Ming Lu,Junwen Pan,Tao Huang,Kuan Cheng,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: ChainV是一个动态整合视觉提示的多模态推理框架，通过视觉补丁选择和注意力机制使推理更短更准确，在数学密集型基准上显著提升精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型在生成长推理链时存在冗余自反思问题，而基于静态视觉参考的CoT压缩方法对多模态推理增益有限。

Method: ChainV首先基于前一步推理进行粗粒度视觉补丁选择，然后通过平均注意力强度识别最具代表性的原子视觉提示，并引入基于一致性的评估机制来调整自反思程度。

Result: 在MathVista基准上实现2.3%精度提升，推理延迟降低51.4%，输出token长度缩短24.5%。

Conclusion: ChainV通过动态整合视觉提示有效提升了多模态推理的准确性和效率，特别适用于需要视觉提示的多步符号推理任务。

Abstract: Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\%$ and shortening output token length by $24.5\%$.

</details>


### [5] [PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.17116)
*Yijun Xu,Jingrui Zhang,Hongyi Liu,Yuhan Chen,Yuanyang Wang,Qingyao Guo,Dingwen Wang,Lei Yu,Chu He*

Main category: cs.CV

TL;DR: PEGS是一个将物理先验与事件流增强集成到3D高斯溅射管道中的框架，用于执行去模糊的目标聚焦建模和运动恢复，在重建大时空尺度运动方面优于主流动态方法。


<details>
  <summary>Details</summary>
Motivation: 由于建模范式的限制、严重的运动模糊和物理一致性不足，大时空尺度上的刚性运动重建仍然是一个具有挑战性的任务。

Method: 提出了三重监督方案：通过加速度约束强制执行物理合理性，利用事件流进行高时间分辨率指导，并使用卡尔曼正则化器融合多源观测。还设计了基于实时运动状态自适应调度训练过程的运动感知模拟退火策略。

Result: 实验表明，与主流动态方法相比，PEGS在重建大时空尺度运动方面表现出优越性能。

Conclusion: PEGS框架通过整合物理先验和事件流增强，成功解决了大时空尺度刚性运动重建的挑战，并贡献了首个针对自然快速刚性运动的RGB-Event配对数据集。

Abstract: Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.

</details>


### [6] [FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle](https://arxiv.org/abs/2511.17171)
*Mario Markov,Stefan Maria Ailuro,Luc Van Gool,Konrad Schindler,Danda Pani Paudel*

Main category: cs.CV

TL;DR: FireScope是一个基于视觉语言模型的推理生成框架，通过结合哨兵2号影像和气候数据来预测野火风险图，并生成互补的推理轨迹，实现了跨大陆的泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有野火风险预测方法缺乏因果推理和多模态理解能力，难以实现可靠的泛化。需要开发能够整合视觉、气候和地理因素的综合框架。

Method: 提出FireScope-Bench数据集和基准，结合Sentinel-2影像、气候数据和专家定义的风险栅格；开发FireScope框架，通过强化学习和视觉监督学习预测风险栅格并生成推理轨迹。

Result: 在美国训练并在欧洲测试时，FireScope实现了显著的性能提升，专家反馈和自动分析确认其推理轨迹具有忠实性和语义意义。

Conclusion: 基于语言的推理可以提升视觉生成模型的泛化能力，FireScope-Bench有望成为推进推理驱动、可解释和可泛化空间建模的基础。

Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.

</details>


### [7] [Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers](https://arxiv.org/abs/2511.17421)
*Christopher Boland,Sotirios Tsaftaris,Sonia Dahdouh*

Main category: cs.CV

TL;DR: 提出一种新颖的知识蒸馏框架，利用在任务相关数据子集上微调的教师网络来缓解在大规模偏差数据上训练的学生网络的捷径学习问题，在多个医学影像数据集上取得优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易学习训练数据中虚假相关的捷径特征，在医学影像分析等高风险应用中，这可能导致模型不使用临床相关特征进行预测，影响稳健性并对患者造成危害。

Method: 提出基于知识蒸馏的框架，使用在无偏数据子集上微调的教师网络来指导在偏差数据上训练的学生网络，针对不同类型捷径特征在网络不同层的表现特点进行缓解。

Result: 在CheXpert、ISIC 2017和SimBA数据集上使用多种架构进行实验，相比传统经验风险最小化、基于增强和基于组别的偏差缓解方法，均取得一致改进，在多数情况下能达到与在无偏数据上训练基线模型相当的性能。

Conclusion: 该方法在真实医学影像场景中具有实际应用价值，特别是在偏差标注有限且捷径特征难以先验识别的情况下。

Abstract: Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.

</details>


### [8] [Counterfactual World Models via Digital Twin-conditioned Video Diffusion](https://arxiv.org/abs/2511.17481)
*Yiqing Shen,Aiza Maksutova,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: CWMDT框架将标准视频扩散模型转化为反事实世界模型，通过构建场景的数字孪生表示，使用大语言模型进行推理，生成反事实视觉序列。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要关注事实观察的前向模拟，无法回答反事实查询（如"如果移除这个对象会发生什么"），这在评估物理AI行为等应用中越来越重要。

Method: 1) 构建场景的数字孪生，显式编码对象及其关系为结构化文本；2) 使用大语言模型推理反事实干预如何随时间传播；3) 用修改后的表示条件化视频扩散模型生成反事实视觉序列。

Result: 在两个基准测试中，CWMDT方法实现了最先进的性能，表明数字孪生等替代视频表示为基于视频前向模拟的世界模型提供了强大的控制信号。

Conclusion: CWMDT框架成功地将标准视频扩散模型转化为有效的反事实世界模型，通过结构化表示和语言模型推理实现了对特定场景属性的针对性干预。

Abstract: World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as "what would happen if this object was removed?", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.

</details>


### [9] [EvDiff: High Quality Video with an Event Camera](https://arxiv.org/abs/2511.17492)
*Weilun Li,Lei Sun,Ruixi Gao,Qi Jiang,Yuqin Ma,Kaiwei Wang,Ming-Hsuan Yang,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: EvDiff是一个基于事件的扩散模型，通过单步前向扩散和代理训练框架，从单色事件流生成高质量彩色视频，在保真度和真实感方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机记录稀疏事件流，但从中重建强度图像是高度不适定问题。现有端到端回归方法产生感知质量较差的结果，且难以扩展模型容量和训练数据。

Method: 提出EvDiff事件扩散模型，采用单步前向扩散降低计算成本，配备时间一致的EvEncoder。新颖的代理训练框架消除对配对事件-图像数据集的依赖，可利用大规模图像数据集。

Result: 能够仅从单色事件流生成高质量彩色视频。在真实世界数据集上的实验表明，该方法在像素级和感知指标上都优于现有方法。

Conclusion: EvDiff在保真度和真实感之间找到了平衡点，为事件相机图像重建提供了有效的解决方案。

Abstract: As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [A Hybrid Computational Intelligence Framework for scRNA-seq Imputation: Integrating scRecover and Random Forests](https://arxiv.org/abs/2511.16923)
*Ali Anaissi,Deshao Liu,Yuanzhe Jia,Weidong Huang,Widad Alyassine,Junaid Akram*

Main category: cs.LG

TL;DR: SCR-MF是一个模块化双阶段工作流，结合了scRecover的dropout检测和missForest的非参数插补，在单细胞RNA测序数据中实现稳健且可解释的插补性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序存在普遍的dropout事件，会掩盖生物信号，需要有效的插补方法来恢复这些丢失的数据。

Method: 采用模块化双阶段工作流：第一阶段使用scRecover进行原理性dropout检测，第二阶段使用missForest进行稳健的非参数插补。

Result: 在公共和模拟数据集上，SCR-MF在大多数情况下达到或超过现有插补方法的性能，同时保持生物保真度和透明度。运行时分析显示其在准确性和计算效率之间取得良好平衡。

Conclusion: SCR-MF适合中等规模单细胞数据集，提供了稳健、可解释且计算效率高的插补解决方案。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables transcriptomic profiling at cellular resolution but suffers from pervasive dropout events that obscure biological signals. We present SCR-MF, a modular two-stage workflow that combines principled dropout detection using scRecover with robust non-parametric imputation via missForest. Across public and simulated datasets, SCR-MF achieves robust and interpretable performance comparable to or exceeding existing imputation methods in most cases, while preserving biological fidelity and transparency. Runtime analysis demonstrates that SCR-MF provides a competitive balance between accuracy and computational efficiency, making it suitable for mid-scale single-cell datasets.

</details>


### [11] [A novel approach to classification of ECG arrhythmia types with latent ODEs](https://arxiv.org/abs/2511.16933)
*Angelina Yan,Matt L. Sampson,Peter Melchior*

Main category: cs.LG

TL;DR: 提出了一种端到端的心电图分类方法，通过潜在ODE建模连续ECG波形，在不同采样频率下保持高性能，解决了可穿戴设备电池寿命与信号保真度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 12导联ECG虽然采样频率高但只能短期监测，而可穿戴ECG虽然能长期监测但受电池限制采样频率较低且不规则，难以进行形态分析。

Method: 训练潜在ODE模型来建模连续ECG波形，从高频单通道信号创建鲁棒特征向量，通过将360Hz ECG下采样到90Hz和45Hz构建三个潜在向量，然后使用梯度提升树进行分类。

Result: 在不同频率下性能下降最小，360Hz、90Hz和45Hz的宏平均AUC-ROC值分别为0.984、0.978和0.976。

Conclusion: 该方法能够绕过信号保真度与电池寿命之间的权衡，使更小的可穿戴设备成为可能，促进心脏健康的长期监测。

Abstract: 12-lead ECGs with high sampling frequency are the clinical gold standard for arrhythmia detection, but their short-term, spot-check nature often misses intermittent events. Wearable ECGs enable long-term monitoring but suffer from irregular, lower sampling frequencies due to battery constraints, making morphology analysis challenging. We present an end-to-end classification pipeline to address these issues. We train a latent ODE to model continuous ECG waveforms and create robust feature vectors from high-frequency single-channel signals. We construct three latent vectors per waveform via downsampling the initial 360 Hz ECG to 90 Hz and 45 Hz. We then use a gradient boosted tree to classify these vectors and test robustness across frequencies. Performance shows minimal degradation, with macro-averaged AUC-ROC values of 0.984, 0.978, and 0.976 at 360 Hz, 90 Hz, and 45 Hz, respectively, suggesting a way to sidestep the trade-off between signal fidelity and battery life. This enables smaller wearables, promoting long-term monitoring of cardiac health.

</details>


### [12] [Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering](https://arxiv.org/abs/2511.17008)
*Zexi Tan,Xiaopeng Luo,Yunlin Liu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 提出EMTC方法，通过重要性感知的变量级掩码和多源视图表示学习，解决多元时间序列聚类中冗余信息导致的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列包含大量冗余信息（如稳态机器运行记录、太阳能发电零输出期），这些冗余降低了模型对判别性时间戳的关注，导致聚类性能瓶颈。现有掩码策略多为独立预处理步骤，无法动态适应聚类关键时间戳的重要性变化。

Method: 提出EMTC方法，包含重要性感知变量级掩码(IVM)和多源视图(MEV)表示学习模块。IVM自适应引导模型学习更具判别性的聚类表示，MEV通过重构和对比学习路径增强泛化能力。

Result: 在15个真实基准数据集上的实验表明，EMTC优于8种最先进方法，平均比最强基线提升4.85%。

Conclusion: EMTC通过动态掩码和多重表示学习，有效解决了多元时间序列聚类中的冗余信息问题，显著提升了聚类性能。

Abstract: Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, with its model architecture composed of Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) representation learning modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the MEV-based reconstruction and contrastive learning pathways enhance the generalization. That is, the MEV reconstruction facilitates multi-perspective complementary to prevent the masking from premature convergence, and the clustering-guided contrastive learning facilitates the joint optimization of representation and clustering. Extensive experiments on 15 real benchmark datasets demonstrate the superiority of EMTC in comparison with eight SOTA methods, where the EMTC achieves an average improvement of 4.85% over the strongest baselines.

</details>


### [13] [Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels](https://arxiv.org/abs/2511.17040)
*Wenzhang Du*

Main category: cs.LG

TL;DR: Step-E是一个将样本选择和模型学习集成到单一优化过程中的框架，通过基于损失的样本排序和渐进式排除高损失样本，有效处理噪声标签和异常值。


<details>
  <summary>Details</summary>
Motivation: 野外收集的训练数据常包含噪声标签和异常值，这会严重降低深度神经网络的性能和可靠性。传统的数据清洗作为独立预处理阶段，无法充分利用下游模型的反馈，也无法适应未知的噪声模式。

Method: Step-E在每个epoch根据损失对样本进行排序，经过短暂预热阶段后，逐步增加从梯度更新中排除的高损失样本比例，形成在线课程学习机制，专注于简单一致的样本并最终忽略持续异常值。

Result: 在CIFAR-100N上，Step-E将ResNet-18模型的测试准确率从43.3%提升到50.4%，明显优于损失截断、自定步调学习和一次性过滤方法，接近干净标签oracle的60.5%。在CIFAR-10N上也优于噪声基线(85.3% vs. 83.9%)，接近干净标签oracle(85.9%)，训练时间开销适中。

Conclusion: Step-E通过集成样本选择和模型学习，有效处理噪声标签数据，在多个数据集上显著提升模型性能，接近干净标签的性能水平。

Abstract: Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the downstream model nor adapt to unknown noise patterns. We propose Step-E, a simple framework that integrates sample selection and model learning into a single optimization process. At each epoch, Step-E ranks samples by loss and gradually increases the fraction of high-loss examples that are excluded from gradient updates after a brief warm-up stage, yielding an online curriculum that focuses on easy and consistent examples and eventually ignores persistent outliers. On CIFAR-100N, Step-E improves the test accuracy of a ResNet-18 model from 43.3% (+/- 0.7%) to 50.4% (+/- 0.9%), clearly outperforming loss truncation, self-paced learning, and one-shot filtering while approaching the clean-label oracle at 60.5% (+/- 0.2%). On CIFAR-10N (aggre), Step-E also improves over the noisy baseline (85.3% vs. 83.9%) and nearly matches the clean-label oracle (85.9%), with only moderate training-time overhead.

</details>


### [14] [ReBaPL: Repulsive Bayesian Prompt Learning](https://arxiv.org/abs/2511.17339)
*Yassir Bendou,Omar Ezzahir,Eduardo Fernandes Montesuma,Gabriel Mahuas,Victoria Shevchenko,Mike Gartrell*

Main category: cs.LG

TL;DR: 提出Repulsive Bayesian Prompt Learning (ReBaPL)方法，通过贝叶斯推理框架优化提示学习，结合循环步长调度和随机梯度哈密顿蒙特卡洛算法，并引入表示空间的排斥力来增强探索多样性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统提示学习方法容易过拟合且泛化能力不足，贝叶斯提示学习通过概率推理增强鲁棒性，但需要更有效地探索复杂的多模态后验分布。

Method: 结合循环步长调度与SGHMC算法，交替进行探索和利用阶段；引入基于最大均值差异和Wasserstein距离的表示空间排斥力，防止过早收敛到单一模式。

Result: 在多个基准数据集上验证了ReBaPL的有效性，相比最先进的提示学习方法表现出更优越的性能。

Conclusion: ReBaPL提供了一种模块化的贝叶斯扩展方法，能够更全面地刻画提示后验分布，显著提升泛化能力，可作为现有基于最大似然估计的提示学习方法的即插即用扩展。

Abstract: Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Detecting and Steering LLMs' Empathy in Action](https://arxiv.org/abs/2511.16699)
*Juan P. Cadile*

Main category: cs.CL

TL;DR: 研究发现共情行为（牺牲任务效率满足人类需求）是LLM激活空间中的线性方向，可在多个模型中检测和操控，但不同模型在操控稳健性上存在差异。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型中是否编码了共情行为，以及这种编码是否独立于安全训练，并研究如何检测和操控这种共情行为。

Method: 使用基于EIA基准的对比提示，在Phi-3-mini-4k、Qwen2.5-7B和Dolphin-Llama-3.1-8B三个模型上进行检测和操控实验，分析激活空间中的线性方向。

Result: 所有模型都能高精度检测共情行为（AUROC 0.996-1.00），但操控效果各异：Qwen和Phi-3可实现双向操控，Dolphin仅支持增强共情的单向操控，反共情操控会导致模型崩溃。

Conclusion: 共情编码独立于安全训练，但安全训练影响操控的稳健性；不同架构模型实现共情的方式不同，检测和操控能力存在模型特异性差异。

Abstract: We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).
  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.
  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).
  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.

</details>


### [16] [Interpretable dimensions support an effect of agentivity and telicity on split intransitivity](https://arxiv.org/abs/2511.16824)
*Eva Neu,Brian Dillon,Katrin Erk*

Main category: cs.CL

TL;DR: 本研究重新探讨了非宾格动词与施事性/终结性之间的关系，使用基于种子词的可解释维度分析，发现两者确实存在关联，且可解释维度结合人类判断能更好地评估语义属性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为描述施事行为的动词更倾向于非作格句法，而描述终结事件的动词更倾向于非宾格句法。但Kim等人(2024)发现人类对施事性和终结性的评分并不能很好地预测不及物动词的句法行为。

Method: 使用基于种子词的可解释维度，计算施事性和终结性量表对立极点的维度值。

Result: 研究结果支持非作格性/非宾格性与施事性/终结性之间的联系。

Conclusion: 使用可解释维度结合人类判断可以为不易在评分任务中评估的语义属性提供有价值的证据。

Abstract: Intransitive verbs fall into two different syntactic classes, unergatives and unaccusatives. It has long been argued that verbs describing an agentive action are more likely to appear in an unergative syntax, and those describing a telic event to appear in an unaccusative syntax. However, recent work by Kim et al. (2024) found that human ratings for agentivity and telicity were a poor predictor of the syntactic behavior of intransitives. Here we revisit this question using interpretable dimensions, computed from seed words on opposite poles of the agentive and telic scales. Our findings support the link between unergativity/unaccusativity and agentivity/telicity, and demonstrate that using interpretable dimensions in conjunction with human judgments can offer valuable evidence for semantic properties that are not easily evaluated in rating tasks.

</details>


### [17] [Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities](https://arxiv.org/abs/2511.17012)
*Junjie Hao,Chun Wang,Ying Qiao,Qiuyue Zuo,Qiya Song,Hua Ma,Xieping Gao*

Main category: cs.CL

TL;DR: 本研究提出监督微调方法增强领域特定信息提取，针对湖南历史名人领域设计细粒度指令模板并进行参数高效微调，Qwen3-8B在微调后达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 湖南历史名人系统数据资源有限，通用大模型在低资源环境下领域知识提取和结构化输出生成表现不佳，需要增强领域特定信息提取能力。

Method: 设计针对湖南历史名人领域的细粒度模式引导指令模板，构建指令调优数据集，对四个公开大模型进行参数高效指令微调，并开发评估标准。

Result: 所有模型在微调后性能均有显著提升，其中Qwen3-8B表现最佳，在100个样本和50次训练迭代下达到89.3866分。

Conclusion: 本研究为垂直大模型在区域历史文化领域的微调提供了新思路，展示了其在文化遗产知识提取和知识图谱构建中成本效益应用的潜力。

Abstract: Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.

</details>


### [18] [Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models](https://arxiv.org/abs/2511.17170)
*Vy Nguyen,Ziqi Xu,Jeffrey Chan,Estrid He,Feng Xia,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 提出Aspect-Based Causal Abstention (ABCA)框架，通过因果推理分析LLM内部知识多样性来实现早期弃权，防止产生不可靠回答。


<details>
  <summary>Details</summary>
Motivation: 现有弃权方法依赖生成后信号，无法预先防止不可靠响应。LLM从不同来源获取的知识具有多面性，这种多样性可用于评估知识可靠性。

Method: ABCA通过分析LLM内部知识的不同方面（如学科、法律背景、时间框架）的多样性，使用因果推理估计条件因果效应来评估查询相关知识的可靠性。

Result: 在标准基准测试中，ABCA提高了弃权可靠性，达到最先进性能，并增强了弃权决策的可解释性。

Conclusion: ABCA框架通过利用LLM内部知识多样性的因果分析，实现了更可靠的早期弃权机制，优于现有方法。

Abstract: Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as "I don't know", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.

</details>


### [19] [A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2511.17208)
*Sizhe Zhou*

Main category: cs.CL

TL;DR: 提出基于事件语义的对话记忆方法，将对话历史表示为简短的事件命题而非独立三元组或不透明摘要，通过构建异质图支持关联检索，在长对话基准上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM对话代理在多轮对话中保持连贯性和个性化的问题，传统方法受限于固定上下文窗口，外部记忆方法在粗粒度检索和细粒度碎片化视图之间难以平衡。

Method: 将每个会话分解为丰富的语篇基本单元（EDUs），包含归一化实体和来源轮次属性，组织会话、EDUs及其参数到异质图中，支持密集相似性搜索和LLM过滤的检索方法。

Result: 在LoCoMo和LongMemEval_S基准测试中，事件中心记忆方法匹配或超越强基线，同时使用更短的QA上下文，表明事件级记忆为长视野对话代理提供了原则性实用基础。

Conclusion: 结构简单的事件级记忆为长视野对话代理提供了原则性和实用性的基础，能够更好地保持对话的连贯性和个性化。

Abstract: LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.

</details>


### [20] [Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats](https://arxiv.org/abs/2511.17315)
*Mateusz Jacniacki,Martí Carmona Serrat*

Main category: cs.CL

TL;DR: 提出了HUMA系统，这是一个基于LLM的多用户对话代理，能够在群聊中使用类人策略和时机进行自然交互，在实验中参与者难以区分AI与人类社区管理者。


<details>
  <summary>Details</summary>
Motivation: 当前大多数对话系统设计为一对一的轮转交流，而非自然的异步群聊。随着AI助手在数字平台中普及，开发自然、类人的交互模式对于维持用户信任和参与度至关重要。

Method: HUMA采用事件驱动架构，包含三个组件：路由器、行动代理和反思模块，能够处理消息、回复、反应并引入真实的响应时间模拟，适应群聊动态。

Result: 在97名参与者的四人角色扮演聊天研究中，参与者对AI和人类社区管理者的分类准确率接近随机水平，无法可靠区分HUMA代理与人类。主观体验在各项指标上差异很小，效应量较小。

Conclusion: 在自然群聊环境中，AI协调者能够达到与人类相当的质量，同时难以被识别为非人类。

Abstract: Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.
  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [21] [Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization](https://arxiv.org/abs/2511.16911)
*Yendo Hu,Yiliang Wu,Weican Chen*

Main category: cs.RO

TL;DR: 提出了一种结合改进多机器人编队避障算法和增强人工势场法的混合算法，解决多无人机场景中传统APF方法的路径冗余、航向突变和碰撞风险问题。


<details>
  <summary>Details</summary>
Motivation: 传统人工势场法在多无人机场景中会导致飞行路径冗余、航向频繁突变，且在避障过程中容易发生无人机间碰撞。

Method: 结合改进的MRF IAPF编队避障算法和增强的单无人机路径规划APF，集成障碍物排斥力、无人机间相互作用力和目标吸引力，并加入碰撞风险评估和辅助子目标策略。

Result: 仿真结果表明，相比传统APF编队算法，该算法在路径长度优化和航向稳定性方面有显著改进，能有效避障并快速恢复编队配置。

Conclusion: 验证了该算法在未知障碍物静态环境中的适用性和有效性。

Abstract: In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.

</details>


### [22] [FORWARD: Dataset of a forwarder operating in rough terrain](https://arxiv.org/abs/2511.17318)
*Mikael Lundbäck,Erik Wallin,Carola Häggström,Mattias Nyström,Andreas Grönlund,Mats Richardson,Petrus Jönsson,William Arnvik,Lucas Hedström,Arvid Fälldin,Martin Servin*

Main category: cs.RO

TL;DR: FORWARD是一个高分辨率多模态数据集，记录了瑞典中部两个采伐现场的伐木集材机作业数据，包含RTK-GNSS、360度摄像头、振动传感器等多种传感器数据，用于开发林业机械的交通性、感知和自主控制模型。


<details>
  <summary>Details</summary>
Motivation: 为开发林业机械的交通性、感知和自主控制模型提供高质量的多模态数据集，支持人工智能、仿真和物理测试平台的研究。

Method: 使用配备多种传感器的大型Komatsu集材机在瑞典中部采伐现场收集数据，包括RTK-GNSS、360度摄像头、振动传感器、CAN总线信号和IMU等，数据采集频率为5Hz，涵盖厘米级精度的车辆位置、行驶速度、燃油消耗等信息。

Result: 构建了包含约18小时常规木材提取工作的数据集，包含高分辨率激光扫描地形数据（约1500点/平方米）、生产日志文件、视频素材和地形数据，并对360度视频材料进行了工作元素标注。

Conclusion: FORWARD数据集为林业机械的自动化控制、交通性分析和仿真校准提供了宝贵资源，有助于提高作业效率、降低燃油消耗并减少环境影响。

Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.

</details>
