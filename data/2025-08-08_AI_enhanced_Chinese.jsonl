{"id": "2508.04834", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04834", "abs": "https://arxiv.org/abs/2508.04834", "authors": ["Morten Roed Frederiksen", "Kasper St\u00f8y"], "title": "On the causality between affective impact and coordinated human-robot reactions", "comment": "7 pages, 5 figures, 29th IEEE International Workshop on Robot and\n  Human Communication (ROMAN)", "summary": "In an effort to improve how robots function in social contexts, this paper\ninvestigates if a robot that actively shares a reaction to an event with a\nhuman alters how the human perceives the robot's affective impact. To verify\nthis, we created two different test setups. One to highlight and isolate the\nreaction element of affective robot expressions, and one to investigate the\neffects of applying specific timing delays to a robot reacting to a physical\nencounter with a human. The first test was conducted with two different groups\n(n=84) of human observers, a test group and a control group both interacting\nwith the robot. The second test was performed with 110 participants using\nincreasingly longer reaction delays for the robot with every ten participants.\nThe results show a statistically significant change (p$<$.05) in perceived\naffective impact for the robots when they react to an event shared with a human\nobserver rather than reacting at random. The result also shows for shared\nphysical interaction, the near-human reaction times from the robot are most\nappropriate for the scenario. The paper concludes that a delay time around\n200ms may render the biggest impact on human observers for small-sized\nnon-humanoid robots. It further concludes that a slightly shorter reaction time\naround 100ms is most effective when the goal is to make the human observers\nfeel they made the biggest impact on the robot.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u4e3b\u52a8\u4e0e\u4eba\u7c7b\u5171\u4eab\u4e8b\u4ef6\u53cd\u5e94\u662f\u5426\u5f71\u54cd\u4eba\u7c7b\u5bf9\u5176\u60c5\u611f\u5f71\u54cd\u7684\u611f\u77e5\uff0c\u53d1\u73b0\u5171\u4eab\u53cd\u5e94\u548c\u8fd1\u4eba\u7c7b\u53cd\u5e94\u65f6\u95f4\uff08\u7ea6200ms\uff09\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u529f\u80fd\uff0c\u63a2\u7d22\u5176\u53cd\u5e94\u884c\u4e3a\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e24\u4e2a\u5b9e\u9a8c\uff1a\u4e00\u4e2a\u9694\u79bb\u673a\u5668\u4eba\u53cd\u5e94\u5143\u7d20\uff0c\u53e6\u4e00\u4e2a\u6d4b\u8bd5\u4e0d\u540c\u53cd\u5e94\u5ef6\u8fdf\u65f6\u95f4\uff08n=84\u548cn=110\uff09\u3002", "result": "\u5171\u4eab\u4e8b\u4ef6\u7684\u673a\u5668\u4eba\u53cd\u5e94\u663e\u8457\u6539\u53d8\u4eba\u7c7b\u611f\u77e5\uff08p<0.05\uff09\uff0c\u8fd1\u4eba\u7c7b\u53cd\u5e94\u65f6\u95f4\uff08200ms\uff09\u6548\u679c\u6700\u4f73\u3002", "conclusion": "200ms\u5ef6\u8fdf\u5bf9\u5c0f\u578b\u975e\u4eba\u5f62\u673a\u5668\u4eba\u6700\u6709\u6548\uff0c100ms\u5ef6\u8fdf\u5219\u8ba9\u4eba\u7c7b\u611f\u89c9\u5bf9\u673a\u5668\u4eba\u5f71\u54cd\u6700\u5927\u3002"}}
{"id": "2508.04714", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04714", "abs": "https://arxiv.org/abs/2508.04714", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "Prescriptive Agents based on Rag for Automated Maintenance (PARAM)", "comment": null, "summary": "Industrial machinery maintenance requires timely intervention to prevent\ncatastrophic failures and optimize operational efficiency. This paper presents\nan integrated Large Language Model (LLM)-based intelligent system for\nprescriptive maintenance that extends beyond traditional anomaly detection to\nprovide actionable maintenance recommendations. Building upon our prior LAMP\nframework for numerical data analysis, we develop a comprehensive solution that\ncombines bearing vibration frequency analysis with multi agentic generation for\nintelligent maintenance planning. Our approach serializes bearing vibration\ndata (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM\nprocessing, enabling few-shot anomaly detection with high accuracy. The system\nclassifies fault types (inner race, outer race, ball/roller, cage faults) and\nassesses severity levels. A multi-agentic component processes maintenance\nmanuals using vector embeddings and semantic search, while also conducting web\nsearches to retrieve comprehensive procedural knowledge and access up-to-date\nmaintenance practices for more accurate and in-depth recommendations. The\nGemini model then generates structured maintenance recommendations includes\nimmediate actions, inspection checklists, corrective measures, parts\nrequirements, and timeline specifications. Experimental validation in bearing\nvibration datasets demonstrates effective anomaly detection and contextually\nrelevant maintenance guidance. The system successfully bridges the gap between\ncondition monitoring and actionable maintenance planning, providing industrial\npractitioners with intelligent decision support. This work advances the\napplication of LLMs in industrial maintenance, offering a scalable framework\nfor prescriptive maintenance across machinery components and industrial\nsectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u68b0\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u7ed3\u5408\u632f\u52a8\u9891\u7387\u5206\u6790\u548c\u591a\u4ee3\u7406\u751f\u6210\u6280\u672f\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7ef4\u62a4\u5efa\u8bae\u3002", "motivation": "\u5de5\u4e1a\u673a\u68b0\u7ef4\u62a4\u9700\u8981\u53ca\u65f6\u5e72\u9884\u4ee5\u9632\u6b62\u707e\u96be\u6027\u6545\u969c\u5e76\u4f18\u5316\u8fd0\u884c\u6548\u7387\uff0c\u4f20\u7edf\u65b9\u6cd5\u4ec5\u80fd\u68c0\u6d4b\u5f02\u5e38\uff0c\u7f3a\u4e4f\u5177\u4f53\u7684\u7ef4\u62a4\u5efa\u8bae\u3002", "method": "\u7cfb\u7edf\u5c06\u8f74\u627f\u632f\u52a8\u6570\u636e\uff08BPFO\u3001BPFI\u3001BSF\u3001FTF\u9891\u7387\uff09\u5e8f\u5217\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u4f9bLLM\u5904\u7406\uff0c\u7ed3\u5408\u591a\u4ee3\u7406\u7ec4\u4ef6\u5206\u6790\u7ef4\u62a4\u624b\u518c\u548c\u7f51\u7edc\u641c\u7d22\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7ef4\u62a4\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7ef4\u62a4\u6307\u5bfc\uff0c\u586b\u8865\u4e86\u72b6\u6001\u76d1\u6d4b\u4e0e\u53ef\u64cd\u4f5c\u7ef4\u62a4\u8ba1\u5212\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86LLM\u5728\u5de5\u4e1a\u7ef4\u62a4\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u8de8\u673a\u68b0\u7ec4\u4ef6\u548c\u5de5\u4e1a\u9886\u57df\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2508.04827", "categories": ["cs.CV", "68T05, 68T07", "I.2.10; I.5.1; I.4.8; J.4"], "pdf": "https://arxiv.org/pdf/2508.04827", "abs": "https://arxiv.org/abs/2508.04827", "authors": ["Chirag Seth", "Divya Naiken", "Keyan Lin"], "title": "A deep learning approach to track eye movements based on events", "comment": null, "summary": "This research project addresses the challenge of accurately tracking eye\nmovements during specific events by leveraging previous research. Given the\nrapid movements of human eyes, which can reach speeds of 300{\\deg}/s, precise\neye tracking typically requires expensive and high-speed cameras. Our primary\nobjective is to locate the eye center position (x, y) using inputs from an\nevent camera. Eye movement analysis has extensive applications in consumer\nelectronics, especially in VR and AR product development. Therefore, our\nultimate goal is to develop an interpretable and cost-effective algorithm using\ndeep learning methods to predict human attention, thereby improving device\ncomfort and enhancing overall user experience. To achieve this goal, we\nexplored various approaches, with the CNN\\_LSTM model proving most effective,\nachieving approximately 81\\% accuracy. Additionally, we propose future work\nfocusing on Layer-wise Relevance Propagation (LRP) to further enhance the\nmodel's interpretability and predictive performance.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u6df1\u5ea6\u5b66\u4e60\uff08CNN_LSTM\u6a21\u578b\uff09\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u773c\u7403\u4e2d\u5fc3\u5b9a\u4f4d\uff0c\u51c6\u786e\u7387\u8fbe81%\uff0c\u672a\u6765\u8ba1\u5212\u901a\u8fc7LRP\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u773c\u7403\u8fd0\u52a8\uff08300\u5ea6/\u79d2\uff09\u7684\u7cbe\u786e\u8ffd\u8e2a\u95ee\u9898\uff0c\u964d\u4f4e\u5bf9\u6602\u8d35\u9ad8\u901f\u76f8\u673a\u7684\u4f9d\u8d56\uff0c\u5e94\u7528\u4e8eVR/AR\u8bbe\u5907\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528CNN_LSTM\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u8f93\u5165\uff0c\u5b9a\u4f4d\u773c\u7403\u4e2d\u5fc3\u4f4d\u7f6e\uff08x, y\uff09\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u7ea6\u4e3a81%\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4f4e\u6210\u672c\u4e14\u9ad8\u6548\u7684\u773c\u7403\u8ffd\u8e2a\u7b97\u6cd5\uff0c\u672a\u6765\u5c06\u901a\u8fc7LRP\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u3002"}}
{"id": "2508.04945", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04945", "abs": "https://arxiv.org/abs/2508.04945", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tianyu Jiang"], "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering", "comment": "18 pages, 5 figures", "summary": "Evaluating visual activity recognition systems is challenging due to inherent\nambiguities in verb semantics and image interpretation. When describing actions\nin images, synonymous verbs can refer to the same event (e.g., brushing vs.\ngrooming), while different perspectives can lead to equally valid but distinct\nverb choices (e.g., piloting vs. operating). Standard exact-match evaluation,\nwhich relies on a single gold answer, fails to capture these ambiguities,\nresulting in an incomplete assessment of model performance. To address this, we\npropose a vision-language clustering framework that constructs verb sense\nclusters, providing a more robust evaluation. Our analysis of the imSitu\ndataset shows that each image maps to an average of 2.8 sense clusters, with\neach cluster representing a distinct perspective of the image. We evaluate\nmultiple activity recognition models and compare our cluster-based evaluation\nwith standard evaluation methods. Additionally, our human alignment analysis\nsuggests that the cluster-based evaluation better aligns with human judgements,\noffering a more nuanced assessment of model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u805a\u7c7b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u89c6\u89c9\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u52a8\u8bcd\u8bed\u4e49\u548c\u56fe\u50cf\u89e3\u91ca\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u7cbe\u786e\u5339\u914d\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u52a8\u8bcd\u8bed\u4e49\u548c\u56fe\u50cf\u89e3\u91ca\u7684\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u805a\u7c7b\u6846\u67b6\uff0c\u6784\u5efa\u52a8\u8bcd\u610f\u4e49\u7c07\uff0c\u7528\u4e8e\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u3002", "result": "\u5728imSitu\u6570\u636e\u96c6\u4e0a\uff0c\u6bcf\u5f20\u56fe\u50cf\u5e73\u5747\u6620\u5c04\u52302.8\u4e2a\u610f\u4e49\u7c07\uff0c\u6bcf\u4e2a\u7c07\u4ee3\u8868\u56fe\u50cf\u7684\u4e0d\u540c\u89c6\u89d2\u3002\u805a\u7c7b\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "conclusion": "\u805a\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\uff0c\u4f18\u4e8e\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2508.05003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05003", "abs": "https://arxiv.org/abs/2508.05003", "authors": ["Song Wang", "Yishu Wei", "Haotian Ma", "Max Lovitt", "Kelly Deng", "Yuan Meng", "Zihan Xu", "Jingze Zhang", "Yunyu Xiao", "Ying Ding", "Xuhai Xu", "Joydeep Ghosh", "Yifan Peng"], "title": "A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health", "comment": null, "summary": "Background: Understanding social determinants of health (SDoH) factors\ncontributing to suicide incidents is crucial for early intervention and\nprevention. However, data-driven approaches to this goal face challenges such\nas long-tailed factor distributions, analyzing pivotal stressors preceding\nsuicide incidents, and limited model explainability. Methods: We present a\nmulti-stage large language model framework to enhance SDoH factor extraction\nfrom unstructured text. Our approach was compared to other state-of-the-art\nlanguage models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning\nmodels (i.e., DeepSeek-R1). We also evaluated how the model's explanations help\npeople annotate SDoH factors more quickly and accurately. The analysis included\nboth automated comparisons and a pilot user study. Results: We show that our\nproposed framework demonstrated performance boosts in the overarching task of\nextracting SDoH factors and in the finer-grained tasks of retrieving relevant\ncontext. Additionally, we show that fine-tuning a smaller, task-specific model\nachieves comparable or better performance with reduced inference costs. The\nmulti-stage design not only enhances extraction but also provides intermediate\nexplanations, improving model explainability. Conclusions: Our approach\nimproves both the accuracy and transparency of extracting suicide-related SDoH\nfrom unstructured texts. These advancements have the potential to support early\nidentification of individuals at risk and inform more effective prevention\nstrategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\uff08SDoH\uff09\uff0c\u63d0\u5347\u81ea\u6740\u4e8b\u4ef6\u76f8\u5173\u56e0\u7d20\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u6740\u4e8b\u4ef6\u4e2dSDoH\u56e0\u7d20\u5206\u6790\u9762\u4e34\u7684\u6311\u6218\uff0c\u5982\u957f\u5c3e\u5206\u5e03\u3001\u5173\u952e\u538b\u529b\u6e90\u5206\u6790\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5e76\u4e0eBioBERT\u3001GPT-3.5-turbo\u548cDeepSeek-R1\u7b49\u6a21\u578b\u5bf9\u6bd4\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u3002", "result": "\u6846\u67b6\u5728SDoH\u56e0\u7d20\u63d0\u53d6\u548c\u76f8\u5173\u4e0a\u4e0b\u6587\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5c0f\u89c4\u6a21\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u5728\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u81ea\u6740\u76f8\u5173SDoH\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u98ce\u9669\u8bc6\u522b\u548c\u9884\u9632\u7b56\u7565\u5236\u5b9a\u3002"}}
{"id": "2508.04780", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.04780", "abs": "https://arxiv.org/abs/2508.04780", "authors": ["Lin Jiang", "Dahai Yu", "Rongchao Xu", "Tian Tang", "Guang Wang"], "title": "Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration", "comment": "9 pages,12 figures", "summary": "The increasing frequency of extreme weather events, such as hurricanes,\nhighlights the urgent need for efficient and equitable power system\nrestoration. Many electricity providers make restoration decisions primarily\nbased on the volume of power restoration requests from each region. However,\nour data-driven analysis reveals significant disparities in request submission\nvolume, as disadvantaged communities tend to submit fewer restoration requests.\nThis disparity makes the current restoration solution inequitable, leaving\nthese communities vulnerable to extended power outages. To address this, we aim\nto propose an equity-aware power restoration strategy that balances both\nrestoration efficiency and equity across communities. However, achieving this\ngoal is challenging for two reasons: the difficulty of predicting repair\ndurations under dataset heteroscedasticity, and the tendency of reinforcement\nlearning agents to favor low-uncertainty actions, which potentially undermine\nequity. To overcome these challenges, we design a predict-then-optimize\nframework called EPOPR with two key components: (1) Equity-Conformalized\nQuantile Regression for uncertainty-aware repair duration prediction, and (2)\nSpatial-Temporal Attentional RL that adapts to varying uncertainty levels\nacross regions for equitable decision-making. Experimental results show that\nour EPOPR effectively reduces the average power outage duration by 3.60% and\ndecreases inequity between different communities by 14.19% compared to\nstate-of-the-art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u7535\u529b\u6062\u590d\u7b56\u7565EPOPR\uff0c\u901a\u8fc7\u9884\u6d4b-\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u73b0\u6709\u6062\u590d\u65b9\u6848\u4e2d\u7684\u4e0d\u516c\u5e73\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u505c\u7535\u65f6\u95f4\u548c\u793e\u533a\u95f4\u7684\u4e0d\u5e73\u7b49\u3002", "motivation": "\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u9891\u53d1\uff0c\u73b0\u6709\u7535\u529b\u6062\u590d\u65b9\u6848\u56e0\u57fa\u4e8e\u8bf7\u6c42\u91cf\u51b3\u7b56\u800c\u5bfc\u81f4\u5f31\u52bf\u793e\u533a\u6062\u590d\u4e0d\u8db3\uff0c\u4e9f\u9700\u517c\u987e\u6548\u7387\u4e0e\u516c\u5e73\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86EPOPR\u6846\u67b6\uff0c\u5305\u542bEquity-Conformalized Quantile Regression\uff08\u9884\u6d4b\u4fee\u590d\u65f6\u95f4\uff09\u548cSpatial-Temporal Attentional RL\uff08\u516c\u5e73\u51b3\u7b56\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aEPOPR\u5e73\u5747\u51cf\u5c11\u505c\u7535\u65f6\u95f43.60%\uff0c\u964d\u4f4e\u793e\u533a\u95f4\u4e0d\u5e73\u7b4914.19%\u3002", "conclusion": "EPOPR\u6210\u529f\u5e73\u8861\u4e86\u7535\u529b\u6062\u590d\u7684\u6548\u7387\u4e0e\u516c\u5e73\uff0c\u4e3a\u672a\u6765\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.04843", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04843", "abs": "https://arxiv.org/abs/2508.04843", "authors": ["Xiao Shou"], "title": "Unified Flow Matching for Long Horizon Event Forecasting", "comment": "7 pages", "summary": "Modeling long horizon marked event sequences is a fundamental challenge in\nmany real-world applications, including healthcare, finance, and user behavior\nmodeling. Existing neural temporal point process models are typically\nautoregressive, predicting the next event one step at a time, which limits\ntheir efficiency and leads to error accumulation in long-range forecasting. In\nthis work, we propose a unified flow matching framework for marked temporal\npoint processes that enables non-autoregressive, joint modeling of inter-event\ntimes and event types, via continuous and discrete flow matching. By learning\ncontinuous-time flows for both components, our method generates coherent long\nhorizon event trajectories without sequential decoding. We evaluate our model\non six real-world benchmarks and demonstrate significant improvements over\nautoregressive and diffusion-based baselines in both accuracy and generation\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u81ea\u56de\u5f52\u5730\u8054\u5408\u5efa\u6a21\u6807\u8bb0\u4e8b\u4ef6\u5e8f\u5217\u7684\u65f6\u95f4\u548c\u7c7b\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u8303\u56f4\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u5728\u957f\u8303\u56f4\u4e8b\u4ef6\u5e8f\u5217\u9884\u6d4b\u4e2d\u6548\u7387\u4f4e\u548c\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u548c\u79bb\u6563\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u95f4\u65f6\u95f4\u548c\u4e8b\u4ef6\u7c7b\u578b\uff0c\u907f\u514d\u987a\u5e8f\u89e3\u7801\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u6027\u548c\u751f\u6210\u6548\u7387\u5747\u4f18\u4e8e\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d41\u5339\u914d\u6846\u67b6\u4e3a\u957f\u8303\u56f4\u6807\u8bb0\u4e8b\u4ef6\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05145", "abs": "https://arxiv.org/abs/2508.05145", "authors": ["Sebastiano Dissegna", "Chiara Di Francescomarino", "Massimiliano Ronzani"], "title": "Graph-based Event Log Repair", "comment": null, "summary": "The quality of event logs in Process Mining is crucial when applying any form\nof analysis to them. In real-world event logs, the acquisition of data can be\nnon-trivial (e.g., due to the execution of manual activities and related manual\nrecording or to issues in collecting, for each event, all its attributes), and\noften may end up with events recorded with some missing information. Standard\napproaches to the problem of trace (or log) reconstruction either require the\navailability of a process model that is used to fill missing values by\nleveraging different reasoning techniques or employ a Machine Learning/Deep\nLearning model to restore the missing values by learning from similar cases. In\nrecent years, a new type of Deep Learning model that is capable of handling\ninput data encoded as graphs has emerged, namely Graph Neural Networks. Graph\nNeural Network models, and even more so Heterogeneous Graph Neural Networks,\noffer the advantage of working with a more natural representation of complex\nmulti-modal sequences like the execution traces in Process Mining, allowing for\nmore expressive and semantically rich encodings.\n  In this work, we focus on the development of a Heterogeneous Graph Neural\nNetwork model that, given a trace containing some incomplete events, will\nreturn the full set of attributes missing from those events. We evaluate our\nwork against a state-of-the-art approach leveraging autoencoders on two\nsynthetic logs and four real event logs, on different types of missing values.\nDifferent from state-of-the-art model-free approaches, which mainly focus on\nrepairing a subset of event attributes, the proposed approach shows very good\nperformance in reconstructing all different event attributes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fee\u590d\u6d41\u7a0b\u6316\u6398\u4e2d\u4e8b\u4ef6\u65e5\u5fd7\u7684\u7f3a\u5931\u5c5e\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u4e8b\u4ef6\u65e5\u5fd7\u5e38\u56e0\u6570\u636e\u91c7\u96c6\u95ee\u9898\u5bfc\u81f4\u4fe1\u606f\u7f3a\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6d41\u7a0b\u6a21\u578b\u6216\u673a\u5668\u5b66\u4e60\uff0c\u4f46HGNN\u80fd\u66f4\u81ea\u7136\u5730\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u5e8f\u5217\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cdHGNN\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u4e0d\u5b8c\u6574\u7684\u4e8b\u4ef6\u8f68\u8ff9\u4e2d\u6062\u590d\u6240\u6709\u7f3a\u5931\u5c5e\u6027\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u65e5\u5fd7\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e0e\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cHGNN\u5728\u6240\u6709\u4e8b\u4ef6\u5c5e\u6027\u7684\u4fee\u590d\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "HGNN\u5728\u4e8b\u4ef6\u65e5\u5fd7\u4fee\u590d\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u5168\u9762\u6062\u590d\u7f3a\u5931\u5c5e\u6027\u3002"}}
{"id": "2508.05337", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05337", "abs": "https://arxiv.org/abs/2508.05337", "authors": ["Jiameng Huang", "Baijiong Lin", "Guhao Feng", "Jierun Chen", "Di He", "Lu Hou"], "title": "Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression", "comment": "Technical Report", "summary": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought\nreasoning with complex reflection behaviors, typically signaled by specific\ntrigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance.\nHowever, these reflection behaviors can lead to the overthinking problem where\nthe generation of redundant reasoning steps that unnecessarily increase token\nusage, raise inference costs, and reduce practical utility. In this paper, we\npropose Certainty-Guided Reflection Suppression (CGRS), a novel method that\nmitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS\noperates by dynamically suppressing the model's generation of reflection\ntriggers when it exhibits high confidence in its current response, thereby\npreventing redundant reflection cycles without compromising output quality. Our\napproach is model-agnostic, requires no retraining or architectural\nmodifications, and can be integrated seamlessly with existing autoregressive\ngeneration pipelines. Extensive experiments across four reasoning benchmarks\n(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it\nreduces token usage by an average of 18.5% to 41.9% while preserving accuracy.\nIt also achieves the optimal balance between length reduction and performance\ncompared to state-of-the-art baselines. These results hold consistently across\nmodel architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3\nfamily) and scales (4B to 32B parameters), highlighting CGRS's practical value\nfor efficient reasoning.", "AI": {"tldr": "CGRS\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u6291\u5236\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u7684\u53cd\u601d\u89e6\u53d1\u8bcd\uff0c\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08LRLMs\uff09\u4e2d\u56e0\u53cd\u601d\u884c\u4e3a\u5bfc\u81f4\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5982\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u548c\u964d\u4f4e\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faCertainty-Guided Reflection Suppression\uff08CGRS\uff09\uff0c\u52a8\u6001\u6291\u5236\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u7684\u53cd\u601d\u89e6\u53d1\u8bcd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCGRS\u5e73\u5747\u51cf\u5c1118.5%\u81f341.9%\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "CGRS\u5728\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.05427", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05427", "abs": "https://arxiv.org/abs/2508.05427", "authors": ["Kartar Kumar Lohana Tharwani", "Rajesh Kumar", "Sumita", "Numan Ahmed", "Yong Tang"], "title": "Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation", "comment": null, "summary": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u5728\u6539\u53d8\u6709\u673a\u5408\u6210\u4e2d\u53cd\u5e94\u7684\u8bbe\u8ba1\u4e0e\u6267\u884c\u65b9\u5f0f\uff0c\u7ed3\u5408\u591a\u79cd\u6280\u672f\u52a0\u901f\u5316\u5b66\u53d1\u73b0\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u504f\u89c1\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22LLMs\u5982\u4f55\u4ece\u7406\u8bba\u5de5\u5177\u53d1\u5c55\u4e3a\u5b9e\u7528\u7684\u5b9e\u9a8c\u5ba4\u52a9\u624b\uff0c\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u7684\u7eff\u8272\u5316\u5b66\u3002", "method": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u5b9e\u65f6\u5149\u8c31\u6280\u672f\uff0c\u4f18\u5316LLMs\u5728\u5408\u6210\u5316\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "result": "LLMs\u663e\u8457\u7f29\u77ed\u53d1\u73b0\u5468\u671f\uff0c\u652f\u6301\u66f4\u73af\u4fdd\u7684\u5316\u5b66\u7814\u7a76\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u504f\u89c1\u548c\u5b89\u5168\u6027\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5f00\u653e\u57fa\u51c6\u548c\u53ef\u89e3\u91ca\u754c\u9762\u7b49\u793e\u533a\u5021\u8bae\uff0cLLMs\u6709\u671b\u5b9e\u73b0\u5feb\u901f\u3001\u53ef\u9760\u4e14\u5305\u5bb9\u7684\u5206\u5b50\u521b\u65b0\u3002"}}
{"id": "2508.05509", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05509", "abs": "https://arxiv.org/abs/2508.05509", "authors": ["Yilin Xiao", "Chuang Zhou", "Qinggang Zhang", "Su Dong", "Shengyuan Chen", "Xiao Huang"], "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAG\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u5206\u89e3\u548c\u4f9d\u8d56\u611f\u77e5\u63a8\u7406\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "LAG\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u539f\u5b50\u5b50\u95ee\u9898\uff0c\u6309\u903b\u8f91\u4f9d\u8d56\u987a\u5e8f\u89e3\u51b3\uff0c\u5e76\u5f15\u5165\u903b\u8f91\u7ec8\u6b62\u673a\u5236\u9632\u6b62\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAG\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "conclusion": "LAG\u4e3a\u73b0\u6709RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.05025", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05025", "abs": "https://arxiv.org/abs/2508.05025", "authors": ["Zhehan Qu", "Tianyi Hu", "Christian Fronk", "Maria Gorlatova"], "title": "Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality", "comment": null, "summary": "Augmented Reality (AR) systems, while enhancing task performance through\nreal-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on\nvirtual content that compromises situational awareness (SA) in safety-critical\nscenarios. This paper investigates SA in AR-guided cardiopulmonary\nresuscitation (CPR), where responders must balance effective compressions with\nvigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR\napp on a Magic Leap 2 that overlays real-time CPR feedback (compression depth\nand rate) and conducted a user study with simulated unexpected incidents (e.g.,\nbleeding) to evaluate SA, in which SA metrics were collected via observation\nand questionnaires administered during freeze-probe events. Eye tracking\nanalysis revealed that higher SA levels were associated with greater saccadic\namplitude and velocity, and with reduced proportion and frequency of fixations\non virtual content. To predict SA, we propose FixGraphPool, a graph neural\nnetwork that structures gaze events (fixations, saccades) into spatiotemporal\ngraphs, effectively capturing dynamic attentional patterns. Our model achieved\n83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and\nstate-of-the-art time-series models by leveraging domain knowledge and\nspatial-temporal information encoded in ET data. These findings demonstrate the\npotential of eye tracking for SA modeling in AR and highlight its utility in\ndesigning AR systems that ensure user safety and situational awareness.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AR\u5728CPR\u4e2d\u5982\u4f55\u5f71\u54cd\u60c5\u5883\u611f\u77e5\uff08SA\uff09\uff0c\u63d0\u51fa\u57fa\u4e8e\u773c\u52a8\u7684SA\u9884\u6d4b\u6a21\u578bFixGraphPool\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "AR\u7cfb\u7edf\u53ef\u80fd\u5f15\u53d1\u8ba4\u77e5\u96a7\u9053\u6548\u5e94\uff0c\u964d\u4f4eSA\uff0c\u5c24\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982CPR\uff09\u4e2d\u9700\u5e73\u8861\u865a\u62df\u6307\u5bfc\u4e0e\u73b0\u5b9e\u8b66\u89c9\u3002", "method": "\u5f00\u53d1AR\u5e94\u7528\u63d0\u4f9bCPR\u5b9e\u65f6\u53cd\u9988\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08\u6a21\u62df\u610f\u5916\u4e8b\u4ef6\uff09\u6536\u96c6SA\u6570\u636e\uff0c\u63d0\u51faFixGraphPool\u6a21\u578b\u5206\u6790\u773c\u52a8\u6570\u636e\u3002", "result": "\u9ad8SA\u4e0e\u773c\u52a8\u7279\u5f81\uff08\u5982\u626b\u89c6\u5e45\u5ea6\u3001\u901f\u5ea6\uff09\u76f8\u5173\uff1bFixGraphPool\u6a21\u578b\u9884\u6d4bSA\u51c6\u786e\u7387\u8fbe83.0%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u773c\u52a8\u6570\u636e\u53ef\u7528\u4e8eAR\u4e2d\u7684SA\u5efa\u6a21\uff0c\u4e3a\u8bbe\u8ba1\u517c\u987e\u5b89\u5168\u4e0eSA\u7684AR\u7cfb\u7edf\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2508.05137", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05137", "abs": "https://arxiv.org/abs/2508.05137", "authors": ["Sachin Dudda Nagaraju", "Ashkan Moradi", "Bendik Skarre Abrahamsen", "Mattijs Elschot"], "title": "FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images", "comment": "Paper Accepted at MICCAI 2025 DeCaf Workshop Track", "summary": "Medical image segmentation plays a crucial role in AI-assisted diagnostics,\nsurgical planning, and treatment monitoring. Accurate and robust segmentation\nmodels are essential for enabling reliable, data-driven clinical decision\nmaking across diverse imaging modalities. Given the inherent variability in\nimage characteristics across modalities, developing a unified model capable of\ngeneralizing effectively to multiple modalities would be highly beneficial.\nThis model could streamline clinical workflows and reduce the need for\nmodality-specific training. However, real-world deployment faces major\nchallenges, including data scarcity, domain shift between modalities (e.g., CT\nvs. MRI), and privacy restrictions that prevent data sharing. To address these\nissues, we propose FedGIN, a Federated Learning (FL) framework that enables\nmultimodal organ segmentation without sharing raw patient data. Our method\nintegrates a lightweight Global Intensity Non-linear (GIN) augmentation module\nthat harmonizes modality-specific intensity distributions during local\ntraining. We evaluated FedGIN using two types of datasets: an imputed dataset\nand a complete dataset. In the limited dataset scenario, the model was\ninitially trained using only MRI data, and CT data was added to assess its\nperformance improvements. In the complete dataset scenario, both MRI and CT\ndata were fully utilized for training on all clients. In the limited-data\nscenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test\ncases compared to FL without GIN and consistently outperformed local baselines.\nIn the complete dataset scenario, FedGIN demonstrated near-centralized\nperformance, with a 30% Dice score improvement over the MRI-only baseline and a\n10% improvement over the CT-only baseline, highlighting its strong\ncross-modality generalization under privacy constraints.", "AI": {"tldr": "FedGIN\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5f3a\u5ea6\u975e\u7ebf\u6027\u589e\u5f3a\u6a21\u5757\u5b9e\u73b0\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u591a\u6a21\u6001\u6570\u636e\u5171\u4eab\u7684\u9690\u79c1\u95ee\u9898\u548c\u57df\u504f\u79fb\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u79cd\u7edf\u4e00\u6a21\u578b\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faFedGIN\u6846\u67b6\uff0c\u7ed3\u5408GIN\u6a21\u5757\u5728\u672c\u5730\u8bad\u7ec3\u4e2d\u534f\u8c03\u591a\u6a21\u6001\u5f3a\u5ea6\u5206\u5e03\uff0c\u8bc4\u4f30\u4e86\u5728\u6709\u9650\u6570\u636e\u548c\u5b8c\u6574\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\uff0cFedGIN\u6bd4\u65e0GIN\u7684\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u63d0\u534712-18%\uff1b\u5728\u5b8c\u6574\u6570\u636e\u573a\u666f\u4e0b\uff0c\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u6027\u80fd\uff0cDice\u5206\u6570\u63d0\u5347\u663e\u8457\u3002", "conclusion": "FedGIN\u5728\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u987e\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05221", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05221", "abs": "https://arxiv.org/abs/2508.05221", "authors": ["Xiao Wang", "Liye Jin", "Xufeng Lou", "Shiao Wang", "Lan Chen", "Bo Jiang", "Zhipeng Zhang"], "title": "ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking", "comment": null, "summary": "Vision-language tracking has received increasing attention in recent years,\nas textual information can effectively address the inflexibility and inaccuracy\nassociated with specifying the target object to be tracked. Existing works\neither directly fuse the fixed language with vision features or simply modify\nusing attention, however, their performance is still limited. Recently, some\nresearchers have explored using text generation to adapt to the variations in\nthe target during tracking, however, these works fail to provide insights into\nthe model's reasoning process and do not fully leverage the advantages of large\nmodels, which further limits their overall performance. To address the\naforementioned issues, this paper proposes a novel reasoning-based\nvision-language tracking framework, named ReasoningTrack, based on a\npre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning)\nand reinforcement learning GRPO are used for the optimization of reasoning and\nlanguage generation. We embed the updated language descriptions and feed them\ninto a unified tracking backbone network together with vision features. Then,\nwe adopt a tracking head to predict the specific location of the target object.\nIn addition, we propose a large-scale long-term vision-language tracking\nbenchmark dataset, termed TNLLT, which contains 200 video sequences. 20\nbaseline visual trackers are re-trained and evaluated on this dataset, which\nbuilds a solid foundation for the vision-language visual tracking task.\nExtensive experiments on multiple vision-language tracking benchmark datasets\nfully validated the effectiveness of our proposed reasoning-based natural\nlanguage generation strategy. The source code of this paper will be released on\nhttps://github.com/Event-AHU/Open_VLTrack", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u6846\u67b6ReasoningTrack\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578bQwen2.5-VL\uff0c\u901a\u8fc7SFT\u548cGRPO\u4f18\u5316\u63a8\u7406\u4e0e\u8bed\u8a00\u751f\u6210\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6TNLLT\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u878d\u5408\u8bed\u8a00\u4e0e\u89c6\u89c9\u7279\u5f81\u65f6\u6027\u80fd\u6709\u9650\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u56e0\u6b64\u63d0\u51fa\u63a8\u7406\u6846\u67b6\u4ee5\u63d0\u5347\u8ddf\u8e2a\u6548\u679c\u3002", "method": "\u4f7f\u7528Qwen2.5-VL\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408SFT\u548cGRPO\u4f18\u5316\u63a8\u7406\u4e0e\u8bed\u8a00\u751f\u6210\uff0c\u5d4c\u5165\u66f4\u65b0\u7684\u8bed\u8a00\u63cf\u8ff0\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u8ddf\u8e2a\u5934\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u63a8\u7406\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b200\u4e2a\u89c6\u9891\u5e8f\u5217\u7684TNLLT\u6570\u636e\u96c6\u3002", "conclusion": "ReasoningTrack\u901a\u8fc7\u63a8\u7406\u548c\u8bed\u8a00\u751f\u6210\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u6027\u80fd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.05316", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05316", "abs": "https://arxiv.org/abs/2508.05316", "authors": ["Yue Duan", "Taicai Chen", "Lei Qi", "Yinghuan Shi"], "title": "Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning", "comment": "Accepted by ICCV 2025", "summary": "Semi-supervised continual learning (SSCL) seeks to leverage both labeled and\nunlabeled data in a sequential learning setup, aiming to reduce annotation\ncosts while managing continual data arrival. SSCL introduces complex\nchallenges, including ensuring effective unlabeled learning (UL), while\nbalancing memory stability (MS) and learning plasticity (LP). Previous SSCL\nefforts have typically focused on isolated aspects of the three, while this\nwork presents USP, a divide-and-conquer framework designed to synergistically\nenhance these three aspects: (1) Feature Space Reservation (FSR) strategy for\nLP, which constructs reserved feature locations for future classes by shaping\nold classes into an equiangular tight frame; (2) Divide-and-Conquer\nPseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels\nacross both high- and low-confidence unlabeled data; and (3)\nClass-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's\noutputs to anchor unlabeled data to stable class means for distillation to\nprevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL\nmethods, with gains up to 5.94% in the last accuracy, validating its\neffectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.", "AI": {"tldr": "USP\u6846\u67b6\u901a\u8fc7\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5b66\u4e60\u5851\u6027\u3001\u65e0\u6807\u7b7e\u5b66\u4e60\u548c\u8bb0\u5fc6\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u5e76\u5904\u7406\u6301\u7eed\u5230\u8fbe\u7684\u6570\u636e\uff0c\u540c\u65f6\u89e3\u51b3\u65e0\u6807\u7b7e\u5b66\u4e60\u3001\u8bb0\u5fc6\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u5851\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "USP\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7b56\u7565\uff1a\u7279\u5f81\u7a7a\u95f4\u4fdd\u7559\uff08FSR\uff09\u7528\u4e8e\u5b66\u4e60\u5851\u6027\uff0c\u5206\u800c\u6cbb\u4e4b\u4f2a\u6807\u7b7e\uff08DCP\uff09\u7528\u4e8e\u65e0\u6807\u7b7e\u5b66\u4e60\uff0c\u7c7b\u5747\u503c\u951a\u5b9a\u65e0\u6807\u7b7e\u84b8\u998f\uff08CUD\uff09\u7528\u4e8e\u8bb0\u5fc6\u7a33\u5b9a\u6027\u3002", "result": "USP\u5728\u6700\u540e\u51c6\u786e\u7387\u4e0a\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u5347\u4e865.94%\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "USP\u6846\u67b6\u901a\u8fc7\u534f\u540c\u589e\u5f3a\u4e09\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2508.05435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05435", "abs": "https://arxiv.org/abs/2508.05435", "authors": ["Vincent Jeanselme", "Brian Tom", "Jessica Barrett"], "title": "Competing Risks: Impact on Risk Estimation and Algorithmic Fairness", "comment": null, "summary": "Accurate time-to-event prediction is integral to decision-making, informing\nmedical guidelines, hiring decisions, and resource allocation. Survival\nanalysis, the quantitative framework used to model time-to-event data, accounts\nfor patients who do not experience the event of interest during the study\nperiod, known as censored patients. However, many patients experience events\nthat prevent the observation of the outcome of interest. These competing risks\nare often treated as censoring, a practice frequently overlooked due to a\nlimited understanding of its consequences. Our work theoretically demonstrates\nwhy treating competing risks as censoring introduces substantial bias in\nsurvival estimates, leading to systematic overestimation of risk and,\ncritically, amplifying disparities. First, we formalize the problem of\nmisclassifying competing risks as censoring and quantify the resulting error in\nsurvival estimates. Specifically, we develop a framework to estimate this error\nand demonstrate the associated implications for predictive performance and\nalgorithmic fairness. Furthermore, we examine how differing risk profiles\nacross demographic groups lead to group-specific errors, potentially\nexacerbating existing disparities. Our findings, supported by an empirical\nanalysis of cardiovascular management, demonstrate that ignoring competing\nrisks disproportionately impacts the individuals most at risk of these events,\npotentially accentuating inequity. By quantifying the error and highlighting\nthe fairness implications of the common practice of considering competing risks\nas censoring, our work provides a critical insight into the development of\nsurvival models: practitioners must account for competing risks to improve\naccuracy, reduce disparities in risk assessment, and better inform downstream\ndecisions.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5c06\u7ade\u4e89\u98ce\u9669\u89c6\u4e3a\u622a\u5c3e\u6570\u636e\u4f1a\u5f15\u5165\u663e\u8457\u504f\u5dee\uff0c\u5bfc\u81f4\u98ce\u9669\u9ad8\u4f30\u5e76\u52a0\u5267\u4e0d\u516c\u5e73\u6027\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u8bef\u5dee\u7684\u6846\u67b6\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e8b\u4ef6\u53d1\u751f\u65f6\u95f4\u5bf9\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u5b58\u5206\u6790\u5e38\u5ffd\u7565\u7ade\u4e89\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u504f\u5dee\u548c\u4e0d\u516c\u5e73\u3002", "method": "\u5f62\u5f0f\u5316\u7ade\u4e89\u98ce\u9669\u8bef\u5206\u7c7b\u95ee\u9898\uff0c\u5f00\u53d1\u91cf\u5316\u8bef\u5dee\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5fc3\u8840\u7ba1\u7ba1\u7406\u6570\u636e\u9a8c\u8bc1\u3002", "result": "\u5ffd\u7565\u7ade\u4e89\u98ce\u9669\u4f1a\u9ad8\u4f30\u98ce\u9669\u5e76\u52a0\u5267\u7fa4\u4f53\u95f4\u5dee\u5f02\uff0c\u5bf9\u9ad8\u98ce\u9669\u7fa4\u4f53\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u751f\u5b58\u6a21\u578b\u9700\u8003\u8651\u7ade\u4e89\u98ce\u9669\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3001\u51cf\u5c11\u4e0d\u516c\u5e73\u6027\uff0c\u5e76\u4e3a\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u652f\u6301\u3002"}}
{"id": "2508.05507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05507", "abs": "https://arxiv.org/abs/2508.05507", "authors": ["Lin Zhu", "Ruonan Liu", "Xiao Wang", "Lizhi Wang", "Hua Huang"], "title": "Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events", "comment": null, "summary": "Event camera, a novel neuromorphic vision sensor, records data with high\ntemporal resolution and wide dynamic range, offering new possibilities for\naccurate visual representation in challenging scenarios. However, event data is\ninherently sparse and noisy, mainly reflecting brightness changes, which\ncomplicates effective feature extraction. To address this, we propose a\nself-supervised pre-training framework to fully reveal latent information in\nevent data, including edge information and texture cues. Our framework consists\nof three stages: Difference-guided Masked Modeling, inspired by the event\nphysical sampling process, reconstructs temporal intensity difference maps to\nextract enhanced information from raw event data. Backbone-fixed Feature\nTransition contrasts event and image features without updating the backbone to\npreserve representations learned from masked modeling and stabilizing their\neffect on contrastive learning. Focus-aimed Contrastive Learning updates the\nentire model to improve semantic discrimination by focusing on high-value\nregions. Extensive experiments show our framework is robust and consistently\noutperforms state-of-the-art methods on various downstream tasks, including\nobject recognition, semantic segmentation, and optical flow estimation. The\ncode and dataset are available at https://github.com/BIT-Vision/EventPretrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u4e14\u5608\u6742\u7684\u4e8b\u4ef6\u6570\u636e\u4e2d\u63d0\u53d6\u6f5c\u5728\u4fe1\u606f\uff0c\u5305\u62ec\u8fb9\u7f18\u4fe1\u606f\u548c\u7eb9\u7406\u7ebf\u7d22\u3002", "motivation": "\u4e8b\u4ef6\u6570\u636e\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u5bbd\u52a8\u6001\u8303\u56f4\uff0c\u4f46\u5176\u7a00\u758f\u6027\u548c\u566a\u58f0\u7279\u6027\u4f7f\u5f97\u7279\u5f81\u63d0\u53d6\u53d8\u5f97\u590d\u6742\u3002", "method": "\u6846\u67b6\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5dee\u5f02\u5f15\u5bfc\u63a9\u7801\u5efa\u6a21\u3001\u4e3b\u5e72\u56fa\u5b9a\u7684\u7279\u5f81\u8f6c\u6362\u548c\u805a\u7126\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u76ee\u6807\u8bc6\u522b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5149\u6d41\u4f30\u8ba1\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u53d6\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u4fe1\u606f\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
