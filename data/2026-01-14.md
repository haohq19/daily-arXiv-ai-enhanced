<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration](https://arxiv.org/abs/2601.07845)
*Shree Charran R,Rahul Kumar Dubey*

Main category: cs.CV

TL;DR: 提出基于边缘AI的实时路边感知节点，用于多类交通违规检测和安全事件分发，在低功耗设备上实现高精度违规识别和V2X通信。


<details>
  <summary>Details</summary>
Motivation: 新兴经济体如印度快速机动化导致执法严重不足（2023年1100万违规记录 vs 每4000辆车仅1名警察），传统监控和人工罚单无法应对此规模，需要自主、协作、节能的边缘AI感知基础设施。

Method: 集成YOLOv8 Nano进行高精度多目标检测，DeepSORT实现时序一致车辆跟踪，规则引导OCR后处理引擎识别退化或多语言车牌（符合MoRTH AIS 159和ISO 7591标准）。部署在NVIDIA Jetson Nano上，通过TensorRT FP16量化优化。

Result: 系统在9.6W功耗下维持28-30 FPS推理，达到97.7%违规检测准确率和84.9% OCR精度，覆盖5类违规（闯红灯、斑马线违规、逆行、非法掉头、超速）。相比YOLOv4 Tiny等模型，获得10.7% mAP提升和1.4倍能效比改进。

Conclusion: 路边边缘AI分析可以增强协作感知和主动道路安全管理，通过V2X协议向联网车辆和智能交通系统后端发布标准化安全事件（CAM和DENM类型），在IEEE智能车辆生态系统中具有应用价值。

Abstract: Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.

</details>


### [2] [Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2](https://arxiv.org/abs/2601.08408)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Jian Liu,Yuyang Wang,Abel Cherouat,Tian Wang*

Main category: cs.CV

TL;DR: 提出基于BLIP-2的轻量级多模态任务平台，集成YOLO-World和YOLOv8-Seg模型，解决无人机边缘设备计算资源有限与视觉语言模型高计算成本之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂场景中需要实时视觉理解和交互，但大型视觉语言模型的高计算成本与无人机边缘设备的有限计算资源之间存在矛盾。

Method: 1) 将BLIP-2与YOLO模型深度集成，利用YOLO的精确感知结果；2) 设计基于K-Means聚类的内容感知关键帧采样机制；3) 实施统一的多任务适应提示优化方案，将YOLO的结构化事件日志作为上下文信息注入。

Result: 平台扩展了BLIP-2的多任务能力，无需在无人机数据上进行任务特定微调，即可有效处理视频级交互任务，生成准确且上下文相关的输出。

Conclusion: 提出的轻量级多模态平台成功解决了无人机边缘设备计算资源限制问题，通过集成YOLO模型和优化机制，实现了高效的实时视觉理解和交互能力。

Abstract: The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.

</details>


### [3] [Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence](https://arxiv.org/abs/2601.08241)
*Michele Fiori,Gabriele Civitarese,Marco Colussi,Claudio Bettini*

Main category: cs.CV

TL;DR: 提出基于事件分割和置信度估计的零样本ADL识别方法，超越时间分割和传统监督方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的零样本ADL识别方法依赖时间分割，与LLM的上下文推理能力不匹配，且缺乏预测置信度估计方法

Method: 采用事件分割替代时间分割，并提出新的预测置信度估计方法

Result: 事件分割在复杂真实数据集上持续优于时间分割方法，甚至超越监督数据驱动方法；置信度度量能有效区分正确与错误预测

Conclusion: 事件分割与置信度估计相结合显著提升零样本ADL识别性能，即使使用相对较小的LLM也能取得优异效果

Abstract: Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.

</details>


### [4] [IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks](https://arxiv.org/abs/2601.08332)
*Ahmed A. Hashim,Ali Al-Shuwaili,Asraa Saeed,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: 提出IGAN模型，结合Inception卷积和空洞卷积，在保持训练稳定性的同时生成高质量图像，在CUB-200和ImageNet数据集上FID提升28-33%


<details>
  <summary>Details</summary>
Motivation: 现有GAN模型（如DCGAN、BigGAN、StyleGAN）在高质量图像生成和训练稳定性之间存在平衡问题，容易出现模式崩溃和不稳定梯度问题

Method: 提出Inception GAN (IGAN)模型，结合Inception启发的深度卷积和空洞卷积，在生成器和判别器中使用dropout和谱归一化技术

Result: 在CUB-200数据集上FID为13.12，ImageNet上为15.08，相比SOTA提升28-33%；Inception Score分别为9.27和68.25，显示图像多样性和质量提升

Conclusion: IGAN模型能够平衡训练稳定性和图像生成质量，为高保真图像合成提供了一个可扩展且计算高效的框架

Abstract: Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.

</details>


### [5] [Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation](https://arxiv.org/abs/2601.08375)
*Yuan Gao,Di Cao,Xiaohuan Xi,Sheng Nie,Shaobo Xia,Cheng Wang*

Main category: cs.CV

TL;DR: 提出LoGo框架，用于地理空间点云的无源无监督域适应，通过局部-全局双共识机制解决长尾分布和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 地理空间点云语义分割面临跨区域域偏移问题，现有域适应方法需要源域数据，但由于隐私、政策和传输限制难以获取，因此需要研究无源无监督域适应方法。

Method: 提出LoGo框架：1) 局部层面：类平衡原型估计模块，采用类内独立锚点挖掘策略而非全局阈值过滤；2) 全局层面：基于最优传输的全局分布对齐模块，将伪标签分配建模为全局优化问题；3) 双一致性伪标签过滤机制，仅保留局部多增强集成预测与全局最优传输分配一致的高置信度伪标签用于自训练。

Result: 未在摘要中提供具体实验结果，但方法设计旨在解决长尾分布导致的特征崩溃问题，并纠正头类别的过度主导，防止模型预测偏向多数类别。

Conclusion: LoGo是针对地理空间点云设计的创新SFUDA框架，通过局部-全局双共识机制有效处理域偏移和类别不平衡问题，为无源域适应提供了新思路。

Abstract: Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.

</details>


### [6] [An IoT-Enabled Smart Aquarium System for Real-Time Water Quality Monitoring and Automated Feeding](https://arxiv.org/abs/2601.08484)
*MD Fatin Ishraque Ayon,Sabrin Nahar,Ataur Rahman,Md. Taslim Arif,Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 基于ESP32微控制器的物联网智能鱼缸系统，集成多种传感器和执行器，实现水质实时监测与自动化控制，通过云端平台管理，显著减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统人工监测鱼缸水质效率低、劳动强度大且易出错，导致水生环境不理想，需要更智能、自动化的解决方案来维持最佳水质。

Method: 采用ESP32微控制器集成pH、TDS、温度、浊度传感器以及伺服喂食器、水泵等执行器，结合边缘处理能力和Blynk物联网平台云端连接，配置智能警报机制和冷却期防止通知疲劳。

Result: 在10升鱼缸环境中测试，系统平均传感器准确率达96%，异常检测响应时间1.2秒，自动化喂食和水循环模块运行可靠性达97%，显著减少人工干预。

Conclusion: 低成本物联网解决方案能够革新鱼缸维护，使水生生态系统管理对住宅和商业应用都更加便捷、可靠和高效。

Abstract: Maintaining optimal water quality in aquariums is critical for aquatic health but remains challenging due to the need for continuous monitoring of multiple parameters. Traditional manual methods are inefficient, labor-intensive, and prone to human error, often leading to suboptimal aquatic conditions. This paper presents an IoT-based smart aquarium system that addresses these limitations by integrating an ESP32 microcontroller with multiple sensors (pH, TDS, temperature, turbidity) and actuators (servo feeder, water pump) for comprehensive real-time water quality monitoring and automated control. The system architecture incorporates edge processing capabilities, cloud connectivity via Blynk IoT platform, and an intelligent alert mechanism with configurable cooldown periods to prevent notification fatigue. Experimental evaluation in a 10-liter aquarium environment demonstrated the system's effectiveness, achieving 96\% average sensor accuracy and 1.2-second response time for anomaly detection. The automated feeding and water circulation modules maintained 97\% operational reliability throughout extended testing, significantly reducing manual intervention while ensuring stable aquatic conditions. This research demonstrates that cost-effective IoT solutions can revolutionize aquarium maintenance, making aquatic ecosystem management more accessible, reliable, and efficient for both residential and commercial applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance](https://arxiv.org/abs/2601.08418)
*Jihang Li,Qing Liu,Zulong Chen,Jing Wang,Wei Wang,Chuanfei Xu,Zeyi Wen*

Main category: cs.LG

TL;DR: Taxon是一个用于分层税码预测的语义对齐专家引导框架，通过特征门控专家混合架构和LLM蒸馏的语义一致性模型，结合多源训练数据，在阿里巴巴税务系统中实现高精度部署。


<details>
  <summary>Details</summary>
Motivation: 税码预测是电子商务平台自动开票和合规管理的关键任务，但现有研究不足。产品需要准确映射到国家标准定义的多层分类层次中，错误会导致财务不一致和监管风险。

Method: 1) 特征门控专家混合架构：自适应地将多模态特征路由到不同分类层次；2) 语义一致性模型：从大型语言模型蒸馏而来，作为领域专家验证产品标题与官方税定义的对齐；3) 多源训练管道：结合策划的税务数据库、发票验证日志和商家注册数据，提供结构和语义监督。

Result: 在专有TaxCode数据集和公共基准测试中达到最先进性能，超越强基线。完整分层路径重建程序显著提高结构一致性，获得最高总体F1分数。已在阿里巴巴税务系统部署，日均处理超50万税码查询，业务高峰期达500万以上请求，准确性、可解释性和鲁棒性均提升。

Conclusion: Taxon框架成功解决了税码预测中的语义对齐和层次结构问题，通过专家引导的多模态特征处理和LLM知识蒸馏，实现了高精度、可解释的税码预测系统，已在阿里巴巴大规模生产环境中验证有效。

Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.

</details>


### [8] [M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting](https://arxiv.org/abs/2601.08631)
*Yaohui Huang,Runmin Zou,Yun Wang,Laeeq Aslam,Ruipeng Dong*

Main category: cs.LG

TL;DR: M²FMoE：一种极端自适应预测模型，通过多分辨率多视图频率建模同时学习常规和极端模式，在极端事件预测中优于现有方法且无需极端事件标签。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在处理极端事件时性能显著下降，因为极端事件具有高方差、不规则动态和稀疏但高影响特性。虽然有些方法加入辅助信号，但仍无法捕捉极端事件的复杂时间动态。

Method: 提出M²FMoE模型，包含三个模块：1) 多视图频率混合专家模块，在傅里叶和小波域为不同频带分配专家；2) 多分辨率自适应融合模块，从粗到细层次聚合频率特征；3) 时间门控集成模块，动态平衡长期趋势和短期频率感知特征。

Result: 在具有极端模式的真实世界水文数据集上的实验表明，M²FMoE优于最先进的基线方法，且不需要极端事件标签。

Conclusion: M²FMoE通过多分辨率多视图频率建模有效捕捉极端事件的复杂动态，在极端事件预测中表现出色，为解决高方差时间序列预测问题提供了有效方案。

Abstract: Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling](https://arxiv.org/abs/2601.07964)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 本文探讨了可执行本体论在游戏开发中的应用，通过boldsea框架实现从算法行为编程到语义世界建模的范式转变，使智能体行为从声明式领域规则自然涌现而非显式编码。


<details>
  <summary>Details</summary>
Motivation: 解决游戏AI架构中的语义-过程鸿沟问题，传统方法（如行为树和面向目标行动规划）主要关注智能体应该做什么，而可执行本体论则建模动作何时变得可能，实现更自然的智能体行为涌现。

Method: 使用boldsea框架实现可执行本体论，通过在生存游戏场景（Winter Feast）中应用，展示如何通过数据流条件而非显式抢占逻辑实现基于优先级的任务中断，并与行为树和面向目标行动规划进行对比分析。

Result: 可执行本体论能够有效解决语义-过程鸿沟，实现更自然的智能体行为涌现，具有时间事件图固有的调试优势，并展示了LLM驱动运行时模型生成的潜力。

Conclusion: 可执行本体论代表了游戏AI开发的范式转变，从算法行为编程转向语义世界建模，为解决游戏AI架构中的根本问题提供了新途径，并具有与LLM等新技术集成的潜力。

Abstract: This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.

</details>


### [10] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 提出基于模型的分层多智能体强化学习框架，用于多核嵌入式系统的热管理和能耗优化调度，结合LLM语义特征提取实现零样本部署，相比传统方法显著提升能效和调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS和任务分配方法存在局限性：基于利用率的启发式方法忽略停顿时间，基于离线分析的方法无法适应运行时变化。需要一种能够动态适应、无需特定工作负载分析的高效调度方案。

Method: 1) 分层多智能体强化学习框架分解指数级动作空间；2) 结合回归技术的环境模型预测热力学和性能状态；3) LLM语义特征提取从OpenMP代码中提取13个代码级特征；4) Dyna-Q启发框架整合直接强化学习和基于模型的规划。

Result: 在多个硬件平台(BOTS、PolybenchC基准测试)上验证：相比Linux ondemand governor，能效提升7.09倍，makespan提升4.0倍。首次决策延迟比基于表格的分析快8300倍，收敛速度比无模型方法快20倍。

Conclusion: 该框架实现了高效的热管理和能耗优化调度，支持零样本部署到新工作负载，解决了现有方法的局限性，为动态嵌入式系统的实际部署提供了可行方案。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [11] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: SANC(E3)是一个理论框架，提出表征单元不应预先给定，而应通过竞争选择、重建和压缩在有限激活容量下作为稳定结果涌现，通过最小化能量函数E3来统一感知、想象、预测、规划和行动。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统预设了固定的原始单元（如token、像素等），绕过了表征单元如何涌现和稳定的核心问题。通用智能需要将经验重组为能在有限资源下实现预测和行动的内部结构。

Method: 提出SANC(E3)公理化框架，包含五个核心公理：有限容量、共现关联、相似性竞争、置信度稳定、重建-压缩-更新权衡。采用伪内存映射I/O机制，使内部回放的格式塔与外部感官输入通过相同公理路径处理。

Result: 从公理推导出12个命题，表明类别形成、层次组织、无监督学习和高级认知活动都可以理解为E3最小化下的格式塔完成过程。实现了感知、想象、预测、规划和行动在单一表征和能量过程中的统一。

Conclusion: SANC(E3)为理解智能如何从有限资源约束下涌现稳定表征单元提供了理论基础，将认知功能统一为格式塔完成的能量最小化过程，为构建更接近人类智能的AI系统提供了新方向。

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training](https://arxiv.org/abs/2601.08141)
*Muhammad Taimoor Hassan,Jawad Ahmed,Muhammad Awais*

Main category: cs.CL

TL;DR: Qalb是一个针对乌尔都语的大语言模型，通过两阶段方法开发：持续预训练和监督微调，显著提升了乌尔都语NLP任务的性能。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为拥有2.3亿使用者的语言，在现代NLP系统中代表性严重不足。现有的多语言模型在乌尔都语特定任务上表现不佳，难以处理该语言的复杂形态、从右到左的Nastaliq文字和丰富的文学传统。

Method: 采用两阶段方法：1）从LLaMA 3.1 8B开始，在19.7亿token的数据集上进行持续预训练，包含18.4亿乌尔都语token和1.4亿英语维基百科token；2）在Alif Urdu-instruct数据集上进行监督微调。

Result: Qalb在乌尔都语特定基准测试中取得显著改进，加权平均得分90.34，比之前的SOTA模型Alif-1.0-Instruct（87.1）高出3.24分，比基础LLaMA-3.1 8B-Instruct模型高出44.64分，在七个多样化任务中达到最先进性能。

Conclusion: 研究表明，在多样化高质量语言数据上进行持续预训练，结合有针对性的指令微调，能够有效将基础模型适应到低资源语言，为乌尔都语NLP提供了强大的解决方案。

Abstract: Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.

</details>


### [13] [It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models](https://arxiv.org/abs/2601.08500)
*Cristian Santini,Marieke Van Erp,Mehwish Alam*

Main category: cs.CL

TL;DR: MHEL-LLaMo提出了一种无监督的历史文本实体链接方法，结合小型语言模型和大型语言模型，通过置信度分数区分简单和困难样本，仅在困难样本上使用LLM，在六个欧洲语言的历史文本基准上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 历史文本的实体链接面临语言变异、噪声输入和语义演变等挑战。现有方法要么需要大量训练数据，要么依赖领域特定规则，限制了可扩展性。需要一种无需微调、可扩展的解决方案来处理低资源历史实体链接任务。

Method: 提出MHEL-LLaMo无监督集成方法：1) 使用多语言双编码器BELA进行候选实体检索；2) 通过指令调优的LLM进行NIL预测和候选选择（提示链）；3) 利用SLM置信度分数区分简单和困难样本，仅在困难样本上应用LLM，降低计算成本并避免简单样本上的幻觉。

Result: 在六个欧洲语言（英语、芬兰语、法语、德语、意大利语、瑞典语）的四个历史文本基准上评估，MHEL-LLaMo无需微调即超越了现有最先进模型，为低资源历史实体链接提供了可扩展解决方案。

Conclusion: MHEL-LLaMo通过结合SLM和LLM的优势，提供了一种高效、可扩展的历史文本实体链接方法，在降低计算成本的同时提升了性能，特别适用于低资源场景。代码已在GitHub开源。

Abstract: Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.

</details>


### [14] [STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays](https://arxiv.org/abs/2601.08510)
*Qiuyu Tian,Yiding Li,Fengyi Chen,Zequn Liu,Youyong Kong,Fan Guo,Yuyao Li,Jinjing Shen,Zhijing Xie,Yiyun Luo,Xin Zhang*

Main category: cs.CL

TL;DR: STAGE是一个统一的电影剧本叙事理解基准测试，包含知识图谱构建、场景事件摘要、长上下文问答和角色扮演四个任务，基于150部中英文电影的剧本和标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注问答或对话生成等单一子任务，缺乏评估模型能否构建连贯故事世界并在多种推理和生成任务中保持一致性的能力。电影剧本作为长篇叙事文本，包含复杂的人物关系、时序事件和对话交互，需要更全面的评估框架。

Method: STAGE定义了四个基于共享叙事世界表示的任务：1) 知识图谱构建；2) 场景级事件摘要；3) 长上下文剧本问答；4) 剧本内角色扮演。基准测试提供了150部中英文电影的清洗后剧本、精心构建的知识图谱以及事件和人物中心的标注。

Result: STAGE基准测试为评估模型在构建世界表示、抽象和验证叙事事件、长叙事推理以及生成角色一致响应方面的能力提供了全面的评估框架和数据集。

Conclusion: STAGE填补了现有基准测试的空白，通过统一的叙事世界表示和多任务评估，能够更全面地评估模型对长篇叙事文本的理解和生成能力，特别是在保持故事世界一致性和角色一致性方面。

Abstract: Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [15] [FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models](https://arxiv.org/abs/2601.08246)
*Yifan Han,Pengfei Yi,Junyan Li,Hanqing Wang,Gaojing Zhang,Qi Peng Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出基于预训练扩散模型的数据高效框架，从人类视频提取语义抓取先验，实现跨多种灵巧手的抓取合成，无需为每种手收集大量抓取数据。


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取合成面临高维度和运动多样性挑战，现有方法依赖大量硬件特定的抓取数据集（仿真或真实采集），成本高且难以扩展到新设计的灵巧手。

Method: 1) 从原始人类视频演示中提取时间对齐的细粒度抓取功能语义；2) 与深度图像的3D场景几何融合，推断语义基础的接触目标；3) 通过运动学感知的重定向模块将功能表示映射到不同灵巧手，无需为每种手重新训练。

Result: 系统能生成稳定、功能适当的多接触抓取，在常见物体和工具上可靠成功，对未见过的物体实例、姿态变化和多种手具现化表现出强泛化能力。

Conclusion: 通过结合人类演示和预训练生成模型的语义先验，为可扩展、硬件无关的灵巧操作开辟了新路径，证明单一深度模态结合基础模型语义足以实现高性能抓取合成。

Abstract: Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.

</details>


### [16] [VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory](https://arxiv.org/abs/2601.08665)
*Shaoan Wang,Yuanfei Luo,Xingyu Chen,Aocheng Luo,Dongyue Li,Chang Liu,Sheng Chen,Yangang Zhang,Junzhi Yu*

Main category: cs.RO

TL;DR: VLingNav：基于语言认知的VLA导航模型，通过自适应思维链机制和视觉辅助语言记忆模块，在多种导航基准上实现SOTA性能，并能零样本迁移到真实机器人平台。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在具身导航中主要依赖从观察到动作的被动映射，缺乏显式推理能力和持久记忆，难以处理复杂、长时程的导航任务。

Method: 1）受人类认知双过程理论启发，引入自适应思维链机制，动态触发显式推理；2）开发视觉辅助语言记忆模块，构建跨模态语义记忆；3）构建Nav-AdaCoT-2.9M数据集，包含自适应CoT标注；4）加入在线专家引导强化学习阶段。

Result: VLingNav在多种具身导航基准上达到最先进性能，能够零样本迁移到真实机器人平台，执行各种导航任务，展现出强大的跨领域和跨任务泛化能力。

Conclusion: VLingNav通过语言驱动的认知方法，结合自适应推理和持久记忆，显著提升了VLA模型在复杂导航任务中的性能，实现了从仿真到真实世界的有效迁移。

Abstract: VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.

</details>
