<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 该论文提出了一种针对非马尔可夫多轮对话图像生成的方法，通过历史条件训练框架、回滚式编辑和基于名称的个性化策略，显著提升了多轮一致性和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在多轮对话图像生成中存在局限性，大多采用马尔可夫假设（仅依赖最近图像），无法处理用户引用历史状态、撤销更改或跨多轮引用实体等复杂场景。

Method: 1) 构建非马尔可夫多轮数据：包括回滚式编辑（强制检索早期视觉状态）和基于名称的多轮个性化（跨轮次绑定名称与外观）；2) 历史条件训练与推理框架：采用令牌级缓存防止多轮身份漂移；3) 改进高保真图像重建和可编辑个性化：包括基于重建的DiT解令牌器和多阶段微调课程。

Result: 显式训练非马尔可夫交互显著提升了多轮一致性和指令遵循能力，同时保持了强大的单轮编辑和个性化性能。

Conclusion: 针对非马尔可夫设置进行专门训练是提升多轮对话图像生成质量的关键，该方法在保持单轮能力的同时，有效解决了历史引用和跨轮一致性等挑战。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

</details>


### [2] [LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models](https://arxiv.org/abs/2601.21220)
*Alvi Md Ishmam,Najibul Haque Sarker,Zaber Ibn Abdul Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: LAMP是一种针对多图像MLLMs的黑盒攻击方法，通过注意力约束和跨图像传染约束实现高效对抗攻击


<details>
  <summary>Details</summary>
Motivation: 多图像MLLMs的脆弱性尚未被探索，现有攻击方法主要针对单图像设置且假设白盒威胁模型，这在现实场景中不实用

Method: 提出LAMP方法：1) 基于注意力的约束防止模型有效聚合跨图像信息；2) 跨图像传染约束使扰动令牌影响干净令牌；3) 索引注意力抑制损失实现位置不变攻击

Result: LAMP在多个视觉语言任务和模型上超越了现有最先进基线，实现了最高的攻击成功率

Conclusion: LAMP为多图像MLLMs的黑盒攻击提供了有效解决方案，揭示了这类模型的安全脆弱性

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance across vision-language tasks. Recent advancements allow these models to process multiple images as inputs. However, the vulnerabilities of multi-image MLLMs remain unexplored. Existing adversarial attacks focus on single-image settings and often assume a white-box threat model, which is impractical in many real-world scenarios. This paper introduces LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs. LAMP applies an attention-based constraint that prevents the model from effectively aggregating information across images. LAMP also introduces a novel cross-image contagious constraint that forces perturbed tokens to influence clean tokens, spreading adversarial effects without requiring all inputs to be modified. Additionally, an index-attention suppression loss enables a robust position-invariant attack. Experimental results show that LAMP outperforms SOTA baselines and achieves the highest attack success rates across multiple vision-language tasks and models.

</details>


### [3] [Hypersolid: Emergent Vision Representations via Short-Range Repulsion](https://arxiv.org/abs/2601.21255)
*Esteban Rodríguez-Betancourt,Edgar Casasola-Murillo*

Main category: cs.CV

TL;DR: Hypersolid是一种自监督学习方法，将表示学习重新解释为离散打包问题，使用短程硬球排斥防止局部碰撞，从而避免表示崩溃


<details>
  <summary>Details</summary>
Motivation: 自监督学习中防止表示崩溃是一个持续挑战，现有方法通常依赖全局正则化（如最大化距离、去相关维度或强制特定分布），作者希望找到更有效的替代方案

Method: 将表示学习重新解释为离散打包问题，将保持信息简化为保持单射性，使用Hypersolid方法，通过短程硬球排斥防止局部碰撞

Result: 该方法产生高分离几何机制，能够保持增强多样性，在细粒度和低分辨率分类任务上表现优异

Conclusion: 通过将表示学习视为离散打包问题并使用局部硬球排斥，可以更有效地防止表示崩溃，在特定任务上优于传统的全局正则化方法

Abstract: A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.

</details>


### [4] [Semantic-Guided Dynamic Sparsification for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2601.21345)
*Ruiqi Liu,Boyu Diao,Zijia An,Runjie Shao,Zhulin An,Fei Wang,Yongjun Xu*

Main category: cs.CV

TL;DR: SGDS通过语义引导的动态稀疏化，在激活空间中塑造类特定稀疏子空间，解决CIL中正交参数约束损害可塑性的问题，实现知识迁移与干扰预防的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统CIL方法冻结预训练模型并使用轻量适配器，通常强制参数正交以防止任务间干扰，但作者认为这种参数约束方法会损害模型的可塑性。

Method: 提出语义引导的动态稀疏化(SGDS)，通过定向稀疏化控制激活空间子空间的方向和秩，促进相似类共享紧凑激活子空间以实现知识迁移，同时为不相似类分配不重叠的激活子空间以防止干扰。

Result: 在多个基准数据集上的广泛实验表明，SGDS实现了最先进的性能。

Conclusion: SGDS通过在激活空间中塑造类特定稀疏子空间，有效缓解干扰而不对参数空间施加刚性约束，为CIL提供了更优的解决方案。

Abstract: Class-Incremental Learning (CIL) requires a model to continually learn new classes without forgetting old ones. A common and efficient solution freezes a pre-trained model and employs lightweight adapters, whose parameters are often forced to be orthogonal to prevent inter-task interference. However, we argue that this parameter-constraining method is detrimental to plasticity. To this end, we propose Semantic-Guided Dynamic Sparsification (SGDS), a novel method that proactively guides the activation space by governing the orientation and rank of its subspaces through targeted sparsification. Specifically, SGDS promotes knowledge transfer by encouraging similar classes to share a compact activation subspace, while simultaneously preventing interference by assigning non-overlapping activation subspaces to dissimilar classes. By sculpting class-specific sparse subspaces in the activation space, SGDS effectively mitigates interference without imposing rigid constraints on the parameter space. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of SGDS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: TwinWeaver框架将患者纵向历史序列化为文本，使大语言模型能统一进行事件预测和预后，在癌症患者数据上显著提升预测准确性和风险分层能力。


<details>
  <summary>Details</summary>
Motivation: 精准肿瘤学需要预测临床事件和病程轨迹，但建模稀疏、多模态的临床时间序列仍是一个关键挑战。现有方法难以有效处理这种复杂数据。

Method: 提出TwinWeaver开源框架，将患者纵向历史序列化为文本格式，使大语言模型能统一进行事件预测和预后。在此基础上构建Genie Digital Twin (GDT)模型，使用93,054名20种癌症类型患者的数据。

Result: GDT显著降低预测误差，中位MASE为0.87（基线0.97，p<0.001）。在生存、进展和治疗切换任务中，平均C-index达0.703（最佳基线0.662）。在分布外临床试验数据上，零样本匹配基线，微调后表现更优，中位MASE 0.75-0.88，事件预测平均C-index 0.672 vs 0.648。

Conclusion: TwinWeaver为纵向临床建模提供了可扩展且透明的基础，其可解释的临床推理扩展增强了模型实用性，为精准肿瘤学提供了有效的预测工具。

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [6] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 开发首个全球儿童发展预训练编码器，使用UNICEF 35.7万儿童数据，在少量样本下显著提升发展迟缓监测性能


<details>
  <summary>Details</summary>
Motivation: 每年大量儿童经历可预防的发展迟缓，但机器学习在新国家部署面临数据瓶颈：可靠模型需要数千样本，而新项目通常只有不到100个样本

Method: 引入首个全球儿童发展预训练编码器，使用UNICEF调查数据在44个国家357,709名儿童上进行训练，采用迁移学习框架

Result: 仅50个训练样本时平均AUC达0.65，优于冷启动梯度提升的0.61；500样本时AUC达0.73；零样本部署到未见国家AUC最高达0.84

Conclusion: 预训练编码器可以改变资源受限环境下SDG 4.2.1监测的机器学习可行性，预训练多样性使少样本泛化成为可能

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [7] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: SIGMA-PPG是一个基于统计先验的生成式PPG基础模型，通过对抗掩码机制和语义一致性约束解决传统方法的局限性，在12个下游任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前PPG信号基础模型面临信号内在冗余和噪声的挑战。标准掩码建模容易产生平凡解，而对比学习方法缺乏形态学精度，需要新的方法来克服这些限制。

Method: 提出统计先验引导的生成式掩码架构(SIGMA-PPG)，包含先验引导的对抗掩码机制（使用强化学习驱动的教师网络利用统计先验创建具有挑战性的学习路径）和通过向量量化实现的语义一致性约束。

Result: 在超过120,000小时数据上预训练后，SIGMA-PPG在12个多样化下游任务中相比5个最先进的基线方法取得了优越的平均性能。

Conclusion: SIGMA-PPG通过创新的对抗掩码机制和语义一致性约束，有效解决了PPG信号建模中的噪声和冗余问题，为PPG基础模型提供了新的解决方案。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [8] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出动态截断半径方法，替代传统固定截断半径，显著降低机器学习原子间势能（MLIPs）的内存消耗和推理时间，同时保持模拟稳定性。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势能（MLIPs）在分子动力学模拟中应用广泛，但面临两个主要瓶颈：推理时间和内存消耗。传统方法使用固定截断半径限制了MLIPs达到实际模拟规模的能力。

Method: 首次引入动态截断半径公式，通过针对每个原子设定特定的邻居数量来诱导底层原子图的稀疏性，从而减少内存消耗和推理时间。该方法在MACE、Nequip、Orbv3和TensorNet四种先进MLIPs上实现。

Result: 动态截断方法使内存消耗减少2.26倍，推理时间加快2.04倍（具体数值因模型和原子系统而异）。在材料和分子数据集上，与固定截断模型相比，精度下降最小。

Conclusion: 动态截断半径方法有效解决了MLIPs的推理时间和内存消耗瓶颈，在保持模拟稳定性和精度的同时显著提升性能，所有实现和训练代码将开源。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [9] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 提出一种基于协方差矩阵幂的机器学习方法，用于早期检测复杂系统中的涌现现象（如癫痫发作、客户流失），通过揭示潜在因果结构实现预测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中涌现现象（癫痫发作、客户流失、疫情爆发）往往源于隐藏的因果交互作用，但数据生成过程未知且部分可观测，难以早期检测。

Method: 从单参数估计器族（经验协方差或精度矩阵的幂）中学习最优特征表示，通过监督学习模块对学习到的表示进行分类，以揭示驱动关键事件的底层结构。

Result: 在癫痫发作检测和客户流失预测上获得竞争性结果，证明结构一致性，且最优协方差幂显示出良好的可识别性并捕获结构特征。

Conclusion: 该方法不仅能预测涌现现象，还能通过可解释的统计结构实现预测性能与可解释性的统一，为复杂系统早期检测提供新途径。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [10] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: PG-TMT：一种用于旋转机械故障预警的物理引导微型Mamba Transformer，通过三分支编码器、物理对齐机制和极值理论校准，实现高精度、可解释的在线状态监测。


<details>
  <summary>Details</summary>
Motivation: 旋转机械的可靠性中心预测需要在非平稳工况、域偏移和严重类别不平衡下保持准确的早期预警信号，同时保持低且可预测的误报率，现有方法难以同时满足这些要求。

Method: 提出物理引导微型Mamba Transformer (PG-TMT)：1) 深度可分离卷积主干捕捉微瞬变；2) Tiny-Mamba状态空间分支建模近线性长程动态；3) 轻量级局部Transformer编码跨通道共振。通过解析时频映射将注意力谱与经典轴承故障阶次带对齐，并使用极值理论建模健康分数超限，实现目标误报强度的阈值校准。

Result: 在CWRU、Paderborn、XJTU-SY和工业试点数据集上，PG-TMT在无泄漏流式协议下获得更高的精确率-召回率AUC（主要针对不平衡问题），竞争性或更好的ROC AUC，在匹配误报强度下更短的平均检测时间，并具有强大的跨域迁移能力。

Conclusion: PG-TMT通过物理对齐表示与EVT校准决策规则的结合，为可靠性中心的预测和健康管理提供了校准、可解释且可直接部署的早期预警系统。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [11] [Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21312)
*Xiaozhuang Li,Xindi Tang,Fang He*

Main category: cs.LG

TL;DR: 提出GAT-PEARL框架，结合图注意力网络和概率嵌入强化学习，解决动态充电网络下自动驾驶电动出租车车队管理问题


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车和充电基础设施的快速发展，自动驾驶电动出租车车队在动态不确定的充电可用性环境中面临管理挑战。现有研究大多假设静态充电网络，这与现实运营存在显著差距

Method: 提出GAT-PEARL元强化学习框架：1) 使用图注意力网络提取鲁棒的空间表示并建模复杂的时空关系；2) 采用概率嵌入的actor-critic强化学习，实现无需重新训练的快速推理适应

Result: 基于中国成都真实数据的模拟显示，GAT-PEARL显著优于传统强化学习方法，在未见过的充电网络布局上表现出优越的泛化能力，在动态环境中实现更高的整体运营效率

Conclusion: GAT-PEARL框架有效解决了动态充电网络环境下的自动驾驶电动出租车车队管理问题，填补了理论模型与现实运营之间的差距，为实际应用提供了可行的解决方案

Abstract: With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.

</details>


### [12] [Graph-Free Root Cause Analysis](https://arxiv.org/abs/2601.21359)
*Luan Pham*

Main category: cs.LG

TL;DR: PRISM是一个无需依赖图即可进行根因分析的高效框架，在735个真实故障案例中达到68%的Top-1准确率，比最佳基线提升258%，每次诊断仅需8ms。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中的故障需要快速根因分析以防止级联损害。现有无需依赖图的RCA方法通常假设根因具有最高的异常分数，但当故障传播时，根因处的微小延迟可能在下游累积成更大的异常，导致这一假设失效。

Method: 提出PRISM框架，为缺乏依赖图的系统设计RCA方法。定义了组件化系统的一类模型，在该模型下PRISM能够提供理论保证的根因分析。

Result: 在9个真实世界数据集的735个故障案例上，PRISM达到68%的Top-1准确率，比最佳基线提升258%，每次诊断仅需8毫秒。

Conclusion: PRISM是一个简单高效的RCA框架，无需依赖图即可在故障传播场景中准确识别根因，具有理论保证和实际应用价值。

Abstract: Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.

</details>


### [13] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: 该论文提出SAGE优化框架，解决推荐系统中LLM模型依赖独立词汇表的问题，并改进OneRec的GBPO方法存在的对称保守主义问题，通过序列级信号解耦和非对称自适应动态机制提升冷启动性能和推荐多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统如OneRec需要构建独立的词汇表，导致维护成本高且扩展性差。同时，其GBPO优化方法存在"对称保守主义"问题，抑制冷启动物品的更新动量，在高噪声环境下无法防止多样性崩溃。

Method: 提出SAGE优化框架：1) 序列级信号解耦：结合几何平均重要性比率和解耦的多目标优势，消除token级方差，解决"奖励崩溃"问题；2) 非对称自适应动态：构建动态梯度流形，对高潜力冷启动物品应用"提升因子"实现超线性更新，并使用"熵感知惩罚"打破信息茧房。

Result: 理论分析和实证结果表明，SAGE有效释放冷启动流量并维持推荐多样性，同时保持了GBPO的数值稳定性。

Conclusion: SAGE提供了一个统一的优化框架，能够高效复用开源LLM架构而无需构建独立词汇表，解决了现有方法的对称保守主义问题，在保持数值稳定的同时提升了冷启动性能和推荐多样性。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [14] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 提出ViKeR方法，通过视觉引导的关键令牌正则化来改进多模态大语言模型的遗忘学习，重点关注答案中不同令牌的重要性差异。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM遗忘学习方法主要沿用LLM的方法，将所有答案令牌同等对待，忽略了它们在遗忘过程中重要性不同的问题，且只关注语言模态而忽略了视觉线索。

Method: 提出视觉引导的关键令牌正则化(ViKeR)：1)利用无关视觉输入预测理想的遗忘后令牌级分布；2)用这些分布正则化遗忘过程，优先处理关键令牌；3)通过信息熵定义关键令牌；4)通过令牌级梯度重加权放大关键令牌的更新。

Result: 在MLLMU和CLEAR基准测试中，该方法能有效执行遗忘学习，同时减轻遗忘效应并保持回答的连贯性。

Conclusion: ViKeR方法通过视觉引导和关键令牌正则化，解决了现有MLLM遗忘学习方法中令牌重要性差异被忽视的问题，显著提升了遗忘学习的效果。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [15] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 论文指出强化学习中期望回报最大化目标本身会导致结果层面的模式崩溃，并提出逆概率缩放修正方法IPS-GRPO来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 许多强化学习问题存在多个质量相当的最优解，但标准期望回报最大化训练的策略通常会崩溃到少数结果上。传统解释归因于探索不足或正则化不够，但作者认为这是期望回报目标本身的结构性问题。

Method: 提出逆概率缩放(IPS)修正方法，移除学习信号中的概率放大效应。将其实例化为IPS-GRPO（Group Relative Policy Optimization的改进版），无需辅助模型或架构改变。

Result: 在推理和分子生成任务中，IPS-GRPO持续减少结果层面的模式崩溃，同时匹配或超过基线性能，表明修正目标函数比添加探索启发式方法更关键。

Conclusion: 期望回报目标本身会导致模式崩溃，逆概率缩放修正从根本上改变学习动态，产生与奖励成比例的终端分布，在多模态设置中防止崩溃，是可靠多模态策略优化的关键。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [16] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: KVW是一种无需训练的遗忘方法，通过识别并削弱知识向量来实现大视觉语言模型的高效数据遗忘，避免梯度计算带来的计算开销。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型存在隐私泄露和有害内容生成等严重问题，现有基于梯度优化的遗忘方法计算成本高昂，需要更高效的解决方案。

Method: KVW（知识向量削弱）是一种免训练的遗忘方法，直接干预完整模型而不进行梯度计算。它识别在遗忘集上激活的知识向量，并逐步削弱它们的贡献，防止模型利用不良知识。

Result: 在MLLMU和CLEAR基准测试中，KVW实现了稳定的遗忘-保留权衡，同时在计算效率上显著优于基于梯度和基于LoRA的遗忘方法。

Conclusion: KVW为大规模视觉语言模型提供了一种高效、无需训练的遗忘解决方案，在保持模型性能的同时有效移除特定数据的影响。

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [17] [Dependence of Equilibrium Propagation Training Success on Network Architecture](https://arxiv.org/abs/2601.21945)
*Qingshan Wang,Clara C. Wanjura,Florian Marquardt*

Main category: cs.LG

TL;DR: 研究探索了基于物理的训练方法（平衡传播）在局部连接晶格架构中的性能，发现稀疏网络能达到与密集网络相当的分类准确率


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展导致能源消耗不可持续增长，这推动了神经形态计算和基于物理的机器学习训练方法的发展。然而，现有理论研究多集中于全连接或密集连接网络，这些架构在实验实现上存在连接性约束等挑战

Method: 采用基于物理的训练方法——平衡传播，在XY模型上训练局部连接的晶格架构，探索不同架构在各种基准任务上的性能，追踪训练过程中空间分布式响应和耦合的演化

Result: 稀疏网络（仅含局部连接）能够达到与密集网络相当的性能表现，为在现实场景中基于平衡传播的架构扩展提供了指导

Conclusion: 局部连接的稀疏网络架构在基于物理的训练方法中具有可行性，为神经形态计算和节能AI系统的实际实现提供了重要参考

Abstract: The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.

</details>


### [18] [Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2601.21978)
*Shiqi Fan,Quanming Yao,Hongyi Nie,Wentao Ma,Zhen Wang,Wen Hua*

Main category: cs.LG

TL;DR: IGETR是一个结合图神经网络和LLM的混合时序知识图谱推理框架，通过三阶段流程提升推理准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时序知识图谱推理方法过于关注上下文关系而忽视结构关系，难以从动态图中提取相关子图，导致推理缺乏结构信息、容易产生幻觉，特别是在时序不一致的情况下

Method: 提出IGETR三阶段框架：1) 使用时序GNN识别结构和时序一致的候选路径；2) LLM引导的路径编辑，利用外部知识修正逻辑和语义不一致；3) 整合精炼后的推理路径生成准确且可解释的预测

Result: 在标准TKG基准测试中达到SOTA性能，在ICEWS数据集上Hits@1相对提升5.6%，Hits@3相对提升8.1%，消融研究确认各组件有效性

Conclusion: IGETR通过结合GNN的结构化时序建模能力和LLM的上下文理解能力，有效解决了时序知识图谱推理中的结构信息缺失和幻觉问题，实现了更准确可靠的预测

Abstract: Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 提出EPDDL语言，为基于动态认知逻辑的认知规划提供统一的PDDL式表示，解决现有规划器语言碎片化问题


<details>
  <summary>Details</summary>
Motivation: 现有基于动态认知逻辑的认知规划器使用不同的片段和临时语言，缺乏统一表示，阻碍了比较、重用和系统化基准开发

Method: 1. 开发抽象事件模型作为认知动作的新表示；2. 基于抽象事件模型和动态认知逻辑形式化定义EPDDL的语法和语义；3. 识别适合现有规划器的有用片段并展示如何在EPDDL中表示

Result: EPDDL能够捕获完整的动态认知逻辑语义，通过代表性基准示例展示了其促进互操作性、可重复评估和未来发展的能力

Conclusion: EPDDL为认知规划提供了统一的领域定义语言，解决了语言碎片化问题，支持互操作性和系统化基准开发

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [20] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK框架通过建模查询不确定性，使用恢复假设和查询锚定聚合，在非忠实查询下实现鲁棒检索。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的检索查询往往是非忠实的（有噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题。

Method: 提出QUARK框架：1) 通过恢复假设显式建模查询不确定性；2) 引入查询锚定聚合来鲁棒地结合这些信号；3) 原始查询作为语义锚点，恢复假设提供受控的辅助证据。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）上，QUARK在稀疏和稠密检索器上均提高了Recall、MRR和nDCG。消融实验显示QUARK对恢复假设数量具有鲁棒性，且锚定聚合优于未锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，并结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK是一个简单有效的免训练框架。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [21] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时程电子健康记录设计的检索增强框架，通过事件和时间感知检索、自适应迭代检索和双路径证据检索推理，提升临床预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含丰富的纵向临床证据，但长时程EHR常超出LLM上下文限制，现有方法通过截断或简单检索会丢失临床相关事件和时间依赖关系，需要专门针对结构化EHR数据的检索增强框架。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知混合EHR检索，保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖；3）双路径证据检索和推理，同时检索和推理事实与反事实证据。

Result: 在四个长时程EHR预测任务上的实验表明，EHR-RAG始终优于最强的LLM基线方法，平均Macro-F1提高了10.76%。

Conclusion: 检索增强的LLMs在结构化EHR数据的临床预测方面具有实际应用潜力，EHR-RAG框架通过专门设计的检索策略有效解决了长时程EHR处理中的挑战。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [22] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该研究将企业级智能文档处理系统的验证问题形式化为基于搜索的软件测试问题，通过比较多种搜索策略来最大化在有限预算内发现的不同故障类型数量。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证需要在有限预算下发现多样化的故障机制，而不仅仅是识别单个最坏情况文档。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，实例化结构风险特征以诱导现实故障条件。比较了进化算法、群体智能、质量多样性、学习型和量子算法等多种搜索策略。

Result: 不同求解器在相同预算下能发现其他方法未发现的故障模式，没有单一策略具有绝对优势。所有求解器的并集最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 研究结果表明求解器之间存在内在互补性，需要基于组合策略的SBST方法来进行稳健的工业IDP验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [23] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致性气候先验和统一坐标引导解决现有方法物理不一致和梯度消失问题


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据且与再分析数据存在领域差距，难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题

Method: 提出ZSSD零样本统计降尺度框架：1) 从再分析数据学习物理一致性气候先验，以地球物理边界和时序信息为条件确保物理有效性；2) 引入统一坐标引导策略解决DPS中的梯度消失问题，确保与大尺度场的一致性

Result: ZSSD在99百分位误差上显著优于现有零样本基线方法，能成功重建复杂天气事件（如热带气旋），并在异质GCMs中表现良好

Conclusion: ZSSD为零样本气候降尺度提供了有效的解决方案，通过物理一致性先验和统一坐标引导解决了现有方法的局限性，在多种GCMs中表现出色

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [24] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: CoMT框架通过两阶段认知对齐方法改进LLM后训练：第一阶段专注于抽象推理模式学习，第二阶段通过置信度校准强化学习优化任务适应，显著提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法将完整推理轨迹作为基本单元进行优化，这与人类解决问题的两阶段认知过程（先获取抽象策略，再适应具体问题）存在根本性不匹配，导致抽象策略与问题特定执行纠缠，限制了泛化能力。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过置信度感知奖励优化中间步骤的任务适应，防止过度自信错误传播。

Result: 在4个模型和8个基准测试中，相比标准方法，分布内性能提升2.19%，分布外性能提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能获得更好的泛化性能，还能显著提高训练效率，证明了认知启发方法在LLM优化中的价值。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [25] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: DeR2是一个用于评估大语言模型在科学推理能力上的受控测试平台，通过四种证据访问机制分离检索损失与推理损失，揭示模型在深度研究任务中的真实表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确评估大语言模型处理新颖科学信息的能力，因为端到端RAG评估中推理与检索、工具链选择混淆，且受到参数记忆和网络数据波动的影响。

Method: 开发DeR2深度研究沙箱，通过四种证据访问机制（仅指令、仅概念、仅相关文档、完整文档集）分离证据获取与推理过程，使用两阶段验证防止参数泄漏，构建基于2023-2025年理论论文的冻结文档库。

Result: 实验显示不同基础模型表现差异显著：部分模型存在模式切换脆弱性（完整文档集表现比仅指令更差），另一些模型存在结构性概念误用（能正确命名概念但无法将其作为程序执行）。

Conclusion: DeR2提供了一个受控环境来评估大语言模型的科学推理能力，揭示了模型在深度研究任务中的实质性局限和改进空间，为模型能力评估提供了细粒度的错误归因框架。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [26] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个针对足球事件流数据的系统化验证框架，通过状态转移模型检测和纠正逻辑不一致性，显著提升数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误、事件缺失），限制了分析模型的可靠性。

Method: 提出VERSA框架，基于状态转移模型定义有效事件序列，实现事件流数据中异常模式的自动检测和纠正。

Result: 在K League 1（2024赛季）数据中发现18.81%的事件存在逻辑不一致；VERSA显著提升了跨数据提供商的一致性，并改善了VAEP等下游任务的性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性，VERSA为足球分析提供了稳定统一的数据表示基础。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [27] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 提出Liquid Interfaces（液态接口）协调范式，将接口从静态技术构件转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的智能体系统。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持具有自适应、概率性和上下文相关推理能力的自主智能体，而系统集成仍主要依赖静态接口和确定性契约，这限制了智能体系统的协调能力。

Method: 提出Liquid Interfaces协调范式，将接口视为短暂的关系事件而非持久技术构件；形式化该模型并设计Liquid Interface Protocol（LIP），管理意图驱动交互、协商执行和在语义不确定性下的短暂性执行；讨论治理影响并提供参考架构。

Result: 建立了Liquid Interfaces的理论框架和协议规范，展示了该方法的实际可行性，为基于智能体的自适应协调系统提供了原则性基础。

Conclusion: Liquid Interfaces为智能体系统的自适应协调提供了新的理论基础，通过将接口动态化和事件化，解决了传统静态接口在支持自适应、概率性智能体协调方面的局限性。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出一种新的预训练方法，使用强化学习在预训练阶段直接提升模型生成质量、安全性和事实性，避免后训练阶段难以纠正预训练习得的模式。


<details>
  <summary>Details</summary>
Motivation: 当前主要通过收集昂贵数据集和多阶段微调对齐来解决LLM的安全性和事实性问题，但这种方法无法纠正预训练阶段习得的模式。需要在预训练阶段就解决这些问题，因为预训练塑造了模型的核心行为，防止不安全或幻觉输出被深度嵌入。

Method: 引入新的预训练方法：流式处理文档，使用强化学习在每一步改进接下来K个生成的token。使用一个经过后训练的强模型来评估候选生成（包括模型rollout、原始后缀和重写后缀）的质量、安全性和事实性。训练早期依赖原始和重写后缀，随着模型改进，RL奖励高质量的rollout。

Result: 相比标准预训练，在事实性方面获得36.2%的相对改进，安全性方面获得18.5%的相对改进，在整体生成质量方面获得高达86.3%的胜率改进。

Conclusion: 该方法从基础层面构建了更高质量、更安全、更事实的模型，通过在预训练阶段直接集成质量、安全性和事实性目标，避免了后训练阶段难以纠正预训练习得模式的问题。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [29] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: 提出OVD框架，用轨迹匹配替代token级概率匹配，通过教师模型的离散言语评分进行知识蒸馏，显著降低内存消耗并提升学生模型性能


<details>
  <summary>Details</summary>
Motivation: 现有token级on-policy蒸馏方法需要token级对齐，限制了学生模型的探索能力，无法有效利用交互环境反馈，且在强化学习中存在严重内存瓶颈

Method: 引入On-policy Verbal Distillation (OVD)框架，用离散言语评分（0-9）进行轨迹匹配，替代token级概率匹配，避免token级对齐，允许学生模型自由探索输出空间

Result: 在Web问答和数学推理任务上显著优于现有方法，Web Q&A任务平均EM提升+12.9%，数学基准提升+25.7%（仅使用一个随机样本训练），同时训练效率更高

Conclusion: OVD框架通过言语评分轨迹匹配解决了现有知识蒸馏方法的内存瓶颈和探索限制问题，在保持高效的同时显著提升了学生模型的推理能力

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1是首个用于工业巡检安全评估的多模态基准数据集，包含真实工业场景下的多传感器数据、像素级分割标注和安全等级标签，覆盖5种典型工业场景。


<details>
  <summary>Details</summary>
Motivation: 工业智能化和无人巡检快速发展，但现有数据集多为模拟数据、单模态感知或缺乏细粒度标注，限制了工业基础模型的鲁棒场景理解和多模态安全推理能力。

Method: 从真实工业环境中41台轮式和轨道式巡检机器人的日常操作中收集数据，覆盖239个有效巡检站点，构建包含5013个巡检实例的数据集。提供可见光图像的像素级分割标注、语义场景描述和安全等级标签，并包含7种同步感知模态。

Result: 发布了InspecSafe-V1数据集，覆盖隧道、电力设施、烧结设备、石油化工和煤炭输送栈桥5种工业场景，包含红外视频、音频、深度点云、雷达点云、气体测量、温湿度等7种同步模态，支持多模态异常识别和跨模态融合。

Conclusion: InspecSafe-V1填补了工业巡检安全评估领域真实多模态数据集的空白，为工业基础模型的多模态异常识别、跨模态融合和综合安全评估提供了重要基准资源。

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [31] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 提出分层主动调优框架HPTune，通过评估已执行和未执行动作来高效调优MPC运动规划器参数，结合快速级和慢速级调优，并整合多普勒激光雷达增强运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有MPC参数调优方法通常只评估已执行动作，导致参数更新效率低下，因为失败事件（如障碍物接近或碰撞）稀疏。需要扩展评估范围以改善调优效率。

Method: 提出分层主动调优框架HPTune：1）快速级调优采用预测接近速度和预测接近距离的风险指标；2）慢速级调优利用扩展评估损失进行闭环反向传播；3）整合多普勒激光雷达提供障碍物速度和位置信息以增强运动预测。

Result: 在高保真模拟器上的大量实验表明，HPTune实现了高效的MPC调优，在复杂环境中优于各种基线方案。HPTune能够通过制定安全、敏捷的避碰策略实现情境定制的运动规划。

Conclusion: HPTune框架通过扩展评估范围到未执行动作，结合分层调优和多普勒激光雷达增强，显著提高了MPC运动规划器的参数调优效率和性能，实现了更安全、更敏捷的避碰策略。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [32] [Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning](https://arxiv.org/abs/2601.21548)
*Irene Ambrosini,Ingo Blakowski,Dmitrii Zendrikov,Cristiano Capone,Luna Gava,Giacomo Indiveri,Chiara De Luca,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 使用脉冲神经网络在神经形态处理器上实现空气曲棍球实时学习控制，通过硬件算法协同设计实现快速强化学习


<details>
  <summary>Details</summary>
Motivation: 空气曲棍球需要在高速度下做出瞬间决策，传统方法难以应对。研究旨在将神经科学启发的硬件与真实世界机器人控制相结合，展示脑启发方法能够处理快速交互任务并支持智能机器的持续学习。

Method: 采用混合信号模拟/数字神经形态处理器上的脉冲神经网络，通过硬件与学习算法协同设计。利用固定随机连接捕捉任务时间结构，在读出层采用局部e-prop学习规则，利用事件驱动活动实现快速高效学习。通过计算机和神经形态芯片在环设置进行实时学习。

Result: 系统在极少数试验中成功实现冰球交互，展示了脉冲神经网络在机器人自主系统中的实际训练能力。实现了实时学习，将神经科学启发的硬件与真实世界机器人控制相连接。

Conclusion: 脑启发方法能够处理快速交互任务，支持智能机器的持续学习，为机器人自主系统的脉冲神经网络实际训练提供了可行方案，展示了硬件算法协同设计的优势。

Abstract: Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.

</details>


### [33] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 提出声音触发的移动操作新范式，让智能体主动感知声音源并执行操作，无需显式指令，通过Habitat-Echo平台和分层基线模型实现


<details>
  <summary>Details</summary>
Motivation: 当前移动操作研究主要依赖预定义文本指令，限制了智能体的自主性和对动态环境事件的响应能力，需要更主动的感知和交互方式

Method: 引入声音触发的移动操作范式，开发Habitat-Echo数据平台（集成声学渲染与物理交互），提出包含高层任务规划器和低层策略模型的分层基线方法

Result: 实验表明基线方法使智能体能主动检测和响应听觉事件，无需逐案指令；在挑战性的双声源场景中，智能体成功从重叠声学干扰中分离主声源执行首次交互，随后操作次声源对象

Conclusion: 声音触发的移动操作范式提升了智能体自主性，Habitat-Echo平台和分层基线方法有效支持智能体在动态环境中主动感知和交互，验证了方法的鲁棒性

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [34] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab是一个轻量级开源机器人学习框架，结合GPU加速仿真、模块化环境和最小化安装配置，提供单命令安装和原生MuJoCo数据结构访问


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习框架存在安装复杂、依赖多、配置繁琐的问题，需要一种轻量级、易安装、GPU加速的解决方案来降低机器人学习的研究门槛

Method: 采用Isaac Lab引入的manager-based API，结合MuJoCo Warp进行GPU加速物理仿真，提供模块化构建块（观测、奖励、事件），实现单命令安装和最小依赖

Result: 开发出mjlab框架，包含速度跟踪、运动模仿和操作任务的参考实现，提供直接访问原生MuJoCo数据结构的能力

Conclusion: mjlab是一个高效、易用的机器人学习框架，通过GPU加速和模块化设计显著降低了机器人学习的研究门槛，为社区提供了有价值的工具

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>
