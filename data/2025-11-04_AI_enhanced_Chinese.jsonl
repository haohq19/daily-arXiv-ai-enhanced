{"id": "2511.00047", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.00047", "abs": "https://arxiv.org/abs/2511.00047", "authors": ["Omkar Kulkarni", "Rohitash Chandra"], "title": "DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection", "comment": null, "summary": "Financial fraud detection is critical for maintaining the integrity of\nfinancial systems, particularly in decentralised environments such as\ncryptocurrency networks. Although Graph Convolutional Networks (GCNs) are\nwidely used for financial fraud detection, graph Transformer models such as\nGraph-BERT are gaining prominence due to their Transformer-based architecture,\nwhich mitigates issues such as over-smoothing. Graph-BERT is designed for\nstatic graphs and primarily evaluated on citation networks with undirected\nedges. However, financial transaction networks are inherently dynamic, with\nevolving structures and directed edges representing the flow of money. To\naddress these challenges, we introduce DynBERG, a novel architecture that\nintegrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture\ntemporal evolution over multiple time steps. Additionally, we modify the\nunderlying algorithm to support directed edges, making DynBERG well-suited for\ndynamic financial transaction analysis. We evaluate our model on the Elliptic\ndataset, which includes Bitcoin transactions, including all transactions during\na major cryptocurrency market event, the Dark Market Shutdown. By assessing\nDynBERG's resilience before and after this event, we analyse its ability to\nadapt to significant market shifts that impact transaction behaviours. Our\nmodel is benchmarked against state-of-the-art dynamic graph classification\napproaches, such as EvolveGCN and GCN, demonstrating superior performance,\noutperforming EvolveGCN before the market shutdown and surpassing GCN after the\nevent. Additionally, an ablation study highlights the critical role of\nincorporating a time-series deep learning component, showcasing the\neffectiveness of GRU in modelling the temporal dynamics of financial\ntransactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86DynBERG\u6a21\u578b\uff0c\u5c06Graph-BERT\u4e0eGRU\u7ed3\u5408\u7528\u4e8e\u52a8\u6001\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\uff0c\u652f\u6301\u6709\u5411\u8fb9\u548c\u65f6\u95f4\u6f14\u5316\u5206\u6790\uff0c\u5728\u6bd4\u7279\u5e01\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709Graph-BERT\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u65e0\u5411\u56fe\u8bbe\u8ba1\uff0c\u800c\u91d1\u878d\u4ea4\u6613\u7f51\u7edc\u662f\u52a8\u6001\u6709\u5411\u7684\uff0c\u9700\u8981\u80fd\u591f\u6355\u6349\u65f6\u95f4\u6f14\u5316\u548c\u8d44\u91d1\u6d41\u5411\u7684\u6a21\u578b\u3002", "method": "\u7ed3\u5408Graph-BERT\u4e0eGRU\u5c42\uff0c\u652f\u6301\u6709\u5411\u8fb9\uff0c\u6355\u6349\u591a\u65f6\u95f4\u6b65\u957f\u7684\u65f6\u5e8f\u6f14\u5316\uff0c\u4e13\u95e8\u9488\u5bf9\u52a8\u6001\u91d1\u878d\u4ea4\u6613\u7f51\u7edc\u8bbe\u8ba1\u3002", "result": "\u5728Elliptic\u6bd4\u7279\u5e01\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728Dark Market Shutdown\u4e8b\u4ef6\u524d\u540e\u5747\u4f18\u4e8eEvolveGCN\u548cGCN\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660eGRU\u7ec4\u4ef6\u5bf9\u65f6\u5e8f\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "DynBERG\u80fd\u6709\u6548\u9002\u5e94\u91cd\u5927\u5e02\u573a\u53d8\u5316\uff0c\u4e3a\u52a8\u6001\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65f6\u5e8f\u6df1\u5ea6\u5b66\u4e60\u7ec4\u4ef6\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u8d21\u732e\u3002"}}
{"id": "2511.00211", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00211", "abs": "https://arxiv.org/abs/2511.00211", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals", "comment": null, "summary": "The increasing adoption of satellite Internet with low-Earth-orbit (LEO)\nsatellites in mega-constellations allows ubiquitous connectivity to rural and\nremote areas. However, weather events have a significant impact on the\nperformance and reliability of satellite Internet. Adverse weather events such\nas snow and rain can disturb the performance and operations of satellite\nInternet's essential ground terminal components, such as satellite antennas,\nsignificantly disrupting the space-ground link conditions between LEO\nsatellites and ground stations. This challenge calls for not only region-based\nweather forecasts but also fine-grained detection capability on ground terminal\ncomponents of fine-grained weather conditions. Such a capability can assist in\nfault diagnostics and mitigation for reliable satellite Internet, but its\nsolutions are lacking, not to mention the effectiveness and generalization that\nare essential in real-world deployments. This paper discusses an efficient\ntransfer learning (TL) method that can enable a ground component to locally\ndetect representative weather-related conditions. The proposed method can\ndetect snow, wet, and other conditions resulting from adverse and typical\nweather events and shows superior performance compared to the typical deep\nlearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL\nmethod also shows the advantage of being generalizable to various scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u536b\u661f\u4e92\u8054\u7f51\u5730\u9762\u7ec8\u7aef\u7ec4\u4ef6\u7684\u7ec6\u7c92\u5ea6\u5929\u6c14\u6761\u4ef6\u68c0\u6d4b\uff0c\u80fd\u591f\u68c0\u6d4b\u96ea\u3001\u6f6e\u6e7f\u7b49\u5929\u6c14\u76f8\u5173\u72b6\u51b5\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5929\u6c14\u4e8b\u4ef6\u5bf9\u4f4e\u8f68\u536b\u661f\u4e92\u8054\u7f51\u6027\u80fd\u548c\u53ef\u9760\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u5730\u9762\u7ec8\u7aef\u7ec4\u4ef6\u5929\u6c14\u6761\u4ef6\u68c0\u6d4b\u80fd\u529b\u6765\u534f\u52a9\u6545\u969c\u8bca\u65ad\u548c\u7f13\u89e3\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u5730\u9762\u7ec4\u4ef6\u80fd\u591f\u672c\u5730\u68c0\u6d4b\u4ee3\u8868\u6027\u7684\u5929\u6c14\u76f8\u5173\u6761\u4ef6\uff0c\u5305\u62ec\u96ea\u3001\u6f6e\u6e7f\u7b49\u7531\u6076\u52a3\u548c\u5178\u578b\u5929\u6c14\u4e8b\u4ef6\u5bfc\u81f4\u7684\u72b6\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u96ea\u3001\u6f6e\u6e7f\u7b49\u5929\u6c14\u6761\u4ef6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8eYOLOv7\u3001YOLOv9\u3001Faster R-CNN\u548cR-YOLO\u7b49\u5178\u578b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u536b\u661f\u4e92\u8054\u7f51\u5730\u9762\u7ec8\u7aef\u7ec4\u4ef6\u7684\u5929\u6c14\u76f8\u5173\u6761\u4ef6\uff0c\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2511.00808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c06RLVR LLM\u8bad\u7ec3\u5e94\u7528\u4e8e\u516c\u5171\u4ea4\u901a\u8fd0\u8425\u4e2d\u7684\u5b9e\u65f6\u9884\u6d4b\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bb9\u5dee\u7684\u5956\u52b1\u51fd\u6570\u6765\u9002\u5e94\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\uff0c\u5728NYC MTA\u670d\u52a1\u8b66\u62a5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u9884\u6d4b\u516c\u5171\u4ea4\u901a\u4e8b\u4ef6\u6301\u7eed\u65f6\u95f4\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9886\u57df\u7a00\u758f\u6027\u548c\u566a\u58f0\u8fde\u7eed\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u800cRLVR\u867d\u7136\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4e8c\u5143\u6b63\u786e\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u566a\u58f0\u8fde\u7eed\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bb9\u5dee\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u8fde\u7eed\u8bef\u5dee\u8303\u56f4\u5185\u7ed9\u4e88\u90e8\u5206\u4fe1\u7528\uff0c\u800c\u4e0d\u662f\u8981\u6c42\u5355\u4e00\u6b63\u786e\u7b54\u6848\uff0c\u5c06RLVR\u9002\u5e94\u4e8e\u8be5\u4efb\u52a1\uff0c\u5e76\u5728NYC MTA\u670d\u52a1\u8b66\u62a5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u901a\u7528\u6307\u4ee4\u8c03\u4f18LLM\u663e\u8457\u4f18\u4e8e\u4e13\u95e8\u7684\u6570\u5b66\u63a8\u7406\u6a21\u578b\uff0c\u540e\u8005\u5728\u5904\u7406\u6a21\u7cca\u7684\u73b0\u5b9e\u4e16\u754c\u6587\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\u3002RLVR\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u57285\u5206\u949f\u51c6\u786e\u7387\u4e0a\u5b9e\u73b0\u4e8635%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "RLVR\u53ef\u4ee5\u6210\u529f\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u566a\u58f0\u9884\u6d4b\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u8bbe\u8ba1\u53cd\u6620\u95ee\u9898\u8fde\u7eed\u6027\u8d28\u7684\u9a8c\u8bc1\u5668\uff0c\u4e8c\u5143\u5956\u52b1\u4e0d\u7a33\u5b9a\u4e14\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.00097", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00097", "abs": "https://arxiv.org/abs/2511.00097", "authors": ["Zihao Guo", "Qingyun Sun", "Ziwei Zhang", "Haonan Yuan", "Huiping Zhuang", "Xingcheng Fu", "Jianxin Li"], "title": "GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation", "comment": "Accepted by the Main Track of NeurIPS-2025", "summary": "Graph incremental learning (GIL), which continuously updates graph models by\nsequential knowledge acquisition, has garnered significant interest recently.\nHowever, existing GIL approaches focus on task-incremental and\nclass-incremental scenarios within a single domain. Graph domain-incremental\nlearning (Domain-IL), aiming at updating models across multiple graph domains,\nhas become critical with the development of graph foundation models (GFMs), but\nremains unexplored in the literature. In this paper, we propose Graph\nDomain-Incremental Learning via Knowledge Dientanglement and Preservation\n(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from\nthe perspectives of embedding shifts and decision boundary deviations.\nSpecifically, to prevent embedding shifts and confusion across incremental\ngraph domains, we first propose the domain-specific parameter-efficient\nfine-tuning together with intra- and inter-domain disentanglement objectives.\nConsequently, to maintain a stable decision boundary, we introduce\ndeviation-free knowledge preservation to continuously fit incremental domains.\nAdditionally, for graphs with unobservable domains, we perform domain-aware\ndistribution discrimination to obtain precise embeddings. Extensive experiments\ndemonstrate the proposed GraphKeeper achieves state-of-the-art results with\n6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,\nwe show GraphKeeper can be seamlessly integrated with various representative\nGFMs, highlighting its broad applicative potential.", "AI": {"tldr": "\u63d0\u51fa\u4e86GraphKeeper\u65b9\u6cd5\u6765\u89e3\u51b3\u56fe\u57df\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u89e3\u8026\u548c\u4fdd\u6301\u6765\u5e94\u5bf9\u5d4c\u5165\u504f\u79fb\u548c\u51b3\u7b56\u8fb9\u754c\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u57df\u5185\u7684\u4efb\u52a1\u589e\u91cf\u548c\u7c7b\u522b\u589e\u91cf\u573a\u666f\uff0c\u800c\u56fe\u57df\u589e\u91cf\u5b66\u4e60\uff08Domain-IL\uff09\u5728\u591a\u4e2a\u56fe\u57df\u95f4\u66f4\u65b0\u6a21\u578b\u7684\u9700\u6c42\u968f\u7740\u56fe\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8be5\u9886\u57df\u5728\u6587\u732e\u4e2d\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "GraphKeeper\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u57df\u7279\u5b9a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7ed3\u5408\u57df\u5185\u548c\u57df\u95f4\u89e3\u8026\u76ee\u6807\u6765\u9632\u6b62\u5d4c\u5165\u504f\u79fb\uff1b2\uff09\u65e0\u504f\u5dee\u77e5\u8bc6\u4fdd\u6301\u6765\u7ef4\u6301\u7a33\u5b9a\u51b3\u7b56\u8fb9\u754c\uff1b3\uff09\u5bf9\u4e8e\u4e0d\u53ef\u89c2\u6d4b\u57df\u7684\u56fe\uff0c\u6267\u884c\u57df\u611f\u77e5\u5206\u5e03\u5224\u522b\u4ee5\u83b7\u5f97\u7cbe\u786e\u5d4c\u5165\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGraphKeeper\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u76f8\u6bd4\u7b2c\u4e8c\u540d\u67096.5%~16.6%\u7684\u63d0\u5347\uff0c\u4e14\u9057\u5fd8\u6548\u5e94\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u8be5\u65b9\u6cd5\u80fd\u4e0e\u5404\u79cd\u4ee3\u8868\u6027\u56fe\u57fa\u7840\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "GraphKeeper\u901a\u8fc7\u77e5\u8bc6\u89e3\u8026\u548c\u4fdd\u6301\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u57df\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.01018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy.", "AI": {"tldr": "\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u513f\u7ae5\u54ee\u5598\u53cd\u590d\u53d1\u4f5c\uff0cLGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u73b0\u6709\u51b3\u7b56\u89c4\u5219\u6709\u663e\u8457\u6539\u8fdb", "motivation": "\u513f\u7ae5\u54ee\u5598\u53cd\u590d\u53d1\u4f5c\u662f\u5e38\u89c1\u4f46\u53ef\u9884\u9632\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7535\u5b50\u75c5\u5386\u6570\u636e\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u51c6\u786e\u8bc6\u522b\u9ad8\u98ce\u9669\u513f\u7ae5\u5e76\u8f6c\u8bca\u8fdb\u884c\u9884\u9632\u6027\u7efc\u5408\u62a4\u7406", "method": "\u4f7f\u7528\u56de\u987e\u6027\u7535\u5b50\u75c5\u5386\u6570\u636e\u8bad\u7ec3\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecLGBM\u3001XGB\u548c\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u7ed3\u5408\u73af\u5883\u6c61\u67d3\u7269\u66b4\u9732\u548c\u793e\u533a\u8fb9\u7f18\u5316\u4fe1\u606f\uff0c\u5728COVID19\u524d\u540e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1", "result": "LGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cAIRE-KIDS_ED\u6a21\u578b\u7684AUC\u4e3a0.712\uff0cF1\u5206\u6570\u4e3a0.51\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u51b3\u7b56\u89c4\u5219\uff08F1=0.334\uff09", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u513f\u7ae5\u54ee\u5598\u53cd\u590d\u53d1\u4f5c\u98ce\u9669\uff0c\u4e3a\u9ad8\u98ce\u9669\u513f\u7ae5\u63d0\u4f9b\u9884\u9632\u6027\u62a4\u7406\u8f6c\u8bca\u63d0\u4f9b\u4e86\u53ef\u884c\u5de5\u5177"}}
{"id": "2511.01149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u534f\u4f5c\u673a\u5236\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u6267\u884c\u95ee\u9898\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5206\u89e3\u6548\u7387\u548c\u534f\u4f5c\u5e73\u8861\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5355\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u6267\u884c\u4e2d\u4efb\u52a1\u5206\u89e3\u548c\u534f\u4f5c\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7edf\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u5f15\u5165\u6a21\u5757\u5316\u5206\u89e3\u673a\u5236\u5c06\u6574\u4f53\u76ee\u6807\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u548c\u8def\u7531\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u5408\u7406\u5206\u5de5\u548c\u5b9e\u65f6\u534f\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u7ea6\u675f\u89e3\u6790\u548c\u5168\u5c40\u4e00\u81f4\u6027\u673a\u5236\u786e\u4fdd\u5b50\u4efb\u52a1\u8fde\u8d2f\u6027\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5206\u89e3\u6548\u7387\u3001\u5b50\u4efb\u52a1\u8986\u76d6\u7387\u548c\u534f\u4f5c\u5e73\u8861\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6574\u4f53\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u5f00\u9500\u4e4b\u95f4\u7684\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8bed\u8a00\u9a71\u52a8\u7684\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u534f\u4f5c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01272", "abs": "https://arxiv.org/abs/2511.01272", "authors": ["Sehui Jeong", "Magaly C. Aviles", "Athena X. Naylor", "Cynthia Sung", "Allison M. Okamura"], "title": "Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics", "comment": null, "summary": "Soft robots employing compliant materials and deformable structures offer\ngreat potential for wearable devices that are comfortable and safe for human\ninteraction. However, achieving both structural integrity and compliance for\ncomfort remains a significant challenge. In this study, we present a novel\nfabrication and design method that combines the advantages of origami\nstructures with the material programmability and wearability of knitted\nfabrics. We introduce a general design method that translates origami patterns\ninto knit designs by programming both stitch and material patterns. The method\ncreates folds in preferred directions while suppressing unintended buckling and\nbending by selectively incorporating heat fusible yarn to create rigid panels\naround compliant creases. We experimentally quantify folding moments and show\nthat stitch patterning enhances folding directionality while the heat fusible\nyarn (1) keeps geometry consistent by reducing edge curl and (2) prevents\nout-of-plane deformations by stiffening panels. We demonstrate the framework\nthrough the successful reproduction of complex origami tessellations, including\nMiura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted\nKaleidocycle robot capable of locomotion. The combination of structural\nreconfigurability, material programmability, and potential for manufacturing\nscalability highlights knitted origami as a promising platform for\nnext-generation wearable robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6298\u7eb8\u7ed3\u6784\u4e0e\u9488\u7ec7\u9762\u6599\u76f8\u7ed3\u5408\u7684\u65b0\u5236\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7a0b\u9488\u8ff9\u548c\u6750\u6599\u56fe\u6848\u6765\u521b\u5efa\u5177\u6709\u53ef\u63a7\u6298\u53e0\u65b9\u5411\u7684\u9488\u7ec7\u6298\u7eb8\u7ed3\u6784\uff0c\u7528\u4e8e\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u5e94\u7528\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u4f7f\u7528\u67d4\u6027\u6750\u6599\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u63d0\u4f9b\u4e86\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5b9e\u73b0\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8212\u9002\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u9700\u8981\u7ed3\u5408\u6298\u7eb8\u7ed3\u6784\u7684\u4f18\u52bf\u4e0e\u9488\u7ec7\u9762\u6599\u7684\u6750\u6599\u53ef\u7f16\u7a0b\u6027\u548c\u53ef\u7a7f\u6234\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7a0b\u9488\u8ff9\u56fe\u6848\u548c\u9009\u62e9\u6027\u52a0\u5165\u70ed\u7194\u7eb1\u7ebf\u6765\u5c06\u6298\u7eb8\u56fe\u6848\u8f6c\u5316\u4e3a\u9488\u7ec7\u8bbe\u8ba1\u3002\u70ed\u7194\u7eb1\u7ebf\u5728\u8936\u76b1\u5468\u56f4\u5f62\u6210\u521a\u6027\u9762\u677f\uff0c\u540c\u65f6\u4fdd\u6301\u67d4\u6027\u8936\u76b1\u3002", "result": "\u5b9e\u9a8c\u91cf\u5316\u4e86\u6298\u53e0\u529b\u77e9\uff0c\u8bc1\u660e\u9488\u8ff9\u56fe\u6848\u589e\u5f3a\u4e86\u6298\u53e0\u65b9\u5411\u6027\uff0c\u70ed\u7194\u7eb1\u7ebf\u51cf\u5c11\u4e86\u8fb9\u7f18\u5377\u66f2\u5e76\u9632\u6b62\u5e73\u9762\u5916\u53d8\u5f62\u3002\u6210\u529f\u590d\u5236\u4e86\u590d\u6742\u7684\u6298\u7eb8\u9576\u5d4c\u56fe\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u7a7f\u6234\u9488\u7ec7\u4e07\u82b1\u7b52\u5faa\u73af\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u9488\u7ec7\u6298\u7eb8\u7ed3\u5408\u4e86\u7ed3\u6784\u53ef\u91cd\u6784\u6027\u3001\u6750\u6599\u53ef\u7f16\u7a0b\u6027\u548c\u5236\u9020\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u5e73\u53f0\u3002"}}
{"id": "2511.00121", "categories": ["cs.LG", "physics.soc-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00121", "abs": "https://arxiv.org/abs/2511.00121", "authors": ["Shoma Yagi", "Jun Ichikawa", "Genki Ichinose"], "title": "Analysis of Line Break prediction models for detecting defensive breakthrough in football", "comment": "14 pages, 8 figures", "summary": "In football, attacking teams attempt to break through the opponent's\ndefensive line to create scoring opportunities. This action, known as a Line\nBreak, is a critical indicator of offensive effectiveness and tactical\nperformance, yet previous studies have mainly focused on shots or goal\nopportunities rather than on how teams break the defensive line. In this study,\nwe develop a machine learning model to predict Line Breaks using event and\ntracking data from the 2023 J1 League season. The model incorporates 189\nfeatures, including player positions, velocities, and spatial configurations,\nand employs an XGBoost classifier to estimate the probability of Line Breaks.\nThe proposed model achieved high predictive accuracy, with an AUC of 0.982 and\na Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such\nas offensive player speed, gaps in the defensive line, and offensive players'\nspatial distributions significantly contribute to the occurrence of Line\nBreaks. Finally, we found a moderate positive correlation between the predicted\nprobability of being Line-Broken and the number of shots and crosses conceded\nat the team level. These results suggest that Line Breaks are closely linked to\nthe creation of scoring opportunities and provide a quantitative framework for\nunderstanding tactical dynamics in football.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528XGBoost\u5206\u7c7b\u5668\u9884\u6d4b\u8db3\u7403\u4e2d\u7684\u9632\u7ebf\u7a81\u7834\u4e8b\u4ef6\uff0c\u6a21\u578b\u57fa\u4e8e\u7403\u5458\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u7a7a\u95f4\u914d\u7f6e\u7b49\u7279\u5f81\uff0c\u5728J1\u8054\u8d5b\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u5ea6\u3002", "motivation": "\u8db3\u7403\u4e2d\u7a81\u7834\u5bf9\u624b\u9632\u7ebf\u662f\u521b\u9020\u5f97\u5206\u673a\u4f1a\u7684\u5173\u952e\u6218\u672f\u884c\u4e3a\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5c04\u95e8\u6216\u8fdb\u7403\u673a\u4f1a\uff0c\u7f3a\u4e4f\u5bf9\u9632\u7ebf\u7a81\u7834\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4f7f\u75282023\u5e74J1\u8054\u8d5b\u7684\u4e8b\u4ef6\u548c\u8ffd\u8e2a\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b189\u4e2a\u7279\u5f81\u7684XGBoost\u5206\u7c7b\u5668\u6a21\u578b\uff0c\u7279\u5f81\u5305\u62ec\u7403\u5458\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u7a7a\u95f4\u914d\u7f6e\u7b49\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u5ea6\u5f88\u9ad8\uff0cAUC\u8fbe\u52300.982\uff0cBrier\u5206\u6570\u4e3a0.015\u3002SHAP\u5206\u6790\u663e\u793a\u8fdb\u653b\u7403\u5458\u901f\u5ea6\u3001\u9632\u7ebf\u7a7a\u9699\u548c\u8fdb\u653b\u7403\u5458\u7a7a\u95f4\u5206\u5e03\u662f\u91cd\u8981\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u9632\u7ebf\u7a81\u7834\u4e0e\u5f97\u5206\u673a\u4f1a\u521b\u9020\u5bc6\u5207\u76f8\u5173\uff0c\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u8db3\u7403\u6218\u672f\u52a8\u6001\u63d0\u4f9b\u4e86\u91cf\u5316\u6846\u67b6\u3002"}}
{"id": "2511.00124", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00124", "abs": "https://arxiv.org/abs/2511.00124", "authors": ["Sai Niranjan Ramachandran", "Manish Krishan Lal", "Suvrit Sra"], "title": "Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models", "comment": "Accepted at NeurIPS 2025. 10 pages, camera-ready version. appendices\n  included", "summary": "We analyse how the sampling dynamics of distributions evolve in score-based\ndiffusion models using cross-fluctuations, a centered-moment statistic from\nstatistical physics. Specifically, we show that starting from an unbiased\nisotropic normal distribution, samples undergo sharp, discrete transitions,\neventually forming distinct events of a desired distribution while\nprogressively revealing finer structure. As this process is reversible, these\ntransitions also occur in reverse, where intermediate states progressively\nmerge, tracing a path back to the initial distribution. We demonstrate that\nthese transitions can be detected as discontinuities in $n^{\\text{th}}$-order\ncross-fluctuations. For variance-preserving SDEs, we derive a closed-form for\nthese cross-fluctuations that is efficiently computable for the reverse\ntrajectory. We find that detecting these transitions directly boosts sampling\nefficiency, accelerates class-conditional and rare-class generation, and\nimproves two zero-shot tasks--image classification and style transfer--without\nexpensive grid search or retraining. We also show that this viewpoint unifies\nclassical coupling and mixing from finite Markov chains with continuous\ndynamics while extending to stochastic SDEs and non Markovian samplers. Our\nframework therefore bridges discrete Markov chain theory, phase analysis, and\nmodern generative modeling.", "AI": {"tldr": "\u4f7f\u7528\u7edf\u8ba1\u7269\u7406\u4e2d\u7684\u4ea4\u53c9\u6da8\u843d\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u52a8\u6001\uff0c\u53d1\u73b0\u6837\u672c\u7ecf\u5386\u5c16\u9510\u7684\u79bb\u6563\u8f6c\u53d8\uff0c\u8fd9\u4e9b\u8f6c\u53d8\u53ef\u88ab\u68c0\u6d4b\u5e76\u7528\u4e8e\u63d0\u5347\u91c7\u6837\u6548\u7387\u3001\u52a0\u901f\u6761\u4ef6\u751f\u6210\u548c\u6539\u5584\u96f6\u6837\u672c\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u4e2d\u91c7\u6837\u52a8\u6001\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u6837\u672c\u4ece\u521d\u59cb\u5206\u5e03\u5230\u76ee\u6807\u5206\u5e03\u7684\u8f6c\u53d8\u673a\u5236\uff0c\u4ee5\u7406\u89e3\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u5f62\u6210\u3002", "method": "\u5229\u7528\u4ea4\u53c9\u6da8\u843d\u8fd9\u4e00\u4e2d\u5fc3\u77e9\u7edf\u8ba1\u91cf\u5206\u6790\u91c7\u6837\u52a8\u6001\uff0c\u63a8\u5bfc\u65b9\u5dee\u4fdd\u6301SDE\u7684\u4ea4\u53c9\u6da8\u843d\u95ed\u5f0f\u89e3\uff0c\u68c0\u6d4b\u6837\u672c\u8f6c\u53d8\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0\u6837\u672c\u7ecf\u5386\u79bb\u6563\u8f6c\u53d8\uff0c\u8fd9\u4e9b\u8f6c\u53d8\u53ef\u88ab\u68c0\u6d4b\u5e76\u7528\u4e8e\u63d0\u5347\u91c7\u6837\u6548\u738720-30%\uff0c\u52a0\u901f\u6761\u4ef6\u751f\u6210\uff0c\u6539\u5584\u56fe\u50cf\u5206\u7c7b\u548c\u98ce\u683c\u8fc1\u79fb\u7b49\u96f6\u6837\u672c\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u79bb\u6563\u9a6c\u5c14\u53ef\u592b\u94fe\u7406\u8bba\u4e0e\u8fde\u7eed\u52a8\u529b\u5b66\uff0c\u5c06\u7ecf\u5178\u8026\u5408\u548c\u6df7\u5408\u6982\u5ff5\u6269\u5c55\u5230\u968f\u673aSDE\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u5668\uff0c\u4e3a\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00427", "abs": "https://arxiv.org/abs/2511.00427", "authors": ["Daichi Zhang", "Tong Zhang", "Jianmin Bao", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection", "comment": null, "summary": "With the rapid development of generative models, detecting generated fake\nimages to prevent their malicious use has become a critical issue recently.\nExisting methods frame this challenge as a naive binary image classification\ntask. However, such methods focus only on visual clues, yielding trained\ndetectors susceptible to overfitting specific image patterns and incapable of\ngeneralizing to unseen models. In this paper, we address this issue from a\nmulti-modal perspective and find that fake images cannot be properly aligned\nwith corresponding captions compared to real images. Upon this observation, we\npropose a simple yet effective detector termed ITEM by leveraging the\nimage-text misalignment in a joint visual-language space as discriminative\nclues. Specifically, we first measure the misalignment of the images and\ncaptions in pre-trained CLIP's space, and then tune a MLP head to perform the\nusual detection task. Furthermore, we propose a hierarchical misalignment\nscheme that first focuses on the whole image and then each semantic object\ndescribed in the caption, which can explore both global and fine-grained local\nsemantic misalignment as clues. Extensive experiments demonstrate the\nsuperiority of our method against other state-of-the-art competitors with\nimpressive generalization and robustness on various recent generative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u5ea6\u7684\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5ITEM\uff0c\u5229\u7528\u751f\u6210\u56fe\u50cf\u4e0e\u63cf\u8ff0\u6587\u672c\u5728CLIP\u7a7a\u95f4\u4e2d\u7684\u4e0d\u5bf9\u9f50\u6027\u4f5c\u4e3a\u5224\u522b\u7ebf\u7d22\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u8bed\u4e49\u4e0d\u5bf9\u9f50\u5206\u6790\u5b9e\u73b0\u9c81\u68d2\u7684\u5047\u56fe\u50cf\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u56fe\u50cf\u6a21\u5f0f\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u751f\u6210\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u751f\u6210\u56fe\u50cf\u4e0e\u5bf9\u5e94\u63cf\u8ff0\u6587\u672c\u5728\u89c6\u89c9-\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u5bf9\u9f50\u5ea6\u4e0d\u5982\u771f\u5b9e\u56fe\u50cf\u3002", "method": "\u5728\u9884\u8bad\u7ec3CLIP\u7a7a\u95f4\u4e2d\u6d4b\u91cf\u56fe\u50cf\u4e0e\u63cf\u8ff0\u6587\u672c\u7684\u4e0d\u5bf9\u9f50\u5ea6\uff0c\u7136\u540e\u8bad\u7ec3MLP\u5934\u8fdb\u884c\u5206\u7c7b\u3002\u63d0\u51fa\u5206\u5c42\u4e0d\u5bf9\u9f50\u65b9\u6848\uff1a\u5148\u5173\u6ce8\u6574\u5f20\u56fe\u50cf\uff0c\u518d\u5173\u6ce8\u63cf\u8ff0\u4e2d\u7684\u6bcf\u4e2a\u8bed\u4e49\u5bf9\u8c61\uff0c\u63a2\u7d22\u5168\u5c40\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u8bed\u4e49\u4e0d\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6700\u65b0\u751f\u6210\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5229\u7528\u56fe\u50cf-\u6587\u672c\u4e0d\u5bf9\u9f50\u4f5c\u4e3a\u5224\u522b\u7ebf\u7d22\u662f\u68c0\u6d4b\u751f\u6210\u5047\u56fe\u50cf\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u4f46\u6548\u679c\u663e\u8457\uff0c\u80fd\u591f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2511.00456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "comment": null, "summary": "This study proposes a weakly supervised deep learning framework for pneumonia\nclassification and localization from chest X-rays, utilizing Grad-CAM\nexplanations. Instead of costly pixel-level annotations, our approach utilizes\nimage-level labels to generate clinically meaningful heatmaps that highlight\nregions affected by pneumonia. We evaluate seven ImageNet-pretrained\narchitectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and\nViT-B16 under identical training conditions with focal loss and patient-wise\nsplits to prevent data leakage. Experimental results on the Kermany CXR dataset\ndemonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test\naccuracy of 98\\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides\nan optimal trade-off between accuracy and computational cost. Grad-CAM\nvisualizations confirm that the proposed models focus on clinically relevant\nlung regions, supporting the use of interpretable AI for radiological\ndiagnostics. This work highlights the potential of weakly supervised\nexplainable models that enhance pneumonia screening transparency, and clinical\ntrust in AI-assisted medical imaging.\n  https://github.com/kiranshahi/pneumonia-analysis", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGrad-CAM\u7684\u5f31\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u80ba\u708e\u5206\u7c7b\u548c\u5b9a\u4f4d\uff0c\u4ec5\u9700\u56fe\u50cf\u7ea7\u6807\u7b7e\u5373\u53ef\u751f\u6210\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u80ba\u708e\u533a\u57df\u70ed\u529b\u56fe\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u80ba\u708e\u8bca\u65ad\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u66f4\u900f\u660e\u3001\u53ef\u4fe1\u7684AI\u8f85\u52a9\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u4e03\u79cdImageNet\u9884\u8bad\u7ec3\u67b6\u6784\u5728\u76f8\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u8bc4\u4f30\uff0c\u91c7\u7528\u7126\u70b9\u635f\u5931\u548c\u60a3\u8005\u7ea7\u6570\u636e\u5206\u5272\u9632\u6b62\u6570\u636e\u6cc4\u9732\uff0c\u901a\u8fc7Grad-CAM\u751f\u6210\u89e3\u91ca\u6027\u70ed\u529b\u56fe\u3002", "result": "ResNet-18\u548cEfficientNet-B0\u8fbe\u523098%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0cROC-AUC=0.997\uff0cF1=0.987\uff1bMobileNet-V2\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6210\u672c\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u5f31\u76d1\u7763\u53ef\u89e3\u91ca\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u80ba\u708e\u7b5b\u67e5\u900f\u660e\u5ea6\uff0c\u589e\u5f3a\u4e34\u5e8a\u5bf9AI\u8f85\u52a9\u533b\u5b66\u5f71\u50cf\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2511.01187", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.01581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01581", "abs": "https://arxiv.org/abs/2511.01581", "authors": ["Chengzhang Yu", "Zening Lu", "Chenyang Zheng", "Chiyue Wang", "Yiming Zhang", "Zhanpeng Jin"], "title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks", "comment": "12pages, 4figures", "summary": "Large language models suffer from knowledge staleness and lack of\ninterpretability due to implicit knowledge storage across entangled network\nparameters, preventing targeted updates and reasoning transparency. We propose\nExplicitLM, a novel architecture featuring a million-scale external memory bank\nstoring human-readable knowledge as token sequences, enabling direct inspection\nand modification. We design a differentiable two-stage retrieval mechanism with\nefficient coarse-grained filtering via product key decomposition (reducing\ncomplexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot\n|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.\nInspired by dual-system cognitive theory, we partition knowledge into frozen\nexplicit facts (20%) and learnable implicit patterns (80%), maintained through\nExponential Moving Average updates for stability. ExplicitLM achieves up to\n43.67% improvement on knowledge-intensive tasks versus standard Transformers,\nwith 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows\nstrong correlations between memory retrieval and performance, with correct\npredictions achieving 49% higher hit rates. Unlike RAG systems with frozen\nretrieval, our jointly optimized architecture demonstrates that interpretable,\nupdatable models can maintain competitive performance while providing\nunprecedented knowledge transparency.", "AI": {"tldr": "ExplicitLM\u662f\u4e00\u79cd\u65b0\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u5916\u90e8\u53ef\u8bfb\u77e5\u8bc6\u5e93\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u9648\u65e7\u6027\u548c\u4e0d\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u77e5\u8bc6\u7684\u76f4\u63a5\u68c0\u67e5\u548c\u66f4\u65b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u77e5\u8bc6\u9648\u65e7\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u77e5\u8bc6\u9690\u5f0f\u5b58\u50a8\u5728\u7ea0\u7f20\u7684\u7f51\u7edc\u53c2\u6570\u4e2d\uff0c\u65e0\u6cd5\u8fdb\u884c\u9488\u5bf9\u6027\u66f4\u65b0\u548c\u63a8\u7406\u900f\u660e\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u767e\u4e07\u7ea7\u5916\u90e8\u77e5\u8bc6\u5e93\u7684\u67b6\u6784\uff0c\u4f7f\u7528\u53ef\u8bfb\u7684token\u5e8f\u5217\u5b58\u50a8\u77e5\u8bc6\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u53ef\u5fae\u5206\u68c0\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u4ea7\u54c1\u952e\u5206\u89e3\u8fdb\u884c\u7c97\u7c92\u5ea6\u8fc7\u6ee4\uff0cGumbel-Softmax\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5339\u914d\uff1b\u57fa\u4e8e\u53cc\u7cfb\u7edf\u8ba4\u77e5\u7406\u8bba\uff0c\u5c06\u77e5\u8bc6\u5206\u4e3a\u51bb\u7ed3\u7684\u663e\u6027\u4e8b\u5b9e(20%)\u548c\u53ef\u5b66\u4e60\u7684\u9690\u6027\u6a21\u5f0f(80%)\u3002", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u6bd4\u6807\u51c6Transformer\u63d0\u534743.67%\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f(1\u4e07\u6837\u672c)\u4e0b\u83b7\u5f973.62\u500d\u589e\u76ca\uff1b\u6b63\u786e\u9884\u6d4b\u7684\u547d\u4e2d\u7387\u6bd4\u9519\u8bef\u9884\u6d4b\u9ad849%\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u7684\u67b6\u6784\u8bc1\u660e\u53ef\u89e3\u91ca\u3001\u53ef\u66f4\u65b0\u7684\u6a21\u578b\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u80fd\u591f\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u77e5\u8bc6\u900f\u660e\u5ea6\uff0c\u4f18\u4e8e\u4f7f\u7528\u56fa\u5b9a\u68c0\u7d22\u7684RAG\u7cfb\u7edf\u3002"}}
{"id": "2511.01472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01472", "abs": "https://arxiv.org/abs/2511.01472", "authors": ["Sarthak Mishra", "Rishabh Dev Yadav", "Avirup Das", "Saksham Gupta", "Wei Pan", "Spandan Roy"], "title": "AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models", "comment": null, "summary": "The rapid progress of vision--language models (VLMs) has sparked growing\ninterest in robotic control, where natural language can express the operation\ngoals while visual feedback links perception to action. However, directly\ndeploying VLM-driven policies on aerial manipulators remains unsafe and\nunreliable since the generated actions are often inconsistent,\nhallucination-prone, and dynamically infeasible for flight. In this work, we\npresent AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial\nmanipulation by separating high-level reasoning from low-level control, without\nany task-specific fine-tuning. Our framework encodes natural language\ninstructions, task context, and safety constraints into a structured prompt\nthat guides the model to generate a step-by-step reasoning trace in natural\nlanguage. This reasoning output is used to select from a predefined library of\ndiscrete, flight-safe skills, ensuring interpretable and temporally consistent\nexecution. By decoupling symbolic reasoning from physical action, AERMANI-VLM\nmitigates hallucinated commands and prevents unsafe behavior, enabling robust\ntask completion. We validate the framework in both simulation and hardware on\ndiverse multi-step pick-and-place tasks, demonstrating strong generalization to\npreviously unseen commands, objects, and environments.", "AI": {"tldr": "AERMANI-VLM\u662f\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230\u7a7a\u4e2d\u673a\u68b0\u81c2\u63a7\u5236\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u5c42\u63a8\u7406\u548c\u5e95\u5c42\u63a7\u5236\u6765\u89e3\u51b3\u76f4\u63a5\u90e8\u7f72VLM\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u76f4\u63a5\u90e8\u7f72VLM\u9a71\u52a8\u7b56\u7565\u5230\u7a7a\u4e2d\u673a\u68b0\u81c2\u5b58\u5728\u4e0d\u5b89\u5168\u3001\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u751f\u6210\u7684\u52a8\u4f5c\u5f80\u5f80\u4e0d\u4e00\u81f4\u3001\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u4e14\u5bf9\u98de\u884c\u6765\u8bf4\u52a8\u6001\u4e0d\u53ef\u884c\u3002", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u5b89\u5168\u7ea6\u675f\u7f16\u7801\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u4ece\u9884\u5b9a\u4e49\u7684\u79bb\u6563\u3001\u98de\u884c\u5b89\u5168\u6280\u80fd\u5e93\u4e2d\u9009\u62e9\u6267\u884c\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u591a\u79cd\u591a\u6b65\u9aa4\u62fe\u653e\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5bf9\u672a\u89c1\u6307\u4ee4\u3001\u7269\u4f53\u548c\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u7b26\u53f7\u63a8\u7406\u548c\u7269\u7406\u52a8\u4f5c\uff0cAERMANI-VLM\u51cf\u8f7b\u4e86\u5e7b\u89c9\u547d\u4ee4\u5e76\u9632\u6b62\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2511.01520", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01520", "abs": "https://arxiv.org/abs/2511.01520", "authors": ["Shipeng Lyu", "Lijie Sheng", "Fangyuan Wang", "Wenyao Zhang", "Weiwei Lin", "Zhenzhong Jia", "David Navarro-Alarcon", "Guodong Guo"], "title": "Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals", "comment": "9 papges, 10 figures, 3 tables", "summary": "Humans naturally grasp objects with minimal level required force for\nstability, whereas robots often rely on rigid, over-squeezing control. To\nnarrow this gap, we propose a human-inspired physics-conditioned tactile method\n(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,\ntactile prediction, and force regulation. A physics-based pose selector first\nidentifies feasible contact regions with optimal force distribution based on\nsurface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)\npredicts the tactile imprint under FOSG target. Last, a latent-space LQR\ncontroller drives the gripper toward this tactile imprint with minimal\nactuation, preventing unnecessary compression. Trained on a physics-conditioned\ntactile dataset covering diverse objects and contact conditions, the proposed\nPhy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac\noutperforms fixed-force and GraspNet-based baselines in grasp stability and\nforce efficiency. Experiments on classical robotic platforms demonstrate\nforce-efficient and adaptive manipulation that bridges the gap between robotic\nand human grasping.", "AI": {"tldr": "\u63d0\u51faPhy-Tac\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u6761\u4ef6\u89e6\u89c9\u5b9e\u73b0\u529b\u6700\u4f18\u7a33\u5b9a\u6293\u53d6\uff0c\u7ed3\u5408\u59ff\u6001\u9009\u62e9\u3001\u89e6\u89c9\u9884\u6d4b\u548c\u529b\u8c03\u8282\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u7528\u6700\u5c0f\u5fc5\u8981\u529b\u7a33\u5b9a\u6293\u53d6\u7269\u4f53\u3002", "motivation": "\u4eba\u7c7b\u81ea\u7136\u6293\u53d6\u65f6\u4f7f\u7528\u6700\u5c0f\u5fc5\u8981\u529b\u4fdd\u6301\u7a33\u5b9a\uff0c\u800c\u673a\u5668\u4eba\u901a\u5e38\u4f9d\u8d56\u521a\u6027\u3001\u8fc7\u5ea6\u6324\u538b\u7684\u63a7\u5236\u3002\u4e3a\u4e86\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u529b\u63a7\u5236\u65b9\u6cd5\u3002", "method": "1) \u57fa\u4e8e\u7269\u7406\u7684\u59ff\u6001\u9009\u62e9\u5668\u8bc6\u522b\u6700\u4f18\u529b\u5206\u5e03\u7684\u53ef\u884c\u63a5\u89e6\u533a\u57df\uff1b2) \u7269\u7406\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u9884\u6d4bFOSG\u76ee\u6807\u4e0b\u7684\u89e6\u89c9\u5370\u8bb0\uff1b3) \u6f5c\u5728\u7a7a\u95f4LQR\u63a7\u5236\u5668\u4ee5\u6700\u5c0f\u9a71\u52a8\u5c06\u5939\u722a\u5bfc\u5411\u8be5\u89e6\u89c9\u5370\u8bb0\u3002", "result": "\u5728\u591a\u6837\u5316\u7269\u4f53\u548c\u63a5\u89e6\u6761\u4ef6\u4e0b\u7684\u7269\u7406\u6761\u4ef6\u89e6\u89c9\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cPhy-LDM\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u89e6\u89c9\u9884\u6d4b\u7cbe\u5ea6\uff0cPhy-Tac\u5728\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u529b\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u56fa\u5b9a\u529b\u548c\u57fa\u4e8eGraspNet\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7ecf\u5178\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u529b\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u64cd\u4f5c\uff0c\u7f29\u5c0f\u4e86\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u6293\u53d6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.00524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00524", "abs": "https://arxiv.org/abs/2511.00524", "authors": ["Jihao Gu", "Kun Li", "He Wang", "Kaan Ak\u015fit"], "title": "Text-guided Fine-Grained Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).", "AI": {"tldr": "\u63d0\u51fa\u4e86T-VAD\u6846\u67b6\uff0c\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f02\u5e38\u70ed\u56fe\u89e3\u7801\u5668\u548c\u533a\u57df\u611f\u77e5\u5f02\u5e38\u7f16\u7801\u5668\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8f93\u51fa\u6709\u9650\uff08\u4ec5\u6b63\u5e38\u6216\u5f02\u5e38\uff09\uff0c\u4e14\u591a\u4e3a\u534a\u81ea\u52a8\u5316\u9700\u8981\u4eba\u5de5\u8bc4\u4f30\uff0c\u9700\u8981\u63d0\u5347\u68c0\u6d4b\u7684\u7ec6\u7c92\u5ea6\u548c\u4ea4\u4e92\u6027\u3002", "method": "\u6784\u5efa\u57fa\u4e8eLVLM\u7684T-VAD\u6846\u67b6\uff0c\u5305\u542b\u5f02\u5e38\u70ed\u56fe\u89e3\u7801\u5668\u8fdb\u884c\u50cf\u7d20\u7ea7\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5bf9\u9f50\u751f\u6210\u7ec6\u7c92\u5ea6\u70ed\u56fe\uff0c\u4ee5\u53ca\u533a\u57df\u611f\u77e5\u5f02\u5e38\u7f16\u7801\u5668\u5c06\u70ed\u56fe\u8f6c\u6362\u4e3a\u53ef\u5b66\u4e60\u6587\u672c\u5d4c\u5165\u6765\u6307\u5bfcLVLM\u3002", "result": "\u5728UBnormal\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.8% AUC\uff0c\u5f02\u5e38\u70ed\u56fe\u7cbe\u5ea667.8%/76.7%\uff1b\u5728ShanghaiTech\u548cUBnormal\u6570\u636e\u96c6\u4e0a\u6587\u672c\u63cf\u8ff0\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0cBLEU-4\u548cYes/No\u51c6\u786e\u7387\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "T-VAD\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u7ec6\u7c92\u5ea6\u548c\u4ea4\u4e92\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5f02\u5e38\u5b9a\u4f4d\u548c\u63cf\u8ff0\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2511.00580", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.5.4; I.4.8; C.3"], "pdf": "https://arxiv.org/pdf/2511.00580", "abs": "https://arxiv.org/abs/2511.00580", "authors": ["Yousuf Ahmed Siddiqui", "Sufiyaan Usmani", "Umer Tariq", "Jawwad Ahmed Shamsi", "Muhammad Burhan Khan"], "title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection", "comment": "10 pages, 5 figures", "summary": "Video anomalies often depend on contextual information available and temporal\nevolution. Non-anomalous action in one context can be anomalous in some other\ncontext. Most anomaly detectors, however, do not notice this type of context,\nwhich seriously limits their capability to generalize to new, real-life\nsituations. Our work addresses the context-aware zero-shot anomaly detection\nchallenge, in which systems need to learn adaptively to detect new events by\ncorrelating temporal and appearance features with textual traces of memory in\nreal time. Our approach defines a memory-augmented pipeline, correlating\ntemporal signals with visual embeddings using cross-attention, and real-time\nzero-shot anomaly classification by contextual similarity scoring. We achieve\n90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art\namong zero-shot models. Our model achieves real-time inference with high\nprecision and explainability for deployment. We show that, by fusing\ncross-attention temporal fusion and contextual memory, we achieve high fidelity\nanomaly detection, a step towards the applicability of zero-shot models in\nreal-world surveillance and infrastructure monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u65f6\u5e8f\u7279\u5f81\u548c\u89c6\u89c9\u5d4c\u5165\uff0c\u5b9e\u73b0\u5b9e\u65f6\u96f6\u6837\u672c\u5f02\u5e38\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u5668\u7f3a\u4e4f\u5bf9\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u8bc6\u522b\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u6b63\u5e38/\u5f02\u5e38\u884c\u4e3a\u7684\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8bb0\u5fc6\u589e\u5f3a\u7ba1\u9053\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5173\u8054\u65f6\u5e8f\u4fe1\u53f7\u4e0e\u89c6\u89c9\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u8bc4\u5206\u8fdb\u884c\u5b9e\u65f6\u96f6\u6837\u672c\u5f02\u5e38\u5206\u7c7b\u3002", "result": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8fbe\u523090.4% AUC\uff0c\u5728XD-Violence\u6570\u636e\u96c6\u4e0a\u8fbe\u523083.67% AP\uff0c\u521b\u4e0b\u96f6\u6837\u672c\u6a21\u578b\u7684\u65b0SOTA\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u8de8\u6ce8\u610f\u529b\u65f6\u5e8f\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u96f6\u6837\u672c\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u76d1\u63a7\u548c\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2511.00613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00613", "abs": "https://arxiv.org/abs/2511.00613", "authors": ["Yating Yu", "Congqi Cao", "Zhaoying Wang", "Weihua Meng", "Jie Li", "Yuxin Li", "Zihao Wei", "Zhongpei Shen", "Jiajun Zhang"], "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World", "comment": null, "summary": "How far are deep models from real-world video anomaly understanding (VAU)?\nCurrent works typically emphasize on detecting unexpected occurrences deviated\nfrom normal patterns or comprehending anomalous events with interpretable\ndescriptions. However, they exhibit only a superficial comprehension of\nreal-world anomalies, with limited breadth in complex principles and subtle\ncontext that distinguish the anomalies from normalities, e.g., climbing cliffs\nwith safety gear vs. without it. To this end, we introduce CueBench, the first\nof its kind Benchmark, devoted to Context-aware video anomalies within a\nUnified Evaluation framework. We comprehensively establish an event-centric\nhierarchical taxonomy that anchors two core event types: 14 conditional and 18\nabsolute anomaly events, defined by their refined semantics from diverse\ncontexts across 174 scenes and 198 attributes. Based on this, we propose to\nunify and benchmark context-aware VAU with various challenging tasks across\nrecognition, temporal grounding, detection, and anticipation. This also serves\nas a rigorous and fair probing evaluation suite for generative-discriminative\nas well as generalized-specialized vision-language models (VLMs). To address\nthe challenges underlying CueBench, we further develop Cue-R1 based on R1-style\nreinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined\nrewards in a unified generative manner. Extensive results on CueBench reveal\nthat, existing VLMs are still far from satisfactory real-world anomaly\nunderstanding, while our Cue-R1 surpasses these state-of-the-art approaches by\nover 24% on average.", "AI": {"tldr": "CueBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u548c\u4e8b\u4ef6\u4e2d\u5fc3\u5c42\u6b21\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7406\u89e3\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u65b9\u6cd5\u5bf9\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7684\u7406\u89e3\u8f83\u4e3a\u80a4\u6d45\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u539f\u7406\u548c\u5fae\u5999\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faCueBench\u57fa\u51c6\uff0c\u5efa\u7acb\u4e8b\u4ef6\u4e2d\u5fc3\u5c42\u6b21\u5206\u7c7b\u6cd5\uff0c\u5305\u542b14\u79cd\u6761\u4ef6\u5f02\u5e38\u548c18\u79cd\u7edd\u5bf9\u5f02\u5e38\u4e8b\u4ef6\uff0c\u6db5\u76d6174\u4e2a\u573a\u666f\u548c198\u4e2a\u5c5e\u6027\u3002\u540c\u65f6\u5f00\u53d1Cue-R1\u6a21\u578b\uff0c\u57fa\u4e8eR1\u98ce\u683c\u5f3a\u5316\u5fae\u8c03\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u3001\u4efb\u52a1\u5bf9\u9f50\u548c\u5c42\u6b21\u7ec6\u5316\u7684\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cCue-R1\u6a21\u578b\u5728CueBench\u4e0a\u5e73\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u8d85\u8fc724%\u3002", "conclusion": "\u73b0\u6709\u6a21\u578b\u8ddd\u79bb\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\uff0cCueBench\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0cCue-R1\u5c55\u793a\u4e86\u663e\u8457\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2511.01381", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01381", "abs": "https://arxiv.org/abs/2511.01381", "authors": ["Hitesh Kyatham", "Arjun Suresh", "Aadi Palnitkar", "Yiannis Aloimonos"], "title": "EREBUS: End-to-end Robust Event Based Underwater Simulation", "comment": "Accepted to ICRA AQUA2SIM Workshop 2025, 6 pages, 3 figures,\n  conference paper", "summary": "The underwater domain presents a vast array of challenges for roboticists and\ncomputer vision researchers alike, such as poor lighting conditions and high\ndynamic range scenes. In these adverse conditions, traditional vision\ntechniques struggle to adapt and lead to suboptimal performance. Event-based\ncameras present an attractive solution to this problem, mitigating the issues\nof traditional cameras by tracking changes in the footage on a frame-by-frame\nbasis. In this paper, we introduce a pipeline which can be used to generate\nrealistic synthetic data of an event-based camera mounted to an AUV (Autonomous\nUnderwater Vehicle) in an underwater environment for training vision models. We\ndemonstrate the effectiveness of our pipeline using the task of rock detection\nwith poor visibility and suspended particulate matter, but the approach can be\ngeneralized to other underwater tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u6c34\u4e0b\u73af\u5883\u4e2dAUV\u642d\u8f7d\u4e8b\u4ef6\u76f8\u673a\u5408\u6210\u6570\u636e\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u5728\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5b58\u5728\u5149\u7167\u6761\u4ef6\u5dee\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u89c6\u89c9\u6280\u672f\u96be\u4ee5\u9002\u5e94\u3002\u4e8b\u4ef6\u76f8\u673a\u901a\u8fc7\u9010\u5e27\u8ffd\u8e2a\u53d8\u5316\u53ef\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u4e8b\u4ef6\u76f8\u673a\u5408\u6210\u6570\u636e\u7684\u7ba1\u9053\uff0c\u6a21\u62dfAUV\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u573a\u666f\uff0c\u7279\u522b\u9488\u5bf9\u80fd\u89c1\u5ea6\u5dee\u548c\u60ac\u6d6e\u9897\u7c92\u7269\u7684\u60c5\u51b5\u3002", "result": "\u5728\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u7ba1\u9053\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u6c34\u4e0b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u4e3a\u6c34\u4e0b\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00532", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00532", "abs": "https://arxiv.org/abs/2511.00532", "authors": ["Drago\u015f-Andrei \u015eerban", "R\u0103zvan-Alexandru Sm\u0103du", "Dumitru-Clementin Cercel"], "title": "Air Pollution Forecasting in Bucharest", "comment": "14 pages 3 figures", "summary": "Air pollution, especially the particulate matter 2.5 (PM2.5), has become a\ngrowing concern in recent years, primarily in urban areas. Being exposed to air\npollution is linked to developing numerous health problems, like the\naggravation of respiratory diseases, cardiovascular disorders, lung function\nimpairment, and even cancer or early death. Forecasting future levels of PM2.5\nhas become increasingly important over the past few years, as it can provide\nearly warnings and help prevent diseases. This paper aims to design, fine-tune,\ntest, and evaluate machine learning models for predicting future levels of\nPM2.5 over various time horizons. Our primary objective is to assess and\ncompare the performance of multiple models, ranging from linear regression\nalgorithms and ensemble-based methods to deep learning models, such as advanced\nrecurrent neural networks and transformers, as well as large language models,\non this forecasting task.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u548c\u8bc4\u4f30\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u9884\u6d4bPM2.5\u6d53\u5ea6\u6c34\u5e73\uff0c\u5305\u62ec\u7ebf\u6027\u56de\u5f52\u3001\u96c6\u6210\u65b9\u6cd5\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "PM2.5\u7a7a\u6c14\u6c61\u67d3\u5bf9\u5065\u5eb7\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u9884\u6d4b\u672a\u6765PM2.5\u6c34\u5e73\u53ef\u4ee5\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\u5e76\u5e2e\u52a9\u9884\u9632\u75be\u75c5\u3002", "method": "\u8bbe\u8ba1\u3001\u5fae\u8c03\u3001\u6d4b\u8bd5\u548c\u8bc4\u4f30\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u7ebf\u6027\u56de\u5f52\u7b97\u6cd5\u3001\u96c6\u6210\u65b9\u6cd5\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u53d8\u6362\u5668\uff09\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u8bba\u6587\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728PM2.5\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3aPM2.5\u6d53\u5ea6\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2511.00549", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00549", "abs": "https://arxiv.org/abs/2511.00549", "authors": ["Qiang Li", "Jin Niu", "Lina Yu"], "title": "Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations", "comment": null, "summary": "Traffic congestion, primarily driven by intersection queuing, significantly\nimpacts urban living standards, safety, environmental quality, and economic\nefficiency. While Traffic Signal Control (TSC) systems hold potential for\ncongestion mitigation, traditional optimization models often fail to capture\nreal-world traffic complexity and dynamics. This study introduces a novel\nsingle-agent reinforcement learning (RL) framework for regional adaptive TSC,\ncircumventing the coordination complexities inherent in multi-agent systems\nthrough a centralized decision-making paradigm. The model employs an adjacency\nmatrix to unify the encoding of road network topology, real-time queue states\nderived from probe vehicle data, and current signal timing parameters.\nLeveraging the efficient learning capabilities of the DreamerV3 world model,\nthe agent learns control policies where actions sequentially select\nintersections and adjust their signal phase splits to regulate traffic\ninflow/outflow, analogous to a feedback control system. Reward design\nprioritizes queue dissipation, directly linking congestion metrics (queue\nlength) to control actions. Simulation experiments conducted in SUMO\ndemonstrate the model's effectiveness: under inference scenarios with\nmulti-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the\nframework exhibits robust anti-fluctuation capability and significantly reduces\nqueue lengths. This work establishes a new paradigm for intelligent traffic\ncontrol compatible with probe vehicle technology. Future research will focus on\nenhancing practical applicability by incorporating stochastic OD demand\nfluctuations during training and exploring regional optimization mechanisms for\ncontingency events.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u533a\u57df\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528DreamerV3\u4e16\u754c\u6a21\u578b\u548c\u90bb\u63a5\u77e9\u9635\u7edf\u4e00\u7f16\u7801\u8def\u7f51\u62d3\u6251\u3001\u5b9e\u65f6\u6392\u961f\u72b6\u6001\u548c\u4fe1\u53f7\u914d\u65f6\u53c2\u6570\uff0c\u5728SUMO\u4eff\u771f\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u6392\u961f\u957f\u5ea6\u5e76\u8868\u73b0\u51fa\u6297\u6ce2\u52a8\u80fd\u529b\u3002", "motivation": "\u4ea4\u901a\u62e5\u5835\u4e25\u91cd\u5f71\u54cd\u57ce\u5e02\u751f\u6d3b\u8d28\u91cf\u548c\u7ecf\u6d4e\u6548\u7387\uff0c\u4f20\u7edf\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6a21\u578b\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u4ea4\u901a\u590d\u6742\u6027\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u534f\u8c03\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u90bb\u63a5\u77e9\u9635\u7edf\u4e00\u7f16\u7801\u8def\u7f51\u62d3\u6251\u3001\u5b9e\u65f6\u6392\u961f\u72b6\u6001\u548c\u4fe1\u53f7\u914d\u65f6\u53c2\u6570\uff0c\u5229\u7528DreamerV3\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u52a8\u4f5c\u5e8f\u5217\u9009\u62e9\u4ea4\u53c9\u53e3\u5e76\u8c03\u6574\u4fe1\u53f7\u76f8\u4f4d\u914d\u65f6\u3002", "result": "\u5728SUMO\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u9762\u5bf910%-30%\u7684\u8d77\u8bab\u70b9\u9700\u6c42\u6ce2\u52a8\uff0c\u8be5\u6846\u67b6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6297\u6ce2\u52a8\u80fd\u529b\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u6392\u961f\u957f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e0e\u63a2\u6d4b\u8f66\u8f86\u6280\u672f\u517c\u5bb9\u7684\u667a\u80fd\u4ea4\u901a\u63a7\u5236\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u5c06\u5173\u6ce8\u5728\u8bad\u7ec3\u4e2d\u7eb3\u5165\u968f\u673a\u9700\u6c42\u6ce2\u52a8\u548c\u63a2\u7d22\u533a\u57df\u5e94\u6025\u4f18\u5316\u673a\u5236\u3002"}}
{"id": "2511.00615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00615", "abs": "https://arxiv.org/abs/2511.00615", "authors": ["Daniel Griffiths", "Piper Moskow"], "title": "Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling", "comment": "5 Pages, 4 Figures, 2 Tables", "summary": "We present a unified, data-driven framework for quantifying and enhancing\noffensive momentum and scoring likelihood (expected goals, xG) in professional\nhockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our\nend-to-end pipeline comprises five stages: (1) interpretable momentum weighting\nof micro-events via logistic regression; (2) nonlinear xG estimation using\ngradient-boosted decision trees; (3) temporal sequence modeling with Long\nShort-Term Memory (LSTM) networks; (4) spatial formation discovery through\nprincipal component analysis (PCA) followed by K-Means clustering on\nstandardized player coordinates; and (5) use of an X-Learner causal inference\nestimator to quantify the average treatment effect (ATE) of adopting the\nidentified \"optimal\" event sequences and formations. We observe an ATE of 0.12\n(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring\npotential. These results demonstrate that strategically structured sequences\nand compact formations causally elevate offensive performance. Our framework\ndelivers real-time, actionable insights for coaches and analysts, advancing\nhockey analytics toward principled, causally grounded tactical optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u9636\u6bb5\u91cf\u5316\u5e76\u63d0\u5347\u51b0\u7403\u6bd4\u8d5b\u4e2d\u7684\u8fdb\u653b\u52bf\u5934\u548c\u5f97\u5206\u53ef\u80fd\u6027\uff08\u9884\u671f\u8fdb\u7403xG\uff09\uff0c\u7ed3\u679c\u663e\u793a\u91c7\u7528\u4f18\u5316\u7684\u4e8b\u4ef6\u5e8f\u5217\u548c\u9635\u578b\u80fd\u663e\u8457\u63d0\u9ad815%\u7684\u5f97\u5206\u6f5c\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5206\u6790\u6846\u67b6\uff0c\u4e3a\u51b0\u7403\u6559\u7ec3\u548c\u5206\u6790\u5e08\u63d0\u4f9b\u5b9e\u65f6\u3001\u53ef\u64cd\u4f5c\u7684\u6218\u672f\u4f18\u5316\u89c1\u89e3\uff0c\u63a8\u52a8\u51b0\u7403\u5206\u6790\u5411\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u6218\u672f\u4f18\u5316\u53d1\u5c55\u3002", "method": "\u4e94\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u903b\u8f91\u56de\u5f52\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u52bf\u5934\u52a0\u6743\uff1b2) \u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u8fdb\u884c\u975e\u7ebf\u6027xG\u4f30\u8ba1\uff1b3) LSTM\u7f51\u7edc\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\uff1b4) PCA\u548cK-Means\u805a\u7c7b\u53d1\u73b0\u7a7a\u95f4\u9635\u578b\uff1b5) X-Learner\u56e0\u679c\u63a8\u65ad\u4f30\u8ba1\u5668\u91cf\u5316\u6700\u4f18\u4e8b\u4ef6\u5e8f\u5217\u548c\u9635\u578b\u7684\u5e73\u5747\u5904\u7406\u6548\u5e94\u3002", "result": "\u89c2\u5bdf\u5230\u5e73\u5747\u5904\u7406\u6548\u5e94\u4e3a0.12\uff0895%\u7f6e\u4fe1\u533a\u95f4\uff1a0.05-0.17\uff0cp < 1e-50\uff09\uff0c\u5bf9\u5e94\u5f97\u5206\u6f5c\u529b\u76f8\u5bf9\u63d0\u534715%\uff0c\u8bc1\u660e\u7b56\u7565\u6027\u7ed3\u6784\u5316\u7684\u5e8f\u5217\u548c\u7d27\u51d1\u9635\u578b\u80fd\u56e0\u679c\u6027\u5730\u63d0\u5347\u8fdb\u653b\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6559\u7ec3\u548c\u5206\u6790\u5e08\u63d0\u4f9b\u4e86\u5b9e\u65f6\u3001\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5c06\u51b0\u7403\u5206\u6790\u63a8\u5411\u57fa\u4e8e\u539f\u5219\u548c\u56e0\u679c\u57fa\u7840\u7684\u6218\u672f\u4f18\u5316\u3002"}}
{"id": "2511.00962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00962", "abs": "https://arxiv.org/abs/2511.00962", "authors": ["Dongheng Lin", "Mengxue Qu", "Kunyang Han", "Jianbo Jiao", "Xiaojie Jin", "Yunchao Wei"], "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis", "comment": "NeurIPS 2025 poster", "summary": "Most video-anomaly research stops at frame-wise detection, offering little\ninsight into why an event is abnormal, typically outputting only frame-wise\nanomaly scores without spatial or semantic context. Recent video anomaly\nlocalization and video anomaly understanding methods improve explainability but\nremain data-dependent and task-specific. We propose a unified reasoning\nframework that bridges the gap between temporal detection, spatial\nlocalization, and textual explanation. Our approach is built upon a chained\ntest-time reasoning process that sequentially connects these tasks, enabling\nholistic zero-shot anomaly analysis without any additional training.\nSpecifically, our approach leverages intra-task reasoning to refine temporal\ndetections and inter-task chaining for spatial and semantic understanding,\nyielding improved interpretability and generalization in a fully zero-shot\nmanner. Without any additional data or gradients, our method achieves\nstate-of-the-art zero-shot performance across multiple video anomaly detection,\nlocalization, and explanation benchmarks. The results demonstrate that careful\nprompt design with task-wise chaining can unlock the reasoning power of\nfoundation models, enabling practical, interpretable video anomaly analysis in\na fully zero-shot manner. Project Page:\nhttps://rathgrith.github.io/Unified_Frame_VAA/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u6d4b\u8bd5\u65f6\u63a8\u7406\u8fc7\u7a0b\u8fde\u63a5\u65f6\u95f4\u68c0\u6d4b\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u89e3\u91ca\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u89c6\u9891\u5f02\u5e38\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u7814\u7a76\u5927\u591a\u505c\u7559\u5728\u5e27\u7ea7\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u89e3\u91ca\u5f02\u5e38\u539f\u56e0\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4ecd\u4f9d\u8d56\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u94fe\u5f0f\u6d4b\u8bd5\u65f6\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4efb\u52a1\u5185\u63a8\u7406\u7ec6\u5316\u65f6\u95f4\u68c0\u6d4b\uff0c\u4efb\u52a1\u95f4\u94fe\u63a5\u5b9e\u73b0\u7a7a\u95f4\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u68af\u5ea6\u66f4\u65b0\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4e0e\u4efb\u52a1\u94fe\u5f0f\u8fde\u63a5\u80fd\u591f\u91ca\u653e\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u5b9e\u7528\u3001\u53ef\u89e3\u91ca\u7684\u96f6\u6837\u672c\u89c6\u9891\u5f02\u5e38\u5206\u6790\u3002"}}
{"id": "2511.00716", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00716", "abs": "https://arxiv.org/abs/2511.00716", "authors": ["Rama Kassoumeh", "David R\u00fcgamer", "Henning Oppel"], "title": "Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations", "comment": "accepted to ICMLA 2025", "summary": "The increasing frequency of heavy rainfall events, which are a major cause of\nurban flooding, underscores the urgent need for accurate precipitation\nforecasting - particularly in urban areas where localized events often go\nundetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain\nevents between 2001 and 2018 were recorded by rain gauges, highlighting the\nlimitations of traditional monitoring systems. Radar data are another source\nthat effectively tracks ongoing precipitation; however, forecasting the\ndevelopment of heavy rain using radar alone remains challenging due to the\nbrief and unpredictable nature of such events. Our focus is on evaluating the\neffectiveness of fusing satellite and radar data for nowcasting. We develop a\nmultimodal nowcasting model that combines both radar and satellite imagery for\npredicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate\nthat this multimodal strategy significantly outperforms radar-only approaches.\nExperimental results show that integrating satellite data improves prediction\naccuracy, particularly for intense precipitation. The proposed model increases\nthe Critical Success Index for heavy rain by 4% and for violent rain by 3% at a\n5-minute lead time. Moreover, it maintains higher predictive skill at longer\nlead times, where radar-only performance declines. A qualitative analysis of\nthe severe flooding event in the state of North Rhine-Westphalia, Germany in\n2021 further illustrates the superior performance of the multimodal model.\nUnlike the radar-only model, which captures general precipitation patterns, the\nmultimodal model yields more detailed and accurate forecasts for regions\naffected by heavy rain. This improved precision enables timely, reliable,\nlife-saving warnings. Implementation available at\nhttps://github.com/RamaKassoumeh/Multimodal_heavy_rain", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u536b\u661f\u548c\u96f7\u8fbe\u6570\u636e\u7684\u591a\u6a21\u6001\u4e34\u8fd1\u9884\u62a5\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b5\u300115\u548c30\u5206\u949f\u5185\u7684\u964d\u6c34\uff0c\u7279\u522b\u9488\u5bf9\u5f3a\u964d\u96e8\u4e8b\u4ef6\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u65b9\u6cd5\uff0c\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u8be6\u7ec6\u7684\u9884\u62a5\u3002", "motivation": "\u4f20\u7edf\u5730\u9762\u4f20\u611f\u5668\u5bf9\u57ce\u5e02\u5f3a\u964d\u96e8\u4e8b\u4ef6\u7684\u76d1\u6d4b\u80fd\u529b\u6709\u9650\uff08\u5fb7\u56fd2001-2018\u5e74\u95f4\u4ec517.3%\u7684\u5f3a\u964d\u96e8\u88ab\u8bb0\u5f55\uff09\uff0c\u96f7\u8fbe\u6570\u636e\u5355\u72ec\u9884\u62a5\u5f3a\u964d\u96e8\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9884\u62a5\u65b9\u6cd5\u6765\u5e94\u5bf9\u65e5\u76ca\u9891\u7e41\u7684\u5f3a\u964d\u96e8\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u591a\u6a21\u6001\u4e34\u8fd1\u9884\u62a5\u6a21\u578b\uff0c\u7ed3\u5408\u96f7\u8fbe\u548c\u536b\u661f\u56fe\u50cf\u6570\u636e\uff0c\u9884\u6d4b5\u300115\u548c30\u5206\u949f\u5185\u7684\u964d\u6c34\u60c5\u51b5\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u65b9\u6cd5\uff0c\u57285\u5206\u949f\u9884\u62a5\u65f6\u95f4\u5185\uff0c\u5f3a\u964d\u96e8\u548c\u66b4\u96e8\u7684\u4e34\u754c\u6210\u529f\u6307\u6570\u5206\u522b\u63d0\u9ad84%\u548c3%\uff0c\u4e14\u5728\u66f4\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u66f4\u9ad8\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u878d\u5408\u536b\u661f\u548c\u96f7\u8fbe\u6570\u636e\u7684\u591a\u6a21\u6001\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u8be6\u7ec6\u7684\u5f3a\u964d\u96e8\u9884\u62a5\uff0c\u6709\u52a9\u4e8e\u53ca\u65f6\u53ef\u9760\u7684\u9884\u8b66\uff0c\u5177\u6709\u91cd\u8981\u7684\u751f\u547d\u5b89\u5168\u4fdd\u969c\u4ef7\u503c\u3002"}}
{"id": "2511.00797", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00797", "abs": "https://arxiv.org/abs/2511.00797", "authors": ["Wang Zixian"], "title": "Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation", "comment": null, "summary": "Pre-trained Transformers often exhibit over-confidence in source patterns and\ndifficulty in forming new target-domain patterns during fine-tuning. We\nformalize the mechanism of output saturation leading to gradient suppression\nthrough standard cross-entropy and softmax analysis, showing that gradient\nsuppression at inflection layers confines adaptation to high-level\nrecombination of existing features while preventing low-level reconstruction.\nWe introduce a set of layer-wise diagnostic metrics -- attention entropy\n(saturation proxy), activation gradient norm, parameter gradient norm, and\nDelta-CKA under a shared PCA basis -- to identify inflection layers\ncharacterized by both low attention entropy and steep gradient decay. Building\non these findings, we propose a diagnose-first, inject-light fine-tuning\nstrategy: selectively inserting LoRA adapters at inflection layers to restore\nsuppressed backward signals with minimal parameter overhead. Experiments on\nBERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and\nover-trained source regimes reveal that over-trained initialization benefits\nfrom inflection-layer LoRA injection, while under-trained initialization\nsuffers performance degradation. When base features are strong, unblocking\ninflection layers facilitates high-level compositional adaptation; when base\nfeatures are weak, full-pathway unblocking is required for low-level\nreconstruction, as supported by joint analysis of layer-wise activation\ngradients and Delta-CKA dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u9884\u8bad\u7ec3Transformer\u5728\u5fae\u8c03\u65f6\u51fa\u73b0\u7684\u8f93\u51fa\u9971\u548c\u548c\u68af\u5ea6\u6291\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8bca\u65ad\u6307\u6807\u6765\u8bc6\u522b\u62d0\u70b9\u5c42\uff0c\u5e76\u901a\u8fc7\u5728\u62d0\u70b9\u5c42\u9009\u62e9\u6027\u6ce8\u5165LoRA\u9002\u914d\u5668\u6765\u6062\u590d\u68af\u5ea6\u4fe1\u53f7\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u521d\u59cb\u5316\u6761\u4ef6\u4e0b\u8be5\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3Transformer\u5728\u5fae\u8c03\u65f6\u7ecf\u5e38\u8868\u73b0\u51fa\u5bf9\u6e90\u57df\u6a21\u5f0f\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u96be\u4ee5\u5f62\u6210\u65b0\u7684\u76ee\u6807\u57df\u6a21\u5f0f\uff0c\u8fd9\u6e90\u4e8e\u8f93\u51fa\u9971\u548c\u5bfc\u81f4\u7684\u68af\u5ea6\u6291\u5236\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4ea4\u53c9\u71b5\u548csoftmax\u5206\u6790\u5f62\u5f0f\u5316\u8f93\u51fa\u9971\u548c\u673a\u5236\uff0c\u63d0\u51fa\u5c42\u95f4\u8bca\u65ad\u6307\u6807\uff08\u6ce8\u610f\u529b\u71b5\u3001\u6fc0\u6d3b\u68af\u5ea6\u8303\u6570\u3001\u53c2\u6570\u68af\u5ea6\u8303\u6570\u3001Delta-CKA\uff09\uff0c\u5e76\u5728\u62d0\u70b9\u5c42\u9009\u62e9\u6027\u6ce8\u5165LoRA\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8fc7\u8bad\u7ec3\u521d\u59cb\u5316\u6761\u4ef6\u4e0b\uff0c\u62d0\u70b9\u5c42LoRA\u6ce8\u5165\u80fd\u63d0\u5347\u6027\u80fd\uff1b\u800c\u5728\u6b20\u8bad\u7ec3\u521d\u59cb\u5316\u6761\u4ef6\u4e0b\uff0c\u5219\u9700\u8981\u5168\u8def\u5f84\u89e3\u963b\u585e\u624d\u80fd\u5b9e\u73b0\u4f4e\u5c42\u91cd\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86Transformer\u5fae\u8c03\u4e2d\u7684\u68af\u5ea6\u6291\u5236\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u8bca\u65ad\u4f18\u5148\u3001\u8f7b\u91cf\u6ce8\u5165\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u4e3a\u4e0d\u540c\u521d\u59cb\u5316\u6761\u4ef6\u4e0b\u7684\u9002\u914d\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.00851", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00851", "abs": "https://arxiv.org/abs/2511.00851", "authors": ["Abhishek Patange", "Sharat Chidambaran", "Prabhat Shankar", "Manjunath G. B.", "Anindya Chatterjee"], "title": "Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics", "comment": "This paper ID 254 has been accepted for presentation in the\n  Demonstration Track of the 13th ACM IKDD CODS Conference on Data Science CODS\n  2025, IISER Pune, India, from December 17 to 20, 2025", "summary": "Slug formation in oil and gas pipelines poses significant challenges to\noperational safety and efficiency, yet existing detection approaches are often\noffline, require domain expertise, and lack real-time interpretability. We\npresent an interactive application that enables end-to-end data-driven slug\ndetection through a compact and user-friendly interface. The system integrates\ndata exploration and labeling, configurable model training and evaluation with\nmultiple classifiers, visualization of classification results with time-series\noverlays, and a real-time inference module that generates persistence-based\nalerts when slug events are detected. The demo supports seamless workflows from\nlabeled CSV uploads to live inference on unseen datasets, making it\nlightweight, portable, and easily deployable. By combining domain-relevant\nanalytics with novel UI/UX features such as snapshot persistence, visual\nlabeling, and real-time alerting, our tool adds significant dissemination value\nas both a research prototype and a practical industrial application. The demo\nshowcases how interactive human-in-the-loop ML systems can bridge the gap\nbetween data science methods and real-world decision-making in critical process\nindustries, with broader applicability to time-series fault diagnosis tasks\nbeyond oil and gas.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u6cb9\u6c14\u7ba1\u9053\u6bb5\u585e\u6d41\u68c0\u6d4b\u7684\u4ea4\u4e92\u5f0f\u5e94\u7528\uff0c\u96c6\u6210\u4e86\u6570\u636e\u63a2\u7d22\u3001\u6807\u6ce8\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u53ef\u89c6\u5316\u548c\u5b9e\u65f6\u63a8\u7406\u529f\u80fd\uff0c\u901a\u8fc7\u7528\u6237\u53cb\u597d\u754c\u9762\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6570\u636e\u9a71\u52a8\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u6bb5\u585e\u6d41\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u79bb\u7ebf\u64cd\u4f5c\u3001\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u4e14\u7f3a\u4e4f\u5b9e\u65f6\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u5de5\u4e1a\u5b9e\u65f6\u76d1\u6d4b\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4ea4\u4e92\u5f0f\u5e94\u7528\uff0c\u5305\u542b\u6570\u636e\u63a2\u7d22\u6807\u6ce8\u3001\u53ef\u914d\u7f6e\u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30\u3001\u5206\u7c7b\u7ed3\u679c\u53ef\u89c6\u5316\uff08\u65f6\u95f4\u5e8f\u5217\u53e0\u52a0\u663e\u793a\uff09\u548c\u5b9e\u65f6\u63a8\u7406\u6a21\u5757\uff08\u57fa\u4e8e\u6301\u4e45\u6027\u8b66\u62a5\uff09\uff0c\u652f\u6301\u4eceCSV\u4e0a\u4f20\u5230\u5b9e\u65f6\u63a8\u7406\u7684\u65e0\u7f1d\u5de5\u4f5c\u6d41\u3002", "result": "\u5f00\u53d1\u51fa\u8f7b\u91cf\u7ea7\u3001\u4fbf\u643a\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u5de5\u5177\uff0c\u7ed3\u5408\u9886\u57df\u76f8\u5173\u5206\u6790\u548c\u521b\u65b0UI/UX\u529f\u80fd\uff08\u5feb\u7167\u6301\u4e45\u5316\u3001\u53ef\u89c6\u5316\u6807\u6ce8\u3001\u5b9e\u65f6\u8b66\u62a5\uff09\uff0c\u65e2\u53ef\u4f5c\u4e3a\u7814\u7a76\u539f\u578b\u53c8\u5177\u6709\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u4eba\u673a\u534f\u540c\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u80fd\u591f\u5f25\u5408\u6570\u636e\u79d1\u5b66\u65b9\u6cd5\u4e0e\u5173\u952e\u8fc7\u7a0b\u5de5\u4e1a\u4e2d\u5b9e\u9645\u51b3\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u6cb9\u6c14\u9886\u57df\u4e4b\u5916\u7684\u65f6\u5e8f\u6545\u969c\u8bca\u65ad\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2511.00880", "categories": ["cs.LG", "cs.AI", "68T07, 90C15, 93E35"], "pdf": "https://arxiv.org/pdf/2511.00880", "abs": "https://arxiv.org/abs/2511.00880", "authors": ["Joonyoung Lim", "Younghwan Yoo"], "title": "KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization", "comment": "12 pages, 8 figures, submitted to ECAI 2025", "summary": "We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm\nthat combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based\nsecond-order policy optimization with safety-aware gradient manipulation. KFCPO\nleverages K-FAC to perform efficient and stable natural gradient updates by\napproximating the Fisher Information Matrix (FIM) in a layerwise, closed form\nmanner, avoiding iterative approximation overheads. To address the tradeoff\nbetween reward maximization and constraint satisfaction, we introduce a margin\naware gradient manipulation mechanism that adaptively adjusts the influence of\nreward and cost gradients based on the agent's proximity to safety boundaries.\nThis method blends gradients using a direction sensitive projection,\neliminating harmful interference and avoiding abrupt changes caused by fixed\nhard thresholds. Additionally, a minibatch level KL rollback strategy is\nadopted to ensure trust region compliance and to prevent destabilizing policy\nshifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves\n10.3% to 50.2% higher average return across environments compared to the best\nbaseline that respected the safety constraint, demonstrating superior balance\nof safety and performance.", "AI": {"tldr": "KFCPO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8eKronecker\u5206\u89e3\u8fd1\u4f3c\u66f2\u7387\u7684\u4e8c\u9636\u7b56\u7565\u4f18\u5316\u548c\u5b89\u5168\u611f\u77e5\u68af\u5ea6\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u56de\u62a5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5956\u52b1\u6700\u5927\u5316\u4e0e\u7ea6\u675f\u6ee1\u8db3\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u9ad8\u6548\u4f18\u5316\u7b56\u7565\u53c8\u80fd\u786e\u4fdd\u5b89\u5168\u7ea6\u675f\u7684\u7b97\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u786c\u9608\u503c\u5bfc\u81f4\u7684\u6027\u80fd\u635f\u5931\u3002", "method": "\u4f7f\u7528K-FAC\u8fd1\u4f3cFisher\u4fe1\u606f\u77e9\u9635\u8fdb\u884c\u9ad8\u6548\u7a33\u5b9a\u7684\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\uff1b\u5f15\u5165\u8fb9\u7f18\u611f\u77e5\u68af\u5ea6\u64cd\u4f5c\u673a\u5236\uff0c\u6839\u636e\u4ee3\u7406\u4e0e\u5b89\u5168\u8fb9\u754c\u7684\u63a5\u8fd1\u7a0b\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u5956\u52b1\u548c\u6210\u672c\u68af\u5ea6\u7684\u5f71\u54cd\uff1b\u91c7\u7528\u5c0f\u6279\u91cfKL\u56de\u6eda\u7b56\u7565\u786e\u4fdd\u4fe1\u4efb\u533a\u57df\u5408\u89c4\u6027\u3002", "result": "\u5728Safety Gymnasium\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKFCPO\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e8610.3%\u523050.2%\u7684\u5e73\u5747\u56de\u62a5\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "KFCPO\u901a\u8fc7\u7ed3\u5408\u4e8c\u9636\u4f18\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u64cd\u4f5c\uff0c\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u4f18\u8d8a\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01266", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01266", "abs": "https://arxiv.org/abs/2511.01266", "authors": ["Joonghyuk Shin", "Zhengqi Li", "Richard Zhang", "Jun-Yan Zhu", "Jaesik Park", "Eli Schechtman", "Xun Huang"], "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls", "comment": "Project webpage: https://joonghyuk.com/motionstream-web/", "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.", "AI": {"tldr": "MotionStream\u5b9e\u73b0\u4e86\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u7684\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u5c06\u53cc\u5411\u6559\u5e08\u6a21\u578b\u84b8\u998f\u4e3a\u56e0\u679c\u5b66\u751f\u6a21\u578b\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u56e0\u679c\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u6c47\uff0c\u652f\u6301\u5b9e\u65f6\u65e0\u9650\u957f\u89c6\u9891\u6d41\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u6761\u4ef6\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u9ad8\uff08\u5206\u949f\u7ea7\uff09\u548c\u975e\u56e0\u679c\u5904\u7406\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u81ea\u5f3a\u5236\u5206\u5e03\u5339\u914d\u84b8\u998f\u5c06\u53cc\u5411\u6559\u5e08\u6a21\u578b\u84b8\u998f\u4e3a\u56e0\u679c\u5b66\u751f\u6a21\u578b\uff0c\u5f15\u5165\u6ed1\u52a8\u7a97\u53e3\u56e0\u679c\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u6c47\uff0c\u7ed3\u5408\u81ea\u5c55\u5f00\u548cKV\u7f13\u5b58\u6eda\u52a8\u8bad\u7ec3\u3002", "result": "\u5728\u5355\u4e2aGPU\u4e0a\u5b9e\u73b0\u6700\u9ad829 FPS\u7684\u6d41\u5f0f\u751f\u6210\uff0c\u8fd0\u52a8\u8ddf\u968f\u548c\u89c6\u9891\u8d28\u91cf\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "MotionStream\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9650\u957f\u5ea6\u5b9e\u65f6\u89c6\u9891\u6d41\u751f\u6210\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u7ed8\u5236\u8f68\u8ff9\u3001\u63a7\u5236\u76f8\u673a\u6216\u4f20\u8f93\u8fd0\u52a8\u83b7\u5f97\u771f\u6b63\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2511.01017", "categories": ["cs.LG", "62M10, 62P12", "G.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2511.01017", "abs": "https://arxiv.org/abs/2511.01017", "authors": ["Haoran Ye", "Qiuzhuang Sun", "Yang Yang"], "title": "SARIMAX-Based Power Outage Prediction During Extreme Weather Events", "comment": "12 pages, 3 figures. This paper presents the solution of Team 12 for\n  the 2025 INFORMS Data Mining Society Data Challenge. The open-source code is\n  available at: https://github.com/yhr-code/2025-INFORMS-DM-Challenge-Team12", "summary": "This study develops a SARIMAX-based prediction system for short-term power\noutage forecasting during extreme weather events. Using hourly data from\nMichigan counties with outage counts and comprehensive weather features, we\nimplement a systematic two-stage feature engineering pipeline: data cleaning to\nremove zero-variance and unknown features, followed by correlation-based\nfiltering to eliminate highly correlated predictors. The selected features are\naugmented with temporal embeddings, multi-scale lag features, and weather\nvariables with their corresponding lags as exogenous inputs to the SARIMAX\nmodel. To address data irregularity and numerical instability, we apply\nstandardization and implement a hierarchical fitting strategy with sequential\noptimization methods, automatic downgrading to ARIMA when convergence fails,\nand historical mean-based fallback predictions as a final safeguard. The model\nis optimized separately for short-term (24 hours) and medium-term (48 hours)\nforecast horizons using RMSE as the evaluation metric. Our approach achieves an\nRMSE of 177.2, representing an 8.4\\% improvement over the baseline method (RMSE\n= 193.4), thereby validating the effectiveness of our feature engineering and\nrobust optimization strategy for extreme weather-related outage prediction.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eSARIMAX\u7684\u77ed\u671f\u505c\u7535\u9884\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7279\u5f81\u5de5\u7a0b\u548c\u7a33\u5065\u4f18\u5316\u7b56\u7565\uff0c\u5728\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u4e2d\u5b9e\u73b0\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53478.4%\u7684\u9884\u6d4b\u6027\u80fd", "motivation": "\u89e3\u51b3\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u671f\u95f4\u77ed\u671f\u7535\u529b\u4e2d\u65ad\u9884\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7279\u5f81\u5de5\u7a0b\uff08\u6570\u636e\u6e05\u7406+\u76f8\u5173\u6027\u8fc7\u6ee4\uff09\uff0c\u7ed3\u5408\u65f6\u95f4\u5d4c\u5165\u3001\u591a\u5c3a\u5ea6\u6ede\u540e\u7279\u5f81\u548c\u5929\u6c14\u53d8\u91cf\u4f5c\u4e3a\u5916\u751f\u8f93\u5165\uff0c\u4f7f\u7528SARIMAX\u6a21\u578b\u5e76\u5b9e\u65bd\u5206\u5c42\u62df\u5408\u7b56\u7565\u548c\u7a33\u5065\u4f18\u5316\u65b9\u6cd5", "result": "\u572824\u5c0f\u65f6\u548c48\u5c0f\u65f6\u9884\u6d4b\u8303\u56f4\u5185\uff0c\u6a21\u578b\u8fbe\u5230RMSE 177.2\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08RMSE 193.4\uff09\u63d0\u53478.4%", "conclusion": "\u6240\u63d0\u51fa\u7684\u7279\u5f81\u5de5\u7a0b\u548c\u7a33\u5065\u4f18\u5316\u7b56\u7565\u5bf9\u6781\u7aef\u5929\u6c14\u76f8\u5173\u505c\u7535\u9884\u6d4b\u6709\u6548\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2511.01307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01307", "abs": "https://arxiv.org/abs/2511.01307", "authors": ["Tae-Young Lee", "Juwon Seo", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models", "comment": "26 pages, 9 figures, 16 tables, NeurIPS 2025", "summary": "Recent advances in diffusion models have enabled high-quality synthesis of\nspecific subjects, such as identities or objects. This capability, while\nunlocking new possibilities in content creation, also introduces significant\nprivacy risks, as personalization techniques can be misused by malicious users\nto generate unauthorized content. Although several studies have attempted to\ncounter this by generating adversarially perturbed samples designed to disrupt\npersonalization, they rely on unrealistic assumptions and become ineffective in\nthe presence of even a few clean images or under simple image transformations.\nTo address these challenges, we shift the protection target from the images to\nthe diffusion model itself to hinder the personalization of specific subjects,\nthrough our novel framework called Anti-Personalized Diffusion Models (APDM).\nWe first provide a theoretical analysis demonstrating that a naive approach of\nexisting loss functions to diffusion models is inherently incapable of ensuring\nconvergence for robust anti-personalization. Motivated by this finding, we\nintroduce Direct Protective Optimization (DPO), a novel loss function that\neffectively disrupts subject personalization in the target model without\ncompromising generative quality. Moreover, we propose a new dual-path\noptimization strategy, coined Learning to Protect (L2P). By alternating between\npersonalization and protection paths, L2P simulates future personalization\ntrajectories and adaptively reinforces protection at each step. Experimental\nresults demonstrate that our framework outperforms existing methods, achieving\nstate-of-the-art performance in preventing unauthorized personalization. The\ncode is available at https://github.com/KU-VGI/APDM.", "AI": {"tldr": "\u63d0\u51fa\u4e86APDM\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fdd\u62a4\u76ee\u6807\u4ece\u56fe\u50cf\u8f6c\u79fb\u5230\u6269\u6563\u6a21\u578b\u672c\u8eab\u6765\u9632\u6b62\u7279\u5b9a\u4e3b\u4f53\u7684\u4e2a\u6027\u5316\uff0c\u5305\u542bDPO\u635f\u5931\u51fd\u6570\u548cL2P\u53cc\u8def\u5f84\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u963b\u6b62\u672a\u7ecf\u6388\u6743\u7684\u4e2a\u6027\u5316\u751f\u6210\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u7279\u5b9a\u4e3b\u4f53\u5408\u6210\u80fd\u529b\u5e26\u6765\u4e86\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709\u57fa\u4e8e\u5bf9\u6297\u6837\u672c\u7684\u65b9\u6cd5\u5728\u5c11\u91cf\u5e72\u51c0\u56fe\u50cf\u6216\u7b80\u5355\u56fe\u50cf\u53d8\u6362\u4e0b\u5931\u6548\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86APDM\u6846\u67b6\uff0c\u5305\u542bDirect Protective Optimization (DPO)\u635f\u5931\u51fd\u6570\u548cLearning to Protect (L2P)\u53cc\u8def\u5f84\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u4e2a\u6027\u5316\u548c\u4fdd\u62a4\u8def\u5f84\u6765\u6a21\u62df\u672a\u6765\u4e2a\u6027\u5316\u8f68\u8ff9\u5e76\u81ea\u9002\u5e94\u5f3a\u5316\u4fdd\u62a4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u9632\u6b62\u672a\u7ecf\u6388\u6743\u4e2a\u6027\u5316\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "APDM\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5c42\u9762\u7684\u4fdd\u62a4\u800c\u975e\u56fe\u50cf\u5c42\u9762\u7684\u6270\u52a8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u5e26\u6765\u7684\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2511.01218", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01218", "abs": "https://arxiv.org/abs/2511.01218", "authors": ["Minh-Duc Nguyen", "Dung D. Le", "Phi Long Nguyen"], "title": "Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations", "comment": "Under Review", "summary": "The rapid growth of electric vehicles (EVs) necessitates the strategic\nplacement of charging stations to optimize resource utilization and minimize\nuser inconvenience. Reinforcement learning (RL) offers an innovative approach\nto identifying optimal charging station locations; however, existing methods\nface challenges due to their deterministic reward systems, which limit\nefficiency. Because real-world conditions are dynamic and uncertain, a\ndeterministic reward structure cannot fully capture the complexities of\ncharging station placement. As a result, evaluation becomes costly and\ntime-consuming, and less reflective of real-world scenarios. To address this\nchallenge, we propose a novel framework that integrates deep RL with\nagent-based simulations to model EV movement and estimate charging demand in\nreal time. Our approach employs a hybrid RL agent with dual Q-networks to\nselect optimal locations and configure charging ports, guided by a hybrid\nreward function that combines deterministic factors with simulation-derived\nfeedback. Case studies in Hanoi, Vietnam, show that our method reduces average\nwaiting times by 53.28% compared to the initial state, outperforming static\nbaseline methods. This scalable and adaptive solution enhances EV\ninfrastructure planning, effectively addressing real-world complexities and\nimproving user experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u4ee3\u7406\u6a21\u62df\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u5e03\u5c40\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u51fd\u6570\u548c\u53ccQ\u7f51\u7edc\u6765\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5145\u7535\u7ad9\u5e03\u5c40\u4f18\u5316\u4e2d\u9762\u4e34\u786e\u5b9a\u6027\u5956\u52b1\u7cfb\u7edf\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u4e0d\u53cd\u6620\u771f\u5b9e\u573a\u666f\u3002", "method": "\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\uff0c\u4f7f\u7528\u5177\u6709\u53ccQ\u7f51\u7edc\u7684\u6df7\u5408RL\u4ee3\u7406\u6765\u9009\u62e9\u6700\u4f18\u4f4d\u7f6e\u548c\u914d\u7f6e\u5145\u7535\u7aef\u53e3\uff0c\u91c7\u7528\u7ed3\u5408\u786e\u5b9a\u6027\u56e0\u7d20\u548c\u6a21\u62df\u53cd\u9988\u7684\u6df7\u5408\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u8d8a\u5357\u6cb3\u5185\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u521d\u59cb\u72b6\u6001\u5c06\u5e73\u5747\u7b49\u5f85\u65f6\u95f4\u51cf\u5c11\u4e8653.28%\uff0c\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u53ef\u6269\u5c55\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u589e\u5f3a\u4e86\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\uff0c\u6709\u6548\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u5e76\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2511.01427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01427", "abs": "https://arxiv.org/abs/2511.01427", "authors": ["Yinchao Ma", "Yuyang Tang", "Wenfei Yang", "Tianzhu Zhang", "Xu Zhou", "Feng Wu"], "title": "UniSOT: A Unified Framework for Multi-Modality Single Object Tracking", "comment": "The paper has been accepted by TPAMI", "summary": "Single object tracking aims to localize target object with specific reference\nmodalities (bounding box, natural language or both) in a sequence of specific\nvideo modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different\nreference modalities enable various human-machine interactions, and different\nvideo modalities are demanded in complex scenarios to enhance tracking\nrobustness. Existing trackers are designed for single or several video\nmodalities with single or several reference modalities, which leads to separate\nmodel designs and limits practical applications. Practically, a unified tracker\nis needed to handle various requirements. To the best of our knowledge, there\nis still no tracker that can perform tracking with these above reference\nmodalities across these video modalities simultaneously. Thus, in this paper,\nwe present a unified tracker, UniSOT, for different combinations of three\nreference modalities and four video modalities with uniform parameters.\nExtensive experimental results on 18 visual tracking, vision-language tracking\nand RGB+X tracking benchmarks demonstrate that UniSOT shows superior\nperformance against modality-specific counterparts. Notably, UniSOT outperforms\nprevious counterparts by over 3.0\\% AUC on TNL2K across all three reference\nmodalities and outperforms Un-Track by over 2.0\\% main metric across all three\nRGB+X video modalities.", "AI": {"tldr": "UniSOT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u76ee\u6807\u8ddf\u8e2a\u5668\uff0c\u80fd\u591f\u5904\u7406\u4e09\u79cd\u53c2\u8003\u6a21\u6001\uff08\u8fb9\u754c\u6846\u3001\u81ea\u7136\u8bed\u8a00\u6216\u4e24\u8005\uff09\u548c\u56db\u79cd\u89c6\u9891\u6a21\u6001\uff08RGB\u3001RGB+\u6df1\u5ea6\u3001RGB+\u70ed\u6210\u50cf\u6216RGB+\u4e8b\u4ef6\uff09\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u4f7f\u7528\u7edf\u4e00\u53c2\u6570\u5b9e\u73b0\u8de8\u6a21\u6001\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u5668\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u6216\u5c11\u6570\u51e0\u79cd\u89c6\u9891\u6a21\u6001\u548c\u53c2\u8003\u6a21\u6001\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u6a21\u578b\u5206\u79bb\u4e14\u9650\u5236\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u8ddf\u8e2a\u5668\u6765\u5904\u7406\u5404\u79cd\u9700\u6c42\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e0a\u8ff0\u6240\u6709\u53c2\u8003\u6a21\u6001\u548c\u89c6\u9891\u6a21\u6001\u7684\u8ddf\u8e2a\u5668\u3002", "method": "\u63d0\u51fa\u4e86UniSOT\u7edf\u4e00\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u53c2\u6570\u8bbe\u8ba1\uff0c\u652f\u6301\u4e09\u79cd\u53c2\u8003\u6a21\u6001\uff08\u8fb9\u754c\u6846\u3001\u81ea\u7136\u8bed\u8a00\u6216\u4e24\u8005\uff09\u548c\u56db\u79cd\u89c6\u9891\u6a21\u6001\uff08RGB\u3001RGB+\u6df1\u5ea6\u3001RGB+\u70ed\u6210\u50cf\u6216RGB+\u4e8b\u4ef6\uff09\u7684\u4e0d\u540c\u7ec4\u5408\u3002", "result": "\u572818\u4e2a\u89c6\u89c9\u8ddf\u8e2a\u3001\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u548cRGB+X\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniSOT\u76f8\u6bd4\u6a21\u6001\u7279\u5b9a\u5bf9\u5e94\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u5728TNL2K\u4e0a\u6240\u6709\u4e09\u79cd\u53c2\u8003\u6a21\u6001\u4e0a\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc73.0% AUC\uff0c\u5728Un-Track\u4e0a\u6240\u6709\u4e09\u79cdRGB+X\u89c6\u9891\u6a21\u6001\u4e0a\u4e3b\u8981\u6307\u6807\u63d0\u5347\u8d85\u8fc72.0%\u3002", "conclusion": "UniSOT\u8bc1\u660e\u4e86\u7edf\u4e00\u8ddf\u8e2a\u5668\u5728\u5904\u7406\u591a\u79cd\u53c2\u8003\u6a21\u6001\u548c\u89c6\u9891\u6a21\u6001\u7ec4\u5408\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01275", "abs": "https://arxiv.org/abs/2511.01275", "authors": ["Zan Li", "Kyongmin Yeo", "Wesley Gifford", "Lara Marcuse", "Madeline Fields", "B\u00fclent Yener"], "title": "Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting", "comment": null, "summary": "Forecasting epileptic seizures from multivariate EEG signals represents a\ncritical challenge in healthcare time series prediction, requiring high\nsensitivity, low false alarm rates, and subject-specific adaptability. We\npresent STAN, an Adversarial Spatio-Temporal Attention Network that jointly\nmodels spatial brain connectivity and temporal neural dynamics through cascaded\nattention blocks with alternating spatial and temporal modules. Unlike existing\napproaches that assume fixed preictal durations or separately process spatial\nand temporal features, STAN captures bidirectional dependencies between spatial\nand temporal patterns through a unified cascaded architecture. Adversarial\ntraining with gradient penalty enables robust discrimination between interictal\nand preictal states learned from clearly defined 15-minute preictal windows.\nContinuous 90-minute pre-seizure monitoring reveals that the learned\nspatio-temporal attention patterns enable early detection: reliable alarms\ntrigger at subject-specific times (typically 15-45 minutes before onset),\nreflecting the model's capacity to capture subtle preictal dynamics without\nrequiring individualized training. Experiments on two benchmark EEG datasets\n(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14\nevents) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011\nfalse detections per hour and 94.2% sensitivity with 0.063 false detections per\nhour, respectively, while maintaining computational efficiency (2.3M\nparameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond\nepilepsy, the proposed framework provides a general paradigm for\nspatio-temporal forecasting in healthcare and other time series domains where\nindividual heterogeneity and interpretability are crucial.", "AI": {"tldr": "STAN\u662f\u4e00\u79cd\u5bf9\u6297\u6027\u65f6\u7a7a\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u591a\u53d8\u91cfEEG\u4fe1\u53f7\u9884\u6d4b\u766b\u75eb\u53d1\u4f5c\uff0c\u901a\u8fc7\u7ea7\u8054\u6ce8\u610f\u529b\u5757\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u5927\u8111\u8fde\u63a5\u6027\u548c\u65f6\u95f4\u795e\u7ecf\u52a8\u6001\uff0c\u5b9e\u73b0\u9ad8\u7075\u654f\u5ea6\u548c\u4f4e\u8bef\u62a5\u7387\u3002", "motivation": "\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u5728\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u9ad8\u7075\u654f\u5ea6\u3001\u4f4e\u8bef\u62a5\u7387\u548c\u53d7\u8bd5\u8005\u7279\u5f02\u6027\u9002\u5e94\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u7684\u53d1\u4f5c\u524d\u6301\u7eed\u65f6\u95f4\u6216\u5206\u522b\u5904\u7406\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u65f6\u7a7a\u6a21\u5f0f\u7684\u53cc\u5411\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faSTAN\u7f51\u7edc\uff0c\u901a\u8fc7\u7ea7\u8054\u6ce8\u610f\u529b\u5757\u4ea4\u66ff\u4f7f\u7528\u7a7a\u95f4\u548c\u65f6\u95f4\u6a21\u5757\uff0c\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u5927\u8111\u8fde\u63a5\u6027\u548c\u65f6\u95f4\u795e\u7ecf\u52a8\u6001\u3002\u91c7\u7528\u5e26\u68af\u5ea6\u60e9\u7f5a\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u4ece\u660e\u786e\u5b9a\u4e49\u768415\u5206\u949f\u53d1\u4f5c\u524d\u7a97\u53e3\u5b66\u4e60\u53d1\u4f5c\u95f4\u671f\u548c\u53d1\u4f5c\u524d\u72b6\u6001\u7684\u7a33\u5065\u533a\u5206\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6EEG\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff1aCHB-MIT\u5934\u76ae\u6570\u636e\u96c696.6%\u7075\u654f\u5ea6\uff0c0.011\u6b21/\u5c0f\u65f6\u8bef\u62a5\uff1bMSSM\u9885\u5185\u6570\u636e\u96c694.2%\u7075\u654f\u5ea6\uff0c0.063\u6b21/\u5c0f\u65f6\u8bef\u62a5\u3002\u8ba1\u7b97\u6548\u7387\u9ad8\uff08230\u4e07\u53c2\u6570\uff0c45\u6beb\u79d2\u5ef6\u8fdf\uff0c180MB\u5185\u5b58\uff09\uff0c\u9002\u5408\u5b9e\u65f6\u8fb9\u7f18\u90e8\u7f72\u3002", "conclusion": "STAN\u6846\u67b6\u4e0d\u4ec5\u4e3a\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u4e3a\u533b\u7597\u548c\u5176\u4ed6\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7684\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u822c\u8303\u5f0f\uff0c\u5176\u4e2d\u4e2a\u4f53\u5f02\u8d28\u6027\u548c\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.01286", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.01286", "abs": "https://arxiv.org/abs/2511.01286", "authors": ["Sivaram Krishnan", "Jinho Choi", "Jihong Park", "Gregory Sherman", "Benjamin Campbell"], "title": "Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks", "comment": null, "summary": "The application of machine learning (ML) to communication systems is expected\nto play a pivotal role in future artificial intelligence (AI)-based\nnext-generation wireless networks. While most existing works focus on ML\ntechniques for static wireless environments, they often face limitations when\napplied to highly dynamic environments, such as flying ad hoc networks\n(FANETs). This paper explores the use of data-driven Koopman approaches to\naddress these challenges. Specifically, we investigate how these approaches can\nmodel UAV trajectory dynamics within FANETs, enabling more accurate predictions\nand improved network performance. By leveraging Koopman operator theory, we\npropose two possible approaches -- centralized and distributed -- to\nefficiently address the challenges posed by the constantly changing topology of\nFANETs. To demonstrate this, we consider a FANET performing surveillance with\nUAVs following pre-determined trajectories and predict\nsignal-to-interference-plus-noise ratios (SINRs) to ensure reliable\ncommunication between UAVs. Our results show that these approaches can\naccurately predict connectivity and isolation events that lead to modelled\ncommunication outages. This capability could help UAVs schedule their\ntransmissions based on these predictions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684Koopman\u65b9\u6cd5\u6765\u89e3\u51b3\u65e0\u4eba\u673a\u81ea\u7ec4\u7f51(FANETs)\u4e2d\u7684\u52a8\u6001\u73af\u5883\u6311\u6218\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u4e24\u79cd\u65b9\u6cd5\u9884\u6d4b\u65e0\u4eba\u673a\u8f68\u8ff9\u52a8\u6001\u548c\u901a\u4fe1\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u65e0\u7ebf\u73af\u5883\uff0c\u5728\u9ad8\u5ea6\u52a8\u6001\u7684FANETs\u73af\u5883\u4e2d\u5e94\u7528\u53d7\u9650\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u7f51\u7edc\u62d3\u6251\u4e0d\u65ad\u53d8\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\uff0c\u63d0\u51fa\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u4e24\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5efa\u6a21\u65e0\u4eba\u673a\u8f68\u8ff9\u52a8\u6001\uff0c\u9884\u6d4b\u4fe1\u53f7\u5e72\u6270\u566a\u58f0\u6bd4(SINR)\u6765\u786e\u4fdd\u65e0\u4eba\u673a\u95f4\u53ef\u9760\u901a\u4fe1\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5bfc\u81f4\u901a\u4fe1\u4e2d\u65ad\u7684\u8fde\u63a5\u548c\u9694\u79bb\u4e8b\u4ef6\uff0c\u4e3a\u65e0\u4eba\u673a\u57fa\u4e8e\u9884\u6d4b\u8c03\u5ea6\u4f20\u8f93\u63d0\u4f9b\u53ef\u80fd\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684Koopman\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3FANETs\u52a8\u6001\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u6311\u6218\uff0c\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.01352", "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "hep-ex", "physics.data-an", "J.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2511.01352", "abs": "https://arxiv.org/abs/2511.01352", "authors": ["Lucie Flek", "Oliver Janik", "Philipp Alexander Jung", "Akbar Karimi", "Timo Saala", "Alexander Schmidt", "Matthias Schott", "Philipp Soldin", "Matthias Thiesmeyer", "Christopher Wiebusch", "Ulrich Willemsen"], "title": "MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks", "comment": "Submitted to Computing and Software for Big Science", "summary": "In this paper, we present a new algorithm, MiniFool, that implements\nphysics-inspired adversarial attacks for testing neural network-based\nclassification tasks in particle and astroparticle physics. While we initially\ndeveloped the algorithm for the search for astrophysical tau neutrinos with the\nIceCube Neutrino Observatory, we apply it to further data from other science\ndomains, thus demonstrating its general applicability. Here, we apply the\nalgorithm to the well-known MNIST data set and furthermore, to Open Data data\nfrom the CMS experiment at the Large Hadron Collider. The algorithm is based on\nminimizing a cost function that combines a $\\chi^2$ based test-statistic with\nthe deviation from the desired target score. The test statistic quantifies the\nprobability of the perturbations applied to the data based on the experimental\nuncertainties. For our studied use cases, we find that the likelihood of a\nflipped classification differs for both the initially correctly and incorrectly\nclassified events. When testing changes of the classifications as a function of\nan attack parameter that scales the experimental uncertainties, the robustness\nof the network decision can be quantified. Furthermore, this allows testing the\nrobustness of the classification of unlabeled experimental data.", "AI": {"tldr": "\u63d0\u51faMiniFool\u7b97\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u542f\u53d1\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u7c92\u5b50\u7269\u7406\u548c\u5929\u4f53\u7c92\u5b50\u7269\u7406\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5728MNIST\u548cCMS\u5b9e\u9a8c\u6570\u636e\u4e0a\u9a8c\u8bc1\u5176\u901a\u7528\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8003\u8651\u5b9e\u9a8c\u4e0d\u786e\u5b9a\u6027\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u4ee5\u6d4b\u8bd5\u795e\u7ecf\u7f51\u7edc\u5728\u7269\u7406\u6570\u636e\u5206\u6790\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728IceCube\u4e2d\u5fae\u5b50\u89c2\u6d4b\u7ad9\u7684\u5929\u4f53\u7269\u7406tau\u4e2d\u5fae\u5b50\u641c\u7d22\u7b49\u5e94\u7528\u4e2d\u3002", "method": "\u57fa\u4e8e\u6700\u5c0f\u5316\u6210\u672c\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u03c7\u00b2\u68c0\u9a8c\u7edf\u8ba1\u91cf\u548c\u76ee\u6807\u5206\u6570\u504f\u5dee\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5e76\u91cf\u5316\u57fa\u4e8e\u5b9e\u9a8c\u4e0d\u786e\u5b9a\u6027\u7684\u6270\u52a8\u6982\u7387\u3002", "result": "\u53d1\u73b0\u6b63\u786e\u5206\u7c7b\u548c\u9519\u8bef\u5206\u7c7b\u4e8b\u4ef6\u7684\u5206\u7c7b\u7ffb\u8f6c\u6982\u7387\u4e0d\u540c\uff0c\u901a\u8fc7\u8c03\u8282\u653b\u51fb\u53c2\u6570\uff08\u5b9e\u9a8c\u4e0d\u786e\u5b9a\u6027\u7f29\u653e\u56e0\u5b50\uff09\u53ef\u4ee5\u91cf\u5316\u7f51\u7edc\u51b3\u7b56\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MiniFool\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u6d4b\u8bd5\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u672a\u6807\u8bb0\u5b9e\u9a8c\u6570\u636e\u7684\u5206\u7c7b\u7a33\u5065\u6027\u8bc4\u4f30\uff0c\u5177\u6709\u8de8\u9886\u57df\u5e94\u7528\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2511.01553", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.01553", "abs": "https://arxiv.org/abs/2511.01553", "authors": ["Elvin Hajizada", "Danielle Rager", "Timothy Shea", "Leobardo Campos-Macias", "Andreas Wild", "Eyke H\u00fcllermeier", "Yulia Sandamirskaya", "Mike Davies"], "title": "Real-time Continual Learning on Intel Loihi 2", "comment": null, "summary": "AI systems on edge devices face a critical challenge in open-world\nenvironments: adapting when data distributions shift and novel classes emerge.\nWhile offline training dominates current paradigms, online continual learning\n(OCL)--where models learn incrementally from non-stationary streams without\ncatastrophic forgetting--remains challenging in power-constrained settings. We\npresent a neuromorphic solution called CLP-SNN: a spiking neural network\narchitecture for Continually Learning Prototypes and its implementation on\nIntel's Loihi 2 chip. Our approach introduces three innovations: (1)\nevent-driven and spatiotemporally sparse local learning, (2) a self-normalizing\nthree-factor learning rule maintaining weight normalization, and (3) integrated\nneurogenesis and metaplasticity for capacity expansion and forgetting\nmitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves\naccuracy competitive with replay methods while being rehearsal-free. CLP-SNN\ndelivers transformative efficiency gains: 70\\times faster (0.33ms vs 23.2ms),\nand 5,600\\times more energy efficient (0.05mJ vs 281mJ) than the best\nalternative OCL on edge GPU. This demonstrates that co-designed brain-inspired\nalgorithms and neuromorphic hardware can break traditional accuracy-efficiency\ntrade-offs for future edge AI systems.", "AI": {"tldr": "CLP-SNN\u662f\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u67b6\u6784\uff0c\u5728Intel Loihi 2\u82af\u7247\u4e0a\u5b9e\u73b0\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7a00\u758f\u5b66\u4e60\u3001\u81ea\u5f52\u4e00\u5316\u5b66\u4e60\u89c4\u5219\u548c\u795e\u7ecf\u53d1\u751f\u673a\u5236\uff0c\u5728OpenLORIS\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e0e\u56de\u653e\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u83b7\u5f9770\u500d\u901f\u5ea6\u63d0\u5347\u548c5600\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u9762\u4e34\u7684\u6570\u636e\u5206\u5e03\u6f02\u79fb\u548c\u65b0\u7c7b\u51fa\u73b0\u7684\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u79bb\u7ebf\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u5728\u529f\u7387\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\uff1a\u4e8b\u4ef6\u9a71\u52a8\u65f6\u7a7a\u7a00\u758f\u5c40\u90e8\u5b66\u4e60\u3001\u81ea\u5f52\u4e00\u5316\u4e09\u56e0\u5b50\u5b66\u4e60\u89c4\u5219\u4fdd\u6301\u6743\u91cd\u5f52\u4e00\u5316\u3001\u96c6\u6210\u795e\u7ecf\u53d1\u751f\u548c\u5143\u53ef\u5851\u6027\u5b9e\u73b0\u5bb9\u91cf\u6269\u5c55\u548c\u9057\u5fd8\u7f13\u89e3\u3002", "result": "\u5728OpenLORIS\u5c11\u6837\u672c\u5b66\u4e60\u5b9e\u9a8c\u4e2d\uff0cCLP-SNN\u8fbe\u5230\u4e0e\u56de\u653e\u65b9\u6cd5\u7ade\u4e89\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b070\u500d\u901f\u5ea6\u63d0\u5347\uff080.33ms vs 23.2ms\uff09\u548c5600\u500d\u80fd\u6548\u63d0\u5347\uff080.05mJ vs 281mJ\uff09\u3002", "conclusion": "\u8111\u542f\u53d1\u7b97\u6cd5\u4e0e\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u534f\u540c\u8bbe\u8ba1\u80fd\u591f\u6253\u7834\u4f20\u7edf\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\uff0c\u4e3a\u672a\u6765\u8fb9\u7f18AI\u7cfb\u7edf\u63d0\u4f9b\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01836", "abs": "https://arxiv.org/abs/2511.01836", "authors": ["Ekdeep Singh Lubana", "Can Rager", "Sai Sumedh R. Hindupur", "Valerie Costa", "Greta Tuckute", "Oam Patel", "Sonia Krishna Murthy", "Thomas Fel", "Daniel Wurgaft", "Eric J. Bigelow", "Johnny Lin", "Demba Ba", "Martin Wattenberg", "Fernanda Viegas", "Melanie Weber", "Aaron Mueller"], "title": "Priors in Time: Missing Inductive Biases for Language Model Interpretability", "comment": "Preprint", "summary": "Recovering meaningful concepts from language model activations is a central\naim of interpretability. While existing feature extraction methods aim to\nidentify concepts that are independent directions, it is unclear if this\nassumption can capture the rich temporal structure of language. Specifically,\nvia a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose\npriors that assume independence of concepts across time, implying stationarity.\nMeanwhile, language model representations exhibit rich temporal dynamics,\nincluding systematic growth in conceptual dimensionality, context-dependent\ncorrelations, and pronounced non-stationarity, in direct conflict with the\npriors of SAEs. Taking inspiration from computational neuroscience, we\nintroduce a new interpretability objective -- Temporal Feature Analysis --\nwhich possesses a temporal inductive bias to decompose representations at a\ngiven time into two parts: a predictable component, which can be inferred from\nthe context, and a residual component, which captures novel information\nunexplained by the context. Temporal Feature Analyzers correctly parse garden\npath sentences, identify event boundaries, and more broadly delineate abstract,\nslow-moving information from novel, fast-moving information, while existing\nSAEs show significant pitfalls in all the above tasks. Overall, our results\nunderscore the need for inductive biases that match the data in designing\nrobust interpretability tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u7279\u5f81\u5206\u6790(Temporal Feature Analysis)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u5f52\u7eb3\u504f\u7f6e\u6765\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u5728\u5904\u7406\u8bed\u8a00\u65f6\u5e8f\u52a8\u6001\u7ed3\u6784\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5047\u8bbe\u6982\u5ff5\u662f\u72ec\u7acb\u65b9\u5411\uff0c\u4f46\u8bed\u8a00\u6a21\u578b\u8868\u793a\u5177\u6709\u4e30\u5bcc\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027(\u5982\u6982\u5ff5\u7ef4\u5ea6\u589e\u957f\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u76f8\u5173\u6027\u3001\u975e\u5e73\u7a33\u6027)\uff0c\u8fd9\u4e0eSAEs\u7684\u72ec\u7acb\u6027\u5148\u9a8c\u76f8\u51b2\u7a81\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u7279\u5f81\u5206\u6790\uff0c\u5c06\u7ed9\u5b9a\u65f6\u95f4\u7684\u8868\u793a\u5206\u89e3\u4e3a\u4e24\u90e8\u5206\uff1a\u53ef\u9884\u6d4b\u90e8\u5206(\u53ef\u4ece\u4e0a\u4e0b\u6587\u63a8\u65ad)\u548c\u6b8b\u5dee\u90e8\u5206(\u6355\u83b7\u4e0a\u4e0b\u6587\u65e0\u6cd5\u89e3\u91ca\u7684\u65b0\u4fe1\u606f)\u3002", "result": "\u65f6\u95f4\u7279\u5f81\u5206\u6790\u5668\u80fd\u6b63\u786e\u89e3\u6790\u82b1\u56ed\u8def\u5f84\u53e5\u5b50\u3001\u8bc6\u522b\u4e8b\u4ef6\u8fb9\u754c\uff0c\u5e76\u66f4\u5e7f\u6cdb\u5730\u533a\u5206\u62bd\u8c61\u6162\u53d8\u4fe1\u606f\u548c\u65b0\u9896\u5feb\u53d8\u4fe1\u606f\uff0c\u800c\u73b0\u6709SAEs\u5728\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u5747\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u8bbe\u8ba1\u7a33\u5065\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u9700\u8981\u4e0e\u6570\u636e\u5339\u914d\u7684\u5f52\u7eb3\u504f\u7f6e\u3002"}}
