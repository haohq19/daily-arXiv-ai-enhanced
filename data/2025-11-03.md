<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Overcoming Prompts Pool Confusion via Parameterized Prompt for Incremental Object Detection](https://arxiv.org/abs/2510.27316)
*Zijia An,Boyu Diao,Ruiqi Liu,Libo Huang,Chuanguang Yang,Fei Wang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: 该论文提出了参数化提示P²IOD方法，用于解决增量目标检测中的类别共现问题，通过神经网络作为参数化提示来自适应整合跨任务知识，并采用参数化提示融合策略约束结构更新。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示池的方法假设增量任务中的类别集合是互斥的，这不适用于目标检测，因为检测图像中存在固有的共现现象。在共现场景中，先前任务的未标记对象可能出现在当前任务图像中，导致提示池混淆。

Method: P²IOD使用神经网络作为参数化提示来自适应整合跨任务知识，并采用参数化提示融合策略来约束提示结构更新，防止灾难性遗忘。

Result: 在PASCAL VOC2007和MS COCO数据集上的广泛实验表明，P²IOD在增量目标检测中有效，并在现有基线中实现了最先进的性能。

Conclusion: P²IOD通过参数化提示方法成功解决了增量目标检测中的类别共现问题，实现了知识自适应整合和有效遗忘抑制。

Abstract: Recent studies have demonstrated that incorporating trainable prompts into
pretrained models enables effective incremental learning. However, the
application of prompts in incremental object detection (IOD) remains
underexplored. Existing prompts pool based approaches assume disjoint class
sets across incremental tasks, which are unsuitable for IOD as they overlook
the inherent co-occurrence phenomenon in detection images. In co-occurring
scenarios, unlabeled objects from previous tasks may appear in current task
images, leading to confusion in prompts pool. In this paper, we hold that
prompt structures should exhibit adaptive consolidation properties across
tasks, with constrained updates to prevent catastrophic forgetting. Motivated
by this, we introduce Parameterized Prompts for Incremental Object Detection
(P$^2$IOD). Leveraging neural networks global evolution properties, P$^2$IOD
employs networks as the parameterized prompts to adaptively consolidate
knowledge across tasks. To constrain prompts structure updates, P$^2$IOD
further engages a parameterized prompts fusion strategy. Extensive experiments
on PASCAL VOC2007 and MS COCO datasets demonstrate that P$^2$IOD's
effectiveness in IOD and achieves the state-of-the-art performance among
existing baselines.

</details>


### [2] [Mitigating Semantic Collapse in Partially Relevant Video Retrieval](https://arxiv.org/abs/2510.27432)
*WonJun Moon,MinSeok Jung,Gilhan Park,Tae-Young Kim,Cheol-Ho Cho,Woojin Jun,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出了一种解决PRVR任务中语义坍缩问题的新框架，通过文本相关性保持学习和跨分支视频对齐来防止查询和视频嵌入的语义坍缩，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有PRVR方法将所有标注的文本-视频对视为正样本，其他为负样本，忽略了视频内和跨视频的丰富语义变化，导致相同视频中不同事件的查询和视频片段嵌入坍缩在一起，而语义相似的查询和不同视频的片段嵌入被推远，限制了检索性能。

Method: 1. 文本相关性保持学习：保持基础模型编码的文本查询语义关系；2. 跨分支视频对齐：通过对比对齐方法在不同时间尺度上解耦层次化视频表示；3. 顺序保持令牌合并和自适应CBVA：生成内部一致但相互区分的视频片段以增强对齐。

Result: 在PRVR基准测试上的广泛实验表明，该框架有效防止了语义坍缩，显著提高了检索准确率。

Conclusion: 提出的框架通过解决文本和视频嵌入空间中的语义坍缩问题，为PRVR任务提供了有效的解决方案，显著提升了检索性能。

Abstract: Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the
content matches a text query. Existing methods treat every annotated text-video
pair as a positive and all others as negatives, ignoring the rich semantic
variation both within a single video and across different videos. Consequently,
embeddings of both queries and their corresponding video-clip segments for
distinct events within the same video collapse together, while embeddings of
semantically similar queries and segments from different videos are driven
apart. This limits retrieval performance when videos contain multiple, diverse
events. This paper addresses the aforementioned problems, termed as semantic
collapse, in both the text and video embedding spaces. We first introduce Text
Correlation Preservation Learning, which preserves the semantic relationships
encoded by the foundation model across text queries. To address collapse in
video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive
alignment method that disentangles hierarchical video representations across
temporal scales. Subsequently, we introduce order-preserving token merging and
adaptive CBVA to enhance alignment by producing video segments that are
internally coherent yet mutually distinctive. Extensive experiments on PRVR
benchmarks demonstrate that our framework effectively prevents semantic
collapse and substantially improves retrieval accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study](https://arxiv.org/abs/2510.26910)
*Kshitij Nikhal,Luke Ackerknecht,Benjamin S. Riggan,Phil Stahlfeld*

Main category: cs.LG

TL;DR: 提出一个结合聚类和少样本预测的框架，利用大规模充电需求数据集发现站点原型，原型特定的专家模型在预测未见过站点需求时优于全局基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于小规模数据集、简单的时间依赖建模以及对运营历史有限站点的弱泛化能力，需要准确理解充电行为以支持电动汽车普及和电网弹性基础设施。

Method: 整合聚类与少样本预测的框架，使用新颖的大规模充电需求数据集发现站点原型，建立原型特定的专家模型进行需求预测。

Result: 原型特定的专家模型在预测未见站点需求时表现优于全局基线模型，为基础设施分段提供预测性能基础。

Conclusion: 通过建立预测性能作为基础设施分段的基础，生成可操作的见解，使运营商能够降低成本、优化能源和定价策略，并支持对气候目标至关重要的电网弹性。

Abstract: The decarbonization of transportation relies on the widespread adoption of
electric vehicles (EVs), which requires an accurate understanding of charging
behavior to ensure cost-effective, grid-resilient infrastructure. Existing work
is constrained by small-scale datasets, simple proximity-based modeling of
temporal dependencies, and weak generalization to sites with limited
operational history. To overcome these limitations, this work proposes a
framework that integrates clustering with few-shot forecasting to uncover site
archetypes using a novel large-scale dataset of charging demand. The results
demonstrate that archetype-specific expert models outperform global baselines
in forecasting demand at unseen sites. By establishing forecast performance as
a basis for infrastructure segmentation, we generate actionable insights that
enable operators to lower costs, optimize energy and pricing strategies, and
support grid resilience critical to climate goals.

</details>


### [4] [Quantitative Bounds for Length Generalization in Transformers](https://arxiv.org/abs/2510.27015)
*Zachary Izzo,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文首次量化分析了Transformer模型实现长度泛化所需的训练序列长度边界，在不同设置下证明了当长序列行为能被训练时短序列行为模拟时，长度泛化就会发生。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明Transformer在训练序列长度超过某个阈值后能实现长度泛化，但该阈值的大小一直未知。本文旨在首次量化分析实现长度泛化所需的训练序列长度边界。

Method: 分析了多种不同设置下的长度泛化问题：ℓ∞误差控制vs平均误差控制、无限精度softmax注意力vs有限精度注意力、单层vs双层Transformer。通过证明当长序列行为能被短序列行为模拟时泛化发生。

Result: 在所有分析场景中，都证明了长度泛化会在长序列内部行为能被训练时短序列行为模拟时发生。给出了Transformer实现泛化所需训练数据长度的定性估计，并通过实验验证了这些见解。

Conclusion: 研究结果深化了对Transformer外推机制的理论理解，并形式化了更复杂任务需要更丰富训练数据才能泛化的直觉认识。

Abstract: We study the problem of length generalization (LG) in transformers: the
ability of a model trained on shorter sequences to maintain performance when
evaluated on much longer, previously unseen inputs. Prior work by Huang et al.
(2025) established that transformers eventually achieve length generalization
once the training sequence length exceeds some finite threshold, but left open
the question of how large it must be. In this work, we provide the first
quantitative bounds on the required training length for length generalization
to occur. Motivated by previous empirical and theoretical work, we analyze LG
in several distinct problem settings: $\ell_\infty$ error control vs. average
error control over an input distribution, infinite-precision softmax attention
vs. finite-precision attention (which reduces to an argmax) in the transformer,
and one- vs. two-layer transformers. In all scenarios, we prove that LG occurs
when the internal behavior of the transformer on longer sequences can be
"simulated" by its behavior on shorter sequences seen during training. Our
bounds give qualitative estimates for the length of training data required for
a transformer to generalize, and we verify these insights empirically. These
results sharpen our theoretical understanding of the mechanisms underlying
extrapolation in transformers, and formalize the intuition that richer training
data is required for generalization on more complex tasks.

</details>


### [5] [Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores](https://arxiv.org/abs/2510.27145)
*Sein Kwon,Seulgi Baek,Hyunseo Yang,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: 提出RelTune框架，通过关系图表示参数依赖关系，使用GNN学习性能相关语义嵌入，结合混合分数引导的贝叶斯优化，实现更高效的数据库参数自动调优。


<details>
  <summary>Details</summary>
Motivation: 现有数据库参数自动调优方法存在三个主要局限：忽略参数间依赖关系、只优化少数参数、贝叶斯优化的代理模型不稳定。这些限制影响了调优效果和效率。

Method: 1) 构建参数关系图表示依赖关系；2) 使用GNN学习性能相关语义嵌入；3) 提出混合分数引导贝叶斯优化(HBO)，结合代理预测和亲和度分数。

Result: 在多个DBMS和工作负载上的实验表明，RelTune比传统BO方法收敛更快、优化效率更高，在所有评估场景中都达到了最先进的性能。

Conclusion: RelTune通过建模参数依赖关系和引入混合优化策略，显著提升了数据库参数自动调优的效果，为高维参数空间优化提供了有效解决方案。

Abstract: Database Management Systems (DBMSs) are fundamental for managing large-scale
and heterogeneous data, and their performance is critically influenced by
configuration parameters. Effective tuning of these parameters is essential for
adapting to diverse workloads and maximizing throughput while minimizing
latency. Recent research has focused on automated configuration optimization
using machine learning; however, existing approaches still exhibit several key
limitations. Most tuning frameworks disregard the dependencies among
parameters, assuming that each operates independently. This simplification
prevents optimizers from leveraging relational effects across parameters,
limiting their capacity to capture performancesensitive interactions. Moreover,
to reduce the complexity of the high-dimensional search space, prior work often
selects only the top few parameters for optimization, overlooking others that
contribute meaningfully to performance. Bayesian Optimization (BO), the most
common method for automatic tuning, is also constrained by its reliance on
surrogate models, which can lead to unstable predictions and inefficient
exploration. To overcome these limitations, we propose RelTune, a novel
framework that represents parameter dependencies as a Relational Graph and
learns GNN-based latent embeddings that encode performancerelevant semantics.
RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),
which combines surrogate predictions with an Affinity Score measuring proximity
to previously high-performing configurations. Experimental results on multiple
DBMSs and workloads demonstrate that RelTune achieves faster convergence and
higher optimization efficiency than conventional BO-based methods, achieving
state-of-the-art performance across all evaluated scenarios.

</details>


### [6] [MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss](https://arxiv.org/abs/2510.27443)
*Meenu Ravi,Shailik Sarkar,Yanshen Sun,Vaishnavi Singh,Chang-Tien Lu*

Main category: cs.LG

TL;DR: 提出了MVeLMA多模态植被损失建模架构，用于预测野火后的县级植被损失，通过多模态特征集成和堆叠集成架构，结合概率建模进行不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 理解野火后植被损失对于制定有效生态恢复策略至关重要，但现有研究未充分探索所有影响因素及其相互作用，且预测模型缺乏可解释性，限制了实际应用。

Method: 使用多模态特征集成管道和堆叠集成架构，结合概率建模进行不确定性估计，生成植被损失置信度地图。

Result: 模型在预测野火后植被损失方面优于多个最先进和基线模型，能够识别高风险县区。

Conclusion: 该研究成果可为未来灾害救援规划、生态政策制定和野生动物恢复管理提供信息支持。

Abstract: Understanding post-wildfire vegetation loss is critical for developing
effective ecological recovery strategies and is often challenging due to the
extended time and effort required to capture the evolving ecosystem features.
Recent works in this area have not fully explored all the contributing factors,
their modalities, and interactions with each other. Furthermore, most research
in this domain is limited by a lack of interpretability in predictive modeling,
making it less useful in real-world settings. In this work, we propose a novel
end-to-end ML pipeline called MVeLMA (\textbf{M}ultimodal \textbf{Ve}getation
\textbf{L}oss \textbf{M}odeling \textbf{A}rchitecture) to predict county-wise
vegetation loss from fire events. MVeLMA uses a multimodal feature integration
pipeline and a stacked ensemble-based architecture to capture different
modalities while also incorporating uncertainty estimation through
probabilistic modeling. Through comprehensive experiments, we show that our
model outperforms several state-of-the-art (SOTA) and baseline models in
predicting post-wildfire vegetation loss. Furthermore, we generate vegetation
loss confidence maps to identify high-risk counties, thereby helping targeted
recovery efforts. The findings of this work have the potential to inform future
disaster relief planning, ecological policy development, and wildlife recovery
management.

</details>


### [7] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出通过重采样方法研究推理模型的思维链分布，而非仅分析单一思维链。通过多个案例研究展示了重采样在因果分析、模型干预和推理理解方面的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常只分析单一思维链，但模型实际上定义了多种可能的思维链分布。单一样本分析不足以理解因果影响和底层计算过程，需要研究完整的分布。

Method: 使用重采样方法研究模型决策，包括：1）在"代理错位"场景中重采样特定句子测量下游影响；2）比较离策略干预与重采样方法的效果；3）引入弹性度量防止类似内容重复出现；4）适配因果中介分析研究隐性影响。

Result: 发现：1）自我保护句子的因果影响很小；2）离策略干预效果小而不稳定；3）关键规划语句难以移除但移除后影响很大；4）未明确提及的提示对思维链有持续累积影响。

Conclusion: 通过重采样研究分布能够实现可靠的因果分析、更清晰的模型推理叙述，以及原则性的思维链干预方法。

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


### [8] [DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm](https://arxiv.org/abs/2510.27504)
*Junkang Liu,Yuxuan Tian,Fanhua Shang,Yuanyuan Liu,Hongying Liu,Junchao Zhou,Daorui Ding*

Main category: cs.LG

TL;DR: 提出了DP-FedPGN算法，通过引入全局梯度范数惩罚来寻找全局平坦最小值，缓解差分隐私联邦学习中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的客户端级差分隐私联邦学习方法会导致更尖锐的损失景观，降低模型泛化能力。局部平坦性不能反映全局平坦性，需要寻找全局平坦最小值。

Method: 在局部损失中引入全局梯度范数惩罚，寻找全局平坦最小值，同时减少局部更新范数，降低梯度裁剪误差。使用Rényi DP提供严格隐私保证。

Result: 在ResNet和Transformer模型上进行了有效性测试，在六个视觉和自然语言处理任务中相比现有最先进算法取得了显著改进。

Conclusion: DP-FedPGN算法不仅找到了更平坦的全局最小值，还减少了局部更新范数，缓解了差分隐私导致的性能下降，同时消除了数据异构性的影响并实现了快速收敛。

Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the
leakage of sensitive information, Client-level Differentially Private Federated
Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually
result in sharper loss landscapes, which leads to a decrease in model
generalization after differential privacy protection. By using Sharpness Aware
Minimization (SAM), the current popular federated learning methods are to find
a local flat minimum value to alleviate this problem. However, the local
flatness may not reflect the global flatness in CL-DPFL. Therefore, to address
this issue and seek global flat minima of models, we propose a new CL-DPFL
algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to
the local loss to find the global flat minimum. Moreover, by using our global
gradient norm penalty, we not only find a flatter global minimum but also
reduce the locally updated norm, which means that we further reduce the error
of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN
mitigates the performance degradation caused by DP. Meanwhile, the proposed
DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves
fast convergence. We also use R\'enyi DP to provide strict privacy guarantees
and provide sensitivity analysis for local updates. Finally, we conduct
effectiveness tests on both ResNet and Transformer models, and achieve
significant improvements in six visual and natural language processing tasks
compared to existing state-of-the-art algorithms. The code is available at
https://github.com/junkangLiu0/DP-FedPGN

</details>


### [9] [Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems](https://arxiv.org/abs/2510.27659)
*Alireza Saleh Abadi,Leen-Kiat Soh*

Main category: cs.LG

TL;DR: 该论文分析了多智能体强化学习中的开放性问题及其对信用分配问题的影响，通过概念分析和实证研究揭示了开放性导致信用分配错误和性能下降。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习快速发展背景下，理解开放系统的动态特性至关重要。传统信用分配方法假设静态环境，无法适应开放系统中智能体、任务和类型的动态变化。

Method: 首先进行概念分析，引入新的开放性子类别来描述环境非平稳性和团队组成变化；然后使用代表性时间和结构算法在开放环境中进行实证研究。

Result: 实证结果表明开放性直接导致信用分配错误，表现为损失函数不稳定和显著的性能下降。

Conclusion: 开放性是多智能体强化学习信用分配问题的重要挑战，需要开发新的方法来应对动态环境中的信用分配问题。

Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Discriminative Rule Learning for Outcome-Guided Process Model Discovery](https://arxiv.org/abs/2510.27343)
*Ali Norouzifar,Wil van der Aalst*

Main category: cs.AI

TL;DR: 提出一种基于结果感知的过程发现方法，通过区分期望和不期望的过程执行，学习可解释的判别规则来分组轨迹，并为每组分别发现过程模型，从而揭示驱动期望和不期望执行的关键模式。


<details>
  <summary>Details</summary>
Motivation: 传统过程发现方法不考虑执行结果，导致模型难以捕捉期望和不期望行为之间的关键差异，不适合一致性检查和性能分析。区分这两种执行结果可以指导过程发现更关注结果。

Method: 学习基于控制流特征的可解释判别规则，将具有相似期望性特征的轨迹分组，然后在每个组内分别应用过程发现技术。

Result: 该方法在多个真实事件日志上得到验证，能够有效隔离和可视化关键过程模式，生成聚焦且可解释的模型。

Conclusion: 通过结果感知的过程发现方法，可以揭示驱动期望和不期望过程执行的结构性差异，为过程改进提供有价值的见解。

Abstract: Event logs extracted from information systems offer a rich foundation for
understanding and improving business processes. In many real-world
applications, it is possible to distinguish between desirable and undesirable
process executions, where desirable traces reflect efficient or compliant
behavior, and undesirable ones may involve inefficiencies, rule violations,
delays, or resource waste. This distinction presents an opportunity to guide
process discovery in a more outcome-aware manner. Discovering a single process
model without considering outcomes can yield representations poorly suited for
conformance checking and performance analysis, as they fail to capture critical
behavioral differences. Moreover, prioritizing one behavior over the other may
obscure structural distinctions vital for understanding process outcomes. By
learning interpretable discriminative rules over control-flow features, we
group traces with similar desirability profiles and apply process discovery
separately within each group. This results in focused and interpretable models
that reveal the drivers of both desirable and undesirable executions. The
approach is implemented as a publicly available tool and it is evaluated on
multiple real-life event logs, demonstrating its effectiveness in isolating and
visualizing critical process patterns.

</details>


### [11] [Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training](https://arxiv.org/abs/2510.27630)
*Dayuan Fu,Yunze Wu,Xiaojie Cai,Lyumanshan Ye,Shijie Xia,Zhen Huang,Weiye Si,Tianze Xu,Jie Sun,Keyu Li,Mohan Jiang,Junfei Wang,Qishuo Hua,Pengrui Lu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: Apollo是一个集成异步人类指导与动作级数据过滤的采样框架，用于训练LLM代理处理长时域、领域专业化任务，相比传统方法显著提升了训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLM代理的方法存在两个问题：基于行为克隆的方法需要密集人工标注，成本过高；基于结果驱动的采样方法由于有效轨迹稀少而容易失效。

Method: Apollo框架允许人类仅在代理偏离正确轨迹时进行干预，提供先验知识和策略建议，同时使用监督控制过滤次优动作防止错误传播。

Result: 在InnovatorBench上的实验表明，Apollo训练GLM-4.5模型相比未训练基线提升50%以上，相比无人交互变体提升28%。

Conclusion: Apollo证明了人类在环采样在长时域、领域专业化任务中的关键作用，其设计具有鲁棒性和有效性。

Abstract: Large Language Model (LLM) agents have recently shown strong potential in
domains such as automated coding, deep research, and graphical user interface
manipulation. However, training them to succeed on long-horizon,
domain-specialized tasks remains challenging. Current methods primarily fall
into two categories. The first relies on dense human annotations through
behavior cloning, which is prohibitively expensive for long-horizon tasks that
can take days or months. The second depends on outcome-driven sampling, which
often collapses due to the rarity of valid positive trajectories on
domain-specialized tasks. We introduce Apollo, a sampling framework that
integrates asynchronous human guidance with action-level data filtering.
Instead of requiring annotators to shadow every step, Apollo allows them to
intervene only when the agent drifts from a promising trajectory, by providing
prior knowledge, strategic advice, etc. This lightweight design makes it
possible to sustain interactions for over 30 hours and produces valuable
trajectories at a lower cost. Apollo then applies supervision control to filter
out sub-optimal actions and prevent error propagation. Together, these
components enable reliable and effective data collection in long-horizon
environments. To demonstrate the effectiveness of Apollo, we evaluate it using
InnovatorBench. Our experiments show that when applied to train the GLM-4.5
model on InnovatorBench, Apollo achieves more than a 50% improvement over the
untrained baseline and a 28% improvement over a variant trained without human
interaction. These results highlight the critical role of human-in-the-loop
sampling and the robustness of Apollo's design in handling long-horizon,
domain-specialized tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 提出了一种基于语义框架识别医疗记录中应报告事件的方法，应用于检测性别暴力事件，在2100万句巴西葡萄牙语医疗记录中验证有效，精确度达0.726。


<details>
  <summary>Details</summary>
Motivation: 解决医疗记录中性别暴力事件漏报问题，通过自动化方法提高公共卫生监测效率。

Method: 使用语义框架定义细粒度模式，在非结构化医疗记录文本中搜索匹配，构建透明、高效、低碳、语言无关的处理流程。

Result: 定义了8个模式，在2100万句语料中识别暴力报告，经语言学家手动评估，整体精确度为0.726。

Conclusion: 该方法能有效识别暴力事件报告，具有鲁棒性，可轻松适应其他健康监测场景，促进NLP在公共卫生系统中的伦理和可解释应用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [13] [Characterizing Selective Refusal Bias in Large Language Models](https://arxiv.org/abs/2510.27087)
*Adel Khorramrouz,Sharon Levy*

Main category: cs.CL

TL;DR: 论文研究了LLM安全护栏中的选择性拒绝偏见，发现模型在拒绝生成针对不同人口群体的有害内容时存在不一致性，这可能导致新的偏见问题。


<details>
  <summary>Details</summary>
Motivation: LLM的安全护栏旨在防止大规模生成有害内容，但这些措施可能无意中引入或反映新的偏见，因为LLM可能拒绝针对某些人口群体而非其他群体生成有害内容。

Method: 通过分析针对个体和交叉人口群体的拒绝率、LLM响应类型以及生成拒绝的长度，来探索LLM护栏中的选择性拒绝偏见。

Result: 结果显示在性别、性取向、国籍和宗教属性方面存在选择性拒绝偏见的证据。通过间接攻击针对先前被拒绝的群体，进一步调查了安全影响。

Conclusion: 研究强调需要在不同人口群体之间实现更公平和稳健的安全护栏性能。

Abstract: Safety guardrails in large language models(LLMs) are developed to prevent
malicious users from generating toxic content at a large scale. However, these
measures can inadvertently introduce or reflect new biases, as LLMs may refuse
to generate harmful content targeting some demographic groups and not others.
We explore this selective refusal bias in LLM guardrails through the lens of
refusal rates of targeted individual and intersectional demographic groups,
types of LLM responses, and length of generated refusals. Our results show
evidence of selective refusal bias across gender, sexual orientation,
nationality, and religion attributes. This leads us to investigate additional
safety implications via an indirect attack, where we target previously refused
groups. Our findings emphasize the need for more equitable and robust
performance in safety guardrails across demographic groups.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict](https://arxiv.org/abs/2510.27333)
*Hao Cheng,Yanbo Jiang,Qingyuan Shi,Qingwen Meng,Keyu Chen,Wenhao Yu,Jianqiang Wang,Sifa Zheng*

Main category: cs.RO

TL;DR: 提出改进的紧急指数(MEI)来量化横向冲突中的规避努力，相比现有指标能更准确地评估自动驾驶在城市场景中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有关键性指标主要针对纵向冲突，难以准确量化城市场景中常见的横向冲突风险，需要开发专门针对横向冲突的评估指标。

Method: 改进原始紧急指数(EI)，优化规避动作可用时间的估计方法，在基于Argoverse-2的公开横向冲突数据集上验证，包含1500多个高质量AV冲突案例和500多个关键事件。

Result: MEI在准确量化关键性和捕捉风险演变方面持续优于既有的ACT和PET指标。

Conclusion: MEI是评估城市冲突和增强自动驾驶安全评估框架的有前景指标。

Abstract: Effective, reliable, and efficient evaluation of autonomous driving safety is
essential to demonstrate its trustworthiness. Criticality metrics provide an
objective means of assessing safety. However, as existing metrics primarily
target longitudinal conflicts, accurately quantifying the risks of lateral
conflicts - prevalent in urban settings - remains challenging. This paper
proposes the Modified-Emergency Index (MEI), a metric designed to quantify
evasive effort in lateral conflicts. Compared to the original Emergency Index
(EI), MEI refines the estimation of the time available for evasive maneuvers,
enabling more precise risk quantification. We validate MEI on a public lateral
conflict dataset based on Argoverse-2, from which we extract over 1,500
high-quality AV conflict cases, including more than 500 critical events. MEI is
then compared with the well-established ACT and the widely used PET metrics.
Results show that MEI consistently outperforms them in accurately quantifying
criticality and capturing risk evolution. Overall, these findings highlight MEI
as a promising metric for evaluating urban conflicts and enhancing the safety
assessment framework for autonomous driving. The open-source implementation is
available at https://github.com/AutoChengh/MEI.

</details>
