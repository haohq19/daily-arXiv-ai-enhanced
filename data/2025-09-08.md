<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Facial Emotion Recognition does not detect feeling unsafe in automated driving](https://arxiv.org/abs/2509.04490)
*Abel van Elburg,Konstantinos Gkentsidis,Mathieu Sarrazin,Sarah Barendswaard,Varun Kotian,Riender Happee*

Main category: cs.CV

TL;DR: 通过驾驶模拟器实验研究自动驾驶风险感知，发现车辆运动和皮电导可以预测主观风险感知，而面部表情识别方法不可靠


<details>
  <summary>Details</summary>
Motivation: 研究自动驾驶车辆的风险感知对公众接受度的重要影响

Method: 使用32名参与者进行驾驶模拟器实验，收集主观舒适度评分、运动数据、面部表情、皮电导、心率和眼动跟踪数据

Result: 动态驾驶风格导致更强的不舒适感，行人跨路事件增加了风险感知；面部表情识别方法不可靠，神经网络模型能够准确预测主观风险感知

Conclusion: 车辆运动和皮电导数据可以对象地评估自动驾驶风险感知，为未来研究提供了方向

Abstract: Trust and perceived safety play a crucial role in the public acceptance of
automated vehicles. To understand perceived risk, an experiment was conducted
using a driving simulator under two automated driving styles and optionally
introducing a crossing pedestrian. Data was collected from 32 participants,
consisting of continuous subjective comfort ratings, motion, webcam footage for
facial expression, skin conductance, heart rate, and eye tracking. The
continuous subjective perceived risk ratings showed significant discomfort
associated with perceived risk during cornering and braking followed by relief
or even positive comfort on continuing the ride. The dynamic driving style
induced a stronger discomfort as compared to the calm driving style. The
crossing pedestrian did not affect discomfort with the calm driving style but
doubled the comfort decrement with the dynamic driving style. This illustrates
the importance of consequences of critical interactions in risk perception.
Facial expression was successfully analyzed for 24 participants but most
(15/24) did not show any detectable facial reaction to the critical event.
Among the 9 participants who did, 8 showed a Happy expression, and only 4
showed a Surprise expression. Fear was never dominant. This indicates that
facial expression recognition is not a reliable method for assessing perceived
risk in automated vehicles. To predict perceived risk a neural network model
was implemented using vehicle motion and skin conductance. The model correlated
well with reported perceived risk, demonstrating its potential for objective
perceived risk assessment in automated vehicles, reducing subjective bias and
highlighting areas for future research.

</details>


### [2] [Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning](https://arxiv.org/abs/2509.04602)
*MinJu Jeon,Si-Woo Kim,Ye-Chan Kim,HyunGee Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: Sali4Vid是一个用于密集视频描述的新框架，通过显著性感知视频重加权和基于语义的自适应字幕检索来解决现有方法的局限性，在YouCook2和ViTT数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端密集视频描述方法存在两个主要问题：(1)只对文本应用时间戳监督，而将所有视频帧同等对待；(2)从固定大小的视频块中检索字幕，忽略了场景转换。

Method: 提出了Sali4Vid框架，包含两个核心组件：显著性感知视频重加权（将时间戳标注转换为基于sigmoid的帧重要性权重）和基于语义的自适应字幕检索（通过帧相似性分割视频以捕捉场景转换并改进字幕检索）。

Result: 在YouCook2和ViTT数据集上实现了最先进的性能，证明了联合改进视频加权和检索对密集视频描述的有效性。

Conclusion: Sali4Vid通过简单而有效的方法解决了现有密集视频描述方法的局限性，显著提升了性能，展示了显著性感知框架的价值。

Abstract: Dense video captioning aims to temporally localize events in video and
generate captions for each event. While recent works propose end-to-end models,
they suffer from two limitations: (1) applying timestamp supervision only to
text while treating all video frames equally, and (2) retrieving captions from
fixed-size video chunks, overlooking scene transitions. To address these, we
propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce
Saliency-aware Video Reweighting, which converts timestamp annotations into
sigmoid-based frame importance weights, and Semantic-based Adaptive Caption
Retrieval, which segments videos by frame similarity to capture scene
transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art
results on YouCook2 and ViTT, demonstrating the benefit of jointly improving
video weighting and retrieval for dense video captioning

</details>


### [3] [WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches](https://arxiv.org/abs/2509.04736)
*Taeyoung Yeon,Vasco Xu,Henry Hoffmann,Karan Ahuja*

Main category: cs.CV

TL;DR: WatchHAR是一个完全在智能手表上运行的音频和惯性传感器活动识别系统，通过端到端优化实现了5倍处理速度提升，在25+活动类别上保持90%以上准确率。


<details>
  <summary>Details</summary>
Motivation: 解决智能手表在无约束环境中完全本地化运行的人体活动识别系统难题，处理隐私和延迟问题。

Method: 引入新颖架构，将传感器数据预处理和推理统一为端到端可训练模块，优化流水线每个组件。

Result: 处理时间：活动事件检测9.3毫秒，多模态活动分类11.8毫秒，性能优于最先进模型。

Conclusion: 该研究推动了设备端活动识别技术，实现了智能手表作为独立、隐私保护和最小侵入性连续活动跟踪设备的潜力。

Abstract: Despite advances in practical and multimodal fine-grained Human Activity
Recognition (HAR), a system that runs entirely on smartwatches in unconstrained
environments remains elusive. We present WatchHAR, an audio and inertial-based
HAR system that operates fully on smartwatches, addressing privacy and latency
issues associated with external data processing. By optimizing each component
of the pipeline, WatchHAR achieves compounding performance gains. We introduce
a novel architecture that unifies sensor data preprocessing and inference into
an end-to-end trainable module, achieving 5x faster processing while
maintaining over 90% accuracy across more than 25 activity classes. WatchHAR
outperforms state-of-the-art models for event detection and activity
classification while running directly on the smartwatch, achieving 9.3 ms
processing time for activity event detection and 11.8 ms for multimodal
activity classification. This research advances on-device activity recognition,
realizing smartwatches' potential as standalone, privacy-aware, and
minimally-invasive continuous activity tracking devices.

</details>


### [4] [Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations](https://arxiv.org/abs/2509.04848)
*Enze Ye,Wei Lin,Shaochi Ren,Yakun Liu,Xiaoping Li,Hao Wang,He Sun,Feng Pan*

Main category: cs.CV

TL;DR: OmniFHT是一种无需姿态信息的3D折射率重建框架，通过傅里叶衍射定理和隐式神经表示，实现高通量流式细胞术中对任意几何形状和多轴旋转细胞的高保真成像。


<details>
  <summary>Details</summary>
Motivation: 现有3D定量相位成像方法假设细胞进行均匀单轴旋转且需要已知每帧姿态，这限制了方法对近球形细胞的适用性，无法准确成像具有复杂旋转的不规则形状细胞，导致只能分析细胞群体的子集。

Method: 采用傅里叶衍射定理和隐式神经表示(INRs)，通过联合优化每个细胞的未知旋转轨迹和体积结构，在弱散射假设下支持任意细胞几何形状和多轴旋转。连续表示允许从稀疏采样投影和有限角度覆盖进行准确重建。

Result: OmniFHT能够使用少至10个视图或仅120度角度范围产生高保真结果，首次实现了对整个流动细胞群体的原位高通量断层成像。

Conclusion: 该方法为流式细胞术平台提供了可扩展且无偏见的无标记形态计量分析解决方案，突破了现有技术的限制。

Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables
label-free, volumetric characterization of individual cells by reconstructing
their refractive index (RI) distributions from multiple viewing angles during
flow through microfluidic channels. However, current imaging methods assume
that cells undergo uniform, single-axis rotation, which require their poses to
be known at each frame. This assumption restricts applicability to
near-spherical cells and prevents accurate imaging of irregularly shaped cells
with complex rotations. As a result, only a subset of the cellular population
can be analyzed, limiting the ability of flow-based assays to perform robust
statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction
framework that leverages the Fourier diffraction theorem and implicit neural
representations (INRs) for high-throughput flow cytometry tomographic imaging.
By jointly optimizing each cell's unknown rotational trajectory and volumetric
structure under weak scattering assumptions, OmniFHT supports arbitrary cell
geometries and multi-axis rotations. Its continuous representation also allows
accurate reconstruction from sparsely sampled projections and restricted
angular coverage, producing high-fidelity results with as few as 10 views or
only 120 degrees of angular range. OmniFHT enables, for the first time, in
situ, high-throughput tomographic imaging of entire flowing cell populations,
providing a scalable and unbiased solution for label-free morphometric analysis
in flow cytometry platforms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series](https://arxiv.org/abs/2509.04921)
*Yuki Takemoto*

Main category: cs.LG

TL;DR: 本研究提出了一种通过生成人工混沌时间序列和重采样技术来模拟金融时间序列数据的方法，使用100亿训练样本进行大规模预训练，并在比特币交易数据上实现了零样本预测，交易策略表现显著优于自相关模型。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测具有挑战性，现有时间序列基础模型需要适应各种预测任务，同时认识到现实世界时间序列具有混沌特性，需要开发新方法来建模金融时间序列。

Method: 生成人工混沌时间序列，应用重采样技术模拟金融时间序列数据作为训练样本，增加重采样间隔扩展预测范围，使用100亿训练样本进行大规模预训练，在真实比特币交易数据上进行零样本预测。

Result: 基于预测结果的简单交易策略在盈利能力评估中表现出显著优于自相关模型的性能，在大规模预训练过程中观察到类似缩放定律的现象。

Conclusion: 如果这种缩放定律在各种混沌模型中都具有鲁棒性，表明通过投入大量计算资源可以预测近未来事件，未来研究应关注更大规模训练和验证该缩放定律在不同混沌模型中的适用性。

Abstract: Time series forecasting plays a critical role in decision-making processes
across diverse fields including meteorology, traffic, electricity, economics,
finance, and so on. Especially, predicting returns on financial instruments is
a challenging problem. Some researchers have proposed time series foundation
models applicable to various forecasting tasks. Simultaneously, based on the
recognition that real-world time series exhibit chaotic properties, methods
have been developed to artificially generate synthetic chaotic time series,
construct diverse datasets and train models. In this study, we propose a
methodology for modeling financial time series by generating artificial chaotic
time series and applying resampling techniques to simulate financial time
series data, which we then use as training samples. Increasing the resampling
interval to extend predictive horizons, we conducted large-scale pre-training
using 10 billion training samples for each case. We subsequently created test
datasets for multiple timeframes using actual Bitcoin trade data and performed
zero-shot prediction without re-training the pre-trained model. The results of
evaluating the profitability of a simple trading strategy based on these
predictions demonstrated significant performance improvements over
autocorrelation models. During the large-scale pre-training process, we
observed a scaling law-like phenomenon that we can achieve predictive
performance at a certain level with extended predictive horizons for chaotic
time series by increasing the number of training samples exponentially. If this
scaling law proves robust and holds true across various chaotic models, it
suggests the potential to predict near-future events by investing substantial
computational resources. Future research should focus on further large-scale
training and verifying the applicability of this scaling law to diverse chaotic
models.

</details>


### [6] [Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization](https://arxiv.org/abs/2509.04977)
*Shuaicheng Niu,Guohao Chen,Deyu Chen,Yifan Zhang,Jiaxiang Wu,Zhiquan Wen,Yaofo Chen,Peilin Zhao,Chunyan Miao,Mingkui Tan*

Main category: cs.LG

TL;DR: 本文提出SAR和SAR^2方法解决测试时自适应(TTA)在混合分布偏移、小批量数据和在线不平衡标签分布等场景下的不稳定性问题，通过梯度感知和表示正则化实现稳定高效的TTA。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在真实部署中面临三个关键挑战：混合分布偏移、小批量数据和在线不平衡标签分布，导致模型性能下降甚至崩溃。研究发现批归一化层是阻碍TTA稳定性的关键因素，而使用组归一化或层归一化后仍存在模型崩溃问题。

Method: 提出SAR方法：1) 通过梯度感知移除噪声样本；2) 使用平坦最小化使模型对剩余噪声样本鲁棒。进一步提出SAR^2方法：1) 冗余正则器降低特征维度间相关性；2) 不公平正则器最大化原型质心的预测熵，惩罚偏向特定类的表示。

Result: 实验结果表明，所提方法在多种野生测试场景下比现有方法表现更稳定，且计算效率高。

Conclusion: SAR和SAR^2方法有效解决了TTA在挑战性测试环境中的稳定性问题，通过梯度筛选和平坦优化防止模型崩溃，通过表示正则化保持特征多样性和公平性，为TTA的实际部署提供了可靠解决方案。

Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model
performance when test data have: 1) mixed distribution shifts, 2) small batch
sizes, 3) online imbalanced label distribution shifts. This is often a key
obstacle preventing existing TTA methods from being deployed in the real world.
In this paper, we investigate the unstable reasons and find that the batch norm
layer is a crucial factor hindering TTA stability. Conversely, TTA can perform
more stably with batch-agnostic norm layers, i.e., group or layer norm.
However, we observe that TTA with group and layer norms does not always succeed
and still suffers many failure cases, i.e., the model collapses into trivial
solutions by assigning the same class label for all samples. By digging into
this, we find that, during the collapse process: 1) the model gradients often
undergo an initial explosion followed by rapid degradation, suggesting that
certain noisy test samples with large gradients may disrupt adaptation; and 2)
the model representations tend to exhibit high correlations and classification
bias. To address this, we first propose a sharpness-aware and reliable entropy
minimization method, called SAR, for stabilizing TTA from two aspects: 1)
remove partial noisy samples with large gradients, 2) encourage model weights
to go to a flat minimum so that the model is robust to the remaining noisy
samples. Based on SAR, we further introduce SAR^2 to prevent representation
collapse with two regularizers: 1) a redundancy regularizer to reduce
inter-dimensional correlations among centroid-invariant features; and 2) an
inequity regularizer to maximize the prediction entropy of a prototype
centroid, thereby penalizing biased representations toward any specific class.
Promising results demonstrate that our methods perform more stably over prior
methods and are computationally efficient under the above wild test scenarios.

</details>


### [7] [MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer](https://arxiv.org/abs/2509.05037)
*Noorul Wahab,Ethar Alzaid,Jiaqi Lv,Adam Shephard,Shan E Ahmed Raza*

Main category: cs.LG

TL;DR: MultiSurv是一个多模态深度生存模型，使用DeepHit架构结合投影层和跨模态注意力机制，整合临床、MRI、RNA-seq和病理学数据，用于前列腺癌和膀胱癌的复发时间预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测癌症患者的时间-事件结果对治疗规划和患者管理至关重要，需要整合多模态异质数据来捕捉互补的预后信号。

Method: 采用DeepHit生存模型，加入投影层和跨模态交叉注意力机制，整合临床数据、MRI影像、RNA-seq和全切片病理特征等多模态数据。

Result: 在前列腺癌生化复发预测任务中，5折交叉验证C-index达到0.843，开发集达到0.818；在膀胱癌复发预测任务中，5折交叉验证C-index为0.662，开发集为0.457。

Conclusion: 多模态整合与深度生存学习相结合为前列腺癌和膀胱癌的个性化风险分层提供了有前景的途径，该框架可广泛应用于涉及异质生物医学数据的生存预测任务。

Abstract: Accurate prediction of time-to-event outcomes is a central challenge in
oncology, with significant implications for treatment planning and patient
management. In this work, we present MultiSurv, a multimodal deep survival
model utilising DeepHit with a projection layer and inter-modality
cross-attention, which integrates heterogeneous patient data, including
clinical, MRI, RNA-seq and whole-slide pathology features. The model is
designed to capture complementary prognostic signals across modalities and
estimate individualised time-to-biochemical recurrence in prostate cancer and
time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the
context of the CHIMERA Grand Challenge, across two of the three provided tasks.
For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed
framework achieved a concordance index (C-index) of 0.843 on 5-folds
cross-validation and 0.818 on CHIMERA development set, demonstrating robust
discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the
model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on
development set, highlighting its adaptability and potential for clinical
translation. These results suggest that leveraging multimodal integration with
deep survival learning provides a promising pathway toward personalised risk
stratification in prostate and bladder cancer. Beyond the challenge setting,
our framework is broadly applicable to survival prediction tasks involving
heterogeneous biomedical data.

</details>


### [8] [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
*Yuqi Pan,Yupeng Feng,Jinghao Zhuang,Siyu Ding,Zehao Liu,Bohan Sun,Yuhong Chou,Han Xu,Xuerui Qiu,Anlin Deng,Anjie Hu,Peng Zhou,Man Yao,Jibin Wu,Jian Yang,Guoliang Sun,Bo Xu,Guoqi Li*

Main category: cs.LG

TL;DR: SpikingBrain是一个受大脑启发的脉冲神经网络模型家族，通过线性/混合线性注意力架构、脉冲神经元和专用硬件优化，解决了Transformer模型在长序列处理中的二次计算复杂度和线性内存增长问题，在非NVIDIA平台上实现了高效的长上下文训练和推理。


<details>
  <summary>Details</summary>
Motivation: 主流Transformer大语言模型面临训练计算随序列长度二次增长、推理内存线性增长的效率瓶颈，且在非NVIDIA平台上构建大模型存在稳定性和效率挑战。

Method: 采用线性/混合线性注意力架构和自适应脉冲神经元；开发高效的基于转换的训练流水线和专用脉冲编码框架；针对MetaX硬件定制训练框架、算子库和并行策略。

Result: 开发了7B线性LLM和76B混合线性MoE LLM，仅用约150B token进行持续预训练即达到开源Transformer基线性能，4M token序列的首token时间加速100倍以上，训练稳定，模型FLOPs利用率达23.4%，脉冲稀疏度达69.15%。

Conclusion: 这项工作展示了大脑启发机制在推动下一代高效可扩展大模型设计方面的潜力，证明了在非NVIDIA平台上开发大规模LLM的可行性。

Abstract: Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records](https://arxiv.org/abs/2509.04485)
*Chris Sainsbury,Andreas Karwath*

Main category: cs.CL

TL;DR: ASCENDgpt是一个基于transformer的心血管风险预测模型，通过创新的表型感知标记化方案将47,155个原始ICD代码映射到176个临床有意义的表型标记，在保持语义信息的同时实现了99.6%的代码压缩。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录(EHR)中ICD代码数量庞大且语义复杂的问题，开发一个既能保持临床可解释性又能提高计算效率的心血管风险预测模型。

Method: 采用表型感知标记化方案压缩ICD代码，使用掩码语言建模目标对19,402名个体的序列进行预训练，然后针对五种心血管结局进行时间到事件预测的微调。

Result: 在测试集上取得了优异的区分性能，平均C-index为0.816（心肌梗死:0.792，卒中:0.824，MACE:0.800，心血管死亡:0.842，全因死亡率:0.824）。

Conclusion: 基于表型的方法能够在保持计算效率的同时实现临床可解释的预测，证明了领域特定标记化和预训练在EHR风险预测任务中的有效性。

Abstract: We present ASCENDgpt, a transformer-based model specifically designed for
cardiovascular risk prediction from longitudinal electronic health records
(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme
that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,
achieving 99.6\% consolidation of diagnosis codes while preserving semantic
information. This phenotype mapping contributes to a total vocabulary of 10,442
tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We
pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a
masked language modeling objective, then fine-tune for time-to-event prediction
of five cardiovascular outcomes: myocardial infarction (MI), stroke, major
adverse cardiovascular events (MACE), cardiovascular death, and all-cause
mortality. Our model achieves excellent discrimination on the held-out test set
with an average C-index of 0.816, demonstrating strong performance across all
outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,
all-cause mortality: 0.824). The phenotype-based approach enables clinically
interpretable predictions while maintaining computational efficiency. Our work
demonstrates the effectiveness of domain-specific tokenization and pretraining
for EHR-based risk prediction tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [10] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 这篇论文提出了一种流动性预测控制框架，采用集合基状态估计和线性规划来处理深度学习感知模块的非高斯噪声，在硬件实验中显著提升了控制性能。


<details>
  <summary>Details</summary>
Motivation: 因深度学习感知模块的噪声通常具有偏差、重尾等非高斯特征，传统高斯噪声假设导致不准确的不确定性量化，影响控制系统的安全性和稳定性。

Method: 使用约束集合光线来弄捕偏差、重尾不确定性，将流动性MPC重构为线性规划问题，采用Minkowski-Lyapunov成本函数和松弛变量来提高计算效率。通过集合光线的椭球近似求解最大稳定终端集和反馈增益。

Result: 在全向运动移动机器人进行了模拟和硬件实验，结果显示该感知敏感性MPC在重尾噪声条件下能够提供稳定准确的控制性能，在状态估计错误边界和整体控制性能方面显著超过传统高斯噪声设计。

Conclusion: 该框架通过显式考虑深度学习感知模块的非高斯噪声特征，提供了一种有效的解决方案，能够在保证闭环稳定性的同时提高控制系统的强镀性和可靠性。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>


### [11] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 通过改进劳动选择模块提供检查后的Q值，DRLR框架减少辅助学习错误，结合SAC策略避免次优化，在模拟和实际机器人任务中验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决传统深度强化学习在机器人任务中面临的辅助学习错误导致探索效率低下和可能氐合到次优化策略的问题

Method: 基于IBRL算法，改进劳动选择模块提供检查后的Q值，使用SAC代替TD3作为RL策略，在桶装载和打开抽屉等机器人任务中验证

Result: 在不同状态-劳动维度和示范质量的任务中都表现出稳健性，成功在真实轮式装载机上部署，验证了框架的有效性

Conclusion: DRLR框架通过维护检查后的Q值和使用SAC策略，有效减少辅助学习错误和避免次优化，在各种机器人任务中都表现出良好的性能和可部署性

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>
